summary
"Because of their occasional need to return to shallow points in a search
tree  existing backtracking methods can sometimes erase meaningful progress
toward solving a search problem. In this paper  we present a method by which
backtrack points can be moved deeper in the search space  thereby avoiding this
difficulty. The technique developed is a variant of dependency-directed
backtracking that uses only polynomial space while still providing useful
control information and retaining the completeness guarantees provided by
earlier approaches."
"Market price systems constitute a well-understood class of mechanisms that
under certain conditions provide effective decentralization of decision making
with minimal communication overhead. In a market-oriented programming approach
to distributed problem solving  we derive the activities and resource
allocations for a set of computational agents by computing the competitive
equilibrium of an artificial economy. WALRAS provides basic constructs for
defining computational market structures  and protocols for deriving their
corresponding price equilibria. In a particular realization of this approach
for a form of multicommodity flow problem  we see that careful construction of
the decision process according to economic principles can lead to efficient
distributed resource allocation  and that the behavior of the system can be
meaningfully analyzed in economic terms."
"We describe an extensive study of search in GSAT  an approximation procedure
for propositional satisfiability. GSAT performs greedy hill-climbing on the
number of satisfied clauses in a truth assignment. Our experiments provide a
more complete picture of GSAT's search than previous accounts. We describe in
detail the two phases of search. rapid hill-climbing followed by a long plateau
search. We demonstrate that when applied to randomly generated 3SAT problems 
there is a very simple scaling with problem size for both the mean number of
satisfied clauses and the mean branching rate. Our results allow us to make
detailed numerical conjectures about the length of the hill-climbing phase  the
average gradient of this phase  and to conjecture that both the average score
and average branching rate decay exponentially during plateau search. We end by
showing how these results can be used to direct future theoretical analysis.
This work provides a case study of how computer experiments can be used to
improve understanding of the theoretical properties of algorithms."
"As real logic programmers normally use cut (!)  an effective learning
procedure for logic programs should be able to deal with it. Because the cut
predicate has only a procedural meaning  clauses containing cut cannot be
learned using an extensional evaluation method  as is done in most learning
systems. On the other hand  searching a space of possible programs (instead of
a space of independent clauses) is unfeasible. An alternative solution is to
generate first a candidate base program which covers the positive examples  and
then make it consistent by inserting cut where appropriate. The problem of
learning programs with cut has not been investigated before and this seems to
be a natural and reasonable approach. We generalize this scheme and investigate
the difficulties that arise. Some of the major shortcomings are actually
caused  in general  by the need for intensional evaluation. As a conclusion 
the analysis of this paper suggests  on precise and technical grounds  that
learning cut is difficult  and current induction techniques should probably be
restricted to purely declarative logic languages."
"To support the goal of allowing users to record and retrieve information 
this paper describes an interactive note-taking system for pen-based computers
with two distinctive features. First  it actively predicts what the user is
going to write. Second  it automatically constructs a custom  button-box user
interface on request. The system is an example of a learning-apprentice
software- agent. A machine learning component characterizes the syntax and
semantics of the user's information. A performance system uses this learned
information to generate completion strings and construct a user interface.
Description of Online Appendix. People like to record information. Doing this
on paper is initially efficient  but lacks flexibility. Recording information
on a computer is less efficient but more powerful. In our new note taking
softwre  the user records information directly on a computer. Behind the
interface  an agent acts for the user. To help  it provides defaults and
constructs a custom user interface. The demonstration is a QuickTime movie of
the note taking agent in action. The file is a binhexed self-extracting
archive. Macintosh utilities for binhex are available from
mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the
dts/mac/sys.soft/quicktime."
"Terminological knowledge representation systems (TKRSs) are tools for
designing and using knowledge bases that make use of terminological languages
(or concept languages). We analyze from a theoretical point of view a TKRS
whose capabilities go beyond the ones of presently available TKRSs. The new
features studied  often required in practical applications  can be summarized
in three main points. First  we consider a highly expressive terminological
language  called ALCNR  including general complements of concepts  number
restrictions and role conjunction. Second  we allow to express inclusion
statements between general concepts  and terminological cycles as a particular
case. Third  we prove the decidability of a number of desirable TKRS-deduction
services (like satisfiability  subsumption and instance checking) through a
sound  complete and terminating calculus for reasoning in ALCNR-knowledge
bases. Our calculus extends the general technique of constraint systems. As a
byproduct of the proof  we get also the result that inclusion statements in
ALCNR can be simulated by terminological cycles  if descriptive semantics is
adopted."
"A formalism is presented for computing and organizing actions for autonomous
agents in dynamic environments. We introduce the notion of teleo-reactive (T-R)
programs whose execution entails the construction of circuitry for the
continuous computation of the parameters and conditions on which agent action
is based. In addition to continuous feedback  T-R programs support parameter
binding and recursion. A primary difference between T-R programs and many other
circuit-based systems is that the circuitry of T-R programs is more compact; it
is constructed at run time and thus does not have to anticipate all the
contingencies that might arise over all possible runs. In addition  T-R
programs are intuitive and easy to write and are written in a form that is
compatible with automatic planning and learning methods. We briefly describe
some experimental applications of T-R programs in the control of simulated and
actual mobile robots."
"Learning the past tense of English verbs - a seemingly minor aspect of
language acquisition - has generated heated debates since 1986  and has become
a landmark task for testing the adequacy of cognitive modeling. Several
artificial neural networks (ANNs) have been implemented  and a challenge for
better symbolic models has been posed. In this paper  we present a
general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree
learning algorithm ID3. We conduct extensive head-to-head comparisons on the
generalization ability between ANN models and the SPA under different
representations. We conclude that the SPA generalizes the past tense of unseen
verbs better than ANN models by a wide margin  and we offer insights as to why
this should be the case. We also discuss a new default strategy for
decision-tree learning algorithms."
"The ability to identify interesting and repetitive substructures is an
essential component to discovering knowledge in structural data. We describe a
new version of our SUBDUE substructure discovery system based on the minimum
description length principle. The SUBDUE system discovers substructures that
compress the original data and represent structural concepts in the data. By
replacing previously-discovered substructures in the data  multiple passes of
SUBDUE produce a hierarchical description of the structural regularities in the
data. SUBDUE uses a computationally-bounded inexact graph match that identifies
similar  but not identical  instances of a substructure and finds an
approximate measure of closeness of two substructures when under computational
constraints. In addition to the minimum description length principle  other
background knowledge can be used by SUBDUE to guide the search towards more
appropriate substructures. Experiments in a variety of domains demonstrate
SUBDUE's ability to find substructures capable of compressing the original data
and to discover structural concepts important to the domain. Description of
Online Appendix. This is a compressed tar file containing the SUBDUE discovery
system  written in C. The program accepts as input databases represented in
graph form  and will output discovered substructures with their corresponding
value."
"The theory revision problem is the problem of how best to go about revising a
deficient domain theory using information contained in examples that expose
inaccuracies. In this paper we present our approach to the theory revision
problem for propositional domain theories. The approach described here  called
PTR  uses probabilities associated with domain theory elements to numerically
track the ``flow'' of proof through the theory. This allows us to measure the
precise role of a clause or literal in allowing or preventing a (desired or
undesired) derivation for a given example. This information is used to
efficiently locate and repair flawed elements of the theory. PTR is proved to
converge to a theory which correctly classifies all examples  and shown
experimentally to be fast and accurate even for deep theories."
"We report on a series of experiments in which all decision trees consistent
with the training data are constructed. These experiments were run to gain an
understanding of the properties of the set of consistent decision trees and the
factors that affect the accuracy of individual trees. In particular  we
investigated the relationship between the size of a decision tree consistent
with some training data and the accuracy of the tree on test data. The
experiments were performed on a massively parallel Maspar computer. The results
of the experiments on several artificial and two real world problems indicate
that  for many of the problems investigated  smaller consistent decision trees
are on average less accurate than the average accuracy of slightly larger
trees."
"This paper analyzes the correctness of the subsumption algorithm used in
CLASSIC  a description logic-based knowledge representation system that is
being used in practical applications. In order to deal efficiently with
individuals in CLASSIC descriptions  the developers have had to use an
algorithm that is incomplete with respect to the standard  model-theoretic
semantics for description logics. We provide a variant semantics for
descriptions with respect to which the current implementation is complete  and
which can be independently motivated. The soundness and completeness of the
polynomial-time subsumption algorithm is established using description graphs 
which are an abstracted version of the implementation structures used in
CLASSIC  and are of independent interest."
"In this paper we describe how to modify GSAT so that it can be applied to
non-clausal formulas. The idea is to use a particular ``score'' function which
gives the number of clauses of the CNF conversion of a formula which are false
under a given truth assignment. Its value is computed in linear time  without
constructing the CNF conversion itself. The proposed methodology applies to
most of the variants of GSAT proposed so far."
"Given a knowledge base KB containing first-order and statistical facts  we
consider a principled method  called the random-worlds method  for computing a
degree of belief that some formula Phi holds given KB. If we are reasoning
about a world or system consisting of N individuals  then we can consider all
possible worlds  or first-order models  with domain {1 ... N} that satisfy KB 
and compute the fraction of them in which Phi is true. We define the degree of
belief to be the asymptotic value of this fraction as N grows large. We show
that when the vocabulary underlying Phi and KB uses constants and unary
predicates only  we can naturally associate an entropy with each world. As N
grows larger  there are many more worlds with higher entropy. Therefore  we can
use a maximum-entropy computation to compute the degree of belief. This result
is in a similar spirit to previous work in physics and artificial intelligence 
but is far more general. Of equal interest to the result itself are the
limitations on its scope. Most importantly  the restriction to unary predicates
seems necessary. Although the random-worlds method makes sense in general  the
connection to maximum entropy seems to disappear in the non-unary case. These
observations suggest unexpected limitations to the applicability of
maximum-entropy methods."
"Information extraction is the task of automatically picking up information of
interest from an unconstrained text. Information of interest is usually
extracted in two steps. First  sentence level processing locates relevant
pieces of information scattered throughout the text; second  discourse
processing merges coreferential information to generate the output. In the
first step  pieces of information are locally identified without recognizing
any relationships among them. A key word search or simple pattern search can
achieve this purpose. The second step requires deeper knowledge in order to
understand relationships among separately identified pieces of information.
Previous information extraction systems focused on the first step  partly
because they were not required to link up each piece of information with other
pieces. To link the extracted pieces of information and map them onto a
structured output format  complex discourse processing is essential. This paper
reports on a Japanese information extraction system that merges information
using a pattern matcher and discourse processor. Evaluation results show a high
level of system performance which approaches human performance."
"This article describes a new system for induction of oblique decision trees.
This system  OC1  combines deterministic hill-climbing with two forms of
randomization to find a good oblique split (in the form of a hyperplane) at
each node of a decision tree. Oblique decision tree methods are tuned
especially for domains in which the attributes are numeric  although they can
be adapted to symbolic or mixed symbolic/numeric attributes. We present
extensive empirical studies  using both real and artificial data  that analyze
OC1's ability to construct oblique trees that are smaller and more accurate
than their axis-parallel counterparts. We also examine the benefits of
randomization for the construction of oblique decision trees."
"This paper introduces a framework for Planning while Learning where an agent
is given a goal to achieve in an environment whose behavior is only partially
known to the agent. We discuss the tractability of various plan-design
processes. We show that for a large natural class of Planning while Learning
systems  a plan can be presented and verified in a reasonable time. However 
coming up algorithmically with a plan  even for simple classes of systems is
apparently intractable. We emphasize the role of off-line plan-design
processes  and show that  in most natural cases  the verification (projection)
part can be carried out in an efficient algorithmic manner."
"The vast amounts of on-line text now available have led to renewed interest
in information extraction (IE) systems that analyze unrestricted text 
producing a structured representation of selected information from the text.
This paper presents a novel approach that uses machine learning to acquire
knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE
discourse component that makes intersentential inferences and identifies
logical relations among information extracted from the text. Previous
corpus-based approaches were limited to lower level processing such as
part-of-speech tagging  lexical disambiguation  and dictionary construction.
Wrap-Up is fully trainable  and not only automatically decides what classifiers
are needed  but even derives the feature set for each classifier automatically.
Performance equals that of a partially trainable discourse module requiring
manual customization for each domain."
"This paper is a multidisciplinary review of empirical  statistical learning
from a graphical model perspective. Well-known examples of graphical models
include Bayesian networks  directed graphs representing a Markov chain  and
undirected networks representing a Markov field. These graphical models are
extended to model data analysis and empirical learning using the notation of
plates. Graphical operations for simplifying and manipulating a problem are
provided including decomposition  differentiation  and the manipulation of
probability models from the exponential family. Two standard algorithm schemas
for learning are reviewed in a graphical framework. Gibbs sampling and the
expectation maximization algorithm. Using these operations and schemas  some
popular algorithms can be synthesized from their graphical specification. This
includes versions of linear regression  techniques for feed-forward networks 
and learning Gaussian and discrete Bayesian networks from data. The paper
concludes by sketching some implications for data analysis and summarizing how
some popular algorithms fall within the framework presented. The main original
contributions here are the decomposition techniques and the demonstration that
graphical models provide a framework for understanding and developing complex
learning algorithms."
"For many years  the intuitions underlying partial-order planning were largely
taken for granted. Only in the past few years has there been renewed interest
in the fundamental principles underlying this paradigm. In this paper  we
present a rigorous comparative analysis of partial-order and total-order
planning by focusing on two specific planners that can be directly compared. We
show that there are some subtle assumptions that underly the wide-spread
intuitions regarding the supposed efficiency of partial-order planning. For
instance  the superiority of partial-order planning can depend critically upon
the search strategy and the structure of the search space. Understanding the
underlying assumptions is crucial for constructing efficient planners."
"Multiclass learning problems involve finding a definition for an unknown
function f(x) whose range is a discrete set containing k &gt 2 values (i.e.  k
``classes''). The definition is acquired by studying collections of training
examples of the form [x_i  f (x_i)]. Existing approaches to multiclass learning
problems include direct application of multiclass algorithms such as the
decision-tree algorithms C4.5 and CART  application of binary concept learning
algorithms to learn individual binary functions for each of the k classes  and
application of binary concept learning algorithms with distributed output
representations. This paper compares these three approaches to a new technique
in which error-correcting codes are employed as a distributed output
representation. We show that these output representations improve the
generalization performance of both C4.5 and backpropagation on a wide range of
multiclass learning tasks. We also demonstrate that this approach is robust
with respect to changes in the size of the training sample  the assignment of
distributed representations to particular classes  and the application of
overfitting avoidance techniques such as decision-tree pruning. Finally  we
show that---like the other methods---the error-correcting code technique can
provide reliable class probability estimates. Taken together  these results
demonstrate that error-correcting output codes provide a general-purpose method
for improving the performance of inductive learning programs on multiclass
problems."
"The paradigms of transformational planning  case-based planning  and plan
debugging all involve a process known as plan adaptation - modifying or
repairing an old plan so it solves a new problem. In this paper we provide a
domain-independent algorithm for plan adaptation  demonstrate that it is sound 
complete  and systematic  and compare it to other adaptation algorithms in the
literature. Our approach is based on a view of planning as searching a graph of
partial plans. Generative planning starts at the graph's root and moves from
node to node using plan-refinement operators. In planning by adaptation  a
library plan - an arbitrary node in the plan graph - is the starting point for
the search  and the plan-adaptation algorithm can apply both the same
refinement operators available to a generative planner and can also retract
constraints and steps from the plan. Our algorithm's completeness ensures that
the adaptation algorithm will eventually search the entire graph and its
systematicity ensures that it will do so without redundantly searching any
parts of the graph."
"Temporal difference (TD) methods constitute a class of methods for learning
predictions in multi-step prediction problems  parameterized by a recency
factor lambda. Currently the most important application of these methods is to
temporal credit assignment in reinforcement learning. Well known reinforcement
learning algorithms  such as AHC or Q-learning  may be viewed as instances of
TD learning. This paper examines the issues of the efficient and general
implementation of TD(lambda) for arbitrary lambda  for use with reinforcement
learning algorithms optimizing the discounted sum of rewards. The traditional
approach  based on eligibility traces  is argued to suffer from both
inefficiency and lack of generality. The TTD (Truncated Temporal Differences)
procedure is proposed as an alternative  that indeed only approximates
TD(lambda)  but requires very little computation per action and can be used
with arbitrary function representation methods. The idea from which it is
derived is fairly simple and not new  but probably unexplored so far.
Encouraging experimental results are presented  suggesting that using lambda
&gt 0 with the TTD procedure allows one to obtain a significant learning
speedup at essentially the same cost as usual TD(0) learning."
"This paper introduces ICET  a new algorithm for cost-sensitive
classification. ICET uses a genetic algorithm to evolve a population of biases
for a decision tree induction algorithm. The fitness function of the genetic
algorithm is the average cost of classification when using the decision tree 
including both the costs of tests (features  measurements) and the costs of
classification errors. ICET is compared here with three other algorithms for
cost-sensitive classification - EG2  CS-ID3  and IDX - and also with C4.5 
which classifies without regard to cost. The five algorithms are evaluated
empirically on five real-world medical datasets. Three sets of experiments are
performed. The first set examines the baseline performance of the five
algorithms on the five datasets and establishes that ICET performs
significantly better than its competitors. The second set tests the robustness
of ICET under a variety of conditions and shows that ICET maintains its
advantage. The third set looks at ICET's search in bias space and discovers a
way to improve the search."
"Theory revision integrates inductive learning and background knowledge by
combining training examples with a coarse domain theory to produce a more
accurate theory. There are two challenges that theory revision and other
theory-guided systems face. First  a representation language appropriate for
the initial theory may be inappropriate for an improved theory. While the
original representation may concisely express the initial theory  a more
accurate theory forced to use that same representation may be bulky 
cumbersome  and difficult to reach. Second  a theory structure suitable for a
coarse domain theory may be insufficient for a fine-tuned theory. Systems that
produce only small  local changes to a theory have limited value for
accomplishing complex structural alterations that may be required.
Consequently  advanced theory-guided learning systems require flexible
representation and flexible structure. An analysis of various theory revision
systems and theory-guided learning systems reveals specific strengths and
weaknesses in terms of these two desired properties. Designed to capture the
underlying qualities of each system  a new system uses theory-guided
constructive induction. Experiments in three domains show improvement over
previous theory-guided systems. This leads to a study of the behavior 
limitations  and potential of theory-guided constructive induction."
"Many studies have been carried out in order to increase the search efficiency
of constraint satisfaction problems; among them  some make use of structural
properties of the constraint network; others take into account semantic
properties of the constraints  generally assuming that all the constraints
possess the given property. In this paper  we propose a new decomposition
method benefiting from both semantic properties of functional constraints (not
bijective constraints) and structural properties of the network; furthermore 
not all the constraints need to be functional. We show that under some
conditions  the existence of solutions can be guaranteed. We first characterize
a particular subset of the variables  which we name a root set. We then
introduce pivot consistency  a new local consistency which is a weak form of
path consistency and can be achieved in O(n^2d^2) complexity (instead of
O(n^3d^3) for path consistency)  and we present associated properties; in
particular  we show that any consistent instantiation of the root set can be
linearly extended to a solution  which leads to the presentation of the
aforementioned new method for solving by decomposing functional CSPs."
"We study the process of multi-agent reinforcement learning in the context of
load balancing in a distributed system  without use of either central
coordination or explicit communication. We first define a precise framework in
which to study adaptive load balancing  important features of which are its
stochastic nature and the purely local information available to individual
agents. Given this framework  we show illuminating results on the interplay
between basic adaptive behavior parameters and their effect on system
efficiency. We then investigate the properties of adaptive load balancing in
heterogeneous populations  and address the issue of exploration vs.
exploitation in that context. Finally  we show that naive use of communication
may not improve  and might even harm system efficiency."
"Since its inception  artificial intelligence has relied upon a theoretical
foundation centered around perfect rationality as the desired property of
intelligent systems. We argue  as others have done  that this foundation is
inadequate because it imposes fundamentally unsatisfiable requirements. As a
result  there has arisen a wide gap between theory and practice in AI 
hindering progress in the field. We propose instead a property called bounded
optimality. Roughly speaking  an agent is bounded-optimal if its program is a
solution to the constrained optimization problem presented by its architecture
and the task environment. We show how to construct agents with this property
for a simple class of machine architectures in a broad class of real-time
environments. We illustrate these results using a simple model of an automated
mail sorting facility. We also define a weaker property  asymptotic bounded
optimality (ABO)  that generalizes the notion of optimality in classical
complexity theory. We then construct universal ABO programs  i.e.  programs
that are ABO no matter what real-time constraints are applied. Universal ABO
programs can be used as building blocks for more complex systems. We conclude
with a discussion of the prospects for bounded optimality as a theoretical
basis for AI  and relate it to similar trends in philosophy  economics  and
game theory."
"We present algorithms that learn certain classes of function-free recursive
logic programs in polynomial time from equivalence queries. In particular  we
show that a single k-ary recursive constant-depth determinate clause is
learnable. Two-clause programs consisting of one learnable recursive clause and
one constant-depth determinate non-recursive clause are also learnable  if an
additional ``basecase'' oracle is assumed. These results immediately imply the
pac-learnability of these classes. Although these classes of learnable
recursive programs are very constrained  it is shown in a companion paper that
they are maximally general  in that generalizing either class in any natural
way leads to a computationally difficult learning problem. Thus  taken together
with its companion paper  this paper establishes a boundary of efficient
learnability for recursive logic programs."
"In a companion paper it was shown that the class of constant-depth
determinate k-ary recursive clauses is efficiently learnable. In this paper we
present negative results showing that any natural generalization of this class
is hard to learn in Valiant's model of pac-learnability. In particular  we show
that the following program classes are cryptographically hard to learn.
programs with an unbounded number of constant-depth linear recursive clauses;
programs with one constant-depth determinate clause containing an unbounded
number of recursive calls; and programs with one linear recursive clause of
constant locality. These results immediately imply the non-learnability of any
more general class of programs. We also show that learning a constant-depth
determinate program with either two linear recursive clauses or one linear
recursive clause and one non-recursive clause is as hard as learning boolean
DNF. Together with positive results from the companion paper  these negative
results establish a boundary of efficient learnability for recursive
function-free clauses."
"There has been evidence that least-commitment planners can efficiently handle
planning problems that involve difficult goal interactions. This evidence has
led to the common belief that delayed-commitment is the ""best"" possible
planning strategy. However  we recently found evidence that eager-commitment
planners can handle a variety of planning problems more efficiently  in
particular those with difficult operator choices. Resigned to the futility of
trying to find a universally successful planning strategy  we devised a planner
that can be used to study which domains and problems are best for which
planning strategies. In this article we introduce this new planning algorithm 
FLECS  which uses a FLExible Commitment Strategy with respect to plan-step
orderings. It is able to use any strategy from delayed-commitment to
eager-commitment. The combination of delayed and eager operator-ordering
commitments allows FLECS to take advantage of the benefits of explicitly using
a simulated execution state and reasoning about planning constraints. FLECS can
vary its commitment strategy across different problems and domains  and also
during the course of a single planning problem. FLECS represents a novel
contribution to planning in that it explicitly provides the choice of which
commitment strategy to use while planning. FLECS provides a framework to
investigate the mapping from planning domains and problems to efficient
planning strategies."
"This paper presents a method for inducing logic programs from examples that
learns a new class of concepts called first-order decision lists  defined as
ordered lists of clauses each ending in a cut. The method  called FOIDL  is
based on FOIL (Quinlan  1990) but employs intensional background knowledge and
avoids the need for explicit negative examples. It is particularly useful for
problems that involve rules with specific exceptions  such as learning the
past-tense of English verbs  a task widely studied in the context of the
symbolic/connectionist debate. FOIDL is able to learn concise  accurate
programs for this problem from significantly fewer examples than previous
methods (both connectionist and symbolic)."
"ion is one of the most promising approaches to improve the performance of
problem solvers. In several domains abstraction by dropping sentences of a
domain description -- as used in most hierarchical planners -- has proven
useful. In this paper we present examples which illustrate significant
drawbacks of abstraction by dropping sentences. To overcome these drawbacks  we
propose a more general view of abstraction involving the change of
representation language. We have developed a new abstraction methodology and a
related sound and complete learning algorithm that allows the complete change
of representation language of planning cases from concrete to abstract.
However  to achieve a powerful change of the representation language  the
abstract language itself as well as rules which describe admissible ways of
abstracting states must be provided in the domain model. This new abstraction
approach is the core of Paris (Plan Abstraction and Refinement in an Integrated
System)  a system in which abstract planning cases are automatically learned
from given concrete cases. An empirical study in the domain of process planning
in mechanical engineering shows significant advantages of the proposed
reasoning from abstract cases over classical hierarchical planning."
"Identifying inaccurate data has long been regarded as a significant and
difficult problem in AI. In this paper  we present a new method for identifying
inaccurate data on the basis of qualitative correlations among related data.
First  we introduce the definitions of related data and qualitative
correlations among related data. Then we put forward a new concept called
support coefficient function (SCF). SCF can be used to extract  represent  and
calculate qualitative correlations among related data within a dataset. We
propose an approach to determining dynamic shift intervals of inaccurate data 
and an approach to calculating possibility of identifying inaccurate data 
respectively. Both of the approaches are based on SCF. Finally we present an
algorithm for identifying inaccurate data by using qualitative correlations
among related data as confirmatory or disconfirmatory evidence. We have
developed a practical system for interpreting infrared spectra by applying the
method  and have fully tested the system against several hundred real spectra.
The experimental results show that the method is significantly better than the
conventional methods used in many similar systems."
"Learning and reasoning are both aspects of what is considered to be
intelligence. Their studies within AI have been separated historically 
learning being the topic of machine learning and neural networks  and reasoning
falling under classical (or symbolic) AI. However  learning and reasoning are
in many ways interdependent. This paper discusses the nature of some of these
interdependencies and proposes a general framework called FLARE  that combines
inductive learning using prior knowledge together with reasoning in a
propositional setting. Several examples that test the framework are presented 
including classical induction  many important reasoning protocols and two
simple expert systems."
"This paper studies the problem of ergodicity of transition probability
matrices in Markovian models  such as hidden Markov models (HMMs)  and how it
makes very difficult the task of learning to represent long-term context for
sequential data. This phenomenon hurts the forward propagation of long-term
context information  as well as learning a hidden state representation to
represent long-term context  which depends on propagating credit information
backwards in time. Using results from Markov chain theory  we show that this
problem of diffusion of context and credit is reduced when the transition
probabilities approach 0 or 1  i.e.  the transition probability matrices are
sparse and the model essentially deterministic. The results found in this paper
apply to learning approaches based on continuous optimization  such as gradient
descent and the Baum-Welch algorithm."
"Symmetric networks designed for energy minimization such as Boltzman machines
and Hopfield nets are frequently investigated for use in optimization 
constraint satisfaction and approximation of NP-hard problems. Nevertheless 
finding a global solution (i.e.  a global minimum for the energy function) is
not guaranteed and even a local solution may take an exponential number of
steps. We propose an improvement to the standard local activation function used
for such networks. The improved algorithm guarantees that a global minimum is
found in linear time for tree-like subnetworks. The algorithm  called activate 
is uniform and does not assume that the network is tree-like. It can identify
tree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid
local minima along these trees. For acyclic networks  the algorithm is
guaranteed to converge to a global minimum from any initial state of the system
(self-stabilization) and remains correct under various types of schedulers. On
the negative side  we show that in the presence of cycles  no uniform algorithm
exists that guarantees optimality even under a sequential asynchronous
scheduler. An asynchronous scheduler can activate only one unit at a time while
a synchronous scheduler can activate any number of units in a single time step.
In addition  no uniform algorithm exists to optimize even acyclic networks when
the scheduler is synchronous. Finally  we show how the algorithm can be
improved using the cycle-cutset scheme. The general algorithm  called
activate-with-cutset  improves over activate and has some performance
guarantees that are related to the size of the network's cycle-cutset."
"Functionality-based recognition systems recognize objects at the category
level by reasoning about how well the objects support the expected function.
Such systems naturally associate a ``measure of goodness'' or ``membership
value'' with a recognized object. This measure of goodness is the result of
combining individual measures  or membership values  from potentially many
primitive evaluations of different properties of the object's shape. A
membership function is used to compute the membership value when evaluating a
primitive of a particular physical property of an object. In previous versions
of a recognition system known as Gruff  the membership function for each of the
primitive evaluations was hand-crafted by the system designer. In this paper 
we provide a learning component for the Gruff system  called Omlet  that
automatically learns membership functions given a set of example objects
labeled with their desired category measure. The learning algorithm is
generally applicable to any problem in which low-level membership values are
combined through an and-or tree structure to give a final overall membership
value."
"This paper presents an approach to learning from situated  interactive
tutorial instruction within an ongoing agent. Tutorial instruction is a
flexible (and thus powerful) paradigm for teaching tasks because it allows an
instructor to communicate whatever types of knowledge an agent might need in
whatever situations might arise. To support this flexibility  however  the
agent must be able to learn multiple kinds of knowledge from a broad range of
instructional interactions. Our approach  called situated explanation  achieves
such learning through a combination of analytic and inductive techniques. It
combines a form of explanation-based learning that is situated for each
instruction with a full suite of contextually guided responses to incomplete
explanations. The approach is implemented in an agent called Instructo-Soar
that learns hierarchies of new tasks and other domain knowledge from
interactive natural language instructions. Instructo-Soar meets three key
requirements of flexible instructability that distinguish it from previous
systems. (1) it can take known or unknown commands at any instruction point;
(2) it can handle instructions that apply to either its current situation or to
a hypothetical situation specified in language (as in  for instance 
conditional instructions); and (3) it can learn  from instructions  each class
of knowledge it uses to perform tasks."
"OPUS is a branch and bound search algorithm that enables efficient admissible
search through spaces for which the order of search operator application is not
significant. The algorithm's search efficiency is demonstrated with respect to
very large machine learning search spaces. The use of admissible search is of
potential value to the machine learning community as it means that the exact
learning biases to be employed for complex learning tasks can be precisely
specified and manipulated. OPUS also has potential for application in other
areas of artificial intelligence  notably  truth maintenance."
"The main aim of this work is the development of a vision-based road detection
system fast enough to cope with the difficult real-time constraints imposed by
moving vehicle applications. The hardware platform  a special-purpose massively
parallel system  has been chosen to minimize system production and operational
costs. This paper presents a novel approach to expectation-driven low-level
image segmentation  which can be mapped naturally onto mesh-connected massively
parallel SIMD architectures capable of handling hierarchical data structures.
The input image is assumed to contain a distorted version of a given template;
a multiresolution stretching process is used to reshape the original template
in accordance with the acquired image content  minimizing a potential function.
The distorted template is the process output."
"In the area of inductive learning  generalization is a main operation  and
the usual definition of induction is based on logical implication. Recently
there has been a rising interest in clausal representation of knowledge in
machine learning. Almost all inductive learning systems that perform
generalization of clauses use the relation theta-subsumption instead of
implication. The main reason is that there is a well-known and simple technique
to compute least general generalizations under theta-subsumption  but not under
implication. However generalization under theta-subsumption is inappropriate
for learning recursive clauses  which is a crucial problem since recursion is
the basic program structure of logic programs. We note that implication between
clauses is undecidable  and we therefore introduce a stronger form of
implication  called T-implication  which is decidable between clauses. We show
that for every finite set of clauses there exists a least general
generalization under T-implication. We describe a technique to reduce
generalizations under implication of a clause to generalizations under
theta-subsumption of what we call an expansion of the original clause. Moreover
we show that for every non-tautological clause there exists a T-complete
expansion  which means that every generalization under T-implication of the
clause is reduced to a generalization under theta-subsumption of the expansion."
"We present a definition of cause and effect in terms of decision-theoretic
primitives and thereby provide a principled foundation for causal reasoning.
Our definition departs from the traditional view of causation in that causal
assertions may vary with the set of decisions available. We argue that this
approach provides added clarity to the notion of cause. Also in this paper  we
examine the encoding of causal relationships in directed acyclic graphs. We
describe a special class of influence diagrams  those in canonical form  and
show its relationship to Pearl's representation of cause and effect. Finally 
we show how canonical form facilitates counterfactual reasoning."
"Characteristic models are an alternative  model based  representation for
Horn expressions. It has been shown that these two representations are
incomparable and each has its advantages over the other. It is therefore
natural to ask what is the cost of translating  back and forth  between these
representations. Interestingly  the same translation questions arise in
database theory  where it has applications to the design of relational
databases. This paper studies the computational complexity of these problems.
Our main result is that the two translation problems are equivalent under
polynomial reductions  and that they are equivalent to the corresponding
decision problem. Namely  translating is equivalent to deciding whether a given
set of models is the set of characteristic models for a given Horn expression.
We also relate these problems to the hypergraph transversal problem  a well
known problem which is related to other applications in AI and for which no
polynomial time algorithm is known. It is shown that in general our translation
problems are at least as hard as the hypergraph transversal problem  and in a
special case they are equivalent to it."
"This article describes an application of three well-known statistical methods
in the field of game-tree search. using a large number of classified Othello
positions  feature weights for evaluation functions with a
game-phase-independent meaning are estimated by means of logistic regression 
Fisher's linear discriminant  and the quadratic discriminant function for
normally distributed features. Thereafter  the playing strengths are compared
by means of tournaments between the resulting versions of a world-class Othello
program. In this application  logistic regression - which is used here for the
first time in the context of game playing - leads to better results than the
other approaches."
"We describe a machine learning method for predicting the value of a
real-valued function  given the values of multiple input variables. The method
induces solutions from samples in the form of ordered disjunctive normal form
(DNF) decision rules. A central objective of the method and representation is
the induction of compact  easily interpretable solutions. This rule-based
decision model can be extended to search efficiently for similar cases prior to
approximating function values. Experimental results on real-world data
demonstrate that the new techniques are competitive with existing machine
learning and statistical methods and can sometimes yield superior regression
performance."
"Many applications -- from planning and scheduling to problems in molecular
biology -- rely heavily on a temporal reasoning component. In this paper  we
discuss the design and empirical analysis of algorithms for a temporal
reasoning system based on Allen's influential interval-based framework for
representing temporal information. At the core of the system are algorithms for
determining whether the temporal information is consistent  and  if so  finding
one or more scenarios that are consistent with the temporal information. Two
important algorithms for these tasks are a path consistency algorithm and a
backtracking algorithm. For the path consistency algorithm  we develop
techniques that can result in up to a ten-fold speedup over an already highly
optimized implementation. For the backtracking algorithm  we develop variable
and value ordering heuristics that are shown empirically to dramatically
improve the performance of the algorithm. As well  we show that a previously
suggested reformulation of the backtracking search problem can reduce the time
and space requirements of the backtracking search. Taken together  the
techniques we develop allow a temporal reasoning component to solve problems
that are of practical size."
"The paper describes an extension of well-founded semantics for logic programs
with two types of negation. In this extension information about preferences
between rules can be expressed in the logical language and derived dynamically.
This is achieved by using a reserved predicate symbol and a naming technique.
Conflicts among rules are resolved whenever possible on the basis of derived
preference information. The well-founded conclusions of prioritized logic
programs can be computed in polynomial time. A legal reasoning example
illustrates the usefulness of the approach."
"Traditional databases commonly support efficient query and update procedures
that operate in time which is sublinear in the size of the database. Our goal
in this paper is to take a first step toward dynamic reasoning in probabilistic
databases with comparable efficiency. We propose a dynamic data structure that
supports efficient algorithms for updating and querying singly connected
Bayesian networks. In the conventional algorithm  new evidence is absorbed in
O(1) time and queries are processed in time O(N)  where N is the size of the
network. We propose an algorithm which  after a preprocessing phase  allows us
to answer queries in time O(log N) at the expense of O(log N) time per evidence
absorption. The usefulness of sub-linear processing time manifests itself in
applications requiring (near) real-time response over large probabilistic
databases. We briefly discuss a potential application of dynamic probabilistic
reasoning in computational biology."
"We introduce an algorithm for combinatorial search on quantum computers that
is capable of significantly concentrating amplitude into solutions for some NP
search problems  on average. This is done by exploiting the same aspects of
problem structure as used by classical backtrack methods to avoid unproductive
search choices. This quantum algorithm is much more likely to find solutions
than the simple direct use of quantum parallelism. Furthermore  empirical
evaluation on small problems shows this quantum algorithm displays the same
phase transition behavior  and at the same location  as seen in many previously
studied classical search methods. Specifically  difficult problem instances are
concentrated near the abrupt change from underconstrained to overconstrained
problems."
"We develop a mean field theory for sigmoid belief networks based on ideas
from statistical mechanics. Our mean field theory provides a tractable
approximation to the true probability distribution in these networks; it also
yields a lower bound on the likelihood of evidence. We demonstrate the utility
of this framework on a benchmark problem in statistical pattern
recognition---the classification of handwritten digits."
"A reported weakness of C4.5 in domains with continuous attributes is
addressed by modifying the formation and evaluation of tests on continuous
attributes. An MDL-inspired penalty is applied to such tests  eliminating some
of them from consideration and altering the relative desirability of all tests.
Empirical trials show that the modifications lead to smaller decision trees
with higher predictive accuracies. Results also confirm that a new version of
C4.5 incorporating these changes is superior to recent approaches that use
global discretization and that construct small trees with multi-interval
splits."
"For many types of machine learning algorithms  one can compute the
statistically `optimal' way to select training data. In this paper  we review
how optimal data selection techniques have been used with feedforward neural
networks. We then show how the same principles may be used to select data for
two alternative  statistically-based learning architectures. mixtures of
Gaussians and locally weighted regression. While the techniques for neural
networks are computationally expensive and approximate  the techniques for
mixtures of Gaussians and locally weighted regression are both efficient and
accurate. Empirically  we observe that the optimality criterion sharply
decreases the number of training examples the learner needs in order to achieve
good performance."
"Inductive theorem provers often diverge. This paper describes a simple
critic  a computer program which monitors the construction of inductive proofs
attempting to identify diverging proof attempts. Divergence is recognized by
means of a ``difference matching'' procedure. The critic then proposes lemmas
and generalizations which ``ripple'' these differences away so that the proof
can go through without divergence. The critic enables the theorem prover Spike
to prove many theorems completely automatically from the definitions alone."
"Termination of logic programs with negated body atoms (here called general
logic programs) is an important topic. One reason is that many computational
mechanisms used to process negated atoms  like Clark's negation as failure and
Chan's constructive negation  are based on termination conditions. This paper
introduces a methodology for proving termination of general logic programs
w.r.t. the Prolog selection rule. The idea is to distinguish parts of the
program depending on whether or not their termination depends on the selection
rule. To this end  the notions of low-  weakly up-  and up-acceptable program
are introduced. We use these notions to develop a methodology for proving
termination of general logic programs  and show how interesting problems in
non-monotonic reasoning can be formalized and implemented by means of
terminating general logic programs."
"Clustering is often used for discovering structure in data. Clustering
systems differ in the objective function used to evaluate clustering quality
and the control strategy used to search the space of clusterings. Ideally  the
search strategy should consistently construct clusterings of high quality  but
be computationally inexpensive as well. In general  we cannot have it both
ways  but we can partition the search so that a system inexpensively constructs
a `tentative' clustering for initial examination  followed by iterative
optimization  which continues to search in background for improved clusterings.
Given this motivation  we evaluate an inexpensive strategy for creating initial
clusterings  coupled with several control strategies for iterative
optimization  each of which repeatedly modifies an initial clustering in search
of a better one. One of these methods appears novel as an iterative
optimization strategy in clustering contexts. Once a clustering has been
constructed it is judged by analysts -- often according to task-specific
criteria. Several authors have abstracted these criteria and posited a generic
performance task akin to pattern completion  where the error rate over
completed patterns is used to `externally' judge clustering utility. Given this
performance task  we adapt resampling-based pruning strategies used by
supervised learning systems to the task of simplifying hierarchical
clusterings  thus promising to ease post-clustering analysis. Finally  we
propose a number of objective functions  based on attribute-selection measures
for decision-tree induction  that might perform well on the error rate and
simplicity dimensions."
"This paper presents new experimental evidence against the utility of Occam's
razor. A~systematic procedure is presented for post-processing decision trees
produced by C4.5. This procedure was derived by rejecting Occam's razor and
instead attending to the assumption that similar objects are likely to belong
to the same class. It increases a decision tree's complexity without altering
the performance of that tree on the training data from which it is inferred.
The resulting more complex decision trees are demonstrated to have  on average 
for a variety of common learning tasks  higher predictive accuracy than the
less complex original decision trees. This result raises considerable doubt
about the utility of Occam's razor as it is commonly applied in modern machine
learning."
"The main operations in Inductive Logic Programming (ILP) are generalization
and specialization  which only make sense in a generality order. In ILP  the
three most important generality orders are subsumption  implication and
implication relative to background knowledge. The two languages used most often
are languages of clauses and languages of only Horn clauses. This gives a total
of six different ordered languages. In this paper  we give a systematic
treatment of the existence or non-existence of least generalizations and
greatest specializations of finite sets of clauses in each of these six ordered
sets. We survey results already obtained by others and also contribute some
answers of our own. Our main new results are  firstly  the existence of a
computable least generalization under implication of every finite set of
clauses containing at least one non-tautologous function-free clause (among
other  not necessarily function-free clauses). Secondly  we show that such a
least generalization need not exist under relative implication  not even if
both the set that is to be generalized and the background knowledge are
function-free. Thirdly  we give a complete discussion of existence and
non-existence of greatest specializations in each of the six ordered languages."
"This paper surveys the field of reinforcement learning from a
computer-science perspective. It is written to be accessible to researchers
familiar with machine learning. Both the historical basis of the field and a
broad selection of current work are summarized. Reinforcement learning is the
problem faced by an agent that learns behavior through trial-and-error
interactions with a dynamic environment. The work described here has a
resemblance to work in psychology  but differs considerably in the details and
in the use of the word ``reinforcement.'' The paper discusses central issues of
reinforcement learning  including trading off exploration and exploitation 
establishing the foundations of the field via Markov decision theory  learning
from delayed reinforcement  constructing empirical models to accelerate
learning  making use of generalization and hierarchy  and coping with hidden
state. It concludes with a survey of some implemented systems and an assessment
of the practical utility of current methods for reinforcement learning."
"Although most scheduling problems are NP-hard  domain specific techniques
perform well in practice but are quite expensive to construct. In adaptive
problem-solving solving  domain specific knowledge is acquired automatically
for a general problem solver with a flexible control architecture. In this
approach  a learning system explores a space of possible heuristic methods for
one well-suited to the eccentricities of the given domain and problem
distribution. In this article  we discuss an application of the approach to
scheduling satellite communications. Using problem distributions based on
actual mission requirements  our approach identifies strategies that not only
decrease the amount of CPU time required to produce schedules  but also
increase the percentage of problems that are solvable within computational
resource limitations."
"Speedup learning seeks to improve the computational efficiency of problem
solving with experience. In this paper  we develop a formal framework for
learning efficient problem solving from random problems and their solutions. We
apply this framework to two different representations of learned knowledge 
namely control rules and macro-operators  and prove theorems that identify
sufficient conditions for learning in each representation. Our proofs are
constructive in that they are accompanied with learning algorithms. Our
framework captures both empirical and explanation-based speedup learning in a
unified fashion. We illustrate our framework with implementations in two
domains. symbolic integration and Eight Puzzle. This work integrates many
strands of experimental and theoretical work in machine learning  including
empirical learning of control rules  macro-operator learning  Explanation-Based
Learning (EBL)  and Probably Approximately Correct (PAC) Learning."
"A fundamental assumption made by classical AI planners is that there is no
uncertainty in the world. the planner has full knowledge of the conditions
under which the plan will be executed and the outcome of every action is fully
predictable. These planners cannot therefore construct contingency plans  i.e. 
plans in which different actions are performed in different circumstances. In
this paper we discuss some issues that arise in the representation and
construction of contingency plans and describe Cassandra  a partial-order
contingency planner. Cassandra uses explicit decision-steps that enable the
agent executing the plan to decide which plan branch to follow. The
decision-steps in a plan result in subgoals to acquire knowledge  which are
planned for in the same way as any other subgoals. Cassandra thus distinguishes
the process of gathering information from the process of making decisions. The
explicit representation of decisions in Cassandra allows a coherent approach to
the problems of contingent planning  and provides a solid base for extensions
such as the use of different decision-making procedures."
"An important problem in geometric reasoning is to find the configuration of a
collection of geometric bodies so as to satisfy a set of given constraints.
Recently  it has been suggested that this problem can be solved efficiently by
symbolically reasoning about geometry. This approach  called degrees of freedom
analysis  employs a set of specialized routines called plan fragments that
specify how to change the configuration of a set of bodies to satisfy a new
constraint while preserving existing constraints. A potential drawback  which
limits the scalability of this approach  is concerned with the difficulty of
writing plan fragments. In this paper we address this limitation by showing how
these plan fragments can be automatically synthesized using first principles
about geometric bodies  actions  and topology."
"Motivated by the control theoretic distinction between controllable and
uncontrollable events  we distinguish between two types of agents within a
multi-agent system. controllable agents  which are directly controlled by the
system's designer  and uncontrollable agents  which are not under the
designer's direct control. We refer to such systems as partially controlled
multi-agent systems  and we investigate how one might influence the behavior of
the uncontrolled agents through appropriate design of the controlled agents. In
particular  we wish to understand which problems are naturally described in
these terms  what methods can be applied to influence the uncontrollable
agents  the effectiveness of such methods  and whether similar methods work
across different domains. Using a game-theoretic framework  this paper studies
the design of partially controlled multi-agent systems in two contexts. in one
context  the uncontrollable agents are expected utility maximizers  while in
the other they are reinforcement learners. We suggest different techniques for
controlling agents' behavior in each domain  assess their success  and examine
their relationship."
"Visual thinking plays an important role in scientific reasoning. Based on the
research in automating diverse reasoning tasks about dynamical systems 
nonlinear controllers  kinematic mechanisms  and fluid motion  we have
identified a style of visual thinking  imagistic reasoning. Imagistic reasoning
organizes computations around image-like  analogue representations so that
perceptual and symbolic operations can be brought to bear to infer structure
and behavior. Programs incorporating imagistic reasoning have been shown to
perform at an expert level in domains that defy current analytic or numerical
methods. We have developed a computational paradigm  spatial aggregation  to
unify the description of a class of imagistic problem solvers. A program
written in this paradigm has the following properties. It takes a continuous
field and optional objective functions as input  and produces high-level
descriptions of structure  behavior  or control actions. It computes a
multi-layer of intermediate representations  called spatial aggregates  by
forming equivalence classes and adjacency relations. It employs a small set of
generic operators such as aggregation  classification  and localization to
perform bidirectional mapping between the information-rich field and
successively more abstract spatial aggregates. It uses a data structure  the
neighborhood graph  as a common interface to modularize computations. To
illustrate our theory  we describe the computational structure of three
implemented problem solvers -- KAM  MAPS  and HIPAIR --- in terms of the
spatial aggregation generic operators by mixing and matching a library of
commonly used routines."
"Finding the stable models of a knowledge base is a significant computational
problem in artificial intelligence. This task is at the computational heart of
truth maintenance systems  autoepistemic logic  and default logic.
Unfortunately  it is NP-hard. In this paper we present a hierarchy of classes
of knowledge bases  Omega_1 Omega_2 ...  with the following properties. first 
Omega_1 is the class of all stratified knowledge bases; second  if a knowledge
base Pi is in Omega_k  then Pi has at most k stable models  and all of them may
be found in time O(lnk)  where l is the length of the knowledge base and n the
number of atoms in Pi; third  for an arbitrary knowledge base Pi  we can find
the minimum k such that Pi belongs to Omega_k in time polynomial in the size of
Pi; and  last  where K is the class of all knowledge bases  it is the case that
union{i=1 to infty} Omega_i = K  that is  every knowledge base belongs to some
class in the hierarchy."
"We propose some domain-independent techniques for bringing well-founded
partial-order planners closer to practicality. The first two techniques are
aimed at improving search control while keeping overhead costs low. One is
based on a simple adjustment to the default A* heuristic used by UCPOP to
select plans for refinement. The other is based on preferring ``zero
commitment'' (forced) plan refinements whenever possible  and using LIFO
prioritization otherwise. A more radical technique is the use of operator
parameter domains to prune search. These domains are initially computed from
the definitions of the operators and the initial and goal conditions  using a
polynomial-time algorithm that propagates sets of constants through the
operator graph  starting in the initial conditions. During planning  parameter
domains can be used to prune nonviable operator instances and to remove
spurious clobbering threats. In experiments based on modifications of UCPOP 
our improved plan and goal selection strategies gave speedups by factors
ranging from 5 to more than 1000 for a variety of problems that are nontrivial
for the unmodified version. Crucially  the hardest problems gave the greatest
improvements. The pruning technique based on parameter domains often gave
speedups by an order of magnitude or more for difficult problems  both with the
default UCPOP search strategy and with our improved strategy. The Lisp code for
our techniques and for the test problems is provided in on-line appendices."
"Cue phrases may be used in a discourse sense to explicitly signal discourse
structure  but also in a sentential sense to convey semantic rather than
structural information. Correctly classifying cue phrases as discourse or
sentential is critical in natural language processing systems that exploit
discourse structure  e.g.  for performing tasks such as anaphora resolution and
plan recognition. This paper explores the use of machine learning for
classifying cue phrases as discourse or sentential. Two machine learning
programs (Cgrendel and C4.5) are used to induce classification models from sets
of pre-classified cue phrases and their features in text and speech. Machine
learning is shown to be an effective technique for not only automating the
generation of classification models  but also for improving upon previous
results. When compared to manually derived classification models already in the
literature  the learned models often perform with higher accuracy and contain
new linguistic insights into the data. In addition  the ability to
automatically construct classification models makes it easier to comparatively
analyze the utility of alternative feature representations of the data.
Finally  the ease of retraining makes the learning approach more scalable and
flexible than manual methods."
"This paper lays part of the groundwork for a domain theory of negotiation 
that is  a way of classifying interactions so that it is clear  given a domain 
which negotiation mechanisms and strategies are appropriate. We define State
Oriented Domains  a general category of interaction. Necessary and sufficient
conditions for cooperation are outlined. We use the notion of worth in an
altered definition of utility  thus enabling agreements in a wider class of
joint-goal reachable situations. An approach is offered for conflict
resolution  and it is shown that even in a conflict situation  partial
cooperative steps can be taken by interacting agents (that is  agents in
fundamental conflict might still agree to cooperate up to a certain point). A
Unified Negotiation Protocol (UNP) is developed that can be used in all types
of encounters. It is shown that in certain borderline cooperative situations  a
partial cooperative agreement (i.e.  one that does not achieve all agents'
goals) might be preferred by all agents  even though there exists a rational
agreement that would achieve all their goals. Finally  we analyze cases where
agents have incomplete information on the goals and worth of other agents.
First we consider the case where agents' goals are private information  and we
analyze what goal declaration strategies the agents might adopt to increase
their utility. Then  we consider the situation where the agents' goals (and
therefore stand-alone costs) are common knowledge  but the worth they attach to
their goals is private information. We introduce two mechanisms  one 'strict' 
the other 'tolerant'  and analyze their affects on the stability and efficiency
of negotiation outcomes."
"First-order learning involves finding a clause-form definition of a relation
from examples of the relation and relevant background information. In this
paper  a particular first-order learning system is modified to customize it for
finding definitions of functional relations. This restriction leads to faster
learning times and  in some cases  to definitions that have higher predictive
accuracy. Other first-order learning systems might benefit from similar
specialization."
"This paper describes an extension to the constraint satisfaction problem
(CSP) called MUSE CSP (MUltiply SEgmented Constraint Satisfaction Problem).
This extension is especially useful for those problems which segment into
multiple sets of partially shared variables. Such problems arise naturally in
signal processing applications including computer vision  speech processing 
and handwriting recognition. For these applications  it is often difficult to
segment the data in only one way given the low-level information utilized by
the segmentation algorithms. MUSE CSP can be used to compactly represent
several similar instances of the constraint satisfaction problem. If multiple
instances of a CSP have some common variables which have the same domains and
constraints  then they can be combined into a single instance of a MUSE CSP 
reducing the work required to apply the constraints. We introduce the concepts
of MUSE node consistency  MUSE arc consistency  and MUSE path consistency. We
then demonstrate how MUSE CSP can be used to compactly represent lexically
ambiguous sentences and the multiple sentence hypotheses that are often
generated by speech recognition algorithms so that grammar constraints can be
used to provide parses for all syntactically correct sentences. Algorithms for
MUSE arc and path consistency are provided. Finally  we discuss how to create a
MUSE CSP from a set of CSPs which are labeled to indicate when the same
variable is shared by more than a single CSP."
"A new method is proposed for exploiting causal independencies in exact
Bayesian network inference. A Bayesian network can be viewed as representing a
factorization of a joint probability into the multiplication of a set of
conditional probabilities. We present a notion of causal independence that
enables one to further factorize the conditional probabilities into a
combination of even smaller factors and consequently obtain a finer-grain
factorization of the joint probability. The new formulation of causal
independence lets us specify the conditional probability of a variable given
its parents in terms of an associative and commutative operator  such as
``or''  ``sum'' or ``max''  on the contribution of each parent. We start with a
simple algorithm VE for Bayesian network inference that  given evidence and a
query variable  uses the factorization to find the posterior distribution of
the query. We show how this algorithm can be extended to exploit causal
independence. Empirical studies  based on the CPCS networks for medical
diagnosis  show that this method is more efficient than previous methods and
allows for inference in larger networks than previous algorithms."
"Efficiently entering information into a computer is key to enjoying the
benefits of computing. This paper describes three intelligent user interfaces.
handwriting recognition  adaptive menus  and predictive fillin. In the context
of adding a personUs name and address to an electronic organizer  tests show
handwriting recognition is slower than typing on an on-screen  soft keyboard 
while adaptive menus and predictive fillin can be twice as fast. This paper
also presents strategies for applying these three interfaces to other
information collection domains."
"Decomposable dependency models possess a number of interesting and useful
properties. This paper presents new characterizations of decomposable models in
terms of independence relationships  which are obtained by adding a single
axiom to the well-known set characterizing dependency models that are
isomorphic to undirected graphs. We also briefly discuss a potential
application of our results to the problem of learning graphical models from
data."
"Instance-based learning techniques typically handle continuous and linear
input values well  but often do not handle nominal input attributes
appropriately. The Value Difference Metric (VDM) was designed to find
reasonable distance values between nominal attribute values  but it largely
ignores continuous attributes  requiring discretization to map continuous
values into nominal values. This paper proposes three new heterogeneous
distance functions  called the Heterogeneous Value Difference Metric (HVDM) 
the Interpolated Value Difference Metric (IVDM)  and the Windowed Value
Difference Metric (WVDM). These new distance functions are designed to handle
applications with nominal attributes  continuous attributes  or both. In
experiments on 48 applications the new distance metrics achieve higher
classification accuracy on average than three previous distance functions on
those datasets that have both nominal and continuous attributes."
"Previous approaches of analyzing spontaneously spoken language often have
been based on encoding syntactic and semantic knowledge manually and
symbolically. While there has been some progress using statistical or
connectionist language models  many current spoken- language systems still use
a relatively brittle  hand-coded symbolic grammar or symbolic semantic
component. In contrast  we describe a so-called screening approach for learning
robust processing of spontaneously spoken language. A screening approach is a
flat analysis which uses shallow sequences of category representations for
analyzing an utterance at various syntactic  semantic and dialog levels. Rather
than using a deeply structured symbolic analysis  we use a flat connectionist
analysis. This screening approach aims at supporting speech and language
processing by using (1) data-driven learning and (2) robustness of
connectionist networks. In order to test this approach  we have developed the
SCREEN system which is based on this new robust  learned and flat analysis. In
this paper  we focus on a detailed description of SCREEN's architecture  the
flat syntactic and semantic analysis  the interaction with a speech recognizer 
and a detailed evaluation analysis of the robustness under the influence of
noisy or incomplete input. The main result of this paper is that flat
representations allow more robust processing of spontaneous spoken language
than deeply structured representations. In particular  we show how the
fault-tolerance and learning capability of connectionist networks can support a
flat analysis for providing more robust spoken-language processing within an
overall hybrid symbolic/connectionist framework."
"Most modern formalisms used in Databases and Artificial Intelligence for
describing an application domain are based on the notions of class (or concept)
and relationship among classes. One interesting feature of such formalisms is
the possibility of defining a class  i.e.  providing a set of properties that
precisely characterize the instances of the class. Many recent articles point
out that there are several ways of assigning a meaning to a class definition
containing some sort of recursion. In this paper  we argue that  instead of
choosing a single style of semantics  we achieve better results by adopting a
formalism that allows for different semantics to coexist. We demonstrate the
feasibility of our argument  by presenting a knowledge representation
formalism  the description logic muALCQ  with the above characteristics. In
addition to the constructs for conjunction  disjunction  negation  quantifiers 
and qualified number restrictions  muALCQ includes special fixpoint constructs
to express (suitably interpreted) recursive definitions. These constructs
enable the usual frame-based descriptions to be combined with definitions of
recursive data structures such as directed acyclic graphs  lists  streams  etc.
We establish several properties of muALCQ  including the decidability and the
computational complexity of reasoning  by formulating a correspondence with a
particular modal logic of programs called the modal mu-calculus."
"We argue that the analysis of agent/environment interactions should be
extended to include the conventions and invariants maintained by agents
throughout their activity. We refer to this thicker notion of environment as a
lifeworld and present a partial set of formal tools for describing structures
of lifeworlds and the ways in which they computationally simplify activity. As
one specific example  we apply the tools to the analysis of the Toast system
and show how versions of the system with very different control structures in
fact implement a common control structure together with different conventions
for encoding task state in the positions or states of objects in the
environment."
"We describe a new paradigm for implementing inference in belief networks 
which consists of two steps. (1) compiling a belief network into an arithmetic
expression called a Query DAG (Q-DAG); and (2) answering queries using a simple
evaluation algorithm. Each node of a Q-DAG represents a numeric operation  a
number  or a symbol for evidence. Each leaf node of a Q-DAG represents the
answer to a network query  that is  the probability of some event of interest.
It appears that Q-DAGs can be generated using any of the standard algorithms
for exact inference in belief networks (we show how they can be generated using
clustering and conditioning algorithms). The time and space complexity of a
Q-DAG generation algorithm is no worse than the time complexity of the
inference algorithm on which it is based. The complexity of a Q-DAG evaluation
algorithm is linear in the size of the Q-DAG  and such inference amounts to a
standard evaluation of the arithmetic expression it represents. The intended
value of Q-DAGs is in reducing the software and hardware resources required to
utilize belief networks in on-line  real-world applications. The proposed
framework also facilitates the development of on-line inference on different
software and hardware platforms due to the simplicity of the Q-DAG evaluation
algorithm. Interestingly enough  Q-DAGs were found to serve other purposes.
simple techniques for reducing Q-DAGs tend to subsume relatively complex
optimization techniques for belief-network inference  such as network-pruning
and computation-caching."
"An algorithm that learns from a set of examples should ideally be able to
exploit the available resources of (a) abundant computing power and (b)
domain-specific knowledge to improve its ability to generalize. Connectionist
theory-refinement systems  which use background knowledge to select a neural
network's topology and initial weights  have proven to be effective at
exploiting domain-specific knowledge; however  most do not exploit available
computing power. This weakness occurs because they lack the ability to refine
the topology of the neural networks they produce  thereby limiting
generalization  especially when given impoverished domain theories. We present
the REGENT algorithm which uses (a) domain-specific knowledge to help create an
initial population of knowledge-based neural networks and (b) genetic operators
of crossover and mutation (specifically designed for knowledge-based networks)
to continually search for better network topologies. Experiments on three
real-world domains indicate that our new algorithm is able to significantly
increase generalization compared to a standard connectionist theory-refinement
system  as well as our previous algorithm for growing knowledge-based networks."
"Several recent studies have compared the relative efficiency of alternative
flaw selection strategies for partial-order causal link (POCL) planning. We
review this literature  and present new experimental results that generalize
the earlier work and explain some of the discrepancies in it. In particular  we
describe the Least-Cost Flaw Repair (LCFR) strategy developed and analyzed by
Joslin and Pollack (1994)  and compare it with other strategies  including
Gerevini and Schubert's (1996) ZLIFO strategy. LCFR and ZLIFO make very
different  and apparently conflicting claims about the most effective way to
reduce search-space size in POCL planning. We resolve this conflict  arguing
that much of the benefit that Gerevini and Schubert ascribe to the LIFO
component of their ZLIFO strategy is better attributed to other causes. We show
that for many problems  a strategy that combines least-cost flaw selection with
the delay of separable threats will be effective in reducing search-space size 
and will do so without excessive computational overhead. Although such a
strategy thus provides a good default  we also show that certain domain
characteristics may reduce its effectiveness."
"We investigate the computational properties of the spatial algebra RCC-5
which is a restricted version of the RCC framework for spatial reasoning. The
satisfiability problem for RCC-5 is known to be NP-complete but not much is
known about its approximately four billion subclasses. We provide a complete
classification of satisfiability for all these subclasses into polynomial and
NP-complete respectively. In the process  we identify all maximal tractable
subalgebras which are four in total."
"The easy-hard-easy pattern in the difficulty of combinatorial search problems
as constraints are added has been explained as due to a competition between the
decrease in number of solutions and increased pruning. We test the generality
of this explanation by examining one of its predictions. if the number of
solutions is held fixed by the choice of problems  then increased pruning
should lead to a monotonic decrease in search cost. Instead  we find the
easy-hard-easy pattern in median search cost even when the number of solutions
is held constant  for some search methods. This generalizes previous
observations of this pattern and shows that the existing theory does not
explain the full range of the peak in search cost. In these cases the pattern
appears to be due to changes in the size of the minimal unsolvable subproblems 
rather than changing numbers of solutions."
"This paper combines two important directions of research in temporal
resoning. that of finding maximal tractable subclasses of Allen's interval
algebra  and that of reasoning with metric temporal information. Eight new
maximal tractable subclasses of Allen's interval algebra are presented  some of
them subsuming previously reported tractable algebras. The algebras allow for
metric temporal constraints on interval starting or ending points  using the
recent framework of Horn DLRs. Two of the algebras can express the notion of
sequentiality between intervals  being the first such algebras admitting both
qualitative and metric time."
"Starting with a likelihood or preference order on worlds  we extend it to a
likelihood ordering on sets of worlds in a natural way  and examine the
resulting logic. Lewis earlier considered such a notion of relative likelihood
in the context of studying counterfactuals  but he assumed a total preference
order on worlds. Complications arise when examining partial orders that are not
present for total orders. There are subtleties involving the exact approach to
lifting the order on worlds to an order on sets of worlds. In addition  the
axiomatization of the logic of relative likelihood in the case of partial
orders gives insight into the connection between relative likelihood and
default reasoning."
"Many AI researchers are today striving to build agent teams for complex 
dynamic multi-agent domains  with intended applications in arenas such as
education  training  entertainment  information integration  and collective
robotics. Unfortunately  uncertainties in these complex  dynamic domains
obstruct coherent teamwork. In particular  team members often encounter
differing  incomplete  and possibly inconsistent views of their environment.
Furthermore  team members can unexpectedly fail in fulfilling responsibilities
or discover unexpected opportunities. Highly flexible coordination and
communication is key in addressing such uncertainties. Simply fitting
individual agents with precomputed coordination plans will not do  for their
inflexibility can cause severe failures in teamwork  and their
domain-specificity hinders reusability. Our central hypothesis is that the key
to such flexibility and reusability is providing agents with general models of
teamwork. Agents exploit such models to autonomously reason about coordination
and communication  providing requisite flexibility. Furthermore  the models
enable reuse across domains  both saving implementation effort and enforcing
consistency. This article presents one general  implemented model of teamwork 
called STEAM. The basic building block of teamwork in STEAM is joint intentions
(Cohen & Levesque  1991b); teamwork in STEAM is based on agents' building up a
(partial) hierarchy of joint intentions (this hierarchy is seen to parallel
Grosz & Kraus's partial SharedPlans  1996). Furthermore  in STEAM  team members
monitor the team's and individual members' performance  reorganizing the team
as necessary. Finally  decision-theoretic communication selectivity in STEAM
ensures reduction in communication overheads of teamwork  with appropriate
sensitivity to the environmental conditions. This article describes STEAM's
application in three different complex domains  and presents detailed empirical
results."
"SEQUITUR is an algorithm that infers a hierarchical structure from a sequence
of discrete symbols by replacing repeated phrases with a grammatical rule that
generates the phrase  and continuing this process recursively. The result is a
hierarchical representation of the original sequence  which offers insights
into its lexical structure. The algorithm is driven by two constraints that
reduce the size of the grammar  and produce structure as a by-product. SEQUITUR
breaks new ground by operating incrementally. Moreover  the method's simple
structure permits a proof that it operates in space and time that is linear in
the size of the input. Our implementation can process 50 000 symbols per second
and has been applied to an extensive range of real world sequences."
"Case-Based Planning (CBP) provides a way of scaling up domain-independent
planning to solve large problems in complex domains. It replaces the detailed
and lengthy search for a solution with the retrieval and adaptation of previous
planning experiences. In general  CBP has been demonstrated to improve
performance over generative (from-scratch) planning. However  the performance
improvements it provides are dependent on adequate judgements as to problem
similarity. In particular  although CBP may substantially reduce planning
effort overall  it is subject to a mis-retrieval problem. The success of CBP
depends on these retrieval errors being relatively rare. This paper describes
the design and implementation of a replay framework for the case-based planner
DERSNLP+EBL. DERSNLP+EBL extends current CBP methodology by incorporating
explanation-based learning techniques that allow it to explain and learn from
the retrieval failures it encounters. These techniques are used to refine
judgements about case similarity in response to feedback when a wrong decision
has been made. The same failure analysis is used in building the case library 
through the addition of repairing cases. Large problems are split and stored as
single goal subproblems. Multi-goal problems are stored only when these smaller
cases fail to be merged into a full solution. An empirical evaluation of this
approach demonstrates the advantage of learning from experienced retrieval
failure."
"Partially observable Markov decision processes (POMDPs) are a natural model
for planning problems where effects of actions are nondeterministic and the
state of the world is not completely observable. It is difficult to solve
POMDPs exactly. This paper proposes a new approximation scheme. The basic idea
is to transform a POMDP into another one where additional information is
provided by an oracle. The oracle informs the planning agent that the current
state of the world is in a certain region. The transformed POMDP is
consequently said to be region observable. It is easier to solve than the
original POMDP. We propose to solve the transformed POMDP and use its optimal
policy to construct an approximate policy for the original POMDP. By
controlling the amount of additional information that the oracle provides  it
is possible to find a proper tradeoff between computational time and
approximation quality. In terms of algorithmic contributions  we study in
details how to exploit region observability in solving the transformed POMDP.
To facilitate the study  we also propose a new exact algorithm for general
POMDPs. The algorithm is conceptually simple and yet is significantly more
efficient than all previous exact algorithms."
"The model of a non-Bayesian agent who faces a repeated game with incomplete
information against Nature is an appropriate tool for modeling general
agent-environment interactions. In such a model the environment state
(controlled by Nature) may change arbitrarily  and the feedback/reward function
is initially unknown. The agent is not Bayesian  that is he does not form a
prior probability neither on the state selection strategy of Nature  nor on his
reward function. A policy for the agent is a function which assigns an action
to every history of observations and actions. Two basic feedback structures are
considered. In one of them -- the perfect monitoring case -- the agent is able
to observe the previous environment state as part of his feedback  while in the
other -- the imperfect monitoring case -- all that is available to the agent is
the reward obtained. Both of these settings refer to partially observable
processes  where the current environment state is unknown. Our main result
refers to the competitive ratio criterion in the perfect monitoring case. We
prove the existence of an efficient stochastic policy that ensures that the
competitive ratio is obtained at almost all stages with an arbitrarily high
probability  where efficiency is measured in terms of rate of convergence. It
is further shown that such an optimal policy does not exist in the imperfect
monitoring case. Moreover  it is proved that in the perfect monitoring case
there does not exist a deterministic policy that satisfies our long run
optimality criterion. In addition  we discuss the maxmin criterion and prove
that a deterministic efficient optimal strategy does exist in the imperfect
monitoring case under this criterion. Finally we show that our approach to
long-run optimality can be viewed as qualitative  which distinguishes it from
previous work in this area."
"Local search algorithms for combinatorial search problems frequently
encounter a sequence of states in which it is impossible to improve the value
of the objective function; moves through these regions  called plateau moves 
dominate the time spent in local search. We analyze and characterize plateaus
for three different classes of randomly generated Boolean Satisfiability
problems. We identify several interesting features of plateaus that impact the
performance of local search algorithms. We show that local minima tend to be
small but occasionally may be very large. We also show that local minima can be
escaped without unsatisfying a large number of clauses  but that systematically
searching for an escape route may be computationally expensive if the local
minimum is large. We show that plateaus with exits  called benches  tend to be
much larger than minima  and that some benches have very few exit states which
local search can use to escape. We show that the solutions (i.e.  global
minima) of randomly generated problem instances form clusters  which behave
similarly to local minima. We revisit several enhancements of local search
algorithms and explain their performance in light of our results. Finally we
discuss strategies for creating the next generation of local search algorithms."
"The assessment of bidirectional heuristic search has been incorrect since it
was first published more than a quarter of a century ago. For quite a long
time  this search strategy did not achieve the expected results  and there was
a major misunderstanding about the reasons behind it. Although there is still
wide-spread belief that bidirectional heuristic search is afflicted by the
problem of search frontiers passing each other  we demonstrate that this
conjecture is wrong. Based on this finding  we present both a new generic
approach to bidirectional heuristic search and a new approach to dynamically
improving heuristic values that is feasible in bidirectional search only. These
approaches are put into perspective with both the traditional and more recently
proposed approaches in order to facilitate a better overall understanding.
Empirical results of experiments with our new approaches show that
bidirectional heuristic search can be performed very efficiently and also with
limited memory. These results suggest that bidirectional heuristic search
appears to be better for solving certain difficult problems than corresponding
unidirectional search. This provides some evidence for the usefulness of a
search strategy that was long neglected. In summary  we show that bidirectional
heuristic search is viable and consequently propose that it be reconsidered."
"Approximating a general formula from above and below by Horn formulas (its
Horn envelope and Horn core  respectively) was proposed by Selman and Kautz
(1991  1996) as a form of ``knowledge compilation '' supporting rapid
approximate reasoning; on the negative side  this scheme is static in that it
supports no updates  and has certain complexity drawbacks pointed out by
Kavvadias  Papadimitriou and Sideri (1993). On the other hand  the many
frameworks and schemes proposed in the literature for theory update and
revision are plagued by serious complexity-theoretic impediments  even in the
Horn case  as was pointed out by Eiter and Gottlob (1992)  and is further
demonstrated in the present paper. More fundamentally  these schemes are not
inductive  in that they may lose in a single update any positive properties of
the represented sets of formulas (small size  Horn structure  etc.). In this
paper we propose a new scheme  incremental recompilation  which combines Horn
approximation and model-based updates; this scheme is inductive and very
efficient  free of the problems facing its constituents. A set of formulas is
represented by an upper and lower Horn approximation. To update  we replace the
upper Horn formula by the Horn envelope of its minimum-change update  and
similarly the lower one by the Horn core of its update; the key fact which
enables this scheme is that Horn envelopes and cores are easy to compute when
the underlying formula is the result of a minimum-change update of a Horn
formula by a clause. We conjecture that efficient algorithms are possible for
more complex updates."
"An important characteristic of many logics for Artificial Intelligence is
their nonmonotonicity. This means that adding a formula to the premises can
invalidate some of the consequences. There may  however  exist formulae that
can always be safely added to the premises without destroying any of the
consequences. we say they respect monotonicity. Also  there may be formulae
that  when they are a consequence  can not be invalidated when adding any
formula to the premises. we call them conservative. We study these two classes
of formulae for preferential logics  and show that they are closely linked to
the formulae whose truth-value is preserved along the (preferential) ordering.
We will consider some preferential logics for illustration  and prove syntactic
characterization results for them. The results in this paper may improve the
efficiency of theorem provers for preferential logics."
"Existing plan synthesis approaches in artificial intelligence fall into two
categories -- domain independent and domain dependent. The domain independent
approaches are applicable across a variety of domains  but may not be very
efficient in any one given domain. The domain dependent approaches need to be
(re)designed for each domain separately  but can be very efficient in the
domain for which they are designed. One enticing alternative to these
approaches is to automatically synthesize domain independent planners given the
knowledge about the domain and the theory of planning. In this paper  we
investigate the feasibility of using existing automated software synthesis
tools to support such synthesis. Specifically  we describe an architecture
called CLAY in which the Kestrel Interactive Development System (KIDS) is used
to derive a domain-customized planner through a semi-automatic combination of a
declarative theory of planning  and the declarative control knowledge specific
to a given domain  to semi-automatically combine them to derive
domain-customized planners. We discuss what it means to write a declarative
theory of planning and control knowledge for KIDS  and illustrate our approach
by generating a class of domain-specific planners using state space
refinements. Our experiments show that the synthesized planners can outperform
classical refinement planners (implemented as instantiations of UCP 
Kambhampati & Srivastava  1995)  using the same control knowledge. We will
contrast the costs and benefits of the synthesis approach with conventional
methods for customizing domain independent planners."
"This paper introduces new algorithms and data structures for quick counting
for machine learning datasets. We focus on the counting task of constructing
contingency tables  but our approach is also applicable to counting the number
of records in a dataset that match conjunctive queries. Subject to certain
assumptions  the costs of these operations can be shown to be independent of
the number of records in the dataset and loglinear in the number of non-zero
entries in the contingency table. We provide a very sparse data structure  the
ADtree  to minimize memory use. We provide analytical worst-case bounds for
this structure for several models of data distribution. We empirically
demonstrate that tractably-sized data structures can be produced for large
real-world datasets by (a) using a sparse tree structure that never allocates
memory for counts of zero  (b) never allocating memory for counts that can be
deduced from other counts  and (c) not bothering to expand the tree fully near
its leaves. We show how the ADtree can be used to accelerate Bayes net
structure finding algorithms  rule learning algorithms  and feature selection
algorithms  and we provide a number of empirical results comparing ADtree
methods against traditional direct counting approaches. We also discuss the
possible uses of ADtrees in other machine learning methods  and discuss the
merits of ADtrees in comparison with alternative representations such as
kd-trees  R-trees and Frequent Sets."
"In this paper we consider the problem of `theory patching'  in which we are
given a domain theory  some of whose components are indicated to be possibly
flawed  and a set of labeled training examples for the domain concept. The
theory patching problem is to revise only the indicated components of the
theory  such that the resulting theory correctly classifies all the training
examples. Theory patching is thus a type of theory revision in which revisions
are made to individual components of the theory. Our concern in this paper is
to determine for which classes of logical domain theories the theory patching
problem is tractable. We consider both propositional and first-order domain
theories  and show that the theory patching problem is equivalent to that of
determining what information contained in a theory is `stable' regardless of
what revisions might be performed to the theory. We show that determining
stability is tractable if the input theory satisfies two conditions. that
revisions to each theory component have monotonic effects on the classification
of examples  and that theory components act independently in the classification
of examples in the theory. We also show how the concepts introduced can be used
to determine the soundness and completeness of particular theory patching
algorithms."
"In this paper we re-investigate windowing for rule learning algorithms. We
show that  contrary to previous results for decision tree learning  windowing
can in fact achieve significant run-time gains in noise-free domains and
explain the different behavior of rule learning algorithms by the fact that
they learn each rule independently. The main contribution of this paper is
integrative windowing  a new type of algorithm that further exploits this
property by integrating good rules into the final theory right after they have
been discovered. Thus it avoids re-learning these rules in subsequent
iterations of the windowing process. Experimental evidence in a variety of
noise-free domains shows that integrative windowing can in fact achieve
substantial run-time gains. Furthermore  we discuss the problem of noise in
windowing and present an algorithm that is able to achieve run-time gains in a
set of experiments in a simple domain with artificial noise."
"This paper presents a comprehensive approach for model-based diagnosis which
includes proposals for characterizing and computing preferred diagnoses 
assuming that the system description is augmented with a system structure (a
directed graph explicating the interconnections between system components).
Specifically  we first introduce the notion of a consequence  which is a
syntactically unconstrained propositional sentence that characterizes all
consistency-based diagnoses and show that standard characterizations of
diagnoses  such as minimal conflicts  correspond to syntactic variations on a
consequence. Second  we propose a new syntactic variation on the consequence
known as negation normal form (NNF) and discuss its merits compared to standard
variations. Third  we introduce a basic algorithm for computing consequences in
NNF given a structured system description. We show that if the system structure
does not contain cycles  then there is always a linear-size consequence in NNF
which can be computed in linear time. For arbitrary system structures  we show
a precise connection between the complexity of computing consequences and the
topology of the underlying system structure. Finally  we present an algorithm
that enumerates the preferred diagnoses characterized by a consequence. The
algorithm is shown to take linear time in the size of the consequence if the
preference criterion satisfies some general conditions."
"One of the most common mechanisms used for speeding up problem solvers is
macro-learning. Macros are sequences of basic operators acquired during problem
solving. Macros are used by the problem solver as if they were basic operators.
The major problem that macro-learning presents is the vast number of macros
that are available for acquisition. Macros increase the branching factor of the
search space and can severely degrade problem-solving efficiency. To make macro
learning useful  a program must be selective in acquiring and utilizing macros.
This paper describes a general method for selective acquisition of macros.
Solvable training problems are generated in increasing order of difficulty. The
only macros acquired are those that take the problem solver out of a local
minimum to a better state. The utility of the method is demonstrated in several
domains  including the domain of NxN sliding-tile puzzles. After learning on
small puzzles  the system is able to efficiently solve puzzles of any size."
"We examine the computational complexity of testing and finding small plans in
probabilistic planning domains with both flat and propositional
representations. The complexity of plan evaluation and existence varies with
the plan type sought; we examine totally ordered plans  acyclic plans  and
looping plans  and partially ordered plans under three natural definitions of
plan value. We show that problems of interest are complete for a variety of
complexity classes. PL  P  NP  co-NP  PP  NP^PP  co-NP^PP  and PSPACE. In the
process of proving that certain planning problems are complete for NP^PP  we
introduce a new basic NP^PP-complete problem  E-MAJSAT  which generalizes the
standard Boolean satisfiability problem to computations involving probabilistic
quantities; our results suggest that the development of good heuristics for
E-MAJSAT could be important for the creation of efficient algorithms for a wide
variety of problems."
"In this paper we describe SYNERGY  which is a highly parallelizable  linear
planning system that is based on the genetic programming paradigm. Rather than
reasoning about the world it is planning for  SYNERGY uses artificial
selection  recombination and fitness measure to generate linear plans that
solve conjunctive goals. We ran SYNERGY on several domains (e.g.  the briefcase
problem and a few variants of the robot navigation problem)  and the
experimental results show that our planner is capable of handling problem
instances that are one to two orders of magnitude larger than the ones solved
by UCPOP. In order to facilitate the search reduction and to enhance the
expressive power of SYNERGY  we also propose two major extensions to our
planning system. a formalism for using hierarchical planning operators  and a
framework for planning in dynamic environments."
"We show that several constraint propagation algorithms (also called (local)
consistency  consistency enforcing  Waltz  filtering or narrowing algorithms)
are instances of algorithms that deal with chaotic iteration. To this end we
propose a simple abstract framework that allows us to classify and compare
these algorithms and to establish in a uniform way their basic properties."
"This paper examines the phenomenon of daydreaming. spontaneously recalling or
imagining personal or vicarious experiences in the past or future. The
following important roles of daydreaming in human cognition are postulated.
plan preparation and rehearsal  learning from failures and successes  support
for processes of creativity  emotion regulation  and motivation.
  A computational theory of daydreaming and its implementation as the program
DAYDREAMER are presented. DAYDREAMER consists of 1) a scenario generator based
on relaxed planning  2) a dynamic episodic memory of experiences used by the
scenario generator  3) a collection of personal goals and control goals which
guide the scenario generator  4) an emotion component in which daydreams
initiate  and are initiated by  emotional states arising from goal outcomes 
and 5) domain knowledge of interpersonal relations and common everyday
occurrences.
  The role of emotions and control goals in daydreaming is discussed. Four
control goals commonly used in guiding daydreaming are presented.
rationalization  failure/success reversal  revenge  and preparation. The role
of episodic memory in daydreaming is considered  including how daydreamed
information is incorporated into memory and later used. An initial version of
DAYDREAMER which produces several daydreams (in English) is currently running."
"Real world combinatorial optimization problems such as scheduling are
typically too complex to solve with exact methods. Additionally  the problems
often have to observe vaguely specified constraints of different importance 
the available data may be uncertain  and compromises between antagonistic
criteria may be necessary. We present a combination of approximate reasoning
based constraints and iterative optimization based heuristics that help to
model and solve such problems in a framework of C++ software libraries called
StarFLIP++. While initially developed to schedule continuous caster units in
steel plants  we present in this paper results from reusing the library
components in a shift scheduling system for the workforce of an industrial
production plant."
"The study of belief change has been an active area in philosophy and AI. In
recent years two special cases of belief change  belief revision and belief
update  have been studied in detail. In a companion paper (Friedman & Halpern 
1997)  we introduce a new framework to model belief change. This framework
combines temporal and epistemic modalities with a notion of plausibility 
allowing us to examine the change of beliefs over time. In this paper  we show
how belief revision and belief update can be captured in our framework. This
allows us to compare the assumptions made by each method  and to better
understand the principles underlying them. In particular  it shows that Katsuno
and Mendelzon's notion of belief update (Katsuno & Mendelzon  1991a) depends on
several strong assumptions that may limit its applicability in artificial
intelligence. Finally  our analysis allow us to identify a notion of minimal
change that underlies a broad range of belief change operations including
revision and update."
"How can the semantic interpretation of a formal symbol system be made
intrinsic to the system  rather than just parasitic on the meanings in our
heads? How can the meanings of the meaningless symbol tokens  manipulated
solely on the basis of their (arbitrary) shapes  be grounded in anything but
other meaningless symbols? The problem is analogous to trying to learn Chinese
from a Chinese/Chinese dictionary alone. A candidate solution is sketched.
Symbolic representations must be grounded bottom-up in nonsymbolic
representations of two kinds. (1) ""iconic representations "" which are analogs
of the proximal sensory projections of distal objects and events  and (2)
""categorical representations "" which are learned and innate feature-detectors
that pick out the invariant features of object and event categories from their
sensory projections. Elementary symbols are the names of these object and event
categories  assigned on the basis of their (nonsymbolic) categorical
representations. Higher-order (3) ""symbolic representations "" grounded in these
elementary symbols  consist of symbol strings describing category membership
relations (e.g.  ""An X is a Y that is Z"")."
"In tree search problem the best-first search algorithm needs too much of
space . To remove such drawbacks of these algorithms the IDA* was developed
which is both space and time cost efficient. But again IDA* can give an optimal
solution for real valued problems like Flow shop scheduling  Travelling
Salesman and 0/1 Knapsack due to their real valued cost estimates. Thus further
modifications are done on it and the Iterative Deepening Branch and Bound
Search Algorithms is developed which meets the requirements. We have tried
using this algorithm for the Flow Shop Scheduling Problem and have found that
it is quite effective."
"Agents are small programs that autonomously take actions based on changes in
their environment or ``state.'' Over the last few years  there have been an
increasing number of efforts to build agents that can interact and/or
collaborate with other agents. In one of these efforts  Eiter  Subrahmanian amd
Pick (AIJ  108(1-2)  pages 179-255) have shown how agents may be built on top
of legacy code. However  their framework assumes that agent states are
completely determined  and there is no uncertainty in an agent's state. Thus 
their framework allows an agent developer to specify how his agents will react
when the agent is 100% sure about what is true/false in the world state. In
this paper  we propose the concept of a \emph{probabilistic agent program} and
show how  given an arbitrary program written in any imperative language  we may
build a declarative ``probabilistic'' agent program on top of it which supports
decision making in the presence of uncertainty. We provide two alternative
semantics for probabilistic agent programs. We show that the second semantics 
though more epistemically appealing  is more complex to compute. We provide
sound and complete algorithms to compute the semantics of \emph{positive} agent
programs."
"The assumptions needed to prove Cox's Theorem are discussed and examined.
Various sets of assumptions under which a Cox-style theorem can be proved are
provided  although all are rather strong and  arguably  not natural."
"We revisit the issue of connections between two leading formalisms in
nonmonotonic reasoning. autoepistemic logic and default logic. For each logic
we develop a comprehensive semantic framework based on the notion of a belief
pair. The set of all belief pairs together with the so called knowledge
ordering forms a complete lattice. For each logic  we introduce several
semantics by means of fixpoints of operators on the lattice of belief pairs.
Our results elucidate an underlying isomorphism of the respective semantic
constructions. In particular  we show that the interpretation of defaults as
modal formulas proposed by Konolige allows us to represent all semantics for
default logic in terms of the corresponding semantics for autoepistemic logic.
Thus  our results conclusively establish that default logic can indeed be
viewed as a fragment of autoepistemic logic. However  as we also demonstrate 
the semantics of Moore and Reiter are given by different operators and occupy
different locations in their corresponding families of semantics. This result
explains the source of the longstanding difficulty to formally relate these two
semantics. In the paper  we also discuss approximating skeptical reasoning with
autoepistemic and default logics and establish constructive principles behind
such approximations."
"Randomized algorithms for deciding satisfiability were shown to be effective
in solving problems with thousands of variables. However  these algorithms are
not complete. That is  they provide no guarantee that a satisfying assignment 
if one exists  will be found. Thus  when studying randomized algorithms  there
are two important characteristics that need to be considered. the running time
and  even more importantly  the accuracy --- a measure of likelihood that a
satisfying assignment will be found  provided one exists. In fact  we argue
that without a reference to the accuracy  the notion of the running time for
randomized algorithms is not well-defined. In this paper  we introduce a formal
notion of accuracy. We use it to define a concept of the running time. We use
both notions to study the random walk strategy GSAT algorithm. We investigate
the dependence of accuracy on properties of input formulas such as
clause-to-variable ratio and the number of satisfying assignments. We
demonstrate that the running time of GSAT grows exponentially in the number of
variables of the input formula for randomly generated 3-CNF formulas and for
the formulas encoding 3- and 4-colorability of graphs."
"Two different types of agency are discussed based on dynamically coherent and
incoherent couplings with an environment respectively. I propose that until a
private syntax (syntactic autonomy) is discovered by dynamically coherent
agents  there are no significant or interesting types of closure or autonomy.
When syntactic autonomy is established  then  because of a process of
description-based selected self-organization  open-ended evolution is enabled.
At this stage  agents depend  in addition to dynamics  on localized  symbolic
memory  thus adding a level of dynamical incoherence to their interaction with
the environment. Furthermore  it is the appearance of syntactic autonomy which
enables much more interesting types of closures amongst agents which share the
same syntax. To investigate how we can study the emergence of syntax from
dynamical systems  experiments with cellular automata leading to emergent
computation to solve non-trivial tasks are discussed. RNA editing is also
mentioned as a process that may have been used to obtain a primordial
biological code necessary open-ended evolution."
"This paper presents a method of computing a revision of a function-free
normal logic program. If an added rule is inconsistent with a program  that is 
if it leads to a situation such that no stable model exists for a new program 
then deletion and addition of rules are performed to avoid inconsistency. We
specify a revision by translating a normal logic program into an abductive
logic program with abducibles to represent deletion and addition of rules. To
compute such deletion and addition  we propose an adaptation of our top-down
abductive proof procedure to compute a relevant abducibles to an added rule. We
compute a minimally revised program  by choosing a minimal set of abducibles
among all the sets of abducibles computed by a top-down proof procedure."
This is a system description for the OSCAR defeasible reasoner.
"Diagnostic reasoning has been characterized logically as consistency-based
reasoning or abductive reasoning. Previous analyses in the literature have
shown  on the one hand  that choosing the (in general more restrictive)
abductive definition may be appropriate or not  depending on the content of the
knowledge base [Console&Torasso91]  and  on the other hand  that  depending on
the choice of the definition the same knowledge should be expressed in
different form [Poole94].
  Since in Model-Based Diagnosis a major problem is finding the right way of
abstracting the behavior of the system to be modeled  this paper discusses the
relation between modeling  and in particular abstraction in the model  and the
notion of diagnosis."
"ACLP is a system which combines abductive reasoning and constraint solving by
integrating the frameworks of Abductive Logic Programming (ALP) and Constraint
Logic Programming (CLP). It forms a general high-level knowledge representation
environment for abductive problems in Artificial Intelligence and other areas.
In ACLP  the task of abduction is supported and enhanced by its non-trivial
integration with constraint solving facilitating its application to complex
problems. The ACLP system is currently implemented on top of the CLP language
of ECLiPSe as a meta-interpreter exploiting its underlying constraint solver
for finite domains. It has been applied to the problems of planning and
scheduling in order to test its computational effectiveness compared with the
direct use of the (lower level) constraint solving framework of CLP on which it
is built. These experiments provide evidence that the abductive framework of
ACLP does not compromise significantly the computational efficiency of the
solutions. Other experiments show the natural ability of ACLP to accommodate
easily and in a robust way new or changing requirements of the original
problem."
"We present a method for relevance sensitive non-monotonic inference from
belief sequences which incorporates insights pertaining to prioritized
inference and relevance sensitive  inconsistency tolerant belief revision.
  Our model uses a finite  logically open sequence of propositional formulas as
a representation for beliefs and defines a notion of inference from
maxiconsistent subsets of formulas guided by two orderings. a temporal
sequencing and an ordering based on relevance relations between the conclusion
and formulas in the sequence. The relevance relations are ternary (using
context as a parameter) as opposed to standard binary axiomatizations. The
inference operation thus defined easily handles iterated revision by
maintaining a revision history  blocks the derivation of inconsistent answers
from a possibly inconsistent sequence and maintains the distinction between
explicit and implicit beliefs. In doing so  it provides a finitely presented
formalism and a plausible model of reasoning for automated agents."
"We propose a combination of probabilistic reasoning from conditional
constraints with approaches to default reasoning from conditional knowledge
bases. In detail  we generalize the notions of Pearl's entailment in system Z 
Lehmann's lexicographic entailment  and Geffner's conditional entailment to
conditional constraints. We give some examples that show that the new notions
of z-  lexicographic  and conditional entailment have similar properties like
their classical counterparts. Moreover  we show that the new notions of z- 
lexicographic  and conditional entailment are proper generalizations of both
their classical counterparts and the classical notion of logical entailment for
conditional constraints."
"This paper describes a system  called PLP  for compiling ordered logic
programs into standard logic programs under the answer set semantics. In an
ordered logic program  rules are named by unique terms  and preferences among
rules are given by a set of dedicated atoms. An ordered logic program is
transformed into a second  regular  extended logic program wherein the
preferences are respected  in that the answer sets obtained in the transformed
theory correspond with the preferred answer sets of the original theory. Since
the result of the translation is an extended logic program  existing logic
programming systems can be used as underlying reasoning engine. In particular 
PLP is conceived as a front-end to the logic programming systems dlv and
smodels."
"The SLDNFA-system results from the LP+ project at the K.U.Leuven  which
investigates logics and proof procedures for these logics for declarative
knowledge representation. Within this project inductive definition logic
(ID-logic) is used as representation logic. Different solvers are being
developed for this logic and one of these is SLDNFA. A prototype of the system
is available and used for investigating how to solve efficiently problems
represented in ID-logic."
"We describe an approach for compiling preferences into logic programs under
the answer set semantics. An ordered logic program is an extended logic program
in which rules are named by unique terms  and in which preferences among rules
are given by a set of dedicated atoms. An ordered logic program is transformed
into a second  regular  extended logic program wherein the preferences are
respected  in that the answer sets obtained in the transformed theory
correspond with the preferred answer sets of the original theory. Our approach
allows both the specification of static orderings (as found in most previous
work)  in which preferences are external to a logic program  as well as
orderings on sets of rules. In large part then  we are interested in describing
a general methodology for uniformly incorporating preference information in a
logic program. Since the result of our translation is an extended logic
program  we can make use of existing implementations  such as dlv and smodels.
To this end  we have developed a compiler  available on the web  as a front-end
for these programming systems."
"This paper proposes two kinds of fuzzy abductive inference in the framework
of fuzzy rule base. The abductive inference processes described here depend on
the semantic of the rule. We distinguish two classes of interpretation of a
fuzzy rule  certainty generation rules and possible generation rules. In this
paper we present the architecture of abductive inference in the first class of
interpretation. We give two kinds of problem that we can resolve by using the
proposed models of inference."
"The goal of the LP+ project at the K.U.Leuven is to design an expressive
logic  suitable for declarative knowledge representation  and to develop
intelligent systems based on Logic Programming technology for solving
computational problems using the declarative specifications. The ID-logic is an
integration of typed classical logic and a definition logic. Different
abductive solvers for this language are being developed. This paper is a report
of the integration of high order aggregates into ID-logic and the consequences
on the solver SLDNFA."
"We propose a new approach to belief revision that provides a way to change
knowledge bases with a minimum of effort. We call this way of revising belief
states optimal belief revision. Our revision method gives special attention to
the fact that most belief revision processes are directed to a specific
informational objective. This approach to belief change is founded on notions
such as optimal context and accessibility. For the sentential model of belief
states we provide both a formal description of contexts as sub-theories
determined by three parameters and a method to construct contexts. Next  we
introduce an accessibility ordering for belief sets  which we then use for
selecting the best (optimal) contexts with respect to the processing effort
involved in the revision. Then  for finitely axiomatizable knowledge bases  we
characterize a finite accessibility ranking from which the accessibility
ordering for the entire base is generated and show how to determine the ranking
of an arbitrary sentence in the language. Finally  we define the adjustment of
the accessibility ranking of a revised base of a belief set."
"High-level robot controllers in realistic domains typically deal with
processes which operate concurrently  change the world continuously  and where
the execution of actions is event-driven as in ``charge the batteries as soon
as the voltage level is low''. While non-logic-based robot control languages
are well suited to express such scenarios  they fare poorly when it comes to
projecting  in a conspicuous way  how the world evolves when actions are
executed. On the other hand  a logic-based control language like \congolog 
based on the situation calculus  is well-suited for the latter. However  it has
problems expressing event-driven behavior. In this paper  we show how these
problems can be overcome by first extending the situation calculus to support
continuous change and event-driven behavior and then presenting \ccgolog  a
variant of \congolog which is based on the extended situation calculus. One
benefit of \ccgolog is that it narrows the gap in expressiveness compared to
non-logic-based control languages while preserving a semantically well-founded
projection mechanism."
"The Smodels system implements the stable model semantics for normal logic
programs. It handles a subclass of programs which contain no function symbols
and are domain-restricted but supports extensions including built-in functions
as well as cardinality and weight constraints. On top of this core engine more
involved systems can be built. As an example  we have implemented total and
partial stable model computation for disjunctive logic programs. An interesting
application method is based on answer set programming  i.e.  encoding an
application problem as a set of rules so that its solutions are captured by the
stable models of the rules. Smodels has been applied to a number of areas
including planning  model checking  reachability analysis  product
configuration  dynamic constraint satisfaction  and feature interaction."
"E-RES is a system that implements the Language E  a logic for reasoning about
narratives of action occurrences and observations. E's semantics is
model-theoretic  but this implementation is based on a sound and complete
reformulation of E in terms of argumentation  and uses general computational
techniques of argumentation frameworks. The system derives sceptical
non-monotonic consequences of a given reformulated theory which exactly
correspond to consequences entailed by E's model-theory. The computation relies
on a complimentary ability of the system to derive credulous non-monotonic
consequences together with a set of supporting assumptions which is sufficient
for the (credulous) conclusion to hold. E-RES allows theories to contain
general action laws  statements about action occurrences  observations and
statements of ramifications (or universal laws). It is able to derive
consequences both forward and backward in time. This paper gives a short
overview of the theoretical basis of E-RES and illustrates its use on a variety
of examples. Currently  E-RES is being extended so that the system can be used
for planning."
"In this paper  we outline the prototype of an automated inference tool 
called QUIP  which provides a uniform implementation for several nonmonotonic
reasoning formalisms. The theoretical basis of QUIP is derived from well-known
results about the computational complexity of nonmonotonic logics and exploits
a representation of the different reasoning tasks in terms of quantified
boolean formulae."
"Over the past decade a considerable amount of research has been done to
expand logic programming languages to handle incomplete information. One such
language is the language of epistemic specifications. As is usual with logic
programming languages  the problem of answering queries is intractable in the
general case. For extended disjunctive logic programs  an idea that has proven
useful in simplifying the investigation of answer sets is the use of splitting
sets. In this paper we will present an extended definition of splitting sets
that will be applicable to epistemic specifications. Furthermore  an extension
of the splitting set theorem will be presented. Also  a characterization of
stratified epistemic specifications will be given in terms of splitting sets.
This characterization leads us to an algorithmic method of computing world
views of a subclass of epistemic logic programs."
"The US Data Encryption Standard  DES for short  is put forward as an
interesting benchmark problem for nonmonotonic reasoning systems because (i) it
provides a set of test cases of industrial relevance which shares features of
randomly generated problems and real-world problems  (ii) the representation of
DES using normal logic programs with the stable model semantics is simple and
easy to understand  and (iii) this subclass of logic programs can be seen as an
interesting special case for many other formalizations of nonmonotonic
reasoning. In this paper we present two encodings of DES as logic programs. a
direct one out of the standard specifications and an optimized one extending
the work of Massacci and Marraro. The computational properties of the encodings
are studied by using them for DES key search with the Smodels system as the
implementation of the stable model semantics. Results indicate that the
encodings and Smodels are quite competitive. they outperform state-of-the-art
SAT-checkers working with an optimized encoding of DES into SAT and are
comparable with a SAT-checker that is customized and tuned for the optimized
SAT encoding."
"We generalize a theorem by Francois Fages that describes the relationship
between the completion semantics and the answer set semantics for logic
programs with negation as failure. The study of this relationship is important
in connection with the emergence of answer set programming. Whenever the two
semantics are equivalent  answer sets can be computed by a satisfiability
solver  and the use of answer set solvers such as smodels and dlv is
unnecessary. A logic programming representation of the blocks world due to
Ilkka Niemelae is discussed as an example."
"We introduced decomposable negation normal form (DNNF) recently as a
tractable form of propositional theories  and provided a number of powerful
logical operations that can be performed on it in polynomial time. We also
presented an algorithm for compiling any conjunctive normal form (CNF) into
DNNF and provided a structure-based guarantee on its space and time complexity.
We present in this paper a linear-time algorithm for converting an ordered
binary decision diagram (OBDD) representation of a propositional theory into an
equivalent DNNF  showing that DNNFs scale as well as OBDDs. We also identify a
subclass of DNNF which we call deterministic DNNF  d-DNNF  and show that the
previous complexity guarantees on compiling DNNF continue to hold for this
stricter subclass  which has stronger properties. In particular  we present a
new operation on d-DNNF which allows us to count its models under the
assertion  retraction and flipping of every literal by traversing the d-DNNF
twice. That is  after such traversal  we can test in constant-time. the
entailment of any literal by the d-DNNF  and the consistency of the d-DNNF
under the retraction or flipping of any literal. We demonstrate the
significance of these new operations by showing how they allow us to implement
linear-time  complete truth maintenance systems and linear-time  complete
belief revision systems for two important classes of propositional theories."
"The paper reports on first preliminary results and insights gained in a
project aiming at implementing the fluent calculus using methods and techniques
based on binary decision diagrams. After reporting on an initial experiment
showing promising results we discuss our findings concerning various techniques
and heuristics used to speed up the reasoning process."
"Planning is a natural domain of application for frameworks of reasoning about
actions and change. In this paper we study how one such framework  the Language
E  can form the basis for planning under (possibly) incomplete information. We
define two types of plans. weak and safe plans  and propose a planner  called
the E-Planner  which is often able to extend an initial weak plan into a safe
plan even though the (explicit) information available is incomplete  e.g. for
cases where the initial state is not completely known. The E-Planner is based
upon a reformulation of the Language E in argumentation terms and a natural
proof theory resulting from the reformulation. It uses an extension of this
proof theory by means of abduction for the generation of plans and adopts
argumentation-based techniques for extending weak plans into safe plans. We
provide representative examples illustrating the behaviour of the E-Planner  in
particular for cases where the status of fluents is incompletely known."
"In an earlier work  we have presented operations of belief change which only
affect the relevant part of a belief base. In this paper  we propose the
application of the same strategy to the problem of model-based diangosis. We
first isolate the subset of the system description which is relevant for a
given observation and then solve the diagnosis problem for this subset."
"We present a general  consistency-based framework for belief change.
Informally  in revising K by A  we begin with A and incorporate as much of K as
consistently possible. Formally  a knowledge base K and sentence A are
expressed  via renaming propositions in K  in separate languages. Using a
maximization process  we assume the languages are the same insofar as
consistently possible. Lastly  we express the resultant knowledge base in a
single language. There may be more than one way in which A can be so extended
by K. in choice revision  one such ``extension'' represents the revised state;
alternately revision consists of the intersection of all such extensions.
  The most general formulation of our approach is flexible enough to express
other approaches to revision and update  the merging of knowledge bases  and
the incorporation of static and dynamic integrity constraints. Our framework
differs from work based on ordinal conditional functions  notably with respect
to iterated revision. We argue that the approach is well-suited for
implementation. the choice revision operator gives better complexity results
than general revision; the approach can be expressed in terms of a finite
knowledge base; and the scope of a revision can be restricted to just those
propositions mentioned in the sentence for revision A."
"SATEN is an object-oriented web-based extraction and belief revision engine.
It runs on any computer via a Java 1.1 enabled browser such as Netscape 4.
SATEN performs belief revision based on the AGM approach. The extraction and
belief revision reasoning engines operate on a user specified ranking of
information. One of the features of SATEN is that it can be used to integrate
mutually inconsistent commensuate rankings into a consistent ranking."
"Answer-set programming (ASP) has emerged recently as a viable programming
paradigm. We describe here an ASP system  DATALOG with constraints or DC  based
on non-monotonic logic. Informally  DC theories consist of propositional
clauses (constraints) and of Horn rules. The semantics is a simple and natural
extension of the semantics of the propositional logic. However  thanks to the
presence of Horn rules in the system  modeling of transitive closure becomes
straightforward. We describe the syntax  use and implementation of DC and
provide experimental results."
"Answer-set programming (ASP) has emerged recently as a viable programming
paradigm well attuned to search problems in AI  constraint satisfaction and
combinatorics. Propositional logic is  arguably  the simplest ASP system with
an intuitive semantics supporting direct modeling of problem constraints.
However  for some applications  especially those requiring that transitive
closure be computed  it requires additional variables and results in large
theories. Consequently  it may not be a practical computational tool for such
problems. On the other hand  ASP systems based on nonmonotonic logics  such as
stable logic programming  can handle transitive closure computation efficiently
and  in general  yield very concise theories as problem representations. Their
semantics is  however  more complex. Searching for the middle ground  in this
paper we introduce a new nonmonotonic logic  DATALOG with constraints or DC.
Informally  DC theories consist of propositional clauses (constraints) and of
Horn rules. The semantics is a simple and natural extension of the semantics of
the propositional logic. However  thanks to the presence of Horn rules in the
system  modeling of transitive closure becomes straightforward. We describe the
syntax and semantics of DC  and study its properties. We discuss an
implementation of DC and present results of experimental study of the
effectiveness of DC  comparing it with CSAT  a satisfiability checker and
SMODELS implementation of stable logic programming. Our results show that DC is
competitive with the other two approaches  in case of many search problems 
often yielding much more efficient solutions."
"We study here the well-known propagation rules for Boolean constraints. First
we propose a simple notion of completeness for sets of such rules and establish
a completeness result. Then we show an equivalence in an appropriate sense
between Boolean constraint propagation and unit propagation  a form of
resolution for propositional logic.
  Subsequently we characterize one set of such rules by means of the notion of
hyper-arc consistency introduced in (Mohr and Masini 1988). Also  we clarify
the status of a similar  though different  set of rules introduced in (Simonis
1989a) and more fully in (Codognet and Diaz 1996)."
"A general notion of algebraic conditional plausibility measures is defined.
Probability measures  ranking functions  possibility measures  and (under the
appropriate definitions) sets of probability measures can all be viewed as
defining algebraic conditional plausibility measures. It is shown that
algebraic conditional plausibility measures can be represented using Bayesian
networks."
"In this paper we present a rule based formalism for filtering variables
domains of constraints. This formalism is well adapted for solving dynamic CSP.
We take diagnosis as an instance problem to illustrate the use of these rules.
A diagnosis problem is seen like finding all the minimal sets of constraints to
be relaxed in the constraint network that models the device to be diagnosed"
"Despite the effort of many researchers in the area of multi-agent systems
(MAS) for designing and programming agents  a few years ago the research
community began to take into account that common features among different MAS
exists. Based on these common features  several tools have tackled the problem
of agent development on specific application domains or specific types of
agents. As a consequence  their scope is restricted to a subset of the huge
application domain of MAS. In this paper we propose a generic infrastructure
for programming agents whose name is Brainstorm/J. The infrastructure has been
implemented as an object oriented framework. As a consequence  our approach
supports a broader scope of MAS applications than previous efforts  being
flexible and reusable."
"In fuzzy propositional logic  to a proposition a partial truth in [0 1] is
assigned. It is well known that under certain circumstances  fuzzy logic
collapses to classical logic. In this paper  we will show that under dual
conditions  fuzzy logic collapses to four-valued (relevance) logic  where
propositions have truth-value true  false  unknown  or contradiction. As a
consequence  fuzzy entailment may be considered as ``in between'' four-valued
(relevance) entailment and classical entailment."
"We propose a new definition of actual cause  using structural equations to
model counterfactuals. We show that the definition yields a plausible and
elegant account of causation that handles well examples which have caused
problems for other definitions and resolves major difficulties in the
traditional account."
"Many logic programming based approaches can be used to describe and solve
combinatorial search problems. On the one hand there is constraint logic
programming which computes a solution as an answer substitution to a query
containing the variables of the constraint satisfaction problem. On the other
hand there are systems based on stable model semantics  abductive systems  and
first order logic model generators which compute solutions as models of some
theory. This paper compares these different approaches from the point of view
of knowledge representation (how declarative are the programs) and from the
point of view of performance (how good are they at solving typical problems)."
"In this paper  we introduce a new machine learning theory based on
multi-channel parallel adaptation for rule discovery. This theory is
distinguished from the familiar parallel-distributed adaptation theory of
neural networks in terms of channel-based convergence to the target rules. We
show how to realize this theory in a learning system named CFRule. CFRule is a
parallel weight-based model  but it departs from traditional neural computing
in that its internal knowledge is comprehensible. Furthermore  when the model
converges upon training  each channel converges to a target rule. The model
adaptation rule is derived by multi-level parallel weight optimization based on
gradient descent. Since  however  gradient descent only guarantees local
optimization  a multi-channel regression-based optimization strategy is
developed to effectively deal with this problem. Formally  we prove that the
CFRule model can explicitly and precisely encode any given rule set. Also  we
prove a property related to asynchronous parallel convergence  which is a
critical element of the multi-channel parallel adaptation theory for rule
learning. Thanks to the quantizability nature of the CFRule model  rules can be
extracted completely and soundly via a threshold-based mechanism. Finally  the
practical application of the theory is demonstrated in DNA promoter recognition
and hepatitis prognosis prediction."
"We present an approach for modelling the structure and coarse content of
legal documents with a view to providing automated support for the drafting of
contracts and contract database retrieval. The approach is designed to be
applicable where contract drafting is based on model-form contracts or on
existing examples of a similar type. The main features of the approach are. (1)
the representation addresses the structure and the interrelationships between
the constituent parts of contracts  but not the text of the document itself;
(2) the representation of documents is separated from the mechanisms that
manipulate it; and (3) the drafting process is subject to a collection of
explicitly stated constraints that govern the structure of the documents. We
describe the representation of document instances and of 'generic documents' 
which are data structures used to drive the creation of new document instances 
and we show extracts from a sample session to illustrate the features of a
prototype system implemented in MacProlog."
"One influential approach to assessing the ""goodness"" of arguments is offered
by the Pragma-Dialectical school (p-d) (Eemeren & Grootendorst 1992). This can
be compared with Rhetorical Structure Theory (RST) (Mann & Thompson 1988)  an
approach that originates in discourse analysis. In p-d terms an argument is
good if it avoids committing a fallacy  whereas in RST terms an argument is
good if it is coherent. RST has been criticised (Snoeck Henkemans 1997) for
providing only a partially functional account of argument  and similar
criticisms have been raised in the Natural Language Generation (NLG)
community-particularly by Moore & Pollack (1992)- with regards to its account
of intentionality in text in general. Mann and Thompson themselves note that
although RST can be successfully applied to a wide range of texts from diverse
domains  it fails to characterise some types of text  most notably legal
contracts. There is ongoing research in the Artificial Intelligence and Law
community exploring the potential for providing electronic support to contract
negotiators  focusing on long-term  complex engineering agreements (see for
example Daskalopulu & Sergot 1997). This paper provides a brief introduction to
RST and illustrates its shortcomings with respect to contractual text. An
alternative approach for modelling argument structure is presented which not
only caters for contractual text  but also overcomes the aforementioned
limitations of RST."
"Information Integration is a young and exciting field with enormous research
and commercial significance in the new world of the Information Society. It
stands at the crossroad of Databases and Artificial Intelligence requiring
novel techniques that bring together different methods from these fields.
Information from disparate heterogeneous sources often with no a-priori common
schema needs to be synthesized in a flexible  transparent and intelligent way
in order to respond to the demands of a query thus enabling a more informed
decision by the user or application program. The field although relatively
young has already found many practical applications particularly for
integrating information over the World Wide Web. This paper gives a brief
introduction of the field highlighting some of the main current and future
research issues and application areas. It attempts to evaluate the current and
potential role of Computational Logic in this and suggests some of the problems
where logic-based techniques could be used."
"Constraint propagation is a general algorithmic approach for pruning the
search space of a CSP. In a uniform way  K. R. Apt has defined a computation as
an iteration of reduction functions over a domain. He has also demonstrated the
need for integrating static properties of reduction functions (commutativity
and semi-commutativity) to design specialized algorithms such as AC3 and DAC.
We introduce here a set of operators for modeling compositions of reduction
functions. Two of the major goals are to tackle parallel computations  and
dynamic behaviours (such as slow convergence)."
"We consider an approach to update nonmonotonic knowledge bases represented as
extended logic programs under answer set semantics. New information is
incorporated into the current knowledge base subject to a causal rejection
principle enforcing that  in case of conflicts  more recent rules are preferred
and older rules are overridden. Such a rejection principle is also exploited in
other approaches to update logic programs  e.g.  in dynamic logic programming
by Alferes et al. We give a thorough analysis of properties of our approach  to
get a better understanding of the causal rejection principle. We review
postulates for update and revision operators from the area of theory change and
nonmonotonic reasoning  and some new properties are considered as well. We then
consider refinements of our semantics which incorporate a notion of minimality
of change. As well  we investigate the relationship to other approaches 
showing that our approach is semantically equivalent to inheritance programs by
Buccafurri et al. and that it coincides with certain classes of dynamic logic
programs  for which we provide characterizations in terms of graph conditions.
Therefore  most of our results about properties of causal rejection principle
apply to these approaches as well. Finally  we deal with computational
complexity of our approach  and outline how the update semantics and its
refinements can be implemented on top of existing logic programming engines."
"We introduce a learning method called ``gradient-based reinforcement
planning'' (GREP). Unlike traditional DP methods that improve their policy
backwards in time  GREP is a gradient-based method that plans ahead and
improves its policy before it actually acts in the environment. We derive
formulas for the exact policy gradient that maximizes the expected future
reward and confirm our ideas with numerical experiments."
"Much work in computer science has adopted competitive analysis as a tool for
decision making under uncertainty. In this work we extend competitive analysis
to the context of multi-agent systems. Unlike classical competitive analysis
where the behavior of an agent's environment is taken to be arbitrary  we
consider the case where an agent's environment consists of other agents. These
agents will usually obey some (minimal) rationality constraints. This leads to
the definition of rational competitive analysis. We introduce the concept of
rational competitive analysis  and initiate the study of competitive analysis
for multi-agent systems. We also discuss the application of rational
competitive analysis to the context of bidding games  as well as to the
classical one-way trading problem."
"This article aims at clarifying the language and practice of scientific
experiment  mainly by hooking observability on calculability."
"Many systems that exhibit nonmonotonic behavior have been described and
studied already in the literature. The general notion of nonmonotonic
reasoning  though  has almost always been described only negatively  by the
property it does not enjoy  i.e. monotonicity. We study here general patterns
of nonmonotonic reasoning and try to isolate properties that could help us map
the field of nonmonotonic reasoning by reference to positive properties. We
concentrate on a number of families of nonmonotonic consequence relations 
defined in the style of Gentzen. Both proof-theoretic and semantic points of
view are developed in parallel. The former point of view was pioneered by D.
Gabbay  while the latter has been advocated by Y. Shoham in. Five such families
are defined and characterized by representation theorems  relating the two
points of view. One of the families of interest  that of preferential
relations  turns out to have been studied by E. Adams. The ""preferential""
models proposed here are a much stronger tool than Adams' probabilistic
semantics. The basic language used in this paper is that of propositional
logic. The extension of our results to first order predicate calculi and the
study of the computational complexity of the decision problems described in
this paper will be treated in another paper."
"This paper presents a logical approach to nonmonotonic reasoning based on the
notion of a nonmonotonic consequence relation. A conditional knowledge base 
consisting of a set of conditional assertions of the type ""if ... then ..."" 
represents the explicit defeasible knowledge an agent has about the way the
world generally behaves. We look for a plausible definition of the set of all
conditional assertions entailed by a conditional knowledge base. In a previous
paper  S. Kraus and the authors defined and studied ""preferential"" consequence
relations. They noticed that not all preferential relations could be considered
as reasonable inference procedures. This paper studies a more restricted class
of consequence relations  ""rational"" relations. It is argued that any
reasonable nonmonotonic inference procedure should define a rational relation.
It is shown that the rational relations are exactly those that may be
represented by a ""ranked"" preferential model  or by a (non-standard)
probabilistic model. The rational closure of a conditional knowledge base is
defined and shown to provide an attractive answer to the question of the title.
Global properties of this closure operation are proved. it is a cumulative
operation. It is also computationally tractable. This paper assumes the
underlying language is propositional."
"It is shown that Darwiche and Pearl's postulates imply an interesting
property  not noticed by the authors."
"A vast and interesting family of natural semantics for belief revision is
defined. Suppose one is given a distance d between any two models. One may then
define the revision of a theory K by a formula a as the theory defined by the
set of all those models of a that are closest  by d  to the set of models of K.
This family is characterized by a set of rationality postulates that extends
the AGM postulates. The new postulates describe properties of iterated
revisions."
"We give a semantics to iterated update by a preference relation on possible
developments. An iterated update is a sequence of formulas  giving (incomplete)
information about successive states of the world. A development is a sequence
of models  describing a possible trajectory through time. We assume a principle
of inertia and prefer those developments  which are compatible with the
information  and avoid unnecessary changes. The logical properties of the
updates defined in this way are considered  and a representation result is
proved."
"A. Tarski proposed the study of infinitary consequence operations as the
central topic of mathematical logic. He considered monotonicity to be a
property of all such operations. In this paper  we weaken the monotonicity
requirement and consider more general operations  inference operations. These
operations describe the nonmonotonic logics both humans and machines seem to be
using when infering defeasible information from incomplete knowledge. We single
out a number of interesting families of inference operations. This study of
infinitary inference operations is inspired by the results of Kraus  Lehmann
and Magidor on finitary nonmonotonic operations  but this paper is
self-contained."
"The Expansion property considered by researchers in Social Choice is shown to
correspond to a logical property of nonmonotonic consequence relations that is
the {\em pure}  i.e.  not involving connectives  version of a previously known
weak rationality condition. The assumption that the union of two definable sets
of models is definable is needed for the soundness part of the result."
"The lexicographic closure of any given finite set D of normal defaults is
defined. A conditional assertion ""if a then b"" is in this lexicographic closure
if  given the defaults D and the fact a  one would conclude b. The
lexicographic closure is essentially a rational extension of D  and of its
rational closure  defined in a previous paper. It provides a logic of normal
defaults that is different from the one proposed by R. Reiter and that is rich
enough not to require the consideration of non-normal defaults. A large number
of examples are provided to show that the lexicographic closure corresponds to
the basic intuitions behind Reiter's logic of defaults."
"We provide a characterization of those nonmonotonic inference operations C
for which C(X) may be described as the set of all logical consequences of X
together with some set of additional assumptions S(X) that depends
anti-monotonically on X (i.e.  X is a subset of Y implies that S(Y) is a subset
of S(X)). The operations represented are exactly characterized in terms of
properties most of which have been studied in Freund-Lehmann(cs.AI/0202031).
Similar characterizations of right-absorbing and cumulative operations are also
provided. For cumulative operations  our results fit in closely with those of
Freund. We then discuss extending finitary operations to infinitary operations
in a canonical way and discuss co-compactness properties. Our results provide a
satisfactory notion of pseudo-compactness  generalizing to deductive
nonmonotonic operations the notion of compactness for monotonic operations.
They also provide an alternative  more elegant and more general  proof of the
existence of an infinitary deductive extension for any finitary deductive
operation (Theorem 7.9 of Freund-Lehmann)."
"Stereotypical reasoning assumes that the situation at hand is one of a kind
and that it enjoys the properties generally associated with that kind of
situation. It is one of the most basic forms of nonmonotonic reasoning. A
formal model for stereotypical reasoning is proposed and the logical properties
of this form of reasoning are studied. Stereotypical reasoning is shown to be
cumulative under weak assumptions."
"We introduce a methodology and framework for expressing general preference
information in logic programming under the answer set semantics. An ordered
logic program is an extended logic program in which rules are named by unique
terms  and in which preferences among rules are given by a set of atoms of form
s < t where s and t are names. An ordered logic program is transformed into a
second  regular  extended logic program wherein the preferences are respected 
in that the answer sets obtained in the transformed program correspond with the
preferred answer sets of the original program. Our approach allows the
specification of dynamic orderings  in which preferences can appear arbitrarily
within a program. Static orderings (in which preferences are external to a
logic program) are a trivial restriction of the general dynamic case. First  we
develop a specific approach to reasoning with preferences  wherein the
preference ordering specifies the order in which rules are to be applied. We
then demonstrate the wide range of applicability of our framework by showing
how other approaches  among them that of Brewka and Eiter  can be captured
within our framework. Since the result of each of these transformations is an
extended logic program  we can make use of existing implementations  such as
dlv and smodels. To this end  we have developed a publicly available compiler
as a front-end for these programming systems."
"Prioritized default reasoning has illustrated its rich expressiveness and
flexibility in knowledge representation and reasoning. However  many important
aspects of prioritized default reasoning have yet to be thoroughly explored. In
this paper  we investigate two properties of prioritized logic programs in the
context of answer set semantics. Specifically  we reveal a close relationship
between mutual defeasibility and uniqueness of the answer set for a prioritized
logic program. We then explore how the splitting technique for extended logic
programs can be extended to prioritized logic programs. We prove splitting
theorems that can be used to simplify the evaluation of a prioritized logic
program under certain conditions."
"The (extended) AGM postulates for belief revision seem to deal with the
revision of a given theory K by an arbitrary formula  but not to constrain the
revisions of two different theories by the same formula. A new postulate is
proposed and compared with other similar postulates that have been proposed in
the literature. The AGM revisions that satisfy this new postulate stand in
one-to-one correspondence with the rational  consistency-preserving relations.
This correspondence is described explicitly. Two viewpoints on iterative
revisions are distinguished and discussed."
"We study fixpoints of operators on lattices. To this end we introduce the
notion of an approximation of an operator. We order approximations by means of
a precision ordering. We show that each lattice operator O has a unique most
precise or ultimate approximation. We demonstrate that fixpoints of this
ultimate approximation provide useful insights into fixpoints of the operator
O.
  We apply our theory to logic programming and introduce the ultimate
Kripke-Kleene  well-founded and stable semantics. We show that the ultimate
Kripke-Kleene and well-founded semantics are more precise then their standard
counterparts We argue that ultimate semantics for logic programming have
attractive epistemological properties and that  while in general they are
computationally more complex than the standard semantics  for many classes of
theories  their complexity is no worse."
"Representing defeasibility is an important issue in common sense reasoning.
In reasoning about action and change  this issue becomes more difficult because
domain and action related defeasible information may conflict with general
inertia rules. Furthermore  different types of defeasible information may also
interfere with each other during the reasoning. In this paper  we develop a
prioritized logic programming approach to handle defeasibilities in reasoning
about action. In particular  we propose three action languages {\cal AT}^{0} 
{\cal AT}^{1} and {\cal AT}^{2} which handle three types of defeasibilities in
action domains named defeasible constraints  defeasible observations and
actions with defeasible and abnormal effects respectively. Each language with a
higher superscript can be viewed as an extension of the language with a lower
superscript. These action languages inherit the simple syntax of {\cal A}
language but their semantics is developed in terms of transition systems where
transition functions are defined based on prioritized logic programs. By
illustrating various examples  we show that our approach eventually provides a
powerful mechanism to handle various defeasibilities in temporal prediction and
postdiction. We also investigate semantic properties of these three action
languages and characterize classes of action domains that present more
desirable solutions in reasoning about action within the underlying action
languages."
"An anticipatory system for guiding plot development in interactive narratives
is described. The executable model is a finite automaton that provides the
implemented system with a look-ahead. The identification of undesirable future
states in the model is used to guide the player  in a transparent manner. In
this way  too radical twists of the plot can be avoided. Since the player
participates in the development of the plot  such guidance can have many forms 
depending on the environment of the player  on the behavior of the other
players  and on the means of player interaction. We present a design method for
interactive narratives which produces designs suitable for the implementation
of anticipatory mechanisms. Use of the method is illustrated by application to
our interactive computer game Kaktus."
"Open logic programs and open entailment have been recently proposed as an
abstract framework for the verification of incomplete specifications based upon
normal logic programs and the stable model semantics. There are obvious
analogies between open predicates and abducible predicates. However  despite
superficial similarities  there are features of open programs that have no
immediate counterpart in the framework of abduction and viceversa. Similarly 
open programs cannot be immediately simulated with answer set programming
(ASP). In this paper we start a thorough investigation of the relationships
between open inference  abduction and ASP. We shall prove that open programs
generalize the other two frameworks. The generalized framework suggests
interesting extensions of abduction under the generalized stable model
semantics. In some cases  we will be able to reduce open inference to abduction
and ASP  thereby estimating its computational complexity. At the same time  the
aforementioned reduction opens the way to new applications of abduction and
ASP."
"In this paper we consider three different kinds of domain-dependent control
knowledge (temporal  procedural and HTN-based) that are useful in planning. Our
approach is declarative and relies on the language of logic programming with
answer set semantics (AnsProlog*). AnsProlog* is designed to plan without
control knowledge. We show how temporal  procedural and HTN-based control
knowledge can be incorporated into AnsProlog* by the modular addition of a
small number of domain-dependent rules  without the need to modify the planner.
We formally prove the correctness of our planner  both in the absence and
presence of the control knowledge. Finally  we perform some initial
experimentation that demonstrates the potential reduction in planning time that
can be achieved when procedural domain knowledge is used to solve planning
problems with large plan length."
"Dung's abstract framework for argumentation enables a study of the
interactions between arguments based solely on an ``attack'' binary relation on
the set of arguments. Various ways to solve conflicts between contradictory
pieces of information have been proposed in the context of argumentation 
nonmonotonic reasoning or logic programming  and can be captured by appropriate
semantics within Dung's framework. A common feature of these semantics is that
one can always maximize in some sense the set of acceptable arguments. We
propose in this paper to extend Dung's framework in order to allow for the
representation of what we call ``restricted'' arguments. these arguments should
only be used if absolutely necessary  that is  in order to support other
arguments that would otherwise be defeated. We modify Dung's preferred
semantics accordingly. a set of arguments becomes acceptable only if it
contains a minimum of restricted arguments  for a maximum of unrestricted
arguments."
"We address a general representation problem for belief change  and describe
two interrelated representations for iterative non-prioritized change. a
logical representation in terms of persistent epistemic states  and a
constructive representation in terms of flocks of bases."
"An extension of an abstract argumentation framework  called collective
argumentation  is introduced in which the attack relation is defined directly
among sets of arguments. The extension turns out to be suitable  in particular 
for representing semantics of disjunctive logic programs. Two special kinds of
collective argumentation are considered in which the opponents can share their
arguments."
"Logic programs with ordered disjunction (LPODs) combine ideas underlying
Qualitative Choice Logic (Brewka et al. KR 2002) and answer set programming.
Logic programming under answer set semantics is extended with a new connective
called ordered disjunction. The new connective allows us to represent
alternative  ranked options for problem solutions in the heads of rules. A
\times B intuitively means. if possible A  but if A is not possible then at
least B. The semantics of logic programs with ordered disjunction is based on a
preference relation on answer sets. LPODs are useful for applications in design
and configuration and can serve as a basis for qualitative decision making."
"In this paper  we investigate the extent to which knowledge compilation can
be used to improve inference from propositional weighted bases. We present a
general notion of compilation of a weighted base that is parametrized by any
equivalence--preserving compilation function. Both negative and positive
results are presented. On the one hand  complexity results are identified 
showing that the inference problem from a compiled weighted base is as
difficult as in the general case  when the prime implicates  Horn cover or
renamable Horn cover classes are targeted. On the other hand  we show that the
inference problem becomes tractable whenever DNNF-compilations are used and
clausal queries are considered. Moreover  we show that the set of all preferred
models of a DNNF-compilation of a weighted base can be computed in time
polynomial in the output size. Finally  we sketch how our results can be used
in model-based diagnosis in order to compute the most probable diagnoses of a
system."
"This paper studies the problem of modeling complex domains of actions and
change within high-level action description languages. We investigate two main
issues of concern. (a) can we represent complex domains that capture together
different problems such as ramifications  non-determinism and concurrency of
actions  at a high-level  close to the given natural ontology of the problem
domain and (b) what features of such a representation can affect  and how  its
computational behaviour. The paper describes the main problems faced in this
representation task and presents the results of an empirical study  carried out
through a series of controlled experiments  to analyze the computational
performance of reasoning in these representations. The experiments compare
different representations obtained  for example  by changing the basic ontology
of the domain or by varying the degree of use of indirect effect laws through
domain constraints. This study has helped to expose the main sources of
computational difficulty in the reasoning and suggest some methodological
guidelines for representing complex domains. Although our work has been carried
out within one particular high-level description language  we believe that the
results  especially those that relate to the problems of representation  are
independent of the specific modeling language."
"This paper introduces the notion of value-based argumentation frameworks  an
extension of the standard argumentation frameworks proposed by Dung  which are
able toshow how rational decision is possible in cases where arguments derive
their force from the social values their acceptance would promote."
"We analyze the problem of defining well-founded semantics for ordered logic
programs within a general framework based on alternating fixpoint theory. We
start by showing that generalizations of existing answer set approaches to
preference are too weak in the setting of well-founded semantics. We then
specify some informal yet intuitive criteria and propose a semantical framework
for preference handling that is more suitable for defining well-founded
semantics for ordered logic programs. The suitability of the new approach is
convinced by the fact that many attractive properties are satisfied by our
semantics. In particular  our semantics is still correct with respect to
various existing answer sets semantics while it successfully overcomes the
weakness of their generalization to well-founded semantics. Finally  we
indicate how an existing preferred well-founded semantics can be captured
within our semantical framework."
"In this paper we present a transformation of finite propositional default
theories into so-called propositional argumentation systems. This
transformation allows to characterize all notions of Reiter's default logic in
the framework of argumentation systems. As a consequence  computing extensions 
or determining wether a given formula belongs to one extension or all
extensions can be answered without leaving the field of classical propositional
logic. The transformation proposed is linear in the number of defaults."
"In the present paper  the existence and multiplicity problems of extensions
are addressed. The focus is on extension of the stable type. The main result of
the paper is an elegant characterization of the existence and multiplicity of
extensions in terms of the notion of dialectical justification  a close cousin
of the notion of admissibility. The characterization is given in the context of
the particular logic for dialectical argumentation DEFLOG. The results are of
direct relevance for several well-established models of defeasible reasoning
(like default logic  logic programming and argumentation frameworks)  since
elsewhere dialectical argumentation has been shown to have close formal
connections with these models."
"Recently  it has been shown that probabilistic entailment under coherence is
weaker than model-theoretic probabilistic entailment. Moreover  probabilistic
entailment under coherence is a generalization of default entailment in System
P. In this paper  we continue this line of research by presenting probabilistic
generalizations of more sophisticated notions of classical default entailment
that lie between model-theoretic probabilistic entailment and probabilistic
entailment under coherence. That is  the new formalisms properly generalize
their counterparts in classical default reasoning  they are weaker than
model-theoretic probabilistic entailment  and they are stronger than
probabilistic entailment under coherence. The new formalisms are useful
especially for handling probabilistic inconsistencies related to conditioning
on zero events. They can also be applied for probabilistic belief revision.
More generally  in the same spirit as a similar previous paper  this paper
sheds light on exciting new formalisms for probabilistic reasoning beyond the
well-known standard ones."
"We seek to find normative criteria of adequacy for nonmonotonic logic similar
to the criterion of validity for deductive logic. Rather than stipulating that
the conclusion of an inference be true in all models in which the premises are
true  we require that the conclusion of a nonmonotonic inference be true in
``almost all'' models of a certain sort in which the premises are true. This
``certain sort'' specification picks out the models that are relevant to the
inference  taking into account factors such as specificity and vagueness  and
previous inferences. The frequencies characterizing the relevant models reflect
known frequencies in our actual world. The criteria of adequacy for a default
inference can be extended by thresholding to criteria of adequacy for an
extension. We show that this avoids the implausibilities that might otherwise
result from the chaining of default inferences. The model proportions  when
construed in terms of frequencies  provide a verifiable grounding of default
rules  and can become the basis for generating default rules from statistics."
"About ten years ago  various notions of preferential entailment have been
introduced. The main reference is a paper by Kraus  Lehmann and Magidor (KLM) 
one of the main competitor being a more general version defined by Makinson
(MAK). These two versions have already been compared  but it is time to revisit
these comparisons. Here are our three main results. (1) These two notions are
equivalent  provided that we restrict our attention  as done in KLM  to the
cases where the entailment respects logical equivalence (on the left and on the
right). (2) A serious simplification of the description of the fundamental
cases in which MAK is equivalent to KLM  including a natural passage in both
ways. (3) The two previous results are given for preferential entailments more
general than considered in some of the original texts  but they apply also to
the original definitions and  for this particular case also  the models can be
simplified."
"This work analyses main features that should be present in knowledge
representation. It suggests a model for representation and a way to implement
this model in software. Representation takes care of both low-level sensor
information and high-level concepts."
"We propose new definitions of (causal) explanation  using structural
equations to model counterfactuals. The definition is based on the notion of
actual cause  as defined and motivated in a companion paper. Essentially  an
explanation is a fact that is not known for certain but  if found to be true 
would constitute an actual cause of the fact to be explained  regardless of the
agent's initial uncertainty. We show that the definition handles well a number
of problematic examples from the literature."
"Recently  several approaches to updating knowledge bases modeled as extended
logic programs have been introduced  ranging from basic methods to incorporate
(sequences of) sets of rules into a logic program  to more elaborate methods
which use an update policy for specifying how updates must be incorporated. In
this paper  we introduce a framework for reasoning about evolving knowledge
bases  which are represented as extended logic programs and maintained by an
update policy. We first describe a formal model which captures various update
approaches  and we define a logical language for expressing properties of
evolving knowledge bases. We then investigate semantical and computational
properties of our framework  where we focus on properties of knowledge states
with respect to the canonical reasoning task of whether a given formula holds
on a given evolving knowledge base. In particular  we present finitary
characterizations of the evolution for certain classes of framework instances 
which can be exploited for obtaining decidability results. In more detail  we
characterize the complexity of reasoning for some meaningful classes of
evolving knowledge bases  ranging from polynomial to double exponential space
complexity."
"In this thesis I present a virtual laboratory which implements five different
models for controlling animats. a rule-based system  a behaviour-based system 
a concept-based system  a neural network  and a Braitenberg architecture.
Through different experiments  I compare the performance of the models and
conclude that there is no ""best"" model  since different models are better for
different things in different contexts.
  The models I chose  although quite simple  represent different approaches for
studying cognition. Using the results as an empirical philosophical aid 
  I note that there is no ""best"" approach for studying cognition  since
different approaches have all advantages and disadvantages  because they study
different aspects of cognition from different contexts. This has implications
for current debates on ""proper"" approaches for cognition. all approaches are a
bit proper  but none will be ""proper enough"". I draw remarks on the notion of
cognition abstracting from all the approaches used to study it  and propose a
simple classification for different types of cognition."
"This paper deals with the revision of partially ordered beliefs. It proposes
a semantic representation of epistemic states by partial pre-orders on
interpretations and a syntactic representation by partially ordered belief
bases. Two revision operations  the revision stemming from the history of
observations and the possibilistic revision  defined when the epistemic state
is represented by a total pre-order  are generalized  at a semantic level  to
the case of a partial pre-order on interpretations  and at a syntactic level 
to the case of a partially ordered belief base. The equivalence between the two
representations is shown for the two revision operations."
"This is the first in a series of connected papers discussing the problem of a
dynamically reconfigurable universal learning neurocomputer that could serve as
a computational model for the whole human brain. The whole series is entitled
""The Brain Zero Project. My Brain as a Dynamically Reconfigurable Universal
Learning Neurocomputer."" (For more information visit the website
www.brain0.com.) This introductory paper is concerned with general methodology.
Its main goal is to explain why it is critically important for both neural
modeling and cognitive modeling to pay much attention to the basic requirements
of the whole brain as a complex computing system. The author argues that it can
be easier to develop an adequate computational model for the whole
""unprogrammed"" (untrained) human brain than to find adequate formal
representations of some nontrivial parts of brain's performance. (In the same
way as  for example  it is easier to describe the behavior of a complex
analytical function than the behavior of its real and/or imaginary part.) The
""curse of dimensionality"" that plagues purely phenomenological (""brainless"")
cognitive theories is a natural penalty for an attempt to represent
insufficiently large parts of brain's performance in a state space of
insufficiently high dimensionality. A ""partial"" modeler encounters ""Catch 22.""
An attempt to simplify a cognitive problem by artificially reducing its
dimensionality makes the problem more difficult."
"As a part of our effort for studying the evolution and development of
cognition  we present results derived from synthetic experimentations in a
virtual laboratory where animats develop koncepts adaptively and ground their
meaning through action. We introduce the term ""koncept"" to avoid confusions and
ambiguity derived from the wide use of the word ""concept"". We present the
models which our animats use for abstracting koncepts from perceptions 
plastically adapt koncepts  and associate koncepts with actions. On a more
philosophical vein  we suggest that knowledge is a property of a cognitive
system  not an element  and therefore observer-dependent."
"This paper presents a model for dynamic adjustment of the motivation degree 
using a reinforcement learning approach  in an action selection mechanism
previously developed by the authors. The learning takes place in the
modification of a parameter of the model of combination of internal and
external stimuli. Experiments that show the claimed properties are presented 
using a VR simulation developed for such purposes. The importance of adaptation
by learning in action selection is also discussed."
"This article analyses the properties of the Internal Behaviour network  an
action selection mechanism previously proposed by the authors  with the aid of
a simulation developed for such ends. A brief review of the Internal Behaviour
network is followed by the explanation of the implementation of the simulation.
Then  experiments are presented and discussed analysing the properties of the
action selection in the proposed model."
"This paper proposes a model for combination of external and internal stimuli
for the action selection in an autonomous agent  based in an action selection
mechanism previously proposed by the authors. This combination model includes
additive and multiplicative elements  which allows to incorporate new
properties  which enhance the action selection. A given parameter a  which is
part of the proposed model  allows to regulate the degree of dependence of the
observed external behaviour from the internal states of the entity."
"Reinforcement learning (RL) involves sequential decision making in uncertain
environments. The aim of the decision-making agent is to maximize the benefit
of acting in its environment over an extended period of time. Finding an
optimal policy in RL may be very slow. To speed up learning  one often used
solution is the integration of planning  for example  Sutton's Dyna algorithm 
or various other methods using macro-actions.
  Here we suggest to separate plannable  i.e.  close to deterministic parts of
the world  and focus planning efforts in this domain. A novel reinforcement
learning method called plannable RL (pRL) is proposed here. pRL builds a simple
model  which is used to search for macro actions. The simplicity of the model
makes planning computationally inexpensive. It is shown that pRL finds an
optimal policy  and that plannable macro actions found by pRL are near-optimal.
In turn  it is unnecessary to try large numbers of macro actions  which enables
fast learning. The utility of pRL is demonstrated by computer simulations."
"Optimization of decision problems in stochastic environments is usually
concerned with maximizing the probability of achieving the goal and minimizing
the expected episode length. For interacting agents in time-critical
applications  learning of the possibility of scheduling of subtasks (events) or
the full task is an additional relevant issue. Besides  there exist highly
stochastic problems where the actual trajectories show great variety from
episode to episode  but completing the task takes almost the same amount of
time. The identification of sub-problems of this nature may promote e.g. 
planning  scheduling and segmenting Markov decision processes. In this work 
formulae for the average duration as well as the standard deviation of the
duration of events are derived. The emerging Bellman-type equation is a simple
extension of Sobel's work (1982). Methods of dynamic programming as well as
methods of reinforcement learning can be applied for our extension. Computer
demonstration on a toy problem serve to highlight the principle."
"Much work has been done on extending the well-founded semantics to general
disjunctive logic programs and various approaches have been proposed. However 
these semantics are different from each other and no consensus is reached about
which semantics is the most intended. In this paper we look at disjunctive
well-founded reasoning from different angles. We show that there is an
intuitive form of the well-founded reasoning in disjunctive logic programming
which can be characterized by slightly modifying some exisitng approaches to
defining disjunctive well-founded semantics  including program transformations 
argumentation  unfounded sets (and resolution-like procedure). We also provide
a bottom-up procedure for this semantics. The significance of our work is not
only in clarifying the relationship among different approaches  but also shed
some light on what is an intended well-founded semantics for disjunctive logic
programs."
"We provide a semantic framework for preference handling in answer set
programming. To this end  we introduce preference preserving consequence
operators. The resulting fixpoint characterizations provide us with a uniform
semantic framework for characterizing preference handling in existing
approaches. Although our approach is extensible to other semantics by means of
an alternating fixpoint theory  we focus here on the elaboration of preferences
under answer set semantics. Alternatively  we show how these approaches can be
characterized by the concept of order preservation. These uniform semantic
characterizations provide us with new insights about interrelationships and
moreover about ways of implementation."
"The work reported here introduces Defeasible Logic Programming (DeLP)  a
formalism that combines results of Logic Programming and Defeasible
Argumentation. DeLP provides the possibility of representing information in the
form of weak rules in a declarative manner  and a defeasible argumentation
inference mechanism for warranting the entailed conclusions.
  In DeLP an argumentation formalism will be used for deciding between
contradictory goals. Queries will be supported by arguments that could be
defeated by other arguments. A query q will succeed when there is an argument A
for q that is warranted  ie  the argument A that supports q is found undefeated
by a warrant procedure that implements a dialectical analysis.
  The defeasible argumentation basis of DeLP allows to build applications that
deal with incomplete and contradictory information in dynamic domains. Thus 
the resulting approach is suitable for representing agent's knowledge and for
providing an argumentation based reasoning mechanism to agents."
"Cooperative constraint solving is an area of constraint programming that
studies the interaction between constraint solvers with the aim of discovering
the interaction patterns that amplify the positive qualities of individual
solvers. Automatisation and formalisation of such studies is an important issue
of cooperative constraint solving.
  In this paper we present a constraint-based analysis of composite solvers
that integrates reasoning about the individual solvers and the processed data.
The idea is to approximate this reasoning by resolution of set constraints on
the finite sets representing the predicates that express all the necessary
properties. We illustrate application of our analysis to two important
cooperation patterns. deterministic choice and loop."
"There is a growing interest in using Kalman-filter models for brain
modelling. In turn  it is of considerable importance to represent Kalman-filter
in connectionist forms with local Hebbian learning rules. To our best
knowledge  Kalman-filter has not been given such local representation. It seems
that the main obstacle is the dynamic adaptation of the Kalman-gain. Here  a
connectionist representation is presented  which is derived by means of the
recursive prediction error method. We show that this method gives rise to
attractive local learning rules and can adapt the Kalman-gain."
"We discuss philosophical issues concerning the notion of cognition basing
ourselves in experimental results in cognitive sciences  especially in computer
simulations of cognitive systems. There have been debates on the ""proper""
approach for studying cognition  but we have realized that all approaches can
be in theory equivalent. Different approaches model different properties of
cognitive systems from different perspectives  so we can only learn from all of
them. We also integrate ideas from several perspectives for enhancing the
notion of cognition  such that it can contain other definitions of cognition as
special cases. This allows us to propose a simple classification of different
types of cognition."
"The paper studies an implementation methodology for partial and disjunctive
stable models where partiality and disjunctions are unfolded from a logic
program so that an implementation of stable models for normal
(disjunction-free) programs can be used as the core inference engine. The
unfolding is done in two separate steps. Firstly  it is shown that partial
stable models can be captured by total stable models using a simple linear and
modular program transformation. Hence  reasoning tasks concerning partial
stable models can be solved using an implementation of total stable models.
Disjunctive partial stable models have been lacking implementations which now
become available as the translation handles also the disjunctive case.
Secondly  it is shown how total stable models of disjunctive programs can be
determined by computing stable models for normal programs. Hence  an
implementation of stable models of normal programs can be used as a core engine
for implementing disjunctive programs. The feasibility of the approach is
demonstrated by constructing a system for computing stable models of
disjunctive programs using the smodels system as the core engine. The
performance of the resulting system is compared to that of dlv which is a
state-of-the-art special purpose system for disjunctive programs."
"When tracking a large number of targets  it is often computationally
expensive to represent the full joint distribution over target states. In cases
where the targets move independently  each target can instead be tracked with a
separate filter. However  this leads to a model-data association problem.
Another approach to solve the problem with computational complexity is to track
only the first moment of the joint distribution  the probability hypothesis
density (PHD). The integral of this distribution over any area S is the
expected number of targets within S. Since no record of object identity is
kept  the model-data association problem is avoided.
  The contribution of this paper is a particle filter implementation of the PHD
filter mentioned above. This PHD particle filter is applied to tracking of
multiple vehicles in terrain  a non-linear tracking problem. Experiments show
that the filter can track a changing number of vehicles robustly  achieving
near-real-time performance."
"Search in cyclic AND/OR graphs was traditionally known to be an unsolved
problem. In the recent past several important studies have been reported in
this domain. In this paper  we have taken a fresh look at the problem. First  a
new and comprehensive theoretical framework for cyclic AND/OR graphs has been
presented  which was found missing in the recent literature. Based on this
framework  two best-first search algorithms  S1 and S2  have been developed. S1
does uninformed search and is a simple modification of the Bottom-up algorithm
by Martelli and Montanari. S2 performs a heuristically guided search and
replicates the modification in Bottom-up's successors  namely HS and AO*. Both
S1 and S2 solve the problem of searching AND/OR graphs in presence of cycles.
We then present a detailed analysis for the correctness and complexity results
of S1 and S2  using the proposed framework. We have observed through
experiments that S1 and S2 output correct results in all cases."
"Thomas M. Strat has developed a decision-theoretic apparatus for
Dempster-Shafer theory (Decision analysis using belief functions  Intern. J.
Approx. Reason. 4(5/6)  391-417  1990). In this apparatus  expected utility
intervals are constructed for different choices. The choice with the highest
expected utility is preferable to others. However  to find the preferred choice
when the expected utility interval of one choice is included in that of
another  it is necessary to interpolate a discerning point in the intervals.
This is done by the parameter rho  defined as the probability that the
ambiguity about the utility of every nonsingleton focal element will turn out
as favorable as possible. If there are several different decision makers  we
might sometimes be more interested in having the highest expected utility among
the decision makers rather than only trying to maximize our own expected
utility regardless of choices made by other decision makers. The preference of
each choice is then determined by the probability of yielding the highest
expected utility. This probability is equal to the maximal interval length of
rho under which an alternative is preferred. We must here take into account not
only the choices already made by other decision makers but also the rational
choices we can assume to be made by later decision makers. In Strats apparatus 
an assumption  unwarranted by the evidence at hand  has to be made about the
value of rho. We demonstrate that no such assumption is necessary. It is
sufficient to assume a uniform probability distribution for rho to be able to
discern the most preferable choice. We discuss when this approach is
justifiable."
"Currently  there is renewed interest in the problem  raised by Shafer in
1985  of updating probabilities when observations are incomplete. This is a
fundamental problem in general  and of particular interest for Bayesian
networks. Recently  Grunwald and Halpern have shown that commonly used updating
strategies fail in this case  except under very special assumptions. In this
paper we propose a new method for updating probabilities with incomplete
observations. Our approach is deliberately conservative. we make no assumptions
about the so-called incompleteness mechanism that associates complete with
incomplete observations. We model our ignorance about this mechanism by a
vacuous lower prevision  a tool from the theory of imprecise probabilities  and
we use only coherence arguments to turn prior into posterior probabilities. In
general  this new approach to updating produces lower and upper posterior
probabilities and expectations  as well as partially determinate decisions.
This is a logical consequence of the existing ignorance about the
incompleteness mechanism. We apply the new approach to the problem of
classification of new evidence in probabilistic expert systems  where it leads
to a new  so-called conservative updating rule. In the special case of Bayesian
networks constructed using expert knowledge  we provide an exact algorithm for
classification based on our updating rule  which has linear-time complexity for
a class of networks wider than polytrees. This result is then extended to the
more general framework of credal networks  where computations are often much
harder than with Bayesian nets. Using an example  we show that our rule appears
to provide a solid basis for reliable updating with incomplete observations 
when no strong assumptions about the incompleteness mechanism are justified."
"As examples such as the Monty Hall puzzle show  applying conditioning to
update a probability distribution on a ``naive space''  which does not take
into account the protocol used  can often lead to counterintuitive results.
Here we examine why. A criterion known as CAR (``coarsening at random'') in the
statistical literature characterizes when ``naive'' conditioning in a naive
space works. We show that the CAR condition holds rather infrequently  and we
provide a procedural characterization of it  by giving a randomized algorithm
that generates all and only distributions for which CAR holds. This
substantially extends previous characterizations of CAR. We also consider more
generalized notions of update such as Jeffrey conditioning and minimizing
relative entropy (MRE). We give a generalization of the CAR condition that
characterizes when Jeffrey conditioning leads to appropriate answers  and show
that there exist some very simple settings in which MRE essentially never gives
the right results. This generalizes and interconnects previous results obtained
in the literature on CAR and MRE."
"Configuring consists in simulating the realization of a complex product from
a catalog of component parts  using known relations between types  and picking
values for object attributes. This highly combinatorial problem in the field of
constraint programming has been addressed with a variety of approaches since
the foundation system R1(McDermott82). An inherent difficulty in solving
configuration problems is the existence of many isomorphisms among
interpretations. We describe a formalism independent approach to improve the
detection of isomorphisms by configurators  which does not require to adapt the
problem model. To achieve this  we exploit the properties of a characteristic
subset of configuration problems  called the structural sub-problem  which
canonical solutions can be produced or tested at a limited cost. In this paper
we present an algorithm for testing the canonicity of configurations  that can
be added as a symmetry breaking constraint to any configurator. The cost and
efficiency of this canonicity test are given."
"This article introduces the idea that probabilistic reasoning (PR) may be
understood as ""information compression by multiple alignment  unification and
search"" (ICMAUS). In this context  multiple alignment has a meaning which is
similar to but distinct from its meaning in bio-informatics  while unification
means a simple merging of matching patterns  a meaning which is related to but
simpler than the meaning of that term in logic.
  A software model  SP61  has been developed for the discovery and formation of
'good' multiple alignments  evaluated in terms of information compression. The
model is described in outline.
  Using examples from the SP61 model  this article describes in outline how the
ICMAUS framework can model various kinds of PR including. PR in best-match
pattern recognition and information retrieval; one-step 'deductive' and
'abductive' PR; inheritance of attributes in a class hierarchy; chains of
reasoning (probabilistic decision networks and decision trees  and PR with
'rules'); geometric analogy problems; nonmonotonic reasoning and reasoning with
default values; modelling the function of a Bayesian network."
"This article presents an overview of the idea that ""information compression
by multiple alignment  unification and search"" (ICMAUS) may serve as a unifying
principle in computing (including mathematics and logic) and in such aspects of
human cognition as the analysis and production of natural language  fuzzy
pattern recognition and best-match information retrieval  concept hierarchies
with inheritance of attributes  probabilistic reasoning  and unsupervised
inductive learning. The ICMAUS concepts are described together with an outline
of the SP61 software model in which the ICMAUS concepts are currently realised.
A range of examples is presented  illustrated with output from the SP61 model."
"We propose a calculus integrating two calculi well-known in Qualitative
Spatial Reasoning (QSR). Frank's projection-based cardinal direction calculus 
and a coarser version of Freksa's relative orientation calculus. An original
constraint propagation procedure is presented  which implements the interaction
between the two integrated calculi. The importance of taking into account the
interaction is shown with a real example providing an inconsistent knowledge
base  whose inconsistency (a) cannot be detected by reasoning separately about
each of the two components of the knowledge  just because  taken separately 
each is consistent  but (b) is detected by the proposed algorithm  thanks to
the interaction knowledge propagated from each of the two compnents to the
other."
"We define a ternary Relation Algebra (RA) of relative position relations on
two-dimensional directed lines (d-lines for short). A d-line has two degrees of
freedom (DFs). a rotational DF (RDF)  and a translational DF (TDF). The
representation of the RDF of a d-line will be handled by an RA of 2D
orientations  CYC_t  known in the literature. A second algebra  TA_t  which
will handle the TDF of a d-line  will be defined. The two algebras  CYC_t and
TA_t  will constitute  respectively  the translational and the rotational
components of the RA  PA_t  of relative position relations on d-lines. the PA_t
atoms will consist of those pairs <t r> of a TA_t atom and a CYC_t atom that
are compatible. We present in detail the RA PA_t  with its converse table  its
rotation table and its composition tables. We show that a (polynomial)
constraint propagation algorithm  known in the literature  is complete for a
subset of PA_t relations including almost all of the atomic relations. We will
discuss the application scope of the RA  which includes incidence geometry  GIS
(Geographic Information Systems)  shape representation  localisation in
(multi-)robot navigation  and the representation of motion prepositions in NLP
(Natural Language Processing). We then compare the RA to existing ones  such as
an algebra for reasoning about rectangles parallel to the axes of an
(orthogonal) coordinate system  a ``spatial Odyssey'' of Allen's interval
algebra  and an algebra for reasoning about 2D segments."
"An intelligent agent will often be uncertain about various properties of its
environment  and when acting in that environment it will frequently need to
quantify its uncertainty. For example  if the agent wishes to employ the
expected-utility paradigm of decision theory to guide its actions  it will need
to assign degrees of belief (subjective probabilities) to various assertions.
Of course  these degrees of belief should not be arbitrary  but rather should
be based on the information available to the agent. This paper describes one
approach for inducing degrees of belief from very rich knowledge bases  that
can include information about particular individuals  statistical correlations 
physical laws  and default rules. We call our approach the random-worlds
method. The method is based on the principle of indifference. it treats all of
the worlds the agent considers possible as being equally likely. It is able to
integrate qualitative default reasoning with quantitative probabilistic
reasoning by providing a language in which both types of information can be
easily expressed. Our results show that a number of desiderata that arise in
direct inference (reasoning from statistical information to conclusions about
individuals) and default reasoning follow directly {from} the semantics of
random worlds. For example  random worlds captures important patterns of
reasoning such as specificity  inheritance  indifference to irrelevant
information  and default assumptions of independence. Furthermore  the
expressive power of the language used and the intuitive semantics of random
worlds allow the method to deal with problems that are beyond the scope of many
other non-deductive reasoning systems."
"This paper describes an approach to the representation and processing of
ontologies in the Semantic Web  based on the ICMAUS theory of computation and
AI. This approach has strengths that complement those of languages based on the
Resource Description Framework (RDF) such as RDF Schema and DAML+OIL. The main
benefits of the ICMAUS approach are simplicity and comprehensibility in the
representation of ontologies  an ability to cope with errors and uncertainties
in knowledge  and a versatile reasoning system with capabilities in the kinds
of probabilistic reasoning that seem to be required in the Semantic Web."
"Interactions are patterns between several attributes in data that cannot be
inferred from any subset of these attributes. While mutual information is a
well-established approach to evaluating the interactions between two
attributes  we surveyed its generalizations as to quantify interactions between
several attributes. We have chosen McGill's interaction information  which has
been independently rediscovered a number of times under various names in
various disciplines  because of its many intuitively appealing properties. We
apply interaction information to visually present the most important
interactions of the data. Visualization of interactions has provided insight
into the structure of data on a number of domains  identifying redundant
attributes and opportunities for constructing new features  discovering
unexpected regularities in data  and have helped during construction of
predictive models; we illustrate the methods on numerous examples. A machine
learning method that disregards interactions may get caught in two traps.
myopia is caused by learning algorithms assuming independence in spite of
interactions  whereas fragmentation arises from assuming an interaction in
spite of independence."
"In this paper we develop an evidential force aggregation method intended for
classification of evidential intelligence into recognized force structures. We
assume that the intelligence has already been partitioned into clusters and use
the classification method individually in each cluster. The classification is
based on a measure of fitness between template and fused intelligence that
makes it possible to handle intelligence reports with multiple nonspecific and
uncertain propositions. With this measure we can aggregate on a level-by-level
basis  starting from general intelligence to achieve a complete force structure
with recognized units on all hierarchical levels."
"Article discusses the application of Kullback-Leibler divergence to the
recognition of speech signals and suggests three algorithms implementing this
divergence criterion. correlation algorithm  spectral algorithm and filter
algorithm. Discussion covers an approach to the problem of speech variability
and is illustrated with the results of experimental modeling of speech signals.
The article gives a number of recommendations on the choice of appropriate
model parameters and provides a comparison to some other methods of speech
recognition."
"Richard Cox [1] set the axiomatic foundations of probable inference and the
algebra of propositions. He showed that consistency within these axioms
requires certain rules for updating belief. In this paper we use the analogy
between probability and utility introduced in [2] to propose an axiomatic
foundation for utility inference and the algebra of preferences. We show that
consistency within these axioms requires certain rules for updating preference.
We discuss a class of utility functions that stems from the axioms of utility
inference and show that this class is the basic building block for any general
multiattribute utility function. We use this class of utility functions
together with the algebra of preferences to construct utility functions
represented by logical operations on the attributes."
"Recent literature in the last Maximum Entropy workshop introduced an analogy
between cumulative probability distributions and normalized utility functions.
Based on this analogy  a utility density function can de defined as the
derivative of a normalized utility function. A utility density function is
non-negative and integrates to unity. These two properties form the basis of a
correspondence between utility and probability. A natural application of this
analogy is a maximum entropy principle to assign maximum entropy utility
values. Maximum entropy utility interprets many of the common utility functions
based on the preference information needed for their assignment  and helps
assign utility values based on partial preference information. This paper
reviews maximum entropy utility and introduces further results that stem from
the duality between probability and utility."
"Abduction  first proposed in the setting of classical logics  has been
studied with growing interest in the logic programming area during the last
years.
  In this paper we study abduction with penalization in the logic programming
framework. This form of abductive reasoning  which has not been previously
analyzed in logic programming  turns out to represent several relevant
problems  including optimization problems  very naturally. We define a formal
model for abduction with penalization over logic programs  which extends the
abductive framework proposed by Kakas and Mancarella. We address knowledge
representation issues  encoding a number of problems in our abductive
framework. In particular  we consider some relevant problems  taken from
different domains  ranging from optimization theory to diagnosis and planning;
their encodings turn out to be simple and elegant in our formalism. We
thoroughly analyze the computational complexity of the main problems arising in
the context of abduction with penalization from logic programs. Finally  we
implement a system supporting the proposed abductive framework on top of the
DLV engine. To this end  we design a translation from abduction problems with
penalties into logic programs with weak constraints. We prove that this
approach is sound and complete."
"We study local-search satisfiability solvers for propositional logic extended
with cardinality atoms  that is  expressions that provide explicit ways to
model constraints on cardinalities of sets. Adding cardinality atoms to the
language of propositional logic facilitates modeling search problems and often
results in concise encodings. We propose two ``native'' local-search solvers
for theories in the extended language. We also describe techniques to reduce
the problem to standard propositional satisfiability and allow us to use
off-the-shelf SAT solvers. We study these methods experimentally. Our general
finding is that native solvers designed specifically for the extended language
perform better than indirect methods relying on SAT solvers."
"We describe WSAT(cc)  a local-search solver for computing models of theories
in the language of propositional logic extended by cardinality atoms. WSAT(cc)
is a processing back-end for the logic PS+  a recently proposed formalism for
answer-set programming."
"This paper presents duality between probability distributions and utility
functions."
"Disjunctive Logic Programming (\DLP) is an advanced formalism for Knowledge
Representation and Reasoning (KRR). \DLP is very expressive in a precise
mathematical sense. it allows to express every property of finite structures
that is decidable in the complexity class $\SigmaP{2}$ ($\NP^{\NP}$).
Importantly  the \DLP encodings are often simple and natural.
  In this paper  we single out some limitations of \DLP for KRR  which cannot
naturally express problems where the size of the disjunction is not known ``a
priori'' (like N-Coloring)  but it is part of the input. To overcome these
limitations  we further enhance the knowledge modelling abilities of \DLP  by
extending this language by {\em Parametric Connectives (OR and AND)}. These
connectives allow us to represent compactly the disjunction/conjunction of a
set of atoms having a given property. We formally define the semantics of the
new language  named $DLP^{\bigvee \bigwedge}$ and we show the usefulness of the
new constructs on relevant knowledge-based problems. We address implementation
issues and discuss related works."
"The research field of Agent-Oriented Software Engineering (AOSE) aims to find
abstractions  languages  methodologies and toolkits for modeling  verifying 
validating and prototyping complex applications conceptualized as Multiagent
Systems (MASs). A very lively research sub-field studies how formal methods can
be used for AOSE. This paper presents a detailed survey of six logic-based
executable agent specification languages that have been chosen for their
potential to be integrated in our ARPEGGIO project  an open framework for
specifying and prototyping a MAS. The six languages are ConGoLog  Agent-0  the
IMPACT agent programming language  DyLog  Concurrent METATEM and Ehhf. For each
executable language  the logic foundations are described and an example of use
is shown. A comparison of the six languages and a survey of similar approaches
complete the paper  together with considerations of the advantages of using
logic-based languages in MAS modeling and prototyping."
"We propose a generalization of expected utility that we call generalized EU
(GEU)  where a decision maker's beliefs are represented by plausibility
measures  and the decision maker's tastes are represented by general (i.e. not
necessarily real-valued) utility functions. We show that every agent 
``rational'' or not  can be modeled as a GEU maximizer. We then show that we
can customize GEU by selectively imposing just the constraints we want. In
particular  we show how each of Savage's postulates corresponds to constraints
on GEU."
"Many different rules for decision making have been introduced in the
literature. We show that a notion of generalized expected utility proposed in
Part I of this paper is a universal decision rule  in the sense that it can
represent essentially all other decision rules."
"This paper describes a novel approach to grammar induction that has been
developed within a framework designed to integrate learning with other aspects
of computing  AI  mathematics and logic. This framework  called ""information
compression by multiple alignment  unification and search"" (ICMAUS)  is founded
on principles of Minimum Length Encoding pioneered by Solomonoff and others.
Most of the paper describes SP70  a computer model of the ICMAUS framework that
incorporates processes for unsupervised learning of grammars. An example is
presented to show how the model can infer a plausible grammar from appropriate
input. Limitations of the current model and how they may be overcome are
briefly discussed."
"We consider the integration of existing cone-shaped and projection-based
calculi of cardinal direction relations  well-known in QSR. The more general 
integrating language we consider is based on convex constraints of the
qualitative form $r(x y)$  $r$ being a cone-shaped or projection-based cardinal
direction atomic relation  or of the quantitative form $(\alpha  \beta)(x y)$ 
with $\alpha  \beta\in [0 2\pi)$ and $(\beta -\alpha)\in [0 \pi ]$. the meaning
of the quantitative constraint  in particular  is that point $x$ belongs to the
(convex) cone-shaped area rooted at $y$  and bounded by angles $\alpha$ and
$\beta$. The general form of a constraint is a disjunction of the form
$[r_1\vee...\vee r_{n_1}\vee (\alpha_1 \beta_1)\vee...\vee (\alpha
_{n_2} \beta_{n_2})](x y)$  with $r_i(x y)$  $i=1... n_1$  and $(\alpha
_i \beta_i)(x y)$  $i=1... n_2$  being convex constraints as described above.
the meaning of such a general constraint is that  for some $i=1... n_1$ 
$r_i(x y)$ holds  or  for some $i=1... n_2$  $(\alpha_i \beta_i)(x y)$ holds. A
conjunction of such general constraints is a $\tcsp$-like CSP  which we will
refer to as an $\scsp$ (Spatial Constraint Satisfaction Problem). An effective
solution search algorithm for an $\scsp$ will be described  which uses (1)
constraint propagation  based on a composition operation to be defined  as the
filtering method during the search  and (2) the Simplex algorithm  guaranteeing
completeness  at the leaves of the search tree. The approach is particularly
suited for large-scale high-level vision  such as  e.g.  satellite-like
surveillance of a geographic area."
"Object oriented constraint programs (OOCPs) emerge as a leading evolution of
constraint programming and artificial intelligence  first applied to a range of
industrial applications called configuration problems. The rich variety of
technical approaches to solving configuration problems (CLP(FD)  CC(FD)  DCSP 
Terminological systems  constraint programs with set variables ...) is a source
of difficulty. No universally accepted formal language exists for communicating
about OOCPs  which makes the comparison of systems difficult. We present here a
Z based specification of OOCPs which avoids the falltrap of hidden object
semantics. The object system is part of the specification  and captures all of
the most advanced notions from the object oriented modeling standard UML. The
paper illustrates these issues and the conciseness and precision of Z by the
specification of a working OOCP that solves an historical AI problem . parsing
a context free grammar. Being written in Z  an OOCP specification also supports
formal proofs. The whole builds the foundation of an adaptative and evolving
framework for communicating about constrained object models and programs."
"In this paper we suggest an architecture for a software agent which operates
a physical device and is capable of making observations and of testing and
repairing the device's components. We present simplified definitions of the
notions of symptom  candidate diagnosis  and diagnosis which are based on the
theory of action language ${\cal AL}$. The definitions allow one to give a
simple account of the agent's behavior in which many of the agent's tasks are
reduced to computing stable models of logic programs."
"We compare two recent extensions of the answer set (stable model) semantics
of logic programs. One of them  due to Lifschitz  Tang and Turner  allows the
bodies and heads of rules to contain nested expressions. The other  due to
Niemela and Simons  uses weight constraints. We show that there is a simple 
modular translation from the language of weight constraints into the language
of nested expressions that preserves the program's answer sets. Nested
expressions can be eliminated from the result of this translation in favor of
additional atoms. The translation makes it possible to compute answer sets for
some programs with weight constraints using satisfiability solvers  and to
prove the strong equivalence of programs with weight constraints using the
logic of here-and there."
"(We apologize for pidgin LaTeX) Schlipf \cite{sch91} proved that Stable Logic
Programming (SLP) solves all $\mathit{NP}$ decision problems. We extend
Schlipf's result to prove that SLP solves all search problems in the class
$\mathit{NP}$. Moreover  we do this in a uniform way as defined in \cite{mt99}.
Specifically  we show that there is a single $\mathrm{DATALOG}^{\neg}$ program
$P_{\mathit{Trg}}$ such that given any Turing machine $M$  any polynomial $p$
with non-negative integer coefficients and any input $\sigma$ of size $n$ over
a fixed alphabet $\Sigma$  there is an extensional database
$\mathit{edb}_{M p \sigma}$ such that there is a one-to-one correspondence
between the stable models of $\mathit{edb}_{M p \sigma} \cup P_{\mathit{Trg}}$
and the accepting computations of the machine $M$ that reach the final state in
at most $p(n)$ steps. Moreover  $\mathit{edb}_{M p \sigma}$ can be computed in
polynomial time from $p$  $\sigma$ and the description of $M$ and the decoding
of such accepting computations from its corresponding stable model of
$\mathit{edb}_{M p \sigma} \cup P_{\mathit{Trg}}$ can be computed in linear
time. A similar statement holds for Default Logic with respect to
$\Sigma_2^\mathrm{P}$-search problems\footnote{The proof of this result
involves additional technical complications and will be a subject of another
publication.}."
"This book develops the conjecture that all kinds of information processing in
computers and in brains may usefully be understood as ""information compression
by multiple alignment  unification and search"". This ""SP theory""  which has
been under development since 1987  provides a unified view of such things as
the workings of a universal Turing machine  the nature of 'knowledge'  the
interpretation and production of natural language  pattern recognition and
best-match information retrieval  several kinds of probabilistic reasoning 
planning and problem solving  unsupervised learning  and a range of concepts in
mathematics and logic. The theory also provides a basis for the design of an
'SP' computer with several potential advantages compared with traditional
digital computers."
"In rule-based systems  goal-oriented computations correspond naturally to the
possible ways that an observation may be explained. In some applications  we
need to compute explanations for a series of observations with the same domain.
The question whether previously computed answers can be recycled arises. A yes
answer could result in substantial savings of repeated computations. For
systems based on classic logic  the answer is YES. For nonmonotonic systems
however  one tends to believe that the answer should be NO  since recycling is
a form of adding information. In this paper  we show that computed answers can
always be recycled  in a nontrivial way  for the class of rewrite procedures
that we proposed earlier for logic programs with negation. We present some
experimental results on an encoding of the logistics domain."
"Recent advances in programming languages study and design have established a
standard way of grounding computational systems representation in category
theory. These formal results led to a better understanding of issues of control
and side-effects in functional and imperative languages. This framework can be
successfully applied to the investigation of the performance of Artificial
Intelligence (AI) inference and cognitive systems. In this paper  we delineate
a categorical formalisation of memory as a control structure driving
performance in inference systems. Abstracting away control mechanisms from
three widely used representations of memory in cognitive systems (scripts 
production rules and clusters) we explain how categorical triples capture the
interaction between learning and problem-solving."
"The field of machine learning (ML) is concerned with the question of how to
construct algorithms that automatically improve with experience. In recent
years many successful ML applications have been developed  such as datamining
programs  information-filtering systems  etc. Although ML algorithms allow the
detection and extraction of interesting patterns of data for several kinds of
problems  most of these algorithms are based on quantitative reasoning  as they
rely on training data in order to infer so-called target functions.
  In the last years defeasible argumentation has proven to be a sound setting
to formalize common-sense qualitative reasoning. This approach can be combined
with other inference techniques  such as those provided by machine learning
theory.
  In this paper we outline different alternatives for combining defeasible
argumentation and machine learning techniques. We suggest how different aspects
of a generic argument-based framework can be integrated with other ML-based
approaches."
"Stable model semantics has become a very popular approach for the management
of negation in logic programming. This approach relies mainly on the closed
world assumption to complete the available knowledge and its formulation has
its basis in the so-called Gelfond-Lifschitz transformation.
  The primary goal of this work is to present an alternative and
epistemic-based characterization of stable model semantics  to the
Gelfond-Lifschitz transformation. In particular  we show that stable model
semantics can be defined entirely as an extension of the Kripke-Kleene
semantics. Indeed  we show that the closed world assumption can be seen as an
additional source of `falsehood' to be added cumulatively to the Kripke-Kleene
semantics. Our approach is purely algebraic and can abstract from the
particular formalism of choice as it is based on monotone operators (under the
knowledge order) over bilattices only."
"We address the problem of the development of representations and their
relationship to the environment. We study a software agent which develops in a
network a representation of its simple environment which captures and
integrates the relationships between agent and environment through a closure
mechanism. The inclusion of a variable behavior modifier allows better
representation development. This can be confirmed with an internal description
of the closure mechanism  and with an external description of the properties of
the representation network."
"This document describes syntax  semantics and implementation guidelines in
order to enrich the DLV system with the possibility to make external C function
calls. This feature is realized by the introduction of parametric external
predicates  whose extension is not specified through a logic program but
implicitly computed through external code."
"This document describes the functions as they are treated in the DLV system.
We give first the language  then specify the main implementation issues."
"We introduce Ak  an extension of the action description language A (Gelfond
and Lifschitz  1993) to handle actions which affect knowledge. We use sensing
actions to increase an agent's knowledge of the world and non-deterministic
actions to remove knowledge. We include complex plans involving conditionals
and loops in our query language for hypothetical reasoning. We also present a
translation of Ak domain descriptions into epistemic logic programs."
"In this paper  we examine the performance of four fuzzy rule generation
methods on Wisconsin breast cancer data. The first method generates fuzzy if
then rules using the mean and the standard deviation of attribute values. The
second approach generates fuzzy if then rules using the histogram of attributes
values. The third procedure generates fuzzy if then rules with certainty of
each attribute into homogeneous fuzzy sets. In the fourth approach  only
overlapping areas are partitioned. The first two approaches generate a single
fuzzy if then rule for each class by specifying the membership function of each
antecedent fuzzy set using the information about attribute values of training
patterns. The other two approaches are based on fuzzy grids with homogeneous
fuzzy partitions of each attribute. The performance of each approach is
evaluated on breast cancer data sets. Simulation results show that the Modified
grid approach has a high classification rate of 99.73 %."
"The integration of different learning and adaptation techniques to overcome
individual limitations and to achieve synergetic effects through the
hybridization or fusion of these techniques has  in recent years  contributed
to a large number of new intelligent system designs. Computational intelligence
is an innovative framework for constructing intelligent hybrid architectures
involving Neural Networks (NN)  Fuzzy Inference Systems (FIS)  Probabilistic
Reasoning (PR) and derivative free optimization techniques such as Evolutionary
Computation (EC). Most of these hybridization approaches  however  follow an ad
hoc design methodology  justified by success in certain application domains.
Due to the lack of a common framework it often remains difficult to compare the
various hybrid systems conceptually and to evaluate their performance
comparatively. This chapter introduces the different generic architectures for
integrating intelligent systems. The designing aspects and perspectives of
different hybrid archirectures like NN-FIS  EC-FIS  EC-NN  FIS-PR and NN-FIS-EC
systems are presented. Some conclusions are also provided towards the end."
"Neuro-fuzzy systems have attracted growing interest of researchers in various
scientific and engineering areas due to the increasing need of intelligent
systems. This paper evaluates the use of two popular soft computing techniques
and conventional statistical approach based on Box--Jenkins autoregressive
integrated moving average (ARIMA) model to predict electricity demand in the
State of Victoria  Australia. The soft computing methods considered are an
evolving fuzzy neural network (EFuNN) and an artificial neural network (ANN)
trained using scaled conjugate gradient algorithm (CGA) and backpropagation
(BP) algorithm. The forecast accuracy is compared with the forecasts used by
Victorian Power Exchange (VPX) and the actual energy demand. To evaluate  we
considered load demand patterns for 10 consecutive months taken every 30 min
for training the different prediction models. Test results show that the
neuro-fuzzy system performed better than neural networks  ARIMA model and the
VPX forecasts."
"Fusion of Artificial Neural Networks (ANN) and Fuzzy Inference Systems (FIS)
have attracted the growing interest of researchers in various scientific and
engineering areas due to the growing need of adaptive intelligent systems to
solve the real world problems. ANN learns from scratch by adjusting the
interconnections between layers. FIS is a popular computing framework based on
the concept of fuzzy set theory  fuzzy if-then rules  and fuzzy reasoning. The
advantages of a combination of ANN and FIS are obvious. There are several
approaches to integrate ANN and FIS and very often it depends on the
application. We broadly classify the integration of ANN and FIS into three
categories namely concurrent model  cooperative model and fully fused model.
This paper starts with a discussion of the features of each model and
generalize the advantages and deficiencies of each model. We further focus the
review on the different types of fused neuro-fuzzy systems and citing the
advantages and disadvantages of each model."
"Long-term rainfall prediction is a challenging task especially in the modern
world where we are facing the major environmental problem of global warming. In
general  climate and rainfall are highly non-linear phenomena in nature
exhibiting what is known as the butterfly effect. While some regions of the
world are noticing a systematic decrease in annual rainfall  others notice
increases in flooding and severe storms. The global nature of this phenomenon
is very complicated and requires sophisticated computer modeling and simulation
to predict accurately. In this paper  we report a performance analysis for
Multivariate Adaptive Regression Splines (MARS)and artificial neural networks
for one month ahead prediction of rainfall. To evaluate the prediction
efficiency  we made use of 87 years of rainfall data in Kerala state  the
southern part of the Indian peninsula situated at latitude -longitude pairs
(8o29'N - 76o57' E). We used an artificial neural network trained using the
scaled conjugate gradient algorithm. The neural network and MARS were trained
with 40 years of rainfall data. For performance evaluation  network predicted
outputs were compared with the actual rainfall data. Simulation results reveal
that MARS is a good forecasting tool and performed better than the considered
neural network."
"Classification of texture pattern is one of the most important problems in
pattern recognition. In this paper  we present a classification method based on
the Discrete Cosine Transform (DCT) coefficients of texture image. As DCT works
on gray level image  the color scheme of each image is transformed into gray
levels. For classifying the images using DCT we used two popular soft computing
techniques namely neurocomputing and neuro-fuzzy computing. We used a
feedforward neural network trained using the backpropagation learning and an
evolving fuzzy neural network to classify the textures. The soft computing
models were trained using 80% of the texture data and remaining was used for
testing and validation purposes. A performance comparison was made among the
soft computing models for the texture classification problem. We also analyzed
the effects of prolonged training of neural networks. It is observed that the
proposed neuro-fuzzy model performed better than neural network."
"Sorting by reversals is an important problem in inferring the evolutionary
relationship between two genomes. The problem of sorting unsigned permutation
has been proven to be NP-hard. The best guaranteed error bounded is the 3/2-
approximation algorithm. However  the problem of sorting signed permutation can
be solved easily. Fast algorithms have been developed both for finding the
sorting sequence and finding the reversal distance of signed permutation. In
this paper  we present a way to view the problem of sorting unsigned
permutation as signed permutation. And the problem can then be seen as
searching an optimal signed permutation in all n2 corresponding signed
permutations. We use genetic algorithm to conduct the search. Our experimental
result shows that the proposed method outperform the 3/2-approximation
algorithm."
"Past few years have witnessed a growing recognition of intelligent techniques
for the construction of efficient and reliable intrusion detection systems. Due
to increasing incidents of cyber attacks  building effective intrusion
detection systems (IDS) are essential for protecting information systems
security  and yet it remains an elusive goal and a great challenge. In this
paper  we report a performance analysis between Multivariate Adaptive
Regression Splines (MARS)  neural networks and support vector machines. The
MARS procedure builds flexible regression models by fitting separate splines to
distinct intervals of the predictor variables. A brief comparison of different
neural network learning algorithms is also given."
"The aim of our research was to apply well-known data mining techniques (such
as linear neural networks  multi-layered perceptrons  probabilistic neural
networks  classification and regression trees  support vector machines and
finally a hybrid decision tree neural network approach) to the problem of
predicting the quality of service in call centers; based on the performance
data actually collected in a call center of a large insurance company. Our aim
was two-fold. First  to compare the performance of models built using the
above-mentioned techniques and  second  to analyze the characteristics of the
input sensitivity in order to better understand the relationship between the
perform-ance evaluation process and the actual performance and in this way help
improve the performance of call centers. In this paper we summarize our
findings."
"The use of intelligent systems for stock market predictions has been widely
established. In this paper  we investigate how the seemingly chaotic behavior
of stock markets could be well represented using several connectionist
paradigms and soft computing techniques. To demonstrate the different
techniques  we considered Nasdaq-100 index of Nasdaq Stock MarketS and the S&P
CNX NIFTY stock index. We analyzed 7 year's Nasdaq 100 main index values and 4
year's NIFTY index values. This paper investigates the development of a
reliable and efficient technique to model the seemingly chaotic behavior of
stock markets. We considered an artificial neural network trained using
Levenberg-Marquardt algorithm  Support Vector Machine (SVM)  Takagi-Sugeno
neuro-fuzzy model and a Difference Boosting Neural Network (DBNN). This paper
briefly explains how the different connectionist paradigms could be formulated
using different learning methods and then investigates whether they can provide
the required level of performance  which are sufficiently good and robust so as
to provide a reliable forecast model for stock market indices. Experiment
results reveal that all the connectionist paradigms considered could represent
the stock indices behavior very accurately."
"The purpose of this paper is to point to the usefulness of applying a linear
mathematical formulation of fuzzy multiple criteria objective decision methods
in organising business activities. In this respect fuzzy parameters of linear
programming are modelled by preference-based membership functions. This paper
begins with an introduction and some related research followed by some
fundamentals of fuzzy set theory and technical concepts of fuzzy multiple
objective decision models. Further a real case study of a manufacturing plant
and the implementation of the proposed technique is presented. Empirical
results clearly show the superiority of the fuzzy technique in optimising
individual objective functions when compared to non-fuzzy approach.
Furthermore  for the problem considered  the optimal solution helps to infer
that by incorporating fuzziness in a linear programming model either in
constraints  or both in objective functions and constraints  provides a similar
(or even better) level of satisfaction for obtained results compared to
non-fuzzy linear programming."
"In this paper  we present MLEANN (Meta-Learning Evolutionary Artificial
Neural Network)  an automatic computational framework for the adaptive
optimization of artificial neural networks wherein the neural network
architecture  activation function  connection weights; learning algorithm and
its parameters are adapted according to the problem. We explored the
performance of MLEANN and conventionally designed artificial neural networks
for function approximation problems. To evaluate the comparative performance 
we used three different well-known chaotic time series. We also present the
state of the art popular neural network learning algorithms and some
experimentation results related to convergence speed and generalization
performance. We explored the performance of backpropagation algorithm;
conjugate gradient algorithm  quasi-Newton algorithm and Levenberg-Marquardt
algorithm for the three chaotic time series. Performances of the different
learning algorithms were evaluated when the activation functions and
architecture were changed. We further present the theoretical background 
algorithm  design strategy and further demonstrate how effective and inevitable
is the proposed MLEANN framework to design a neural network  which is smaller 
faster and with a better generalization performance."
"The phylogenetic tree construction is to infer the evolutionary relationship
between species from the experimental data. However  the experimental data are
often imperfect and conflicting each others. Therefore  it is important to
extract the motif from the imperfect data. The largest compatible subset
problem is that  given a set of experimental data  we want to discard the
minimum such that the remaining is compatible. The largest compatible subset
problem can be viewed as the vertex cover problem in the graph theory that has
been proven to be NP-hard. In this paper  we propose a hybrid Evolutionary
Computing (EC) method for this problem. The proposed method combines the EC
approach and the algorithmic approach for special structured graphs. As a
result  the complexity of the problem is dramatically reduced. Experiments were
performed on randomly generated graphs with different edge densities. The
vertex covers produced by the proposed method were then compared to the vertex
covers produced by a 2-approximation algorithm. The experimental results showed
that the proposed method consistently outperformed a classical 2- approximation
algorithm. Furthermore  a significant improvement was found when the graph
density was small."
"Decision-making is a process of choosing among alternative courses of action
for solving complicated problems where multi-criteria objectives are involved.
The past few years have witnessed a growing recognition of Soft Computing
technologies that underlie the conception  design and utilization of
intelligent systems. Several works have been done where engineers and
scientists have applied intelligent techniques and heuristics to obtain optimal
decisions from imprecise information. In this paper  we present a concurrent
fuzzy-neural network approach combining unsupervised and supervised learning
techniques to develop the Tactical Air Combat Decision Support System (TACDSS).
Experiment results clearly demonstrate the efficiency of the proposed
technique."
"In a universe with a single currency  there would be no foreign exchange
market  no foreign exchange rates  and no foreign exchange. Over the past
twenty-five years  the way the market has performed those tasks has changed
enormously. The need for intelligent monitoring systems has become a necessity
to keep track of the complex forex market. The vast currency market is a
foreign concept to the average individual. However  once it is broken down into
simple terms  the average individual can begin to understand the foreign
exchange market and use it as a financial instrument for future investing. In
this paper  we attempt to compare the performance of hybrid soft computing and
hard computing techniques to predict the average monthly forex rates one month
ahead. The soft computing models considered are a neural network trained by the
scaled conjugate gradient algorithm and a neuro-fuzzy model implementing a
Takagi-Sugeno fuzzy inference system. We also considered Multivariate Adaptive
Regression Splines (MARS)  Classification and Regression Trees (CART) and a
hybrid CART-MARS technique. We considered the exchange rates of Australian
dollar with respect to US dollar  Singapore dollar  New Zealand dollar 
Japanese yen and United Kingdom pounds. The models were trained using 70% of
the data and remaining was used for testing and validation purposes. It is
observed that the proposed hybrid models could predict the forex rates more
accurately than all the techniques when applied individually. Empirical results
also reveal that the hybrid hard computing approach also improved some of our
previous work using a neuro-fuzzy approach."
"The rapid e-commerce growth has made both business community and customers
face a new situation. Due to intense competition on one hand and the customer's
option to choose from several alternatives business community has realized the
necessity of intelligent marketing strategies and relationship management. Web
usage mining attempts to discover useful knowledge from the secondary data
obtained from the interactions of the users with the Web. Web usage mining has
become very critical for effective Web site management  creating adaptive Web
sites  business and support services  personalization  network traffic flow
analysis and so on. In this paper  we present the important concepts of Web
usage mining and its various practical applications. We further present a novel
approach 'intelligent-miner' (i-Miner) to optimize the concurrent architecture
of a fuzzy clustering algorithm (to discover web data clusters) and a fuzzy
inference system to analyze the Web site visitor trends. A hybrid evolutionary
fuzzy clustering algorithm is proposed in this paper to optimally segregate
similar user interests. The clustered data is then used to analyze the trends
using a Takagi-Sugeno fuzzy inference system learned using a combination of
evolutionary algorithm and neural network learning. Proposed approach is
compared with self-organizing maps (to discover patterns) and several function
approximation techniques like neural networks  linear genetic programming and
Takagi-Sugeno fuzzy inference system (to analyze the clusters). The results are
graphically illustrated and the practical significance is discussed in detail.
Empirical results clearly show that the proposed Web usage-mining framework is
efficient."
"Normally a decision support system is build to solve problem where
multi-criteria decisions are involved. The knowledge base is the vital part of
the decision support containing the information or data that is used in
decision-making process. This is the field where engineers and scientists have
applied several intelligent techniques and heuristics to obtain optimal
decisions from imprecise information. In this paper  we present a hybrid
neuro-genetic learning approach for the adaptation a Mamdani fuzzy inference
system for the Tactical Air Combat Decision Support System (TACDSS). Some
simulation results demonstrating the difference of the learning techniques and
are also provided."
"Several adaptation techniques have been investigated to optimize fuzzy
inference systems. Neural network learning algorithms have been used to
determine the parameters of fuzzy inference system. Such models are often
called as integrated neuro-fuzzy models. In an integrated neuro-fuzzy model
there is no guarantee that the neural network learning algorithm converges and
the tuning of fuzzy inference system will be successful. Success of
evolutionary search procedures for optimization of fuzzy inference system is
well proven and established in many application areas. In this paper  we will
explore how the optimization of fuzzy inference systems could be further
improved using a meta-heuristic approach combining neural network learning and
evolutionary computation. The proposed technique could be considered as a
methodology to integrate neural networks  fuzzy inference systems and
evolutionary search procedures. We present the theoretical frameworks and some
experimental results to demonstrate the efficiency of the proposed technique."
"Evolutionary artificial neural networks (EANNs) refer to a special class of
artificial neural networks (ANNs) in which evolution is another fundamental
form of adaptation in addition to learning. Evolutionary algorithms are used to
adapt the connection weights  network architecture and learning algorithms
according to the problem environment. Even though evolutionary algorithms are
well known as efficient global search algorithms  very often they miss the best
local solutions in the complex solution space. In this paper  we propose a
hybrid meta-heuristic learning approach combining evolutionary learning and
local search methods (using 1st and 2nd order error information) to improve the
learning and faster convergence obtained using a direct evolutionary approach.
The proposed technique is tested on three different chaotic time series and the
test results are compared with some popular neuro-fuzzy systems and a recently
developed cutting angle method of global optimization. Empirical results reveal
that the proposed technique is efficient in spite of the computational
complexity."
"The academic literature suggests that the extent of exporting by
multinational corporation subsidiaries (MCS) depends on their product
manufactured  resources  tax protection  customers and markets  involvement
strategy  financial independence and suppliers' relationship with a
multinational corporation (MNC). The aim of this paper is to model the complex
export pattern behaviour using a Takagi-Sugeno fuzzy inference system in order
to determine the actual volume of MCS export output (sales exported). The
proposed fuzzy inference system is optimised by using neural network learning
and evolutionary computation. Empirical results clearly show that the proposed
approach could model the export behaviour reasonable well compared to a direct
neural network approach."
"The costs of fatalities and injuries due to traffic accident have a great
impact on society. This paper presents our research to model the severity of
injury resulting from traffic accidents using artificial neural networks and
decision trees. We have applied them to an actual data set obtained from the
National Automotive Sampling System (NASS) General Estimates System (GES).
Experiment results reveal that in all the cases the decision tree outperforms
the neural network. Our research analysis also shows that the three most
important factors in fatal injury are. driver's seat belt usage  light
condition of the roadway  and driver's alcohol usage."
"This paper presents a comparative study of six soft computing models namely
multilayer perceptron networks  Elman recurrent neural network  radial basis
function network  Hopfield model  fuzzy inference system and hybrid fuzzy
neural network for the hourly electricity demand forecast of Czech Republic.
The soft computing models were trained and tested using the actual hourly load
data for seven years. A comparison of the proposed techniques is presented for
predicting 2 day ahead demands for electricity. Simulation results indicate
that hybrid fuzzy neural network and radial basis function networks are the
best candidates for the analysis and forecasting of electricity demand."
"Decision-making is a process of choosing among alternative courses of action
for solving complicated problems where multi-criteria objectives are involved.
The past few years have witnessed a growing recognition of Soft Computing (SC)
technologies that underlie the conception  design and utilization of
intelligent systems. In this paper  we present different SC paradigms involving
an artificial neural network trained using the scaled conjugate gradient
algorithm  two different fuzzy inference methods optimised using neural network
learning/evolutionary algorithms and regression trees for developing
intelligent decision support systems. We demonstrate the efficiency of the
different algorithms by developing a decision support system for a Tactical Air
Combat Environment (TACE). Some empirical comparisons between the different
algorithms are also provided."
"In this paper  we present a state-based regression function for planning
domains where an agent does not have complete information and may have sensing
actions. We consider binary domains and employ the 0-approximation [Son & Baral
2001] to define the regression function. In binary domains  the use of
0-approximation means using 3-valued states. Although planning using this
approach is incomplete with respect to the full semantics  we adopt it to have
a lower complexity. We prove the soundness and completeness of our regression
formulation with respect to the definition of progression. More specifically 
we show that (i) a plan obtained through regression for a planning problem is
indeed a progression solution of that planning problem  and that (ii) for each
plan found through progression  using regression one obtains that plan or an
equivalent one. We then develop a conditional planner that utilizes our
regression function. We prove the soundness and completeness of our planning
algorithm and present experimental results with respect to several well known
planning problems in the literature."
"Defeasible logic is a rule-based nonmonotonic logic  with both strict and
defeasible rules  and a priority relation on rules. We show that inference in
the propositional form of the logic can be performed in linear time. This
contrasts markedly with most other propositional nonmonotonic logics  in which
inference is intractable."
"Defeasible argumentation has experienced a considerable growth in AI in the
last decade. Theoretical results have been combined with development of
practical applications in AI & Law  Case-Based Reasoning and various
knowledge-based systems. However  the dialectical process associated with
inference is computationally expensive. This paper focuses on speeding up this
inference process by pruning the involved search space. Our approach is
twofold. On one hand  we identify distinguished literals for computing defeat.
On the other hand  we restrict ourselves to a subset of all possible
conflicting arguments by introducing dialectical constraints."
"Main purposes of the paper are followings. 1) To show examples of the
calculations in domain of QFT via ``derivative rules'' of an expert system; 2)
To consider advantages and disadvantage that technology of the calculations; 3)
To reflect about how one would develop new physical theories  what knowledge
would be useful in their investigations and how this problem can be connected
with designing an expert system."
"We will try to tackle both the theoretical and practical aspects of a very
important problem in chess programming as stated in the title of this article -
the issue of draw detection by move repetition. The standard approach that has
so far been employed in most chess programs is based on utilising positional
matrices in original and compressed format as well as on the implementation of
the so-called bitboard format.
  The new approach that we will be trying to introduce is based on using
variant strings generated by the search algorithm (searcher) during the tree
expansion in decision making. We hope to prove that this approach is more
efficient than the standard treatment of the issue  especially in positions
with few pieces (endgames). To illustrate what we have in mind a machine
language routine that implements our theoretical assumptions is attached. The
routine is part of the Axon chess program  developed by the authors. Axon  in
its current incarnation  plays chess at master strength (ca. 2400-2450 Elo 
based on both Axon vs computer programs and Axon vs human masters in over 3000
games altogether)."
"Learning to respond to voice-text input involves the subject's ability in
understanding the phonetic and text based contents and his/her ability to
communicate based on his/her experience. The neuro-cognitive facility of the
subject has to support two important domains in order to make the learning
process complete. In many cases  though the understanding is complete  the
response is partial. This is one valid reason why we need to support the
information from the subject with scalable techniques such as Natural Language
Processing (NLP) for abstraction of the contents from the output. This paper
explores the feasibility of using NLP modules interlaced with Neural Networks
to perform the required task in autogenic training related to medical
applications."
"Generalized evolutionary algorithm based on Tsallis canonical distribution is
proposed. The algorithm uses Tsallis generalized canonical distribution to
weigh the configurations for `selection' instead of Gibbs-Boltzmann
distribution. Our simulation results show that for an appropriate choice of
non-extensive index that is offered by Tsallis statistics  evolutionary
algorithms based on this generalization outperform algorithms based on
Gibbs-Boltzmann distribution."
"In this paper we present and evaluate a search strategy called Decomposition
Based Search (DBS) which is based on two steps. subproblem generation and
subproblem solution. The generation of subproblems is done through value
ranking and domain splitting. Subdomains are explored so as to generate 
according to the heuristic chosen  promising subproblems first.
  We show that two well known search strategies  Limited Discrepancy Search
(LDS) and Iterative Broadening (IB)  can be seen as special cases of DBS. First
we present a tuning of DBS that visits the same search nodes as IB  but avoids
restarts. Then we compare both theoretically and computationally DBS and LDS
using the same heuristic. We prove that DBS has a higher probability of being
successful than LDS on a comparable number of nodes  under realistic
assumptions. Experiments on a constraint satisfaction problem and an
optimization problem show that DBS is indeed very effective if compared to LDS."
"Solution techniques for Constraint Satisfaction and Optimisation Problems
often make use of backtrack search methods  exploiting variable and value
ordering heuristics. In this paper  we propose and analyse a very simple method
to apply in case the value ordering heuristic produces ties. postponing the
branching decision. To this end  we group together values in a tie  branch on
this sub-domain  and defer the decision among them to lower levels of the
search tree. We show theoretically and experimentally that this simple
modification can dramatically improve the efficiency of the search strategy.
Although in practise similar methods may have been applied already  to our
knowledge  no empirical or theoretical study has been proposed in the
literature to identify when and to what extent this strategy should be used."
"In this paper  we propose an effective search procedure that interleaves two
steps. subproblem generation and subproblem solution. We mainly focus on the
first part. It consists of a variable domain value ranking based on reduced
costs. Exploiting the ranking  we generate  in a Limited Discrepancy Search
tree  the most promising subproblems first. An interesting result is that
reduced costs provide a very precise ranking that allows to almost always find
the optimal solution in the first generated subproblem  even if its dimension
is significantly smaller than that of the original problem. Concerning the
proof of optimality  we exploit a way to increase the lower bound for
subproblems at higher discrepancies. We show experimental results on the TSP
and its time constrained variant to show the effectiveness of the proposed
approach  but the technique could be generalized for other problems."
"One proposes a first alternative rule of combination to WAO (Weighted Average
Operator) proposed recently by Josang  Daniel and Vannoorenberghe  called
Proportional Conflict Redistribution rule (denoted PCR1). PCR1 and WAO are
particular cases of WO (the Weighted Operator) because the conflicting mass is
redistributed with respect to some weighting factors. In this first PCR rule 
the proportionalization is done for each non-empty set with respect to the
non-zero sum of its corresponding mass matrix - instead of its mass column
average as in WAO  but the results are the same as Ph. Smets has pointed out.
Also  we extend WAO (which herein gives no solution) for the degenerate case
when all column sums of all non-empty sets are zero  and then the conflicting
mass is transferred to the non-empty disjunctive form of all non-empty sets
together; but if this disjunctive form happens to be empty  then one considers
an open world (i.e. the frame of discernment might contain new hypotheses) and
thus all conflicting mass is transferred to the empty set. In addition to WAO 
we propose a general formula for PCR1 (WAO for non-degenerate cases)."
"In this paper one proposes a simple algorithm of combining the fusion rules 
those rules which first use the conjunctive rule and then the transfer of
conflicting mass to the non-empty sets  in such a way that they gain the
property of associativity and fulfill the Markovian requirement for dynamic
fusion. Also  a new rule  SDL-improved  is presented."
"FLUX is a programming method for the design of agents that reason logically
about their actions and sensor information in the presence of incomplete
knowledge. The core of FLUX is a system of Constraint Handling Rules  which
enables agents to maintain an internal model of their environment by which they
control their own behavior. The general action representation formalism of the
fluent calculus provides the formal semantics for the constraint solver. FLUX
exhibits excellent computational behavior due to both a carefully restricted
expressiveness and the inference paradigm of progression."
"Boltzmann selection is an important selection mechanism in evolutionary
algorithms as it has theoretical properties which help in theoretical analysis.
However  Boltzmann selection is not used in practice because a good annealing
schedule for the `inverse temperature' parameter is lacking. In this paper we
propose a Cauchy annealing schedule for Boltzmann selection scheme based on a
hypothesis that selection-strength should increase as evolutionary process goes
on and distance between two selection strengths should decrease for the process
to converge. To formalize these aspects  we develop formalism for selection
mechanisms using fitness distributions and give an appropriate measure for
selection-strength. In this paper  we prove an important result  by which we
derive an annealing schedule called Cauchy annealing schedule. We demonstrate
the novelty of proposed annealing schedule using simulations in the framework
of genetic algorithms."
"In this paper we propose five versions of a Proportional Conflict
Redistribution rule (PCR) for information fusion together with several
examples. From PCR1 to PCR2  PCR3  PCR4  PCR5 one increases the complexity of
the rules and also the exactitude of the redistribution of conflicting masses.
PCR1 restricted from the hyper-power set to the power set and without
degenerate cases gives the same result as the Weighted Average Operator (WAO)
proposed recently by J{\o}sang  Daniel and Vannoorenberghe but does not satisfy
the neutrality property of vacuous belief assignment. That's why improved PCR
rules are proposed in this paper. PCR4 is an improvement of minC and Dempster's
rules. The PCR rules redistribute the conflicting mass  after the conjunctive
rule has been applied  proportionally with some functions depending on the
masses assigned to their corresponding columns in the mass matrix. There are
infinitely many ways these functions (weighting factors) can be chosen
depending on the complexity one wants to deal with in specific applications and
fusion systems. Any fusion combination rule is at some degree ad-hoc."
"This paper presents in detail the generalized pignistic transformation (GPT)
succinctly developed in the Dezert-Smarandache Theory (DSmT) framework as a
tool for decision process. The GPT allows to provide a subjective probability
measure from any generalized basic belief assignment given by any corpus of
evidence. We mainly focus our presentation on the 3D case and provide the
complete result obtained by the GPT and its validation drawn from the
probability theory."
"Since no fusion theory neither rule fully satisfy all needed applications 
the author proposes a Unification of Fusion Theories and a combination of
fusion rules in solving problems/applications. For each particular application 
one selects the most appropriate model  rule(s)  and algorithm of
implementation. We are working in the unification of the fusion theories and
rules  which looks like a cooking recipe  better we'd say like a logical chart
for a computer programmer  but we don't see another method to comprise/unify
all things. The unification scenario presented herein  which is now in an
incipient form  should periodically be updated incorporating new discoveries
from the fusion and engineering research."
"Normal forms for logic programs under stable/answer set semantics are
introduced. We argue that these forms can simplify the study of program
properties  mainly consistency. The first normal form  called the {\em kernel}
of the program  is useful for studying existence and number of answer sets. A
kernel program is composed of the atoms which are undefined in the Well-founded
semantics  which are those that directly affect the existence of answer sets.
The body of rules is composed of negative literals only. Thus  the kernel form
tends to be significantly more compact than other formulations. Also  it is
possible to check consistency of kernel programs in terms of colorings of the
Extended Dependency Graph program representation which we previously developed.
The second normal form is called {\em 3-kernel.} A 3-kernel program is composed
of the atoms which are undefined in the Well-founded semantics. Rules in
3-kernel programs have at most two conditions  and each rule either belongs to
a cycle  or defines a connection between cycles. 3-kernel programs may have
positive conditions. The 3-kernel normal form is very useful for the static
analysis of program consistency  i.e.  the syntactic characterization of
existence of answer sets. This result can be obtained thanks to a novel
graph-like representation of programs  called Cycle Graph which presented in
the companion article \cite{Cos04b}."
"This paper may look like a glossary of the fusion rules and we also introduce
new ones presenting their formulas and examples. Conjunctive  Disjunctive 
Exclusive Disjunctive  Mixed Conjunctive-Disjunctive rules  Conditional rule 
Dempster's  Yager's  Smets' TBM rule  Dubois-Prade's  Dezert-Smarandache
classical and hybrid rules  Murphy's average rule 
Inagaki-Lefevre-Colot-Vannoorenberghe Unified Combination rules [and  as
particular cases. Iganaki's parameterized rule  Weighting Average Operator 
minC (M. Daniel)  and newly Proportional Conflict Redistribution rules
(Smarandache-Dezert) among which PCR5 is the most exact way of redistribution
of the conflicting mass to non-empty sets following the path of the conjunctive
rule]  Zhang's Center Combination rule  Convolutive x-Averaging  Consensus
Operator (Josang)  Cautious Rule (Smets)  ?-junctions rules (Smets)  etc. and
three new T-norm & T-conorm rules adjusted from fuzzy and neutrosophic sets to
information fusion (Tchamova-Smarandache). Introducing the degree of union and
degree of inclusion with respect to the cardinal of sets not with the fuzzy set
point of view  besides that of intersection  many fusion rules can be improved.
There are corner cases where each rule might have difficulties working or may
not get an expected result."
"There are many examples in the literature that suggest that
indistinguishability is intransitive  despite the fact that the
indistinguishability relation is typically taken to be an equivalence relation
(and thus transitive). It is shown that if the uncertainty perception and the
question of when an agent reports that two things are indistinguishable are
both carefully modeled  the problems disappear  and indistinguishability can
indeed be taken to be an equivalence relation. Moreover  this model also
suggests a logic of vagueness that seems to solve many of the problems related
to vagueness discussed in the philosophical literature. In particular  it is
shown here how the logic can handle the sorites paradox."
"A careful analysis of conditioning in the Sleeping Beauty problem is done 
using the formal model for reasoning about knowledge and probability developed
by Halpern and Tuttle. While the Sleeping Beauty problem has been viewed as
revealing problems with conditioning in the presence of imperfect recall  the
analysis done here reveals that the problems are not so much due to imperfect
recall as to asynchrony. The implications of this analysis for van Fraassen's
Reflection Principle and Savage's Sure-Thing Principle are considered."
"The paper is an attempt to generalize a methodology  which is similar to the
bounded-input bounded-output method currently widely used for the system
stability studies. The presented earlier methodology allows decomposition of
input space into bounded subspaces and defining for each subspace its bounding
surface. It also defines a corresponding predefined control  which maps any
point of a bounded input into a desired bounded output subspace. This
methodology was improved by providing a mechanism for the fast defining a
bounded surface. This paper presents enhanced bounded-input
bounded-predefined-control bounded-output approach  which provides adaptability
feature to the control and allows transferring of a controlled system along a
suboptimal trajectory."
"The number of probability distributions required to populate a conditional
probability table (CPT) in a Bayesian network  grows exponentially with the
number of parent-nodes associated with that table. If the table is to be
populated through knowledge elicited from a domain expert then the sheer
magnitude of the task forms a considerable cognitive barrier. In this paper we
devise an algorithm to populate the CPT while easing the extent of knowledge
acquisition. The input to the algorithm consists of a set of weights that
quantify the relative strengths of the influences of the parent-nodes on the
child-node  and a set of probability distributions the number of which grows
only linearly with the number of associated parent-nodes. These are elicited
from the domain expert. The set of probabilities are obtained by taking into
consideration the heuristics that experts use while arriving at probabilistic
estimations. The algorithm is used to populate the CPT by computing appropriate
weighted sums of the elicited distributions. We invoke the methods of
information geometry to demonstrate how these weighted sums capture the
expert's judgemental strategy."
"Consider the problem of tracking a set of moving targets. Apart from the
tracking result  it is often important to know where the tracking fails  either
to steer sensors to that part of the state-space  or to inform a human operator
about the status and quality of the obtained information. An intuitive quality
measure is the correlation between two tracking results based on uncorrelated
observations. In the case of Bayesian trackers such a correlation measure could
be the Kullback-Leibler difference.
  We focus on a scenario with a large number of military units moving in some
terrain. The units are observed by several types of sensors and ""meta-sensors""
with force aggregation capabilities. The sensors register units of different
size. Two separate multi-target probability hypothesis density (PHD) particle
filters are used to track some type of units (e.g.  companies) and their
sub-units (e.g.  platoons)  respectively  based on observations of units of
those sizes. Each observation is used in one filter only.
  Although the state-space may well be the same in both filters  the posterior
PHD distributions are not directly comparable -- one unit might correspond to
three or four spatially distributed sub-units. Therefore  we introduce a
mapping function between distributions for different unit size  based on
doctrine knowledge of unit configuration.
  The mapped distributions can now be compared -- locally or globally -- using
some measure  which gives the correlation between two PHD distributions in a
bounded volume of the state-space. To locate areas where the tracking fails  a
discretized quality map of the state-space can be generated by applying the
measure locally to different parts of the space."
"We describe the recently introduced extremal optimization algorithm and apply
it to target detection and association problems arising in pre-processing for
multi-target tracking.
  Here we consider the problem of pre-processing for multiple target tracking
when the number of sensor reports received is very large and arrives in large
bursts. In this case  it is sometimes necessary to pre-process reports before
sending them to tracking modules in the fusion system. The pre-processing step
associates reports to known tracks (or initializes new tracks for reports on
objects that have not been seen before). It could also be used as a pre-process
step before clustering  e.g.  in order to test how many clusters to use.
  The pre-processing is done by solving an approximate version of the original
problem. In this approximation  not all pair-wise conflicts are calculated. The
approximation relies on knowing how many such pair-wise conflicts that are
necessary to compute. To determine this  results on phase-transitions occurring
when coloring (or clustering) large random instances of a particular graph
ensemble are used."
"The management and combination of uncertain  imprecise  fuzzy and even
paradoxical or high conflicting sources of information has always been  and
still remains today  of primal importance for the development of reliable
modern information systems involving artificial reasoning. In this chapter  we
present a survey of our recent theory of plausible and paradoxical reasoning 
known as Dezert-Smarandache Theory (DSmT) in the literature  developed for
dealing with imprecise  uncertain and paradoxical sources of information. We
focus our presentation here rather on the foundations of DSmT  and on the two
important new rules of combination  than on browsing specific applications of
DSmT available in literature. Several simple examples are given throughout the
presentation to show the efficiency and the generality of this new approach.
The last part of this chapter concerns the presentation of the neutrosophic
logic  the neutro-fuzzy inference and its connection with DSmT. Fuzzy logic and
neutrosophic logic are useful tools in decision making after fusioning the
information using the DSm hybrid rule of combination of masses."
"In this paper  we propose a new method based on Hidden Markov Models to
interpret temporal sequences of sensor data from mobile robots to automatically
detect features. Hidden Markov Models have been used for a long time in pattern
recognition  especially in speech recognition. Their main advantages over other
methods (such as neural networks) are their ability to model noisy temporal
signals of variable length. We show in this paper that this approach is well
suited for interpretation of temporal sequences of mobile-robot sensor data. We
present two distinct experiments and results. the first one in an indoor
environment where a mobile robot learns to detect features like open doors or
T-intersections  the second one in an outdoor environment where a different
mobile robot has to identify situations like climbing a hill or crossing a
rock."
"In this paper  we present a rich semantic network based on a differential
analysis. We then detail implemented measures that take into account common and
differential features between words. In a last section  we describe some
industrial applications."
"Answer set programming (ASP) with disjunction offers a powerful tool for
declaratively representing and solving hard problems. Many NP-complete problems
can be encoded in the answer set semantics of logic programs in a very concise
and intuitive way  where the encoding reflects the typical ""guess and check""
nature of NP problems. The property is encoded in a way such that polynomial
size certificates for it correspond to stable models of a program. However  the
problem-solving capacity of full disjunctive logic programs (DLPs) is beyond
NP  and captures a class of problems at the second level of the polynomial
hierarchy. While these problems also have a clear ""guess and check"" structure 
finding an encoding in a DLP reflecting this structure may sometimes be a
non-obvious task  in particular if the ""check"" itself is a coNP-complete
problem; usually  such problems are solved by interleaving separate guess and
check programs  where the check is expressed by inconsistency of the check
program. In this paper  we present general transformations of head-cycle free
(extended) disjunctive logic programs into stratified and positive (extended)
disjunctive logic programs based on meta-interpretation techniques. The answer
sets of the original and the transformed program are in simple correspondence 
and  moreover  inconsistency of the original program is indicated by a
designated answer set of the transformed program. Our transformations
facilitate the integration of separate ""guess"" and ""check"" programs  which are
often easy to obtain  automatically into a single disjunctive logic program.
Our results complement recent results on meta-interpretation in ASP  and extend
methods and techniques for a declarative ""guess and check"" problem solving
paradigm through ASP."
"This paper presents an approach to enhance search engines with information
about word senses available in WordNet. The approach exploits information about
the conceptual relations within the lexical-semantic net. In the wrapper for
search engines presented  WordNet information is used to specify user's request
or to classify the results of a publicly available web search engine  like
google  yahoo  etc."
"This paper reports about experiments with GermaNet as a resource within
domain specific document analysis. The main question to be answered is. How is
the coverage of GermaNet in a specific domain? We report about results of a
field test of GermaNet for analyses of autopsy protocols and present a sketch
about the integration of GermaNet inside XDOC. Our remarks will contribute to a
GermaNet user's wish list."
"The aim of the project presented in this paper is to design a system for an
NLG architecture  which supports the documentation process of eBusiness models.
A major task is to enrich the formal description of an eBusiness model with
additional information needed in an NLG task."
"Lexical semantic resources  like WordNet  are often used in real applications
of natural language document processing. For example  we integrated GermaNet in
our document suite XDOC of processing of German forensic autopsy protocols. In
addition to the hypernymy and synonymy relation  we want to adapt GermaNet's
verb frames for our analysis. In this paper we outline an approach for the
domain related enrichment of GermaNet verb frames by corpus based syntactic and
co-occurred data analyses of real documents."
"Real applications of natural language document processing are very often
confronted with domain specific lexical gaps during the analysis of documents
of a new domain. This paper describes an approach for the derivation of domain
specific concepts for the extension of an existing ontology. As resources we
need an initial ontology and a partially processed corpus of a domain. We
exploit the specific characteristic of the sublanguage in the corpus. Our
approach is based on syntactical structures (noun phrases) and compound
analyses to extract information required for the extension of GermaNet's
lexical resources."
"We suggest to employ techniques from Natural Language Processing (NLP) and
Knowledge Representation (KR) to transform existing documents into documents
amenable for the Semantic Web. Semantic Web documents have at least part of
their semantics and pragmatics marked up explicitly in both a machine
processable as well as human readable manner. XML and its related standards
(XSLT  RDF  Topic Maps etc.) are the unifying platform for the tools and
methodologies developed for different application scenarios."
"This text introduces the twin deadlocks of strong artificial life.
Conceptualization of life is a deadlock both because of the existence of a
continuum between the inert and the living  and because we only know one
instance of life. Computationalism is a second deadlock since it remains a
matter of faith. Nevertheless  artificial life realizations quickly progress
and recent constructions embed an always growing set of the intuitive
properties of life. This growing gap between theory and realizations should
sooner or later crystallize in some kind of paradigm shift and then give clues
to break the twin deadlocks."
"In this chapter we describe new neural-network techniques developed for
visual mining clinical electroencephalograms (EEGs)  the weak electrical
potentials invoked by brain activity. These techniques exploit fruitful ideas
of Group Method of Data Handling (GMDH). Section 2 briefly describes the
standard neural-network techniques which are able to learn well-suited
classification modes from data presented by relevant features. Section 3
introduces an evolving cascade neural network technique which adds new input
nodes as well as new neurons to the network while the training error decreases.
This algorithm is applied to recognize artifacts in the clinical EEGs. Section
4 presents the GMDH-type polynomial networks learnt from data. We applied this
technique to distinguish the EEGs recorded from an Alzheimer and a healthy
patient as well as recognize EEG artifacts. Section 5 describes the new
neural-network technique developed to induce multi-class concepts from data. We
used this technique for inducing a 16-class concept from the large-scale
clinical EEG data. Finally we discuss perspectives of applying the
neural-network techniques to clinical EEGs."
"Bayesian averaging over classification models allows the uncertainty of
classification outcomes to be evaluated  which is of crucial importance for
making reliable decisions in applications such as financial in which risks have
to be estimated. The uncertainty of classification is determined by a trade-off
between the amount of data available for training  the diversity of a
classifier ensemble and the required performance. The interpretability of
classification models can also give useful information for experts responsible
for making reliable classifications. For this reason Decision Trees (DTs) seem
to be attractive classification models. The required diversity of the DT
ensemble can be achieved by using the Bayesian model averaging all possible
DTs. In practice  the Bayesian approach can be implemented on the base of a
Markov Chain Monte Carlo (MCMC) technique of random sampling from the posterior
distribution. For sampling large DTs  the MCMC method is extended by Reversible
Jump technique which allows inducing DTs under given priors. For the case when
the prior information on the DT size is unavailable  the sweeping technique
defining the prior implicitly reveals a better performance. Within this Chapter
we explore the classification uncertainty of the Bayesian MCMC techniques on
some datasets from the StatLog Repository and real financial data. The
classification uncertainty is compared within an Uncertainty Envelope technique
dealing with the class posterior distribution and a given confidence
probability. This technique provides realistic estimates of the classification
uncertainty which can be easily interpreted in statistical terms with the aim
of risk evaluation."
"Multiple Classifier Systems (MCSs) allow evaluation of the uncertainty of
classification outcomes that is of crucial importance for safety critical
applications. The uncertainty of classification is determined by a trade-off
between the amount of data available for training  the classifier diversity and
the required performance. The interpretability of MCSs can also give useful
information for experts responsible for making reliable classifications. For
this reason Decision Trees (DTs) seem to be attractive classification models
for experts. The required diversity of MCSs exploiting such classification
models can be achieved by using two techniques  the Bayesian model averaging
and the randomised DT ensemble. Both techniques have revealed promising results
when applied to real-world problems. In this paper we experimentally compare
the classification uncertainty of the Bayesian model averaging with a
restarting strategy and the randomised DT ensemble on a synthetic dataset and
some domain problems commonly used in the machine learning community. To make
the Bayesian DT averaging feasible  we use a Markov Chain Monte Carlo
technique. The classification uncertainty is evaluated within an Uncertainty
Envelope technique dealing with the class posterior distribution and a given
confidence probability. Exploring a full posterior distribution  this technique
produces realistic estimates which can be easily interpreted in statistical
terms. In our experiments we found out that the Bayesian DTs are superior to
the randomised DT ensembles within the Uncertainty Envelope technique."
"Artificial intelligence (AI) research has evolved over the last few decades
and knowledge acquisition research is at the core of AI research. PKAW-04 is
one of three international knowledge acquisition workshops held in the
Pacific-Rim  Canada and Europe over the last two decades. PKAW-04 has a strong
emphasis on incremental knowledge acquisition  machine learning  neural nets
and active mining.
  The proceedings contain 19 papers that were selected by the program committee
among 24 submitted papers. All papers were peer reviewed by at least two
reviewers. The papers in these proceedings cover the methods and tools as well
as the applications related to develop expert systems or knowledge based
systems."
"In the frame of designing a knowledge discovery system  we have developed
stochastic models based on high-order hidden Markov models. These models are
capable to map sequences of data into a Markov chain in which the transitions
between the states depend on the \texttt{n} previous states according to the
order of the model. We study the process of achieving information extraction
fromspatial and temporal data by means of an unsupervised classification. We
use therefore a French national database related to the land use of a region 
named Teruti  which describes the land use both in the spatial and temporal
domain. Land-use categories (wheat  corn  forest  ...) are logged every year on
each site regularly spaced in the region. They constitute a temporal sequence
of images in which we look for spatial and temporal dependencies. The temporal
segmentation of the data is done by means of a second-order Hidden Markov Model
(\hmmd) that appears to have very good capabilities to locate stationary
segments  as shown in our previous work in speech recognition. Thespatial
classification is performed by defining a fractal scanning ofthe images with
the help of a Hilbert-Peano curve that introduces atotal order on the sites 
preserving the relation ofneighborhood between the sites. We show that the
\hmmd performs aclassification that is meaningful for the agronomists.Spatial
and temporal classification may be achieved simultaneously by means of a 2
levels \hmmd that measures the \aposteriori probability to map a temporal
sequence of images onto a set of hidden classes."
"Our ongoing work aims at defining an ontology-centered approach for building
expertise models for the CommonKADS methodology. This approach (which we have
named ""OntoKADS"") is founded on a core problem-solving ontology which
distinguishes between two conceptualization levels. at an object level  a set
of concepts enable us to define classes of problem-solving situations  and at a
meta level  a set of meta-concepts represent modeling primitives. In this
article  our presentation of OntoKADS will focus on the core ontology and  in
particular  on roles - the primitive situated at the interface between domain
knowledge and reasoning  and whose ontological status is still much debated. We
first propose a coherent  global  ontological framework which enables us to
account for this primitive. We then show how this novel characterization of the
primitive allows definition of new rules for the construction of expertise
models."
"Automatic or assisted workflow composition is a field of intense research for
applications to the world wide web or to business process modeling. Workflow
composition is traditionally addressed in various ways  generally via theorem
proving techniques. Recent research observed that building a composite workflow
bears strong relationships with finite model search  and that some workflow
languages can be defined as constrained object metamodels . This lead to
consider the viability of applying configuration techniques to this problem 
which was proven feasible. Constrained based configuration expects a
constrained object model as input. The purpose of this document is to formally
specify the constrained object model involved in ongoing experiments and
research using the Z specification language."
"To the reduct problems of decision system  the paper proposes the notion of
dynamic core according to the dynamic reduct model. It describes various formal
definitions of dynamic core  and discusses some properties about dynamic core.
All of these show that dynamic core possesses the essential characters of the
feature core."
"Correlated time series are time series that  by virtue of the underlying
process to which they refer  are expected to influence each other strongly. We
introduce a novel approach to handle such time series  one that models their
interaction as a two-dimensional cellular automaton and therefore allows them
to be treated as a single entity. We apply our approach to the problems of
filling gaps and predicting values in rainfall time series. Computational
results show that the new approach compares favorably to Kalman smoothing and
filtering."
"ATNoSFERES is a Pittsburgh style Learning Classifier System (LCS) in which
the rules are represented as edges of an Augmented Transition Network.
Genotypes are strings of tokens of a stack-based language  whose execution
builds the labeled graph. The original ATNoSFERES  using a bitstring to
represent the language tokens  has been favorably compared in previous work to
several Michigan style LCSs architectures in the context of Non Markov
problems. Several modifications of ATNoSFERES are proposed here. the most
important one conceptually being a representational change. each token is now
represented by an integer  hence the genotype is a string of integers; several
other modifications of the underlying grammar language are also proposed. The
resulting ATNoSFERES-II is validated on several standard animat Non Markov
problems  on which it outperforms all previously published results in the LCS
literature. The reasons for these improvement are carefully analyzed  and some
assumptions are proposed on the underlying mechanisms in order to explain these
good results."
"We present a declarative language  PP  for the high-level specification of
preferences between possible solutions (or trajectories) of a planning problem.
This novel language allows users to elegantly express non-trivial 
multi-dimensional preferences and priorities over such preferences. The
semantics of PP allows the identification of most preferred trajectories for a
given goal. We also provide an answer set programming implementation of
planning problems with PP preferences."
"Clustering is a widely used technique in data mining applications for
discovering patterns in underlying data. Most traditional clustering algorithms
are limited to handling datasets that contain either numeric or categorical
attributes. However  datasets with mixed types of attributes are common in real
life data mining applications. In this paper  we propose a novel
divide-and-conquer technique to solve this problem. First  the original mixed
dataset is divided into two sub-datasets. the pure categorical dataset and the
pure numeric dataset. Next  existing well established clustering algorithms
designed for different types of datasets are employed to produce corresponding
clusters. Last  the clustering results on the categorical and numeric dataset
are combined as a categorical dataset  on which the categorical data clustering
algorithm is used to get the final clusters. Our contribution in this paper is
to provide an algorithm framework for the mixed attributes clustering problem 
in which existing clustering algorithms can be easily integrated  the
capabilities of different kinds of clustering algorithms and characteristics of
different types of datasets could be fully exploited. Comparisons with other
clustering algorithms on real life datasets illustrate the superiority of our
approach."
"Clustering categorical data is an integral part of data mining and has
attracted much attention recently. In this paper  we present k-histogram  a new
efficient algorithm for clustering categorical data. The k-histogram algorithm
extends the k-means algorithm to categorical domain by replacing the means of
clusters with histograms  and dynamically updates histograms in the clustering
process. Experimental results on real datasets show that k-histogram algorithm
can produce better clustering results than k-modes algorithm  the one related
with our work most closely."
"This report describes a new version of the OntoSpec methodology for ontology
building. Defined by the LaRIA Knowledge Engineering Team (University of
Picardie Jules Verne  Amiens  France)  OntoSpec aims at helping builders to
model ontological knowledge (upstream of formal representation). The
methodology relies on a set of rigorously-defined modelling primitives and
principles. Its application leads to the elaboration of a semi-informal
ontology  which is independent of knowledge representation languages. We
recently enriched the OntoSpec methodology by endowing it with a new resource 
the DOLCE top-level ontology defined at the LOA (IST-CNR  Trento  Italy). The
goal of this integration is to provide modellers with additional help in
structuring application ontologies  while maintaining independence
vis-\`{a}-vis formal representation languages. In this report  we first provide
an overview of the OntoSpec methodology's general principles and then describe
the DOLCE re-engineering process. A complete version of DOLCE-OS (i.e. a
specification of DOLCE in the semi-informal OntoSpec language) is presented in
an appendix."
"In this paper we present a new approach for marker less human motion capture
from conventional camera feeds. The aim of our study is to recover 3D positions
of key points of the body that can serve for gait analysis. Our approach is
based on foreground segmentation  an articulated body model and particle
filters. In order to be generic and simple no restrictive dynamic modelling was
used. A new modified particle filtering algorithm was introduced. It is used
efficiently to search the model configuration space. This new algorithm which
we call Interval Particle Filtering reorganizes the configurations search space
in an optimal deterministic way and proved to be efficient in tracking natural
human movement. Results for human motion capture from a single camera are
presented and compared to results obtained from a marker based system. The
system proved to be able to track motion successfully even in partial
occlusions."
"The aim of our study is to detect balance disorders and a tendency towards
the falls in the elderly  knowing gait parameters. In this paper we present a
new tool for gait analysis based on markerless human motion capture  from
camera feeds. The system introduced here  recovers the 3D positions of several
key points of the human body while walking. Foreground segmentation  an
articulated body model and particle filtering are basic elements of our
approach. No dynamic model is used thus this system can be described as generic
and simple to implement. A modified particle filtering algorithm  which we call
Interval Particle Filtering  is used to reorganise and search through the
model's configurations search space in a deterministic optimal way. This
algorithm was able to perform human movement tracking with success. Results
from the treatment of a single cam feeds are shown and compared to results
obtained using a marker based human motion capture system."
"An agent often has a number of hypotheses  and must choose among them based
on observations  or outcomes of experiments. Each of these observations can be
viewed as providing evidence for or against various hypotheses. All the
attempts to formalize this intuition up to now have assumed that associated
with each hypothesis h there is a likelihood function \mu_h  which is a
probability measure that intuitively describes how likely each observation is 
conditional on h being the correct hypothesis. We consider an extension of this
framework where there is uncertainty as to which of a number of likelihood
functions is appropriate  and discuss how one formal approach to defining
evidence  which views evidence as a function from priors to posteriors  can be
generalized to accommodate this uncertainty."
"Being able to analyze and interpret signal coming from electroencephalogram
(EEG) recording can be of high interest for many applications including medical
diagnosis and Brain-Computer Interfaces. Indeed  human experts are today able
to extract from this signal many hints related to physiological as well as
cognitive states of the recorded subject and it would be very interesting to
perform such task automatically but today no completely automatic system
exists. In previous studies  we have compared human expertise and automatic
processing tools  including artificial neural networks (ANN)  to better
understand the competences of each and determine which are the difficult
aspects to integrate in a fully automatic system. In this paper  we bring more
elements to that study in reporting the main results of a practical experiment
which was carried out in an hospital for sleep pathology study. An EEG
recording was studied and labeled by a human expert and an ANN. We describe
here the characteristics of the experiment  both human and neuronal procedure
of analysis  compare their performances and point out the main limitations
which arise from this study."
"Train timetabling is a difficult and very tightly constrained combinatorial
problem that deals with the construction of train schedules. We focus on the
particular problem of local reconstruction of the schedule following a small
perturbation  seeking minimisation of the total accumulated delay by adapting
times of departure and arrival for each train and allocation of resources
(tracks  routing nodes  etc.). We describe a permutation-based evolutionary
algorithm that relies on a semi-greedy heuristic to gradually reconstruct the
schedule by inserting trains one after the other following the permutation.
This algorithm can be hybridised with ILOG commercial MIP programming tool
CPLEX in a coarse-grained manner. the evolutionary part is used to quickly
obtain a good but suboptimal solution and this intermediate solution is refined
using CPLEX. Experimental results are presented on a large real-world case
involving more than one million variables and 2 million constraints. Results
are surprisingly good as the evolutionary algorithm  alone or hybridised 
produces excellent solutions much faster than CPLEX alone."
"Evolutionary computing (EC) is an exciting development in Computer Science.
It amounts to building  applying and studying algorithms based on the Darwinian
principles of natural selection. In this paper we briefly introduce the main
concepts behind evolutionary computing. We present the main components all
evolutionary algorithms (EA)  sketch the differences between different types of
EAs and survey application areas ranging from optimization  modeling and
simulation to entertainment."
This article is taken out.
"A fuzzy controller is usually designed by formulating the knowledge of a
human expert into a set of linguistic variables and fuzzy rules. Among the most
successful methods to automate the fuzzy controllers development process are
evolutionary algorithms. In this work  we propose the Recurrent Fuzzy Voronoi
(RFV) model  a representation for recurrent fuzzy systems. It is an extension
of the FV model proposed by Kavka and Schoenauer that extends the application
domain to include temporal problems. The FV model is a representation for fuzzy
controllers based on Voronoi diagrams that can represent fuzzy systems with
synergistic rules  fulfilling the $\epsilon$-completeness property and
providing a simple way to introduce a priory knowledge. In the proposed
representation  the temporal relations are embedded by including internal units
that provide feedback by connecting outputs to inputs. These internal units act
as memory elements. In the RFV model  the semantic of the internal units can be
specified together with the a priori rules. The geometric interpretation of the
rules allows the use of geometric variational operators during the evolution.
The representation and the algorithms are validated in two problems in the area
of system identification and evolutionary robotics."
"When solving numerical constraints such as nonlinear equations and
inequalities  solvers often exploit pruning techniques  which remove redundant
value combinations from the domains of variables  at pruning steps. To find the
complete solution set  most of these solvers alternate the pruning steps with
branching steps  which split each problem into subproblems. This forms the
so-called branch-and-prune framework  well known among the approaches for
solving numerical constraints. The basic branch-and-prune search strategy that
uses domain bisections in place of the branching steps is called the bisection
search. In general  the bisection search works well in case (i) the solutions
are isolated  but it can be improved further in case (ii) there are continuums
of solutions (this often occurs when inequalities are involved). In this paper 
we propose a new branch-and-prune search strategy along with several variants 
which not only allow yielding better branching decisions in the latter case 
but also work as well as the bisection search does in the former case. These
new search algorithms enable us to employ various pruning techniques in the
construction of inner and outer approximations of the solution set. Our
experiments show that these algorithms speed up the solving process often by
one order of magnitude or more when solving problems with continuums of
solutions  while keeping the same performance as the bisection search when the
solutions are isolated."
"IS success is a complex concept  and its evaluation is complicated 
unstructured and not readily quantifiable. Numerous scientific publications
address the issue of success in the IS field as well as in other fields. But 
little efforts have been done for processing indeterminacy and uncertainty in
success research. This paper shows a formal method for mapping success using
Neutrosophic Success Map. This is an emerging tool for processing indeterminacy
and uncertainty in success research. EIS success have been analyzed using this
tool."
"In this paper  a mathematical schema theory is developed. This theory has
three roots. brain theory schemas  grid automata  and block-shemas. In Section
2 of this paper  elements of the theory of grid automata necessary for the
mathematical schema theory are presented. In Section 3  elements of brain
theory necessary for the mathematical schema theory are presented. In Section
4  other types of schemas are considered. In Section 5  the mathematical schema
theory is developed. The achieved level of schema representation allows one to
model by mathematical tools virtually any type of schemas considered before 
including schemas in neurophisiology  psychology  computer science  Internet
technology  databases  logic  and mathematics."
"Data-based classification is fundamental to most branches of science. While
recent years have brought enormous progress in various areas of statistical
computing and clustering  some general challenges in clustering remain. model
selection  robustness  and scalability to large datasets. We consider the
important problem of deciding on the optimal number of clusters  given an
arbitrary definition of space and clusteriness. We show how to construct a
cluster information criterion that allows objective model selection. Differing
from other approaches  our truecluster method does not require specific
assumptions about underlying distributions  dissimilarity definitions or
cluster models. Truecluster puts arbitrary clustering algorithms into a generic
unified (sampling-based) statistical framework. It is scalable to big datasets
and provides robust cluster assignments and case-wise diagnostics. Truecluster
will make clustering more objective  allows for automation  and will save time
and costs. Free R software is available."
"An original approach  termed Divide-and-Evolve is proposed to hybridize
Evolutionary Algorithms (EAs) with Operational Research (OR) methods in the
domain of Temporal Planning Problems (TPPs). Whereas standard Memetic
Algorithms use local search methods to improve the evolutionary solutions  and
thus fail when the local method stops working on the complete problem  the
Divide-and-Evolve approach splits the problem at hand into several  hopefully
easier  sub-problems  and can thus solve globally problems that are intractable
when directly fed into deterministic OR algorithms. But the most prominent
advantage of the Divide-and-Evolve approach is that it immediately opens up an
avenue for multi-objective optimization  even though the OR method that is used
is single-objective. Proof of concept approach on the standard
(single-objective) Zeno transportation benchmark is given  and a small original
multi-objective benchmark is proposed in the same Zeno framework to assess the
multi-objective capabilities of the proposed methodology  a breakthrough in
Temporal Planning."
"This article considers evidence from physical and biological sciences to show
machines are deficient compared to biological systems at incorporating
intelligence. Machines fall short on two counts. firstly  unlike brains 
machines do not self-organize in a recursive manner; secondly  machines are
based on classical logic  whereas Nature's intelligence may depend on quantum
mechanics."
"Constraint Programming (CP) has proved an effective paradigm to model and
solve difficult combinatorial satisfaction and optimisation problems from
disparate domains. Many such problems arising from the commercial world are
permeated by data uncertainty. Existing CP approaches that accommodate
uncertainty are less suited to uncertainty arising due to incomplete and
erroneous data  because they do not build reliable models and solutions
guaranteed to address the user's genuine problem as she perceives it. Other
fields such as reliable computation offer combinations of models and associated
methods to handle these types of uncertain data  but lack an expressive
framework characterising the resolution methodology independently of the model.
  We present a unifying framework that extends the CP formalism in both model
and solutions  to tackle ill-defined combinatorial problems with incomplete or
erroneous data. The certainty closure framework brings together modelling and
solving methodologies from different fields into the CP paradigm to provide
reliable and efficient approches for uncertain constraint problems. We
demonstrate the applicability of the framework on a case study in network
diagnosis. We define resolution forms that give generic templates  and their
associated operational semantics  to derive practical solution methods for
reliable solutions."
"The application of Genetic Programming to the discovery of empirical laws is
often impaired by the huge size of the search space  and consequently by the
computer resources needed. In many cases  the extreme demand for memory and CPU
is due to the massive growth of non-coding segments  the introns. The paper
presents a new program evolution framework which combines distribution-based
evolution in the PBIL spirit  with grammar-based genetic programming; the
information is stored as a probability distribution on the gra mmar rules 
rather than in a population. Experiments on a real-world like problem show that
this approach gives a practical solution to the problem of intron growth."
"This paper deals with the problem of classifying signals. The new method for
building so called local classifiers and local features is presented. The
method is a combination of the lifting scheme and the support vector machines.
Its main aim is to produce effective and yet comprehensible classifiers that
would help in understanding processes hidden behind classified signals. To
illustrate the method we present the results obtained on an artificial and a
real dataset."
"Open answer set programming (OASP) is an extension of answer set programming
where one may ground a program with an arbitrary superset of the program's
constants. We define a fixed point logic (FPL) extension of Clark's completion
such that open answer sets correspond to models of FPL formulas and identify a
syntactic subclass of programs  called (loosely) guarded programs. Whereas
reasoning with general programs in OASP is undecidable  the FPL translation of
(loosely) guarded programs falls in the decidable (loosely) guarded fixed point
logic (mu(L)GF). Moreover  we reduce normal closed ASP to loosely guarded OASP 
enabling for the first time  a characterization of an answer set semantics by
muLGF formulas. We further extend the open answer set semantics for programs
with generalized literals. Such generalized programs (gPs) have interesting
properties  e.g.  the ability to express infinity axioms. We restrict the
syntax of gPs such that both rules and generalized literals are guarded. Via a
translation to guarded fixed point logic  we deduce 2-exptime-completeness of
satisfiability checking in such guarded gPs (GgPs). Bound GgPs are restricted
GgPs with exptime-complete satisfiability checking  but still sufficiently
expressive to optimally simulate computation tree logic (CTL). We translate
Datalog lite programs to GgPs  establishing equivalence of GgPs under an open
answer set semantics  alternation-free muGF  and Datalog lite."
"Consistency check has been the only criterion for theory evaluation in
logic-based approaches to reasoning about actions. This work goes beyond that
and contributes to the metatheory of actions by investigating what other
properties a good domain description in reasoning about actions should have. We
state some metatheoretical postulates concerning this sore spot. When all
postulates are satisfied together we have a modular action theory. Besides
being easier to understand and more elaboration tolerant in McCarthy's sense 
modular theories have interesting properties. We point out the problems that
arise when the postulates about modularity are violated and propose algorithmic
checks that can help the designer of an action theory to overcome them."
"The estimation of linear causal models (also known as structural equation
models) from data is a well-known problem which has received much attention in
the past. Most previous work has  however  made an explicit or implicit
assumption of gaussianity  limiting the identifiability of the models. We have
recently shown (Shimizu et al  2005; Hoyer et al  2006) that for non-gaussian
distributions the full causal model can be estimated in the no hidden variables
case. In this contribution  we discuss the estimation of the model when
confounding latent variables are present. Although in this case uniqueness is
no longer guaranteed  there is at most a finite set of models which can fit the
data. We develop an algorithm for estimating this set  and describe numerical
simulations which confirm the theoretical arguments and demonstrate the
practical viability of the approach. Full Matlab code is provided for all
simulations."
"Shock physics experiments are often complicated and expensive. As a result 
researchers are unable to conduct as many experiments as they would like -
leading to sparse data sets. In this paper  Support Vector Machines for
regression are applied to velocimetry data sets for shock damaged and melted
tin metal. Some success at interpolating between data sets is achieved.
Implications for future work are discussed."
"In this paper  we study clustering with respect to the k-modes objective
function  a natural formulation of clustering for categorical data. One of the
main contributions of this paper is to establish the connection between k-modes
and k-median  i.e.  the optimum of k-median is at most twice the optimum of
k-modes for the same categorical data clustering problem. Based on this
observation  we derive a deterministic algorithm that achieves an approximation
factor of 2. Furthermore  we prove that the distance measure in k-modes defines
a metric. Hence  we are able to extend existing approximation algorithms for
metric k-median to k-modes. Empirical results verify the superiority of our
method."
"A model of an organism as an autonomous intelligent system has been proposed.
This model was used to analyze learning of an organism in various environmental
conditions. Processes of learning were divided into two types. strong and weak
processes taking place in the absence and the presence of aprioristic
information about an object respectively. Weak learning is synonymous to
adaptation when aprioristic programs already available in a system (an
organism) are started. It was shown that strong learning is impossible for both
an organism and any autonomous intelligent system. It was shown also that the
knowledge base of an organism cannot be updated. Therefore  all behavior
programs of an organism are congenital. A model of a conditioned reflex as a
series of consecutive measurements of environmental parameters has been
advanced. Repeated measurements are necessary in this case to reduce the error
during decision making."
"This paper presents two new promising rules of combination for the fusion of
uncertain and potentially highly conflicting sources of evidences in the
framework of the theory of belief functions in order to palliate the well-know
limitations of Dempster's rule and to work beyond the limits of applicability
of the Dempster-Shafer theory. We present both a new class of adaptive
combination rules (ACR) and a new efficient Proportional Conflict
Redistribution (PCR) rule allowing to deal with highly conflicting sources for
static and dynamic fusion applications."
"Fuzzy automata  whose input alphabet is a set of numbers or symbols  are a
formal model of computing with values. Motivated by Zadeh's paradigm of
computing with words rather than numbers  Ying proposed a kind of fuzzy
automata  whose input alphabet consists of all fuzzy subsets of a set of
symbols  as a formal model of computing with all words. In this paper  we
introduce a somewhat general formal model of computing with (some special)
words. The new features of the model are that the input alphabet only comprises
some (not necessarily all) fuzzy subsets of a set of symbols and the fuzzy
transition function can be specified arbitrarily. By employing the methodology
of fuzzy control  we establish a retraction principle from computing with words
to computing with values for handling crisp inputs and a generalized extension
principle from computing with words to computing with all words for handling
fuzzy inputs. These principles show that computing with values and computing
with all words can be respectively implemented by computing with words. Some
algebraic properties of retractions and generalized extensions are addressed as
well."
"Through the Internet and the World-Wide Web  a vast number of information
sources has become available  which offer information on various subjects by
different providers  often in heterogeneous formats. This calls for tools and
methods for building an advanced information-processing infrastructure. One
issue in this area is the selection of suitable information sources in query
answering. In this paper  we present a knowledge-based approach to this
problem  in the setting where one among a set of information sources
(prototypically  data repositories) should be selected for evaluating a user
query. We use extended logic programs (ELPs) to represent rich descriptions of
the information sources  an underlying domain theory  and user queries in a
formal query language (here  XML-QL  but other languages can be handled as
well). Moreover  we use ELPs for declarative query analysis and generation of a
query description. Central to our approach are declarative source-selection
programs  for which we define syntax and semantics. Due to the structured
nature of the considered data items  the semantics of such programs must
carefully respect implicit context information in source-selection rules  and
furthermore combine it with possible user preferences. A prototype
implementation of our approach has been realized exploiting the DLV KR system
and its plp front-end for prioritized ELPs. We describe a representative
example involving specific movie databases  and report about experimental
results."
"It is well known that perspective alignment plays a major role in the
planning and interpretation of spatial language. In order to understand the
role of perspective alignment and the cognitive processes involved  we have
made precise complete cognitive models of situated embodied agents that
self-organise a communication system for dialoging about the position and
movement of real world objects in their immediate surroundings. We show in a
series of robotic experiments which cognitive mechanisms are necessary and
sufficient to achieve successful spatial language and why and how perspective
alignment can take place  either implicitly or based on explicit marking."
"We extend the 0-approximation of sensing actions and incomplete information
in [Son and Baral 2000] to action theories with static causal laws and prove
its soundness with respect to the possible world semantics. We also show that
the conditional planning problem with respect to this approximation is
NP-complete. We then present an answer set programming based conditional
planner  called ASCP  that is capable of generating both conformant plans and
conditional plans in the presence of sensing actions  incomplete information
about the initial state  and static causal laws. We prove the correctness of
our implementation and argue that our planner is sound and complete with
respect to the proposed approximation. Finally  we present experimental results
comparing ASCP to other planners."
"Computing and storing probabilities is a hard problem as soon as one has to
deal with complex distributions over multiple random variables. The problem of
efficient representation of probability distributions is central in term of
computational efficiency in the field of probabilistic reasoning. The main
problem arises when dealing with joint probability distributions over a set of
random variables. they are always represented using huge probability arrays. In
this paper  a new method based on binary-tree representation is introduced in
order to store efficiently very large joint distributions. Our approach
approximates any multidimensional joint distributions using an adaptive
discretization of the space. We make the assumption that the lower is the
probability mass of a particular region of feature space  the larger is the
discretization step. This assumption leads to a very optimized representation
in term of time and memory. The other advantages of our approach are the
ability to refine dynamically the distribution every time it is needed leading
to a more accurate representation of the probability distribution and to an
anytime representation of the distribution."
"In order to more effectively cope with the real-world problems of vagueness 
{\it fuzzy discrete event systems} (FDESs) were proposed recently  and the
supervisory control theory of FDESs was developed. In view of the importance of
failure diagnosis  in this paper  we present an approach of the failure
diagnosis in the framework of FDESs. More specifically. (1) We formalize the
definition of diagnosability for FDESs  in which the observable set and failure
set of events are {\it fuzzy}  that is  each event has certain degree to be
observable and unobservable  and  also  each event may possess different
possibility of failure occurring. (2) Through the construction of
observability-based diagnosers of FDESs  we investigate its some basic
properties. In particular  we present a necessary and sufficient condition for
diagnosability of FDESs. (3) Some examples serving to illuminate the
applications of the diagnosability of FDESs are described. To conclude  some
related issues are raised for further consideration."
"Classification of ordinal data is one of the most important tasks of relation
learning. In this thesis a novel framework for ordered classes is proposed. The
technique reduces the problem of classifying ordered classes to the standard
two-class problem. The introduced method is then mapped into support vector
machines and neural networks. Compared with a well-known approach using
pairwise objects as training samples  the new algorithm has a reduced
complexity and training time. A second novel model  the unimodal model  is also
introduced and a parametric version is mapped into neural networks. Several
case studies are presented to assert the validity of the proposed models."
"Imagination is the critical point in developing of realistic artificial
intelligence (AI) systems. One way to approach imagination would be simulation
of its properties and operations. We developed two models. AI-Brain Network
Hierarchy of Languages and Semantical Holographic Calculus as well as
simulation system ScriptWriter that emulate the process of imagination through
an automatic animation of English texts. The purpose of this paper is to
demonstrate the model and to present ScriptWriter system
http.//nvo.sdsc.edu/NVO/JCSG/get_SRB_mime_file2.cgi//home/tamara.sdsc/test/demo.zip?F=/home/tamara.sdsc/test/demo.zip&M=application/x-gtar
for simulation of the imagination."
"In Dempster-Shafer belief theory  general beliefs are expressed as belief
mass distribution functions over frames of discernment. In Subjective Logic
beliefs are expressed as belief mass distribution functions over binary frames
of discernment. Belief representations in Subjective Logic  which are called
opinions  also contain a base rate parameter which express the a priori belief
in the absence of evidence. Philosophically  beliefs are quantitative
representations of evidence as perceived by humans or by other intelligent
agents. The basic operators of classical probability calculus  such as addition
and multiplication  can be applied to opinions  thereby making belief calculus
practical. Through the equivalence between opinions and Beta probability
density functions  this also provides a calculus for Beta probability density
functions. This article explains the basic elements of belief calculus."
"The problem of combining beliefs in the Dempster-Shafer belief theory has
attracted considerable attention over the last two decades. The classical
Dempster's Rule has often been criticised  and many alternative rules for
belief combination have been proposed in the literature. The consensus operator
for combining beliefs has nice properties and produces more intuitive results
than Dempster's rule  but has the limitation that it can only be applied to
belief distribution functions on binary state spaces. In this paper we present
a generalisation of the consensus operator that can be applied to Dirichlet
belief functions on state spaces of arbitrary size. This rule  called the
cumulative rule of belief combination  can be derived from classical
statistical theory  and corresponds well with human intuition."
"Artificial Intelligence (AI) has recently become a real formal science. the
new millennium brought the first mathematically sound  asymptotically optimal 
universal problem solvers  providing a new  rigorous foundation for the
previously largely heuristic field of General AI and embedded agents. At the
same time there has been rapid progress in practical methods for learning true
sequence-processing programs  as opposed to traditional methods limited to
stationary pattern association. Here we will briefly review some of the new
results  and speculate about future developments  pointing out that the time
intervals between the most notable events in over 40 000 years or 2^9 lifetimes
of human history have sped up exponentially  apparently converging to zero
within the next few decades. Or is this impression just a by-product of the way
humans allocate memory space to past events?"
"In this paper we propose a new family of Belief Conditioning Rules (BCRs) for
belief revision. These rules are not directly related with the fusion of
several sources of evidence but with the revision of a belief assignment
available at a given time according to the new truth (i.e. conditioning
constraint) one has about the space of solutions of the problem."
"In this note we introduce the notion of islands for restricting local search.
We show how we can construct islands for CNF SAT problems  and how much search
space can be eliminated by restricting search to the island."
"Knowing the norms of a domain is crucial  but there exist no repository of
norms. We propose a method to extract them from texts. texts generally do not
describe a norm  but rather how a state-of-affairs differs from it. Answers
concerning the cause of the state-of-affairs described often reveal the
implicit norm. We apply this idea to the domain of driving  and validate it by
designing algorithms that identify  in a text  the ""basic"" norms to which it
refers implicitly."
"Norms are essential to extend inference. inferences based on norms are far
richer than those based on logical implications. In the recent decades  much
effort has been devoted to reason on a domain  once its norms are represented.
How to extract and express those norms has received far less attention.
Extraction is difficult. as the readers are supposed to know them  the norms of
a domain are seldom made explicit. For one thing  extracting norms requires a
language to represent them  and this is the topic of this paper. We apply this
language to represent norms in the domain of driving  and show that it is
adequate to reason on the causes of accidents  as described by car-crash
reports."
"In this paper we consider and analyze the behavior of two combinational rules
for temporal (sequential) attribute data fusion for target type estimation. Our
comparative analysis is based on Dempster's fusion rule proposed in
Dempster-Shafer Theory (DST) and on the Proportional Conflict Redistribution
rule no. 5 (PCR5) recently proposed in Dezert-Smarandache Theory (DSmT). We
show through very simple scenario and Monte-Carlo simulation  how PCR5 allows a
very efficient Target Type Tracking and reduces drastically the latency delay
for correct Target Type decision with respect to Demspter's rule. For cases
presenting some short Target Type switches  Demspter's rule is proved to be
unable to detect the switches and thus to track correctly the Target Type
changes. The approach proposed here is totally new  efficient and promising to
be incorporated in real-time Generalized Data Association - Multi Target
Tracking systems (GDA-MTT) and provides an important result on the behavior of
PCR5 with respect to Dempster's rule. The MatLab source code is provided in"
"This paper introduces the notion of qualitative belief assignment to model
beliefs of human experts expressed in natural language (with linguistic
labels). We show how qualitative beliefs can be efficiently combined using an
extension of Dezert-Smarandache Theory (DSmT) of plausible and paradoxical
quantitative reasoning to qualitative reasoning. We propose a new arithmetic on
linguistic labels which allows a direct extension of classical DSm fusion rule
or DSm Hybrid rules. An approximate qualitative PCR5 rule is also proposed
jointly with a Qualitative Average Operator. We also show how crisp or interval
mappings can be used to deal indirectly with linguistic labels. A very simple
example is provided to illustrate our qualitative fusion rules."
"The management and combination of uncertain  imprecise  fuzzy and even
paradoxical or high conflicting sources of information has always been  and
still remains today  of primal importance for the development of reliable
modern information systems involving artificial reasoning. In this
introduction  we present a survey of our recent theory of plausible and
paradoxical reasoning  known as Dezert-Smarandache Theory (DSmT) in the
literature  developed for dealing with imprecise  uncertain and paradoxical
sources of information. We focus our presentation here rather on the
foundations of DSmT  and on the two important new rules of combination  than on
browsing specific applications of DSmT available in literature. Several simple
examples are given throughout the presentation to show the efficiency and the
generality of this new approach."
"We study an alternative to the prevailing approach to modelling qualitative
spatial reasoning (QSR) problems as constraint satisfaction problems. In the
standard approach  a relation between objects is a constraint whereas in the
alternative approach it is a variable. The relation-variable approach greatly
simplifies integration and implementation of QSR. To substantiate this point 
we discuss several QSR algorithms from the literature which in the
relation-variable approach reduce to the customary constraint propagation
algorithm enforcing generalised arc-consistency."
"I explore the use of sets of probability measures as a representation of
uncertainty."
"We present a state-based regression function for planning domains where an
agent does not have complete information and may have sensing actions. We
consider binary domains and employ a three-valued characterization of domains
with sensing actions to define the regression function. We prove the soundness
and completeness of our regression formulation with respect to the definition
of progression. More specifically  we show that (i) a plan obtained through
regression for a planning problem is indeed a progression solution of that
planning problem  and that (ii) for each plan found through progression  using
regression one obtains that plan or an equivalent one."
"A modification of OWL-S regarding parameter description is proposed. It is
strictly based on Description Logic. In addition to class description of
parameters it also allows the modelling of relations between parameters and the
precise description of the size of data to be supplied to a service. In
particular  it solves two major issues identified within current proposals for
a Semantic Web Service annotation standard."
"The paper describes the ALVIS annotation format designed for the indexing of
large collections of documents in topic-specific search engines. This paper is
exemplified on the biological domain and on MedLine abstracts  as developing a
specialized search engine for biologists is one of the ALVIS case studies. The
ALVIS principle for linguistic annotations is based on existing works and
standard propositions. We made the choice of stand-off annotations rather than
inserted mark-up. Annotations are encoded as XML elements which form the
linguistic subsection of the document record."
"The aim of this paper is to provide a sound framework for addressing a
difficult problem. the automatic construction of an autonomous agent's modular
architecture. We combine results from two apparently uncorrelated domains.
Autonomous planning through Markov Decision Processes and a General Data
Clustering Approach using a kernel-like method. Our fundamental idea is that
the former is a good framework for addressing autonomy whereas the latter
allows to tackle self-organizing problems."
"In this paper we elaborate on a specific application in the context of hybrid
description logic programs (hybrid DLPs)  namely description logic Semantic Web
type systems (DL-types) which are used for term typing of LP rules based on a
polymorphic  order-sorted  hybrid DL-typed unification as procedural semantics
of hybrid DLPs. Using Semantic Web ontologies as type systems facilitates
interchange of domain-independent rules over domain boundaries via dynamically
typing and mapping of explicitly defined type ontologies."
"In this paper we describe an architecture of a system that answer the
question . Why did the accident happen? from the textual description of an
accident. We present briefly the different parts of the architecture and then
we describe with more detail the semantic part of the system i.e. the part in
which the norm-based reasoning is performed on the explicit knowlege extracted
from the text."
"We develop a system which must be able to perform the same inferences that a
human reader of an accident report can do and more particularly to determine
the apparent causes of the accident. We describe the general framework in which
we are situated  linguistic and semantic levels of the analysis and the
inference rules used by the system."
"The k-modes algorithm has become a popular technique in solving categorical
data clustering problems in different application domains. However  the
algorithm requires random selection of initial points for the clusters.
Different initial points often lead to considerable distinct clustering
results. In this paper we present an experimental study on applying a
farthest-point heuristic based initialization method to k-modes clustering to
improve its performance. Experiments show that new initialization method leads
to better clustering accuracy than random selection initialization method for
k-modes clustering."
"The opening book is an important component of a chess engine  and thus
computer chess programmers have been developing automated methods to improve
the quality of their books. For chess  which has a very rich opening theory 
large databases of high-quality games can be used as the basis of an opening
book  from which statistics relating to move choices from given positions can
be collected. In order to find out whether the opening books used by modern
chess engines in machine versus machine competitions are ``comparable'' to
those used by chess players in human versus human competitions  we carried out
analysis on 26 test positions using statistics from two opening books one
compiled from humans' games and the other from machines' games. Our analysis
using several nonparametric measures  shows that  overall  there is a strong
association between humans' and machines' choices of opening moves when using a
book to guide their choices."
"We present a new local approximation algorithm for computing Maximum a
Posteriori (MAP) and log-partition function for arbitrary exponential family
distribution represented by a finite-valued pair-wise Markov random field
(MRF)  say $G$. Our algorithm is based on decomposition of $G$ into {\em
appropriately} chosen small components; then computing estimates locally in
each of these components and then producing a {\em good} global solution. We
show that if the underlying graph $G$ either excludes some finite-sized graph
as its minor (e.g. Planar graph) or has low doubling dimension (e.g. any graph
with {\em geometry})  then our algorithm will produce solution for both
questions within {\em arbitrary accuracy}. We present a message-passing
implementation of our algorithm for MAP computation using self-avoiding walk of
graph. In order to evaluate the computational cost of this implementation  we
derive novel tight bounds on the size of self-avoiding walk tree for arbitrary
graph.
  As a consequence of our algorithmic result  we show that the normalized
log-partition function (also known as free-energy) for a class of {\em regular}
MRFs will converge to a limit  that is computable to an arbitrary accuracy."
"Creation procedure of associative patterns ensemble in terms of formal logic
with using neural net-work (NN) model is formulated. It is shown that the
associative patterns set is created by means of unique procedure of NN work
which having individual parameters of entrance stimulus transformation. It is
ascer-tained that the quantity of the selected associative patterns possesses
is a constant."
"In case-based reasoning  the adaptation step depends in general on
domain-dependent knowledge  which motivates studies on adaptation knowledge
acquisition (AKA). CABAMAKA is an AKA system based on principles of knowledge
discovery from databases. This system explores the variations within the case
base to elicit adaptation knowledge. It has been successfully tested in an
application of case-based decision support to breast cancer treatment."
"Recently  the diagnosability of {\it stochastic discrete event systems}
(SDESs) was investigated in the literature  and  the failure diagnosis
considered was {\it centralized}. In this paper  we propose an approach to {\it
decentralized} failure diagnosis of SDESs  where the stochastic system uses
multiple local diagnosers to detect failures and each local diagnoser possesses
its own information. In a way  the centralized failure diagnosis of SDESs can
be viewed as a special case of the decentralized failure diagnosis presented in
this paper with only one projection. The main contributions are as follows. (1)
We formalize the notion of codiagnosability for stochastic automata  which
means that a failure can be detected by at least one local stochastic diagnoser
within a finite delay. (2) We construct a codiagnoser from a given stochastic
automaton with multiple projections  and the codiagnoser associated with the
local diagnosers is used to test codiagnosability condition of SDESs. (3) We
deal with a number of basic properties of the codiagnoser. In particular  a
necessary and sufficient condition for the codiagnosability of SDESs is
presented. (4) We give a computing method in detail to check whether
codiagnosability is violated. And (5) some examples are described to illustrate
the applications of the codiagnosability and its computing method."
"The management and combination of uncertain  imprecise  fuzzy and even
paradoxical or high conflicting sources of information has always been and
still remains of primal importance for the development of reliable information
fusion systems. In this short survey paper  we present the theory of plausible
and paradoxical reasoning  known as DSmT (Dezert-Smarandache Theory) in
literature  developed for dealing with imprecise  uncertain and potentially
highly conflicting sources of information. DSmT is a new paradigm shift for
information fusion and recent publications have shown the interest and the
potential ability of DSmT to solve fusion problems where Dempster's rule used
in Dempster-Shafer Theory (DST) provides counter-intuitive results or fails to
provide useful result at all. This paper is focused on the foundations of DSmT
and on its main rules of combination (classic  hybrid and Proportional Conflict
Redistribution rules). Shafer's model on which is based DST appears as a
particular and specific case of DSm hybrid model which can be easily handled by
DSmT as well. Several simple but illustrative examples are given throughout
this paper to show the interest and the generality of this new theory."
"Reaction RuleML is a general  practical  compact and user-friendly
XML-serialized language for the family of reaction rules. In this white paper
we give a review of the history of event / action /state processing and
reaction rule approaches and systems in different domains  define basic
concepts and give a classification of the event  action  state processing and
reasoning space as well as a discussion of relevant / related work"
"A fuzzy logic based classification engine has been developed for classifying
mass spectra obtained with an imaging internal source Fourier transform mass
spectrometer (I^2LD-FTMS). Traditionally  an operator uses the relative
abundance of ions with specific mass-to-charge (m/z) ratios to categorize
spectra. An operator does this by comparing the spectrum of m/z versus
abundance of an unknown sample against a library of spectra from known samples.
Automated positioning and acquisition allow I^2LD-FTMS to acquire data from
very large grids  this would require classification of up to 3600 spectrum per
hour to keep pace with the acquisition. The tedious job of classifying numerous
spectra generated in an I^2LD-FTMS imaging application can be replaced by a
fuzzy rule base if the cues an operator uses can be encapsulated. We present
the translation of linguistic rules to a fuzzy classifier for mineral phases in
basalt. This paper also describes a method for gathering statistics on ions 
which are not currently used in the rule base  but which may be candidates for
making the rule base more accurate and complete or to form new rule bases based
on data obtained from known samples. A spatial method for classifying spectra
with low membership values  based on neighboring sample classifications  is
also presented."
"Description Logics (DLs) are appropriate  widely used  logics for managing
structured knowledge. They allow reasoning about individuals and concepts  i.e.
set of individuals with common properties. Typically  DLs are limited to
dealing with crisp  well defined concepts. That is  concepts for which the
problem whether an individual is an instance of it is yes/no question. More
often than not  the concepts encountered in the real world do not have a
precisely defined criteria of membership. we may say that an individual is an
instance of a concept only to a certain degree  depending on the individual's
properties. The DLs that deal with such fuzzy concepts are called fuzzy DLs. In
order to deal with fuzzy  incomplete  indeterminate and inconsistent concepts 
we need to extend the fuzzy DLs  combining the neutrosophic logic with a
classical DL. In particular  concepts become neutrosophic (here neutrosophic
means fuzzy  incomplete  indeterminate  and inconsistent)  thus reasoning about
neutrosophic concepts is supported. We'll define its syntax  its semantics  and
describe its properties."
"Support Vector Machines (SVMs) are well-established Machine Learning (ML)
algorithms. They rely on the fact that i) linear learning can be formalized as
a well-posed optimization problem; ii) non-linear learning can be brought into
linear learning thanks to the kernel trick and the mapping of the initial
search space onto a high dimensional feature space. The kernel is designed by
the ML expert and it governs the efficiency of the SVM approach. In this paper 
a new approach for the automatic design of kernels by Genetic Programming 
called the Evolutionary Kernel Machine (EKM)  is presented. EKM combines a
well-founded fitness function inspired from the margin criterion  and a
co-evolution framework ensuring the computational scalability of the approach.
Empirical validation on standard ML benchmark demonstrates that EKM is
competitive using state-of-the-art SVMs with tuned hyper-parameters."
"Functional brain imaging is a source of spatio-temporal data mining problems.
A new framework hybridizing multi-objective and multi-modal optimization is
proposed to formalize these data mining problems  and addressed through
Evolutionary Computation (EC). The merits of EC for spatio-temporal data mining
are demonstrated as the approach facilitates the modelling of the experts'
requirements  and flexibly accommodates their changing goals."
"The paper suggests the use of Multi-Valued Decision Diagrams (MDDs) as the
supporting data structure for a generic global constraint. We give an algorithm
for maintaining generalized arc consistency (GAC) on this constraint that
amortizes the cost of the GAC computation over a root-to-terminal path in the
search tree. The technique used is an extension of the GAC algorithm for the
regular language constraint on finite length input. Our approach adds support
for skipped variables  maintains the reduced property of the MDD dynamically
and provides domain entailment detection. Finally we also show how to adapt the
approach to constraint types that are closely related to MDDs  such as AOMDDs
and Case DAGs."
"Did natural consciousness and intelligent systems arise out of a path that
was co-evolutionary to evolution? Can we explain human self-consciousness as
having risen out of such an evolutionary path? If so how could it have been?
  In this first part of a two-part paper (titled IXI)  we take a learning
system perspective to the problem of consciousness and intelligent systems  an
approach that may look unseasonable in this age of fMRI's and high tech
neuroscience.
  We posit conscious intelligent systems in natural environments and wonder how
natural factors influence their design paths. Such a perspective allows us to
explain seamlessly a variety of natural factors  factors ranging from the rise
and presence of the human mind  man's sense of I  his self-consciousness and
his looping thought processes to factors like reproduction  incubation 
extinction  sleep  the richness of natural behavior  etc. It even allows us to
speculate on a possible human evolution scenario and other natural phenomena."
"This is the second part of a paper on Conscious Intelligent Systems. We use
the understanding gained in the first part (Conscious Intelligent Systems Part
1. IXI (arxiv id cs.AI/0612056)) to look at understanding. We see how the
presence of mind affects understanding and intelligent systems; we see that the
presence of mind necessitates language. The rise of language in turn has
important effects on understanding. We discuss the humanoid question and how
the question of self-consciousness (and by association mind/thought/language)
would affect humanoids too."
"A product configurator which is complete  backtrack free and able to compute
the valid domains at any state of the configuration can be constructed by
building a Binary Decision Diagram (BDD). Despite the fact that the size of the
BDD is exponential in the number of variables in the worst case  BDDs have
proved to work very well in practice. Current BDD-based techniques can only
handle interactive configuration with small finite domains. In this paper we
extend the approach to handle string variables constrained by regular
expressions. The user is allowed to change the strings by adding letters at the
end of the string. We show how to make a data structure that can perform fast
valid domain computations given some assignment on the set of string variables.
  We first show how to do this by using one large DFA. Since this approach is
too space consuming to be of practical use  we construct a data structure that
simulates the large DFA and in most practical cases are much more space
efficient. As an example a configuration problem on $n$ string variables with
only one solution in which each string variable is assigned to a value of
length of $k$ the former structure will use $\Omega(k^n)$ space whereas the
latter only need $O(kn)$. We also show how this framework easily can be
combined with the recent BDD techniques to allow both boolean  integer and
string variables in the configuration problem."
"Recently  M. Chertkov and V.Y. Chernyak derived an exact expression for the
partition sum (normalization constant) corresponding to a graphical model 
which is an expansion around the Belief Propagation solution. By adding
correction terms to the BP free energy  one for each ""generalized loop"" in the
factor graph  the exact partition sum is obtained. However  the usually
enormous number of generalized loops generally prohibits summation over all
correction terms. In this article we introduce Truncated Loop Series BP
(TLSBP)  a particular way of truncating the loop series of M. Chertkov and V.Y.
Chernyak by considering generalized loops as compositions of simple loops. We
analyze the performance of TLSBP in different scenarios  including the Ising
model  regular random graphs and on Promedas  a large probabilistic medical
diagnostic system. We show that TLSBP often improves upon the accuracy of the
BP solution  at the expense of increased computation time. We also show that
the performance of TLSBP strongly depends on the degree of interaction between
the variables. For weak interactions  truncating the series leads to
significant improvements  whereas for strong interactions it can be
ineffective  even if a high number of terms is considered."
"In this paper  the traditional k-modes clustering algorithm is extended by
weighting attribute value matches in dissimilarity computation. The use of
attribute value weighting technique makes it possible to generate clusters with
stronger intra-similarities  and therefore achieve better clustering
performance. Experimental results on real life datasets show that these value
weighting based k-modes algorithms are superior to the standard k-modes
algorithm with respect to clustering accuracy."
"In Verification and in (optimal) AI Planning  a successful method is to
formulate the application as boolean satisfiability (SAT)  and solve it with
state-of-the-art DPLL-based procedures. There is a lack of understanding of why
this works so well. Focussing on the Planning context  we identify a form of
problem structure concerned with the symmetrical or asymmetrical nature of the
cost of achieving the individual planning goals. We quantify this sort of
structure with a simple numeric parameter called AsymRatio  ranging between 0
and 1. We run experiments in 10 benchmark domains from the International
Planning Competitions since 2000; we show that AsymRatio is a good indicator of
SAT solver performance in 8 of these domains. We then examine carefully crafted
synthetic planning domains that allow control of the amount of structure  and
that are clean enough for a rigorous analysis of the combinatorial search
space. The domains are parameterized by size  and by the amount of structure.
The CNFs we examine are unsatisfiable  encoding one planning step less than the
length of the optimal plan. We prove upper and lower bounds on the size of the
best possible DPLL refutations  under different settings of the amount of
structure  as a function of size. We also identify the best possible sets of
branching variables (backdoors). With minimum AsymRatio  we prove exponential
lower bounds  and identify minimal backdoors of size linear in the number of
variables. With maximum AsymRatio  we identify logarithmic DPLL refutations
(and backdoors)  showing a doubly exponential gap between the two structural
extreme cases. The reasons for this behavior -- the proof arguments --
illuminate the prototypical patterns of structure causing the empirical
behavior observed in the competition benchmarks."
"This short paper introduces two new fusion rules for combining quantitative
basic belief assignments. These rules although very simple have not been
proposed in literature so far and could serve as useful alternatives because of
their low computation cost with respect to the recent advanced Proportional
Conflict Redistribution rules developed in the DSmT framework."
"Constraint Programming (CP) has been successfully applied to both constraint
satisfaction and constraint optimization problems. A wide variety of
specialized global constraints provide critical assistance in achieving a good
model that can take advantage of the structure of the problem in the search for
a solution. However  a key outstanding issue is the representation of 'ad-hoc'
constraints that do not have an inherent combinatorial nature  and hence are
not modeled well using narrowly specialized global constraints. We attempt to
address this issue by considering a hybrid of search and compilation.
Specifically we suggest the use of Reduced Ordered Multi-Valued Decision
Diagrams (ROMDDs) as the supporting data structure for a generic global
constraint. We give an algorithm for maintaining generalized arc consistency
(GAC) on this constraint that amortizes the cost of the GAC computation over a
root-to-leaf path in the search tree without requiring asymptotically more
space than used for the MDD. Furthermore we present an approach for
incrementally maintaining the reduced property of the MDD during the search 
and show how this can be used for providing domain entailment detection.
Finally we discuss how to apply our approach to other similar data structures
such as AOMDDs and Case DAGs. The technique used can be seen as an extension of
the GAC algorithm for the regular language constraint on finite length input."
"For academics and practitioners concerned with computers  business and
mathematics  one central issue is supporting decision makers. In this paper  we
propose a generalization of Decision Matrix Method (DMM)  using Neutrosophic
logic. It emerges as an alternative to the existing logics and it represents a
mathematical model of uncertainty and indeterminacy. This paper proposes the
Neutrosophic Decision Matrix Method as a more realistic tool for decision
making. In addition  a de-neutrosophication process is included."
"This paper constructs a tree structure for the music rhythm using the
L-system. It models the structure as an automata and derives its complexity. It
also solves the complexity for the L-system. This complexity can resolve the
similarity between trees. This complexity serves as a measure of psychological
complexity for rhythms. It resolves the music complexity of various
compositions including the Mozart effect K488.
  Keyword. music perception  psychological complexity  rhythm  L-system 
automata  temporal associative memory  inverse problem  rewriting rule 
bracketed string  tree similarity"
"Using qualitative reasoning with geographic information  contrarily  for
instance  with robotics  looks not only fastidious (i.e.. encoding knowledge
Propositional Logics PL)  but appears to be computational complex  and not
tractable at all  most of the time. However  knowledge fusion or revision  is a
common operation performed when users merge several different data sets in a
unique decision making process  without much support. Introducing logics would
be a great improvement  and we propose in this paper  means for deciding -a
priori- if one application can benefit from a complete revision  under only the
assumption of a conjecture that we name the ""containment conjecture""  which
limits the size of the minimal conflicts to revise. We demonstrate that this
conjecture brings us the interesting computational property of performing a
not-provable but global  revision  made of many local revisions  at a tractable
size. We illustrate this approach on an application."
"In case-based reasoning  the adaptation of a source case in order to solve
the target problem is at the same time crucial and difficult to implement. The
reason for this difficulty is that  in general  adaptation strongly depends on
domain-dependent knowledge. This fact motivates research on adaptation
knowledge acquisition (AKA). This paper presents an approach to AKA based on
the principles and techniques of knowledge discovery from databases and
data-mining. It is implemented in CABAMAKA  a system that explores the
variations within the case base to elicit adaptation knowledge. This system has
been successfully tested in an application of case-based reasoning to decision
support in the domain of breast cancer treatment."
"In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework."
"Motivation. Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately  their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT  a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step  HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary  secondary and tertiary structures  accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results. We used the SCOP
database to perform our experiments. Throughout  we apply leave-one-family-out
cross-validation over protein superfamilies. First  we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then  we performed two
sets of experiments. In a first experiment  we compared structure weighted
models against standard pHMMs and against each other. In a second experiment 
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves  and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER  and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models."
"This paper proposes an approach to training rough set models using Bayesian
framework trained using Markov Chain Monte Carlo (MCMC) method. The prior
probabilities are constructed from the prior knowledge that good rough set
models have fewer rules. Markov Chain Monte Carlo sampling is conducted through
sampling in the rough set granule space and Metropolis algorithm is used as an
acceptance criteria. The proposed method is tested to estimate the risk of HIV
given demographic data. The results obtained shows that the proposed approach
is able to achieve an average accuracy of 58% with the accuracy varying up to
66%. In addition the Bayesian rough set give the probabilities of the estimated
HIV status as well as the linguistic rules describing how the demographic
parameters drive the risk of HIV."
"Noise  corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust 
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise."
"Evolutionary Learning proceeds by evolving a population of classifiers  from
which it generally returns (with some notable exceptions) the single
best-of-run classifier as final result. In the meanwhile  Ensemble Learning 
one of the most efficient approaches in supervised Machine Learning for the
last decade  proceeds by building a population of diverse classifiers. Ensemble
Learning with Evolutionary Computation thus receives increasing attention. The
Evolutionary Ensemble Learning (EEL) approach presented in this paper features
two contributions. First  a new fitness function  inspired by co-evolution and
enforcing the classifier diversity  is presented. Further  a new selection
criterion based on the classification margin is proposed. This criterion is
used to extract the classifier ensemble from the final population only
(Off-line) or incrementally along evolution (On-line). Experiments on a set of
benchmark problems show that Off-line outperforms single-hypothesis
evolutionary learning and state-of-art Boosting and generates smaller
classifier ensembles."
"Gaussian mixture models (GMM) and support vector machines (SVM) are
introduced to classify faults in a population of cylindrical shells. The
proposed procedures are tested on a population of 20 cylindrical shells and
their performance is compared to the procedure  which uses multi-layer
perceptrons (MLP). The modal properties extracted from vibration data are used
to train the GMM  SVM and MLP. It is observed that the GMM produces 98%  SVM
produces 94% classification accuracy while the MLP produces 88% classification
rates."
"The act of bluffing confounds game designers to this day. The very nature of
bluffing is even open for debate  adding further complication to the process of
creating intelligent virtual players that can bluff  and hence play 
realistically. Through the use of intelligent  learning agents  and carefully
designed agent outlooks  an agent can in fact learn to predict its opponents
reactions based not only on its own cards  but on the actions of those around
it. With this wider scope of understanding  an agent can in learn to bluff its
opponents  with the action representing not an illogical action  as bluffing is
often viewed  but rather as an act of maximising returns through an effective
statistical optimisation. By using a tee dee lambda learning algorithm to
continuously adapt neural network agent intelligence  agents have been shown to
be able to learn to bluff without outside prompting  and even to learn to call
each others bluffs in free  competitive play."
"The semiring-based constraint satisfaction problems (semiring CSPs)  proposed
by Bistarelli  Montanari and Rossi \cite{BMR97}  is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem  the idea is  first working in the abstract problem and
finding its optimal solutions  then using them to solve the concrete problem.
  In particular  we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover  for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$  if $t$ is optimal in
$\alpha(P)$  then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$."
"This paper proposes a neuro-rough model based on multi-layered perceptron and
rough set. The neuro-rough model is then tested on modelling the risk of HIV
from demographic data. The model is formulated using Bayesian framework and
trained using Monte Carlo method and Metropolis criterion. When the model was
tested to estimate the risk of HIV infection given the demographic data it was
found to give the accuracy of 62%. The proposed model is able to combine the
accuracy of the Bayesian MLP model and the transparency of Bayesian rough set
model."
"Water plays a pivotal role in many physical processes  and most importantly
in sustaining human life  animal life and plant life. Water supply entities
therefore have the responsibility to supply clean and safe water at the rate
required by the consumer. It is therefore necessary to implement mechanisms and
systems that can be employed to predict both short-term and long-term water
demands. The increasingly growing field of computational intelligence
techniques has been proposed as an efficient tool in the modelling of dynamic
phenomena. The primary objective of this paper is to compare the efficiency of
two computational intelligence techniques in water demand forecasting. The
techniques under comparison are the Artificial Neural Networks (ANNs) and the
Support Vector Machines (SVMs). In this study it was observed that the ANNs
perform better than the SVMs. This performance is measured against the
generalisation ability of the two."
"An ensemble based approach for dealing with missing data  without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result  is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data  and hence  saves time."
"Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions  which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research  neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs."
"The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture  as advocated in Brooks'
subsumption architecture  and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior  and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented  demonstrating the
efficiency and showing the recursiveness of this approach. In particular  the
sensitivity with respect to the proposed elementary behaviors  and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail."
"This paper introduces a continuous model for Multi-cellular Developmental
Design. The cells are fixed on a 2D grid and exchange ""chemicals"" with their
neighbors during the growth process. The quantity of chemicals that a cell
produces  as well as the differentiation value of the cell in the phenotype 
are controlled by a Neural Network (the genotype) that takes as inputs the
chemicals produced by the neighboring cells at the previous time step. In the
proposed model  the number of iterations of the growth process is not
pre-determined  but emerges during evolution. only organisms for which the
growth process stabilizes give a phenotype (the stable state)  others are
declared nonviable. The optimization of the controller is done using the NEAT
algorithm  that optimizes both the topology and the weights of the Neural
Networks. Though each cell only receives local information from its neighbors 
the experimental results of the proposed approach on the 'flags' problems (the
phenotype must match a given 2D pattern) are almost as good as those of a
direct regression approach using the same model with global information.
Moreover  the resulting multi-cellular organisms exhibit almost perfect
self-healing characteristics."
"This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data  it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated  which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place  which was not used in the training and was
found to be in good agreement."
"This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies  for the purposes of condition monitoring  has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%  which is higher than the previously
used multilayer perceptron."
"This paper presents bushing condition monitoring frameworks that use
multi-layer perceptrons (MLP)  radial basis functions (RBF) and support vector
machines (SVM) classifiers. The first level of the framework determines if the
bushing is faulty or not while the second level determines the type of fault.
The diagnostic gases in the bushings are analyzed using the dissolve gas
analysis. MLP gives superior performance in terms of accuracy and training time
than SVM and RBF. In addition  an on-line bushing condition monitoring
approach  which is able to adapt to newly acquired data are introduced. This
approach is able to accommodate new classes that are introduced by incoming
data and is implemented using an incremental learning algorithm that uses MLP.
The testing results improved from 67.5% to 95.8% as new data were introduced
and the testing results improved from 60% to 95.3% as new conditions were
introduced. On average the confidence value of the framework on its decision
was 0.92."
"This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC)  highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers  quantum gates and quantum algorithms
and then it presents references  ideas and research guidelines on how QC can be
used to deal with some basic AI problems  such as search and pattern matching 
as soon as quantum computers become widely available."
"Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore 
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First  instead of maximizing the trace of the
cluster crosstable  we propose to maximize a chi-square transformation of this
crosstable. Thus  the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations  taking into
account the marginals. Second  we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available."
"This paper describes a system capable of semi-automatically filling an XML
template from free texts in the clinical domain (practice guidelines). The XML
template includes semantic information not explicitly encoded in the text
(pairs of conditions and actions/recommendations). Therefore  there is a need
to compute the exact scope of conditions over text sequences expressing the
required actions. We present a system developed for this task. We show that it
yields good performance when applied to the analysis of French practice
guidelines."
"Representing and reasoning about qualitative temporal information is an
essential part of many artificial intelligence tasks. Lots of models have been
proposed in the litterature for representing such temporal information. All
derive from a point-based or an interval-based framework. One fundamental
reasoning task that arises in applications of these frameworks is given by the
following scheme. given possibly indefinite and incomplete knowledge of the
binary relationships between some temporal objects  find the consistent
scenarii between all these objects. All these models require transitive tables
-- or similarly inference rules-- for solving such tasks. We have defined an
alternative model  S-languages - to represent qualitative temporal information 
based on the only two relations of \emph{precedence} and \emph{simultaneity}.
In this paper  we show how this model enables to avoid transitive tables or
inference rules to handle this kind of problem."
"This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally 
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless  the
70-odd definitions presented here are  to the authors' knowledge  the largest
and most well referenced collection there is."
"Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities  which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution  or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform  Ogmios  which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources  which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity  domain semantic awareness and
performance can be handled all together."
"We consider the problem of finding an n-agent joint-policy for the optimal
finite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem
of very high complexity (NEXP-hard in n >= 2). In this paper  we propose a new
mathematical programming approach for the problem. Our approach is based on two
ideas. First  we represent each agent's policy in the sequence-form and not in
the tree-form  thereby obtaining a very compact representation of the set of
joint-policies. Second  using this compact representation  we solve this
problem as an instance of combinatorial optimization for which we formulate a
mixed integer linear program (MILP). The optimal solution of the MILP directly
yields an optimal joint-policy for the Dec-Pomdp. Computational experience
shows that formulating and solving the MILP requires significantly less time to
solve benchmark Dec-Pomdp problems than existing algorithms. For example  the
multi-agent tiger problem for horizon 4 is solved in 72 secs with the MILP
whereas existing algorithms require several hours to solve it."
"In this paper  we employ Probabilistic Neural Network (PNN) with image and
data processing techniques to implement a general purpose automated leaf
recognition algorithm. 12 leaf features are extracted and orthogonalized into 5
principal variables which consist the input vector of the PNN. The PNN is
trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater
than 90%. Compared with other approaches  our algorithm is an accurate
artificial intelligence approach which is fast in execution and easy in
implementation."
"When Kurt Goedel layed the foundations of theoretical computer science in
1931  he also introduced essential concepts of the theory of Artificial
Intelligence (AI). Although much of subsequent AI research has focused on
heuristics  which still play a major role in many practical AI applications  in
the new millennium AI theory has finally become a full-fledged formal science 
with important optimality results for embodied agents living in unknown
environments  obtained through a combination of theory a la Goedel and
probability theory. Here we look back at important milestones of AI history 
mention essential recent results  and speculate about what we may expect from
the next 25 years  emphasizing the significance of the ongoing dramatic
hardware speedups  and discussing Goedel-inspired  self-referential 
self-improving universal problem solvers."
"We develop a general framework for MAP estimation in discrete and Gaussian
graphical models using Lagrangian relaxation techniques. The key idea is to
reformulate an intractable estimation problem as one defined on a more
tractable graph  but subject to additional constraints. Relaxing these
constraints gives a tractable dual problem  one defined by a thin graph  which
is then optimized by an iterative procedure. When this iterative optimization
leads to a consistent estimate  one which also satisfies the constraints  then
it corresponds to an optimal MAP estimate of the original model. Otherwise
there is a ``duality gap''  and we obtain a bound on the optimal solution.
Thus  our approach combines convex optimization with dynamic programming
techniques applicable for thin graphs. The popular tree-reweighted max-product
(TRMP) method may be seen as solving a particular class of such relaxations 
where the intractable graph is relaxed to a set of spanning trees. We also
consider relaxations to a set of small induced subgraphs  thin subgraphs (e.g.
loops)  and a connected tree obtained by ``unwinding'' cycles. In addition  we
propose a new class of multiscale relaxations that introduce ``summary''
variables. The potential benefits of such generalizations include. reducing or
eliminating the ``duality gap'' in hard problems  reducing the number or
Lagrange multipliers in the dual problem  and accelerating convergence of the
iterative optimization procedure."
"This paper addresses a method to analyze the covert social network foundation
hidden behind the terrorism disaster. It is to solve a node discovery problem 
which means to discover a node  which functions relevantly in a social network 
but escaped from monitoring on the presence and mutual relationship of nodes.
The method aims at integrating the expert investigator's prior understanding 
insight on the terrorists' social network nature derived from the complex graph
theory  and computational data processing. The social network responsible for
the 9/11 attack in 2001 is used to execute simulation experiment to evaluate
the performance of the method."
"Methods to solve a node discovery problem for a social network are presented.
Covert nodes refer to the nodes which are not observable directly. They
transmit the influence and affect the resulting collaborative activities among
the persons in a social network  but do not appear in the surveillance logs
which record the participants of the collaborative activities. Discovering the
covert nodes is identifying the suspicious logs where the covert nodes would
appear if the covert nodes became overt. The performance of the methods is
demonstrated with a test dataset generated from computationally synthesized
networks and a real organization."
"An empty spot refers to an empty hard-to-fill space which can be found in the
records of the social interaction  and is the clue to the persons in the
underlying social network who do not appear in the records. This contribution
addresses a problem to predict relevant empty spots in social interaction.
Homogeneous and inhomogeneous networks are studied as a model underlying the
social interaction. A heuristic predictor function approach is presented as a
new method to address the problem. Simulation experiment is demonstrated over a
homogeneous network. A test data in the form of baskets is generated from the
simulated communication. Precision to predict the empty spots is calculated to
demonstrate the performance of the presented approach."
"To appear in Theory and Practice of Logic Programming (TPLP)  2008.
  We are researching the interaction between the rule and the ontology layers
of the Semantic Web  by comparing two options. 1) using OWL and its rule
extension SWRL to develop an integrated ontology/rule language  and 2) layering
rules on top of an ontology with RuleML and OWL. Toward this end  we are
developing the SWORIER system  which enables efficient automated reasoning on
ontologies and rules  by translating all of them into Prolog and adding a set
of general rules that properly capture the semantics of OWL. We have also
enabled the user to make dynamic changes on the fly  at run time. This work
addresses several of the concerns expressed in previous work  such as negation 
complementary classes  disjunctive heads  and cardinality  and it discusses
alternative approaches for dealing with inconsistencies in the knowledge base.
In addition  for efficiency  we implemented techniques called
extensionalization  avoiding reanalysis  and code minimization."
"Most definitions of ontology  viewed as a ""specification of a
conceptualization""  agree on the fact that if an ontology can take different
forms  it necessarily includes a vocabulary of terms and some specification of
their meaning in relation to the domain's conceptualization. And as domain
knowledge is mainly conveyed through scientific and technical texts  we can
hope to extract some useful information from them for building ontology. But is
it as simple as this? In this article we shall see that the lexical structure 
i.e. the network of words linked by linguistic relationships  does not
necessarily match the domain conceptualization. We have to bear in mind that
writing documents is the concern of textual linguistics  of which one of the
principles is the incompleteness of text  whereas building ontology - viewed as
task-independent knowledge - is concerned with conceptualization based on
formal and not natural languages. Nevertheless  the famous Sapir and Whorf
hypothesis  concerning the interdependence of thought and language  is also
applicable to formal languages. This means that the way an ontology is built
and a concept is defined depends directly on the formal language which is used;
and the results will not be the same. The introduction of the notion of
ontoterminology allows to take into account epistemological principles for
formal ontology building."
"Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
""intelligent"" processing. The biological basis of this computing is sketched
and the matter of learning is examined."
"In this paper we introduce a new selection scheme in cellular genetic
algorithms (cGAs). Anisotropic Selection (AS) promotes diversity and allows
accurate control of the selective pressure. First we compare this new scheme
with the classical rectangular grid shapes solution according to the selective
pressure. we can obtain the same takeover time with the two techniques although
the spreading of the best individual is different. We then give experimental
results that show to what extent AS promotes the emergence of niches that
support low coupling and high cohesion. Finally  using a cGA with anisotropic
selection on a Quadratic Assignment Problem we show the existence of an
anisotropic optimal value for which the best average performance is observed.
Further work will focus on the selective pressure self-adjustment ability
provided by this new selection scheme."
"This philosophical paper explores the relation between modern scientific
simulations and the future of the universe. We argue that a simulation of an
entire universe will result from future scientific activity. This requires us
to tackle the challenge of simulating open-ended evolution at all levels in a
single simulation. The simulation should encompass not only biological
evolution  but also physical evolution (a level below) and cultural evolution
(a level above). The simulation would allow us to probe what would happen if we
would ""replay the tape of the universe"" with the same or different laws and
initial conditions. We also distinguish between real-world and artificial-world
modelling. Assuming that intelligent life could indeed simulate an entire
universe  this leads to two tentative hypotheses. Some authors have argued that
we may already be in a simulation run by an intelligent entity. Or  if such a
simulation could be made real  this would lead to the production of a new
universe. This last direction is argued with a careful speculative
philosophical approach  emphasizing the imperative to find a solution to the
heat death problem in cosmology. The reader is invited to consult Annex 1 for
an overview of the logical structure of this paper. -- Keywords. far future 
future of science  ALife  simulation  realization  cosmology  heat death 
fine-tuning  physical eschatology  cosmological natural selection  cosmological
artificial selection  artificial cosmogenesis  selfish biocosm hypothesis 
meduso-anthropic principle  developmental singularity hypothesis  role of
intelligent life."
This paper has been withdrawn.
"In this paper  we describe a new algorithm that consists in combining an
eye-tracker for minimizing the fatigue of a user during the evaluation process
of Interactive Evolutionary Computation. The approach is then applied to the
Interactive One-Max optimization problem."
"In this paper  I present a method to solve a node discovery problem in a
networked organization. Covert nodes refer to the nodes which are not
observable directly. They affect social interactions  but do not appear in the
surveillance logs which record the participants of the social interactions.
Discovering the covert nodes is defined as identifying the suspicious logs
where the covert nodes would appear if the covert nodes became overt. A
mathematical model is developed for the maximal likelihood estimation of the
network behind the social interactions and for the identification of the
suspicious logs. Precision  recall  and F measure characteristics are
demonstrated with the dataset generated from a real organization and the
computationally synthesized datasets. The performance is close to the
theoretical limit for any covert nodes in the networks of any topologies and
sizes if the ratio of the number of observation to the number of possible
communication patterns is large."
"In an emergency situation  the actors need an assistance allowing them to
react swiftly and efficiently. In this prospect  we present in this paper a
decision support system that aims to prepare actors in a crisis situation
thanks to a decision-making support. The global architecture of this system is
presented in the first part. Then we focus on a part of this system which is
designed to represent the information of the current situation. This part is
composed of a multiagent system that is made of factual agents. Each agent
carries a semantic feature and aims to represent a partial part of a situation.
The agents develop thanks to their interactions by comparing their semantic
features using proximity measures and according to specific ontologies."
"A new method is presented  that can help a person become aware of his or her
unconscious preferences  and convey them to others in the form of verbal
explanation. The method combines the concepts of reflection  visualization  and
verbalization. The method was tested in an experiment where the unconscious
preferences of the subjects for various artworks were investigated. In the
experiment  two lessons were learned. The first is that it helps the subjects
become aware of their unconscious preferences to verbalize weak preferences as
compared with strong preferences through discussion over preference diagrams.
The second is that it is effective to introduce an adjustable factor into
visualization to adapt to the differences in the subjects and to foster their
mutual understanding."
"The textured images' classification assumes to consider the images in terms
of area with the same texture. In uncertain environment  it could be better to
take an imprecise decision or to reject the area corresponding to an unlearning
class. Moreover  on the areas that are the classification units  we can have
more than one texture. These considerations allows us to develop a belief
decision model permitting to reject an area as unlearning and to decide on
unions and intersections of learning classes. The proposed approach finds all
its justification in an application of seabed characterization from sonar
images  which contributes to an illustration."
"We study two aspects of information semantics. (i) the collection of all
relationships  (ii) tracking and spotting anomaly and change. The first is
implemented by endowing all relevant information spaces with a Euclidean metric
in a common projected space. The second is modelled by an induced ultrametric.
A very general way to achieve a Euclidean embedding of different information
spaces based on cross-tabulation counts (and from other input data formats) is
provided by Correspondence Analysis. From there  the induced ultrametric that
we are particularly interested in takes a sequential - e.g. temporal - ordering
of the data into account. We employ such a perspective to look at narrative 
""the flow of thought and the flow of language"" (Chafe). In application to
policy decision making  we show how we can focus analysis in a small number of
dimensions."
"In this paper we extend Inagaki Weighted Operators fusion rule (WO) in
information fusion by doing redistribution of not only the conflicting mass 
but also of masses of non-empty intersections  that we call Double Weighted
Operators (DWO). Then we propose a new fusion rule Class of Proportional
Redistribution of Intersection Masses (CPRIM)  which generates many interesting
particular fusion rules in information fusion. Both formulas are presented for
any number of sources of information. An application and comparison with other
fusion rules are given in the last section."
"In this chapter  we propose a new practical codification of the elements of
the Venn diagram in order to easily manipulate the focal elements. In order to
reduce the complexity  the eventual constraints must be integrated in the
codification at the beginning. Hence  we only consider a reduced hyper power
set $D_r^\Theta$ that can be $2^\Theta$ or $D^\Theta$. We describe all the
steps of a general belief function framework. The step of decision is
particularly studied  indeed  when we can decide on intersections of the
singletons of the discernment space no actual decision functions are easily to
use. Hence  two approaches are proposed  an extension of previous one and an
approach based on the specificity of the elements on which to decide. The
principal goal of this chapter is to provide practical codes of a general
belief function framework for the researchers and users needing the belief
function theory."
"In this paper  we propose in Dezert-Smarandache Theory (DSmT) framework  a
new probabilistic transformation  called DSmP  in order to build a subjective
probability measure from any basic belief assignment defined on any model of
the frame of discernment. Several examples are given to show how the DSmP
transformation works and we compare it to main existing transformations
proposed in the literature so far. We show the advantages of DSmP over
classical transformations in term of Probabilistic Information Content (PIC).
The direct extension of this transformation for dealing with qualitative belief
assignments is also presented."
"We discuss metacognitive modelling as an enhancement to cognitive modelling
and computing. Metacognitive control mechanisms should enable AI systems to
self-reflect  reason about their actions  and to adapt to new situations. In
this respect  we propose implementation details of a knowledge taxonomy and an
augmented data mining life cycle which supports a live integration of obtained
models."
"Each cognitive science tries to understand a set of cognitive behaviors. The
structuring of knowledge of this nature's aspect is far from what it can be
expected about a science. Until now universal standard consistently describing
the set of cognitive behaviors has not been found  and there are many questions
about the cognitive behaviors for which only there are opinions of members of
the scientific community. This article has three proposals. The first proposal
is to raise to the scientific community the necessity of unified the cognitive
behaviors. The second proposal is claim the application of the Newton's
reasoning rules about nature of his book  Philosophiae Naturalis Principia
Mathematica  to the cognitive behaviors. The third is to propose a scientific
theory  currently developing  that follows the rules established by Newton to
make sense of nature  and could be the theory to explain all the cognitive
behaviors."
"Various local search approaches have recently been applied to machine
scheduling problems under multiple objectives. Their foremost consideration is
the identification of the set of Pareto optimal alternatives. An important
aspect of successfully solving these problems lies in the definition of an
appropriate neighbourhood structure. Unclear in this context remains  how
interdependencies within the fitness landscape affect the resolution of the
problem.
  The paper presents a study of neighbourhood search operators for multiple
objective flow shop scheduling. Experiments have been carried out with twelve
different combinations of criteria. To derive exact conclusions  small problem
instances  for which the optimal solutions are known  have been chosen.
Statistical tests show that no single neighbourhood operator is able to equally
identify all Pareto optimal alternatives. Significant improvements however have
been obtained by hybridising the solution algorithm using a randomised variable
neighbourhood search technique."
"The paper describes the proposition and application of a local search
metaheuristic for multi-objective optimization problems. It is based on two
main principles of heuristic search  intensification through variable
neighborhoods  and diversification through perturbations and successive
iterations in favorable regions of the search space. The concept is
successfully tested on permutation flow shop scheduling problems under multiple
objectives. While the obtained results are encouraging in terms of their
quality  another positive attribute of the approach is its' simplicity as it
does require the setting of only very few parameters. The implementation of the
Pareto Iterated Local Search metaheuristic is based on the MOOPPS computer
system of local search heuristics for multi-objective scheduling which has been
awarded the European Academic Software Award 2002 in Ronneby  Sweden
(http.//www.easa-award.net/  http.//www.bth.se/llab/easa_2002.nsf)"
"The article describes an investigation of the effectiveness of genetic
algorithms for multi-objective combinatorial optimization (MOCO) by presenting
an application for the vehicle routing problem with soft time windows. The work
is motivated by the question  if and how the problem structure influences the
effectiveness of different configurations of the genetic algorithm.
Computational results are presented for different classes of vehicle routing
problems  varying in their coverage with time windows  time window size 
distribution and number of customers. The results are compared with a simple 
but effective local search approach for multi-objective combinatorial
optimization problems."
"The talk describes a general approach of a genetic algorithm for multiple
objective optimization problems. A particular dominance relation between the
individuals of the population is used to define a fitness operator  enabling
the genetic algorithm to adress even problems with efficient  but
convex-dominated alternatives. The algorithm is implemented in a multilingual
computer program  solving vehicle routing problems with time windows under
multiple objectives. The graphical user interface of the program shows the
progress of the genetic algorithm and the main parameters of the approach can
be easily modified. In addition to that  the program provides powerful decision
support to the decision maker. The software has proved it's excellence at the
finals of the European Academic Software Award EASA  held at the Keble college/
University of Oxford/ Great Britain."
"The article presents a framework for the resolution of rich vehicle routing
problems which are difficult to address with standard optimization techniques.
We use local search on the basis on variable neighborhood search for the
construction of the solutions  but embed the techniques in a flexible framework
that allows the consideration of complex side constraints of the problem such
as time windows  multiple depots  heterogeneous fleets  and  in particular 
multiple optimization criteria. In order to identify a compromise alternative
that meets the requirements of the decision maker  an interactive procedure is
integrated in the resolution of the problem  allowing the modification of the
preference information articulated by the decision maker. The framework is
prototypically implemented in a computer system. First results of test runs on
multiple depot vehicle routing problems with time windows are reported."
"The integration of fuzzy set theory and fuzzy logic into scheduling is a
rather new aspect with growing importance for manufacturing applications 
resulting in various unsolved aspects. In the current paper  we investigate an
improved local search technique for fuzzy scheduling problems with fitness
plateaus  using a multi criteria formulation of the problem. We especially
address the problem of changing job priorities over time as studied at the
Sherwood Press Ltd  a Nottingham based printing company  who is a collaborator
on the project."
"The article proposes a heuristic approximation approach to the bin packing
problem under multiple objectives. In addition to the traditional objective of
minimizing the number of bins  the heterogeneousness of the elements in each
bin is minimized  leading to a biobjective formulation of the problem with a
tradeoff between the number of bins and their heterogeneousness. An extension
of the Best-Fit approximation algorithm is presented to solve the problem.
Experimental investigations have been carried out on benchmark instances of
different size  ranging from 100 to 1000 items. Encouraging results have been
obtained  showing the applicability of the heuristic approach to the described
problem."
"The article presents a local search approach for the solution of timetabling
problems in general  with a particular implementation for competition track 3
of the International Timetabling Competition 2007 (ITC 2007). The heuristic
search procedure is based on Threshold Accepting to overcome local optima. A
stochastic neighborhood is proposed and implemented  randomly removing and
reassigning events from the current solution.
  The overall concept has been incrementally obtained from a series of
experiments  which we describe in each (sub)section of the paper. In result  we
successfully derived a potential candidate solution approach for the finals of
track 3 of the ITC 2007."
"The paper presents a study of local search heuristics in general and variable
neighborhood search in particular for the resolution of an assignment problem
studied in the practical work of universities. Here  students have to be
assigned to scientific topics which are proposed and supported by members of
staff. The problem involves the optimization under given preferences of
students which may be expressed when applying for certain topics.
  It is possible to observe that variable neighborhood search leads to superior
results for the tested problem instances. One instance is taken from an actual
case  while others have been generated based on the real world data to support
the analysis with a deeper analysis.
  An extension of the problem has been formulated by integrating a second
objective function that simultaneously balances the workload of the members of
staff while maximizing utility of the students. The algorithmic approach has
been prototypically implemented in a computer system. One important aspect in
this context is the application of the research work to problems of other
scientific institutions  and therefore the provision of decision support
functionalities."
"We introduce an extended tableau calculus for answer set programming (ASP).
The proof system is based on the ASP tableaux defined in [Gebser&Schaub  ICLP
2006]  with an added extension rule. We investigate the power of Extended ASP
Tableaux both theoretically and empirically. We study the relationship of
Extended ASP Tableaux with the Extended Resolution proof system defined by
Tseitin for sets of clauses  and separate Extended ASP Tableaux from ASP
Tableaux by giving a polynomial-length proof for a family of normal logic
programs P_n for which ASP Tableaux has exponential-length minimal proofs with
respect to n. Additionally  Extended ASP Tableaux imply interesting insight
into the effect of program simplification on the lengths of proofs in ASP.
Closely related to Extended ASP Tableaux  we empirically investigate the effect
of redundant rules on the efficiency of ASP solving.
  To appear in Theory and Practice of Logic Programming (TPLP)."
"In this paper  a Gaifman-Shapiro-style module architecture is tailored to the
case of Smodels programs under the stable model semantics. The composition of
Smodels program modules is suitably limited by module conditions which ensure
the compatibility of the module system with stable models. Hence the semantics
of an entire Smodels program depends directly on stable models assigned to its
modules. This result is formalized as a module theorem which truly strengthens
Lifschitz and Turner's splitting-set theorem for the class of Smodels programs.
To streamline generalizations in the future  the module theorem is first proved
for normal programs and then extended to cover Smodels programs using a
translation from the latter class of programs to the former class. Moreover 
the respective notion of module-level equivalence  namely modular equivalence 
is shown to be a proper congruence relation. it is preserved under
substitutions of modules that are modularly equivalent. Principles for program
decomposition are also addressed. The strongly connected components of the
respective dependency graph can be exploited in order to extract a module
structure when there is no explicit a priori knowledge about the modules of a
program. The paper includes a practical demonstration of tools that have been
developed for automated (de)composition of Smodels programs.
  To appear in Theory and Practice of Logic Programming."
"Most research related to unithood were conducted as part of a larger effort
for the determination of termhood. Consequently  novelties are rare in this
small sub-field of term extraction. In addition  existing work were mostly
empirically motivated and derived. We propose a new probabilistically-derived
measure  independent of any influences of termhood  that provides dedicated
measures to gather linguistic evidence from parsed text and statistical
evidence from Google search engine for the measurement of unithood. Our
comparative study using 1 825 test cases against an existing
empirically-derived function revealed an improvement in terms of precision 
recall and accuracy."
"Most works related to unithood were conducted as part of a larger effort for
the determination of termhood. Consequently  the number of independent research
that study the notion of unithood and produce dedicated techniques for
measuring unithood is extremely small. We propose a new approach  independent
of any influences of termhood  that provides dedicated measures to gather
linguistic evidence from parsed text and statistical evidence from Google
search engine for the measurement of unithood. Our evaluations revealed a
precision and recall of 98.68% and 91.82% respectively with an accuracy at
95.42% in measuring the unithood of 1005 test cases."
"An increasing number of approaches for ontology engineering from text are
gearing towards the use of online sources such as company intranet and the
World Wide Web. Despite such rise  not much work can be found in aspects of
preprocessing and cleaning dirty texts from online sources. This paper presents
an enhancement of an Integrated Scoring for Spelling error correction 
Abbreviation expansion and Case restoration (ISSAC). ISSAC is implemented as
part of a text preprocessing phase in an ontology engineering system. New
evaluations performed on the enhanced ISSAC using 700 chat records reveal an
improved accuracy of 98% as compared to 96.5% and 71% based on the use of only
basic ISSAC and of Aspell  respectively."
"We present a domain-independent algorithm that computes macros in a novel
way. Our algorithm computes macros ""on-the-fly"" for a given set of states and
does not require previously learned or inferred information  nor prior domain
knowledge. The algorithm is used to define new domain-independent tractable
classes of classical planning that are proved to include \emph{Blocksworld-arm}
and \emph{Towers of Hanoi}."
"In this study  we reproduce two new hybrid intelligent systems  involve three
prominent intelligent computing and approximate reasoning methods. Self
Organizing feature Map (SOM)  Neruo-Fuzzy Inference System and Rough Set Theory
(RST) called SONFIS and SORST. We show how our algorithms can be construed as a
linkage of government-society interactions  where government catches various
states of behaviors. solid (absolute) or flexible. So  transition of society 
by changing of connectivity parameters (noise) from order to disorder is
inferred."
"The paper presents the investigation and implementation of the relationship
between diversity and the performance of multiple classifiers on classification
accuracy. The study is critical as to build classifiers that are strong and can
generalize better. The parameters of the neural network within the committee
were varied to induce diversity; hence structural diversity is the focus for
this study. The hidden nodes and the activation function are the parameters
that were varied. The diversity measures that were adopted from ecology such as
Shannon and Simpson were used to quantify diversity. Genetic algorithm is used
to find the optimal ensemble by using the accuracy as the cost function. The
results observed shows that there is a relationship between structural
diversity and accuracy. It is observed that the classification accuracy of an
ensemble increases as the diversity increases. There was an increase of 3%-6%
in the classification accuracy."
"The paper presents an exponential pheromone deposition rule to modify the
basic ant system algorithm which employs constant deposition rule. A stability
analysis using differential equation is carried out to find out the values of
parameters that make the ant system dynamics stable for both kinds of
deposition rule. A roadmap of connected cities is chosen as the problem
environment where the shortest route between two given cities is required to be
discovered. Simulations performed with both forms of deposition approach using
Elitist Ant System model reveal that the exponential deposition approach
outperforms the classical one by a large extent. Exhaustive experiments are
also carried out to find out the optimum setting of different controlling
parameters for exponential deposition approach and an empirical relationship
between the major controlling parameters of the algorithm and some features of
problem environment."
"This article presents a unique design for a parser using the Ant Colony
Optimization algorithm. The paper implements the intuitive thought process of
human mind through the activities of artificial ants. The scheme presented here
uses a bottom-up approach and the parsing program can directly use ambiguous or
redundant grammars. We allocate a node corresponding to each production rule
present in the given grammar. Each node is connected to all other nodes
(representing other production rules)  thereby establishing a completely
connected graph susceptible to the movement of artificial ants. Each ant tries
to modify this sentential form by the production rule present in the node and
upgrades its position until the sentential form reduces to the start symbol S.
Successful ants deposit pheromone on the links that they have traversed
through. Eventually  the optimum path is discovered by the links carrying
maximum amount of pheromone concentration. The design is simple  versatile 
robust and effective and obviates the calculation of the above mentioned sets
and precedence relation tables. Further advantages of our scheme lie in i)
ascertaining whether a given string belongs to the language represented by the
grammar  and ii) finding out the shortest possible path from the given string
to the start symbol S in case multiple routes exist."
"The paper presents an exponential pheromone deposition approach to improve
the performance of classical Ant System algorithm which employs uniform
deposition rule. A simplified analysis using differential equations is carried
out to study the stability of basic ant system dynamics with both exponential
and constant deposition rules. A roadmap of connected cities  where the
shortest path between two specified cities are to be found out  is taken as a
platform to compare Max-Min Ant System model (an improved and popular model of
Ant System algorithm) with exponential and constant deposition rules. Extensive
simulations are performed to find the best parameter settings for non-uniform
deposition approach and experiments with these parameter settings revealed that
the above approach outstripped the traditional one by a large extent in terms
of both solution quality and convergence time."
"We address here two major challenges presented by dynamic data mining. 1) the
stability challenge. we have implemented a rigorous incremental density-based
clustering algorithm  independent from any initial conditions and ordering of
the data-vectors stream  2) the cognitive challenge. we have implemented a
stringent selection process of association rules between clusters at time t-1
and time t for directly generating the main conclusions about the dynamics of a
data-stream. We illustrate these points with an application to a two years and
2600 documents scientific information database."
"Data-stream clustering is an ever-expanding subdomain of knowledge
extraction. Most of the past and present research effort aims at efficient
scaling up for the huge data repositories. Our approach focuses on qualitative
improvement  mainly for ""weak signals"" detection and precise tracking of
topical evolutions in the framework of information watch - though scalability
is intrinsically guaranteed in a possibly distributed implementation. Our
GERMEN algorithm exhaustively picks up the whole set of density peaks of the
data at time t  by identifying the local perturbations induced by the current
document vector  such as changing cluster borders  or new/vanishing clusters.
Optimality yields from the uniqueness 1) of the density landscape for any value
of our zoom parameter  2) of the cluster allocation operated by our border
propagation rule. This results in a rigorous independence from the data
presentation ranking or any initialization parameter. We present here as a
first step the only assessment of a static view resulting from one year of the
CNRS/INIST Pascal database in the field of geotechnics."
"This paper gives an introduction to this issue  and presents the framework
and the main steps of the Rosa project. Four teams of researchers  agronomists 
computer scientists  psychologists and linguists were involved during five
years within this project that aimed at the development of a knowledge based
system. The purpose of the Rosa system is the modelling and the comparison of
farm spatial organizations. It relies on a formalization of agronomical
knowledge and thus induces a joint knowledge building process involving both
the agronomists and the computer scientists. The paper describes the steps of
the modelling process as well as the filming procedures set up by the
psychologists and linguists in order to make explicit and to analyze the
underlying knowledge building process."
"Collaborative tagging systems  such as Delicious  CiteULike  and others 
allow users to annotate resources  e.g.  Web pages or scientific papers  with
descriptive labels called tags. The social annotations contributed by thousands
of users  can potentially be used to infer categorical knowledge  classify
documents or recommend new relevant information. Traditional text inference
methods do not make best use of social annotation  since they do not take into
account variations in individual users' perspectives and vocabulary. In a
previous work  we introduced a simple probabilistic model that takes interests
of individual annotators into account in order to find hidden topics of
annotated resources. Unfortunately  that approach had one major shortcoming.
the number of topics and interests must be specified a priori. To address this
drawback  we extend the model to a fully Bayesian framework  which offers a way
to automatically estimate these numbers. In particular  the model allows the
number of interests and topics to change as suggested by the structure of the
data. We evaluate the proposed model in detail on the synthetic and real-world
data by comparing its performance to Latent Dirichlet Allocation on the topic
extraction task. For the latter evaluation  we apply the model to infer topics
of Web resources from social annotations obtained from Delicious in order to
discover new resources similar to a specified one. Our empirical results
demonstrate that the proposed model is a promising method for exploiting social
knowledge contained in user-generated annotations."
"Airport gate assignment is of great importance in airport operations. In this
paper  we study the Airport Gate Assignment Problem (AGAP)  propose a new model
and implement the model with Optimization Programming language (OPL). With the
objective to minimize the number of conflicts of any two adjacent aircrafts
assigned to the same gate  we build a mathematical model with logical
constraints and the binary constraints  which can provide an efficient
evaluation criterion for the Airlines to estimate the current gate assignment.
To illustrate the feasibility of the model we construct experiments with the
data obtained from Continental Airlines  Houston Gorge Bush Intercontinental
Airport IAH  which indicate that our model is both energetic and effective.
Moreover  we interpret experimental results  which further demonstrate that our
proposed model can provide a powerful tool for airline companies to estimate
the efficiency of their current work of gate assignment."
"This paper investigates the use of different Artificial Intelligence methods
to predict the values of several continuous variables from a Steam Generator.
The objective was to determine how the different artificial intelligence
methods performed in making predictions on the given dataset. The artificial
intelligence methods evaluated were Neural Networks  Support Vector Machines 
and Adaptive Neuro-Fuzzy Inference Systems. The types of neural networks
investigated were Multi-Layer Perceptions  and Radial Basis Function. Bayesian
and committee techniques were applied to these neural networks. Each of the AI
methods considered was simulated in Matlab. The results of the simulations
showed that all the AI methods were capable of predicting the Steam Generator
data reasonably accurately. However  the Adaptive Neuro-Fuzzy Inference system
out performed the other methods in terms of accuracy and ease of
implementation  while still achieving a fast execution time as well as a
reasonable training time."
"We introduce novel results for approximate inference on planar graphical
models using the loop calculus framework. The loop calculus (Chertkov and
Chernyak  2006) allows to express the exact partition function of a graphical
model as a finite sum of terms that can be evaluated once the belief
propagation (BP) solution is known. In general  full summation over all
correction terms is intractable. We develop an algorithm for the approach
presented in (Certkov et al.  2008) which represents an efficient truncation
scheme on planar graphs and a new representation of the series in terms of
Pfaffians of matrices. We analyze the performance of the algorithm for the
partition function approximation for models with binary variables and pairwise
interactions on grids and other planar graphs. We study in detail both the loop
series and the equivalent Pfaffian series and show that the first term of the
Pfaffian series for the general  intractable planar model  can provide very
accurate approximations. The algorithm outperforms previous truncation schemes
of the loop series and is competitive with other state-of-the-art methods for
approximate inference."
"In this paper we present the N-norms/N-conorms in neutrosophic logic and set
as extensions of T-norms/T-conorms in fuzzy logic and set. Also  as an
extension of the Intuitionistic Fuzzy Topology we present the Neutrosophic
Topologies."
"When a considerable number of mutations have no effects on fitness values 
the fitness landscape is said neutral. In order to study the interplay between
neutrality  which exists in many real-world applications  and performances of
metaheuristics  it is useful to design landscapes which make it possible to
tune precisely neutral degree distribution. Even though many neutral landscape
models have already been designed  none of them are general enough to create
landscapes with specific neutral degree distributions. We propose three steps
to design such landscapes. first using an algorithm we construct a landscape
whose distribution roughly fits the target one  then we use a simulated
annealing heuristic to bring closer the two distributions and finally we affect
fitness values to each neutral network. Then using this new family of fitness
landscapes we are able to highlight the interplay between deceptiveness and
neutrality."
"The pharmacovigilance databases consist of several case reports involving
drugs and adverse events (AEs). Some methods are applied consistently to
highlight all signals  i.e. all statistically significant associations between
a drug and an AE. These methods are appropriate for verification of more
complex relationships involving one or several drug(s) and AE(s) (e.g;
syndromes or interactions) but do not address the identification of them. We
propose a method for the extraction of these relationships based on Formal
Concept Analysis (FCA) associated with disproportionality measures. This method
identifies all sets of drugs and AEs which are potential signals  syndromes or
interactions. Compared to a previous experience of disproportionality analysis
without FCA  the addition of FCA was more efficient for identifying false
positives related to concomitant drugs."
"Domain experts should provide relevant domain knowledge to an Intelligent
Tutoring System (ITS) so that it can guide a learner during problemsolving
learning activities. However  for many ill-defined domains  the domain
knowledge is hard to define explicitly. In previous works  we showed how
sequential pattern mining can be used to extract a partial problem space from
logged user interactions  and how it can support tutoring services during
problem-solving exercises. This article describes an extension of this approach
to extract a problem space that is richer and more adapted for supporting
tutoring services. We combined sequential pattern mining with (1) dimensional
pattern mining (2) time intervals  (3) the automatic clustering of valued
actions and (4) closed sequences mining. Some tutoring services have been
implemented and an experiment has been conducted in a tutoring system."
"In this paper we propose the CTS (Concious Tutoring System) technology  a
biologically plausible cognitive agent based on human brain functions.This
agent is capable of learning and remembering events and any related information
such as corresponding procedures  stimuli and their emotional valences. Our
proposed episodic memory and episodic learning mechanism are closer to the
current multiple-trace theory in neuroscience  because they are inspired by it
[5] contrary to other mechanisms that are incorporated in cognitive agents.
This is because in our model emotions play a role in the encoding and
remembering of events. This allows the agent to improve its behavior by
remembering previously selected behaviors which are influenced by its emotional
mechanism. Moreover  the architecture incorporates a realistic memory
consolidation process based on a data mining algorithm."
"Consumers of mass media must have a comprehensive  balanced and plural
selection of news to get an unbiased perspective; but achieving this goal can
be very challenging  laborious and time consuming. News stories development
over time  its (in)consistency  and different level of coverage across the
media outlets are challenges that a conscientious reader has to overcome in
order to alleviate bias.
  In this paper we present an intelligent agent framework currently
facilitating analysis of the main sources of on-line news in El Salvador. We
show how prior tools of text analysis and Web 2.0 technologies can be combined
with minimal manual intervention to help individuals on their rational decision
process  while holding media outlets accountable for their work."
"We study the logic of comparative concept similarity $\CSL$ introduced by
Sheremet  Tishkovsky  Wolter and Zakharyaschev to capture a form of qualitative
similarity comparison. In this logic we can formulate assertions of the form ""
objects A are more similar to B than to C"". The semantics of this logic is
defined by structures equipped by distance functions evaluating the similarity
degree of objects. We consider here the particular case of the semantics
induced by \emph{minspaces}  the latter being distance spaces where the minimum
of a set of distances always exists. It turns out that the semantics over
arbitrary minspaces can be equivalently specified in terms of preferential
structures  typical of conditional logics. We first give a direct
axiomatisation of this logic over Minspaces. We next define a decision
procedure in the form of a tableaux calculus. Both the calculus and the
axiomatisation take advantage of the reformulation of the semantics in terms of
preferential structures."
"Data mining algorithms are now able to efficiently deal with huge amount of
data. Various kinds of patterns may be discovered and may have some great
impact on the general development of knowledge. In many domains  end users may
want to have their data mined by data mining tools in order to extract patterns
that could impact their business. Nevertheless  those users are often
overwhelmed by the large quantity of patterns extracted in such a situation.
Moreover  some privacy issues  or some commercial one may lead the users not to
be able to mine the data by themselves. Thus  the users may not have the
possibility to perform many experiments integrating various constraints in
order to focus on specific patterns they would like to extract. Post processing
of patterns may be an answer to that drawback. Thus  in this paper we present a
framework that could allow end users to manage collections of patterns. We
propose to use an efficient data structure on which some algebraic operators
may be used in order to retrieve or access patterns in pattern bases."
"Empirical evidence suggests that hashing is an effective strategy for
dimensionality reduction and practical nonparametric estimation. In this paper
we provide exponential tail bounds for feature hashing and show that the
interaction between random subspaces is negligible with high probability. We
demonstrate the feasibility of this approach with experimental results for a
new use case -- multitask learning with hundreds of thousands of tasks."
"We propose a new extended format to represent constraint networks using XML.
This format allows us to represent constraints defined either in extension or
in intension. It also allows us to reference global constraints. Any instance
of the problems CSP (Constraint Satisfaction Problem)  QCSP (Quantified CSP)
and WCSP (Weighted CSP) can be represented using this format."
"The present work consisted in developing a plateau game. There are the
traditional ones (monopoly  cluedo  ect.) but those which interest us leave
less place at the chance (luck) than to the strategy such that the chess game.
Kallah is an old African game  its rules are simple but the strategies to be
used are very complex to implement. Of course  they are based on a strongly
mathematical basis as in the film ""Rain-Man"" where one can see that gambling
can be payed with strategies based on mathematical theories. The Artificial
Intelligence gives the possibility ""of thinking"" to a machine and  therefore 
allows it to make decisions. In our work  we use it to give the means to the
computer choosing its best movement."
"Current research on qualitative spatial representation and reasoning mainly
focuses on one single aspect of space. In real world applications  however 
multiple spatial aspects are often involved simultaneously.
  This paper investigates problems arising in reasoning with combined
topological and directional information. We use the RCC8 algebra and the
Rectangle Algebra (RA) for expressing topological and directional information
respectively. We give examples to show that the bipath-consistency algorithm
BIPATH is incomplete for solving even basic RCC8 and RA constraints. If
topological constraints are taken from some maximal tractable subclasses of
RCC8  and directional constraints are taken from a subalgebra  termed DIR49  of
RA  then we show that BIPATH is able to separate topological constraints from
directional ones. This means  given a set of hybrid topological and directional
constraints from the above subclasses of RCC8 and RA  we can transfer the joint
satisfaction problem in polynomial time to two independent satisfaction
problems in RCC8 and RA. For general RA constraints  we give a method to
compute solutions that satisfy all topological constraints and approximately
satisfy each RA constraint to any prescribed precision."
"Direction relations between extended spatial objects are important
commonsense knowledge. Recently  Goyal and Egenhofer proposed a formal model 
known as Cardinal Direction Calculus (CDC)  for representing direction
relations between connected plane regions. CDC is perhaps the most expressive
qualitative calculus for directional information  and has attracted increasing
interest from areas such as artificial intelligence  geographical information
science  and image retrieval. Given a network of CDC constraints  the
consistency problem is deciding if the network is realizable by connected
regions in the real plane. This paper provides a cubic algorithm for checking
consistency of basic CDC constraint networks  and proves that reasoning with
CDC is in general an NP-Complete problem. For a consistent network of basic CDC
constraints  our algorithm also returns a 'canonical' solution in cubic time.
This cubic algorithm is also adapted to cope with cardinal directions between
possibly disconnected regions  in which case currently the best algorithm is of
time complexity O(n^5)."
"In this paper  we address the problem of generating preferred plans by
combining the procedural control knowledge specified by Hierarchical Task
Networks (HTNs) with rich qualitative user preferences. The outcome of our work
is a language for specifyin user preferences  tailored to HTN planning 
together with a provably optimal preference-based planner  HTNPLAN  that is
implemented as an extension of SHOP2. To compute preferred plans  we propose an
approach based on forward-chaining heuristic search. Our heuristic uses an
admissible evaluation function measuring the satisfaction of preferences over
partial plans. Our empirical evaluation demonstrates the effectiveness of our
HTNPLAN heuristics. We prove our approach sound and optimal with respect to the
plans it generates by appealing to a situation calculus semantics of our
preference language and of HTN planning. While our implementation builds on
SHOP2  the language and techniques proposed here are relevant to a broad range
of HTN planners."
"We study the notion of informedness in a client-consultant setting. Using a
software simulator  we examine the extent to which it pays off for consultants
to provide their clients with advice that is well-informed  or with advice that
is merely meant to appear to be well-informed. The latter strategy is
beneficial in that it costs less resources to keep up-to-date  but carries the
risk of a decreased reputation if the clients discover the low level of
informedness of the consultant. Our experimental results indicate that under
different circumstances  different strategies yield the optimal results (net
profit) for the consultants."
"We describe in this article a multiagent urban traffic simulation  as we
believe individual-based modeling is necessary to encompass the complex
influence the actions of an individual vehicle can have on the overall flow of
vehicles. We first describe how we build a graph description of the network
from purely geometric data  ESRI shapefiles. We then explain how we include
traffic related data to this graph. We go on after that with the model of the
vehicle agents. origin and destination  driving behavior  multiple lanes 
crossroads  and interactions with the other vehicles in day-to-day  ?ordinary?
traffic. We conclude with the presentation of the resulting simulation of this
model on the Rouen agglomeration."
"2007 was the first international congress on the ?square of oppositions?. A
first attempt to structure debate using n-opposition theory was presented along
with the results of a first experiment on the web. Our proposal for this paper
is to define relations between arguments through a structure of opposition
(square of oppositions is one structure of opposition). We will be trying to
answer the following questions. How to organize debates on the web 2.0? How to
structure them in a logical way? What is the role of n-opposition theory  in
this context? We present in this paper results of three experiments
(Betapolitique 2007  ECAP 2008  Intermed 2008)."
"We propose Interactive Differential Evolution (IDE) based on paired
comparisons for reducing user fatigue and evaluate its convergence speed in
comparison with Interactive Genetic Algorithms (IGA) and tournament IGA. User
interface and convergence performance are two big keys for reducing Interactive
Evolutionary Computation (IEC) user fatigue. Unlike IGA and conventional IDE 
users of the proposed IDE and tournament IGA do not need to compare whole
individuals each other but compare pairs of individuals  which largely
decreases user fatigue. In this paper  we design a pseudo-IEC user and evaluate
another factor  IEC convergence performance  using IEC simulators and show that
our proposed IDE converges significantly faster than IGA and tournament IGA 
i.e. our proposed one is superior to others from both user interface and
convergence performance points of view."
"This paper describes application of information granulation theory  on the
back analysis of Jeffrey mine southeast wall Quebec. In this manner  using a
combining of Self Organizing Map (SOM) and rough set theory (RST)  crisp and
rough granules are obtained. Balancing of crisp granules and sub rough granules
is rendered in close-open iteration. Combining of hard and soft computing 
namely finite difference method (FDM) and computational intelligence and taking
in to account missing information are two main benefits of the proposed method.
As a practical example  reverse analysis on the failure of the southeast wall
Jeffrey mine is accomplished."
"Fault diagnosis has become a very important area of research during the last
decade due to the advancement of mechanical and electrical systems in
industries. The automobile is a crucial field where fault diagnosis is given a
special attention. Due to the increasing complexity and newly added features in
vehicles  a comprehensive study has to be performed in order to achieve an
appropriate diagnosis model. A diagnosis system is capable of identifying the
faults of a system by investigating the observable effects (or symptoms). The
system categorizes the fault into a diagnosis class and identifies a probable
cause based on the supplied fault symptoms. Fault categorization and
identification are done using similarity matching techniques. The development
of diagnosis classes is done by making use of previous experience  knowledge or
information within an application area. The necessary information used may come
from several sources of knowledge  such as from system analysis. In this paper
similarity matching techniques for fault diagnosis in automotive infotainment
applications are discussed."
"Diverse recommendation techniques have been already proposed and encapsulated
into several e-business applications  aiming to perform a more accurate
evaluation of the existing information and accordingly augment the assistance
provided to the users involved. This paper reports on the development and
integration of a recommendation module in an agent-based transportation
transactions management system. The module is built according to a novel hybrid
recommendation technique  which combines the advantages of collaborative
filtering and knowledge-based approaches. The proposed technique and supporting
module assist customers in considering in detail alternative transportation
transactions that satisfy their requests  as well as in evaluating completed
transactions. The related services are invoked through a software agent that
constructs the appropriate knowledge rules and performs a synthesis of the
recommendation policy."
"We study decompositions of NVALUE  a global constraint that can be used to
model a wide range of problems where values need to be counted. Whilst
decomposition typically hinders propagation  we identify one decomposition that
maintains a global view as enforcing bound consistency on the decomposition
achieves bound consistency on the original global NVALUE constraint. Such
decompositions offer the prospect for advanced solving techniques like nogood
learning and impact based branching heuristics. They may also help SAT and IP
solvers take advantage of the propagation of global constraints."
"Symmetry is an important feature of many constraint programs. We show that
any symmetry acting on a set of symmetry breaking constraints can be used to
break symmetry. Different symmetries pick out different solutions in each
symmetry class. We use these observations in two methods for eliminating
symmetry from a problem. These methods are designed to have many of the
advantages of symmetry breaking methods that post static symmetry breaking
constraint without some of the disadvantages. In particular  the two methods
prune the search space using fast and efficient propagation of posted
constraints  whilst reducing the conflict between symmetry breaking and
branching heuristics. Experimental results show that the two methods perform
well on some standard benchmarks."
"Fuzzy constraints are a popular approach to handle preferences and
over-constrained problems in scenarios where one needs to be cautious  such as
in medical or space applications. We consider here fuzzy constraint problems
where some of the preferences may be missing. This models  for example 
settings where agents are distributed and have privacy issues  or where there
is an ongoing preference elicitation process. In this setting  we study how to
find a solution which is optimal irrespective of the missing preferences. In
the process of finding such a solution  we may elicit preferences from the user
if necessary. However  our goal is to ask the user as little as possible. We
define a combined solving and preference elicitation scheme with a large number
of different instantiations  each corresponding to a concrete algorithm which
we compare experimentally. We compute both the number of elicited preferences
and the ""user effort""  which may be larger  as it contains all the preference
values the user has to compute to be able to respond to the elicitation
requests. While the number of elicited preferences is important when the
concern is to communicate as little information as possible  the user effort
measures also the hidden work the user has to do to be able to communicate the
elicited preferences. Our experimental results show that some of our algorithms
are very good at finding a necessarily optimal solution while asking the user
for only a very small fraction of the missing preferences. The user effort is
also very small for the best algorithms. Finally  we test these algorithms on
hard constraint problems with possibly missing constraints  where the aim is to
find feasible solutions irrespective of the missing constraints."
"We propose new filtering algorithms for the SEQUENCE constraint and some
extensions of the SEQUENCE constraint based on network flows. We enforce domain
consistency on the SEQUENCE constraint in $O(n^2)$ time down a branch of the
search tree. This improves upon the best existing domain consistency algorithm
by a factor of $O(\log n)$. The flows used in these algorithms are derived from
a linear program. Some of them differ from the flows used to propagate global
constraints like GCC since the domains of the variables are encoded as costs on
the edges rather than capacities. Such flows are efficient for maintaining
bounds consistency over large domains and may be useful for other global
constraints."
"We introduce the weighted CFG constraint and propose a propagation algorithm
that enforces domain consistency in $O(n^3|G|)$ time. We show that this
algorithm can be decomposed into a set of primitive arithmetic constraints
without hindering propagation."
"Modelling emotion has become a challenge nowadays. Therefore  several models
have been produced in order to express human emotional activity. However  only
a few of them are currently able to express the close relationship existing
between emotion and cognition. An appraisal-coping model is presented here 
with the aim to simulate the emotional impact caused by the evaluation of a
particular situation (appraisal)  along with the consequent cognitive reaction
intended to face the situation (coping). This model is applied to the
""Cascades"" problem  a small arithmetical exercise designed for ten-year-old
pupils. The goal is to create a model corresponding to a child's behaviour when
solving the problem using his own strategies."
"Modeling emotion has become a challenge nowadays. Therefore  several models
have been produced in order to express human emotional activity. However  only
a few of them are currently able to express the close relationship existing
between emotion and cognition. An appraisal-coping model is presented here 
with the aim to simulate the emotional impact caused by the evaluation of a
particular situation (appraisal)  along with the consequent cognitive reaction
intended to face the situation (coping). This model is applied to the
?Cascades? problem  a small arithmetical exercise designed for ten-year-old
pupils. The goal is to create a model corresponding to a child's behavior when
solving the problem using his own strategies."
"Rough set theory  a mathematical tool to deal with inexact or uncertain
knowledge in information systems  has originally described the indiscernibility
of elements by equivalence relations. Covering rough sets are a natural
extension of classical rough sets by relaxing the partitions arising from
equivalence relations to coverings. Recently  some topological concepts such as
neighborhood have been applied to covering rough sets. In this paper  we
further investigate the covering rough sets based on neighborhoods by
approximation operations. We show that the upper approximation based on
neighborhoods can be defined equivalently without using neighborhoods. To
analyze the coverings themselves  we introduce unary and composition operations
on coverings. A notion of homomorphismis provided to relate two covering
approximation spaces. We also examine the properties of approximations
preserved by the operations and homomorphisms  respectively."
"In Pawlak's rough set theory  a set is approximated by a pair of lower and
upper approximations. To measure numerically the roughness of an approximation 
Pawlak introduced a quantitative measure of roughness by using the ratio of the
cardinalities of the lower and upper approximations. Although the roughness
measure is effective  it has the drawback of not being strictly monotonic with
respect to the standard ordering on partitions. Recently  some improvements
have been made by taking into account the granularity of partitions. In this
paper  we approach the roughness measure in an axiomatic way. After
axiomatically defining roughness measure and partition measure  we provide a
unified construction of roughness measure  called strong Pawlak roughness
measure  and then explore the properties of this measure. We show that the
improved roughness measures in the literature are special instances of our
strong Pawlak roughness measure and introduce three more strong Pawlak
roughness measures as well. The advantage of our axiomatic approach is that
some properties of a roughness measure follow immediately as soon as the
measure satisfies the relevant axiomatic definition."
"Adaptation has long been considered as the Achilles' heel of case-based
reasoning since it requires some domain-specific knowledge that is difficult to
acquire. In this paper  two strategies are combined in order to reduce the
knowledge engineering cost induced by the adaptation knowledge (CA) acquisition
task. CA is learned from the case base by the means of knowledge discovery
techniques  and the CA acquisition sessions are opportunistically triggered 
i.e.  at problem-solving time."
"Real-time heuristic search algorithms are suitable for situated agents that
need to make their decisions in constant time. Since the original work by Korf
nearly two decades ago  numerous extensions have been suggested. One of the
most intriguing extensions is the idea of backtracking wherein the agent
decides to return to a previously visited state as opposed to moving forward
greedily. This idea has been empirically shown to have a significant impact on
various performance measures. The studies have been carried out in particular
empirical testbeds with specific real-time search algorithms that use
backtracking. Consequently  the extent to which the trends observed are
characteristic of backtracking in general is unclear. In this paper  we present
the first entirely theoretical study of backtracking in real-time heuristic
search. In particular  we present upper bounds on the solution cost exponential
and linear in a parameter regulating the amount of backtracking. The results
hold for a wide class of real-time heuristic search algorithms that includes
many existing algorithms as a small subclass."
"This paper presents several novel generalization bounds for the problem of
learning kernels based on the analysis of the Rademacher complexity of the
corresponding hypothesis sets. Our bound for learning kernels with a convex
combination of p base kernels has only a log(p) dependency on the number of
kernels  p  which is considerably more favorable than the previous best bound
given for the same problem. We also give a novel bound for learning with a
linear combination of p base kernels with an L_2 regularization whose
dependency on p is only in p^{1/4}."
"This paper formulates a necessary and sufficient condition for a generic
graph matching problem to be equivalent to the maximum vertex and edge weight
clique problem in a derived association graph. The consequences of this results
are threefold. first  the condition is general enough to cover a broad range of
practical graph matching problems; second  a proof to establish equivalence
between graph matching and clique search reduces to showing that a given graph
matching problem satisfies the proposed condition; and third  the result sets
the scene for generic continuous solutions for a broad range of graph matching
problems. To illustrate the mathematical framework  we apply it to a number of
graph matching problems  including the problem of determining the graph edit
distance."
"This paper extends k-means algorithms from the Euclidean domain to the domain
of graphs. To recompute the centroids  we apply subgradient methods for solving
the optimization-based formulation of the sample mean of graphs. To accelerate
the k-means algorithm for graphs without trading computational time against
solution quality  we avoid unnecessary graph distance calculations by
exploiting the triangle inequality of the underlying distance metric following
Elkan's k-means algorithm proposed in \cite{Elkan03}. In experiments we show
that the accelerated k-means algorithm are faster than the standard k-means
algorithm for graphs provided there is a cluster structure in the data."
"Traditional staging is based on a formal approach of similarity leaning on
dramaturgical ontologies and instanciation variations. Inspired by interactive
data mining  that suggests different approaches  we give an overview of
computer science and theater researches using computers as partners of the
actor to escape the a priori specification of roles."
"We address the problem of belief change in (nonmonotonic) logic programming
under answer set semantics. Unlike previous approaches to belief change in
logic programming  our formal techniques are analogous to those of
distance-based belief revision in propositional logic. In developing our
results  we build upon the model theory of logic programs furnished by SE
models. Since SE models provide a formal  monotonic characterisation of logic
programs  we can adapt techniques from the area of belief revision to belief
change in logic programs. We introduce methods for revising and merging logic
programs  respectively. For the former  we study both subset-based revision as
well as cardinality-based revision  and we show that they satisfy the majority
of the AGM postulates for revision. For merging  we consider operators
following arbitration merging and IC merging  respectively. We also present
encodings for computing the revision as well as the merging of logic programs
within the same logic programming framework  giving rise to a direct
implementation of our approach in terms of off-the-shelf answer set solvers.
These encodings reflect in turn the fact that our change operators do not
increase the complexity of the base formalism."
"Nearly 15 years ago  a set of qualitative spatial relations between oriented
straight line segments (dipoles) was suggested by Schlieder. This work received
substantial interest amongst the qualitative spatial reasoning community.
However  it turned out to be difficult to establish a sound constraint calculus
based on these relations. In this paper  we present the results of a new
investigation into dipole constraint calculi which uses algebraic methods to
derive sound results on the composition of relations and other properties of
dipole calculi. Our results are based on a condensed semantics of the dipole
relations.
  In contrast to the points that are normally used  dipoles are extended and
have an intrinsic direction. Both features are important properties of natural
objects. This allows for a straightforward representation of prototypical
reasoning tasks for spatial agents. As an example  we show how to generate
survey knowledge from local observations in a street network. The example
illustrates the fast constraint-based reasoning capabilities of the dipole
calculus. We integrate our results into two reasoning tools which are publicly
available."
"In this book we introduce a new procedure called \alpha-Discounting Method
for Multi-Criteria Decision Making (\alpha-D MCDM)  which is as an alternative
and extension of Saaty Analytical Hierarchy Process (AHP). It works for any
number of preferences that can be transformed into a system of homogeneous
linear equations. A degree of consistency (and implicitly a degree of
inconsistency) of a decision-making problem are defined. \alpha-D MCDM is
afterwards generalized to a set of preferences that can be transformed into a
system of linear and or non-linear homogeneous and or non-homogeneous equations
and or inequalities. The general idea of \alpha-D MCDM is to assign non-null
positive parameters \alpha_1  \alpha_2  and so on \alpha_p to the coefficients
in the right-hand side of each preference that diminish or increase them in
order to transform the above linear homogeneous system of equations which has
only the null-solution  into a system having a particular non-null solution.
After finding the general solution of this system  the principles used to
assign particular values to all parameters \alpha is the second important part
of \alpha-D  yet to be deeper investigated in the future. In the current book
we propose the Fairness Principle  i.e. each coefficient should be discounted
with the same percentage (we think this is fair. not making any favoritism or
unfairness to any coefficient)  but the reader can propose other principles.
For consistent decision-making problems with pairwise comparisons 
\alpha-Discounting Method together with the Fairness Principle give the same
result as AHP. But for weak inconsistent decision-making problem 
\alpha-Discounting together with the Fairness Principle give a different result
from AHP. Many consistent  weak inconsistent  and strong inconsistent examples
are given in this book."
"This paper proposes a design for a system to generate constraint solvers that
are specialised for specific problem models. It describes the design in detail
and gives preliminary experimental results showing the feasibility and
effectiveness of the approach."
"Machine Consciousness is the study of consciousness in a biological 
philosophical  mathematical and physical perspective and designing a model that
can fit into a programmable system architecture. Prime objective of the study
is to make the system architecture behave consciously like a biological model
does. Present work has developed a feasible definition of consciousness  that
characterizes consciousness with four parameters i.e.  parasitic  symbiotic 
self referral and reproduction. Present work has also developed a biologically
inspired consciousness architecture that has following layers. quantum layer 
cellular layer  organ layer and behavioral layer and traced the characteristics
of consciousness at each layer. Finally  the work has estimated physical and
algorithmic architecture to devise a system that can behave consciously."
"To study the communication between information systems  Wang et al. [C. Wang 
C. Wu  D. Chen  Q. Hu  and C. Wu  Communicating between information systems 
Information Sciences 178 (2008) 3228-3239] proposed two concepts of type-1 and
type-2 consistent functions. Some properties of such functions and induced
relation mappings have been investigated there. In this paper  we provide an
improvement of the aforementioned work by disclosing the symmetric relationship
between type-1 and type-2 consistent functions. We present more properties of
consistent functions and induced relation mappings and improve upon several
deficient assertions in the original work. In particular  we unify and extend
type-1 and type-2 consistent functions into the so-called
neighborhood-consistent functions. This provides a convenient means for
studying the communication between information systems based on various
neighborhoods."
"Recently  Wang et al. discussed the properties of fuzzy information systems
under homomorphisms in the paper [C. Wang  D. Chen  L. Zhu  Homomorphisms
between fuzzy information systems  Applied Mathematics Letters 22 (2009)
1045-1050]  where homomorphisms are based upon the concepts of consistent
functions and fuzzy relation mappings. In this paper  we classify consistent
functions as predecessor-consistent and successor-consistent  and then proceed
to present more properties of consistent functions. In addition  we improve
some characterizations of fuzzy relation mappings provided by Wang et al."
"In this paper  research on AI based modeling technique to optimize
development of new alloys with necessitated improvements in properties and
chemical mixture over existing alloys as per functional requirements of product
is done. The current research work novels AI in lieu of predictions to
establish association between material and product customary. Advanced
computational simulation techniques like CFD  FEA interrogations are made
viable to authenticate product dynamics in context to experimental
investigations. Accordingly  the current research is focused towards binding
relationships between material design and product design domains. The input to
feed forward back propagation prediction network model constitutes of material
design features. Parameters relevant to product design strategies are furnished
as target outputs. The outcomes of ANN shows good sign of correlation between
material and product design domains. The study enriches a new path to
illustrate material factors at the time of new product development."
"Currently  criminals profile (CP) is obtained from investigators or forensic
psychologists interpretation  linking crime scene characteristics and an
offenders behavior to his or her characteristics and psychological profile.
This paper seeks an efficient and systematic discovery of nonobvious and
valuable patterns between variables from a large database of solved cases via a
probabilistic network (PN) modeling approach. The PN structure can be used to
extract behavioral patterns and to gain insight into what factors influence
these behaviors. Thus  when a new case is being investigated and the profile
variables are unknown because the offender has yet to be identified  the
observed crime scene variables are used to infer the unknown variables based on
their connections in the structure and the corresponding numerical
(probabilistic) weights. The objective is to produce a more systematic and
empirical approach to profiling  and to use the resulting PN model as a
decision tool."
"Constraint programming can definitely be seen as a model-driven paradigm. The
users write programs for modeling problems. These programs are mapped to
executable models to calculate the solutions. This paper focuses on efficient
model management (definition and transformation). From this point of view  we
propose to revisit the design of constraint-programming systems. A model-driven
architecture is introduced to map solving-independent constraint models to
solving-dependent decision models. Several important questions are examined 
such as the need for a visual highlevel modeling language  and the quality of
metamodeling techniques to implement the transformations. A main result is the
s-COMMA platform that efficiently implements the chain from modeling to solving
constraint problems"
"An important challenge in constraint programming is to rewrite constraint
models into executable programs calculat- ing the solutions. This phase of
constraint processing may require translations between constraint programming
lan- guages  transformations of constraint representations  model
optimizations  and tuning of solving strategies. In this paper  we introduce a
pivot metamodel describing the common fea- tures of constraint models including
different kinds of con- straints  statements like conditionals and loops  and
other first-class elements like object classes and predicates. This metamodel
is general enough to cope with the constructions of many languages  from
object-oriented modeling languages to logic languages  but it is independent
from them. The rewriting operations manipulate metamodel instances apart from
languages. As a consequence  the rewriting operations apply whatever languages
are selected and they are able to manage model semantic information. A bridge
is created between the metamodel space and languages using parsing techniques.
Tools from the software engineering world can be useful to implement this
framework."
"Transforming constraint models is an important task in re- cent constraint
programming systems. User-understandable models are defined during the modeling
phase but rewriting or tuning them is manda- tory to get solving-efficient
models. We propose a new architecture al- lowing to define bridges between any
(modeling or solver) languages and to implement model optimizations. This
architecture follows a model- driven approach where the constraint modeling
process is seen as a set of model transformations. Among others  an interesting
feature is the def- inition of transformations as concept-oriented rules  i.e.
based on types of model elements where the types are organized into a hierarchy
called a metamodel."
"The methodology of Bayesian Model Averaging (BMA) is applied for assessment
of newborn brain maturity from sleep EEG. In theory this methodology provides
the most accurate assessments of uncertainty in decisions. However  the
existing BMA techniques have been shown providing biased assessments in the
absence of some prior information enabling to explore model parameter space in
details within a reasonable time. The lack in details leads to disproportional
sampling from the posterior distribution. In case of the EEG assessment of
brain maturity  BMA results can be biased because of the absence of information
about EEG feature importance. In this paper we explore how the posterior
information about EEG features can be used in order to reduce a negative impact
of disproportional sampling on BMA performance. We use EEG data recorded from
sleeping newborns to test the efficiency of the proposed BMA technique."
"In this paper we describe an original computational model for solving
different types of Distributed Constraint Satisfaction Problems (DCSP). The
proposed model is called Controller-Agents for Constraints Solving (CACS). This
model is intended to be used which is an emerged field from the integration
between two paradigms of different nature. Multi-Agent Systems (MAS) and the
Constraint Satisfaction Problem paradigm (CSP) where all constraints are
treated in central manner as a black-box. This model allows grouping
constraints to form a subset that will be treated together as a local problem
inside the controller. Using this model allows also handling non-binary
constraints easily and directly so that no translating of constraints into
binary ones is needed. This paper presents the implementation outlines of a
prototype of DCSP solver  its usage methodology and overview of the CACS
application for timetabling problems."
"Model transformations operate on models conforming to precisely defined
metamodels. Consequently  it often seems relatively easy to chain them. the
output of a transformation may be given as input to a second one if metamodels
match. However  this simple rule has some obvious limitations. For instance  a
transformation may only use a subset of a metamodel. Therefore  chaining
transformations appropriately requires more information. We present here an
approach that automatically discovers more detailed information about actual
chaining constraints by statically analyzing transformations. The objective is
to provide developers who decide to chain transformations with more data on
which to base their choices. This approach has been successfully applied to the
case of a library of endogenous transformations. They all have the same source
and target metamodel but have some hidden chaining constraints. In such a case 
the simple metamodel matching rule given above does not provide any useful
information."
"This article presents the results of the research carried out on the
development of a medical diagnostic system applied to the Acute Bacterial
Meningitis  using the Case Based Reasoning methodology. The research was
focused on the implementation of the adaptation stage  from the integration of
Case Based Reasoning and Rule Based Expert Systems. In this adaptation stage we
use a higher level RBC that stores and allows reutilizing change experiences 
combined with a classic rule-based inference engine. In order to take into
account the most evident clinical situation  a pre-diagnosis stage is
implemented using a rule engine that  given an evident situation  emits the
corresponding diagnosis and avoids the complete process."
"Recent advancement in web services plays an important role in business to
business and business to consumer interaction. Discovery mechanism is not only
used to find a suitable service but also provides collaboration between service
providers and consumers by using standard protocols. A static web service
discovery mechanism is not only time consuming but requires continuous human
interaction. This paper proposed an efficient dynamic web services discovery
mechanism that can locate relevant and updated web services from service
registries and repositories with timestamp based on indexing value and
categorization for faster and efficient discovery of service. The proposed
prototype focuses on quality of service issues and introduces concept of local
cache  categorization of services  indexing mechanism  CSP (Constraint
Satisfaction Problem) solver  aging and usage of translator. Performance of
proposed framework is evaluated by implementing the algorithm and correctness
of our method is shown. The results of proposed framework shows greater
performance and accuracy in dynamic discovery mechanism of web services
resolving the existing issues of flexibility  scalability  based on quality of
service  and discovers updated and most relevant services with ease of usage."
"Fuzzy Description Logics (DLs) are a family of logics which allow the
representation of (and the reasoning with) structured knowledge affected by
vagueness. Although most of the not very expressive crisp DLs  such as ALC 
enjoy the Finite Model Property (FMP)  this is not the case once we move into
the fuzzy case. In this paper we show that if we allow arbitrary knowledge
bases  then the fuzzy DLs ALC under Lukasiewicz and Product fuzzy logics do not
verify the FMP even if we restrict to witnessed models; in other words  finite
satisfiability and witnessed satisfiability are different for arbitrary
knowledge bases. The aim of this paper is to point out the failure of FMP
because it affects several algorithms published in the literature for reasoning
under fuzzy ALC."
"The basic aim of our study is to give a possible model for handling uncertain
information. This model is worked out in the framework of DATALOG. At first the
concept of fuzzy Datalog will be summarized  then its extensions for
intuitionistic- and interval-valued fuzzy logic is given and the concept of
bipolar fuzzy Datalog is introduced. Based on these ideas the concept of
multivalued knowledge-base will be defined as a quadruple of any background
knowledge; a deduction mechanism; a connecting algorithm  and a function set of
the program  which help us to determine the uncertainty levels of the results.
At last a possible evaluation strategy is given."
"RefereeToolbox is a java package implementing combination operators for
fusing evidences. It is downloadable from.
http.//refereefunction.fredericdambreville.com/releases RefereeToolbox is based
on an interpretation of the fusion rules by means of Referee Functions. This
approach implies a dissociation between the definition of the combination and
its actual implementation  which is common to all referee-based combinations.
As a result  RefereeToolbox is designed with the aim to be generic and
evolutive."
"LEXSYS  (Legume Expert System) was a project conceived at IITA (International
Institute of Tropical Agriculture) Ibadan Nigeria. It was initiated by the
COMBS (Collaborative Group on Maize-Based Systems Research in the 1990. It was
meant for a general framework for characterizing on-farm testing for technology
design for sustainable cereal-based cropping system. LEXSYS is not a true
expert system as the name would imply  but simply a user-friendly information
system. This work is an attempt to give a formal representation of the existing
system and then present areas where intelligent agent can be applied."
"Computing value of information (VOI) is a crucial task in various aspects of
decision-making under uncertainty  such as in meta-reasoning for search; in
selecting measurements to make  prior to choosing a course of action; and in
managing the exploration vs. exploitation tradeoff. Since such applications
typically require numerous VOI computations during a single run  it is
essential that VOI be computed efficiently. We examine the issue of anytime
estimation of VOI  as frequently it suffices to get a crude estimate of the
VOI  thus saving considerable computational resources. As a case study  we
examine VOI estimation in the measurement selection problem. Empirical
evaluation of the proposed scheme in this domain shows that computational
resources can indeed be significantly reduced  at little cost in expected
rewards achieved in the overall decision problem."
"Formalism based on GA is an alternative to distributed representation models
developed so far --- Smolensky's tensor product  Holographic Reduced
Representations (HRR) and Binary Spatter Code (BSC). Convolutions are replaced
by geometric products  interpretable in terms of geometry which seems to be the
most natural language for visualization of higher concepts. This paper recalls
the main ideas behind the GA model and investigates recognition test results
using both inner product and a clipped version of matrix representation. The
influence of accidental blade equality on recognition is also studied. Finally 
the efficiency of the GA model is compared to that of previously developed
models."
"We report (to our knowledge) the first evaluation of Constraint Satisfaction
as a computational framework for solving closest string problems. We show that
careful consideration of symbol occurrences can provide search heuristics that
provide several orders of magnitude speedup at and above the optimal distance.
We also report (to our knowledge) the first analysis and evaluation -- using
any technique -- of the computational difficulties involved in the
identification of all closest strings for a given input set. We describe
algorithms for web-scale distributed solution of closest string problems  both
purely based on AI backtrack search and also hybrid numeric-AI methods."
"We consider the problem of jointly training structured models for extraction
from sources whose instances enjoy partial overlap. This has important
applications like user-driven ad-hoc information extraction on the web. Such
applications present new challenges in terms of the number of sources and their
arbitrary pattern of overlap not seen by earlier collective training schemes
applied on two sources. We present an agreement-based learning framework and
alternatives within it to trade-off tractability  robustness to noise  and
extent of agreement. We provide a principled scheme to discover low-noise
agreement sets in unlabeled data across the sources. Through extensive
experiments over 58 real datasets  we establish that our method of additively
rewarding agreement over maximal segments of text provides the best trade-offs 
and also scores over alternatives such as collective inference  staged
training  and multi-view learning."
"A technique to study the dynamics of solving of a research task is suggested.
The research task was based on specially developed software Right- Wrong
Responder (RWR)  with the participants having to reveal the response logic of
the program. The participants interacted with the program in the form of a
semi-binary dialogue  which implies the feedback responses of only two kinds -
""right"" or ""wrong"". The technique has been applied to a small pilot group of
volunteer participants. Some of them have successfully solved the task
(solvers) and some have not (non-solvers). In the beginning of the work  the
solvers did more wrong moves than non-solvers  and they did less wrong moves
closer to the finish of the work. A phase portrait of the work both in solvers
and non-solvers showed definite cycles that may correspond to sequences of
partially true hypotheses that may be formulated by the participants during the
solving of the task."
"This paper constructively proves the existence of an effective procedure
generating a computable (total) function that is not contained in any given
effectively enumerable set of such functions. The proof implies the existence
of machines that process informal concepts such as computable (total) functions
beyond the limits of any given Turing machine or formal system  that is  these
machines can  in a certain sense  ""compute"" function values beyond these
limits. We call these machines creative. We argue that any ""intelligent""
machine should be capable of processing informal concepts such as computable
(total) functions  that is  it should be creative. Finally  we introduce
hypotheses on creative machines which were developed on the basis of
theoretical investigations and experiments with computer programs. The
hypotheses say that machine intelligence is the execution of a self-developing
procedure starting from any universal programming language and any input."
"Mountain river torrents and snow avalanches generate human and material
damages with dramatic consequences. Knowledge about natural phenomenona is
often lacking and expertise is required for decision and risk management
purposes using multi-disciplinary quantitative or qualitative approaches.
Expertise is considered as a decision process based on imperfect information
coming from more or less reliable and conflicting sources. A methodology mixing
the Analytic Hierarchy Process (AHP)  a multi-criteria aid-decision method  and
information fusion using Belief Function Theory is described. Fuzzy Sets and
Possibilities theories allow to transform quantitative and qualitative criteria
into a common frame of discernment for decision in Dempster-Shafer Theory (DST
) and Dezert-Smarandache Theory (DSmT) contexts. Main issues consist in basic
belief assignments elicitation  conflict identification and management  fusion
rule choices  results validation but also in specific needs to make a
difference between importance and reliability and uncertainty in the fusion
process."
"A lot of mathematical knowledge has been formalized and stored in
repositories by now. different mathematical theorems and theories have been
taken into consideration and included in mathematical repositories.
Applications more distant from pure mathematics  however --- though based on
these theories --- often need more detailed knowledge about the underlying
theories. In this paper we present an example Mizar formalization from the area
of electrical engineering focusing on stability theory which is based on
complex analysis. We discuss what kind of special knowledge is necessary here
and which amount of this knowledge is included in existing repositories."
"It is hypothesized that creativity arises from the self-mending capacity of
an internal model of the world  or worldview. The uniquely honed worldview of a
creative individual results in a distinctive style that is recognizable within
and across domains. It is further hypothesized that creativity is domaingeneral
in the sense that there exist multiple avenues by which the distinctiveness of
one's worldview can be expressed. These hypotheses were tested using art
students and creative writing students. Art students guessed significantly
above chance both which painting was done by which of five famous artists  and
which artwork was done by which of their peers. Similarly  creative writing
students guessed significantly above chance both which passage was written by
which of five famous writers  and which passage was written by which of their
peers. These findings support the hypothesis that creative style is
recognizable. Moreover  creative writing students guessed significantly above
chance which of their peers produced particular works of art  supporting the
hypothesis that creative style is recognizable not just within but across
domains."
"Approximate dynamic programming has been used successfully in a large variety
of domains  but it relies on a small set of provided approximation features to
calculate solutions reliably. Large and rich sets of features can cause
existing algorithms to overfit because of a limited number of samples. We
address this shortcoming using $L_1$ regularization in approximate linear
programming. Because the proposed method can automatically select the
appropriate richness of features  its performance does not degrade with an
increasing number of features. These results rely on new and stronger sampling
bounds for regularized approximate linear programs. We also propose a
computationally efficient homotopy method. The empirical evaluation of the
approach shows that the proposed method performs well on simple MDPs and
standard benchmark problems."
"We discuss how to use a Genetic Regulatory Network as an evolutionary
representation to solve a typical GP reinforcement problem  the pole balancing.
The network is a modified version of an Artificial Regulatory Network proposed
a few years ago  and the task could be solved only by finding a proper way of
connecting inputs and outputs to the network. We show that the representation
is able to generalize well over the problem domain  and discuss the performance
of different models of this kind."
"Programs to solve so-called constraint problems are complex pieces of
software which require many design decisions to be made more or less
arbitrarily by the implementer. These decisions affect the performance of the
finished solver significantly. Once a design decision has been made  it cannot
easily be reversed  although a different decision may be more appropriate for a
particular problem.
  We investigate using machine learning to make these decisions automatically
depending on the problem to solve with the alldifferent constraint as an
example. Our system is capable of making non-trivial  multi-level decisions
that improve over always making a default choice."
"In this paper the author presents a kind of Soft Computing Technique  mainly
an application of fuzzy set theory of Prof. Zadeh [16]  on a problem of Medical
Experts Systems. The choosen problem is on design of a physician's decision
model which can take crisp as well as fuzzy data as input  unlike the
traditional models. The author presents a mathematical model based on fuzzy set
theory for physician aided evaluation of a complete representation of
information emanating from the initial interview including patient past
history  present symptoms  and signs observed upon physical examination and
results of clinical and diagnostic tests."
"An important problem in computational social choice theory is the complexity
of undesirable behavior among agents  such as control  manipulation  and
bribery in election systems. These kinds of voting strategies are often
tempting at the individual level but disastrous for the agents as a whole.
Creating election systems where the determination of such strategies is
difficult is thus an important goal.
  An interesting set of elections is that of scoring protocols. Previous work
in this area has demonstrated the complexity of misuse in cases involving a
fixed number of candidates  and of specific election systems on unbounded
number of candidates such as Borda. In contrast  we take the first step in
generalizing the results of computational complexity of election misuse to
cases of infinitely many scoring protocols on an unbounded number of
candidates. Interesting families of systems include $k$-approval and $k$-veto
elections  in which voters distinguish $k$ candidates from the candidate set.
  Our main result is to partition the problems of these families based on their
complexity. We do so by showing they are polynomial-time computable  NP-hard 
or polynomial-time equivalent to another problem of interest. We also
demonstrate a surprising connection between manipulation in election systems
and some graph theory problems."
"In the last two decades  a number of methods have been proposed for
forecasting based on fuzzy time series. Most of the fuzzy time series methods
are presented for forecasting of car road accidents. However  the forecasting
accuracy rates of the existing methods are not good enough. In this paper  we
compared our proposed new method of fuzzy time series forecasting with existing
methods. Our method is based on means based partitioning of the historical data
of car road accidents. The proposed method belongs to the kth order and
time-variant methods. The proposed method can get the best forecasting accuracy
rate for forecasting the car road accidents than the existing methods."
"In this paper  a new learning algorithm for adaptive network intrusion
detection using naive Bayesian classifier and decision tree is presented  which
performs balance detections and keeps false positives at acceptable level for
different types of network attacks  and eliminates redundant attributes as well
as contradictory examples from training data that make the detection model
complex. The proposed algorithm also addresses some difficulties of data mining
such as handling continuous attribute  dealing with missing attribute values 
and reducing noise in training data. Due to the large volumes of security audit
data as well as the complex and dynamic properties of intrusion behaviours 
several data miningbased intrusion detection techniques have been applied to
network-based traffic data and host-based data in the last decades. However 
there remain various issues needed to be examined towards current intrusion
detection systems (IDS). We tested the performance of our proposed algorithm
with existing learning algorithms by employing on the KDD99 benchmark intrusion
detection dataset. The experimental results prove that the proposed algorithm
achieved high detection rates (DR) and significant reduce false positives (FP)
for different types of network intrusions using limited computational
resources."
"This paper presents a combination of several automated reasoning and proof
presentation tools with the Mizar system for formalization of mathematics. The
combination forms an online service called MizAR  similar to the SystemOnTPTP
service for first-order automated reasoning. The main differences to
SystemOnTPTP are the use of the Mizar language that is oriented towards human
mathematicians (rather than the pure first-order logic used in SystemOnTPTP) 
and setting the service in the context of the large Mizar Mathematical Library
of previous theorems definitions  and proofs (rather than the isolated problems
that are solved in SystemOnTPTP). These differences poses new challenges and
new opportunities for automated reasoning and for proof presentation tools.
This paper describes the overall structure of MizAR  and presents the automated
reasoning systems and proof presentation tools that are combined to make MizAR
a useful mathematical service."
"Structured and semi-structured data describing entities  taxonomies and
ontologies appears in many domains. There is a huge interest in integrating
structured information from multiple sources; however integrating structured
data to infer complex common structures is a difficult task because the
integration must aggregate similar structures while avoiding structural
inconsistencies that may appear when the data is combined. In this work  we
study the integration of structured social metadata. shallow personal
hierarchies specified by many individual users on the SocialWeb  and focus on
inferring a collection of integrated  consistent taxonomies. We frame this task
as an optimization problem with structural constraints. We propose a new
inference algorithm  which we refer to as Relational Affinity Propagation (RAP)
that extends affinity propagation (Frey and Dueck 2007) by introducing
structural constraints. We validate the approach on a real-world social media
dataset  collected from the photosharing website Flickr. Our empirical results
show that our proposed approach is able to construct deeper and denser
structures compared to an approach using only the standard affinity propagation
algorithm."
"The paper offers a mathematical formalization of the Turing test. This
formalization makes it possible to establish the conditions under which some
Turing machine will pass the Turing test and the conditions under which every
Turing machine (or every Turing machine of the special class) will fail the
Turing test."
"Many social Web sites allow users to annotate the content with descriptive
metadata  such as tags  and more recently to organize content hierarchically.
These types of structured metadata provide valuable evidence for learning how a
community organizes knowledge. For instance  we can aggregate many personal
hierarchies into a common taxonomy  also known as a folksonomy  that will aid
users in visualizing and browsing social content  and also to help them in
organizing their own content. However  learning from social metadata presents
several challenges  since it is sparse  shallow  ambiguous  noisy  and
inconsistent. We describe an approach to folksonomy learning based on
relational clustering  which exploits structured metadata contained in personal
hierarchies. Our approach clusters similar hierarchies using their structure
and tag statistics  then incrementally weaves them into a deeper  bushier tree.
We study folksonomy learning using social metadata extracted from the
photo-sharing site Flickr  and demonstrate that the proposed approach addresses
the challenges. Moreover  comparing to previous work  the approach produces
larger  more accurate folksonomies  and in addition  scales better."
"Symmetry is an important feature of many constraint programs. We show that
any problem symmetry acting on a set of symmetry breaking constraints can be
used to break symmetry. Different symmetries pick out different solutions in
each symmetry class. This simple but powerful idea can be used in a number of
different ways. We describe one application within model restarts  a search
technique designed to reduce the conflict between symmetry breaking and the
branching heuristic. In model restarts  we restart search periodically with a
random symmetry of the symmetry breaking constraints. Experimental results show
that this symmetry breaking technique is effective in practice on some standard
benchmark problems."
"We propose automatically learning probabilistic Hierarchical Task Networks
(pHTNs) in order to capture a user's preferences on plans  by observing only
the user's behavior. HTNs are a common choice of representation for a variety
of purposes in planning  including work on learning in planning. Our
contributions are (a) learning structure and (b) representing preferences. In
contrast  prior work employing HTNs considers learning method preconditions
(instead of structure) and representing domain physics or search control
knowledge (rather than preferences). Initially we will assume that the observed
distribution of plans is an accurate representation of user preference  and
then generalize to the situation where feasibility constraints frequently
prevent the execution of preferred plans. In order to learn a distribution on
plans we adapt an Expectation-Maximization (EM) technique from the discipline
of (probabilistic) grammar induction  taking the perspective of task reductions
as productions in a context-free grammar over primitive actions. To account for
the difference between the distributions of possible and preferred plans we
subsequently modify this core EM technique  in short  by rescaling its input."
"Brain-Like Stochastic Search (BLiSS) refers to this task. given a family of
utility functions U(u A)  where u is a vector of parameters or task
descriptors  maximize or minimize U with respect to u  using networks (Option
Nets) which input A and learn to generate good options u stochastically. This
paper discusses why this is crucial to brain-like intelligence (an area funded
by NSF) and to many applications  and discusses various possibilities for
network design and training. The appendix discusses recent research  relations
to work on stochastic optimization in operations research  and relations to
engineering-based approaches to understanding neocortex."
"We introduce a framework for representing a variety of interesting problems
as inference over the execution of probabilistic model programs. We represent a
""solution"" to such a problem as a guide program which runs alongside the model
program and influences the model program's random choices  leading the model
program to sample from a different distribution than from its priors. Ideally
the guide program influences the model program to sample from the posteriors
given the evidence. We show how the KL- divergence between the true posterior
distribution and the distribution induced by the guided model program can be
efficiently estimated (up to an additive constant) by sampling multiple
executions of the guided model program. In addition  we show how to use the
guide program as a proposal distribution in importance sampling to
statistically prove lower bounds on the probability of the evidence and on the
probability of a hypothesis and the evidence. We can use the quotient of these
two bounds as an estimate of the conditional probability of the hypothesis
given the evidence. We thus turn the inference problem into a heuristic search
for better guide programs."
"The basic unit of meaning on the Semantic Web is the RDF statement  or
triple  which combines a distinct subject  predicate and object to make a
definite assertion about the world. A set of triples constitutes a graph  to
which they give a collective meaning. It is upon this simple foundation that
the rich  complex knowledge structures of the Semantic Web are built. Yet the
very expressiveness of RDF  by inviting comparison with real-world knowledge 
highlights a fundamental shortcoming  in that RDF is limited to statements of
absolute fact  independent of the context in which a statement is asserted.
This is in stark contrast with the thoroughly context-sensitive nature of human
thought. The model presented here provides a particularly simple means of
contextualizing an RDF triple by associating it with related statements in the
same graph. This approach  in combination with a notion of graph similarity  is
sufficient to select only those statements from an RDF graph which are
subjectively most relevant to the context of the requesting process."
"In this Information system age many organizations consider information system
as their weapon to compete or gain competitive advantage or give the best
services for non profit organizations. Game Information System as combining
Information System and game is breakthrough to achieve organizations'
performance. The Game Information System will run the Information System with
game and how game can be implemented to run the Information System. Game is not
only for fun and entertainment  but will be a challenge to combine fun and
entertainment with Information System. The Challenge to run the information
system with entertainment  deliver the entertainment with information system
all at once. Game information system can be implemented in many sectors as like
the information system itself but in difference's view. A view of game which
people can joy and happy and do their transaction as a fun things."
"In order to get strategic positioning for competition in business
organization  the information system must be ahead in this information age
where the information as one of the weapons to win the competition and in the
right hand the information will become a right bullet. The information system
with the information technology support isn't enough if just only on internet
or implemented with internet technology. The growth of information technology
as tools for helping and making people easy to use must be accompanied by
wanting to make fun and happy when they make contact with the information
technology itself. Basically human like to play  since childhood human have
been playing  free and happy and when human grow up they can't play as much as
when human was in their childhood. We have to develop the information system
which is not perform information system itself but can help human to explore
their natural instinct for playing  making fun and happiness when they interact
with the information system. Virtual information system is the way to present
playing and having fun atmosphere on working area."
"Earthquake DSS is an information technology environment which can be used by
government to sharpen  make faster and better the earthquake mitigation
decision. Earthquake DSS can be delivered as E-government which is not only for
government itself but in order to guarantee each citizen's rights for
education  training and information about earthquake and how to overcome the
earthquake. Knowledge can be managed for future use and would become mining by
saving and maintain all the data and information about earthquake and
earthquake mitigation in Indonesia. Using Web technology will enhance global
access and easy to use. Datawarehouse as unNormalized database for
multidimensional analysis will speed the query process and increase reports
variation. Link with other Disaster DSS in one national disaster DSS  link with
other government information system and international will enhance the
knowledge and sharpen the reports."
"Markov decision processes (MDPs) are widely used for modeling decision-making
problems in robotics  automated control  and economics. Traditional MDPs assume
that the decision maker (DM) knows all states and actions. However  this may
not be true in many situations of interest. We define a new framework  MDPs
with unawareness (MDPUs) to deal with the possibilities that a DM may not be
aware of all possible actions. We provide a complete characterization of when a
DM can learn to play near-optimally in an MDPU  and give an algorithm that
learns to play near-optimally when it is possible to do so  as efficiently as
possible. In particular  we characterize when a near-optimal solution can be
found in polynomial time."
"Existing value function approximation methods have been successfully used in
many applications  but they often lack useful a priori error bounds. We propose
a new approximate bilinear programming formulation of value function
approximation  which employs global optimization. The formulation provides
strong a priori guarantees on both robust and expected policy loss by
minimizing specific norms of the Bellman residual. Solving a bilinear program
optimally is NP-hard  but this is unavoidable because the Bellman-residual
minimization itself is NP-hard. We describe and analyze both optimal and
approximate algorithms for solving bilinear programs. The analysis shows that
this algorithm offers a convergent generalization of approximate policy
iteration. We also briefly analyze the behavior of bilinear programming
algorithms under incomplete samples. Finally  we demonstrate that the proposed
approach can consistently minimize the Bellman residual on simple benchmark
problems."
"Different notions of equivalence  such as the prominent notions of strong and
uniform equivalence  have been studied in Answer-Set Programming  mainly for
the purpose of identifying programs that can serve as substitutes without
altering the semantics  for instance in program optimization. Such semantic
comparisons are usually characterized by various selections of models in the
logic of Here-and-There (HT). For uniform equivalence however  correct
characterizations in terms of HT-models can only be obtained for finite
theories  respectively programs. In this article  we show that a selection of
countermodels in HT captures uniform equivalence also for infinite theories.
This result is turned into coherent characterizations of the different notions
of equivalence by countermodels  as well as by a mixture of HT-models and
countermodels (so-called equivalence interpretations). Moreover  we generalize
the so-called notion of relativized hyperequivalence for programs to
propositional theories  and apply the same methodology in order to obtain a
semantic characterization which is amenable to infinite settings. This allows
for a lifting of the results to first-order theories under a very general
semantics given in terms of a quantified version of HT. We thus obtain a
general framework for the study of various notions of equivalence for theories
under answer-set semantics. Moreover  we prove an expedient property that
allows for a simplified treatment of extended signatures  and provide further
results for non-ground logic programs. In particular  uniform equivalence
coincides under open and ordinary answer-set semantics  and for finite
non-ground programs under these semantics  also the usual characterization of
uniform equivalence in terms of maximal and total HT-models of the grounding is
correct  even for infinite domains  when corresponding ground programs are
infinite."
"Human disease diagnosis is a complicated process and requires high level of
expertise. Any attempt of developing a web-based expert system dealing with
human disease diagnosis has to overcome various difficulties. This paper
describes a project work aiming to develop a web-based fuzzy expert system for
diagnosing human diseases. Now a days fuzzy systems are being used successfully
in an increasing number of application areas; they use linguistic rules to
describe systems. This research project focuses on the research and development
of a web-based clinical tool designed to improve the quality of the exchange of
health information between health care professionals and patients.
Practitioners can also use this web-based tool to corroborate diagnosis. The
proposed system is experimented on various scenarios in order to evaluate it's
performance. In all the cases  proposed system exhibits satisfactory results."
"In the area of computer science focusing on creating machines that can engage
on behaviors that humans consider intelligent. The ability to create
intelligent machines has intrigued humans since ancient times and today with
the advent of the computer and 50 years of research into various programming
techniques  the dream of smart machines is becoming a reality. Researchers are
creating systems which can mimic human thought  understand speech  beat the
best human chessplayer  and countless other feats never before possible.
Ability of the human to estimate the information is most brightly shown in
using of natural languages. Using words of a natural language for valuation
qualitative attributes  for example  the person pawns uncertainty in form of
vagueness in itself estimations. Vague sets  vague judgments  vague conclusions
takes place there and then  where and when the reasonable subject exists and
also is interested in something. The vague sets theory has arisen as the answer
to an illegibility of language the reasonable subject speaks. Language of a
reasonable subject is generated by vague events which are created by the reason
and which are operated by the mind. The theory of vague sets represents an
attempt to find such approximation of vague grouping which would be more
convenient  than the classical theory of sets in situations where the natural
language plays a significant role. Such theory has been offered by known
American mathematician Gau and Buehrer .In our paper we are describing how
vagueness of linguistic variables can be solved by using the vague set
theory.This paper is mainly designed for one of directions of the eventology
(the theory of the random vague events)  which has arisen within the limits of
the probability theory and which pursue the unique purpose to describe
eventologically a movement of reason."
"Ontologies usually suffer from the semantic heterogeneity when simultaneously
used in information sharing  merging  integrating and querying processes.
Therefore  the similarity identification between ontologies being used becomes
a mandatory task for all these processes to handle the problem of semantic
heterogeneity. In this paper  we propose an efficient technique for similarity
measurement between two ontologies. The proposed technique identifies all
candidate pairs of similar concepts without omitting any similar pair. The
proposed technique can be used in different types of operations on ontologies
such as merging  mapping and aligning. By analyzing its results a reasonable
improvement in terms of completeness  correctness and overall quality of the
results has been found."
"Many formal languages have been proposed to express or represent Ontologies 
including RDF  RDFS  DAML+OIL and OWL. Most of these languages are based on XML
syntax  but with various terminologies and expressiveness. Therefore  choosing
a language for building an Ontology is the main step. The main point of
choosing language to represent Ontology is based mainly on what the Ontology
will represent or be used for. That language should have a range of quality
support features such as ease of use  expressive power  compatibility  sharing
and versioning  internationalisation. This is because different kinds of
knowledge-based applications need different language features. The main
objective of these languages is to add semantics to the existing information on
the web. The aims of this paper is to provide a good knowledge of existing
language and understanding of these languages and how could be used."
"Semantic Web is actually an extension of the current one in that it
represents information more meaningfully for humans and computers alike. It
enables the description of contents and services in machine-readable form  and
enables annotating  discovering  publishing  advertising and composing services
to be automated. It was developed based on Ontology  which is considered as the
backbone of the Semantic Web. In other words  the current Web is transformed
from being machine-readable to machine-understandable. In fact  Ontology is a
key technique with which to annotate semantics and provide a common 
comprehensible foundation for resources on the Semantic Web. Moreover  Ontology
can provide a common vocabulary  a grammar for publishing data  and can supply
a semantic description of data which can be used to preserve the Ontologies and
keep them ready for inference. This paper provides basic concepts of web
services and the Semantic Web  defines the structure and the main applications
of ontology  and provides many relevant terms are explained in order to provide
a basic understanding of ontologies."
"Finding the structure of a graphical model has been received much attention
in many fields. Recently  it is reported that the non-Gaussianity of data
enables us to identify the structure of a directed acyclic graph without any
prior knowledge on the structure. In this paper  we propose a novel
non-Gaussianity based algorithm for more general type of models; chain graphs.
The algorithm finds an ordering of the disjoint subsets of variables by
iteratively evaluating the independence between the variable subset and the
residuals when the remaining variables are regressed on those. However  its
computational cost grows exponentially according to the number of variables.
Therefore  we further discuss an efficient approximate approach for applying
the algorithm to large sized graphs. We illustrate the algorithm with
artificial and real-world datasets."
"Notions of core  support and inversion of a soft set have been defined and
studied. Soft approximations are soft sets developed through core and support 
and are used for granulating the soft space. Membership structure of a soft set
has been probed in and many interesting properties presented. The mathematical
apparatus developed so far in this paper yields a detailed analysis of two
works viz. [N. Cagman  S. Enginoglu  Soft set theory and uni-int decision
making  European Jr. of Operational Research (article in press  available
online 12 May 2010)] and [N. Cagman  S. Enginoglu  Soft matrix theory and its
decision making  Computers and Mathematics with Applications 59 (2010) 3308 -
3314.]. We prove (Theorem 8.1) that uni-int method of Cagman is equivalent to a
core-support expression which is computationally far less expansive than
uni-int. This also highlights some shortcomings in Cagman's uni-int method and
thus motivates us to improve the method. We first suggest an improvement in
uni-int method and then present a new conjecture to solve the optimum choice
problem given by Cagman and Enginoglu. Our Example 8.6 presents a case where
the optimum choice is intuitively clear yet both uni-int methods (Cagman's and
our improved one) give wrong answer but the new conjecture solves the problem
correctly."
"In recent years there has been growing interest in solutions for the delivery
of clinical care for the elderly  due to the large increase in aging
population. Monitoring a patient in his home environment is necessary to ensure
continuity of care in home settings  but  to be useful  this activity must not
be too invasive for patients and a burden for caregivers. We prototyped a
system called SINDI (Secure and INDependent lIving)  focused on i) collecting a
limited amount of data about the person and the environment through Wireless
Sensor Networks (WSN)  and ii) inferring from these data enough information to
support caregivers in understanding patients' well being and in predicting
possible evolutions of their health. Our hierarchical logic-based model of
health combines data from different sources  sensor data  tests results 
common-sense knowledge and patient's clinical profile at the lower level  and
correlation rules between health conditions across upper levels. The logical
formalization and the reasoning process are based on Answer Set Programming.
The expressive power of this logic programming paradigm makes it possible to
reason about health evolution even when the available information is incomplete
and potentially incoherent  while declarativity simplifies rules specification
by caregivers and allows automatic encoding of knowledge. This paper describes
how these issues have been targeted in the application scenario of the SINDI
system."
"The technical report presents a generic exact solution approach for
minimizing the project duration of the resource-constrained project scheduling
problem with generalized precedences (Rcpsp/max). The approach uses lazy clause
generation  i.e.  a hybrid of finite domain and Boolean satisfiability solving 
in order to apply nogood learning and conflict-driven search on the solution
generation. Our experiments show the benefit of lazy clause generation for
finding an optimal solutions and proving its optimality in comparison to other
state-of-the-art exact and non-exact methods. The method is highly robust. it
matched or bettered the best known results on all of the 2340 instances we
examined except 3  according to the currently available data on the PSPLib. Of
the 631 open instances in this set it closed 573 and improved the bounds of 51
of the remaining 58 instances."
"The search strategy of a CP solver is determined by the variable and value
ordering heuristics it employs and by the branching scheme it follows. Although
the effects of variable and value ordering heuristics on search effort have
been widely studied  the effects of different branching schemes have received
less attention. In this paper we study this effect through an experimental
evaluation that includes standard branching schemes such as 2-way  d-way  and
dichotomic domain splitting  as well as variations of set branching where
branching is performed on sets of values. We also propose and evaluate a
generic approach to set branching where the partition of a domain into sets is
created using the scores assigned to values by a value ordering heuristic  and
a clustering algorithm from machine learning. Experimental results demonstrate
that although exponential differences between branching schemes  as predicted
in theory between 2-way and d-way branching  are not very common  still the
choice of branching scheme can make quite a difference on certain classes of
problems. Set branching methods are very competitive with 2-way branching and
outperform it on some problem classes. A statistical analysis of the results
reveals that our generic clustering-based set branching method is the best
among the methods compared."
"In this paper  we address the problem of creating believable agents (virtual
characters) in video games. We consider only one meaning of believability 
``giving the feeling of being controlled by a player''  and outline the problem
of its evaluation. We present several models for agents in games which can
produce believable behaviours  both from industry and research. For high level
of believability  learning and especially imitation learning seems to be the
way to go. We make a quick overview of different approaches to make video
games' agents learn from players. To conclude we propose a two-step method to
develop new models for believable agents. First we must find the criteria for
believability for our application and define an evaluation method. Then the
model and the learning algorithm can be designed."
"Classic evaluation methods of believable agents are time-consuming because
they involve many human to judge agents. They are well suited to validate work
on new believable behaviours models. However  during the implementation 
numerous experiments can help to improve agents' believability. We propose a
method which aim at assessing how much an agent's behaviour looks like humans'
behaviours. By representing behaviours with vectors  we can store data computed
for humans and then evaluate as many agents as needed without further need of
humans. We present a test experiment which shows that even a simple evaluation
following our method can reveal differences between quite believable agents and
humans. This method seems promising although  as shown in our experiment 
results' analysis can be difficult."
"In this short paper I briefly discuss 3D war Game based on artificial
intelligence concepts called AI WAR. Going in to the details  I present the
importance of CAICL language and how this language is used in AI WAR. Moreover
I also present a designed and implemented 3D War Cybug for AI WAR using CAICL
and discus the implemented strategy to defeat its enemies during the game life."
"Levesque introduced the notion of only-knowing to precisely capture the
beliefs of a knowledge base. He also showed how only-knowing can be used to
formalize non-monotonic behavior within a monotonic logic. Despite its appeal 
all attempts to extend only-knowing to the many agent case have undesirable
properties. A belief model by Halpern and Lakemeyer  for instance  appeals to
proof-theoretic constructs in the semantics and needs to axiomatize validity as
part of the logic. It is also not clear how to generalize their ideas to a
first-order case. In this paper  we propose a new account of multi-agent
only-knowing which  for the first time  has a natural possible-world semantics
for a quantified language with equality. We then provide  for the propositional
fragment  a sound and complete axiomatization that faithfully lifts Levesque's
proof theory to the many agent case. We also discuss comparisons to the earlier
approach by Halpern and Lakemeyer."
"In this paper we present an optimal Bangla Keyboard Layout  which distributes
the load equally on both hands so that maximizing the ease and minimizing the
effort. Bangla alphabet has a large number of letters  for this it is difficult
to type faster using Bangla keyboard. Our proposed keyboard will maximize the
speed of operator as they can type with both hands parallel. Here we use the
association rule of data mining to distribute the Bangla characters in the
keyboard. First  we analyze the frequencies of data consisting of monograph 
digraph and trigraph  which are derived from data wire-house  and then used
association rule of data mining to distribute the Bangla characters in the
layout. Finally  we propose a Bangla Keyboard Layout. Experimental results on
several keyboard layout shows the effectiveness of the proposed approach with
better performance."
"This paper presents an optimal Bangla Keyboard Layout  which distributes the
load equally on both hands so that maximizing the ease and minimizing the
effort. Bangla alphabet has a large number of letters  for this it is difficult
to type faster using Bangla keyboard. Our proposed keyboard will maximize the
speed of operator as they can type with both hands parallel. Here we use the
association rule of data mining to distribute the Bangla characters in the
keyboard. First  we analyze the frequencies of data consisting of monograph 
digraph and trigraph  which are derived from data wire-house  and then used
association rule of data mining to distribute the Bangla characters in the
layout. Experimental results on several data show the effectiveness of the
proposed approach with better performance."
"Bangla alphabet has a large number of letters  for this it is complicated to
type faster using Bangla keyboard. The proposed keyboard will maximize the
speed of operator as they can type with both hands parallel. Association rule
of data mining to distribute the Bangla characters in the keyboard is used
here. The frequencies of data consisting of monograph  digraph and trigraph are
analyzed  which are derived from data wire-house  and then used association
rule of data mining to distribute the Bangla characters in the layout.
Experimental results on several data show the effectiveness of the proposed
approach with better performance. This paper presents an optimal Bangla
Keyboard Layout  which distributes the load equally on both hands so that
maximizing the ease and minimizing the effort."
"Support Vector Machines (SVMs) are popular tools for data mining tasks such
as classification  regression  and density estimation. However  original SVM
(C-SVM) only considers local information of data points on or over the margin.
Therefore  C-SVM loses robustness. To solve this problem  one approach is to
translate (i.e.  to move without rotation or change of shape) the hyperplane
according to the distribution of the entire data. But existing work can only be
applied for 1-D case. In this paper  we propose a simple and efficient method
called General Scaled SVM (GS-SVM) to extend the existing approach to
multi-dimensional case. Our method translates the hyperplane according to the
distribution of data projected on the normal vector of the hyperplane. Compared
with C-SVM  GS-SVM has better performance on several data sets."
"The problem of measuring similarity of graphs and their nodes is important in
a range of practical problems. There is a number of proposed measures  some of
them being based on iterative calculation of similarity between two graphs and
the principle that two nodes are as similar as their neighbors are. In our
work  we propose one novel method of that sort  with a refined concept of
similarity of two nodes that involves matching of their neighbors. We prove
convergence of the proposed method and show that it has some additional
desirable properties that  to our knowledge  the existing methods lack. We
illustrate the method on two specific problems and empirically compare it to
other methods."
"The paper proposes artificial intelligence technique called hill climbing to
find numerical solutions of Diophantine Equations. Such equations are important
as they have many applications in fields like public key cryptography  integer
factorization  algebraic curves  projective curves and data dependency in super
computers. Importantly  it has been proved that there is no general method to
find solutions of such equations. This paper is an attempt to find numerical
solutions of Diophantine equations using steepest ascent version of Hill
Climbing. The method  which uses tree representation to depict possible
solutions of Diophantine equations  adopts a novel methodology to generate
successors. The heuristic function used help to make the process of finding
solution as a minimization process. The work illustrates the effectiveness of
the proposed methodology using a class of Diophantine equations given by a1. x1
p1 + a2. x2 p2 + ...... + an . xn pn = N where ai and N are integers. The
experimental results validate that the procedure proposed is successful in
finding solutions of Diophantine Equations with sufficiently large powers and
large number of variables."
"This paper is mainly concerned with the question of how to decompose
multiclass classification problems into binary subproblems. We extend known
Jensen-Shannon bounds on the Bayes risk of binary problems to hierarchical
multiclass problems and use these bounds to develop a heuristic procedure for
constructing hierarchical multiclass decomposition for multinomials. We test
our method and compare it to the well known ""all-pairs"" decomposition. Our
tests are performed using a new authorship determination benchmark test of
machine learning authors. The new method consistently outperforms the all-pairs
decomposition when the number of classes is small and breaks even on larger
multiclass problems. Using both methods  the classification accuracy we
achieve  using an SVM over a feature set consisting of both high frequency
single tokens and high frequency token-pairs  appears to be exceptionally high
compared to known results in authorship determination."
"The iDian (previously named as the Operation Agent System) is a framework
designed to enable computer users to operate software in natural language.
Distinct from current speech-recognition systems  our solution supports
format-free combinations of orders  and is open to both developers and
customers. We used a multi-layer structure to build the entire framework 
approached rule-based natural language processing  and implemented demos
narrowing down to Windows  text-editing and a few other applications. This
essay will firstly give an overview of the entire system  and then scrutinize
the functions and structure of the system  and finally discuss the prospective
de-velopment  esp. on-line interaction functions."
"In this work we present a protocol for self-synchronized duty-cycling in
wireless sensor networks with energy harvesting capabilities. The protocol is
implemented in Wiselib  a library of generic algorithms for sensor networks.
Simulations are conducted with the sensor network simulator Shawn. They are
based on the specifications of real hardware known as iSense sensor nodes. The
experimental results show that the proposed mechanism is able to adapt to
changing energy availabilities. Moreover  it is shown that the system is very
robust against packet loss."
"Active Learning Method (ALM) is a soft computing method used for modeling and
control based on fuzzy logic. All operators defined for fuzzy sets must serve
as either fuzzy S-norm or fuzzy T-norm. Despite being a powerful modeling
method  ALM does not possess operators which serve as S-norms and T-norms which
deprive it of a profound analytical expression/form. This paper introduces two
new operators based on morphology which satisfy the following conditions.
First  they serve as fuzzy S-norm and T-norm. Second  they satisfy Demorgans
law  so they complement each other perfectly. These operators are investigated
via three viewpoints. Mathematics  Geometry and fuzzy logic."
"Substitutability  interchangeability and related concepts in Constraint
Programming were introduced approximately twenty years ago and have given rise
to considerable subsequent research. We survey this work  classify  and relate
the different concepts  and indicate directions for future work  in particular
with respect to making connections with research into symmetry breaking. This
paper is a condensed version of a larger work in progress."
"Concept drift refers to a non stationary learning problem over time. The
training and the application data often mismatch in real life problems. In this
report we present a context of concept drift problem 1. We focus on the issues
relevant to adaptive training set formation. We present the framework and
terminology  and formulate a global picture of concept drift learners design.
We start with formalizing the framework for the concept drifting data in
Section 1. In Section 2 we discuss the adaptivity mechanisms of the concept
drift learners. In Section 3 we overview the principle mechanisms of concept
drift learners. In this chapter we give a general picture of the available
algorithms and categorize them based on their properties. Section 5 discusses
the related research fields and Section 5 groups and presents major concept
drift applications. This report is intended to give a bird's view of concept
drift research field  provide a context of the research and position it within
broad spectrum of research fields and applications."
"We introduce a new perspective on spectral dimensionality reduction which
views these methods as Gaussian Markov random fields (GRFs). Our unifying
perspective is based on the maximum entropy principle which is in turn inspired
by maximum variance unfolding. The resulting model  which we call maximum
entropy unfolding (MEU) is a nonlinear generalization of principal component
analysis. We relate the model to Laplacian eigenmaps and isomap. We show that
parameter fitting in the locally linear embedding (LLE) is approximate maximum
likelihood MEU. We introduce a variant of LLE that performs maximum likelihood
exactly. Acyclic LLE (ALLE). We show that MEU and ALLE are competitive with the
leading spectral approaches on a robot navigation visualization and a human
motion capture data set. Finally the maximum likelihood perspective allows us
to introduce a new approach to dimensionality reduction based on L1
regularization of the Gaussian random field via the graphical lasso."
"Human can be distinguished by different limb movements and unique ground
reaction force. Cumulative foot pressure image is a 2-D cumulative ground
reaction force during one gait cycle. Although it contains pressure spatial
distribution information and pressure temporal distribution information  it
suffers from several problems including different shoes and noise  when putting
it into practice as a new biometric for pedestrian identification. In this
paper  we propose a hierarchical translation-invariant representation for
cumulative foot pressure images  inspired by the success of Convolutional deep
belief network for digital classification. Key contribution in our approach is
discriminative hierarchical sparse coding scheme which helps to learn useful
discriminative high-level visual features. Based on the feature representation
of cumulative foot pressure images  we develop a pedestrian recognition system
which is invariant to three different shoes and slight local shape change.
Experiments are conducted on a proposed open dataset that contains more than
2800 cumulative foot pressure images from 118 subjects. Evaluations suggest the
effectiveness of the proposed method and the potential of cumulative foot
pressure images as a biometric."
"Current work in planning with preferences assume that the user's preference
models are completely specified and aim to search for a single solution plan.
In many real-world planning scenarios  however  the user probably cannot
provide any information about her desired plans  or in some cases can only
express partial preferences. In such situations  the planner has to present not
only one but a set of plans to the user  with the hope that some of them are
similar to the plan she prefers. We first propose the usage of different
measures to capture quality of plan sets that are suitable for such scenarios.
domain-independent distance measures defined based on plan elements (actions 
states  causal links) if no knowledge of the user's preferences is given  and
the Integrated Convex Preference measure in case the user's partial preference
is provided. We then investigate various heuristic approaches to find set of
plans according to these measures  and present empirical results demonstrating
the promise of our approach."
"Performing effective preference-based data retrieval requires detailed and
preferentially meaningful structurized information about the current user as
well as the items under consideration. A common problem is that representations
of items often only consist of mere technical attributes  which do not resemble
human perception. This is particularly true for integral items such as movies
or songs. It is often claimed that meaningful item features could be extracted
from collaborative rating data  which is becoming available through social
networking services. However  there is only anecdotal evidence supporting this
claim; but if it is true  the extracted information could very valuable for
preference-based data retrieval. In this paper  we propose a methodology to
systematically check this common claim. We performed a preliminary
investigation on a large collection of movie ratings and present initial
evidence."
"We suggest a procedure that is relevant both to electronic performance and
human psychology  so that the creative logic and the respect for human nature
appear in a good agreement. The idea is to create an electronic card containing
basic information about a person's psychological behavior in order to make it
possible to quickly decide about the suitability of one for another. This
""psychological electronics"" approach could be tested via student projects."
"Meaning negotiation (MN) is the general process with which agents reach an
agreement about the meaning of a set of terms. Artificial Intelligence scholars
have dealt with the problem of MN by means of argumentations schemes  beliefs
merging and information fusion operators  and ontology alignment but the
proposed approaches depend upon the number of participants. In this paper  we
give a general model of MN for an arbitrary number of agents  in which each
participant discusses with the others her viewpoint by exhibiting it in an
actual set of constraints on the meaning of the negotiated terms. We call this
presentation of individual viewpoints an angle. The agents do not aim at
forming a common viewpoint but  instead  at agreeing about an acceptable common
angle. We analyze separately the process of MN by two agents (\emph{bilateral}
or \emph{pairwise} MN) and by more than two agents (\emph{multiparty} MN)  and
we use game theoretic models to understand how the process develops in both
cases. the models are Bargaining Game for bilateral MN and English Auction for
multiparty MN. We formalize the process of reaching such an agreement by giving
a deduction system that comprises of rules that are consistent and adequate for
representing MN."
"Phase transitions in many complex combinational problems have been widely
studied in the past decade. In this paper  we investigate phase transitions in
the knowledge compilation empirically  where DFA  OBDD and d-DNNF are chosen as
the target languages to compile random k-SAT instances. We perform intensive
experiments to analyze the sizes of compilation results and draw the following
conclusions. there exists an easy-hard-easy pattern in compilations; the peak
point of sizes in the pattern is only related to the ratio of the number of
clauses to that of variables when k is fixed  regardless of target languages;
most sizes of compilation results increase exponentially with the number of
variables growing  but there also exists a phase transition that separates a
polynomial-increment region from the exponential-increment region; Moreover  we
explain why the phase transition in compilations occurs by analyzing
microstructures of DFAs  and conclude that a kind of solution
interchangeability with more than 2 variables has a sharp transition near the
peak point of the easy-hard-easy pattern  and thus it has a great impact on
sizes of DFAs."
"A definition of intelligence is given in terms of performance that can be
quantitatively measured. In this study  we have presented a conceptual model of
Intelligent Agent System for Automatic Vehicle Checking Agent (VCA). To achieve
this goal  we have introduced several kinds of agents that exhibit intelligent
features. These are the Management agent  internal agent  External Agent 
Watcher agent and Report agent. Metrics and measurements are suggested for
evaluating the performance of Automatic Vehicle Checking Agent (VCA). Calibrate
data and test facilities are suggested to facilitate the development of
intelligent systems."
"This paper presents the design and development of a proposed rule based
Decision Support System that will help students in selecting the best suitable
faculty/major decision while taking admission in Gomal University  Dera Ismail
Khan  Pakistan. The basic idea of our approach is to design a model for testing
and measuring the student capabilities like intelligence  understanding 
comprehension  mathematical concepts plus his/her past academic record plus
his/her intelligence level  and applying the module results to a rule-based
decision support system to determine the compatibility of those capabilities
with the available faculties/majors in Gomal University. The result is shown as
a list of suggested faculties/majors with the student capabilities and
abilities."
"Heuristics are crucial tools in decreasing search effort in varied fields of
AI. In order to be effective  a heuristic must be efficient to compute  as well
as provide useful information to the search algorithm. However  some well-known
heuristics which do well in reducing backtracking are so heavy that the gain of
deploying them in a search algorithm might be outweighed by their overhead.
  We propose a rational metareasoning approach to decide when to deploy
heuristics  using CSP backtracking search as a case study. In particular  a
value of information approach is taken to adaptive deployment of solution-count
estimation heuristics for value ordering. Empirical results show that indeed
the proposed mechanism successfully balances the tradeoff between decreasing
backtracking and heuristic computational overhead  resulting in a significant
overall search time reduction."
"Regularization is a well studied problem in the context of neural networks.
It is usually used to improve the generalization performance when the number of
input samples is relatively small or heavily contaminated with noise. The
regularization of a parametric model can be achieved in different manners some
of which are early stopping (Morgan and Bourlard  1990)  weight decay  output
smoothing that are used to avoid overfitting during the training of the
considered model. From a Bayesian point of view  many regularization techniques
correspond to imposing certain prior distributions on model parameters (Krogh
and Hertz  1991). Using Bishop's approximation (Bishop  1995) of the objective
function when a restricted type of noise is added to the input of a parametric
function  we derive the higher order terms of the Taylor expansion and analyze
the coefficients of the regularization terms induced by the noisy input. In
particular we study the effect of penalizing the Hessian of the mapping
function with respect to the input in terms of generalization performance. We
also show how we can control independently this coefficient by explicitly
penalizing the Jacobian of the mapping function on corrupted inputs."
"We solve constraint satisfaction problems through translation to answer set
programming (ASP). Our reformulations have the property that unit-propagation
in the ASP solver achieves well defined local consistency properties like arc 
bound and range consistency. Experiments demonstrate the computational value of
this approach."
"Recent papers address the issue of updating the instance level of knowledge
bases expressed in Description Logic following a model-based approach. One of
the outcomes of these papers is that the result of updating a knowledge base K
is generally not expressible in the Description Logic used to express K. In
this paper we introduce a formula-based approach to this problem  by revisiting
some research work on formula-based updates developed in the '80s  in
particular the WIDTIO (When In Doubt  Throw It Out) approach. We show that our
operator enjoys desirable properties  including that both insertions and
deletions according to such operator can be expressed in the DL used for the
original KB. Also  we present polynomial time algorithms for the evolution of
the instance level knowledge bases expressed in the most expressive Description
Logics of the DL-lite family."
"We present in this paper a novel approach for training deterministic
auto-encoders. We show that by adding a well chosen penalty term to the
classical reconstruction cost function  we can achieve results that equal or
surpass those attained by other regularized auto-encoders as well as denoising
auto-encoders on a range of datasets. This penalty term corresponds to the
Frobenius norm of the Jacobian matrix of the encoder activations with respect
to the input. We show that this penalty term results in a localized space
contraction which in turn yields robust features on the activation layer.
Furthermore  we show how this penalty term is related to both regularized
auto-encoders and denoising encoders and how it can be seen as a link between
deterministic and non-deterministic auto-encoders. We find empirically that
this penalty helps to carve a representation that better captures the local
directions of variation dictated by the data  corresponding to a
lower-dimensional non-linear manifold  while being more invariant to the vast
majority of directions orthogonal to the manifold. Finally  we show that by
using the learned features to initialize a MLP  we achieve state of the art
classification error on a range of datasets  surpassing other methods of
pre-training."
"The study of arguments as abstract entities and their interaction as
introduced by Dung (Artificial Intelligence 177  1995) has become one of the
most active research branches within Artificial Intelligence and Reasoning. A
main issue for abstract argumentation systems is the selection of acceptable
sets of arguments. Value-based argumentation  as introduced by Bench-Capon (J.
Logic Comput. 13  2003)  extends Dung's framework. It takes into account the
relative strength of arguments with respect to some ranking representing an
audience. an argument is subjectively accepted if it is accepted with respect
to some audience  it is objectively accepted if it is accepted with respect to
all audiences. Deciding whether an argument is subjectively or objectively
accepted  respectively  are computationally intractable problems. In fact  the
problems remain intractable under structural restrictions that render the main
computational problems for non-value-based argumentation systems tractable. In
this paper we identify nontrivial classes of value-based argumentation systems
for which the acceptance problems are polynomial-time tractable. The classes
are defined by means of structural restrictions in terms of the underlying
graphical structure of the value-based system. Furthermore we show that the
acceptance problems are intractable for two classes of value-based systems that
where conjectured to be tractable by Dunne (Artificial Intelligence 171  2007)."
"In this paper  we investigate the hybrid tractability of binary Quantified
Constraint Satisfaction Problems (QCSPs). First  a basic tractable class of
binary QCSPs is identified by using the broken-triangle property. In this
class  the variable ordering for the broken-triangle property must be same as
that in the prefix of the QCSP. Second  we break this restriction to allow that
existentially quantified variables can be shifted within or out of their
blocks  and thus identify some novel tractable classes by introducing the
broken-angle property. Finally  we identify a more generalized tractable class 
i.e.  the min-of-max extendable class for QCSPs."
"Most current planners assume complete domain models and focus on generating
correct plans. Unfortunately  domain modeling is a laborious and error-prone
task. While domain experts cannot guarantee completeness  often they are able
to circumscribe the incompleteness of the model by providing annotations as to
which parts of the domain model may be incomplete. In such cases  the goal
should be to generate plans that are robust with respect to any known
incompleteness of the domain. In this paper  we first introduce annotations
expressing the knowledge of the domain incompleteness  and formalize the notion
of plan robustness with respect to an incomplete domain model. We then propose
an approach to compiling the problem of finding robust plans to the conformant
probabilistic planning problem. We present experimental results with
Probabilistic-FF  a state-of-the-art planner  showing the promise of our
approach."
"Despite the prevalence of the Computational Theory of Mind and the
Connectionist Model  the establishing of the key principles of the Cognitive
Science are still controversy and inconclusive. This paper proposes the concept
of Pattern Recognition as Necessary and Sufficient Principle for a general
cognitive science modeling  in a very ambitious scientific proposal. A formal
physical definition of the pattern recognition concept is also proposed to
solve many key conceptual gaps on the field."
"As was shown recently  many important AI problems require counting the number
of models of propositional formulas. The problem of counting models of such
formulas is  according to present knowledge  computationally intractable in a
worst case. Based on the Davis-Putnam procedure  we present an algorithm  CDP 
that computes the exact number of models of a propositional CNF or DNF formula
F. Let m and n be the number of clauses and variables of F  respectively  and
let p denote the probability that a literal l of F occurs in a clause C of F 
then the average running time of CDP is shown to be O(nm^d)  where
d=-1/log(1-p). The practical performance of CDP has been estimated in a series
of experiments on a wide variety of CNF formulas."
"This paper presents a new approach to identifying and eliminating mislabeled
training instances for supervised learning. The goal of this approach is to
improve classification accuracies produced by learning algorithms by improving
the quality of the training data. Our approach uses a set of learning
algorithms to create classifiers that serve as noise filters for the training
data. We evaluate single algorithm  majority vote and consensus filters on five
datasets that are prone to labeling errors. Our experiments illustrate that
filtering significantly improves classification accuracy for noise levels up to
30 percent. An analytical and empirical evaluation of the precision of our
approach shows that consensus filters are conservative at throwing away good
data at the expense of retaining bad data and that majority filters are better
at detecting bad data at the expense of throwing away good data. This suggests
that for situations in which there is a paucity of data  consensus filters are
preferable  whereas majority vote filters are preferable for situations with an
abundance of data."
"In many real-world learning tasks  it is expensive to acquire a sufficient
number of labeled examples for training. This paper investigates methods for
reducing annotation cost by `sample selection'. In this approach  during
training the learning program examines many unlabeled examples and selects for
labeling only those that are most informative at each stage. This avoids
redundantly labeling examples that contribute little new information. Our work
follows on previous research on Query By Committee  extending the
committee-based paradigm to the context of probabilistic classification. We
describe a family of empirical methods for committee-based sample selection in
probabilistic classification models  which evaluate the informativeness of an
example by measuring the degree of disagreement between several model variants.
These variants (the committee) are drawn randomly from a probability
distribution conditioned by the training set labeled so far. The method was
applied to the real-world natural language processing task of stochastic
part-of-speech tagging. We find that all variants of the method achieve a
significant reduction in annotation cost  although their computational
efficiency differs. In particular  the simplest variant  a two member committee
with no parameters to tune  gives excellent results. We also show that sample
selection yields a significant reduction in the size of the model used by the
tagger."
"We investigate the problem of reasoning in the propositional fragment of
MBNF  the logic of minimal belief and negation as failure introduced by
Lifschitz  which can be considered as a unifying framework for several
nonmonotonic formalisms  including default logic  autoepistemic logic 
circumscription  epistemic queries  and logic programming. We characterize the
complexity and provide algorithms for reasoning in propositional MBNF. In
particular  we show that entailment in propositional MBNF lies at the third
level of the polynomial hierarchy  hence it is harder than reasoning in all the
above mentioned propositional formalisms for nonmonotonic reasoning. We also
prove the exact correspondence between negation as failure in MBNF and negative
introspection in Moore's autoepistemic logic."
"We show how to find a minimum weight loop cutset in a Bayesian network with
high probability. Finding such a loop cutset is the first step in the method of
conditioning for inference. Our randomized algorithm for finding a loop cutset
outputs a minimum loop cutset after O(c 6^k kn) steps with probability at least
1 - (1 - 1/(6^k))^c6^k  where c > 1 is a constant specified by the user  k is
the minimal size of a minimum weight loop cutset  and n is the number of
vertices. We also show empirically that a variant of this algorithm often finds
a loop cutset that is closer to the minimum weight loop cutset than the ones
found by the best deterministic algorithms known."
"Recently model checking representation and search techniques were shown to be
efficiently applicable to planning  in particular to non-deterministic
planning. Such planning approaches use Ordered Binary Decision Diagrams (OBDDs)
to encode a planning domain as a non-deterministic finite automaton and then
apply fast algorithms from model checking to search for a solution. OBDDs can
effectively scale and can provide universal plans for complex planning domains.
We are particularly interested in addressing the complexities arising in
non-deterministic  multi-agent domains. In this article  we present UMOP  a new
universal OBDD-based planning framework for non-deterministic  multi-agent
domains. We introduce a new planning domain description language  NADL  to
specify non-deterministic  multi-agent domains. The language contributes the
explicit definition of controllable agents and uncontrollable environment
agents. We describe the syntax and semantics of NADL and show how to build an
efficient OBDD-based representation of an NADL description. The UMOP planning
system uses NADL and different OBDD-based universal planning algorithms. It
includes the previously developed strong and strong cyclic planning algorithms.
In addition  we introduce our new optimistic planning algorithm that relaxes
optimality guarantees and generates plausible universal plans in some domains
where no strong nor strong cyclic solution exists. We present empirical results
applying UMOP to domains ranging from deterministic and single-agent with no
environment actions to non-deterministic and multi-agent with complex
environment actions. UMOP is shown to be a rich and efficient planning system."
"This paper reviews the connections between Graphplan's planning-graph and the
dynamic constraint satisfaction problem and motivates the need for adapting CSP
search techniques to the Graphplan algorithm. It then describes how explanation
based learning  dependency directed backtracking  dynamic variable ordering 
forward checking  sticky values and random-restart search strategies can be
adapted to Graphplan. Empirical results are provided to demonstrate that these
augmentations improve Graphplan's performance significantly (up to 1000x
speedups) on several benchmark problems. Special attention is paid to the
explanation-based learning and dependency directed backtracking techniques as
they are empirically found to be most useful in improving the performance of
Graphplan."
"We investigate the space efficiency of a Propositional Knowledge
Representation (PKR) formalism. Intuitively  the space efficiency of a
formalism F in representing a certain piece of knowledge A  is the size of the
shortest formula of F that represents A. In this paper we assume that knowledge
is either a set of propositional interpretations (models) or a set of
propositional formulae (theorems). We provide a formal way of talking about the
relative ability of PKR formalisms to compactly represent a set of models or a
set of theorems. We introduce two new compactness measures  the corresponding
classes  and show that the relative space efficiency of a PKR formalism in
representing models/theorems is directly related to such classes. In
particular  we consider formalisms for nonmonotonic reasoning  such as
circumscription and default logic  as well as belief revision operators and the
stable model semantics for logic programs with negation. One interesting result
is that formalisms with the same time complexity do not necessarily belong to
the same space efficiency class."
"Partially observable Markov decision processes (POMDPs) provide an elegant
mathematical framework for modeling complex decision and planning problems in
stochastic domains in which states of the system are observable only
indirectly  via a set of imperfect or noisy observations. The modeling
advantage of POMDPs  however  comes at a price -- exact methods for solving
them are computationally very expensive and thus applicable in practice only to
very simple problems. We focus on efficient approximation (heuristic) methods
that attempt to alleviate the computational problem and trade off accuracy for
speed. We have two objectives here. First  we survey various approximation
methods  analyze their properties and relations and provide some new insights
into their differences. Second  we present a number of new approximation
methods and novel refinements of existing techniques. The theoretical results
are supported by experiments on a problem from the agent navigation domain."
"Pearl and Dechter (1996) claimed that the d-separation criterion for
conditional independence in acyclic causal networks also applies to networks of
discrete variables that have feedback cycles  provided that the variables of
the system are uniquely determined by the random disturbances. I show by
example that this is not true in general. Some condition stronger than
uniqueness is needed  such as the existence of a causal dynamics guaranteed to
lead to the unique solution."
"Functional relationships between objects  called `attributes'  are of
considerable importance in knowledge representation languages  including
Description Logics (DLs). A study of the literature indicates that papers have
made  often implicitly  different assumptions about the nature of attributes.
whether they are always required to have a value  or whether they can be
partial functions. The work presented here is the first explicit study of this
difference for subclasses of the CLASSIC DL  involving the same-as concept
constructor. It is shown that although determining subsumption between concept
descriptions has the same complexity (though requiring different algorithms) 
the story is different in the case of determining the least common subsumer
(lcs). For attributes interpreted as partial functions  the lcs exists and can
be computed relatively easily; even in this case our results correct and extend
three previous papers about the lcs of DLs. In the case where attributes must
have a value  the lcs may not exist  and even if it exists it may be of
exponential size. Interestingly  it is possible to decide in polynomial time if
the lcs exists."
"We study the complexity of the combination of the Description Logics ALCQ and
ALCQI with a terminological formalism based on cardinality restrictions on
concepts. These combinations can naturally be embedded into C^2  the two
variable fragment of predicate logic with counting quantifiers  which yields
decidability in NExpTime. We show that this approach leads to an optimal
solution for ALCQI  as ALCQI with cardinality restrictions has the same
complexity as C^2 (NExpTime-complete). In contrast  we show that for ALCQ  the
problem can be solved in ExpTime. This result is obtained by a reduction of
reasoning with cardinality restrictions to reasoning with the (in general
weaker) terminological formalism of general axioms for ALCQ extended with
nominals. Using the same reduction  we show that  for the extension of ALCQI
with nominals  reasoning with general axioms is a NExpTime-complete problem.
Finally  we sharpen this result and show that pure concept satisfiability for
ALCQI with nominals is NExpTime-complete. Without nominals  this problem is
known to be PSpace-complete."
"The local search algorithm WSat is one of the most successful algorithms for
solving the satisfiability (SAT) problem. It is notably effective at solving
hard Random 3-SAT instances near the so-called `satisfiability threshold'  but
still shows a peak in search cost near the threshold and large variations in
cost over different instances. We make a number of significant contributions to
the analysis of WSat on high-cost random instances  using the
recently-introduced concept of the backbone of a SAT instance. The backbone is
the set of literals which are entailed by an instance. We find that the number
of solutions predicts the cost well for small-backbone instances but is much
less relevant for the large-backbone instances which appear near the threshold
and dominate in the overconstrained region. We show a very strong correlation
between search cost and the Hamming distance to the nearest solution early in
WSat's search. This pattern leads us to introduce a measure of the backbone
fragility of an instance  which indicates how persistent the backbone is as
clauses are removed. We propose that high-cost random instances for local
search are those with very large backbones which are also backbone-fragile. We
suggest that the decay in cost beyond the satisfiability threshold is due to
increasing backbone robustness (the opposite of backbone fragility). Our
hypothesis makes three correct predictions. First  that the backbone robustness
of an instance is negatively correlated with the local search cost when other
factors are controlled for. Second  that backbone-minimal instances (which are
3-SAT instances altered so as to be more backbone-fragile) are unusually hard
for WSat. Third  that the clauses most often unsatisfied during search are
those whose deletion has the most effect on the backbone. In understanding the
pathologies of local search methods  we hope to contribute to the development
of new and better techniques."
"This paper describes a novel method by which a spoken dialogue system can
learn to choose an optimal dialogue strategy from its experience interacting
with human users. The method is based on a combination of reinforcement
learning and performance modeling of spoken dialogue systems. The reinforcement
learning component applies Q-learning (Watkins  1989)  while the performance
modeling component applies the PARADISE evaluation framework (Walker et al. 
1997) to learn the performance function (reward) used in reinforcement
learning. We illustrate the method with a spoken dialogue system named ELVIS
(EmaiL Voice Interactive System)  that supports access to email over the phone.
We conduct a set of experiments for training an optimal dialogue strategy on a
corpus of 219 dialogues in which human users interact with ELVIS over the
phone. We then test that strategy on a corpus of 18 dialogues. We show that
ELVIS can learn to optimize its strategy selection for agent initiative  for
reading messages  and for summarizing email folders."
"We show that for several variations of partially observable Markov decision
processes  polynomial-time algorithms for finding control policies are unlikely
to or simply don't have guarantees of finding policies within a constant factor
or a constant summand of optimal. Here ""unlikely"" means ""unless some complexity
classes collapse "" where the collapses considered are P=NP  P=PSPACE  or P=EXP.
Until or unless these collapses are shown to hold  any control-policy designer
must choose between such performance guarantees and efficient computation."
"The paper addresses the problem of computing goal orderings  which is one of
the longstanding issues in AI planning. It makes two new contributions. First 
it formally defines and discusses two different goal orderings  which are
called the reasonable and the forced ordering. Both orderings are defined for
simple STRIPS operators as well as for more complex ADL operators supporting
negation and conditional effects. The complexity of these orderings is
investigated and their practical relevance is discussed. Secondly  two
different methods to compute reasonable goal orderings are developed. One of
them is based on planning graphs  while the other investigates the set of
actions directly. Finally  it is shown how the ordering relations  which have
been derived for a given set of goals G  can be used to compute a so-called
goal agenda that divides G into an ordered set of subgoals. Any planner can
then  in principle  use the goal agenda to plan for increasing sets of
subgoals. This can lead to an exponential complexity reduction  as the solution
to a complex planning problem is found by solving easier subproblems. Since
only a polynomial overhead is caused by the goal agenda computation  a
potential exists to dramatically speed up planning algorithms as we demonstrate
in the empirical evaluation  where we use this method in the IPP planner."
"The goal of this research is to develop agents that are adaptive and
predictable and timely. At first blush  these three requirements seem
contradictory. For example  adaptation risks introducing undesirable side
effects  thereby making agents' behavior less predictable. Furthermore 
although formal verification can assist in ensuring behavioral predictability 
it is known to be time-consuming. Our solution to the challenge of satisfying
all three requirements is the following. Agents have finite-state automaton
plans  which are adapted online via evolutionary learning (perturbation)
operators. To ensure that critical behavioral constraints are always satisfied 
agents' plans are first formally verified. They are then reverified after every
adaptation. If reverification concludes that constraints are violated  the
plans are repaired. The main objective of this paper is to improve the
efficiency of reverification after learning  so that agents have a sufficiently
rapid response time. We present two solutions. positive results that certain
learning operators are a priori guaranteed to preserve useful classes of
behavioral assurance constraints (which implies that no reverification is
needed for these operators)  and efficient incremental reverification
algorithms for those learning operators that have negative a priori results."
"A major problem in machine learning is that of inductive bias. how to choose
a learner's hypothesis space so that it is large enough to contain a solution
to the problem being learnt  yet small enough to ensure reliable generalization
from reasonably-sized training sets. Typically such bias is supplied by hand
through the skill and insights of experts. In this paper a model for
automatically learning bias is investigated. The central assumption of the
model is that the learner is embedded within an environment of related learning
tasks. Within such an environment the learner can sample from multiple tasks 
and hence it can search for a hypothesis space that contains good solutions to
many of the problems in the environment. Under certain restrictions on the set
of all hypothesis spaces available to the learner  we show that a hypothesis
space that performs well on a sufficiently large number of training tasks will
also perform well when learning novel tasks in the same environment. Explicit
bounds are also derived demonstrating that learning multiple tasks within an
environment of related tasks can potentially give much better generalization
than learning a single task."
"The chief aim of this paper is to propose mean-field approximations for a
broad class of Belief networks  of which sigmoid and noisy-or networks can be
seen as special cases. The approximations are based on a powerful mean-field
theory suggested by Plefka. We show that Saul  Jaakkola and Jordan' s approach
is the first order approximation in Plefka's approach  via a variational
derivation. The application of Plefka's theory to belief networks is not
computationally tractable. To tackle this problem we propose new approximations
based on Taylor series. Small scale experiments show that the proposed schemes
are attractive."
"The recent approaches of extending the GRAPHPLAN algorithm to handle more
expressive planning formalisms raise the question of what the formal meaning of
""expressive power"" is. We formalize the intuition that expressive power is a
measure of how concisely planning domains and plans can be expressed in a
particular formalism by introducing the notion of ""compilation schemes"" between
planning formalisms. Using this notion  we analyze the expressiveness of a
large family of propositional planning formalisms  ranging from basic STRIPS to
a formalism with conditional effects  partial state specifications  and
propositional formulae in the preconditions. One of the results is that
conditional effects cannot be compiled away if plan size should grow only
linearly but can be compiled away if we allow for polynomial growth of the
resulting plans. This result confirms that the recently proposed extensions to
the GRAPHPLAN algorithm concerning conditional effects are optimal with respect
to the ""compilability"" framework. Another result is that general propositional
formulae cannot be compiled into conditional effects if the plan size should be
preserved linearly. This implies that allowing general propositional formulae
in preconditions and effect conditions adds another level of difficulty in
generating a plan."
"In order to generate plans for agents with multiple actuators  agent teams 
or distributed controllers  we must be able to represent and plan using
concurrent actions with interacting effects. This has historically been
considered a challenging task requiring a temporal planner with the ability to
reason explicitly about time. We show that with simple modifications  the
STRIPS action representation language can be used to represent interacting
actions. Moreover  algorithms for partial-order planning require only small
modifications in order to be applied in such multiagent domains. We demonstrate
this fact by developing a sound and complete partial-order planner for planning
with concurrent interacting actions  POMP  that extends existing partial-order
planners in a straightforward way. These results open the way to the use of
partial-order planners for the centralized control of cooperative multiagent
systems."
"Domain-independent planning is a hard combinatorial problem. Taking into
account plan quality makes the task even more difficult. This article
introduces Planning by Rewriting (PbR)  a new paradigm for efficient
high-quality domain-independent planning. PbR exploits declarative
plan-rewriting rules and efficient local search techniques to transform an
easy-to-generate  but possibly suboptimal  initial plan into a high-quality
plan. In addition to addressing the issues of planning efficiency and plan
quality  this framework offers a new anytime planning algorithm. We have
implemented this planner and applied it to several existing domains. The
experimental results show that the PbR approach provides significant savings in
planning effort while generating high-quality plans."
"Partially observable Markov decision processes (POMDPs) have recently become
popular among many AI researchers because they serve as a natural model for
planning under uncertainty. Value iteration is a well-known algorithm for
finding optimal policies for POMDPs. It typically takes a large number of
iterations to converge. This paper proposes a method for accelerating the
convergence of value iteration. The method has been evaluated on an array of
benchmark problems and was found to be very effective. It enabled value
iteration to converge after only a few iterations on all the test problems."
"We tackle the problem of planning in nondeterministic domains  by presenting
a new approach to conformant planning. Conformant planning is the problem of
finding a sequence of actions that is guaranteed to achieve the goal despite
the nondeterminism of the domain. Our approach is based on the representation
of the planning domain as a finite state automaton. We use Symbolic Model
Checking techniques  in particular Binary Decision Diagrams  to compactly
represent and efficiently search the automaton. In this paper we make the
following contributions. First  we present a general planning algorithm for
conformant planning  which applies to fully nondeterministic domains  with
uncertainty in the initial condition and in action effects. The algorithm is
based on a breadth-first  backward search  and returns conformant plans of
minimal length  if a solution to the planning problem exists  otherwise it
terminates concluding that the problem admits no conformant solution. Second 
we provide a symbolic representation of the search space based on Binary
Decision Diagrams (BDDs)  which is the basis for search techniques derived from
symbolic model checking. The symbolic representation makes it possible to
analyze potentially large sets of states and transitions in a single
computation step  thus providing for an efficient implementation. Third  we
present CMBP (Conformant Model Based Planner)  an efficient implementation of
the data structures and algorithm described above  directly based on BDD
manipulations  which allows for a compact representation of the search layers
and an efficient implementation of the search steps. Finally  we present an
experimental comparison of our approach with the state-of-the-art conformant
planners CGP  QBFPLAN and GPT. Our analysis includes all the planning problems
from the distribution packages of these systems  plus other problems defined to
stress a number of specific factors. Our approach appears to be the most
effective. CMBP is strictly more expressive than QBFPLAN and CGP and  in all
the problems where a comparison is possible  CMBP outperforms its competitors 
sometimes by orders of magnitude."
"Stochastic sampling algorithms  while an attractive alternative to exact
algorithms in very large Bayesian network models  have been observed to perform
poorly in evidential reasoning with extremely unlikely evidence. To address
this problem  we propose an adaptive importance sampling algorithm  AIS-BN 
that shows promising convergence rates even under extreme conditions and seems
to outperform the existing sampling algorithms consistently. Three sources of
this performance improvement are (1) two heuristics for initialization of the
importance function that are based on the theoretical properties of importance
sampling in finite-dimensional integrals and the structural advantages of
Bayesian networks  (2) a smooth learning method for the importance function 
and (3) a dynamic weighting function for combining samples from different
stages of the algorithm. We tested the performance of the AIS-BN algorithm
along with two state of the art general purpose sampling algorithms  likelihood
weighting (Fung and Chang  1989; Shachter and Peot  1989) and self-importance
sampling (Shachter and Peot  1989). We used in our tests three large real
Bayesian network models available to the scientific community. the CPCS network
(Pradhan et al.  1994)  the PathFinder network (Heckerman  Horvitz  and
Nathwani  1990)  and the ANDES network (Conati  Gertner  VanLehn  and Druzdzel 
1997)  with evidence as unlikely as 10^-41. While the AIS-BN algorithm always
performed better than the other two algorithms  in the majority of the test
cases it achieved orders of magnitude improvement in precision of the results.
Improvement in speed given a desired precision is even more dramatic  although
we are unable to report numerical results here  as the other algorithms almost
never achieved the precision reached even by the first few iterations of the
AIS-BN algorithm."
"In recent years  many improvements to backtracking algorithms for solving
constraint satisfaction problems have been proposed. The techniques for
improving backtracking algorithms can be conveniently classified as look-ahead
schemes and look-back schemes. Unfortunately  look-ahead and look-back schemes
are not entirely orthogonal as it has been observed empirically that the
enhancement of look-ahead techniques is sometimes counterproductive to the
effects of look-back techniques. In this paper  we focus on the relationship
between the two most important look-ahead techniques---using a variable
ordering heuristic and maintaining a level of local consistency during the
backtracking search---and the look-back technique of conflict-directed
backjumping (CBJ). We show that there exists a ""perfect"" dynamic variable
ordering such that CBJ becomes redundant. We also show theoretically that as
the level of local consistency that is maintained in the backtracking search is
increased  the less that backjumping will be an improvement. Our theoretical
results partially explain why a backtracking algorithm doing more in the
look-ahead phase cannot benefit more from the backjumping look-back scheme.
Finally  we show empirically that adding CBJ to a backtracking algorithm that
maintains generalized arc consistency (GAC)  an algorithm that we refer to as
GAC-CBJ  can still provide orders of magnitude speedups. Our empirical results
contrast with Bessiere and Regin's conclusion (1996) that CBJ is useless to an
algorithm that maintains arc consistency."
"This paper presents an implemented system for recognizing the occurrence of
events described by simple spatial-motion verbs in short image sequences. The
semantics of these verbs is specified with event-logic expressions that
describe changes in the state of force-dynamic relations between the
participants of the event. An efficient finite representation is introduced for
the infinite sets of intervals that occur when describing liquid and
semi-liquid events. Additionally  an efficient procedure using this
representation is presented for inferring occurrences of compound events 
described with event-logic expressions  from occurrences of primitive events.
Using force dynamics and event logic to specify the lexical semantics of events
allows the system to be more robust than prior systems based on motion profile."
"An ensemble consists of a set of individually trained classifiers (such as
neural networks or decision trees) whose predictions are combined when
classifying novel instances. Previous research has shown that an ensemble is
often more accurate than any of the single classifiers in the ensemble. Bagging
(Breiman  1996c) and Boosting (Freund and Shapire  1996; Shapire  1990) are two
relatively new but popular methods for producing ensembles. In this paper we
evaluate these methods on 23 data sets using both neural networks and decision
trees as our classification algorithm. Our results clearly indicate a number of
conclusions. First  while Bagging is almost always more accurate than a single
classifier  it is sometimes much less accurate than Boosting. On the other
hand  Boosting can create ensembles that are less accurate than a single
classifier -- especially when using neural networks. Analysis indicates that
the performance of the Boosting methods is dependent on the characteristics of
the data set being examined. In fact  further results show that Boosting
ensembles may overfit noisy data sets  thus decreasing its performance.
Finally  consistent with previous studies  our work suggests that most of the
gain in an ensemble's performance comes in the first few classifiers combined;
however  relatively large gains can be seen up to 25 classifiers when Boosting
decision trees."
"This paper presents an evolutionary algorithm with a new goal-sequence
domination scheme for better decision support in multi-objective optimization.
The approach allows the inclusion of advanced hard/soft priority and constraint
information on each objective component  and is capable of incorporating
multiple specifications with overlapping or non-overlapping objective functions
via logical 'OR' and 'AND' connectives to drive the search towards multiple
regions of trade-off. In addition  we propose a dynamic sharing scheme that is
simple and adaptively estimated according to the on-line population
distribution without needing any a priori parameter setting. Each feature in
the proposed algorithm is examined to show its respective contribution  and the
performance of the algorithm is compared with other evolutionary optimization
methods. It is shown that the proposed algorithm has performed well in the
diversity of evolutionary search and uniform distribution of non-dominated
individuals along the final trade-offs  without significant computational
effort. The algorithm is also applied to the design optimization of a practical
servo control system for hard disk drives with a single voice-coil-motor
actuator. Results of the evolutionary designed servo control system show a
superior closed-loop performance compared to classical PID or RPT approaches."
"This paper presents GRT  a domain-independent heuristic planning system for
STRIPS worlds. GRT solves problems in two phases. In the pre-processing phase 
it estimates the distance between each fact and the goals of the problem  in a
backward direction. Then  in the search phase  these estimates are used in
order to further estimate the distance between each intermediate state and the
goals  guiding so the search process in a forward direction and on a best-first
basis. The paper presents the benefits from the adoption of opposite directions
between the preprocessing and the search phases  discusses some difficulties
that arise in the pre-processing phase and introduces techniques to cope with
them. Moreover  it presents several methods of improving the efficiency of the
heuristic  by enriching the representation and by reducing the size of the
problem. Finally  a method of overcoming local optimal states  based on domain
axioms  is proposed. According to it  difficult problems are decomposed into
easier sub-problems that have to be solved sequentially. The performance
results from various domains  including those of the recent planning
competitions  show that GRT is among the fastest planners."
"In the recent literature of Artificial Intelligence  an intensive research
effort has been spent  for various algebras of qualitative relations used in
the representation of temporal and spatial knowledge  on the problem of
classifying the computational complexity of reasoning problems for subsets of
algebras. The main purpose of these researches is to describe a restricted set
of maximal tractable subalgebras  ideally in an exhaustive fashion with respect
to the hosting algebras. In this paper we introduce a novel algebra for
reasoning about Spatial Congruence  show that the satisfiability problem in the
spatial algebra MC-4 is NP-complete  and present a complete classification of
tractability in the algebra  based on the individuation of three maximal
tractable subclasses  one containing the basic relations. The three algebras
are formed by 14  10 and 9 relations out of 16 which form the full algebra."
"Gradient-based approaches to direct policy search in reinforcement learning
have received much recent attention as a means to solve problems of partial
observability and to avoid some of the problems associated with policy
degradation in value-function methods. In this paper we introduce GPOMDP  a
simulation-based algorithm for generating a biased estimate of the gradient of
the average reward in Partially Observable Markov Decision Processes POMDPs
controlled by parameterized stochastic policies. A similar algorithm was
proposed by (Kimura et al. 1995). The algorithm's chief advantages are that it
requires storage of only twice the number of policy parameters  uses one free
beta (which has a natural interpretation in terms of bias-variance trade-off) 
and requires no knowledge of the underlying state. We prove convergence of
GPOMDP  and show how the correct choice of the parameter beta is related to the
mixing time of the controlled POMDP. We briefly describe extensions of GPOMDP
to controlled Markov chains  continuous state  observation and control spaces 
multiple-agents  higher-order derivatives  and a version for training
stochastic policies with internal states. In a companion paper (Baxter et al. 
this volume) we show how the gradient estimates generated by GPOMDP can be used
in both a traditional stochastic gradient algorithm and a conjugate-gradient
procedure to find local optima of the average reward."
"In this paper  we present algorithms that perform gradient ascent of the
average reward in a partially observable Markov decision process (POMDP). These
algorithms are based on GPOMDP  an algorithm introduced in a companion paper
(Baxter and Bartlett  this volume)  which computes biased estimates of the
performance gradient in POMDPs. The algorithm's chief advantages are that it
uses only one free parameter beta  which has a natural interpretation in terms
of bias-variance trade-off  it requires no knowledge of the underlying state 
and it can be applied to infinite state  control and observation spaces. We
show how the gradient estimates produced by GPOMDP can be used to perform
gradient ascent  both with a traditional stochastic-gradient algorithm  and
with an algorithm based on conjugate-gradients that utilizes gradient
information to bracket maxima in line searches. Experimental results are
presented illustrating both the theoretical results of (Baxter and Bartlett 
this volume) on a toy problem  and practical aspects of the algorithms on a
number of more realistic problems."
"Description Logics (DLs) are suitable  well-known  logics for managing
structured knowledge. They allow reasoning about individuals and well defined
concepts  i.e.  set of individuals with common properties. The experience in
using DLs in applications has shown that in many cases we would like to extend
their capabilities. In particular  their use in the context of Multimedia
Information Retrieval (MIR) leads to the convincement that such DLs should
allow the treatment of the inherent imprecision in multimedia object content
representation and retrieval. In this paper we will present a fuzzy extension
of ALC  combining Zadeh's fuzzy logic with a classical DL. In particular 
concepts becomes fuzzy and  thus  reasoning about imprecise concepts is
supported. We will define its syntax  its semantics  describe its properties
and present a constraint propagation calculus for reasoning in it."
"Top-down induction of decision trees has been observed to suffer from the
inadequate functioning of the pruning phase. In particular  it is known that
the size of the resulting tree grows linearly with the sample size  even though
the accuracy of the tree does not improve. Reduced Error Pruning is an
algorithm that has been used as a representative technique in attempts to
explain the problems of decision tree learning. In this paper we present
analyses of Reduced Error Pruning in three different settings. First we study
the basic algorithmic properties of the method  properties that hold
independent of the input decision tree and pruning examples. Then we examine a
situation that intuitively should lead to the subtree under consideration to be
replaced by a leaf node  one in which the class label and attribute values of
the pruning examples are independent of each other. This analysis is conducted
under two different assumptions. The general analysis shows that the pruning
probability of a node fitting pure noise is bounded by a function that
decreases exponentially as the size of the tree grows. In a specific analysis
we assume that the examples are distributed uniformly to the tree. This
assumption lets us approximate the number of subtrees that are pruned because
they do not receive any pruning examples. This paper clarifies the different
variants of the Reduced Error Pruning algorithm  brings new insight to its
algorithmic properties  analyses the algorithm with less imposed assumptions
than before  and includes the previously overlooked empty subtrees to the
analysis."
"This paper investigates the problems arising in the construction of a program
to play the game of contract bridge. These problems include both the difficulty
of solving the game's perfect information variant  and techniques needed to
address the fact that bridge is not  in fact  a perfect information game. GIB 
the program being described  involves five separate technical advances.
partition search  the practical application of Monte Carlo techniques to
realistic problems  a focus on achievable sets to solve problems inherent in
the Monte Carlo approach  an extension of alpha-beta pruning from total orders
to arbitrary distributive lattices  and the use of squeaky wheel optimization
to find approximately optimal solutions to cardplay problems. GIB is currently
believed to be of approximately expert caliber  and is currently the strongest
computer bridge program in the world."
"Enforcing local consistencies is one of the main features of constraint
reasoning. Which level of local consistency should be used when searching for
solutions in a constraint network is a basic question. Arc consistency and
partial forms of arc consistency have been widely studied  and have been known
for sometime through the forward checking or the MAC search algorithms. Until
recently  stronger forms of local consistency remained limited to those that
change the structure of the constraint graph  and thus  could not be used in
practice  especially on large networks. This paper focuses on the local
consistencies that are stronger than arc consistency  without changing the
structure of the network  i.e.  only removing inconsistent values from the
domains. In the last five years  several such local consistencies have been
proposed by us or by others. We make an overview of all of them  and highlight
some relations between them. We compare them both theoretically and
experimentally  considering their pruning efficiency and the time required to
enforce them."
"In this paper  we present a method for recognising an agent's behaviour in
dynamic  noisy  uncertain domains  and across multiple levels of abstraction.
We term this problem on-line plan recognition under uncertainty and view it
generally as probabilistic inference on the stochastic process representing the
execution of the agent's plan. Our contributions in this paper are twofold. In
terms of probabilistic inference  we introduce the Abstract Hidden Markov Model
(AHMM)  a novel type of stochastic processes  provide its dynamic Bayesian
network (DBN) structure and analyse the properties of this network. We then
describe an application of the Rao-Blackwellised Particle Filter to the AHMM
which allows us to construct an efficient  hybrid inference method for this
model. In terms of plan recognition  we propose a novel plan recognition
framework based on the AHMM as the plan execution model. The Rao-Blackwellised
hybrid inference for AHMM can take advantage of the independence properties
inherent in a model of plan execution  leading to an algorithm for online
probabilistic plan recognition that scales well with the number of levels in
the plan hierarchy. This illustrates that while stochastic models for plan
execution can be complex  they exhibit special structures which  if exploited 
can lead to efficient plan recognition algorithms. We demonstrate the
usefulness of the AHMM framework via a behaviour recognition system in a
complex spatial environment using distributed video surveillance data."
"We describe and evaluate the algorithmic techniques that are used in the FF
planning system. Like the HSP system  FF relies on forward state space search 
using a heuristic that estimates goal distances by ignoring delete lists.
Unlike HSP's heuristic  our method does not assume facts to be independent. We
introduce a novel search strategy that combines hill-climbing with systematic
search  and we show how other powerful heuristic information can be extracted
and used to prune the search space. FF was the most successful automatic
planner at the recent AIPS-2000 planning competition. We review the results of
the competition  give data for other benchmark domains  and investigate the
reasons for the runtime performance of FF compared to HSP."
"The First Trading Agent Competition (TAC) was held from June 22nd to July
8th  2000. TAC was designed to create a benchmark problem in the complex domain
of e-marketplaces and to motivate researchers to apply unique approaches to a
common task. This article describes ATTac-2000  the first-place finisher in
TAC. ATTac-2000 uses a principled bidding strategy that includes several
elements of adaptivity. In addition to the success at the competition  isolated
empirical results are presented indicating the robustness and effectiveness of
ATTac-2000's adaptive strategy."
"The theoretical properties of qualitative spatial reasoning in the RCC8
framework have been analyzed extensively. However  no empirical investigation
has been made yet. Our experiments show that the adaption of the algorithms
used for qualitative temporal reasoning can solve large RCC8 instances  even if
they are in the phase transition region -- provided that one uses the maximal
tractable subsets of RCC8 that have been identified by us. In particular  we
demonstrate that the orthogonal combination of heuristic methods is successful
in solving almost all apparently hard instances in the phase transition region
up to a certain size in reasonable time."
"This paper presents our work on development of OWL-driven systems for formal
representation and reasoning about terminological knowledge and facts in
petrology. The long-term aim of our project is to provide solid foundations for
a large-scale integration of various kinds of knowledge  including basic terms 
rock classification algorithms  findings and reports. We describe three steps
we have taken towards that goal here. First  we develop a semi-automated
procedure for transforming a database of igneous rock samples to texts in a
controlled natural language (CNL)  and then a collection of OWL ontologies.
Second  we create an OWL ontology of important petrology terms currently
described in natural language thesauri. We describe a prototype of a tool for
collecting definitions from domain experts. Third  we present an approach to
formalization of current industrial standards for classification of rock
samples  which requires linear equations in OWL 2. In conclusion  we discuss a
range of opportunities arising from the use of semantic technologies in
petrology and outline the future work in this area."
"This paper discusses a system that accelerates reinforcement learning by
using transfer from related tasks. Without such transfer  even if two tasks are
very similar at some abstract level  an extensive re-learning effort is
required. The system achieves much of its power by transferring parts of
previously learned solutions rather than a single complete solution. The system
exploits strong features in the multi-dimensional function produced by
reinforcement learning in solving a particular task. These features are stable
and easy to recognize early in the learning process. They generate a
partitioning of the state space and thus the function. The partition is
represented as a graph. This is used to index and compose functions stored in a
case base to form a close approximation to the solution of the new task.
Experiments demonstrate that function composition often produces more than an
order of magnitude increase in learning rate compared to a basic reinforcement
learning algorithm."
"We propose a logical/mathematical framework for statistical parameter
learning of parameterized logic programs  i.e. definite clause programs
containing probabilistic facts with a parameterized distribution. It extends
the traditional least Herbrand model semantics in logic programming to
distribution semantics  possible world semantics with a probability
distribution which is unconditionally applicable to arbitrary logic programs
including ones for HMMs  PCFGs and Bayesian networks. We also propose a new EM
algorithm  the graphical EM algorithm  that runs for a class of parameterized
logic programs representing sequential decision processes where each decision
is exclusive and independent. It runs on a new data structure called support
graphs describing the logical relationship between observations and their
explanations  and learns parameters by computing inside and outside probability
generalized for logic programs. The complexity analysis shows that when
combined with OLDT search for all explanations for observations  the graphical
EM algorithm  despite its generality  has the same time complexity as existing
EM algorithms  i.e. the Baum-Welch algorithm for HMMs  the Inside-Outside
algorithm for PCFGs  and the one for singly connected Bayesian networks that
have been developed independently in each research field. Learning experiments
with PCFGs using two corpora of moderate size indicate that the graphical EM
algorithm can significantly outperform the Inside-Outside algorithm."
"I consider the problem of learning an optimal path graphical model from data
and show the problem to be NP-hard for the maximum likelihood and minimum
description length approaches and a Bayesian approach. This hardness result
holds despite the fact that the problem is a restriction of the polynomially
solvable problem of finding the optimal tree graphical model."
"Simple conceptual graphs are considered as the kernel of most knowledge
representation formalisms built upon Sowa's model. Reasoning in this model can
be expressed by a graph homomorphism called projection  whose semantics is
usually given in terms of positive  conjunctive  existential FOL. We present
here a family of extensions of this model  based on rules and constraints 
keeping graph homomorphism as the basic operation. We focus on the formal
definitions of the different models obtained  including their operational
semantics and relationships with FOL  and we analyze the decidability and
complexity of the associated problems (consistency and deduction). As soon as
rules are involved in reasonings  these problems are not decidable  but we
exhibit a condition under which they fall in the polynomial hierarchy. These
results extend and complete the ones already published by the authors. Moreover
we systematically study the complexity of some particular cases obtained by
restricting the form of constraints and/or rules."
"Fusions are a simple way of combining logics. For normal modal logics 
fusions have been investigated in detail. In particular  it is known that 
under certain conditions  decidability transfers from the component logics to
their fusion. Though description logics are closely related to modal logics 
they are not necessarily normal. In addition  ABox reasoning in description
logics is not covered by the results from modal logics. In this paper  we
extend the decidability transfer results from normal modal logics to a large
class of description logics. To cover different description logics in a uniform
way  we introduce abstract description systems  which can be seen as a common
generalization of description and modal logics  and show the transfer results
in this general setting."
"Inductive logic programming  or relational learning  is a powerful paradigm
for machine learning or data mining. However  in order for ILP to become
practically useful  the efficiency of ILP systems must improve substantially.
To this end  the notion of a query pack is introduced. it structures sets of
similar queries. Furthermore  a mechanism is described for executing such query
packs. A complexity analysis shows that considerable efficiency improvements
can be achieved through the use of this query pack execution mechanism. This
claim is supported by empirical results obtained by incorporating support for
query pack execution in two existing learning systems."
"Recent trends in planning research have led to empirical comparison becoming
commonplace. The field has started to settle into a methodology for such
comparisons  which for obvious practical reasons requires running a subset of
planners on a subset of problems. In this paper  we characterize the
methodology and examine eight implicit assumptions about the problems  planners
and metrics used in many of these comparisons. The problem assumptions are.
PR1) the performance of a general purpose planner should not be
penalized/biased if executed on a sampling of problems and domains  PR2) minor
syntactic differences in representation do not affect performance  and PR3)
problems should be solvable by STRIPS capable planners unless they require ADL.
The planner assumptions are. PL1) the latest version of a planner is the best
one to use  PL2) default parameter settings approximate good performance  and
PL3) time cut-offs do not unduly bias outcome. The metrics assumptions are. M1)
performance degrades similarly for each planner when run on degraded runtime
environments (e.g.  machine platform) and M2) the number of plan steps
distinguishes performance. We find that most of these assumptions are not
supported empirically; in particular  that planners are affected differently by
these assumptions. We conclude with a call to the community to devote research
resources to improving the state of the practice and especially to enhancing
the available benchmark problems."
"An approach to the construction of classifiers from imbalanced datasets is
described. A dataset is imbalanced if the classification categories are not
approximately equally represented. Often real-world data sets are predominately
composed of ""normal"" examples with only a small percentage of ""abnormal"" or
""interesting"" examples. It is also the case that the cost of misclassifying an
abnormal (interesting) example as a normal example is often much higher than
the cost of the reverse error. Under-sampling of the majority (normal) class
has been proposed as a good means of increasing the sensitivity of a classifier
to the minority class. This paper shows that a combination of our method of
over-sampling the minority (abnormal) class and under-sampling the majority
(normal) class can achieve better classifier performance (in ROC space) than
only under-sampling the majority class. This paper also shows that a
combination of our method of over-sampling the minority class and
under-sampling the majority class can achieve better classifier performance (in
ROC space) than varying the loss ratios in Ripper or class priors in Naive
Bayes. Our method of over-sampling the minority class involves creating
synthetic minority class examples. Experiments are performed using C4.5  Ripper
and a Naive Bayes classifier. The method is evaluated using the area under the
Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy."
"Common wisdom has it that small distinctions in the probabilities
(parameters) quantifying a belief network do not matter much for the results of
probabilistic queries. Yet  one can develop realistic scenarios under which
small variations in network parameters can lead to significant changes in
computed queries. A pending theoretical question is then to analytically
characterize parameter changes that do or do not matter. In this paper  we
study the sensitivity of probabilistic queries to changes in network parameters
and prove some tight bounds on the impact that such parameters can have on
queries. Our analytic results pinpoint some interesting situations under which
parameter changes do or do not matter. These results are important for
knowledge engineers as they help them identify influential network parameters.
They also help explain some of the previous experimental results and
observations with regards to network robustness against parameter changes."
"Recent years are seeing an increasing need for on-line monitoring of teams of
cooperating agents  e.g.  for visualization  or performance tracking. However 
in monitoring deployed teams  we often cannot rely on the agents to always
communicate their state to the monitoring system. This paper presents a
non-intrusive approach to monitoring by 'overhearing'  where the monitored
team's state is inferred (via plan-recognition) from team-members' routine
communications  exchanged as part of their coordinated task execution  and
observed (overheard) by the monitoring system. Key challenges in this approach
include the demanding run-time requirements of monitoring  the scarceness of
observations (increasing monitoring uncertainty)  and the need to scale-up
monitoring to address potentially large teams. To address these  we present a
set of complementary novel techniques  exploiting knowledge of the social
structures and procedures in the monitored team. (i) an efficient probabilistic
plan-recognition algorithm  well-suited for processing communications as
observations; (ii) an approach to exploiting knowledge of the team's social
behavior to predict future observations during execution (reducing monitoring
uncertainty); and (iii) monitoring algorithms that trade expressivity for
scalability  representing only certain useful monitoring hypotheses  but
allowing for any number of agents and their different activities to be
represented in a single coherent entity. We present an empirical evaluation of
these techniques  in combination and apart  in monitoring a deployed team of
agents  running on machines physically distributed across the country  and
engaged in complex  dynamic task execution. We also compare the performance of
these techniques to human expert and novice monitors  and show that the
techniques presented are capable of monitoring at human-expert levels  despite
the difficulty of the task."
"Spoken dialogue systems promise efficient and natural access to a large
variety of information sources and services from any phone. However  current
spoken dialogue systems are deficient in their strategies for preventing 
identifying and repairing problems that arise in the conversation. This paper
reports results on automatically training a Problematic Dialogue Predictor to
predict problematic human-computer dialogues using a corpus of 4692 dialogues
collected with the 'How May I Help You' (SM) spoken dialogue system. The
Problematic Dialogue Predictor can be immediately applied to the system's
decision of whether to transfer the call to a human customer care agent  or be
used as a cue to the system's dialogue manager to modify its behavior to repair
problems  and even perhaps  to prevent them. We show that a Problematic
Dialogue Predictor using automatically-obtainable features from the first two
exchanges in the dialogue can predict problematic dialogues 13.2% more
accurately than the baseline."
"Recent advances in the study of voting classification algorithms have brought
empirical and theoretical results clearly showing the discrimination power of
ensemble classifiers. It has been previously argued that the search of this
classification power in the design of the algorithms has marginalized the need
to obtain interpretable classifiers. Therefore  the question of whether one
might have to dispense with interpretability in order to keep classification
strength is being raised in a growing number of machine learning or data mining
papers. The purpose of this paper is to study both theoretically and
empirically the problem. First  we provide numerous results giving insight into
the hardness of the simplicity-accuracy tradeoff for voting classifiers. Then
we provide an efficient ""top-down and prune"" induction heuristic  WIDC  mainly
derived from recent results on the weak learning and boosting frameworks. It is
to our knowledge the first attempt to build a voting classifier as a base
formula using the weak learning framework (the one which was previously highly
successful for decision tree induction)  and not the strong learning framework
(as usual for such classifiers with boosting-like approaches). While it uses a
well-known induction scheme previously successful in other classes of concept
representations  thus making it easy to implement and compare  WIDC also relies
on recent or new results we give about particular cases of boosting known as
partition boosting and ranking loss boosting. Experimental results on
thirty-one domains  most of which readily available  tend to display the
ability of WIDC to produce small  accurate  and interpretable decision
committees."
"We propose a perspective on knowledge compilation which calls for analyzing
different compilation approaches according to two key dimensions. the
succinctness of the target compilation language  and the class of queries and
transformations that the language supports in polytime. We then provide a
knowledge compilation map  which analyzes a large number of existing target
compilation languages according to their succinctness and their polytime
transformations and queries. We argue that such analysis is necessary for
placing new compilation approaches within the context of existing ones. We also
go beyond classical  flat target compilation languages based on CNF and DNF 
and consider a richer  nested class based on directed acyclic graphs (such as
OBDDs)  which we show to include a relatively large number of target
compilation languages."
"The problem of organizing information for multidocument summarization so that
the generated summary is coherent has received relatively little attention.
While sentence ordering for single document summarization can be determined
from the ordering of sentences in the input article  this is not the case for
multidocument summarization where summary sentences may be drawn from different
input articles. In this paper  we propose a methodology for studying the
properties of ordering information in the news genre and describe experiments
done on a corpus of multiple acceptable orderings we developed for the task.
Based on these experiments  we implemented a strategy for ordering information
that combines constraints from chronological order of events and topical
relatedness. Evaluation of our augmented algorithm shows a significant
improvement of the ordering over two baseline strategies."
"We consider the problem of designing the the utility functions of the
utility-maximizing agents in a multi-agent system so that they work
synergistically to maximize a global utility. The particular problem domain we
explore is the control of network routing by placing agents on all the routers
in the network. Conventional approaches to this task have the agents all use
the Ideal Shortest Path routing Algorithm (ISPA). We demonstrate that in many
cases  due to the side-effects of one agent's actions on another agent's
performance  having agents use ISPA's is suboptimal as far as global aggregate
cost is concerned  even when they are only used to route infinitesimally small
amounts of traffic. The utility functions of the individual agents are not
""aligned"" with the global utility  intuitively speaking. As a particular
example of this we present an instance of Braess' paradox in which adding new
links to a network whose agents all use the ISPA results in a decrease in
overall throughput. We also demonstrate that load-balancing  in which the
agents' decisions are collectively made to optimize the global cost incurred by
all traffic currently being routed  is suboptimal as far as global cost
averaged across time is concerned. This is also due to 'side-effects'  in this
case of current routing decision on future traffic. The mathematics of
Collective Intelligence (COIN) is concerned precisely with the issue of
avoiding such deleterious side-effects in multi-agent systems  both over time
and space. We present key concepts from that mathematics and use them to derive
an algorithm whose ideal version should have better performance than that of
having all agents use the ISPA  even in the infinitesimal limit. We present
experiments verifying this  and also showing that a machine-learning-based
version of this COIN algorithm in which costs are only imprecisely estimated
via empirical means (a version potentially applicable in the real world) also
outperforms the ISPA  despite having access to less information than does the
ISPA. In particular  this COIN algorithm almost always avoids Braess' paradox."
"This paper addresses the problem of planning under uncertainty in large
Markov Decision Processes (MDPs). Factored MDPs represent a complex state space
using state variables and the transition model using a dynamic Bayesian
network. This representation often allows an exponential reduction in the
representation size of structured MDPs  but the complexity of exact solution
algorithms for such MDPs can grow exponentially in the representation size. In
this paper  we present two approximate solution algorithms that exploit
structure in factored MDPs. Both use an approximate value function represented
as a linear combination of basis functions  where each basis function involves
only a small subset of the domain variables. A key contribution of this paper
is that it shows how the basic operations of both algorithms can be performed
efficiently in closed form  by exploiting both additive and context-specific
structure in a factored MDP. A central element of our algorithms is a novel
linear program decomposition technique  analogous to variable elimination in
Bayesian networks  which reduces an exponentially large LP to a provably
equivalent  polynomial-sized one. One algorithm uses approximate linear
programming  and the second approximate dynamic programming. Our dynamic
programming algorithm is novel in that it uses an approximation based on
max-norm  a technique that more directly minimizes the terms that appear in
error bounds for approximate MDP algorithms. We provide experimental results on
problems with over 10^40 states  demonstrating a promising indication of the
scalability of our approach  and compare our algorithm to an existing
state-of-the-art approach  showing  in some problems  exponential gains in
computation time."
"The human intelligence lies in the algorithm  the nature of algorithm lies in
the classification  and the classification is equal to outlier detection. A lot
of algorithms have been proposed to detect outliers  meanwhile a lot of
definitions. Unsatisfying point is that definitions seem vague  which makes the
solution an ad hoc one. We analyzed the nature of outliers  and give two clear
definitions. We then develop an efficient RDD algorithm  which converts outlier
problem to pattern and degree problem. Furthermore  a collapse mechanism was
introduced by IIR algorithm  which can be united seamlessly with the RDD
algorithm and serve for the final decision. Both algorithms are originated from
the study on general AI. The combined edition is named as Pe algorithm  which
is the basis of the intelligent decision. Here we introduce longest k-turn
subsequence problem and corresponding solution as an example to interpret the
function of Pe algorithm in detecting curve-type outliers. We also give a
comparison between IIR algorithm and Pe algorithm  where we can get a better
understanding at both algorithms. A short discussion about intelligence is
added to demonstrate the function of the Pe algorithm. Related experimental
results indicate its robustness."
"We present a novel Natural Evolution Strategy (NES) variant  the Rank-One NES
(R1-NES)  which uses a low rank approximation of the search distribution
covariance matrix. The algorithm allows computation of the natural gradient
with cost linear in the dimensionality of the parameter space  and excels in
solving high-dimensional non-separable problems  including the best result to
date on the Rosenbrock function (512 dimensions)."
"Galles and Pearl claimed that ""for recursive models  the causal model
framework does not add any restrictions to counterfactuals  beyond those
imposed by Lewis's [possible-worlds] framework."" This claim is examined
carefully  with the goal of clarifying the exact relationship between causal
models and Lewis's framework. Recursive models are shown to correspond
precisely to a subclass of (possible-world) counterfactual structures. On the
other hand  a slight generalization of recursive models  models where all
equations have unique solutions  is shown to be incomparable in expressive
power to counterfactual structures  despite the fact that the Galles and Pearl
arguments should apply to them as well. The problem with the Galles and Pearl
argument is identified. an axiom that they viewed as irrelevant  because it
involved disjunction (which was not in their language)  is not irrelevant at
all."
"We look more carefully at the modeling of causality using structural
equations. It is clear that the structural equations can have a major impact on
the conclusions we draw about causality. In particular  the choice of variables
and their values can also have a significant impact on causality. These choices
are  to some extent  subjective. We consider what counts as an appropriate
choice. More generally  we consider what makes a model an appropriate model 
especially if we want to take defaults into account  as was argued is necessary
in recent work."
"Two distinct algorithms are presented to extract (schemata of) resolution
proofs from closed tableaux for propositional schemata. The first one handles
the most efficient version of the tableau calculus but generates very complex
derivations (denoted by rather elaborate rewrite systems). The second one has
the advantage that much simpler systems can be obtained  however the considered
proof procedure is less efficient."
"In the current study we examine an application of the machine learning
methods to model the retention constants in the thin layer chromatography
(TLC). This problem can be described with hundreds or even thousands of
descriptors relevant to various molecular properties  most of them redundant
and not relevant for the retention constant prediction. Hence we employed
feature selection to significantly reduce the number of attributes.
Additionally we have tested application of the bagging procedure to the feature
selection. The random forest regression models were built using selected
variables. The resulting models have better correlation with the experimental
data than the reference models obtained with linear regression. The
cross-validation confirms robustness of the models."
"Nowadays ontologies present a growing interest in Data Fusion applications.
As a matter of fact  the ontologies are seen as a semantic tool for describing
and reasoning about sensor data  objects  relations and general domain
theories. In addition  uncertainty is perhaps one of the most important
characteristics of the data and information handled by Data Fusion. However 
the fundamental nature of ontologies implies that ontologies describe only
asserted and veracious facts of the world. Different probabilistic  fuzzy and
evidential approaches already exist to fill this gap; this paper recaps the
most popular tools. However none of the tools meets exactly our purposes.
Therefore  we constructed a Dempster-Shafer ontology that can be imported into
any specific domain ontology and that enables us to instantiate it in an
uncertain manner. We also developed a Java application that enables reasoning
about these uncertain ontological instances."
"Individuals have an intuitive perception of what makes a good coincidence.
Though the sensitivity to coincidences has often been presented as resulting
from an erroneous assessment of probability  it appears to be a genuine
competence  based on non-trivial computations. The model presented here
suggests that coincidences occur when subjects perceive complexity drops.
Co-occurring events are  together  simpler than if considered separately. This
model leads to a possible redefinition of subjective probability."
"The study of opinions  their formation and change  is one of the defining
topics addressed by social psychology  but in recent years other disciplines 
like computer science and complexity  have tried to deal with this issue.
Despite the flourishing of different models and theories in both fields 
several key questions still remain unanswered. The understanding of how
opinions change and the way they are affected by social influence are
challenging issues requiring a thorough analysis of opinion per se but also of
the way in which they travel between agents' minds and are modulated by these
exchanges. To account for the two-faceted nature of opinions  which are mental
entities undergoing complex social processes  we outline a preliminary model in
which a cognitive theory of opinions is put forward and it is paired with a
formal description of them and of their spreading among minds. Furthermore 
investigating social influence also implies the necessity to account for the
way in which people change their minds  as a consequence of interacting with
other people  and the need to explain the higher or lower persistence of such
changes."
"The study of opinions  their formation and change  is one of the defining
topics addressed by social psychology  but in recent years other disciplines 
as computer science and complexity  have addressed this challenge. Despite the
flourishing of different models and theories in both fields  several key
questions still remain unanswered. The aim of this paper is to challenge the
current theories on opinion by putting forward a cognitively grounded model
where opinions are described as specific mental representations whose main
properties are put forward. A comparison with reputation will be also
presented."
"For large  real-world inductive learning problems  the number of training
examples often must be limited due to the costs associated with procuring 
preparing  and storing the training examples and/or the computational costs
associated with learning from them. In such circumstances  one question of
practical importance is. if only n training examples can be selected  in what
proportion should the classes be represented? In this article we help to answer
this question by analyzing  for a fixed training-set size  the relationship
between the class distribution of the training data and the performance of
classification trees induced from these data. We study twenty-six data sets
and  for each  determine the best class distribution for learning. The
naturally occurring class distribution is shown to generally perform well when
classifier performance is evaluated using undifferentiated error rate (0/1
loss). However  when the area under the ROC curve is used to evaluate
classifier performance  a balanced distribution is shown to perform well. Since
neither of these choices for class distribution always generates the
best-performing classifier  we introduce a budget-sensitive progressive
sampling algorithm for selecting training examples based on the class
associated with each example. An empirical analysis of this algorithm shows
that the class distribution of the resulting training set yields classifiers
with good (nearly-optimal) classification performance."
"In recent years research in the planning community has moved increasingly
toward s application of planners to realistic problems involving both time and
many typ es of resources. For example  interest in planning demonstrated by the
space res earch community has inspired work in observation scheduling 
planetary rover ex ploration and spacecraft control domains. Other temporal and
resource-intensive domains including logistics planning  plant control and
manufacturing have also helped to focus the community on the modelling and
reasoning issues that must be confronted to make planning technology meet the
challenges of application. The International Planning Competitions have acted
as an important motivating fo rce behind the progress that has been made in
planning since 1998. The third com petition (held in 2002) set the planning
community the challenge of handling tim e and numeric resources. This
necessitated the development of a modelling langua ge capable of expressing
temporal and numeric properties of planning domains. In this paper we describe
the language  PDDL2.1  that was used in the competition. We describe the syntax
of the language  its formal semantics and the validation of concurrent plans.
We observe that PDDL2.1 has considerable modelling power --- exceeding the
capabilities of current planning technology --- and presents a number of
important challenges to the research community."
"Despite the significant progress in multiagent teamwork  existing research
does not address the optimality of its prescriptions nor the complexity of the
teamwork problem. Without a characterization of the optimality-complexity
tradeoffs  it is impossible to determine whether the assumptions and
approximations made by a particular theory gain enough efficiency to justify
the losses in overall performance. To provide a tool for use by multiagent
researchers in evaluating this tradeoff  we present a unified framework  the
COMmunicative Multiagent Team Decision Problem (COM-MTDP). The COM-MTDP model
combines and extends existing multiagent theories  such as decentralized
partially observable Markov decision processes and economic team theory. In
addition to their generality of representation  COM-MTDPs also support the
analysis of both the optimality of team performance and the computational
complexity of the agents' decision problem. In analyzing complexity  we present
a breakdown of the computational complexity of constructing optimal teams under
various classes of problem domains  along the dimensions of observability and
communication cost. In analyzing optimality  we exploit the COM-MTDP's ability
to encode existing teamwork theories and models to encode two instantiations of
joint intentions theory taken from the literature. Furthermore  the COM-MTDP
model provides a basis for the development of novel team coordination
algorithms. We derive a domain-independent criterion for optimal communication
and provide a comparative analysis of the two joint intentions instantiations
with respect to this optimal policy. We have implemented a reusable 
domain-independent software package based on COM-MTDPs to analyze teamwork
coordination strategies  and we demonstrate its use by encoding and evaluating
the two joint intentions strategies within an example domain."
"Adjustable autonomy refers to entities dynamically varying their own
autonomy  transferring decision-making control to other entities (typically
agents transferring control to human users) in key situations. Determining
whether and when such transfers-of-control should occur is arguably the
fundamental research problem in adjustable autonomy. Previous work has
investigated various approaches to addressing this problem but has often
focused on individual agent-human interactions. Unfortunately  domains
requiring collaboration between teams of agents and humans reveal two key
shortcomings of these previous approaches. First  these approaches use rigid
one-shot transfers of control that can result in unacceptable coordination
failures in multiagent settings. Second  they ignore costs (e.g.  in terms of
time delays or effects on actions) to an agent's team due to such
transfers-of-control. To remedy these problems  this article presents a novel
approach to adjustable autonomy  based on the notion of a transfer-of-control
strategy. A transfer-of-control strategy consists of a conditional sequence of
two types of actions. (i) actions to transfer decision-making control (e.g. 
from an agent to a user or vice versa) and (ii) actions to change an agent's
pre-specified coordination constraints with team members  aimed at minimizing
miscoordination costs. The goal is for high-quality individual decisions to be
made with minimal disruption to the coordination of the team. We present a
mathematical model of transfer-of-control strategies. The model guides and
informs the operationalization of the strategies using Markov Decision
Processes  which select an optimal strategy  given an uncertain environment and
costs to the individuals and teams. The approach has been carefully evaluated 
including via its use in a real-world  deployed multi-agent system that assists
a research group in its daily activities."
"In this paper  we analyze the decision version of the NK landscape model from
the perspective of threshold phenomena and phase transitions under two random
distributions  the uniform probability model and the fixed ratio model. For the
uniform probability model  we prove that the phase transition is easy in the
sense that there is a polynomial algorithm that can solve a random instance of
the problem with the probability asymptotic to 1 as the problem size tends to
infinity. For the fixed ratio model  we establish several upper bounds for the
solubility threshold  and prove that random instances with parameters above
these upper bounds can be solved polynomially. This  together with our
empirical study for random instances generated below and in the phase
transition region  suggests that the phase transition of the fixed ratio model
is also easy."
"This paper presents an approach to expert-guided subgroup discovery. The main
step of the subgroup discovery process  the induction of subgroup descriptions 
is performed by a heuristic beam search algorithm  using a novel parametrized
definition of rule quality which is analyzed in detail. The other important
steps of the proposed subgroup discovery process are the detection of
statistically significant properties of selected subgroups and subgroup
visualization. statistically significant properties are used to enrich the
descriptions of induced subgroups  while the visualization shows subgroup
properties in the form of distributions of the numbers of examples in the
subgroups. The approach is illustrated by the results obtained for a medical
problem of early detection of patient risk groups."
"Independence -- the study of what is relevant to a given problem of reasoning
-- has received an increasing attention from the AI community. In this paper 
we consider two basic forms of independence  namely  a syntactic one and a
semantic one. We show features and drawbacks of them. In particular  while the
syntactic form of independence is computationally easy to check  there are
cases in which things that intuitively are not relevant are not recognized as
such. We also consider the problem of forgetting  i.e.  distilling from a
knowledge base only the part that is relevant to the set of queries constructed
from a subset of the alphabet. While such process is computationally hard  it
allows for a simplification of subsequent reasoning  and can thus be viewed as
a form of compilation. once the relevant part of a knowledge base has been
extracted  all reasoning tasks to be performed can be simplified."
"We present a probabilistic generative model for timing deviations in
expressive music performance. The structure of the proposed model is equivalent
to a switching state space model. The switch variables correspond to discrete
note locations as in a musical score. The continuous hidden variables denote
the tempo. We formulate two well known music recognition problems  namely tempo
tracking and automatic transcription (rhythm quantization) as filtering and
maximum a posteriori (MAP) state estimation tasks. Exact computation of
posterior features such as the MAP state is intractable in this model class  so
we introduce Monte Carlo methods for integration and optimization. We compare
Markov Chain Monte Carlo (MCMC) methods (such as Gibbs sampling  simulated
annealing and iterative improvement) and sequential Monte Carlo methods
(particle filters). Our simulation results suggest better results with
sequential methods. The methods can be applied in both online and batch
scenarios such as tempo tracking and transcription and are thus potentially
useful in a number of music applications such as adaptive automatic
accompaniment  score typesetting and music information retrieval."
"Bayesian belief networks have grown to prominence because they provide
compact representations for many problems for which probabilistic inference is
appropriate  and there are algorithms to exploit this compactness. The next
step is to allow compact representations of the conditional probabilities of a
variable given its parents. In this paper we present such a representation that
exploits contextual independence in terms of parent contexts; which variables
act as parents may depend on the value of other variables. The internal
representation is in terms of contextual factors (confactors) that is simply a
pair of a context and a table. The algorithm  contextual variable elimination 
is based on the standard variable elimination algorithm that eliminates the
non-query variables in turn  but when eliminating a variable  the tables that
need to be multiplied can depend on the context. This algorithm reduces to
standard variable elimination when there is no contextual independence
structure to exploit. We show how this can be much more efficient than variable
elimination when there is structure to exploit. We explain why this new method
can exploit more structure than previous methods for structured belief network
inference and an analogous algorithm that uses trees."
"In this article we present an algorithm to compute bounds on the marginals of
a graphical model. For several small clusters of nodes upper and lower bounds
on the marginal values are computed independently of the rest of the network.
The range of allowed probability distributions over the surrounding nodes is
restricted using earlier computed bounds. As we will show  this can be
considered as a set of constraints in a linear programming problem of which the
objective function is the marginal probability of the center nodes. In this way
knowledge about the maginals of neighbouring clusters is passed to other
clusters thereby tightening the bounds on their marginals. We show that sharp
bounds can be obtained for undirected and directed graphs that are used for
practical applications  but for which exact computations are infeasible."
"Policies of Markov Decision Processes (MDPs) determine the next action to
execute from the current state and  possibly  the history (the past states).
When the number of states is large  succinct representations are often used to
compactly represent both the MDPs and the policies in a reduced amount of
space. In this paper  some problems related to the size of succinctly
represented policies are analyzed. Namely  it is shown that some MDPs have
policies that can only be represented in space super-polynomial in the size of
the MDP  unless the polynomial hierarchy collapses. This fact motivates the
study of the problem of deciding whether a given MDP has a policy of a given
size and reward. Since some algorithms for MDPs work by finding a succinct
representation of the value function  the problem of deciding the existence of
a succinct representation of a value function of a given size and reward is
also considered."
"We describe a system for specifying the effects of actions. Unlike those
commonly used in AI planning  our system uses an action description language
that allows one to specify the effects of actions using domain rules  which are
state constraints that can entail new action effects from old ones.
Declaratively  an action domain in our language corresponds to a nonmonotonic
causal theory in the situation calculus. Procedurally  such an action domain is
compiled into a set of logical theories  one for each action in the domain 
from which fully instantiated successor state-like axioms and STRIPS-like
systems are then generated. We expect the system to be a useful tool for
knowledge engineers writing action specifications for classical AI planning
systems  GOLOG systems  and other systems where formal specifications of
actions are needed."
"VHPOP is a partial order causal link (POCL) planner loosely based on UCPOP.
It draws from the experience gained in the early to mid 1990's on flaw
selection strategies for POCL planning  and combines this with more recent
developments in the field of domain independent planning such as distance based
heuristics and reachability analysis. We present an adaptation of the additive
heuristic for plan space planning  and modify it to account for possible reuse
of existing actions in a plan. We also propose a large set of novel flaw
selection strategies  and show how these can help us solve more problems than
previously possible by POCL planners. VHPOP also supports planning with
durative actions by incorporating standard techniques for temporal constraint
reasoning. We demonstrate that the same heuristic techniques used to boost the
performance of classical POCL planning can be effective in domains with
durative actions as well. The result is a versatile heuristic POCL planner
competitive with established CSP-based and heuristic state space planners."
"The SHOP2 planning system received one of the awards for distinguished
performance in the 2002 International Planning Competition. This paper
describes the features of SHOP2 which enabled it to excel in the competition 
especially those aspects of SHOP2 that deal with temporal and metric planning
domains."
"Hierarchical task decomposition is a method used in many agent systems to
organize agent knowledge. This work shows how the combination of a hierarchy
and persistent assertions of knowledge can lead to difficulty in maintaining
logical consistency in asserted knowledge. We explore the problematic
consequences of persistent assumptions in the reasoning process and introduce
novel potential solutions. Having implemented one of the possible solutions 
Dynamic Hierarchical Justification  its effectiveness is demonstrated with an
empirical analysis."
"The proliferation of online information sources has led to an increased use
of wrappers for extracting data from Web sources. While most of the previous
research has focused on quick and efficient generation of wrappers  the
development of tools for wrapper maintenance has received less attention. This
is an important research problem because Web sources often change in ways that
prevent the wrappers from extracting data correctly. We present an efficient
algorithm that learns structural information about data from positive examples
alone. We describe how this information can be used for two wrapper maintenance
applications. wrapper verification and reinduction. The wrapper verification
system detects when a wrapper is not extracting correct data  usually because
the Web source has changed its format. The reinduction algorithm automatically
recovers from changes in the Web source by identifying data on Web pages so
that a new wrapper may be generated for this source. To validate our approach 
we monitored 27 wrappers over a period of a year. The verification algorithm
correctly discovered 35 of the 37 wrapper changes  and made 16 mistakes 
resulting in precision of 0.73 and recall of 0.95. We validated the reinduction
algorithm on ten Web sources. We were able to successfully reinduce the
wrappers  obtaining precision and recall values of 0.90 and 0.80 on the data
extraction task."
"The cognitive research on reputation has shown several interesting properties
that can improve both the quality of services and the security in distributed
electronic environments. In this paper  the impact of reputation on
decision-making under scarcity of information will be shown. First  a cognitive
theory of reputation will be presented  then a selection of simulation
experimental results from different studies will be discussed. Such results
concern the benefits of reputation when agents need to find out good sellers in
a virtual market-place under uncertainty and informational cheating."
"In this paper we examine the application of the random forest classifier for
the all relevant feature selection problem. To this end we first examine two
recently proposed all relevant feature selection algorithms  both being a
random forest wrappers  on a series of synthetic data sets with varying size.
We show that reasonable accuracy of predictions can be achieved and that
heuristic algorithms that were designed to handle the all relevant problem 
have performance that is close to that of the reference ideal algorithm. Then 
we apply one of the algorithms to four families of semi-synthetic data sets to
assess how the properties of particular data set influence results of feature
selection. Finally we test the procedure using a well-known gene expression
data set. The relevance of nearly all previously established important genes
was confirmed  moreover the relevance of several new ones is discovered."
"Unary operator domains -- i.e.  domains in which operators have a single
effect -- arise naturally in many control problems. In its most general form 
the problem of STRIPS planning in unary operator domains is known to be as hard
as the general STRIPS planning problem -- both are PSPACE-complete. However 
unary operator domains induce a natural structure  called the domain's causal
graph. This graph relates between the preconditions and effect of each domain
operator. Causal graphs were exploited by Williams and Nayak in order to
analyze plan generation for one of the controllers in NASA's Deep-Space One
spacecraft. There  they utilized the fact that when this graph is acyclic  a
serialization ordering over any subgoal can be obtained quickly. In this paper
we conduct a comprehensive study of the relationship between the structure of a
domain's causal graph and the complexity of planning in this domain. On the
positive side  we show that a non-trivial polynomial time plan generation
algorithm exists for domains whose causal graph induces a polytree with a
constant bound on its node indegree. On the negative side  we show that even
plan existence is hard when the graph is a directed-path singly connected DAG.
More generally  we show that the number of paths in the causal graph is closely
related to the complexity of planning in the associated domain. Finally we
relate our results to the question of complexity of planning with serializable
subgoals."
"Recently  planning based on answer set programming has been proposed as an
approach towards realizing declarative planning systems. In this paper  we
present the language Kc  which extends the declarative planning language K by
action costs. Kc provides the notion of admissible and optimal plans  which are
plans whose overall action costs are within a given limit resp. minimum over
all plans (i.e.  cheapest plans). As we demonstrate  this novel language allows
for expressing some nontrivial planning tasks in a declarative way.
Furthermore  it can be utilized for representing planning problems under other
optimality criteria  such as computing ``shortest'' plans (with the least
number of steps)  and refinement combinations of cheapest and fastest plans. We
study complexity aspects of the language Kc and provide a transformation to
logic programs  such that planning problems are solved via answer set
programming. Furthermore  we report experimental results on selected problems.
Our experience is encouraging that answer set planning may be a valuable
approach to expressive planning systems in which intricate planning problems
can be naturally specified and solved."
"In common-interest stochastic games all players receive an identical payoff.
Players participating in such games must learn to coordinate with each other in
order to receive the highest-possible value. A number of reinforcement learning
algorithms have been proposed for this problem  and some have been shown to
converge to good solutions in the limit. In this paper we show that using very
simple model-based algorithms  much better (i.e.  polynomial) convergence rates
can be attained. Moreover  our model-based algorithms are guaranteed to
converge to the optimal value  unlike many of the existing algorithms."
"SAPA is a domain-independent heuristic forward chaining planner that can
handle durative actions  metric resource constraints  and deadline goals. It is
designed to be capable of handling the multi-objective nature of metric
temporal planning. Our technical contributions include (i) planning-graph based
methods for deriving heuristics that are sensitive to both cost and makespan
(ii) techniques for adjusting the heuristic estimates to take action
interactions and metric resource limitations into account and (iii) a linear
time greedy post-processing technique to improve execution flexibility of the
solution plans. An implementation of SAPA using many of the techniques
presented in this paper was one of the best domain independent planners for
domains with metric and temporal constraints in the third International
Planning Competition  held at AIPS-02. We describe the technical details of
extracting the heuristics and present an empirical evaluation of the current
implementation of SAPA."
"The recent emergence of heavily-optimized modal decision procedures has
highlighted the key role of empirical testing in this domain. Unfortunately 
the introduction of extensive empirical tests for modal logics is recent  and
so far none of the proposed test generators is very satisfactory. To cope with
this fact  we present a new random generation method that provides benefits
over previous methods for generating empirical tests. It fixes and much
generalizes one of the best-known methods  the random CNF_[]m test  allowing
for generating a much wider variety of problems  covering in principle the
whole input space. Our new method produces much more suitable test sets for the
current generation of modal decision procedures. We analyze the features of the
new method by means of an extensive collection of empirical tests."
"Despite their near dominance  heuristic state search planners still lag
behind disjunctive planners in the generation of parallel plans in classical
planning. The reason is that directly searching for parallel solutions in state
space planners would require the planners to branch on all possible subsets of
parallel actions  thus increasing the branching factor exponentially. We
present a variant of our heuristic state search planner AltAlt  called AltAltp
which generates parallel plans by using greedy online parallelization of
partial plans. The greedy approach is significantly informed by the use of
novel distance heuristics that AltAltp derives from a graphplan-style planning
graph for the problem. While this approach is not guaranteed to provide optimal
parallel plans  empirical results show that AltAltp is capable of generating
good quality parallel plans at a fraction of the cost incurred by the
disjunctive planners."
"We address the problem of propositional logic-based abduction  i.e.  the
problem of searching for a best explanation for a given propositional
observation according to a given propositional knowledge base. We give a
general algorithm  based on the notion of projection; then we study
restrictions over the representations of the knowledge base and of the query 
and find new polynomial classes of abduction problems."
"We present some techniques for planning in domains specified with the recent
standard language PDDL2.1  supporting 'durative actions' and numerical
quantities. These techniques are implemented in LPG  a domain-independent
planner that took part in the 3rd International Planning Competition (IPC). LPG
is an incremental  any time system producing multi-criteria quality plans. The
core of the system is based on a stochastic local search method and on a
graph-based representation called 'Temporal Action Graphs' (TA-graphs). This
paper focuses on temporal planning  introducing TA-graphs and proposing some
techniques to guide the search in LPG using this representation. The
experimental results of the 3rd IPC  as well as further results presented in
this paper  show that our techniques can be very effective. Often LPG
outperforms all other fully-automated planners of the 3rd IPC in terms of speed
to derive a solution  or quality of the solutions that can be produced."
"TALplanner is a forward-chaining planner that relies on domain knowledge in
the shape of temporal logic formulas in order to prune irrelevant parts of the
search space. TALplanner recently participated in the third International
Planning Competition  which had a clear emphasis on increasing the complexity
of the problem domains being used as benchmark tests and the expressivity
required to represent these domains in a planning system. Like many other
planners  TALplanner had support for some but not all aspects of this increase
in expressivity  and a number of changes to the planner were required. After a
short introduction to TALplanner  this article describes some of the changes
that were made before and during the competition. We also describe the process
of introducing suitable domain knowledge for several of the competition
domains."
"The automatic generation of decision trees based on off-line reasoning on
models of a domain is a reasonable compromise between the advantages of using a
model-based approach in technical domains and the constraints imposed by
embedded applications. In this paper we extend the approach to deal with
temporal information. We introduce a notion of temporal decision tree  which is
designed to make use of relevant information as long as it is acquired  and we
present an algorithm for compiling such trees from a model-based reasoning
system."
"The performance of anytime algorithms can be improved by simultaneously
solving several instances of algorithm-problem pairs. These pairs may include
different instances of a problem (such as starting from a different initial
state)  different algorithms (if several alternatives exist)  or several runs
of the same algorithm (for non-deterministic algorithms). In this paper we
present a methodology for designing an optimal scheduling policy based on the
statistical characteristics of the algorithms involved. We formally analyze the
case where the processes share resources (a single-processor model)  and
provide an algorithm for optimal scheduling. We analyze  theoretically and
empirically  the behavior of our scheduling algorithm for various distribution
types. Finally  we present empirical results of applying our scheduling
algorithm to the Latin Square problem."
"Auctions are becoming an increasingly popular method for transacting
business  especially over the Internet. This article presents a general
approach to building autonomous bidding agents to bid in multiple simultaneous
auctions for interacting goods. A core component of our approach learns a model
of the empirical price dynamics based on past data and uses the model to
analytically calculate  to the greatest extent possible  optimal bids. We
introduce a new and general boosting-based algorithm for conditional density
estimation problems of this kind  i.e.  supervised learning problems in which
the goal is to estimate the entire conditional distribution of the real-valued
label. This approach is fully implemented as ATTac-2001  a top-scoring agent in
the second Trading Agent Competition (TAC-01). We present experiments
demonstrating the effectiveness of our boosting-based price predictor relative
to several reasonable alternatives."
"Planning with numeric state variables has been a challenge for many years 
and was a part of the 3rd International Planning Competition (IPC-3). Currently
one of the most popular and successful algorithmic techniques in STRIPS
planning is to guide search by a heuristic function  where the heuristic is
based on relaxing the planning task by ignoring the delete lists of the
available actions. We present a natural extension of ``ignoring delete lists''
to numeric state variables  preserving the relevant theoretical properties of
the STRIPS relaxation under the condition that the numeric task at hand is
``monotonic''. We then identify a subset of the numeric IPC-3 competition
language  ``linear tasks''  where monotonicity can be achieved by
pre-processing. Based on that  we extend the algorithms used in the heuristic
planning system FF to linear tasks. The resulting system Metric-FF is 
according to the IPC-3 results which we discuss  one of the two currently most
efficient numeric planners."
"Nanson's and Baldwin's voting rules select a winner by successively
eliminating candidates with low Borda scores. We show that these rules have a
number of desirable computational properties. In particular  with unweighted
votes  it is NP-hard to manipulate either rule with one manipulator  whilst
with weighted votes  it is NP-hard to manipulate either rule with a small
number of candidates and a coalition of manipulators. As only a couple of other
voting rules are known to be NP-hard to manipulate with a single manipulator 
Nanson's and Baldwin's rules appear to be particularly resistant to
manipulation from a theoretical perspective. We also propose a number of
approximation methods for manipulating these two rules. Experiments demonstrate
that both rules are often difficult to manipulate in practice. These results
suggest that elimination style voting rules deserve further study."
"Search is a major technique for planning. It amounts to exploring a state
space of planning domains typically modeled as a directed graph. However 
prohibitively large sizes of the search space make search expensive. Developing
better heuristic functions has been the main technique for improving search
efficiency. Nevertheless  recent studies have shown that improving heuristics
alone has certain fundamental limits on improving search efficiency. Recently 
a new direction of research called partial order based reduction (POR) has been
proposed as an alternative to improving heuristics. POR has shown promise in
speeding up searches.
  POR has been extensively studied in model checking research and is a key
enabling technique for scalability of model checking systems. Although the POR
theory has been extensively studied in model checking  it has never been
developed systematically for planning before. In addition  the conditions for
POR in the model checking theory are abstract and not directly applicable in
planning. Previous works on POR algorithms for planning did not establish the
connection between these algorithms and existing theory in model checking.
  In this paper  we develop a theory for POR in planning. The new theory we
develop connects the stubborn set theory in model checking and POR methods in
planning. We show that previous POR algorithms in planning can be explained by
the new theory. Based on the new theory  we propose a new  stronger POR
algorithm. Experimental results on various planning domains show further search
cost reduction using the new algorithm."
"Set and multiset variables in constraint programming have typically been
represented using subset bounds. However  this is a weak representation that
neglects potentially useful information about a set such as its cardinality.
For set variables  the length-lex (LL) representation successfully provides
information about the length (cardinality) and position in the lexicographic
ordering. For multiset variables  where elements can be repeated  we consider
richer representations that take into account additional information. We study
eight different representations in which we maintain bounds according to one of
the eight different orderings. length-(co)lex (LL/LC)  variety-(co)lex (VL/VC) 
length-variety-(co)lex (LVL/LVC)  and variety-length-(co)lex (VLL/VLC)
orderings. These representations integrate together information about the
cardinality  variety (number of distinct elements in the multiset)  and
position in some total ordering. Theoretical and empirical comparisons of
expressiveness and compactness of the eight representations suggest that
length-variety-(co)lex (LVL/LVC) and variety-length-(co)lex (VLL/VLC) usually
give tighter bounds after constraint propagation. We implement the eight
representations and evaluate them against the subset bounds representation with
cardinality and variety reasoning. Results demonstrate that they offer
significantly better pruning and runtime."
"This paper reports the outcome of the third in the series of biennial
international planning competitions  held in association with the International
Conference on AI Planning and Scheduling (AIPS) in 2002. In addition to
describing the domains  the planners and the objectives of the competition  the
paper includes analysis of the results. The results are analysed from several
perspectives  in order to address the questions of comparative performance
between planners  comparative difficulty of domains  the degree of agreement
between planners about the relative difficulty of individual problem instances
and the question of how well planners scale relative to one another over
increasingly difficult problems. The paper addresses these questions through
statistical analysis of the raw results of the competition  in order to
determine which results can be considered to be adequately supported by the
data. The paper concludes with a discussion of some challenges for the future
of the competition series."
"As computational agents are developed for increasingly complicated e-commerce
applications  the complexity of the decisions they face demands advances in
artificial intelligence techniques. For example  an agent representing a seller
in an auction should try to maximize the seller's profit by reasoning about a
variety of possibly uncertain pieces of information  such as the maximum prices
various buyers might be willing to pay  the possible prices being offered by
competing sellers  the rules by which the auction operates  the dynamic arrival
and matching of offers to buy and sell  and so on. A naive application of
multiagent reasoning techniques would require the seller's agent to explicitly
model all of the other agents through an extended time horizon  rendering the
problem intractable for many realistically-sized problems. We have instead
devised a new strategy that an agent can use to determine its bid price based
on a more tractable Markov chain model of the auction process. We have
experimentally identified the conditions under which our new strategy works
well  as well as how well it works in comparison to the optimal performance the
agent could have achieved had it known the future. Our results show that our
new strategy in general performs well  outperforming other tractable heuristic
strategies in a majority of experiments  and is particularly effective in a
'seller?s market'  where many buy offers are available."
"OWL 2 has been standardized by the World Wide Web Consortium (W3C) as a
family of ontology languages for the Semantic Web. The most expressive of these
languages is OWL 2 Full  but to date no reasoner has been implemented for this
language. Consistency and entailment checking are known to be undecidable for
OWL 2 Full. We have translated a large fragment of the OWL 2 Full semantics
into first-order logic  and used automated theorem proving systems to do
reasoning based on this theory. The results are promising  and indicate that
this approach can be applied in practice for effective OWL reasoning  beyond
the capabilities of current Semantic Web reasoners.
  This is an extended version of a paper with the same title that has been
published at CADE 2011  LNAI 6803  pp. 446-460. The extended version provides
appendices with additional resources that were used in the reported evaluation."
"This paper introduces 'just enough' principles and 'systems engineering'
approach to the practice of ontology development to provide a minimal yet
complete  lightweight  agile and integrated development process  supportive of
stakeholder management and implementation independence."
"In this paper  we investigate the following question. how could you write
such computer programs that can work like conscious beings? The motivation
behind this question is that we want to create such applications that can see
the future. The aim of this paper is to provide an overall conceptual framework
for this new approach to machine consciousness. So we introduce a new
programming paradigm called Consciousness Oriented Programming (COP)."
"There has been a noticeable shift in the relative composition of the industry
in the developed countries in recent years; manufacturing is decreasing while
the service sector is becoming more important. However  currently most
simulation models for investigating service systems are still built in the same
way as manufacturing simulation models  using a process-oriented world view 
i.e. they model the flow of passive entities through a system. These kinds of
models allow studying aspects of operational management but are not well suited
for studying the dynamics that appear in service systems due to human
behaviour. For these kinds of studies we require tools that allow modelling the
system and entities using an object-oriented world view  where intelligent
objects serve as abstract ""actors"" that are goal directed and can behave
proactively. In our work we combine process-oriented discrete event simulation
modelling and object-oriented agent based simulation modelling to investigate
the impact of people management practices on retail productivity. In this
paper  we reveal in a series of experiments what impact considering proactivity
can have on the output accuracy of simulation models of human centric systems.
The model and data we use for this investigation are based on a case study in a
UK department store. We show that considering proactivity positively influences
the validity of these kinds of models and therefore allows analysts to make
better recommendations regarding strategies to apply people management
practises."
"A fact apparently not observed earlier in the literature of nonmonotonic
reasoning is that Reiter  in his default logic paper  did not directly
formalize informal defaults. Instead  he translated a default into a certain
natural language proposition and provided a formalization of the latter. A few
years later  Moore noted that propositions like the one used by Reiter are
fundamentally different than defaults and exhibit a certain autoepistemic
nature. Thus  Reiter had developed his default logic as a formalization of
autoepistemic propositions rather than of defaults.
  The first goal of this paper is to show that some problems of Reiter's
default logic as a formal way to reason about informal defaults are directly
attributable to the autoepistemic nature of default logic and to the mismatch
between informal defaults and the Reiter's formal defaults  the latter being a
formal expression of the autoepistemic propositions Reiter used as a
representation of informal defaults.
  The second goal of our paper is to compare the work of Reiter and Moore.
While each of them attempted to formalize autoepistemic propositions  the modes
of reasoning in their respective logics were different. We revisit Moore's and
Reiter's intuitions and present them from the perspective of autotheoremhood 
where theories can include propositions referring to the theory's own theorems.
We then discuss the formalization of this perspective in the logics of Moore
and Reiter  respectively  using the unifying semantic framework for default and
autoepistemic logics that we developed earlier. We argue that Reiter's default
logic is a better formalization of Moore's intuitions about autoepistemic
propositions than Moore's own autoepistemic logic."
"In 1991  Michael Gelfond introduced the language of epistemic specifications.
The goal was to develop tools for modeling problems that require some form of
meta-reasoning  that is  reasoning over multiple possible worlds. Despite their
relevance to knowledge representation  epistemic specifications have received
relatively little attention so far. In this paper  we revisit the formalism of
epistemic specification. We offer a new definition of the formalism  propose
several semantics (one of which  under syntactic restrictions we assume  turns
out to be equivalent to the original semantics by Gelfond)  derive some
complexity results and  finally  show the effectiveness of the formalism for
modeling problems requiring meta-reasoning considered recently by Faber and
Woltran. All these results show that epistemic specifications deserve much more
attention that has been afforded to them so far."
"We discuss the evolution of aspects of nonmonotonic reasoning towards the
computational paradigm of answer-set programming (ASP). We give a general
overview of the roots of ASP and follow up with the personal perspective on
research developments that helped verbalize the main principles of ASP and
differentiated it from the classical logic programming."
"UCT  a state-of-the art algorithm for Monte Carlo tree sampling (MCTS)  is
based on UCB  a sampling policy for the Multi-armed Bandit Problem (MAB) that
minimizes the accumulated regret. However  MCTS differs from MAB in that only
the final choice  rather than all arm pulls  brings a reward  that is  the
simple regret  as opposite to the cumulative regret  must be minimized. This
ongoing work aims at applying meta-reasoning techniques to MCTS  which is
non-trivial. We begin by introducing policies for multi-armed bandits with
lower simple regret than UCB  and an algorithm for MCTS which combines
cumulative and simple regret minimization and outperforms UCT. We also develop
a sampling scheme loosely based on a myopic version of perfect value of
information. Finite-time and asymptotic analysis of the policies is provided 
and the algorithms are compared empirically."
"Self-Organizing Maps are commonly used for unsupervised learning purposes.
This paper is dedicated to the certain modification of SOM called SOMN
(Self-Organizing Mixture Networks) used as a mechanism for representing
grayscale digital images. Any grayscale digital image regarded as a
distribution function can be approximated by the corresponding Gaussian
mixture. In this paper  the use of SOMN is proposed in order to obtain such
approximations for input grayscale images in unsupervised manner."
"Two different conceptions of emergence are reconciled as two instances of the
phenomenon of detection. In the process of comparing these two conceptions  we
find that the notions of complexity and detection allow us to form a unified
definition of emergence that clearly delineates the role of the observer."
"The aim of this paper is to announce the release of a novel system for
abstract argumentation which is based on decomposition and dynamic programming.
We provide first experimental evaluations to show the feasibility of this
approach."
"Dung's famous abstract argumentation frameworks represent the core formalism
for many problems and applications in the field of argumentation which
significantly evolved within the last decade. Recent work in the field has thus
focused on implementations for these frameworks  whereby one of the main
approaches is to use Answer-Set Programming (ASP). While some of the
argumentation semantics can be nicely expressed within the ASP language  others
required rather cumbersome encoding techniques. Recent advances in ASP systems 
in particular  the metasp optimization frontend for the ASP-package
gringo/claspD provides direct commands to filter answer sets satisfying certain
subset-minimality (or -maximality) constraints. This allows for much simpler
encodings compared to the ones in standard ASP language. In this paper  we
experimentally compare the original encodings (for the argumentation semantics
based on preferred  semi-stable  and respectively  stage extensions) with new
metasp encodings. Moreover  we provide novel encodings for the recently
introduced resolution-based grounded semantics. Our experimental results
indicate that the metasp approach works well in those cases where the
complexity of the encoded problem is adequately mirrored within the metasp
approach."
"In a knowledge discovery process  interpretation and evaluation of the mined
results are indispensable in practice. In the case of data clustering  however 
it is often difficult to see in what aspect each cluster has been formed. This
paper proposes a method for automatic and objective characterization or
""verbalization"" of the clusters obtained by mixture models  in which we collect
conjunctions of propositions (attribute-value pairs) that help us interpret or
evaluate the clusters. The proposed method provides us with a new  in-depth and
consistent tool for cluster interpretation/evaluation  and works for various
types of datasets including continuous attributes and missing values.
Experimental results with a couple of standard datasets exhibit the utility of
the proposed method  and the importance of the feedbacks from the
interpretation/evaluation step."
"A brain-computer interface (BCI) may be used to control a prosthetic or
orthotic hand using neural activity from the brain. The core of this
sensorimotor BCI lies in the interpretation of the neural information extracted
from electroencephalogram (EEG). It is desired to improve on the interpretation
of EEG to allow people with neuromuscular disorders to perform daily
activities. This paper investigates the possibility of discriminating between
the EEG associated with wrist and finger movements. The EEG was recorded from
test subjects as they executed and imagined five essential hand movements using
both hands. Independent component analysis (ICA) and time-frequency techniques
were used to extract spectral features based on event-related
(de)synchronisation (ERD/ERS)  while the Bhattacharyya distance (BD) was used
for feature reduction. Mahalanobis distance (MD) clustering and artificial
neural networks (ANN) were used as classifiers and obtained average accuracies
of 65 % and 71 % respectively. This shows that EEG discrimination between wrist
and finger movements is possible. The research introduces a new combination of
motor tasks to BCI research."
"We present a constraint-based approach to interactive product configuration.
Our configurator tool FdConfig is based on feature models for the
representation of the product domain. Such models can be directly mapped into
constraint satisfaction problems and dealt with by appropriate constraint
solvers. During the interactive configuration process the user generates new
constraints as a result of his configuration decisions and even may retract
constraints posted earlier. We discuss the configuration process  explain the
underlying techniques and show optimizations."
"Answer-Set Programming (ASP) is an established declarative programming
paradigm. However  classical ASP lacks subprogram calls as in procedural
programming  and access to external computations (like remote procedure calls)
in general. The feature is desired for increasing modularity and---assuming
proper access in place---(meta-)reasoning over subprogram results. While
HEX-programs extend classical ASP with external source access  they do not
support calls of (sub-)programs upfront. We present nested HEX-programs  which
extend HEX-programs to serve the desired feature  in a user-friendly manner.
Notably  the answer sets of called sub-programs can be individually accessed.
This is particularly useful for applications that need to reason over answer
sets like belief set merging  user-defined aggregate functions  or preferences
of answer sets."
"Statistical relational learning techniques have been successfully applied in
a wide range of relational domains. In most of these applications  the human
designers capitalized on their background knowledge by following a
trial-and-error trajectory  where relational features are manually defined by a
human engineer  parameters are learned for those features on the training data 
the resulting model is validated  and the cycle repeats as the engineer adjusts
the set of features. This paper seeks to streamline application development in
large relational domains by introducing a light-weight approach that
efficiently evaluates relational features on pieces of the relational graph
that are streamed to it one at a time. We evaluate our approach on two social
media tasks and demonstrate that it leads to more accurate models that are
learned faster."
"In order to give appropriate semantics to qualitative conditionals of the
form ""if A then normally B""  ordinal conditional functions (OCFs) ranking the
possible worlds according to their degree of plausibility can be used. An OCF
accepting all conditionals of a knowledge base R can be characterized as the
solution of a constraint satisfaction problem. We present a high-level 
declarative approach using constraint logic programming techniques for solving
this constraint satisfaction problem. In particular  the approach developed
here supports the generation of all minimal solutions; these minimal solutions
are of special interest as they provide a basis for model-based inference from
R."
"Publishing private data on external servers incurs the problem of how to
avoid unwanted disclosure of confidential data. We study a problem of
confidentiality in extended disjunctive logic programs and show how it can be
solved by extended abduction. In particular  we analyze how credulous
non-monotonic reasoning affects confidentiality."
"In this paper  we present domain-specific languages (DSLs) that we devised
for their use in the implementation of a finite domain constraint programming
system  available as library(clpfd) in SWI-Prolog and YAP-Prolog. These DSLs
are used in propagator selection and constraint reification. In these areas 
they lead to concise specifications that are easy to read and reason about. At
compilation time  these specifications are translated to Prolog code  reducing
interpretative run-time overheads. The devised languages can be used in the
implementation of other finite domain constraint solvers as well and may
contribute to their correctness  conciseness and efficiency."
"In this work a stand-alone preprocessor for SAT is presented that is able to
perform most of the known preprocessing techniques. Preprocessing a formula in
SAT is important for performance since redundancy can be removed. The
preprocessor is part of the SAT solver riss and is called Coprocessor. Not only
riss  but also MiniSat 2.2 benefit from it  because the SatELite preprocessor
of MiniSat does not implement recent techniques. By using more advanced
techniques  Coprocessor is able to reduce the redundancy in a formula further
and improves the overall solving performance."
"We present an application focused on the design of resilient long-reach
passive optical networks. We specifically consider dual-parented networks
whereby each customer must be connected to two metro sites via local exchange
sites. An important property of such a placement is resilience to single metro
node failure. The objective of the application is to determine the optimal
position of a set of metro nodes such that the total optical fibre length is
minimized. We prove that this problem is NP-Complete. We present two
alternative combinatorial optimisation approaches to finding an optimal metro
node placement using. a mixed integer linear programming (MIP) formulation of
the problem; and  a hybrid approach that uses clustering as a preprocessing
step. We consider a detailed case-study based on a network for Ireland. The
hybrid approach scales well and finds solutions that are close to optimal  with
a runtime that is two orders-of-magnitude better than the MIP model."
"Artificial general intelligence (AGI) refers to research aimed at tackling
the full problem of artificial intelligence  that is  create truly intelligent
agents. This sets it apart from most AI research which aims at solving
relatively narrow domains  such as character recognition  motion planning  or
increasing player satisfaction in games. But how do we know when an agent is
truly intelligent? A common point of reference in the AGI community is Legg and
Hutter's formal definition of universal intelligence  which has the appeal of
simplicity and generality but is unfortunately incomputable. Games of various
kinds are commonly used as benchmarks for ""narrow"" AI research  as they are
considered to have many important properties. We argue that many of these
properties carry over to the testing of general intelligence as well. We then
sketch how such testing could practically be carried out. The central part of
this sketch is an extension of universal intelligence to deal with finite time 
and the use of sampling of the space of games expressed in a suitably biased
game description language."
"We propose a structured approach to the problem of retrieval of images by
content and present a description logic that has been devised for the semantic
indexing and retrieval of images containing complex objects. As other
approaches do  we start from low-level features extracted with image analysis
to detect and characterize regions in an image. However  in contrast with
feature-based approaches  we provide a syntax to describe segmented regions as
basic objects and complex objects as compositions of basic ones. Then we
introduce a companion extensional semantics for defining reasoning services 
such as retrieval  classification  and subsumption. These services can be used
for both exact and approximate matching  using similarity measures. Using our
logical approach as a formal specification  we implemented a complete
client-server image retrieval system  which allows a user to pose both queries
by sketch and queries by example. A set of experiments has been carried out on
a testbed of images to assess the retrieval capabilities of the system in
comparison with expert users ranking. Results are presented adopting a
well-established measure of quality borrowed from textual information
retrieval."
"Wind energy plays an increasing role in the supply of energy world-wide. The
energy output of a wind farm is highly dependent on the weather condition
present at the wind farm. If the output can be predicted more accurately 
energy suppliers can coordinate the collaborative production of different
energy sources more efficiently to avoid costly overproductions.
  With this paper  we take a computer science perspective on energy prediction
based on weather data and analyze the important parameters as well as their
correlation on the energy output. To deal with the interaction of the different
parameters we use symbolic regression based on the genetic programming tool
DataModeler.
  Our studies are carried out on publicly available weather and energy data for
a wind farm in Australia. We reveal the correlation of the different variables
for the energy output. The model obtained for energy prediction gives a very
reliable prediction of the energy output for newly given weather data."
"We consider the problem of reconstructing vehicle trajectories from sparse
sequences of GPS points  for which the sampling interval is between 10 seconds
and 2 minutes. We introduce a new class of algorithms  called altogether path
inference filter (PIF)  that maps GPS data in real time  for a variety of
trade-offs and scenarios  and with a high throughput. Numerous prior approaches
in map-matching can be shown to be special cases of the path inference filter
presented in this article. We present an efficient procedure for automatically
training the filter on new data  with or without ground truth observations. The
framework is evaluated on a large San Francisco taxi dataset and is shown to
improve upon the current state of the art. This filter also provides insights
about driving patterns of drivers. The path inference filter has been deployed
at an industrial scale inside the Mobile Millennium traffic information system 
and is used to map fleets of data in San Francisco  Sacramento  Stockholm and
Porto."
"Software agents can be used to automate many of the tedious  time-consuming
information processing tasks that humans currently have to complete manually.
However  to do so  agent plans must be capable of representing the myriad of
actions and control flows required to perform those tasks. In addition  since
these tasks can require integrating multiple sources of remote information ?
typically  a slow  I/O-bound process ? it is desirable to make execution as
efficient as possible. To address both of these needs  we present a flexible
software agent plan language and a highly parallel execution system that enable
the efficient execution of expressive agent plans. The plan language allows
complex tasks to be more easily expressed by providing a variety of operators
for flexibly processing the data as well as supporting subplans (for
modularity) and recursion (for indeterminate looping). The executor is based on
a streaming dataflow model of execution to maximize the amount of operator and
data parallelism possible at runtime. We have implemented both the language and
executor in a system called THESEUS. Our results from testing THESEUS show that
streaming dataflow execution can yield significant speedups over both
traditional serial (von Neumann) as well as non-streaming dataflow-style
execution that existing software and robot agent execution systems currently
support. In addition  we show how plans written in the language we present can
represent certain types of subtasks that cannot be accomplished using the
languages supported by network query engines. Finally  we demonstrate that the
increased expressivity of our plan language does not hamper performance;
specifically  we show how data can be integrated from multiple remote sources
just as efficiently using our architecture as is possible with a
state-of-the-art streaming-dataflow network query engine."
"This work focuses on improving state-of-the-art in stochastic local search
(SLS) for solving Boolean satisfiability (SAT) instances arising from
real-world industrial SAT application domains. The recently introduced SLS
method CRSat has been shown to noticeably improve on previously suggested SLS
techniques in solving such real-world instances by combining
justification-based local search with limited Boolean constraint propagation on
the non-clausal formula representation form of Boolean circuits. In this work 
we study possibilities of further improving the performance of CRSat by
exploiting circuit-level structural knowledge for developing new search
heuristics for CRSat. To this end  we introduce and experimentally evaluate a
variety of search heuristics  many of which are motivated by circuit-level
heuristics originally developed in completely different contexts  e.g.  for
electronic design automation applications. To the best of our knowledge  most
of the heuristics are novel in the context of SLS for SAT and  more generally 
SLS for constraint satisfaction problems."
"This paper studies the problem of learning diagnostic policies from training
examples. A diagnostic policy is a complete description of the decision-making
actions of a diagnostician (i.e.  tests followed by a diagnostic decision) for
all possible combinations of test results. An optimal diagnostic policy is one
that minimizes the expected total cost  which is the sum of measurement costs
and misdiagnosis costs. In most diagnostic settings  there is a tradeoff
between these two kinds of costs. This paper formalizes diagnostic decision
making as a Markov Decision Process (MDP). The paper introduces a new family of
systematic search algorithms based on the AO* algorithm to solve this MDP. To
make AO* efficient  the paper describes an admissible heuristic that enables
AO* to prune large parts of the search space. The paper also introduces several
greedy algorithms including some improvements over previously-published
methods. The paper then addresses the question of learning diagnostic policies
from examples. When the probabilities of diseases and test results are computed
from training data  there is a great danger of overfitting. To reduce
overfitting  regularizers are integrated into the search algorithms. Finally 
the paper compares the proposed methods on five benchmark diagnostic data sets.
The studies show that in most cases the systematic search methods produce
better diagnostic policies than the greedy methods. In addition  the studies
show that for training sets of realistic size  the systematic search algorithms
are practical on todays desktop computers."
"Variable elimination is a general technique for constraint processing. It is
often discarded because of its high space complexity. However  it can be
extremely useful when combined with other techniques. In this paper we study
the applicability of variable elimination to the challenging problem of finding
still-lifes. We illustrate several alternatives. variable elimination as a
stand-alone algorithm  interleaved with search  and as a source of good quality
lower bounds. We show that these techniques are the best known option both
theoretically and empirically. In our experiments we have been able to solve
the n=20 instance  which is far beyond reach with alternative approaches."
"This is the second of three planned papers describing ZAP  a satisfiability
engine that substantially generalizes existing tools while retaining the
performance characteristics of modern high performance solvers. The fundamental
idea underlying ZAP is that many problems passed to such engines contain rich
internal structure that is obscured by the Boolean representation used; our
goal is to define a representation in which this structure is apparent and can
easily be exploited to improve computational performance. This paper presents
the theoretical basis for the ideas underlying ZAP  arguing that existing ideas
in this area exploit a single  recurring structure in that multiple database
axioms can be obtained by operating on a single axiom using a subgroup of the
group of permutations on the literals in the problem. We argue that the group
structure precisely captures the general structure at which earlier approaches
hinted  and give numerous examples of its use. We go on to extend the
Davis-Putnam-Logemann-Loveland inference procedure to this broader setting  and
show that earlier computational improvements are either subsumed or left intact
by the new method. The third paper in this series discusses ZAPs implementation
and presents experimental performance results."
"Stochastic processes that involve the creation of objects and relations over
time are widespread  but relatively poorly studied. For example  accurate fault
diagnosis in factory assembly processes requires inferring the probabilities of
erroneous assembly operations  but doing this efficiently and accurately is
difficult. Modeled as dynamic Bayesian networks  these processes have discrete
variables with very large domains and extremely high dimensionality. In this
paper  we introduce relational dynamic Bayesian networks (RDBNs)  which are an
extension of dynamic Bayesian networks (DBNs) to first-order logic. RDBNs are a
generalization of dynamic probabilistic relational models (DPRMs)  which we had
proposed in our previous work to model dynamic uncertain domains. We first
extend the Rao-Blackwellised particle filtering described in our earlier work
to RDBNs. Next  we lift the assumptions associated with Rao-Blackwellization in
RDBNs and propose two new forms of particle filtering. The first one uses
abstraction hierarchies over the predicates to smooth the particle filters
estimates. The second employs kernel density estimation with a kernel function
specifically designed for relational domains. Experiments show these two
methods greatly outperform standard particle filtering on the task of assembly
plan execution monitoring."
"We present a uniform non-monotonic solution to the problems of reasoning
about action on the basis of an argumentation-theoretic approach. Our theory is
provably correct relative to a sensible minimisation policy introduced on top
of a temporal propositional logic. Sophisticated problem domains can be
formalised in our framework. As much attention of researchers in the field has
been paid to the traditional and basic problems in reasoning about actions such
as the frame  the qualification and the ramification problems  approaches to
these problems within our formalisation lie at heart of the expositions
presented in this paper."
"In this paper we present a new approach to modeling finite set domain
constraint problems using Reduced Ordered Binary Decision Diagrams (ROBDDs). We
show that it is possible to construct an efficient set domain propagator which
compactly represents many set domains and set constraints using ROBDDs. We
demonstrate that the ROBDD-based approach provides unprecedented flexibility in
modeling constraint satisfaction problems  leading to performance improvements.
We also show that the ROBDD-based modeling approach can be extended to the
modeling of integer and multiset constraint problems in a straightforward
manner. Since domain propagation is not always practical  we also show how to
incorporate less strict consistency notions into the ROBDD framework  such as
set bounds  cardinality bounds and lexicographic bounds consistency. Finally 
we present experimental results that demonstrate the ROBDD-based solver
performs better than various more conventional constraint solvers on several
standard set constraint problems."
"We present a novel approach to the automatic acquisition of taxonomies or
concept hierarchies from a text corpus. The approach is based on Formal Concept
Analysis (FCA)  a method mainly used for the analysis of data  i.e. for
investigating and processing explicitly given information. We follow Harris
distributional hypothesis and model the context of a certain term as a vector
representing syntactic dependencies which are automatically acquired from the
text corpus with a linguistic parser. On the basis of this context information 
FCA produces a lattice that we convert into a special kind of partial order
constituting a concept hierarchy. The approach is evaluated by comparing the
resulting concept hierarchies with hand-crafted taxonomies for two domains.
tourism and finance. We also directly compare our approach with hierarchical
agglomerative clustering as well as with Bi-Section-KMeans as an instance of a
divisive clustering algorithm. Furthermore  we investigate the impact of using
different measures weighting the contribution of each attribute as well as of
applying a particular smoothing technique to cope with data sparseness."
"This is the third of three papers describing ZAP  a satisfiability engine
that substantially generalizes existing tools while retaining the performance
characteristics of modern high-performance solvers. The fundamental idea
underlying ZAP is that many problems passed to such engines contain rich
internal structure that is obscured by the Boolean representation used; our
goal has been to define a representation in which this structure is apparent
and can be exploited to improve computational performance. The first paper
surveyed existing work that (knowingly or not) exploited problem structure to
improve the performance of satisfiability engines  and the second paper showed
that this structure could be understood in terms of groups of permutations
acting on individual clauses in any particular Boolean theory. We conclude the
series by discussing the techniques needed to implement our ideas  and by
reporting on their performance on a variety of problem instances."
"When dealing with incomplete data in statistical learning  or incomplete
observations in probabilistic inference  one needs to distinguish the fact that
a certain event is observed from the fact that the observed event has happened.
Since the modeling and computational complexities entailed by maintaining this
proper distinction are often prohibitive  one asks for conditions under which
it can be safely ignored. Such conditions are given by the missing at random
(mar) and coarsened at random (car) assumptions. In this paper we provide an
in-depth analysis of several questions relating to mar/car assumptions. Main
purpose of our study is to provide criteria by which one may evaluate whether a
car assumption is reasonable for a particular data collecting or observational
process. This question is complicated by the fact that several distinct
versions of mar/car assumptions exist. We therefore first provide an overview
over these different versions  in which we highlight the distinction between
distributional and coarsening variable induced versions. We show that
distributional versions are less restrictive and sufficient for most
applications. We then address from two different perspectives the question of
when the mar/car assumption is warranted. First we provide a static analysis
that characterizes the admissibility of the car assumption in terms of the
support structure of the joint probability distribution of complete data and
incomplete observations. Here we obtain an equivalence characterization that
improves and extends a recent result by Grunwald and Halpern. We then turn to a
procedural analysis that characterizes the admissibility of the car assumption
in terms of procedural models for the actual data (or observation) generating
process. The main result of this analysis is that the stronger coarsened
completely at random (ccar) condition is arguably the most reasonable
assumption  as it alone corresponds to data coarsening procedures that satisfy
a natural robustness property."
"Partially observable Markov decision processes (POMDPs) form an attractive
and principled framework for agent planning under uncertainty. Point-based
approximate techniques for POMDPs compute a policy based on a finite set of
points collected in advance from the agents belief space. We present a
randomized point-based value iteration algorithm called Perseus. The algorithm
performs approximate value backup stages  ensuring that in each backup stage
the value of each point in the belief set is improved; the key observation is
that a single backup may improve the value of many belief points. Contrary to
other point-based methods  Perseus backs up only a (randomly selected) subset
of points in the belief set  sufficient for improving the value of each belief
point in the set. We show how the same idea can be extended to dealing with
continuous action spaces. Experimental results show the potential of Perseus in
large scale POMDP problems."
"Logical hidden Markov models (LOHMMs) upgrade traditional hidden Markov
models to deal with sequences of structured symbols in the form of logical
atoms  rather than flat characters.
  This note formally introduces LOHMMs and presents solutions to the three
central inference problems for LOHMMs. evaluation  most likely hidden state
sequence and parameter estimation. The resulting representation and algorithms
are experimentally evaluated on problems from the domain of bioinformatics."
"We describe the version of the GPT planner used in the probabilistic track of
the 4th International Planning Competition (IPC-4). This version  called mGPT 
solves Markov Decision Processes specified in the PPDDL language by extracting
and using different classes of lower bounds along with various heuristic-search
algorithms. The lower bounds are extracted from deterministic relaxations where
the alternative probabilistic effects of an action are mapped into different 
independent  deterministic actions. The heuristic-search algorithms use these
lower bounds for focusing the updates and delivering a consistent value
function over all states reachable from the initial state and the greedy
policy."
"Despite recent progress in AI planning  many benchmarks remain challenging
for current planners. In many domains  the performance of a planner can greatly
be improved by discovering and exploiting information about the domain
structure that is not explicitly encoded in the initial PDDL formulation. In
this paper we present and compare two automated methods that learn relevant
information from previous experience in a domain and use it to solve new
problem instances. Our methods share a common four-step strategy. First  a
domain is analyzed and structural information is extracted  then
macro-operators are generated based on the previously discovered structure. A
filtering and ranking procedure selects the most useful macro-operators.
Finally  the selected macros are used to speed up future searches. We have
successfully used such an approach in the fourth international planning
competition IPC-4. Our system  Macro-FF  extends Hoffmanns state-of-the-art
planner FF 2.3 with support for two kinds of macro-operators  and with
engineering enhancements. We demonstrate the effectiveness of our ideas on
benchmarks from international planning competitions. Our results indicate a
large reduction in search effort in those complex domains where structural
information can be inferred."
"The Optiplan planning system is the first integer programming-based planner
that successfully participated in the international planning competition. This
engineering note describes the architecture of Optiplan and provides the
integer programming formulation that enabled it to perform reasonably well in
the competition. We also touch upon some recent developments that make integer
programming encodings significantly more competitive."
"We study an approach to policy selection for large relational Markov Decision
Processes (MDPs). We consider a variant of approximate policy iteration (API)
that replaces the usual value-function learning step with a learning step in
policy space. This is advantageous in domains where good policies are easier to
represent and learn than the corresponding value functions  which is often the
case for the relational MDPs we are interested in. In order to apply API to
such problems  we introduce a relational policy language and corresponding
learner. In addition  we introduce a new bootstrapping routine for goal-based
planning domains  based on random walks. Such bootstrapping is necessary for
many large relational MDPs  where reward is extremely sparse  as API is
ineffective in such domains when initialized with an uninformed policy. Our
experiments show that the resulting system is able to find good policies for a
number of classical planning domains and their stochastic variants by solving
them as extremely large relational MDPs. The experiments also point to some
limitations of our approach  suggesting future work."
"Tabu search is one of the most effective heuristics for locating high-quality
solutions to a diverse array of NP-hard combinatorial optimization problems.
Despite the widespread success of tabu search  researchers have a poor
understanding of many key theoretical aspects of this algorithm  including
models of the high-level run-time dynamics and identification of those search
space features that influence problem difficulty. We consider these questions
in the context of the job-shop scheduling problem (JSP)  a domain where tabu
search algorithms have been shown to be remarkably effective. Previously  we
demonstrated that the mean distance between random local optima and the nearest
optimal solution is highly correlated with problem difficulty for a well-known
tabu search algorithm for the JSP introduced by Taillard. In this paper  we
discuss various shortcomings of this measure and develop a new model of problem
difficulty that corrects these deficiencies. We show that Taillards algorithm
can be modeled with high fidelity as a simple variant of a straightforward
random walk. The random walk model accounts for nearly all of the variability
in the cost required to locate both optimal and sub-optimal solutions to random
JSPs  and provides an explanation for differences in the difficulty of random
versus structured JSPs. Finally  we discuss and empirically substantiate two
novel predictions regarding tabu search algorithm behavior. First  the method
for constructing the initial solution is highly unlikely to impact the
performance of tabu search. Second  tabu tenure should be selected to be as
small as possible while simultaneously avoiding search stagnation; values
larger than necessary lead to significant degradations in performance."
"Code optimization and high level synthesis can be posed as constraint
satisfaction and optimization problems  such as graph coloring used in register
allocation. Graph coloring is also used to model more traditional CSPs relevant
to AI  such as planning  time-tabling and scheduling. Provably optimal
solutions may be desirable for commercial and defense applications.
Additionally  for applications such as register allocation and code
optimization  naturally-occurring instances of graph coloring are often small
and can be solved optimally. A recent wave of improvements in algorithms for
Boolean satisfiability (SAT) and 0-1 Integer Linear Programming (ILP) suggests
generic problem-reduction methods  rather than problem-specific heuristics 
because (1) heuristics may be upset by new constraints  (2) heuristics tend to
ignore structure  and (3) many relevant problems are provably inapproximable.
  Problem reductions often lead to highly symmetric SAT instances  and
symmetries are known to slow down SAT solvers. In this work  we compare several
avenues for symmetry breaking  in particular when certain kinds of symmetry are
present in all generated instances. Our focus on reducing CSPs to SAT allows us
to leverage recent dramatic improvement in SAT solvers and automatically
benefit from future progress. We can use a variety of black-box SAT solvers
without modifying their source code because our symmetry-breaking techniques
are static  i.e.  we detect symmetries and add symmetry breaking predicates
(SBPs) during pre-processing.
  An important result of our work is that among the types of
instance-independent SBPs we studied and their combinations  the simplest and
least complete constructions are the most effective. Our experiments also
clearly indicate that instance-independent symmetries should mostly be
processed together with instance-specific symmetries rather than at the
specification level  contrary to what has been suggested in the literature."
"A decision process in which rewards depend on history rather than merely on
the current state is called a decision process with non-Markovian rewards
(NMRDP). In decision-theoretic planning  where many desirable behaviours are
more naturally expressed as properties of execution sequences rather than as
properties of states  NMRDPs form a more natural model than the commonly
adopted fully Markovian decision process (MDP) model. While the more tractable
solution methods developed for MDPs do not directly apply in the presence of
non-Markovian rewards  a number of solution methods for NMRDPs have been
proposed in the literature. These all exploit a compact specification of the
non-Markovian reward function in temporal logic  to automatically translate the
NMRDP into an equivalent MDP which is solved using efficient MDP solution
methods. This paper presents NMRDPP (Non-Markovian Reward Decision Process
Planner)  a software platform for the development and experimentation of
methods for decision-theoretic planning with non-Markovian rewards. The current
version of NMRDPP implements  under a single interface  a family of methods
based on existing as well as new approaches which we describe in detail. These
include dynamic programming  heuristic search  and structured methods. Using
NMRDPP  we compare the methods and identify certain problem features that
affect their performance. NMRDPPs treatment of non-Markovian rewards is
inspired by the treatment of domain-specific search control knowledge in the
TLPlan planner  which it incorporates as a special case. In the First
International Probabilistic Planning Competition  NMRDPP was able to compete
and perform well in both the domain-independent and hand-coded tracks  using
search control knowledge in the latter."
"Boolean optimization finds a wide range of application domains  that
motivated a number of different organizations of Boolean optimizers since the
mid 90s. Some of the most successful approaches are based on iterative calls to
an NP oracle  using either linear search  binary search or the identification
of unsatisfiable sub-formulas. The increasing use of Boolean optimizers in
practical settings raises the question of confidence in computed results. For
example  the issue of confidence is paramount in safety critical settings. One
way of increasing the confidence of the results computed by Boolean optimizers
is to develop techniques for validating the results. Recent work studied the
validation of Boolean optimizers based on branch-and-bound search. This paper
complements existing work  and develops methods for validating Boolean
optimizers that are based on iterative calls to an NP oracle. This entails
implementing solutions for validating both satisfiable and unsatisfiable
answers from the NP oracle. The work described in this paper can be applied to
a wide range of Boolean optimizers  that find application in Pseudo-Boolean
Optimization and in Maximum Satisfiability. Preliminary experimental results
indicate that the impact of the proposed method in overall performance is
negligible."
"The article presents a study on the biobjective inventory routing problem.
Contrary to most previous research  the problem is treated as a true
multi-objective optimization problem  with the goal of identifying
Pareto-optimal solutions. Due to the hardness of the problem at hand  a
reference point based optimization approach is presented and implemented into
an optimization and decision support system  which allows for the computation
of a true subset of the optimal outcomes. Experimental investigation involving
local search metaheuristics are conducted on benchmark data  and numerical
results are reported and analyzed."
"Variable neighborhood search (VNS) is a metaheuristic for solving
optimization problems based on a simple principle. systematic changes of
neighborhoods within the search  both in the descent to local minima and in the
escape from the valleys which contain them. Designing these neighborhoods and
applying them in a meaningful fashion is not an easy task. Moreover  an
appropriate order in which they are applied must be determined. In this paper
we attempt to investigate this issue. Assume that we are given an optimization
problem that is intended to be solved by applying the VNS scheme  how many and
which types of neighborhoods should be investigated and what could be
appropriate selection criteria to apply these neighborhoods. More specifically 
does it pay to ""look ahead"" (see  e.g.  in the context of VNS and GRASP) when
attempting to switch from one neighborhood to another?"
"In this paper we demonstrate that two common problems in Machine
Learning---imbalanced and overlapping data distributions---do not have
independent effects on the performance of SVM classifiers. This result is
notable since it shows that a model of either of these factors must account for
the presence of the other. Our study of the relationship between these problems
has lead to the discovery of a previously unreported form of ""covert""
overfitting which is resilient to commonly used empirical regularization
techniques. We demonstrate the existance of this covert phenomenon through
several methods based around the parametric regularization of trained SVMs. Our
findings in this area suggest a possible approach to quantifying overlap in
real world data sets."
"In the theory of belief functions  many measures of uncertainty have been
introduced. However  it is not always easy to understand what these measures
really try to represent. In this paper  we re-interpret some measures of
uncertainty in the theory of belief functions. We present some interests and
drawbacks of the existing measures. On these observations  we introduce a
measure of contradiction. Therefore  we present some degrees of non-specificity
and Bayesianity of a mass. We propose a degree of specificity based on the
distance between a mass and its most specific associated mass. We also show how
to use the degree of specificity to measure the specificity of a fusion rule.
Illustrations on simple examples are given."
"We discuss an attentional model for simultaneous object tracking and
recognition that is driven by gaze data. Motivated by theories of perception 
the model consists of two interacting pathways. identity and control  intended
to mirror the what and where pathways in neuroscience models. The identity
pathway models object appearance and performs classification using deep
(factored)-Restricted Boltzmann Machines. At each point in time the
observations consist of foveated images  with decaying resolution toward the
periphery of the gaze. The control pathway models the location  orientation 
scale and speed of the attended object. The posterior distribution of these
states is estimated with particle filtering. Deeper in the control pathway  we
encounter an attentional mechanism that learns to select gazes so as to
minimize tracking uncertainty. Unlike in our previous work  we introduce gaze
selection strategies which operate in the presence of partial information and
on a continuous action space. We show that a straightforward extension of the
existing approach to the partial information setting results in poor
performance  and we propose an alternative method based on modeling the reward
surface as a Gaussian Process. This approach gives good performance in the
presence of partial information and allows us to expand the action space from a
small  discrete set of fixation points to a continuous domain."
"Several rules for social choice are examined from a unifying point of view
that looks at them as procedures for revising a system of degrees of belief in
accordance with certain specified logical constraints. Belief is here a social
attribute  its degrees being measured by the fraction of people who share a
given opinion. Different known rules and some new ones are obtained depending
on which particular constraints are assumed. These constraints allow to model
different notions of choiceness. In particular  we give a new method to deal
with approval-disapproval-preferential voting."
"We investigate training and using Gaussian kernel SVMs by approximating the
kernel with an explicit finite- dimensional polynomial feature representation
based on the Taylor expansion of the exponential. Although not as efficient as
the recently-proposed random Fourier features [Rahimi and Recht  2007] in terms
of the number of features  we show how this polynomial representation can
provide a better approximation in terms of the computational cost involved.
This makes our ""Taylor features"" especially attractive for use on very large
data sets  in conjunction with online or stochastic training."
"Today  available methods that assess AI systems are focused on using
empirical techniques to measure the performance of algorithms in some specific
tasks (e.g.  playing chess  solving mazes or land a helicopter). However  these
methods are not appropriate if we want to evaluate the general intelligence of
AI and  even less  if we compare it with human intelligence. The ANYNT project
has designed a new method of evaluation that tries to assess AI systems using
well known computational notions and problems which are as general as possible.
This new method serves to assess general intelligence (which allows us to learn
how to solve any new kind of problem we face) and not only to evaluate
performance on a set of specific tasks. This method not only focuses on
measuring the intelligence of algorithms  but also to assess any intelligent
system (human beings  animals  AI  aliens? ...)  and letting us to place their
results on the same scale and  therefore  to be able to compare them. This new
approach will allow us (in the future) to evaluate and compare any kind of
intelligent system known or even to build/find  be it artificial or biological.
This master thesis aims at ensuring that this new method provides consistent
results when evaluating AI algorithms  this is done through the design and
implementation of prototypes of universal intelligence tests and their
application to different intelligent systems (AI algorithms and humans beings).
From the study we analyze whether the results obtained by two different
intelligent systems are properly located on the same scale and we propose
changes and refinements to these prototypes in order to  in the future  being
able to achieve a truly universal intelligence test."
"We provide an overview of the organization and results of the deterministic
part of the 4th International Planning Competition  i.e.  of the part concerned
with evaluating systems doing deterministic planning. IPC-4 attracted even more
competing systems than its already large predecessors  and the competition
event was revised in several important respects. After giving an introduction
to the IPC  we briefly explain the main differences between the deterministic
part of IPC-4 and its predecessors. We then introduce formally the language
used  called PDDL2.2 that extends PDDL2.1 by derived predicates and timed
initial literals. We list the competing systems and overview the results of the
competition. The entire set of data is far too large to be presented in full.
We provide a detailed summary; the complete data is available in an online
appendix. We explain how we awarded the competition prizes."
"PDDL2.1 was designed to push the envelope of what planning algorithms can do 
and it has succeeded. It adds two important features. durative actions which
take time (and may have continuous effects); and objective functions for
measuring the quality of plans. The concept of durative actions is flawed; and
the treatment of their semantics reveals too strong an attachment to the way
many contemporary planners work. Future PDDL innovators should focus on
producing a clean semantics for additions to the language  and let planner
implementers worry about coupling their algorithms to problems expressed in the
latest version of the language."
"The addition of durative actions to PDDL2.1 sparked some controversy. Fox and
Long argued that actions should be considered as instantaneous  but can start
and stop processes. Ultimately  a limited notion of durative actions was
incorporated into the language. I argue that this notion is still impoverished 
and that the underlying philosophical position of regarding durative actions as
being a shorthand for a start action  process  and stop action ignores the
realities of modelling and execution for complex systems."
"We present a partial-order  conformant  probabilistic planner  Probapop which
competed in the blind track of the Probabilistic Planning Competition in IPC-4.
We explain how we adapt distance based heuristics for use with probabilistic
domains. Probapop also incorporates heuristics based on probability of success.
We explain the successes and difficulties encountered during the design and
implementation of Probapop."
"Between 1998 and 2004  the planning community has seen vast progress in terms
of the sizes of benchmark examples that domain-independent planners can tackle
successfully. The key technique behind this progress is the use of heuristic
functions based on relaxing the planning task at hand  where the relaxation is
to assume that all delete lists are empty. The unprecedented success of such
methods  in many commonly used benchmark examples  calls for an understanding
of what classes of domains these methods are well suited for. In the
investigation at hand  we derive a formal background to such an understanding.
We perform a case study covering a range of 30 commonly used STRIPS and ADL
benchmark domains  including all examples used in the first four international
planning competitions. We *prove* connections between domain structure and
local search topology -- heuristic cost surface properties -- under an
idealized version of the heuristic functions used in modern planners. The
idealized heuristic function is called h^+  and differs from the practically
used functions in that it returns the length of an *optimal* relaxed plan 
which is NP-hard to compute. We identify several key characteristics of the
topology under h^+  concerning the existence/non-existence of unrecognized dead
ends  as well as the existence/non-existence of constant upper bounds on the
difficulty of escaping local minima and benches. These distinctions divide the
(set of all) planning domains into a taxonomy of classes of varying h^+
topology. As it turns out  many of the 30 investigated domains lie in classes
with a relatively easy topology. Most particularly  12 of the domains lie in
classes where FFs search algorithm  provided with h^+  is a polynomial solving
mechanism. We also present results relating h^+ to its approximation as
implemented in FF. The behavior regarding dead ends is provably the same. We
summarize the results of an empirical investigation showing that  in many
domains  the topological qualities of h^+ are largely inherited by the
approximation. The overall investigation gives a rare example of a successful
analysis of the connections between typical-case problem structure  and search
performance. The theoretical investigation also gives hints on how the
topological phenomena might be automatically recognizable by domain analysis
techniques. We outline some preliminary steps we made into that direction."
"A non-binary Constraint Satisfaction Problem (CSP) can be solved directly
using extended versions of binary techniques. Alternatively  the non-binary
problem can be translated into an equivalent binary one. In this case  it is
generally accepted that the translated problem can be solved by applying
well-established techniques for binary CSPs. In this paper we evaluate the
applicability of the latter approach. We demonstrate that the use of standard
techniques for binary CSPs in the encodings of non-binary problems is
problematic and results in models that are very rarely competitive with the
non-binary representation. To overcome this  we propose specialized arc
consistency and search algorithms for binary encodings  and we evaluate them
theoretically and empirically. We consider three binary representations; the
hidden variable encoding  the dual encoding  and the double encoding.
Theoretical and empirical results show that  for certain classes of non-binary
constraints  binary encodings are a competitive option  and in many cases  a
better one than the non-binary representation."
"In a peer-to-peer inference system  each peer can reason locally but can also
solicit some of its acquaintances  which are peers sharing part of its
vocabulary. In this paper  we consider peer-to-peer inference systems in which
the local theory of each peer is a set of propositional clauses defined upon a
local vocabulary. An important characteristic of peer-to-peer inference systems
is that the global theory (the union of all peer theories) is not known (as
opposed to partition-based reasoning systems). The main contribution of this
paper is to provide the first consequence finding algorithm in a peer-to-peer
setting. DeCA. It is anytime and computes consequences gradually from the
solicited peer to peers that are more and more distant. We exhibit a sufficient
condition on the acquaintance graph of the peer-to-peer inference system for
guaranteeing the completeness of this algorithm. Another important contribution
is to apply this general distributed reasoning setting to the setting of the
Semantic Web through the Somewhere semantic peer-to-peer data management
system. The last contribution of this paper is to provide an experimental
analysis of the scalability of the peer-to-peer infrastructure that we propose 
on large networks of 1000 peers."
"In this paper  we introduce DLS-MC  a new stochastic local search algorithm
for the maximum clique problem. DLS-MC alternates between phases of iterative
improvement  during which suitable vertices are added to the current clique 
and plateau search  during which vertices of the current clique are swapped
with vertices not contained in the current clique. The selection of vertices is
solely based on vertex penalties that are dynamically adjusted during the
search  and a perturbation mechanism is used to overcome search stagnation. The
behaviour of DLS-MC is controlled by a single parameter  penalty delay  which
controls the frequency at which vertex penalties are reduced. We show
empirically that DLS-MC achieves substantial performance improvements over
state-of-the-art algorithms for the maximum clique problem over a large range
of the commonly used DIMACS benchmark instances."
"Open distributed multi-agent systems are gaining interest in the academic
community and in industry. In such open settings  agents are often coordinated
using standardized agent conversation protocols. The representation of such
protocols (for analysis  validation  monitoring  etc) is an important aspect of
multi-agent applications. Recently  Petri nets have been shown to be an
interesting approach to such representation  and radically different approaches
using Petri nets have been proposed. However  their relative strengths and
weaknesses have not been examined. Moreover  their scalability and suitability
for different tasks have not been addressed. This paper addresses both these
challenges. First  we analyze existing Petri net representations in terms of
their scalability and appropriateness for overhearing  an important task in
monitoring open multi-agent systems. Then  building on the insights gained  we
introduce a novel representation using Colored Petri nets that explicitly
represent legal joint conversation states and messages. This representation
approach offers significant improvements in scalability and is particularly
suitable for overhearing. Furthermore  we show that this new representation
offers a comprehensive coverage of all conversation features of FIPA
conversation standards. We also present a procedure for transforming AUML
conversation protocol diagrams (a standard human-readable representation)  to
our Colored Petri net representation."
"The hm admissible heuristics for (sequential and temporal) regression
planning are defined by a parameterized relaxation of the optimal cost function
in the regression search space  where the parameter m offers a trade-off
between the accuracy and computational cost of theheuristic. Existing methods
for computing the hm heuristic require time exponential in m  limiting them to
small values (m andlt= 2). The hm heuristic can also be viewed as the optimal
cost function in a relaxation of the search space. this paper presents relaxed
search  a method for computing this function partially by searching in the
relaxed space. The relaxed search method  because it computes hm only
partially  is computationally cheaper and therefore usable for higher values of
m. The (complete) hm heuristic is combined with partial hm heuristics  for m =
3 ...  computed by relaxed search  resulting in a more accurate heuristic.
  This use of the relaxed search method to improve on the hm heuristic is
evaluated by comparing two optimal temporal planners. TP4  which does not use
it  and HSP*a  which uses it but is otherwise identical to TP4. The comparison
is made on the domains used in the 2004 International Planning Competition  in
which both planners participated. Relaxed search is found to be cost effective
in some of these domains  but not all. Analysis reveals a characterization of
the domains in which relaxed search can be expected to be cost effective  in
terms of two measures on the original and relaxed search spaces. In the domains
where relaxed search is cost effective  expanding small states is
computationally cheaper than expanding large states and small states tend to
have small successor states."
"Recently  a variety of constraint programming and Boolean satisfiability
approaches to scheduling problems have been introduced. They have in common the
use of relatively simple propagation mechanisms and an adaptive way to focus on
the most constrained part of the problem. In some cases  these methods compare
favorably to more classical constraint programming methods relying on
propagation algorithms for global unary or cumulative resource constraints and
dedicated search heuristics. In particular  we described an approach that
combines restarting  with a generic adaptive heuristic and solution guided
branching on a simple model based on a decomposition of disjunctive
constraints. In this paper  we introduce an adaptation of this technique for an
important subclass of job shop scheduling problems (JSPs)  where the objective
function involves minimization of earliness/tardiness costs. We further show
that our technique can be improved by adding domain specific information for
one variant of the JSP (involving time lag constraints). In particular we
introduce a dedicated greedy heuristic  and an improved model for the case
where the maximal time lag is 0 (also referred to as no-wait JSPs)."
"The Universal Intelligence Measure is a recently proposed formal definition
of intelligence. It is mathematically specified  extremely general  and
captures the essence of many informal definitions of intelligence. It is based
on Hutter's Universal Artificial Intelligence theory  an extension of Ray
Solomonoff's pioneering work on universal induction. Since the Universal
Intelligence Measure is only asymptotically computable  building a practical
intelligence test from it is not straightforward. This paper studies the
practical issues involved in developing a real-world UIM-based performance
metric. Based on our investigation  we develop a prototype implementation which
we use to evaluate a number of different artificial agents."
"Multiple sequence alignment (MSA) is a ubiquitous problem in computational
biology. Although it is NP-hard to find an optimal solution for an arbitrary
number of sequences  due to the importance of this problem researchers are
trying to push the limits of exact algorithms further. Since MSA can be cast as
a classical path finding problem  it is attracting a growing number of AI
researchers interested in heuristic search algorithms as a challenge with
actual practical relevance. In this paper  we first review two previous 
complementary lines of research. Based on Hirschbergs algorithm  Dynamic
Programming needs O(kN^(k-1)) space to store both the search frontier and the
nodes needed to reconstruct the solution path  for k sequences of length N.
Best first search  on the other hand  has the advantage of bounding the search
space that has to be explored using a heuristic. However  it is necessary to
maintain all explored nodes up to the final solution in order to prevent the
search from re-expanding them at higher cost. Earlier approaches to reduce the
Closed list are either incompatible with pruning methods for the Open list  or
must retain at least the boundary of the Closed list. In this article  we
present an algorithm that attempts at combining the respective advantages; like
A* it uses a heuristic for pruning the search space  but reduces both the
maximum Open and Closed size to O(kN^(k-1))  as in Dynamic Programming. The
underlying idea is to conduct a series of searches with successively increasing
upper bounds  but using the DP ordering as the key for the Open priority queue.
With a suitable choice of thresholds  in practice  a running time below four
times that of A* can be expected. In our experiments we show that our algorithm
outperforms one of the currently most successful algorithms for optimal
multiple sequence alignments  Partial Expansion A*  both in time and memory.
Moreover  we apply a refined heuristic based on optimal alignments not only of
pairs of sequences  but of larger subsets. This idea is not new; however  to
make it practically relevant we show that it is equally important to bound the
heuristic computation appropriately  or the overhead can obliterate any
possible gain. Furthermore  we discuss a number of improvements in time and
space efficiency with regard to practical implementations. Our algorithm  used
in conjunction with higher-dimensional heuristics  is able to calculate for the
first time the optimal alignment for almost all of the problems in Reference 1
of the benchmark database BAliBASE."
"This article develops Probabilistic Hybrid Action Models (PHAMs)  a realistic
causal model for predicting the behavior generated by modern percept-driven
robot plans. PHAMs represent aspects of robot behavior that cannot be
represented by most action models used in AI planning. the temporal structure
of continuous control processes  their non-deterministic effects  several modes
of their interferences  and the achievement of triggering conditions in
closed-loop robot plans.
  The main contributions of this article are. (1) PHAMs  a model of concurrent
percept-driven behavior  its formalization  and proofs that the model generates
probably  qualitatively accurate predictions; and (2) a resource-efficient
inference method for PHAMs based on sampling projections from probabilistic
action models and state descriptions. We show how PHAMs can be applied to
planning the course of action of an autonomous robot office courier based on
analytical and experimental results."
"We present a novel framework for integrating prior knowledge into
discriminative classifiers. Our framework allows discriminative classifiers
such as Support Vector Machines (SVMs) to utilize prior knowledge specified in
the generative setting. The dual objective of fitting the data and respecting
prior knowledge is formulated as a bilevel program  which is solved
(approximately) via iterative application of second-order cone programming. To
test our approach  we consider the problem of using WordNet (a semantic
database of English language) to improve low-sample classification accuracy of
newsgroup categorization. WordNet is viewed as an approximate  but readily
available source of background knowledge  and our framework is capable of
utilizing it in a flexible way."
"Fast Downward is a classical planning system based on heuristic search. It
can deal with general deterministic planning problems encoded in the
propositional fragment of PDDL2.2  including advanced features like ADL
conditions and effects and derived predicates (axioms). Like other well-known
planners such as HSP and FF  Fast Downward is a progression planner  searching
the space of world states of a planning task in the forward direction. However 
unlike other PDDL planning systems  Fast Downward does not use the
propositional PDDL representation of a planning task directly. Instead  the
input is first translated into an alternative representation called
multi-valued planning tasks  which makes many of the implicit constraints of a
propositional planning task explicit. Exploiting this alternative
representation  Fast Downward uses hierarchical decompositions of planning
tasks for computing its heuristic function  called the causal graph heuristic 
which is very different from traditional HSP-like heuristics based on ignoring
negative interactions of operators.
  In this article  we give a full account of Fast Downwards approach to solving
multi-valued planning tasks. We extend our earlier discussion of the causal
graph heuristic to tasks involving axioms and conditional effects and present
some novel techniques for search control that are used within Fast Downwards
best-first search algorithm. preferred operators transfer the idea of helpful
actions from local search to global best-first search  deferred evaluation of
heuristic functions mitigates the negative effect of large branching factors on
search performance  and multi-heuristic best-first search combines several
heuristic evaluation functions within a single search algorithm in an
orthogonal way. We also describe efficient data structures for fast state
expansion (successor generators and axiom evaluators) and present a new
non-heuristic search algorithm called focused iterative-broadening search 
which utilizes the information encoded in causal graphs in a novel way.
  Fast Downward has proven remarkably successful. It won the ""classical (i.e. 
propositional  non-optimising) track of the 4th International Planning
Competition at ICAPS 2004  following in the footsteps of planners such as FF
and LPG. Our experiments show that it also performs very well on the benchmarks
of the earlier planning competitions and provide some insights about the
usefulness of the new search enhancements."
"Distributed Constraint Satisfaction (DCSP) has long been considered an
important problem in multi-agent systems research. This is because many
real-world problems can be represented as constraint satisfaction and these
problems often present themselves in a distributed form. In this article  we
present a new complete  distributed algorithm called Asynchronous Partial
Overlay (APO) for solving DCSPs that is based on a cooperative mediation
process. The primary ideas behind this algorithm are that agents  when acting
as a mediator  centralize small  relevant portions of the DCSP  that these
centralized subproblems overlap  and that agents increase the size of their
subproblems along critical paths within the DCSP as the problem solving
unfolds. We present empirical evidence that shows that APO outperforms other
known  complete DCSP techniques."
"As partial justification of their framework for iterated belief revision
Darwiche and Pearl convincingly argued against Boutiliers natural revision and
provided a prototypical revision operator that fits into their scheme. We show
that the Darwiche-Pearl arguments lead naturally to the acceptance of a smaller
class of operators which we refer to as admissible. Admissible revision ensures
that the penultimate input is not ignored completely  thereby eliminating
natural revision  but includes the Darwiche-Pearl operator  Nayaks
lexicographic revision operator  and a newly introduced operator called
restrained revision. We demonstrate that restrained revision is the most
conservative of admissible revision operators  effecting as few changes as
possible  while lexicographic revision is the least conservative  and point out
that restrained revision can also be viewed as a composite operator  consisting
of natural revision preceded by an application of a ""backwards revision""
operator previously studied by Papini. Finally  we propose the establishment of
a principled approach for choosing an appropriate revision operator in
different contexts and discuss future work."
"In recent years  CP-nets have emerged as a useful tool for supporting
preference elicitation  reasoning  and representation. CP-nets capture and
support reasoning with qualitative conditional preference statements 
statements that are relatively natural for users to express. In this paper  we
extend the CP-nets formalism to handle another class of very natural
qualitative statements one often uses in expressing preferences in daily life -
statements of relative importance of attributes. The resulting formalism 
TCP-nets  maintains the spirit of CP-nets  in that it remains focused on using
only simple and natural preference statements  uses the ceteris paribus
semantics  and utilizes a graphical representation of this information to
reason about its consistency and to perform  possibly constrained  optimization
using it. The extra expressiveness it provides allows us to better model
tradeoffs users would like to make  more faithfully representing their
preferences."
"Linear Temporal Logic (LTL) is widely used for defining conditions on the
execution paths of dynamic systems. In the case of dynamic systems that allow
for nondeterministic evolutions  one has to specify  along with an LTL formula
f  which are the paths that are required to satisfy the formula. Two extreme
cases are the universal interpretation A.f  which requires that the formula be
satisfied for all execution paths  and the existential interpretation E.f 
which requires that the formula be satisfied for some execution path.
  When LTL is applied to the definition of goals in planning problems on
nondeterministic domains  these two extreme cases are too restrictive. It is
often impossible to develop plans that achieve the goal in all the
nondeterministic evolutions of a system  and it is too weak to require that the
goal is satisfied by some execution.
  In this paper we explore alternative interpretations of an LTL formula that
are between these extreme cases. We define a new language that permits an
arbitrary combination of the A and E quantifiers  thus allowing  for instance 
to require that each finite execution can be extended to an execution
satisfying an LTL formula (AE.f)  or that there is some finite execution whose
extensions all satisfy an LTL formula (EA.f). We show that only eight of these
combinations of path quantifiers are relevant  corresponding to an alternation
of the quantifiers of length one (A and E)  two (AE and EA)  three (AEA and
EAE)  and infinity ((AE)* and (EA)*). We also present a planning algorithm for
the new language that is based on an automata-theoretic approach  and study its
complexity."
"A delta-model is a satisfying assignment of a Boolean formula for which any
small alteration  such as a single bit flip  can be repaired by flips to some
small number of other bits  yielding a new satisfying assignment. These
satisfying assignments represent robust solutions to optimization problems
(e.g.  scheduling) where it is possible to recover from unforeseen events
(e.g.  a resource becoming unavailable). The concept of delta-models was
introduced by Ginsberg  Parkes and Roy (AAAI 1998)  where it was proved that
finding delta-models for general Boolean formulas is NP-complete. In this
paper  we extend that result by studying the complexity of finding delta-models
for classes of Boolean formulas which are known to have polynomial time
satisfiability solvers. In particular  we examine 2-SAT  Horn-SAT  Affine-SAT 
dual-Horn-SAT  0-valid and 1-valid SAT. We see a wide variation in the
complexity of finding delta-models  e.g.  while 2-SAT and Affine-SAT have
polynomial time tests for delta-models  testing whether a Horn-SAT formula has
one is NP-complete."
"Multimodal conversational interfaces provide a natural means for users to
communicate with computer systems through multiple modalities such as speech
and gesture. To build effective multimodal interfaces  automated interpretation
of user multimodal inputs is important. Inspired by the previous investigation
on cognitive status in multimodal human machine interaction  we have developed
a greedy algorithm for interpreting user referring expressions (i.e. 
multimodal reference resolution). This algorithm incorporates the cognitive
principles of Conversational Implicature and Givenness Hierarchy and applies
constraints from various sources (e.g.  temporal  semantic  and contextual) to
resolve references. Our empirical results have shown the advantage of this
algorithm in efficiently resolving a variety of user references. Because of its
simplicity and generality  this approach has the potential to improve the
robustness of multimodal input interpretation."
"This paper presents a new framework for anytime heuristic search where the
task is to achieve as many goals as possible within the allocated resources. We
show the inadequacy of traditional distance-estimation heuristics for tasks of
this type and present alternative heuristics that are more appropriate for
multiple-goal search. In particular  we introduce the marginal-utility
heuristic  which estimates the cost and the benefit of exploring a subtree
below a search node. We developed two methods for online learning of the
marginal-utility heuristic. One is based on local similarity of the partial
marginal utility of sibling nodes  and the other generalizes marginal-utility
over the state feature space. We apply our adaptive and non-adaptive
multiple-goal search algorithms to several problems  including focused
crawling  and show their superiority over existing methods."
"We present a heuristic search algorithm for solving first-order Markov
Decision Processes (FOMDPs). Our approach combines first-order state
abstraction that avoids evaluating states individually  and heuristic search
that avoids evaluating all states. Firstly  in contrast to existing systems 
which start with propositionalizing the FOMDP and then perform state
abstraction on its propositionalized version we apply state abstraction
directly on the FOMDP avoiding propositionalization. This kind of abstraction
is referred to as first-order state abstraction. Secondly  guided by an
admissible heuristic  the search is restricted to those states that are
reachable from the initial state. We demonstrate the usefulness of the above
techniques for solving FOMDPs with a system  referred to as FluCaP (formerly 
FCPlanner)  that entered the probabilistic track of the 2004 International
Planning Competition (IPC2004) and demonstrated an advantage over other
planners on the problems represented in first-order terms."
"Suppose we want to build a system that answers a natural language question by
representing its semantics as a logical form and computing the answer given a
structured database of facts. The core part of such a system is the semantic
parser that maps questions to logical forms. Semantic parsers are typically
trained from examples of questions annotated with their target logical forms 
but this type of annotation is expensive.
  Our goal is to learn a semantic parser from question-answer pairs instead 
where the logical form is modeled as a latent variable. Motivated by this
challenging learning problem  we develop a new semantic formalism 
dependency-based compositional semantics (DCS)  which has favorable linguistic 
statistical  and computational properties. We define a log-linear distribution
over DCS logical forms and estimate the parameters using a simple procedure
that alternates between beam search and numerical optimization. On two standard
semantic parsing benchmarks  our system outperforms all existing
state-of-the-art systems  despite using no annotated logical forms."
"It was recently proved that a sound and complete qualitative simulator does
not exist  that is  as long as the input-output vocabulary of the
state-of-the-art QSIM algorithm is used  there will always be input models
which cause any simulator with a coverage guarantee to make spurious
predictions in its output. In this paper  we examine whether a meaningfully
expressive restriction of this vocabulary is possible so that one can build a
simulator with both the soundness and completeness properties. We prove several
negative results. All sound qualitative simulators  employing subsets of the
QSIM representation which retain the operating region transition feature  and
support at least the addition and constancy constraints  are shown to be
inherently incomplete. Even when the simulations are restricted to run in a
single operating region  a constraint vocabulary containing just the addition 
constancy  derivative  and multiplication relations makes the construction of
sound and complete qualitative simulators impossible."
"We study properties of programs with monotone and convex constraints. We
extend to these formalisms concepts and results from normal logic programming.
They include the notions of strong and uniform equivalence with their
characterizations  tight programs and Fages Lemma  program completion and loop
formulas. Our results provide an abstract account of properties of some recent
extensions of logic programming with aggregates  especially the formalism of
lparse programs. They imply a method to compute stable models of lparse
programs by means of off-the-shelf solvers of pseudo-boolean constraints  which
is often much faster than the smodels system."
"We characterize the search landscape of random instances of the job shop
scheduling problem (JSP). Specifically  we investigate how the expected values
of (1) backbone size  (2) distance between near-optimal schedules  and (3)
makespan of random schedules vary as a function of the job to machine ratio
(N/M). For the limiting cases N/M approaches 0 and N/M approaches infinity we
provide analytical results  while for intermediate values of N/M we perform
experiments. We prove that as N/M approaches 0  backbone size approaches 100% 
while as N/M approaches infinity the backbone vanishes. In the process we show
that as N/M approaches 0 (resp. N/M approaches infinity)  simple priority rules
almost surely generate an optimal schedule  providing theoretical evidence of
an ""easy-hard-easy"" pattern of typical-case instance difficulty in job shop
scheduling. We also draw connections between our theoretical results and the
""big valley"" picture of JSP landscapes."
"We consider interactive tools that help users search for their most preferred
item in a large collection of options. In particular  we examine
example-critiquing  a technique for enabling users to incrementally construct
preference models by critiquing example options that are presented to them. We
present novel techniques for improving the example-critiquing technology by
adding suggestions to its displayed options. Such suggestions are calculated
based on an analysis of users current preference model and their potential
hidden preferences. We evaluate the performance of our model-based suggestion
techniques with both synthetic and real users. Results show that such
suggestions are highly attractive to users and can stimulate them to express
more preferences to improve the chance of identifying their most preferred item
by up to 78%."
"The Partially Observable Markov Decision Process has long been recognized as
a rich framework for real-world planning and control problems  especially in
robotics. However exact solutions in this framework are typically
computationally intractable for all but the smallest problems. A well-known
technique for speeding up POMDP solving involves performing value backups at
specific belief points  rather than over the entire belief simplex. The
efficiency of this approach  however  depends greatly on the selection of
points. This paper presents a set of novel techniques for selecting informative
belief points which work well in practice. The point selection procedure is
combined with point-based value backups to form an effective anytime POMDP
algorithm called Point-Based Value Iteration (PBVI). The first aim of this
paper is to introduce this algorithm and present a theoretical analysis
justifying the choice of belief selection technique. The second aim of this
paper is to provide a thorough empirical comparison between PBVI and other
state-of-the-art POMDP methods  in particular the Perseus algorithm  in an
effort to highlight their similarities and differences. Evaluation is performed
using both standard POMDP domains and realistic robotic tasks."
"Efficient representations and solutions for large decision problems with
continuous and discrete variables are among the most important challenges faced
by the designers of automated decision support systems. In this paper  we
describe a novel hybrid factored Markov decision process (MDP) model that
allows for a compact representation of these problems  and a new hybrid
approximate linear programming (HALP) framework that permits their efficient
solutions. The central idea of HALP is to approximate the optimal value
function by a linear combination of basis functions and optimize its weights by
linear programming. We analyze both theoretical and computational aspects of
this approach  and demonstrate its scale-up potential on several hybrid
optimization problems."
"This paper introduces and analyzes a battery of inference models for the
problem of semantic role labeling. one based on constraint satisfaction  and
several strategies that model the inference as a meta-learning problem using
discriminative classifiers. These classifiers are developed with a rich set of
novel features that encode proposition and sentence-level information. To our
knowledge  this is the first work that. (a) performs a thorough analysis of
learning-based inference models for semantic role labeling  and (b) compares
several inference strategies in this context. We evaluate the proposed
inference strategies in the framework of the CoNLL-2005 shared task using only
automatically-generated syntactic information. The extensive experimental
evaluation and analysis indicates that all the proposed inference strategies
are successful -they all outperform the current best results reported in the
CoNLL-2005 evaluation exercise- but each of the proposed approaches has its
advantages and disadvantages. Several important traits of a state-of-the-art
SRL combination strategy emerge from this analysis. (i) individual models
should be combined at the granularity of candidate arguments rather than at the
granularity of complete solutions; (ii) the best combination strategy uses an
inference model based in learning; and (iii) the learning-based inference
benefits from max-margin classifiers and global feedback."
"In contrast to the existing approaches to bisimulation for fuzzy systems  we
introduce a behavioral distance to measure the behavioral similarity of states
in a nondeterministic fuzzy-transition system. This behavioral distance is
defined as the greatest fixed point of a suitable monotonic function and
provides a quantitative analogue of bisimilarity. The behavioral distance has
the important property that two states are at zero distance if and only if they
are bisimilar. Moreover  for any given threshold  we find that states with
behavioral distances bounded by the threshold are equivalent. In addition  we
show that two system combinators---parallel composition and product---are
non-expansive with respect to our behavioral distance  which makes
compositional verification possible."
"In a field of research about general reasoning mechanisms  it is essential to
have appropriate benchmarks. Ideally  the benchmarks should reflect possible
applications of the developed technology. In AI Planning  researchers more and
more tend to draw their testing examples from the benchmark collections used in
the International Planning Competition (IPC). In the organization of (the
deterministic part of) the fourth IPC  IPC-4  the authors therefore invested
significant effort to create a useful set of benchmarks. They come from five
different (potential) real-world applications of planning. airport ground
traffic control  oil derivative transportation in pipeline networks 
model-checking safety properties  power supply restoration  and UMTS call
setup. Adapting and preparing such an application for use as a benchmark in the
IPC involves  at the time  inevitable (often drastic) simplifications  as well
as careful choice between  and engineering of  domain encodings. For the first
time in the IPC  we used compilations to formulate complex domain features in
simple languages such as STRIPS  rather than just dropping the more interesting
problem constraints in the simpler language subsets. The article explains and
discusses the five application domains and their adaptation to form the PDDL
test suites used in IPC-4. We summarize known theoretical results on structural
properties of the domains  regarding their computational complexity and
provable properties of their topology under the h+ function (an idealized
version of the relaxed plan heuristic). We present new (empirical) results
illuminating properties such as the quality of the most wide-spread heuristic
functions (planning graph  serial planning graph  and relaxed plan)  the growth
of propositional representations over instance size  and the number of actions
available to achieve each fact; we discuss these data in conjunction with the
best results achieved by the different kinds of planners participating in
IPC-4."
"In this paper we present pddl+  a planning domain description language for
modelling mixed discrete-continuous planning domains. We describe the syntax
and modelling style of pddl+  showing that the language makes convenient the
modelling of complex time-dependent effects. We provide a formal semantics for
pddl+ by mapping planning instances into constructs of hybrid automata. Using
the syntax of HAs as our semantic model we construct a semantic mapping to
labelled transition systems to complete the formal interpretation of pddl+
planning instances. An advantage of building a mapping from pddl+ to HA theory
is that it forms a bridge between the Planning and Real Time Systems research
communities. One consequence is that we can expect to make use of some of the
theoretical properties of HAs. For example  for a restricted class of HAs the
Reachability problem (which is equivalent to Plan Existence) is decidable.
pddl+ provides an alternative to the continuous durative action model of
pddl2.1  adding a more flexible and robust model of time-dependent behaviour."
"In this paper  we show that there is a close relation between consistency in
a constraint network and set intersection. A proof schema is provided as a
generic way to obtain consistency properties from properties on set
intersection. This approach not only simplifies the understanding of and
unifies many existing consistency results  but also directs the study of
consistency to that of set intersection properties in many situations  as
demonstrated by the results on the convexity and tightness of constraints in
this paper. Specifically  we identify a new class of tree convex constraints
where local consistency ensures global consistency. This generalizes row convex
constraints. Various consistency results are also obtained on constraint
networks where only some  in contrast to all in the existing work constraints
are tight."
"In this paper  we study the possibility of designing non-trivial random CSP
models by exploiting the intrinsic connection between structures and
typical-case hardness. We show that constraint consistency  a notion that has
been developed to improve the efficiency of CSP algorithms  is in fact the key
to the design of random CSP models that have interesting phase transition
behavior and guaranteed exponential resolution complexity without putting much
restriction on the parameter of constraint tightness or the domain size of the
problem. We propose a very flexible framework for constructing problem
instances withinteresting behavior and develop a variety of concrete methods to
construct specific random CSP models that enforce different levels of
constraint consistency. A series of experimental studies with interesting
observations are carried out to illustrate the effectiveness of introducing
structural elements in random instances  to verify the robustness of our
proposal  and to investigate features of some specific models based on our
framework that are highly related to the behavior of backtracking search
algorithms."
"In this paper  we present two alternative approaches to defining answer sets
for logic programs with arbitrary types of abstract constraint atoms (c-atoms).
These approaches generalize the fixpoint-based and the level mapping based
answer set semantics of normal logic programs to the case of logic programs
with arbitrary types of c-atoms. The results are four different answer set
definitions which are equivalent when applied to normal logic programs. The
standard fixpoint-based semantics of logic programs is generalized in two
directions  called answer set by reduct and answer set by complement. These
definitions  which differ from each other in the treatment of
negation-as-failure (naf) atoms  make use of an immediate consequence operator
to perform answer set checking  whose definition relies on the notion of
conditional satisfaction of c-atoms w.r.t. a pair of interpretations. The other
two definitions  called strongly and weakly well-supported models  are
generalizations of the notion of well-supported models of normal logic programs
to the case of programs with c-atoms. As for the case of fixpoint-based
semantics  the difference between these two definitions is rooted in the
treatment of naf atoms. We prove that answer sets by reduct (resp. by
complement) are equivalent to weakly (resp. strongly) well-supported models of
a program  thus generalizing the theorem on the correspondence between stable
models and well-supported models of a normal logic program to the class of
programs with c-atoms. We show that the newly defined semantics coincide with
previously introduced semantics for logic programs with monotone c-atoms  and
they extend the original answer set semantics of normal logic programs. We also
study some properties of answer sets of programs with c-atoms  and relate our
definitions to several semantics for logic programs with aggregates presented
in the literature."
"Many combinatorial optimization problems such as the bin packing and multiple
knapsack problems involve assigning a set of discrete objects to multiple
containers. These problems can be used to model task and resource allocation
problems in multi-agent systems and distributed systms  and can also be found
as subproblems of scheduling problems. We propose bin completion  a
branch-and-bound strategy for one-dimensional  multicontainer packing problems.
Bin completion combines a bin-oriented search space with a powerful dominance
criterion that enables us to prune much of the space. The performance of the
basic bin completion framework can be enhanced by using a number of extensions 
including nogood-based pruning techniques that allow further exploitation of
the dominance criterion. Bin completion is applied to four problems. multiple
knapsack  bin covering  min-cost covering  and bin packing. We show that our
bin completion algorithms yield new  state-of-the-art results for the multiple
knapsack  bin covering  and min-cost covering problems  outperforming previous
algorithms by several orders of magnitude with respect to runtime on some
classes of hard  random problem instances. For the bin packing problem  we
demonstrate significant improvements compared to most previous results  but
show that bin completion is not competitive with current state-of-the-art
cutting-stock based approaches."
"In real-life temporal scenarios  uncertainty and preferences are often
essential and coexisting aspects. We present a formalism where quantitative
temporal constraints with both preferences and uncertainty can be defined. We
show how three classical notions of controllability (that is  strong  weak  and
dynamic)  which have been developed for uncertain temporal problems  can be
generalized to handle preferences as well. After defining this general
framework  we focus on problems where preferences follow the fuzzy approach 
and with properties that assure tractability. For such problems  we propose
algorithms to check the presence of the controllability properties. In
particular  we show that in such a setting dealing simultaneously with
preferences and uncertainty does not increase the complexity of controllability
testing. We also develop a dynamic execution algorithm  of polynomial
complexity  that produces temporal plans under uncertainty that are optimal
with respect to fuzzy preferences."
"In the recent years several research efforts have focused on the concept of
time granularity and its applications. A first stream of research investigated
the mathematical models behind the notion of granularity and the algorithms to
manage temporal data based on those models. A second stream of research
investigated symbolic formalisms providing a set of algebraic operators to
define granularities in a compact and compositional way. However  only very
limited manipulation algorithms have been proposed to operate directly on the
algebraic representation making it unsuitable to use the symbolic formalisms in
applications that need manipulation of granularities.
  This paper aims at filling the gap between the results from these two streams
of research  by providing an efficient conversion from the algebraic
representation to the equivalent low-level representation based on the
mathematical models. In addition  the conversion returns a minimal
representation in terms of period length. Our results have a major practical
impact. users can more easily define arbitrary granularities in terms of
algebraic operators  and then access granularity reasoning and other services
operating efficiently on the equivalent  minimal low-level representation. As
an example  we illustrate the application to temporal constraint reasoning with
multiple granularities.
  From a technical point of view  we propose an hybrid algorithm that
interleaves the conversion of calendar subexpressions into periodical sets with
the minimization of the period length. The algorithm returns set-based
granularity representations having minimal period length  which is the most
relevant parameter for the performance of the considered reasoning services.
Extensive experimental work supports the techniques used in the algorithm  and
shows the efficiency and effectiveness of the algorithm."
"We consider the problem of computing a lightest derivation of a global
structure using a set of weighted rules. A large variety of inference problems
in AI can be formulated in this framework. We generalize A* search and
heuristics derived from abstractions to a broad class of lightest derivation
problems. We also describe a new algorithm that searches for lightest
derivations using a hierarchy of abstractions. Our generalization of A* gives a
new algorithm for searching AND/OR graphs in a bottom-up fashion. We discuss
how the algorithms described here provide a general architecture for addressing
the pipeline problem --- the problem of passing information back and forth
between various stages of processing in a perceptual system. We consider
examples in computer vision and natural language processing. We apply the
hierarchical search algorithm to the problem of estimating the boundaries of
convex objects in grayscale images and compare it to other search methods. A
second set of experiments demonstrate the use of a new compositional model for
finding salient curves in images."
"In this paper  we construct and investigate a hierarchy of spatio-temporal
formalisms that result from various combinations of propositional spatial and
temporal logics such as the propositional temporal logic PTL  the spatial
logics RCC-8  BRCC-8  S4u and their fragments. The obtained results give a
clear picture of the trade-off between expressiveness and computational
realisability within the hierarchy. We demonstrate how different combining
principles as well as spatial and temporal primitives can produce NP-  PSPACE- 
EXPSPACE-  2EXPSPACE-complete  and even undecidable spatio-temporal logics out
of components that are at most NP- or PSPACE-complete."
"The treatment of exogenous events in planning is practically important in
many real-world domains where the preconditions of certain plan actions are
affected by such events. In this paper we focus on planning in temporal domains
with exogenous events that happen at known times  imposing the constraint that
certain actions in the plan must be executed during some predefined time
windows. When actions have durations  handling such temporal constraints adds
an extra difficulty to planning. We propose an approach to planning in these
domains which integrates constraint-based temporal reasoning into a graph-based
planning framework using local search. Our techniques are implemented in a
planner that took part in the 4th International Planning Competition (IPC-4). A
statistical analysis of the results of IPC-4 demonstrates the effectiveness of
our approach in terms of both CPU-time and plan quality. Additional experiments
show the good performance of the temporal reasoning techniques integrated into
our planner."
"In this commentary I argue that although PDDL is a very useful standard for
the planning competition  its design does not properly consider the issue of
domain modeling. Hence  I would not advocate its use in specifying planning
domains outside of the context of the planning competition. Rather  the field
needs to explore different approaches and grapple more directly with the
problem of effectively modeling and utilizing all of the diverse pieces of
knowledge we typically have about planning domains."
"PDDL was originally conceived and constructed as a lingua franca for the
International Planning Competition. PDDL2.1 embodies a set of extensions
intended to support the expression of something closer to real planning
problems. This objective has only been partially achieved  due in large part to
a deliberate focus on not moving too far from classical planning models and
solution methods."
"I comment on the PDDL 2.1 language and its use in the planning competition 
focusing on the choices made for accommodating time and concurrency. I also
discuss some methodological issues that have to do with the move toward more
expressive planning languages and the balance needed in planning research
between semantics and computation."
"Most classical scheduling formulations assume a fixed and known duration for
each activity. In this paper  we weaken this assumption  requiring instead that
each duration can be represented by an independent random variable with a known
mean and variance. The best solutions are ones which have a high probability of
achieving a good makespan. We first create a theoretical framework  formally
showing how Monte Carlo simulation can be combined with deterministic
scheduling algorithms to solve this problem. We propose an associated
deterministic scheduling problem whose solution is proved  under certain
conditions  to be a lower bound for the probabilistic problem. We then propose
and investigate a number of techniques for solving such problems based on
combinations of Monte Carlo simulation  solutions to the associated
deterministic problem  and either constraint programming or tabu search. Our
empirical results demonstrate that a combination of the use of the associated
deterministic problem and Monte Carlo simulation results in algorithms that
scale best both in terms of problem size and uncertainty. Further experiments
point to the correlation between the quality of the deterministic solution and
the quality of the probabilistic solution as a major factor responsible for
this success."
"This paper is concerned with a class of algorithms that perform exhaustive
search on propositional knowledge bases. We show that each of these algorithms
defines and generates a propositional language. Specifically  we show that the
trace of a search can be interpreted as a combinational circuit  and a search
algorithm then defines a propositional language consisting of circuits that are
generated across all possible executions of the algorithm. In particular  we
show that several versions of exhaustive DPLL search correspond to such
well-known languages as FBDD  OBDD  and a precisely-defined subset of d-DNNF.
By thus mapping search algorithms to propositional languages  we provide a
uniform and practical framework in which successful search techniques can be
harnessed for compilation of knowledge into various languages of interest  and
a new methodology whereby the power and limitations of search algorithms can be
understood by looking up the tractability and succinctness of the corresponding
propositional languages."
"The best performing algorithms for a particular oversubscribed scheduling
application  Air Force Satellite Control Network (AFSCN) scheduling  appear to
have little in common. Yet  through careful experimentation and modeling of
performance in real problem instances  we can relate characteristics of the
best algorithms to characteristics of the application. In particular  we find
that plateaus dominate the search spaces (thus favoring algorithms that make
larger changes to solutions) and that some randomization in exploration is
critical to good performance (due to the lack of gradient information on the
plateaus). Based on our explanations of algorithm performance  we develop a new
algorithm that combines characteristics of the best performers; the new
algorithms performance is better than the previous best. We show how hypothesis
driven experimentation and search modeling can both explain algorithm
performance and motivate the design of a new algorithm."
"This paper describes Marvin  a planner that competed in the Fourth
International Planning Competition (IPC 4). Marvin uses
action-sequence-memoisation techniques to generate macro-actions  which are
then used during search for a solution plan. We provide an overview of its
architecture and search behaviour  detailing the algorithms used. We also
empirically demonstrate the effectiveness of its features in various planning
domains; in particular  the effects on performance due to the use of
macro-actions  the novel features of its search behaviour  and the native
support of ADL and Derived Predicates."
"We describe how to convert the heuristic search algorithm A* into an anytime
algorithm that finds a sequence of improved solutions and eventually converges
to an optimal solution. The approach we adopt uses weighted heuristic search to
find an approximate solution quickly  and then continues the weighted search to
find improved solutions as well as to improve a bound on the suboptimality of
the current solution. When the time available to solve a search problem is
limited or uncertain  this creates an anytime heuristic search algorithm that
allows a flexible tradeoff between search time and solution quality. We analyze
the properties of the resulting Anytime A* algorithm  and consider its
performance in three domains; sliding-tile puzzles  STRIPS planning  and
multiple sequence alignment. To illustrate the generality of this approach  we
also describe how to transform the memory-efficient search algorithm Recursive
Best-First Search (RBFS) into an anytime algorithm."
"In this paper we apply computer-aided theorem discovery technique to discover
theorems about strongly equivalent logic programs under the answer set
semantics. Our discovered theorems capture new classes of strongly equivalent
logic programs that can lead to new program simplification rules that preserve
strong equivalence. Specifically  with the help of computers  we discovered
exact conditions that capture the strong equivalence between a rule and the
empty set  between two rules  between two rules and one of the two rules 
between two rules and another rule  and between three rules and two of the
three rules."
"The QXORSAT problem is the quantified version of the satisfiability problem
XORSAT in which the connective exclusive-or is used instead of the usual or. We
study the phase transition associated with random QXORSAT instances. We give a
description of this phase transition in the case of one alternation of
quantifiers  thus performing an advanced practical and theoretical study on the
phase transition of a quantified roblem."
"The paper presents a new sampling methodology for Bayesian networks that
samples only a subset of variables and applies exact inference to the rest.
Cutset sampling is a network structure-exploiting application of the
Rao-Blackwellisation principle to sampling in Bayesian networks. It improves
convergence by exploiting memory-based inference algorithms. It can also be
viewed as an anytime approximation of the exact cutset-conditioning algorithm
developed by Pearl. Cutset sampling can be implemented efficiently when the
sampled variables constitute a loop-cutset of the Bayesian network and  more
generally  when the induced width of the networks graph conditioned on the
observed sampled variables is bounded by a constant w. We demonstrate
empirically the benefit of this scheme on a range of benchmarks."
"Numerous formalisms and dedicated algorithms have been designed in the last
decades to model and solve decision making problems. Some formalisms  such as
constraint networks  can express ""simple"" decision problems  while others are
designed to take into account uncertainties  unfeasible decisions  and
utilities. Even in a single formalism  several variants are often proposed to
model different types of uncertainty (probability  possibility...) or utility
(additive or not). In this article  we introduce an algebraic graphical model
that encompasses a large number of such formalisms. (1) we first adapt previous
structures from Friedman  Chu and Halpern for representing uncertainty 
utility  and expected utility in order to deal with generic forms of sequential
decision making; (2) on these structures  we then introduce composite graphical
models that express information via variables linked by ""local"" functions 
thanks to conditional independence; (3) on these graphical models  we finally
define a simple class of queries which can represent various scenarios in terms
of observabilities and controllabilities. A natural decision-tree semantics for
such queries is completed by an equivalent operational semantics  which induces
generic algorithms. The proposed framework  called the
Plausibility-Feasibility-Utility (PFU) framework  not only provides a better
understanding of the links between existing formalisms  but it also covers yet
unpublished frameworks (such as possibilistic influence diagrams) and unifies
formalisms such as quantified boolean formulas and influence diagrams. Our
backtrack and variable elimination generic algorithms are a first step towards
unified algorithms."
"Matchmaking arises when supply and demand meet in an electronic marketplace 
or when agents search for a web service to perform some task  or even when
recruiting agencies match curricula and job profiles. In such open
environments  the objective of a matchmaking process is to discover best
available offers to a given request. We address the problem of matchmaking from
a knowledge representation perspective  with a formalization based on
Description Logics. We devise Concept Abduction and Concept Contraction as
non-monotonic inferences in Description Logics suitable for modeling
matchmaking in a logical framework  and prove some related complexity results.
We also present reasonable algorithms for semantic matchmaking based on the
devised inferences  and prove that they obey to some commonsense properties.
Finally  we report on the implementation of the proposed matchmaking framework 
which has been used both as a mediator in e-marketplaces and for semantic web
services discovery."
"Solution-Guided Multi-Point Constructive Search (SGMPCS) is a novel
constructive search technique that performs a series of resource-limited tree
searches where each search begins either from an empty solution (as in
randomized restart) or from a solution that has been encountered during the
search. A small number of these ""elite solutions is maintained during the
search. We introduce the technique and perform three sets of experiments on the
job shop scheduling problem. First  a systematic  fully crossed study of SGMPCS
is carried out to evaluate the performance impact of various parameter
settings. Second  we inquire into the diversity of the elite solution set 
showing  contrary to expectations  that a less diverse set leads to stronger
performance. Finally  we compare the best parameter setting of SGMPCS from the
first two experiments to chronological backtracking  limited discrepancy
search  randomized restart  and a sophisticated tabu search algorithm on a set
of well-known benchmark problems. Results demonstrate that SGMPCS is
significantly better than the other constructive techniques tested  though lags
behind the tabu search."
"This essay explores the limits of Turing machines concerning the modeling of
minds and suggests alternatives to go beyond those limits."
"This paper compares various optimization methods for fuzzy inference system
optimization. The optimization methods compared are genetic algorithm  particle
swarm optimization and simulated annealing. When these techniques were
implemented it was observed that the performance of each technique within the
fuzzy inference system classification was context dependent."
"We introduce matrix and its block to the Dung's theory of argumentation
framework. It is showed that each argumentation framework has a matrix
representation  and the indirect attack relation and indirect defence relation
can be characterized by computing the matrix. This provide a powerful
mathematics way to determine the ""controversial arguments"" in an argumentation
framework. Also  we introduce several kinds of blocks based on the matrix  and
various prudent semantics of argumentation frameworks can all be determined by
computing and comparing the matrices and their blocks which we have defined. In
contrast with traditional method of directed graph  the matrix method has an
excellent advantage. computability(even can be realized on computer easily).
So  there is an intensive perspective to import the theory of matrix to the
research of argumentation frameworks and its related areas."
"Real-time search methods are suited for tasks in which the agent is
interacting with an initially unknown environment in real time. In such
simultaneous planning and learning problems  the agent has to select its
actions in a limited amount of time  while sensing only a local part of the
environment centered at the agents current location. Real-time heuristic search
agents select actions using a limited lookahead search and evaluating the
frontier states with a heuristic function. Over repeated experiences  they
refine heuristic values of states to avoid infinite loops and to converge to
better solutions. The wide spread of such settings in autonomous software and
hardware agents has led to an explosion of real-time search algorithms over the
last two decades. Not only is a potential user confronted with a hodgepodge of
algorithms  but he also faces the choice of control parameters they use. In
this paper we address both problems. The first contribution is an introduction
of a simple three-parameter framework (named LRTS) which extracts the core
ideas behind many existing algorithms. We then prove that LRTA*  epsilon-LRTA* 
SLA*  and gamma-Trap algorithms are special cases of our framework. Thus  they
are unified and extended with additional features. Second  we prove
completeness and convergence of any algorithm covered by the LRTS framework.
Third  we prove several upper-bounds relating the control parameters and
solution quality. Finally  we analyze the influence of the three control
parameters empirically in the realistic scalable domains of real-time
navigation on initially unknown maps from a commercial role-playing game as
well as routing in ad hoc sensor networks."
"This paper introduces the SEQ BIN meta-constraint with a polytime algorithm
achieving general- ized arc-consistency according to some properties. SEQ BIN
can be used for encoding counting con- straints such as CHANGE  SMOOTH or
INCREAS- ING NVALUE. For some of these constraints and some of their variants
GAC can be enforced with a time and space complexity linear in the sum of
domain sizes  which improves or equals the best known results of the
literature."
"The Taaable projet goal is to create a case-based reasoning system for
retrieval and adaptation of cooking recipes. Within this framework  we are
discussing the temporal aspects of recipes and the means of representing those
in order to adapt their text."
"Designing component-based constraint solvers is a complex problem. Some
components are required  some are optional and there are interdependencies
between the components. Because of this  previous approaches to solver design
and modification have been ad-hoc and limited. We present a system that
transforms a description of the components and the characteristics of the
target constraint solver into a constraint problem. Solving this problem yields
the description of a valid solver. Our approach represents a significant step
towards the automated design and synthesis of constraint solvers that are
specialised for individual constraint problem classes or instances."
"Classification of targets by radar has proved to be notoriously difficult
with the best systems still yet to attain sufficiently high levels of
performance and reliability. In the current contribution we explore a new
design of radar based target recognition  where angular diversity is used in a
cognitive manner to attain better performance. Performance is bench- marked
against conventional classification schemes. The proposed scheme can easily be
extended to cognitive target recognition based on multiple diversity
strategies."
"We consider an extension of the setting of label ranking  in which the
learner is allowed to make predictions in the form of partial instead of total
orders. Predictions of that kind are interpreted as a partial abstention. If
the learner is not sufficiently certain regarding the relative order of two
alternatives  it may abstain from this decision and instead declare these
alternatives as being incomparable. We propose a new method for learning to
predict partial orders that improves on an existing approach  both
theoretically and empirically. Our method is based on the idea of thresholding
the probabilities of pairwise preferences between labels as induced by a
predicted (parameterized) probability distribution on the set of all rankings."
"Covering model provides a general framework for granular computing in that
overlapping among granules are almost indispensable. For any given covering 
both intersection and union of covering blocks containing an element are
exploited as granules to form granular worlds at different abstraction levels 
respectively  and transformations among these different granular worlds are
also discussed. As an application of the presented multi-granular perspective
on covering  relational interpretation and axiomization of four types of
covering based rough upper approximation operators are investigated  which can
be dually applied to lower ones."
"Slow Feature Analysis (SFA) extracts features representing the underlying
causes of changes within a temporally coherent high-dimensional raw sensory
input signal. Our novel incremental version of SFA (IncSFA) combines
incremental Principal Components Analysis and Minor Components Analysis. Unlike
standard batch-based SFA  IncSFA adapts along with non-stationary environments 
is amenable to episodic training  is not corrupted by outliers  and is
covariance-free. These properties make IncSFA a generally useful unsupervised
preprocessor for autonomous learning agents and robots. In IncSFA  the CCIPCA
and MCA updates take the form of Hebbian and anti-Hebbian updating  extending
the biological plausibility of SFA. In both single node and deep network
versions  IncSFA learns to encode its input streams (such as high-dimensional
video) by informative slow features representing meaningful abstract
environmental properties. It can handle cases where batch SFA fails."
"Many performance metrics have been introduced for the evaluation of
classification performance  with different origins and niches of application.
accuracy  macro-accuracy  area under the ROC curve  the ROC convex hull  the
absolute error  and the Brier score (with its decomposition into refinement and
calibration). One way of understanding the relation among some of these metrics
is the use of variable operating conditions (either in the form of
misclassification costs or class proportions). Thus  a metric may correspond to
some expected loss over a range of operating conditions. One dimension for the
analysis has been precisely the distribution we take for this range of
operating conditions  leading to some important connections in the area of
proper scoring rules. However  we show that there is another dimension which
has not received attention in the analysis of performance metrics. This new
dimension is given by the decision rule  which is typically implemented as a
threshold choice method when using scoring models. In this paper  we explore
many old and new threshold choice methods. fixed  score-uniform  score-driven 
rate-driven and optimal  among others. By calculating the loss of these methods
for a uniform range of operating conditions we get the 0-1 loss  the absolute
error  the Brier score (mean squared error)  the AUC and the refinement loss
respectively. This provides a comprehensive view of performance metrics as well
as a systematic approach to loss minimisation  namely. take a model  apply
several threshold choice methods consistent with the information which is (and
will be) available about the operating condition  and compare their expected
losses. In order to assist in this procedure we also derive several connections
between the aforementioned performance metrics  and we highlight the role of
calibration in choosing the threshold choice method."
"Probabilistic Logic Programming (PLP)  exemplified by Sato and Kameya's
PRISM  Poole's ICL  Raedt et al's ProbLog and Vennekens et al's LPAD  is aimed
at combining statistical and logical knowledge representation and inference. A
key characteristic of PLP frameworks is that they are conservative extensions
to non-probabilistic logic programs which have been widely used for knowledge
representation. PLP frameworks extend traditional logic programming semantics
to a distribution semantics  where the semantics of a probabilistic logic
program is given in terms of a distribution over possible models of the
program. However  the inference techniques used in these works rely on
enumerating sets of explanations for a query answer. Consequently  these
languages permit very limited use of random variables with continuous
distributions. In this paper  we present a symbolic inference procedure that
uses constraints and represents sets of explanations without enumeration. This
permits us to reason over PLPs with Gaussian or Gamma-distributed random
variables (in addition to discrete-valued random variables) and linear equality
constraints over reals. We develop the inference procedure in the context of
PRISM; however the procedure's core ideas can be easily applied to other PLP
languages as well. An interesting aspect of our inference procedure is that
PRISM's query evaluation process becomes a special case in the absence of any
continuous random variables in the program. The symbolic inference procedure
enables us to reason over complex probabilistic models such as Kalman filters
and a large subclass of Hybrid Bayesian networks that were hitherto not
possible in PLP frameworks. (To appear in Theory and Practice of Logic
Programming)."
"We consider the task of performing probabilistic inference with probabilistic
logical models. Many algorithms for approximate inference with such models are
based on sampling. From a logic programming perspective  sampling boils down to
repeatedly calling the same queries on a knowledge base composed of a static
part and a dynamic part. The larger the static part  the more redundancy there
is in these repeated calls. This is problematic since inefficient sampling
yields poor approximations.
  We show how to apply logic program specialization to make sampling-based
inference more efficient. We develop an algorithm that specializes the
definitions of the query predicates with respect to the static part of the
knowledge base. In experiments on real-world data we obtain speedups of up to
an order of magnitude  and these speedups grow with the data-size."
"In this paper  the continuity and strong continuity in domain-free
information algebras and labeled information algebras are introduced
respectively. A more general concept of continuous function which is defined
between two domain-free continuous information algebras is presented. It is
shown that  with the operations combination and focusing  the set of all
continuous functions between two domain-free s-continuous information algebras
forms a new s-continuous information algebra. By studying the relationship
between domain-free information algebras and labeled information algebras  it
is demonstrated that they do correspond to each other on s-compactness."
"We study propagation of the RegularGcc global constraint. This ensures that
each row of a matrix of decision variables satisfies a Regular constraint  and
each column satisfies a Gcc constraint. On the negative side  we prove that
propagation is NP-hard even under some strong restrictions (e.g. just 3 values 
just 4 states in the automaton  or just 5 columns to the matrix). On the
positive side  we identify two cases where propagation is fixed parameter
tractable. In addition  we show how to improve propagation over a simple
decomposition into separate Regular and Gcc constraints by identifying some
necessary but insufficient conditions for a solution. We enforce these
conditions with some additional weighted row automata. Experimental results
demonstrate the potential of these methods on some standard benchmark problems."
"Fuzzy rule based models have a capability to approximate any continuous
function to any degree of accuracy on a compact domain. The majority of FLC
design process relies on heuristic knowledge of experience operators. In order
to make the design process automatic we present a genetic approach to learn
fuzzy rules as well as membership function parameters. Moreover  several
statistical information criteria such as the Akaike information criterion
(AIC)  the Bhansali-Downham information criterion (BDIC)  and the
Schwarz-Rissanen information criterion (SRIC) are used to construct optimal
fuzzy models by reducing fuzzy rules. A genetic scheme is used to design
Takagi-Sugeno-Kang (TSK) model for identification of the antecedent rule
parameters and the identification of the consequent parameters. Computer
simulations are presented confirming the performance of the constructed fuzzy
logic controller."
"We mathematically model Ignacio Matte Blanco's principles of symmetric and
asymmetric being through use of an ultrametric topology. We use for this the
highly regarded 1975 book of this Chilean psychiatrist and pyschoanalyst (born
1908  died 1995). Such an ultrametric model corresponds to hierarchical
clustering in the empirical data  e.g. text. We show how an ultrametric
topology can be used as a mathematical model for the structure of the logic
that reflects or expresses Matte Blanco's symmetric being  and hence of the
reasoning and thought processes involved in conscious reasoning or in reasoning
that is lacking  perhaps entirely  in consciousness or awareness of itself. In
a companion paper we study how symmetric (in the sense of Matte Blanco's)
reasoning can be demarcated in a context of symmetric and asymmetric reasoning
provided by narrative text."
"This paper is the continuation of our research work about linguistic
truth-valued concept lattice. In order to provide a mathematical tool for
mining tacit knowledge  we establish a concrete model of 6-ary linguistic
truth-valued concept lattice and introduce a mining algorithm through the
structure consistency. Specifically  we utilize the attributes to depict
knowledge  propose the 6-ary linguistic truth-valued attribute extended context
and congener context to characterize tacit knowledge  and research the
necessary and sufficient conditions of forming tacit knowledge. We respectively
give the algorithms of generating the linguistic truth-valued congener context
and constructing the linguistic truth-valued concept lattice."
"Large-scale  parallel clusters composed of commodity processors are
increasingly available  enabling the use of vast processing capabilities and
distributed RAM to solve hard search problems. We investigate Hash-Distributed
A* (HDA*)  a simple approach to parallel best-first search that asynchronously
distributes and schedules work among processors based on a hash function of the
search state. We use this approach to parallelize the A* algorithm in an
optimal sequential version of the Fast Downward planner  as well as a 24-puzzle
solver. The scaling behavior of HDA* is evaluated experimentally on a shared
memory  multicore machine with 8 cores  a cluster of commodity machines using
up to 64 cores  and large-scale high-performance clusters  using up to 2400
processors. We show that this approach scales well  allowing the effective
utilization of large amounts of distributed memory to optimally solve problems
which require terabytes of RAM. We also compare HDA* to Transposition-table
Driven Scheduling (TDS)  a hash-based parallelization of IDA*  and show that 
in planning  HDA* significantly outperforms TDS. A simple hybrid which combines
HDA* and TDS to exploit strengths of both algorithms is proposed and evaluated."
"We review some existing methods for the computation of first order moments on
junction trees using Shafer-Shenoy algorithm. First  we consider the problem of
first order moments computation as vertices problem in junction trees. In this
way  the problem is solved using the memory space of an order of the junction
tree edge-set cardinality. After that  we consider two algorithms 
Lauritzen-Nilsson algorithm  and Mau\'a et al. algorithm  which computes the
first order moments as the normalization problem in junction tree  using the
memory space of an order of the junction tree leaf-set cardinality."
"We present a technique for the animation of a 3D kinematic tongue model  one
component of the talking head of an acoustic-visual (AV) speech synthesizer.
The skeletal animation approach is adapted to make use of a deformable rig
controlled by tongue motion capture data obtained with electromagnetic
articulography (EMA)  while the tongue surface is extracted from volumetric
magnetic resonance imaging (MRI) data. Initial results are shown and future
work outlined."
"This paper draws on diverse areas of computer science to develop a unified
view of computation.
  (1) Optimization in operations research  where a numerical objective function
is maximized under constraints  is generalized from the numerical total order
to a non-numerical partial order that can be interpreted in terms of
information. (2) Relations are generalized so that there are relations of which
the constituent tuples have numerical indexes  whereas in other relations these
indexes are variables. The distinction is essential in our definition of
constraint satisfaction problems. (3) Constraint satisfaction problems are
formulated in terms of semantics of conjunctions of atomic formulas of
predicate logic. (4) Approximation structures  which are available for several
important domains  are applied to solutions of constraint satisfaction
problems.
  As application we treat constraint satisfaction problems over reals. These
cover a large part of numerical analysis  most significantly nonlinear
equations and inequalities. The chaotic algorithm analyzed in the paper
combines the efficiency of floating-point computation with the correctness
guarantees of arising from our logico-mathematical model of
constraint-satisfaction problems."
"We built a multiagent simulation of urban traffic to model both ordinary
traffic and emergency or crisis mode traffic. This simulation first builds a
modeled road network based on detailed geographical information. On this
network  the simulation creates two populations of agents. the Transporters and
the Mobiles. Transporters embody the roads themselves; they are utilitarian and
meant to handle the low level realism of the simulation. Mobile agents embody
the vehicles that circulate on the network. They have one or several
destinations they try to reach using initially their beliefs of the structure
of the network (length of the edges  speed limits  number of lanes etc.).
Nonetheless  when confronted to a dynamic  emergent prone environment (other
vehicles  unexpectedly closed ways or lanes  traffic jams etc.)  the rather
reactive agent will activate more cognitive modules to adapt its beliefs 
desires and intentions. It may change its destination(s)  change the tactics
used to reach the destination (favoring less used roads  following other
agents  using general headings)  etc. We describe our current validation of our
model and the next planned improvements  both in validation and in
functionalities."
"After more than sixty years  Shannon's research [1-3] continues to raise
fundamental questions  such as the one formulated by Luce [4 5]  which is still
unanswered. ""Why is information theory not very applicable to psychological
problems  despite apparent similarities of concepts?"" On this topic  Pinker
[6]  one of the foremost defenders of the computational theory of mind [6]  has
argued that thought is simply a type of computation  and that the gap between
human cognition and computational models may be illusory. In this context  in
his latest book  titled Thinking Fast and Slow [8]  Kahneman [7 8] provides
further theoretical interpretation by differentiating the two assumed systems
of the cognitive functioning of the human mind. He calls them intuition (system
1) determined to be an associative (automatic  fast and perceptual) machine 
and reasoning (system 2) required to be voluntary and to operate logical-
deductively. In this paper  we propose an ansatz inspired by Ausubel's learning
theory for investigating  from the constructivist perspective [9-12] 
information processing in the working memory of cognizers. Specifically  a
thought experiment is performed utilizing the mind of a dual-natured creature
known as Maxwell's demon. a tiny ""man-machine"" solely equipped with the
characteristics of system 1  which prevents it from reasoning. The calculation
presented here shows that [...]. This result indicates that when the system 2
is shut down  both an intelligent being  as well as a binary machine  incur the
same energy cost per unit of information processed  which mathematically proves
the computational attribute of the system 1  as Kahneman [7 8] theorized. This
finding links information theory to human psychological features and opens a
new path toward the conception of a multi-bit reasoning machine."
"The holistic approach to sustainable urban planning implies using different
models in an integrated way that is capable of simulating the urban system. As
the interconnection of such models is not a trivial task  one of the key
elements that may be applied is the description of the urban geometric
properties in an ""interoperable"" way. Focusing on air quality as one of the
most pronounced urban problems  the geometric aspects of a city may be
described by objects such as those defined in CityGML  so that an appropriate
air quality model can be applied for estimating the quality of the urban air on
the basis of atmospheric flow and chemistry equations.
  In this paper we first present theoretical background and motivations for the
interconnection of 3D city models and other models related to sustainable
development and urban planning. Then we present a practical experiment based on
the interconnection of CityGML with an air quality model. Our approach is based
on the creation of an ontology of air quality models and on the extension of an
ontology of urban planning process (OUPP) that acts as an ontology mediator."
"In a world where communication and information sharing are at the heart of
our business  the terminology needs are most pressing. It has become imperative
to identify the terms used and defined in a consensual and coherent way while
preserving linguistic diversity. To streamline and strengthen the process of
acquisition  representation and exploitation of scenarii of train accidents  it
is necessary to harmonize and standardize the terminology used by players in
the security field. The research aims to significantly improve analytical
activities and operations of the various safety studies  by tracking the error
in system  hardware  software and human. This paper presents the contribution
of ontology to modeling scenarii for rail accidents through a knowledge model
based on a generic ontology and domain ontology. After a detailed presentation
of the state of the art material  this article presents the first results of
the developed model."
"The ability to model search in a constraint solver can be an essential asset
for solving combinatorial problems. However  existing infrastructure for
defining search heuristics is often inadequate. Either modeling capabilities
are extremely limited or users are faced with a general-purpose programming
language whose features are not tailored towards writing search heuristics. As
a result  major improvements in performance may remain unexplored.
  This article introduces search combinators  a lightweight and
solver-independent method that bridges the gap between a conceptually simple
modeling language for search (high-level  functional and naturally
compositional) and an efficient implementation (low-level  imperative and
highly non-modular). By allowing the user to define application-tailored search
strategies from a small set of primitives  search combinators effectively
provide a rich domain-specific language (DSL) for modeling search to the user.
Remarkably  this DSL comes at a low implementation cost to the developer of a
constraint solver.
  The article discusses two modular implementation approaches and shows  by
empirical evaluation  that search combinators can be implemented without
overhead compared to a native  direct implementation in a constraint solver."
"In Multi-Source Feedback or 360 Degree Feedback  data on the performance of
an individual are collected systematically from a number of stakeholders and
are used for improving performance. The 360-Degree Feedback approach provides a
consistent management philosophy meeting the criterion outlined previously. The
360-degree feedback appraisal process describes a human resource methodology
that is frequently used for both employee appraisal and employee development.
Used in employee performance appraisals  the 360-degree feedback methodology is
differentiated from traditional  top-down appraisal methods in which the
supervisor responsible for the appraisal provides the majority of the data.
Instead it seeks to use information gained from other sources to provide a
fuller picture of employees' performances. Similarly  when this technique used
in employee development it augments employees' perceptions of training needs
with those of the people with whom they interact. The 360-degree feedback based
appraisal is a comprehensive method where in the feedback about the employee
comes from all the sources that come into contact with the employee on his/her
job. The respondents for an employee can be her/his peers  managers 
subordinates team members  customers  suppliers and vendors. Hence anyone who
comes into contact with the employee  the 360 degree appraisal has four
components that include self-appraisal  superior's appraisal  subordinate's
appraisal student's appraisal and peer's appraisal .The proposed system is an
attempt to implement the 360 degree feedback based appraisal system in
academics especially engineering colleges."
"We propose a simple method for combining together voting rules that performs
a run-off between the different winners of each voting rule. We prove that this
combinator has several good properties. For instance  even if just one of the
base voting rules has a desirable property like Condorcet consistency  the
combination inherits this property. In addition  we prove that combining voting
rules together in this way can make finding a manipulation more computationally
difficult. Finally  we study the impact of this combinator on approximation
methods that find close to optimal manipulations."
"Languages for open-universe probabilistic models (OUPMs) can represent
situations with an unknown number of objects and iden- tity uncertainty. While
such cases arise in a wide range of important real-world appli- cations 
existing general purpose inference methods for OUPMs are far less efficient
than those available for more restricted lan- guages and model classes. This
paper goes some way to remedying this deficit by in- troducing  and proving
correct  a generaliza- tion of Gibbs sampling to partial worlds with possibly
varying model structure. Our ap- proach draws on and extends previous generic
OUPM inference methods  as well as aux- iliary variable samplers for
nonparametric mixture models. It has been implemented for BLOG  a well-known
OUPM language. Combined with compile-time optimizations  the resulting
algorithm yields very substan- tial speedups over existing methods on sev- eral
test cases  and substantially improves the practicality of OUPM languages
generally."
"Qualitative possibilistic networks  also known as min-based possibilistic
networks  are important tools for handling uncertain information in the
possibility theory frame- work. Despite their importance  only the junction
tree adaptation has been proposed for exact reasoning with such networks. This
paper explores alternative algorithms using compilation techniques. We first
propose possibilistic adaptations of standard compilation-based probabilistic
methods. Then  we develop a new  purely possibilistic  method based on the
transformation of the initial network into a possibilistic base. A comparative
study shows that this latter performs better than the possibilistic adap-
tations of probabilistic methods. This result is also confirmed by experimental
results."
"Possibilistic answer set programming (PASP) extends answer set programming
(ASP) by attaching to each rule a degree of certainty. While such an extension
is important from an application point of view  existing semantics are not
well-motivated  and do not always yield intuitive results. To develop a more
suitable semantics  we first introduce a characterization of answer sets of
classical ASP programs in terms of possibilistic logic where an ASP program
specifies a set of constraints on possibility distributions. This
characterization is then naturally generalized to define answer sets of PASP
programs. We furthermore provide a syntactic counterpart  leading to a
possibilistic generalization of the well-known Gelfond-Lifschitz reduct  and we
show how our framework can readily be implemented using standard ASP solvers."
"Performing sensitivity analysis for influence diagrams using the decision
circuit framework is particularly convenient  since the partial derivatives
with respect to every parameter are readily available [Bhattacharjya and
Shachter  2007; 2008]. In this paper we present three non-linear sensitivity
analysis methods that utilize this partial derivative information and therefore
do not require re-evaluating the decision situation multiple times.
Specifically  we show how to efficiently compare strategies in decision
situations  perform sensitivity to risk aversion and compute the value of
perfect hedging [Seyller  2008]."
"Many machine learning applications require the ability to learn from and
reason about noisy multi-relational data. To address this  several effective
representations have been developed that provide both a language for expressing
the structural regularities of a domain  and principled support for
probabilistic inference. In addition to these two aspects  however  many
applications also involve a third aspect-the need to reason about
similarities-which has not been directly supported in existing frameworks. This
paper introduces probabilistic similarity logic (PSL)  a general-purpose
framework for joint reasoning about similarity in relational domains that
incorporates probabilistic reasoning about similarities and relational
structure in a principled way. PSL can integrate any existing domain-specific
similarity measures and also supports reasoning about similarities between sets
of entities. We provide efficient inference and learning techniques for PSL and
demonstrate its effectiveness both in common relational tasks and in settings
that require reasoning about similarity."
"The Next Generation Air Transportation System will introduce new  advanced
sensor technologies into the cockpit. With the introduction of such systems 
the responsibilities of the pilot are expected to dramatically increase. In the
ALARMS (ALerting And Reasoning Management System) project for NASA  we focus on
a key challenge of this environment  the quick and efficient handling of
aircraft sensor alerts. It is infeasible to alert the pilot on the state of all
subsystems at all times. Furthermore  there is uncertainty as to the true
hazard state despite the evidence of the alerts  and there is uncertainty as to
the effect and duration of actions taken to address these alerts. This paper
reports on the first steps in the construction of an application designed to
handle Next Generation alerts. In ALARMS  we have identified 60 different
aircraft subsystems and 20 different underlying hazards. In this paper  we show
how a Bayesian network can be used to derive the state of the underlying
hazards  based on the sensor input. Then  we propose a framework whereby an
automated system can plan to address these hazards in cooperation with the
pilot  using a Time-Dependent Markov Process (TMDP). Different hazards and
pilot states will call for different alerting automation plans. We demonstrate
this emerging application of Bayesian networks and TMDPs to cockpit automation 
for a use case where a small number of hazards are present  and analyze the
resulting alerting automation policies."
"Relational Continuous Models (RCMs) represent joint probability densities
over attributes of objects  when the attributes have continuous domains. With
relational representations  they can model joint probability distributions over
large numbers of variables compactly in a natural way. This paper presents a
new exact lifted inference algorithm for RCMs  thus it scales up to large
models of real world applications. The algorithm applies to Relational Pairwise
Models which are (relational) products of potentials of arity 2. Our algorithm
is unique in two ways. First  it substantially improves the efficiency of
lifted inference with variables of continuous domains. When a relational model
has Gaussian potentials  it takes only linear-time compared to cubic time of
previous methods. Second  it is the first exact inference algorithm which
handles RCMs in a lifted way. The algorithm is illustrated over an example from
econometrics. Experimental results show that our algorithm outperforms both a
groundlevel inference algorithm and an algorithm built with previously-known
lifted methods."
"We propose a new point-based method for approximate planning in Dec-POMDP
which outperforms the state-of-the-art approaches in terms of solution quality.
It uses a heuristic estimation of the prior probability of beliefs to choose a
bounded number of policy trees. this choice is formulated as a combinatorial
optimisation problem minimising the error induced by pruning."
"Partially-Observable Markov Decision Processes (POMDPs) are typically solved
by finding an approximate global solution to a corresponding belief-MDP. In
this paper  we offer a new planning algorithm for POMDPs with continuous state 
action and observation spaces. Since such domains have an inherent notion of
locality  we can find an approximate solution using local optimization methods.
We parameterize the belief distribution as a Gaussian mixture  and use the
Extended Kalman Filter (EKF) to approximate the belief update. Since the EKF is
a first-order filter  we can marginalize over the observations analytically. By
using feedback control and state estimation during policy execution  we recover
a behavior that is effectively conditioned on incoming observations despite the
unconditioned planning. Local optimization provides no guarantees of global
optimality  but it allows us to tackle domains that are at least an order of
magnitude larger than the current state-of-the-art. We demonstrate the
scalability of our algorithm by considering a simulated hand-eye coordination
domain with 16 continuous state dimensions and 6 continuous action dimensions."
"Computing the probability of a formula given the probabilities or weights
associated with other formulas is a natural extension of logical inference to
the probabilistic setting. Surprisingly  this problem has received little
attention in the literature to date  particularly considering that it includes
many standard inference problems as special cases. In this paper  we propose
two algorithms for this problem. formula decomposition and conditioning  which
is an exact method  and formula importance sampling  which is an approximate
method. The latter is  to our knowledge  the first application of model
counting to approximate probabilistic inference. Unlike conventional
variable-based algorithms  our algorithms work in the dual realm of logical
formulas. Theoretically  we show that our algorithms can greatly improve
efficiency by exploiting the structural information in the formulas.
Empirically  we show that they are indeed quite powerful  often achieving
substantial performance gains over state-of-the-art schemes."
"Decentralized POMDPs provide an expressive framework for multi-agent
sequential decision making. While fnite-horizon DECPOMDPs have enjoyed
signifcant success  progress remains slow for the infnite-horizon case mainly
due to the inherent complexity of optimizing stochastic controllers
representing agent policies. We present a promising new class of algorithms for
the infnite-horizon case  which recasts the optimization problem as inference
in a mixture of DBNs. An attractive feature of this approach is the
straightforward adoption of existing inference techniques in DBNs for solving
DEC-POMDPs and supporting richer representations such as factored or continuous
states and actions. We also derive the Expectation Maximization (EM) algorithm
to optimize the joint policy represented as DBNs. Experiments on benchmark
domains show that EM compares favorably against the state-of-the-art solvers."
"We describe a framework and an algorithm for solving hybrid influence
diagrams with discrete  continuous  and deterministic chance variables  and
discrete and continuous decision variables. A continuous chance variable in an
influence diagram is said to be deterministic if its conditional distributions
have zero variances. The solution algorithm is an extension of Shenoy's fusion
algorithm for discrete influence diagrams. We describe an extended
Shenoy-Shafer architecture for propagation of discrete  continuous  and utility
potentials in hybrid influence diagrams that include deterministic chance
variables. The algorithm and framework are illustrated by solving two small
examples."
"The paper introduces k-bounded MAP inference  a parameterization of MAP
inference in Markov logic networks. k-Bounded MAP states are MAP states with at
most k active ground atoms of hidden (non-evidence) predicates. We present a
novel delayed column generation algorithm and provide empirical evidence that
the algorithm efficiently computes k-bounded MAP states for meaningful
real-world graph matching problems. The underlying idea is that  instead of
solving one large optimization problem  it is often more efficient to tackle
several small ones."
"Rollating walkers are popular mobility aids used by older adults to improve
balance control. There is a need to automatically recognize the activities
performed by walker users to better understand activity patterns  mobility
issues and the context in which falls are more likely to happen. We design and
compare several techniques to recognize walker related activities. A
comprehensive evaluation with control subjects and walker users from a
retirement community is presented."
"Belief merging is an important but difficult problem in Artificial
Intelligence  especially when sources of information are pervaded with
uncertainty. Many merging operators have been proposed to deal with this
problem in possibilistic logic  a weighted logic which is powerful for handling
inconsistency and deal- ing with uncertainty. They often result in a
possibilistic knowledge base which is a set of weighted formulas. Although
possibilistic logic is inconsistency tolerant  it suers from the well-known
""drowning effect"". Therefore  we may still want to obtain a consistent possi-
bilistic knowledge base as the result of merg- ing. In such a case  we argue
that it is not always necessary to keep weighted informa- tion after merging.
In this paper  we define a merging operator that maps a set of pos- sibilistic
knowledge bases and a formula rep- resenting the integrity constraints to a
clas- sical knowledge base by using lexicographic ordering. We show that it
satisfies nine pos- tulates that generalize basic postulates for propositional
merging given in [11]. These postulates capture the principle of minimal change
in some sense. We then provide an algorithm for generating the resulting knowl-
edge base of our merging operator. Finally  we discuss the compatibility of our
merging operator with propositional merging and es- tablish the advantage of
our merging opera- tor over existing semantic merging operators in the
propositional case."
"The standard coherence criterion for lower previsions is expressed using an
infinite number of linear constraints. For lower previsions that are
essentially defined on some finite set of gambles on a finite possibility
space  we present a reformulation of this criterion that only uses a finite
number of constraints. Any such lower prevision is coherent if it lies within
the convex polytope defined by these constraints. The vertices of this polytope
are the extreme coherent lower previsions for the given set of gambles. Our
reformulation makes it possible to compute them. We show how this is done and
illustrate the procedure and its results."
"Decision circuits perform efficient evaluation of influence diagrams 
building on the ad- vances in arithmetic circuits for belief net- work
inference [Darwiche  2003; Bhattachar- jya and Shachter  2007]. We show how
even more compact decision circuits can be con- structed for dynamic
programming in influ- ence diagrams with separable value functions and
conditionally independent subproblems. Once a decision circuit has been
constructed based on the diagram's ""global"" graphical structure  it can be
compiled to exploit ""lo- cal"" structure for efficient evaluation and sen-
sitivity analysis."
"In this paper  we present the Difference- Based Causality Learner (DBCL)  an
algorithm for learning a class of discrete-time dynamic models that represents
all causation across time by means of difference equations driving change in a
system. We motivate this representation with real-world mechanical systems and
prove DBCL's correctness for learning structure from time series data  an
endeavour that is complicated by the existence of latent derivatives that have
to be detected. We also prove that  under common assumptions for causal
discovery  DBCL will identify the presence or absence of feedback loops  making
the model more useful for predicting the effects of manipulating variables when
the system is in equilibrium. We argue analytically and show empirically the
advantages of DBCL over vector autoregression (VAR) and Granger causality
models as well as modified forms of Bayesian and constraintbased structure
discovery algorithms. Finally  we show that our algorithm can discover causal
directions of alpha rhythms in human brains from EEG data."
"We present decentralized rollout sampling policy iteration (DecRSPI) - a new
algorithm for multi-agent decision problems formalized as DEC-POMDPs. DecRSPI
is designed to improve scalability and tackle problems that lack an explicit
model. The algorithm uses Monte- Carlo methods to generate a sample of
reachable belief states. Then it computes a joint policy for each belief state
based on the rollout estimations. A new policy representation allows us to
represent solutions compactly. The key benefits of the algorithm are its linear
time complexity over the number of agents  its bounded memory usage and good
solution quality. It can solve larger problems that are intractable for
existing planning algorithms. Experimental results confirm the effectiveness
and scalability of the approach."
"A branch-and-bound approach to solving influ- ence diagrams has been
previously proposed in the literature  but appears to have never been
implemented and evaluated - apparently due to the difficulties of computing
effective bounds for the branch-and-bound search. In this paper  we describe
how to efficiently compute effective bounds  and we develop a practical
implementa- tion of depth-first branch-and-bound search for influence diagram
evaluation that outperforms existing methods for solving influence diagrams
with multiple stages."
"Despite the intractability of generic optimal partially observable Markov
decision process planning  there exist important problems that have highly
structured models. Previous researchers have used this insight to construct
more efficient algorithms for factored domains  and for domains with
topological structure in the flat state dynamics model. In our work  motivated
by findings from the education community relevant to automated tutoring  we
consider problems that exhibit a form of topological structure in the factored
dynamics model. Our Reachable Anytime Planner for Imprecisely-sensed Domains
(RAPID) leverages this structure to efficiently compute a good initial envelope
of reachable states under the optimal MDP policy in time linear in the number
of state variables. RAPID performs partially-observable planning over the
limited envelope of states  and slowly expands the state space considered as
time allows. RAPID performs well on a large tutoring-inspired problem
simulation with 122 state variables  corresponding to a flat state space of
over 10^30 states."
"UCT has recently emerged as an exciting new adversarial reasoning technique
based on cleverly balancing exploration and exploitation in a Monte-Carlo
sampling setting. It has been particularly successful in the game of Go but the
reasons for its success are not well understood and attempts to replicate its
success in other domains such as Chess have failed. We provide an in-depth
analysis of the potential of UCT in domain-independent settings  in cases where
heuristic values are available  and the effect of enhancing random playouts to
more informed playouts between two weak minimax players. To provide further
insights  we develop synthetic game tree instances and discuss interesting
properties of UCT  both empirically and analytically."
"Probabilistic Logic Programming (PLP)  exemplified by Sato and Kameya's
PRISM  Poole's ICL  De Raedt et al's ProbLog and Vennekens et al's LPAD 
combines statistical and logical knowledge representation and inference.
Inference in these languages is based on enumerative construction of proofs
over logic programs. Consequently  these languages permit very limited use of
random variables with continuous distributions. In this paper  we extend PRISM
with Gaussian random variables and linear equality constraints  and consider
the problem of parameter learning in the extended language. Many statistical
models such as finite mixture models and Kalman filter can be encoded in
extended PRISM. Our EM-based learning algorithm uses a symbolic inference
procedure that represents sets of derivations without enumeration. This permits
us to learn the distribution parameters of extended PRISM programs with
discrete as well as Gaussian variables. The learning algorithm naturally
generalizes the ones used for PRISM and Hybrid Bayesian Networks."
"Decision making whenever and wherever it is happened is key to organizations
success. In order to make correct decision  individuals  teams and
organizations need both knowledge management (to manage content) and
collaboration (to manage group processes) to make that more effective and
efficient. In this paper  we explain the knowledge management and collaboration
convergence. Then  we propose a formal description of mixed and multimodal
decision making (MDM) process where decision may be made by three possible
modes. individual  collective or hybrid. Finally  we explicit the MDM process
based on UML-G profile."
"We consider infinite-horizon $\gamma$-discounted Markov Decision Processes 
for which it is known that there exists a stationary optimal policy. We
consider the algorithm Value Iteration and the sequence of policies
$\pi_1 ... \pi_k$ it implicitely generates until some iteration $k$. We provide
performance bounds for non-stationary policies involving the last $m$ generated
policies that reduce the state-of-the-art bound for the last stationary policy
$\pi_k$ by a factor $\frac{1-\gamma}{1-\gamma^m}$. In particular  the use of
non-stationary policies allows to reduce the usual asymptotic performance
bounds of Value Iteration with errors bounded by $\epsilon$ at each iteration
from $\frac{\gamma}{(1-\gamma)^2}\epsilon$ to
$\frac{\gamma}{1-\gamma}\epsilon$  which is significant in the usual situation
when $\gamma$ is close to 1. Given Bellman operators that can only be computed
with some error $\epsilon$  a surprising consequence of this result is that the
problem of ""computing an approximately optimal non-stationary policy"" is much
simpler than that of ""computing an approximately optimal stationary policy"" 
and even slightly simpler than that of ""approximately computing the value of
some fixed policy""  since this last problem only has a guarantee of
$\frac{1}{1-\gamma}\epsilon$."
"Informledge System (ILS) is a knowledge network with autonomous nodes and
intelligent links that integrate and structure the pieces of knowledge. In this
paper  we aim to put forward the link dynamics involved in intelligent
processing of information in ILS. There has been advancement in knowledge
management field which involve managing information in databases from a single
domain. ILS works with information from multiple domains stored in distributed
way in the autonomous nodes termed as Knowledge Network Node (KNN). Along with
the concept under consideration  KNNs store the processed information linking
concepts and processors leading to the appropriate processing of information."
"Expert systems use human knowledge often stored as rules within the computer
to solve problems that generally would entail human intelligence. Today  with
information systems turning out to be more pervasive and with the myriad
advances in information technologies  automating computer fault diagnosis is
becoming so fundamental that soon every enterprise has to endorse it. This
paper proposes an expert system called Expert PC Troubleshooter for diagnosing
computer problems. The system is composed of a user interface  a rule-base  an
inference engine  and an expert interface. Additionally  the system features a
fuzzy-logic module to troubleshoot POST beep errors  and an intelligent agent
that assists in the knowledge acquisition process. The proposed system is meant
to automate the maintenance  repair  and operations (MRO) process  and free-up
human technicians from manually performing routine  laborious  and
timeconsuming maintenance tasks. As future work  the proposed system is to be
parallelized so as to boost its performance and speed-up its various
operations."
"Some aspects of the result of applying unit resolution on a CNF formula can
be formalized as functions with domain a set of partial truth assignments. We
are interested in two ways for computing such functions  depending on whether
the result is the production of the empty clause or the assignment of a
variable with a given truth value. We show that these two models can compute
the same functions with formulae of polynomially related sizes  and we explain
how this result is related to the CNF encoding of Boolean constraints."
"The development of expert system for treatment of Diabetes disease by using
natural methods is new information technology derived from Artificial
Intelligent research using ESTA (Expert System Text Animation) System. The
proposed expert system contains knowledge about various methods of natural
treatment methods (Massage  Herbal/Proper Nutrition  Acupuncture  Gems) for
Diabetes diseases of Human Beings. The system is developed in the ESTA (Expert
System shell for Text Animation) which is Visual Prolog 7.3 Application. The
knowledge for the said system will be acquired from domain experts  texts and
other related sources."
"In this report  we will be interested at Dynamic Bayesian Network (DBNs) as a
model that tries to incorporate temporal dimension with uncertainty. We start
with basics of DBN where we especially focus in Inference and Learning concepts
and algorithms. Then we will present different levels and methods of creating
DBNs as well as approaches of incorporating temporal dimension in static
Bayesian network."
"Through history  the human being tried to relay its daily tasks to other
creatures  which was the main reason behind the rise of civilizations. It
started with deploying animals to automate tasks in the field of
agriculture(bulls)  transportation (e.g. horses and donkeys)  and even
communication (pigeons). Millenniums after  come the Golden age with
""Al-jazari"" and other Muslim inventors  which were the pioneers of automation 
this has given birth to industrial revolution in Europe  centuries after. At
the end of the nineteenth century  a new era was to begin  the computational
era  the most advanced technological and scientific development that is driving
the mankind and the reason behind all the evolutions of science; such as
medicine  communication  education  and physics. At this edge of technology
engineers and scientists are trying to model a machine that behaves the same as
they do  which pushed us to think about designing and implementing ""Things
that-Thinks""  then artificial intelligence was. In this work we will cover each
of the major discoveries and studies in the field of machine cognition  which
are the ""Elementary Perceiver and Memorizer""(EPAM) and ""The General Problem
Solver""(GPS). The First one focus mainly on implementing the human-verbal
learning behavior  while the second one tries to model an architecture that is
able to solve problems generally (e.g. theorem proving  chess playing  and
arithmetic). We will cover the major goals and the main ideas of each model  as
well as comparing their strengths and weaknesses  and finally giving their
fields of applications. And Finally  we will suggest a real life implementation
of a cognitive machine."
"We present a system for recognising human activity given a symbolic
representation of video content. The input of our system is a set of
time-stamped short-term activities (STA) detected on video frames. The output
is a set of recognised long-term activities (LTA)  which are pre-defined
temporal combinations of STA. The constraints on the STA that  if satisfied 
lead to the recognition of a LTA  have been expressed using a dialect of the
Event Calculus. In order to handle the uncertainty that naturally occurs in
human activity recognition  we adapted this dialect to a state-of-the-art
probabilistic logic programming framework. We present a detailed evaluation and
comparison of the crisp and probabilistic approaches through experimentation on
a benchmark dataset of human surveillance videos."
"The article discusses some applications of fuzzy logic ideas to formalizing
of the Case-Based Reasoning (CBR) process and to measuring the effectiveness of
CBR systems"
"One of the big challenges in the development of probabilistic relational (or
probabilistic logical) modeling and learning frameworks is the design of
inference techniques that operate on the level of the abstract model
representation language  rather than on the level of ground  propositional
instances of the model. Numerous approaches for such ""lifted inference""
techniques have been proposed. While it has been demonstrated that these
techniques will lead to significantly more efficient inference on some specific
models  there are only very recent and still quite restricted results that show
the feasibility of lifted inference on certain syntactically defined classes of
models. Lower complexity bounds that imply some limitations for the feasibility
of lifted inference on more expressive model classes were established early on
in (Jaeger 2000). However  it is not immediate that these results also apply to
the type of modeling languages that currently receive the most attention  i.e. 
weighted  quantifier-free formulas. In this paper we extend these earlier
results  and show that under the assumption that NETIME =/= ETIME  there is no
polynomial lifted inference algorithm for knowledge bases of weighted 
quantifier- and function-free formulas. Further strengthening earlier results 
this is also shown to hold for approximate inference  and for knowledge bases
not containing the equality predicate."
"Statistical evidence of the influence of neighborhood topology on the
performance of particle swarm optimization (PSO) algorithms has been shown in
many works. However  little has been done about the implications could have the
percolation threshold in determining the topology of this neighborhood. This
work addresses this problem for individuals that  like robots  are able to
sense in a limited neighborhood around them. Based on the concept of
percolation threshold  and more precisely  the disk percolation model in 2D  we
show that better results are obtained for low values of radius  when
individuals occasionally ask others their best visited positions  with the
consequent decrease of computational complexity. On the other hand  since
percolation threshold is a universal measure  it could have a great interest to
compare the performance of different hybrid PSO algorithms."
"The solution of the biobjective IRP is rather challenging  even for
metaheuristics. We are still lacking a profound understanding of appropriate
solution representations and effective neighborhood structures. Clearly  both
the delivery volumes and the routing aspects of the alternatives need to be
reflected in an encoding  and must be modified when searching by means of local
search. Our work contributes to the better understanding of such solution
representations. On the basis of an experimental investigation  the advantages
and drawbacks of two encodings are studied and compared."
"Today  one's disposes of large datasets composed of thousands of geographic
objects. However  for many processes  which require the appraisal of an expert
or much computational time  only a small part of these objects can be taken
into account. In this context  robust sampling methods become necessary. In
this paper  we propose a sampling method based on clustering techniques. Our
method consists in dividing the objects in clusters  then in selecting in each
cluster  the most representative objects. A case-study in the context of a
process dedicated to knowledge revision for geographic data generalisation is
presented. This case-study shows that our method allows to select relevant
samples of objects."
"Both humans and artificial systems frequently use trial and error methods to
problem solving. In order to be effective  this type of strategy implies having
high quality control knowledge to guide the quest for the optimal solution.
Unfortunately  this control knowledge is rarely perfect. Moreover  in
artificial systems-as in humans-self-evaluation of one's own knowledge is often
difficult. Yet  this self-evaluation can be very useful to manage knowledge and
to determine when to revise it. The objective of our work is to propose an
automated approach to evaluate the quality of control knowledge in artificial
systems based on a specific trial and error strategy  namely the informed tree
search strategy. Our revision approach consists in analysing the system's
execution logs  and in using the belief theory to evaluate the global quality
of the knowledge. We present a real-world industrial application in the form of
an experiment using this approach in the domain of cartographic generalisation.
Thus far  the results of using our approach have been encouraging."
"In this paper we develop a fuzzy model for the description of the process of
Analogical Reasoning by representing its main steps as fuzzy subsets of a set
of linguistic labels characterizing the individuals' performance in each step
and we use the Shannon- Wiener diversity index as a measure of the individuals'
abilities in analogical problem solving. This model is compared with a
stochastic model presented in author's earlier papers by introducing a finite
Markov chain on the steps of the process of Analogical Reasoning. A classroom
experiment is also presented to illustrate the use of our results in practice."
"The causal structure of cognition can be simulated but not implemented
computationally  just as the causal structure of a comet can be simulated but
not implemented computationally. The only thing that allows us even to imagine
otherwise is that cognition  unlike a comet  is invisible (to all but the
cognizer)."
"Within the framework proposed in this paper  we address the issue of
extending the certain networks to a fuzzy certain networks in order to cope
with a vagueness and limitations of existing models for decision under
imprecise and uncertain knowledge. This paper proposes a framework that
combines two disciplines to exploit their own advantages in uncertain and
imprecise knowledge representation problems. The framework proposed is a
possibilistic logic based one in which Bayesian nodes and their properties are
represented by local necessity-valued knowledge base. Data in properties are
interpreted as set of valuated formulas. In our contribution possibilistic
Bayesian networks have a qualitative part and a quantitative part  represented
by local knowledge bases. The general idea is to study how a fusion of these
two formalisms would permit representing compact way to solve efficiently
problems for knowledge representation. We show how to apply possibility and
necessity measures to the problem of knowledge representation with large scale
data. On the other hand fuzzification of crisp certainty degrees to fuzzy
variables improves the quality of the network and tends to bring smoothness and
robustness in the network performance. The general aim is to provide a new
approach for decision under uncertainty that combines three methodologies.
Bayesian networks certainty distribution and fuzzy logic."
"The main objective of this paper is to develop a new semantic Network
structure  based on the fuzzy sets theory  used in Artificial Intelligent
system in order to provide effective on-line assistance to users of new
technological systems. This Semantic Networks is used to describe the knowledge
of an ""ideal"" expert while fuzzy sets are used both to describe the approximate
and uncertain knowledge of novice users who intervene to match fuzzy labels of
a query with categories from an ""ideal"" expert. The technical system we
consider is a word processor software  with Objects such as ""Word"" and Goals
such as ""Cut"" or ""Copy"". We suggest to consider the set of the system's Goals
as a set of linguistic variables to which corresponds a set of possible
linguistic values based on the fuzzy set. We consider  therefore  a set of
interpretation's levels for these possible values to which corresponds a set of
membership functions. We also propose a method to measure the similarity degree
between different fuzzy linguistic variables for the partition of the semantic
network in class of similar objects to make easy the diagnosis of the user's
fuzzy queries."
"Feature weighting is a technique used to approximate the optimal degree of
influence of individual features. This paper presents a feature weighting
method for Document Image Retrieval System (DIRS) based on keyword spotting. In
this method  we weight the feature using coefficient of multiple correlations.
Coefficient of multiple correlations can be used to describe the synthesized
effects and correlation of each feature. The aim of this paper is to show that
feature weighting increases the performance of DIRS. After applying the feature
weighting method to DIRS the average precision is 93.23% and average recall
become 98.66% respectively"
"In this paper  we are trying to examine trade offs between fuzzy logic and
certain Bayesian networks and we propose to combine their respective advantages
into fuzzy certain Bayesian networks (FCBN)  a certain Bayesian networks of
fuzzy random variables. This paper deals with different definitions and
classifications of uncertainty  sources of uncertainty  and theories and
methodologies presented to deal with uncertainty. Fuzzification of crisp
certainty degrees to fuzzy variables improves the quality of the network and
tends to bring smoothness and robustness in the network performance. The aim is
to provide a new approach for decision under uncertainty that combines three
methodologies. Bayesian networks certainty distribution and fuzzy logic. Within
the framework proposed in this paper  we address the issue of extending the
certain networks to a fuzzy certain networks in order to cope with a vagueness
and limitations of existing models for decision under imprecise and uncertain
knowledge."
"Holding commercial negotiations and selecting the best supplier in supply
chain management systems are among weaknesses of producers in production
process. Therefore  applying intelligent systems may have an effective role in
increased speed and improved quality in the selections .This paper introduces a
system which tries to trade using multi-agents systems and holding negotiations
between any agents. In this system  an intelligent agent is considered for each
segment of chains which it tries to send order and receive the response with
attendance in negotiation medium and communication with other agents .This
paper introduces how to communicate between agents  characteristics of
multi-agent and standard registration medium of each agent in the environment.
JADE (Java Application Development Environment) was used for implementation and
simulation of agents cooperation."
"The similarity between trajectory patterns in clustering has played an
important role in discovering movement behaviour of different groups of mobile
objects. Several approaches have been proposed to measure the similarity
between sequences in trajectory data. Most of these measures are based on
Euclidean space or on spatial network and some of them have been concerned with
temporal aspect or ordering types. However  they are not appropriate to
characteristics of spatiotemporal mobility patterns in wireless networks. In
this paper  we propose a new similarity measure for mobility patterns in
cellular space of wireless network. The framework for constructing our measure
is composed of two phases as follows. First  we present formal definitions to
capture mathematically two spatial and temporal similarity measures for
mobility patterns. And then  we define the total similarity measure by means of
a weighted combination of these similarities. The truth of the partial and
total similarity measures are proved in mathematics. Furthermore  instead of
the time interval or ordering  our work makes use of the timestamp at which two
mobility patterns share the same cell. A case study is also described to give a
comparison of the combination measure with other ones."
"Feature reduction is an important concept which is used for reducing
dimensions to decrease the computation complexity and time of classification.
Since now many approaches have been proposed for solving this problem  but
almost all of them just presented a fix output for each input dataset that some
of them aren't satisfied cases for classification. In this we proposed an
approach as processing input dataset to increase accuracy rate of each feature
extraction methods. First of all  a new concept called dispelling classes
gradually (DCG) is proposed to increase separability of classes based on their
labels. Next  this method is used to process input dataset of the feature
reduction approaches to decrease the misclassification error rate of their
outputs more than when output is achieved without any processing. In addition
our method has a good quality to collate with noise based on adapting dataset
with feature reduction approaches. In the result part  two conditions (With
process and without that) are compared to support our idea by using some of UCI
datasets."
"Software aging is a phenomenon that refers to progressive performance
degradation or transient failures or even crashes in long running software
systems such as web servers. It mainly occurs due to the deterioration of
operating system resource  fragmentation and numerical error accumulation. A
primitive method to fight against software aging is software rejuvenation.
Software rejuvenation is a proactive fault management technique aimed at
cleaning up the system internal state to prevent the occurrence of more severe
crash failures in the future. It involves occasionally stopping the running
software  cleaning its internal state and restarting it. An optimized schedule
for performing the software rejuvenation has to be derived in advance because a
long running application could not be put down now and then as it may lead to
waste of cost. This paper proposes a method to derive an accurate and optimized
schedule for rejuvenation of a web server (Apache) by using Radial Basis
Function (RBF) based Feed Forward Neural Network  a variant of Artificial
Neural Networks (ANN). Aging indicators are obtained through experimental setup
involving Apache web server and clients  which acts as input to the neural
network model. This method is better than existing ones because usage of RBF
leads to better accuracy and speed in convergence."
"A software agent may be a member of a Multi-Agent System (MAS) which is
collectively performing a range of complex and intelligent tasks. In the
hospital  scheduling decisions are finding difficult to schedule because of the
dynamic changes and distribution. In order to face this problem with dynamic
changes in the hospital  a new method  Distributed Optimized Patient Scheduling
with Grouping (DOPSG) has been proposed. The goal of this method is that there
is no necessity for knowing patient agents information globally. With minimal
information this method works effectively. Scheduling problem can be solved for
multiple departments in the hospital. Patient agents have been scheduled to the
resource agent based on the patient priority to reduce the waiting time of
patient agent and to reduce idle time of resources."
"The approach described here allows using membership function to represent
imprecise and uncertain knowledge by learning in Fuzzy Semantic Networks. This
representation has a great practical interest due to the possibility to realize
on the one hand  the construction of this membership function from a simple
value expressing the degree of interpretation of an Object or a Goal as
compared to an other and on the other hand  the adjustment of the membership
function during the apprenticeship. We show  how to use these membership
functions to represent the interpretation of an Object (respectively of a Goal)
user as compared to an system Object (respectively to a Goal). We also show the
possibility to make decision for each representation of an user Object compared
to a system Object. This decision is taken by determining decision coefficient
calculates according to the nucleus of the membership function of the user
Object."
"This paper presents a method of optimization  based on both Bayesian Analysis
technical and Gallois Lattice  of a Fuzzy Semantic Networks. The technical
System we use learn by interpreting an unknown word using the links created
between this new word and known words. The main link is provided by the context
of the query. When novice's query is confused with an unknown verb (goal)
applied to a known noun denoting either an object in the ideal user's Network
or an object in the user's Network  the system infer that this new verb
corresponds to one of the known goal. With the learning of new words in natural
language as the interpretation  which was produced in agreement with the user 
the system improves its representation scheme at each experiment with a new
user and  in addition  takes advantage of previous discussions with users. The
semantic Net of user objects thus obtained by these kinds of learning is not
always optimal because some relationships between couple of user objects can be
generalized and others suppressed according to values of forces that
characterize them. Indeed  to simplify the obtained Net  we propose to proceed
to an inductive Bayesian analysis  on the Net obtained from Gallois lattice.
The objective of this analysis can be seen as an operation of filtering of the
obtained descriptive graph."
"The approach described here allows to use the fuzzy Object Based
Representation of imprecise and uncertain knowledge. This representation has a
great practical interest due to the possibility to realize reasoning on
classification with a fuzzy semantic network based system. For instance  the
distinction between necessary  possible and user classes allows to take into
account exceptions that may appear on fuzzy knowledge-base and facilitates
integration of user's Objects in the base. This approach describes the
theoretical aspects of the architecture of the whole experimental A.I. system
we built in order to provide effective on-line assistance to users of new
technological systems. the understanding of ""how it works"" and ""how to complete
tasks"" from queries in quite natural languages. In our model  procedural
semantic networks are used to describe the knowledge of an ""ideal"" expert while
fuzzy sets are used both to describe the approximative and uncertain knowledge
of novice users in fuzzy semantic networks which intervene to match fuzzy
labels of a query with categories from our ""ideal"" expert."
"Answer Set Programming (ASP) is a well-established paradigm of declarative
programming in close relationship with other declarative formalisms such as SAT
Modulo Theories  Constraint Handling Rules  FO(.)  PDDL and many others. Since
its first informal editions  ASP systems have been compared in the now
well-established ASP Competition. The Third (Open) ASP Competition  as the
sequel to the ASP Competitions Series held at the University of Potsdam in
Germany (2006-2007) and at the University of Leuven in Belgium in 2009  took
place at the University of Calabria (Italy) in the first half of 2011.
Participants competed on a pre-selected collection of benchmark problems  taken
from a variety of domains as well as real world applications. The Competition
ran on two tracks. the Model and Solve (M&S) Track  based on an open problem
encoding  and open language  and open to any kind of system based on a
declarative specification paradigm; and the System Track  run on the basis of
fixed  public problem encodings  written in a standard ASP language. This paper
discusses the format of the Competition and the rationale behind it  then
reports the results for both tracks. Comparison with the second ASP competition
and state-of-the-art solutions for some of the benchmark domains is eventually
discussed.
  To appear in Theory and Practice of Logic Programming (TPLP)."
"The paper introduces AND/OR importance sampling for probabilistic graphical
models. In contrast to importance sampling  AND/OR importance sampling caches
samples in the AND/OR space and then extracts a new sample mean from the stored
samples. We prove that AND/OR importance sampling may have lower variance than
importance sampling; thereby providing a theoretical justification for
preferring it over importance sampling. Our empirical evaluation demonstrates
that AND/OR importance sampling is far more accurate than importance sampling
in many cases."
"In this paper  we consider planning in stochastic shortest path (SSP)
problems  a subclass of Markov Decision Problems (MDP). We focus on medium-size
problems whose state space can be fully enumerated. This problem has numerous
important applications  such as navigation and planning under uncertainty. We
propose a new approach for constructing a multi-level hierarchy of
progressively simpler abstractions of the original problem. Once computed  the
hierarchy can be used to speed up planning by first finding a policy for the
most abstract level and then recursively refining it into a solution to the
original problem. This approach is fully automated and delivers a speed-up of
two orders of magnitude over a state-of-the-art MDP solver on sample problems
while returning near-optimal solutions. We also prove theoretical bounds on the
loss of solution optimality resulting from the use of abstractions."
"The problem of learning discrete Bayesian networks from data is encoded as a
weighted MAX-SAT problem and the MaxWalkSat local search algorithm is used to
address it. For each dataset  the per-variable summands of the (BDeu) marginal
likelihood for different choices of parents ('family scores') are computed
prior to applying MaxWalkSat. Each permissible choice of parents for each
variable is encoded as a distinct propositional atom and the associated family
score encoded as a 'soft' weighted single-literal clause. Two approaches to
enforcing acyclicity are considered. either by encoding the ancestor relation
or by attaching a total order to each graph and encoding that. The latter
approach gives better results. Learning experiments have been conducted on 21
synthetic datasets sampled from 7 BNs. The largest dataset has 10 000
datapoints and 60 variables producing (for the 'ancestor' encoding) a weighted
CNF input file with 19 932 atoms and 269 367 clauses. For most datasets 
MaxWalkSat quickly finds BNs with higher BDeu score than the 'true' BN. The
effect of adding prior information is assessed. It is further shown that
Bayesian model averaging can be effected by collecting BNs generated during the
search."
"This paper describes a new algorithm to solve the decision making problem in
Influence Diagrams based on algorithms for credal networks. Decision nodes are
associated to imprecise probability distributions and a reformulation is
introduced that finds the global maximum strategy with respect to the expected
utility. We work with Limited Memory Influence Diagrams  which generalize most
Influence Diagram proposals and handle simultaneous decisions. Besides the
global optimum method  we explore an anytime approximate solution with a
guaranteed maximum error and show that imprecise probabilities are handled in a
straightforward way. Complexity issues and experiments with random diagrams and
an effects-based military planning problem are discussed."
"A graphical multiagent model (GMM) represents a joint distribution over the
behavior of a set of agents. One source of knowledge about agents' behavior may
come from gametheoretic analysis  as captured by several graphical game
representations developed in recent years. GMMs generalize this approach to
express arbitrary distributions  based on game descriptions or other sources of
knowledge bearing on beliefs about agent behavior. To illustrate the
flexibility of GMMs  we exhibit game-derived models that allow probabilistic
deviation from equilibrium  as well as models based on heuristic action choice.
We investigate three different methods of integrating these models into a
single model representing the combined knowledge sources. To evaluate the
predictive performance of the combined model  we treat as actual outcome the
behavior produced by a reinforcement learning process. We find that combining
the two knowledge sources  using any of the methods  provides better
predictions than either source alone. Among the combination methods  mixing
data outperforms the opinion pool and direct update methods investigated in
this empirical trial."
"We conjecture that the worst case number of experiments necessary and
sufficient to discover a causal graph uniquely given its observational Markov
equivalence class can be specified as a function of the largest clique in the
Markov equivalence class. We provide an algorithm that computes intervention
sets that we believe are optimal for the above task. The algorithm builds on
insights gained from the worst case analysis in Eberhardt et al. (2005) for
sequences of experiments when all possible directed acyclic graphs over N
variables are considered. A simulation suggests that our conjecture is correct.
We also show that a generalization of our conjecture to other classes of
possible graph hypotheses cannot be given easily  and in what sense the
algorithm is then no longer optimal."
"Bounded policy iteration is an approach to solving infinite-horizon POMDPs
that represents policies as stochastic finite-state controllers and iteratively
improves a controller by adjusting the parameters of each node using linear
programming. In the original algorithm  the size of the linear programs  and
thus the complexity of policy improvement  depends on the number of parameters
of each node  which grows with the size of the controller. But in practice  the
number of parameters of a node with non-zero values is often very small  and
does not grow with the size of the controller. Based on this observation  we
develop a version of bounded policy iteration that leverages the sparse
structure of a stochastic finite-state controller. In each iteration  it
improves a policy by the same amount as the original algorithm  but with much
better scalability."
"Approximate inference in dynamic systems is the problem of estimating the
state of the system given a sequence of actions and partial observations. High
precision estimation is fundamental in many applications like diagnosis 
natural language processing  tracking  planning  and robotics. In this paper we
present an algorithm that samples possible deterministic executions of a
probabilistic sequence. The algorithm takes advantage of a compact
representation (using first order logic) for actions and world states to
improve the precision of its estimation. Theoretical and empirical results show
that the algorithm's expected error is smaller than propositional sampling and
Sequential Monte Carlo (SMC) sampling techniques."
"While known algorithms for sensitivity analysis and parameter tuning in
probabilistic networks have a running time that is exponential in the size of
the network  the exact computational complexity of these problems has not been
established as yet. In this paper we study several variants of the tuning
problem and show that these problems are NPPP-complete in general. We further
show that the problems remain NP-complete or PP-complete  for a number of
restricted variants. These complexity results provide insight in whether or not
recent achievements in sensitivity analysis and tuning can be extended to more
general  practicable methods."
"Approximate linear programming (ALP) is an efficient approach to solving
large factored Markov decision processes (MDPs). The main idea of the method is
to approximate the optimal value function by a set of basis functions and
optimize their weights by linear programming (LP). This paper proposes a new
ALP approximation. Comparing to the standard ALP formulation  we decompose the
constraint space into a set of low-dimensional spaces. This structure allows
for solving the new LP efficiently. In particular  the constraints of the LP
can be satisfied in a compact form without an exponential dependence on the
treewidth of ALP constraints. We study both practical and theoretical aspects
of the proposed approach. Moreover  we demonstrate its scale-up potential on an
MDP with more than 2^100 states."
"Graphical models are usually learned without regard to the cost of doing
inference with them. As a result  even if a good model is learned  it may
perform poorly at prediction  because it requires approximate inference. We
propose an alternative. learning models with a score function that directly
penalizes the cost of inference. Specifically  we learn arithmetic circuits
with a penalty on the number of edges in the circuit (in which the cost of
inference is linear). Our algorithm is equivalent to learning a Bayesian
network with context-specific independence by greedily splitting conditional
distributions  at each step scoring the candidates by compiling the resulting
network into an arithmetic circuit  and using its size as the penalty. We show
how this can be done efficiently  without compiling a circuit from scratch for
each candidate. Experiments on several real-world domains show that our
algorithm is able to learn tractable models with very large treewidth  and
yields more accurate predictions than a standard context-specific Bayesian
network learner  in far less time."
"An efficient policy search algorithm should estimate the local gradient of
the objective function  with respect to the policy parameters  from as few
trials as possible. Whereas most policy search methods estimate this gradient
by observing the rewards obtained during policy trials  we show  both
theoretically and empirically  that taking into account the sensor data as well
gives better gradient estimates and hence faster learning. The reason is that
rewards obtained during policy execution vary from trial to trial due to noise
in the environment; sensor data  which correlates with the noise  can be used
to partially correct for this variation  resulting in an estimatorwith lower
variance."
"Bayesian networks can be used to extract explanations about the observed
state of a subset of variables. In this paper  we explicate the desiderata of
an explanation and confront them with the concept of explanation proposed by
existing methods. The necessity of taking into account causal approaches when a
causal graph is available is discussed. We then introduce causal explanation
trees  based on the construction of explanation trees using the measure of
causal information ow (Ay and Polani  2006). This approach is compared to
several other methods on known networks."
"Model-based Bayesian reinforcement learning has generated significant
interest in the AI community as it provides an elegant solution to the optimal
exploration-exploitation tradeoff in classical reinforcement learning.
Unfortunately  the applicability of this type of approach has been limited to
small domains due to the high complexity of reasoning about the joint posterior
over model parameters. In this paper  we consider the use of factored
representations combined with online planning techniques  to improve
scalability of these methods. The main contribution of this paper is a Bayesian
framework for learning the structure and parameters of a dynamical system 
while also simultaneously planning a (near-)optimal sequence of actions."
"In this work we present Cutting Plane Inference (CPI)  a Maximum A Posteriori
(MAP) inference method for Statistical Relational Learning. Framed in terms of
Markov Logic and inspired by the Cutting Plane Method  it can be seen as a meta
algorithm that instantiates small parts of a large and complex Markov Network
and then solves these using a conventional MAP method. We evaluate CPI on two
tasks  Semantic Role Labelling and Joint Entity Resolution  while plugging in
two different MAP inference methods. the current method of choice for MAP
inference in Markov Logic  MaxWalkSAT  and Integer Linear Programming. We
observe that when used with CPI both methods are significantly faster than when
used alone. In addition  CPI improves the accuracy of MaxWalkSAT and maintains
the exactness of Integer Linear Programming."
"Deciding what to sense is a crucial task  made harder by dependencies and by
a nonadditive utility function. We develop approximation algorithms for
selecting an optimal set of measurements  under a dependency structure modeled
by a tree-shaped Bayesian network (BN). Our approach is a generalization of
composing anytime algorithm represented by conditional performance profiles.
This is done by relaxing the input monotonicity assumption  and extending the
local compilation technique to more general classes of performance profiles
(PPs). We apply the extended scheme to selecting a subset of measurements for
choosing a maximum expectation variable in a binary valued BN  and for
minimizing the worst variance in a Gaussian BN."
"This paper develops a measure for bounding the performance of AND/OR search
algorithms for solving a variety of queries over graphical models. We show how
drawing a connection to the recent notion of hypertree decompositions allows to
exploit determinism in the problem specification and produce tighter bounds. We
demonstrate on a variety of practical problem instances that we are often able
to improve upon existing bounds by several orders of magnitude."
"We present and evaluate new techniques for designing algorithm portfolios. In
our view  the problem has both a scheduling aspect and a machine learning
aspect. Prior work has largely addressed one of the two aspects in isolation.
Building on recent work on the scheduling aspect of the problem  we present a
technique that addresses both aspects simultaneously and has attractive
theoretical guarantees. Experimentally  we show that this technique can be used
to improve the performance of state-of-the-art algorithms for Boolean
satisfiability  zero-one integer programming  and A.I. planning."
"Numerous temporal inference tasks such as fault monitoring and anomaly
detection exhibit a persistence property. for example  if something breaks  it
stays broken until an intervention. When modeled as a Dynamic Bayesian Network 
persistence adds dependencies between adjacent time slices  often making exact
inference over time intractable using standard inference algorithms. However 
we show that persistence implies a regular structure that can be exploited for
efficient inference. We present three successively more general classes of
models. persistent causal chains (PCCs)  persistent causal trees (PCTs) and
persistent polytrees (PPTs)  and the corresponding exact inference algorithms
that exploit persistence. We show that analytic asymptotic bounds for our
algorithms compare favorably to junction tree inference; and we demonstrate
empirically that we can perform exact smoothing on the order of 100 times
faster than the approximate Boyen-Koller method on randomly generated instances
of persistent tree models. We also show how to handle non-persistent variables
and how persistence can be exploited effectively for approximate filtering."
"Planning can often be simpli ed by decomposing the task into smaller tasks
arranged hierarchically. Charlin et al. [4] recently showed that the hierarchy
discovery problem can be framed as a non-convex optimization problem. However 
the inherent computational di culty of solving such an optimization problem
makes it hard to scale to realworld problems. In another line of research 
Toussaint et al. [18] developed a method to solve planning problems by
maximumlikelihood estimation. In this paper  we show how the hierarchy
discovery problem in partially observable domains can be tackled using a
similar maximum likelihood approach. Our technique rst transforms the problem
into a dynamic Bayesian network through which a hierarchical structure can
naturally be discovered while optimizing the policy. Experimental results
demonstrate that this approach scales better than previous techniques based on
non-convex optimization."
"We address the problem of identifying dynamic sequential plans in the
framework of causal Bayesian networks  and show that the problem is reduced to
identifying causal effects  for which there are complete identi cation
algorithms available in the literature."
"In this paper we introduce Refractor Importance Sampling (RIS)  an
improvement to reduce error variance in Bayesian network importance sampling
propagation under evidential reasoning. We prove the existence of a collection
of importance functions that are close to the optimal importance function under
evidential reasoning. Based on this theoretic result we derive the RIS
algorithm. RIS approaches the optimal importance function by applying localized
arc changes to minimize the divergence between the evidence-adjusted importance
function and the optimal importance function. The validity and performance of
RIS is empirically tested with a large setof synthetic Bayesian networks and
two real-world networks."
"The paper introduces a generalization for known probabilistic models such as
log-linear and graphical models  called here multiplicative models. These
models  that express probabilities via product of parameters are shown to
capture multiple forms of contextual independence between variables  including
decision graphs and noisy-OR functions. An inference algorithm for
multiplicative models is provided and its correctness is proved. The complexity
analysis of the inference algorithm uses a more refined parameter than the
tree-width of the underlying graph  and shows the computational cost does not
exceed that of the variable elimination algorithm in graphical models. The
paper ends with examples where using the new models and algorithm is
computationally beneficial."
"Online learning aims to perform nearly as well as the best hypothesis in
hindsight. For some hypothesis classes  though  even finding the best
hypothesis offline is challenging. In such offline cases  local search
techniques are often employed and only local optimality guaranteed. For online
decision-making with such hypothesis classes  we introduce local regret  a
generalization of regret that aims to perform nearly as well as only nearby
hypotheses. We then present a general algorithm to minimize local regret with
arbitrary locality graphs. We also show how the graph structure can be
exploited to drastically speed learning. These algorithms are then demonstrated
on a diverse set of online problems. online disjunct learning  online Max-SAT 
and online decision tree learning."
"The rules of d-separation provide a framework for deriving conditional
independence facts from model structure. However  this theory only applies to
simple directed graphical models. We introduce relational d-separation  a
theory for deriving conditional independence in relational models. We provide a
sound  complete  and computationally efficient method for relational
d-separation  and we present empirical results that demonstrate effectiveness."
"Decision circuits have been developed to perform efficient evaluation of
influence diagrams [Bhattacharjya and Shachter  2007]  building on the advances
in arithmetic circuits for belief network inference [Darwiche 2003]. In the
process of model building and analysis  we perform sensitivity analysis to
understand how the optimal solution changes in response to changes in the
model. When sequential decision problems under uncertainty are represented as
decision circuits  we can exploit the efficient solution process embodied in
the decision circuit and the wealth of derivative information available to
compute the value of information for the uncertainties in the problem and the
effects of changes to model parameters on the value and the optimal strategy."
"This is the Proceedings of the Twenty-Fifth Conference on Uncertainty in
Artificial Intelligence  which was held in Montreal  QC  Canada  June 18 - 21
2009."
"Computing the probability of evidence even with known error bounds is
NP-hard. In this paper we address this hard problem by settling on an easier
problem. We propose an approximation which provides high confidence lower
bounds on probability of evidence but does not have any guarantees in terms of
relative or absolute error. Our proposed approximation is a randomized
importance sampling scheme that uses the Markov inequality. However  a
straight-forward application of the Markov inequality may lead to poor lower
bounds. We therefore propose several heuristic measures to improve its
performance in practice. Empirical evaluation of our scheme with state-of-
the-art lower bounding schemes reveals the promise of our approach."
"Choquet expected utility (CEU) is one of the most sophisticated decision
criteria used in decision theory under uncertainty. It provides a
generalisation of expected utility enhancing both descriptive and prescriptive
possibilities. In this paper  we investigate the use of CEU for path-planning
under uncertainty with a special focus on robust solutions. We first recall the
main features of the CEU model and introduce some examples showing its
descriptive potential. Then we focus on the search for Choquet-optimal paths in
multivalued implicit graphs where costs depend on different scenarios. After
discussing complexity issues  we propose two different heuristic search
algorithms to solve the problem. Finally  numerical experiments are reported 
showing the practical efficiency of the proposed algorithms."
"The ways in which an agent's actions affect the world can often be modeled
compactly using a set of relational probabilistic planning rules. This paper
addresses the problem of learning such rule sets for multiple related tasks. We
take a hierarchical Bayesian approach  in which the system learns a prior
distribution over rule sets. We present a class of prior distributions
parameterized by a rule set prototype that is stochastically modified to
produce a task-specific rule set. We also describe a coordinate ascent
algorithm that iteratively optimizes the task-specific rule sets and the prior
distribution. Experiments using this algorithm show that transferring
information from related tasks significantly reduces the amount of training
data required to predict action effects in blocks-world domains."
"We formulate in this paper the mini-bucket algorithm for approximate
inference in terms of exact inference on an approximate model produced by
splitting nodes in a Bayesian network. The new formulation leads to a number of
theoretical and practical implications. First  we show that branchand- bound
search algorithms that use minibucket bounds may operate in a drastically
reduced search space. Second  we show that the proposed formulation inspires
new minibucket heuristics and allows us to analyze existing heuristics from a
new perspective. Finally  we show that this new formulation allows mini-bucket
approximations to benefit from recent advances in exact inference  allowing one
to significantly increase the reach of these approximations."
"We describe the semantic foundations for elicitation of generalized
additively independent (GAI) utilities using the minimax regret criterion  and
propose several new query types and strategies for this purpose. Computational
feasibility is obtained by exploiting the local GAI structure in the model. Our
results provide a practical approach for implementing preference-based
constrained configuration optimization as well as effective search in
multiattribute product databases."
"Although a number of related algorithms have been developed to evaluate
influence diagrams  exploiting the conditional independence in the diagram  the
exact solution has remained intractable for many important problems. In this
paper we introduce decision circuits as a means to exploit the local structure
usually found in decision problems and to improve the performance of influence
diagram analysis. This work builds on the probabilistic inference algorithms
using arithmetic circuits to represent Bayesian belief networks [Darwiche 
2003]. Once compiled  these arithmetic circuits efficiently evaluate
probabilistic queries on the belief network  and methods have been developed to
exploit both the global and local structure of the network. We show that
decision circuits can be constructed in a similar fashion and promise similar
benefits."
"We present a memory-bounded optimization approach for solving
infinite-horizon decentralized POMDPs. Policies for each agent are represented
by stochastic finite state controllers. We formulate the problem of optimizing
these policies as a nonlinear program  leveraging powerful existing nonlinear
optimization techniques for solving the problem. While existing solvers only
guarantee locally optimal solutions  we show that our formulation produces
higher quality controllers than the state-of-the-art approach. We also
incorporate a shared source of randomness in the form of a correlation device
to further increase solution quality with only a limited increase in space and
time. Our experimental results show that nonlinear optimization can be used to
provide high quality  concise solutions to decentralized decision problems
under uncertainty."
"Most real-world dynamic systems are composed of different components that
often evolve at very different rates. In traditional temporal graphical models 
such as dynamic Bayesian networks  time is modeled at a fixed granularity 
generally selected based on the rate at which the fastest component evolves.
Inference must then be performed at this fastest granularity  potentially at
significant computational cost. Continuous Time Bayesian Networks (CTBNs) avoid
time-slicing in the representation by modeling the system as evolving
continuously over time. The expectation-propagation (EP) inference algorithm of
Nodelman et al. (2005) can then vary the inference granularity over time  but
the granularity is uniform across all parts of the system  and must be selected
in advance. In this paper  we provide a new EP algorithm that utilizes a
general cluster graph architecture where clusters contain distributions that
can overlap in both space (set of variables) and time. This architecture allows
different parts of the system to be modeled at very different time
granularities  according to their current rate of evolution. We also provide an
information-theoretic criterion for dynamically re-partitioning the clusters
during inference to tune the level of approximation to the current rate of
evolution. This avoids the need to hand-select the appropriate granularity  and
allows the granularity to adapt as information is transmitted across the
network. We present experiments demonstrating that this approach can result in
significant computational savings."
"Compiling graphical models has recently been under intense investigation 
especially for probabilistic modeling and processing. We present here a novel
data structure for compiling weighted graphical models (in particular 
probabilistic models)  called AND/OR Multi-Valued Decision Diagram (AOMDD).
This is a generalization of our previous work on constraint networks  to
weighted models. The AOMDD is based on the frameworks of AND/OR search spaces
for graphical models  and Ordered Binary Decision Diagrams (OBDD). The AOMDD is
a canonical representation of a graphical model  and its size and compilation
time are bounded exponentially by the treewidth of the graph  rather than
pathwidth as is known for OBDDs. We discuss a Variable Elimination schedule for
compilation  and present the general APPLY algorithm that combines two weighted
AOMDDs  and also present a search based method for compilation method. The
preliminary experimental evaluation is quite encouraging  showing the potential
of the AOMDD data structure."
"The paper evaluates the power of best-first search over AND/OR search spaces
for solving the Most Probable Explanation (MPE) task in Bayesian networks. The
main virtue of the AND/OR representation of the search space is its sensitivity
to the structure of the problem  which can translate into significant time
savings. In recent years depth-first AND/OR Branch-and- Bound algorithms were
shown to be very effective when exploring such search spaces  especially when
using caching. Since best-first strategies are known to be superior to
depth-first when memory is utilized  exploring the best-first control strategy
is called for. The main contribution of this paper is in showing that a recent
extension of AND/OR search algorithms from depth-first Branch-and-Bound to
best-first is indeed very effective for computing the MPE in Bayesian networks.
We demonstrate empirically the superiority of the best-first search approach on
various probabilistic networks."
"Searching the complete space of possible Bayesian networks is intractable for
problems of interesting size  so Bayesian network structure learning
algorithms  such as the commonly used Sparse Candidate algorithm  employ
heuristics. However  these heuristics also restrict the types of relationships
that can be learned exclusively from data. They are unable to learn
relationships that exhibit ""correlation-immunity""  such as parity. To learn
Bayesian networks in the presence of correlation-immune relationships  we
extend the Sparse Candidate algorithm with a technique called ""skewing"". This
technique uses the observation that relationships that are correlation-immune
under a specific input distribution may not be correlation-immune under
another  sufficiently different distribution. We show that by extending Sparse
Candidate with this technique we are able to discover relationships between
random variables that are approximately correlation-immune  with a
significantly lower computational cost than the alternative of considering
multiple parents of a node at a time."
"Survey propagation (SP) is an exciting new technique that has been remarkably
successful at solving very large hard combinatorial problems  such as
determining the satisfiability of Boolean formulas. In a promising attempt at
understanding the success of SP  it was recently shown that SP can be viewed as
a form of belief propagation  computing marginal probabilities over certain
objects called covers of a formula. This explanation was  however  shortly
dismissed by experiments suggesting that non-trivial covers simply do not exist
for large formulas. In this paper  we show that these experiments were
misleading. not only do covers exist for large hard random formulas  SP is
surprisingly accurate at computing marginals over these covers despite the
existence of many cycles in the formulas. This re-opens a potentially simpler
line of reasoning for understanding SP  in contrast to some alternative lines
of explanation that have been proposed assuming covers do not exist."
"Relational Markov Random Fields are a general and flexible framework for
reasoning about the joint distribution over attributes of a large number of
interacting entities. The main computational difficulty in learning such models
is inference. Even when dealing with complete data  where one can summarize a
large domain by sufficient statistics  learning requires one to compute the
expectation of the sufficient statistics given different parameter choices. The
typical solution to this problem is to resort to approximate inference
procedures  such as loopy belief propagation. Although these procedures are
quite efficient  they still require computation that is on the order of the
number of interactions (or features) in the model. When learning a large
relational model over a complex domain  even such approximations require
unrealistic running time. In this paper we show that for a particular class of
relational MRFs  which have inherent symmetry  we can perform the inference
needed for learning procedures using a template-level belief propagation. This
procedure's running time is proportional to the size of the relational model
rather than the size of the domain. Moreover  we show that this computational
procedure is equivalent to sychronous loopy belief propagation. This enables a
dramatic speedup in inference and learning time. We use this procedure to learn
relational MRFs for capturing the joint distribution of large protein-protein
interaction networks."
"Preferences play an important role in our everyday lives. CP-networks  or
CP-nets in short  are graphical models for representing conditional qualitative
preferences under ceteris paribus (""all else being equal"") assumptions. Despite
their intuitive nature and rich representation  dominance testing with CP-nets
is computationally complex  even when the CP-nets are restricted to
binary-valued preferences. Tractable algorithms exist for binary CP-nets  but
these algorithms are incomplete for multi-valued CPnets. In this paper  we
identify a class of multivalued CP-nets  which we call more-or-less CPnets 
that have the same computational complexity as binary CP-nets. More-or-less
CP-nets exploit the monotonicity of the attribute values and use intervals to
aggregate values that induce similar preferences. We then present a search
control rule for dominance testing that effectively prunes the search space
while preserving completeness."
"Relational Markov Decision Processes are a useful abstraction for complex
reinforcement learning problems and stochastic planning problems. Recent work
developed representation schemes and algorithms for planning in such problems
using the value iteration algorithm. However  exact versions of more complex
algorithms  including policy iteration  have not been developed or analyzed.
The paper investigates this potential and makes several contributions. First we
observe two anomalies for relational representations showing that the value of
some policies is not well defined or cannot be calculated for restricted
representation schemes used in the literature. On the other hand  we develop a
variant of policy iteration that can get around these anomalies. The algorithm
includes an aspect of policy improvement in the process of policy evaluation
and thus differs from the original algorithm. We show that despite this
difference the algorithm converges to the optimal policy."
"Combining first-order logic and probability has long been a goal of AI.
Markov logic (Richardson & Domingos  2006) accomplishes this by attaching
weights to first-order formulas and viewing them as templates for features of
Markov networks. Unfortunately  it does not have the full power of first-order
logic  because it is only defined for finite domains. This paper extends Markov
logic to infinite domains  by casting it in the framework of Gibbs measures
(Georgii  1988). We show that a Markov logic network (MLN) admits a Gibbs
measure as long as each ground atom has a finite number of neighbors. Many
interesting cases fall in this category. We also show that an MLN admits a
unique measure if the weights of its non-unit clauses are small enough. We then
examine the structure of the set of consistent measures in the non-unique case.
Many important phenomena  including systems with phase transitions  are
represented by MLNs with non-unique measures. We relate the problem of
satisfiability in first-order logic to the properties of MLN measures  and
discuss how Markov logic relates to previous infinite models."
"Counterfactual statements  e.g.  ""my headache would be gone had I taken an
aspirin"" are central to scientific discourse  and are formally interpreted as
statements derived from ""alternative worlds"". However  since they invoke
hypothetical states of affairs  often incompatible with what is actually known
or observed  testing counterfactuals is fraught with conceptual and practical
difficulties. In this paper  we provide a complete characterization of
""testable counterfactuals "" namely  counterfactual statements whose
probabilities can be inferred from physical experiments. We provide complete
procedures for discerning whether a given counterfactual is testable and  if
so  expressing its probability in terms of experimental data."
"Memory-Bounded Dynamic Programming (MBDP) has proved extremely effective in
solving decentralized POMDPs with large horizons. We generalize the algorithm
and improve its scalability by reducing the complexity with respect to the
number of observations from exponential to polynomial. We derive error bounds
on solution quality with respect to this new approximation and analyze the
convergence behavior. To evaluate the effectiveness of the improvements  we
introduce a new  larger benchmark problem. Experimental results show that
despite the high complexity of decentralized POMDPs  scalable solution
techniques such as MBDP perform surprisingly well."
"Assistive systems for persons with cognitive disabilities (e.g. dementia) are
difficult to build due to the wide range of different approaches people can
take to accomplishing the same task  and the significant uncertainties that
arise from both the unpredictability of client's behaviours and from noise in
sensor readings. Partially observable Markov decision process (POMDP) models
have been used successfully as the reasoning engine behind such assistive
systems for small multi-step tasks such as hand washing. POMDP models are a
powerful  yet flexible framework for modelling assistance that can deal with
uncertainty and utility. Unfortunately  POMDPs usually require a very labour
intensive  manual procedure for their definition and construction. Our previous
work has described a knowledge driven method for automatically generating POMDP
activity recognition and context sensitive prompting systems for complex tasks.
We call the resulting POMDP a SNAP (SyNdetic Assistance Process). The
spreadsheet-like result of the analysis does not correspond to the POMDP model
directly and the translation to a formal POMDP representation is required. To
date  this translation had to be performed manually by a trained POMDP expert.
In this paper  we formalise and automate this translation process using a
probabilistic relational model (PRM) encoded in a relational database. We
demonstrate the method by eliciting three assistance tasks from non-experts. We
validate the resulting POMDP models using case-based simulations to show that
they are reasonable for the domains. We also show a complete case study of a
designer specifying one database  including an evaluation in a real-life
experiment with a human actor."
"There are several contexts of non-monotonic reasoning where a priority
between rules is established whose purpose is preventing conflicts.
  One formalism that has been widely employed for non-monotonic reasoning is
the sceptical one known as Defeasible Logic. In Defeasible Logic the tool used
for conflict resolution is a preference relation between rules  that
establishes the priority among them.
  In this paper we investigate how to modify such a preference relation in a
defeasible logic theory in order to change the conclusions of the theory
itself. We argue that the approach we adopt is applicable to legal reasoning
where users  in general  cannot change facts or rules  but can propose their
preferences about the relative strength of the rules.
  We provide a comprehensive study of the possible combinatorial cases and we
identify and analyse the cases where the revision process is successful.
  After this analysis  we identify three revision/update operators and study
them against the AGM postulates for belief revision operators  to discover that
only a part of these postulates are satisfied by the three operators."
"We apply decision theoretic techniques to construct non-player characters
that are able to assist a human player in collaborative games. The method is
based on solving Markov decision processes  which can be difficult when the
game state is described by many variables. To scale to more complex games  the
method allows decomposition of a game task into subtasks  each of which can be
modelled by a Markov decision process. Intention recognition is used to infer
the subtask that the human is currently performing  allowing the helper to
assist the human in performing the correct task. Experiments show that the
method can be effective  giving near-human level performance in helping a human
in a collaborative game."
"We consider the problem of using a heuristic policy to improve the value
approximation by the Upper Confidence Bound applied in Trees (UCT) algorithm in
non-adversarial settings such as planning with large-state space Markov
Decision Processes. Current improvements to UCT focus on either changing the
action selection formula at the internal nodes or the rollout policy at the
leaf nodes of the search tree. In this work  we propose to add an auxiliary arm
to each of the internal nodes  and always use the heuristic policy to roll out
simulations at the auxiliary arms. The method aims to get fast convergence to
optimal values at states where the heuristic policy is optimal  while retaining
similar approximation as the original UCT in other states. We show that
bootstrapping with the proposed method in the new algorithm  UCT-Aux  performs
better compared to the original UCT algorithm and its variants in two benchmark
experiment settings. We also examine conditions under which UCT-Aux works well."
"We consider in this paper the formulation of approximate inference in
Bayesian networks as a problem of exact inference on an approximate network
that results from deleting edges (to reduce treewidth). We have shown in
earlier work that deleting edges calls for introducing auxiliary network
parameters to compensate for lost dependencies  and proposed intuitive
conditions for determining these parameters. We have also shown that our method
corresponds to IBP when enough edges are deleted to yield a polytree  and
corresponds to some generalizations of IBP when fewer edges are deleted. In
this paper  we propose a different criteria for determining auxiliary
parameters based on optimizing the KL-divergence between the original and
approximate networks. We discuss the relationship between the two methods for
selecting parameters  shedding new light on IBP and its generalizations. We
also discuss the application of our new method to approximating inference
problems which are exponential in constrained treewidth  including MAP and
nonmyopic value of information."
"In Bayesian networks  a Most Probable Explanation (MPE) is a complete
variable instantiation with a highest probability given the current evidence.
In this paper  we discuss the problem of finding robustness conditions of the
MPE under single parameter changes. Specifically  we ask the question. How much
change in a single network parameter can we afford to apply while keeping the
MPE unchanged? We will describe a procedure  which is the first of its kind 
that computes this answer for each parameter in the Bayesian network variable
in time O(n exp(w))  where n is the number of network variables and w is its
treewidth."
"The paper analyzes theoretically and empirically the performance of
likelihood weighting (LW) on a subset of nodes in Bayesian networks. The
proposed scheme requires fewer samples to converge due to reduction in sampling
variance. The method exploits the structure of the network to bound the
complexity of exact inference used to compute sampling distributions  similar
to Gibbs cutset sampling. Yet  the extension of the previosly proposed cutset
sampling principles to likelihood weighting is non-trivial due to differences
in the sampling processes of Gibbs sampler and LW. We demonstrate empirically
that likelihood weighting on a cutset (LWLC) is effective time-wise and has a
lower rejection rate than LW when applied to networks with many deterministic
probabilities. Finally  we show that the performance of likelihood weighting on
a cutset can be improved further by caching computed sampling distributions
and  consequently  learning 'zeros' of the target distribution."
"Linear-time computational techniques have been developed for combining
evidence which is available on a number of contending hypotheses. They offer a
means of making the computation-intensive calculations involved more efficient
in certain circumstances. Unfortunately  they restrict the orthogonal sum of
evidential functions to the dichotomous structure applies only to elements and
their complements. In this paper  we present a novel evidence structure in
terms of a triplet and a set of algorithms for evidential reasoning. The merit
of this structure is that it divides a set of evidence into three subsets 
distinguishing trivial evidential elements from important ones focusing some
particular elements. It avoids the deficits of the dichotomous structure in
representing the preference of evidence and estimating the basic probability
assignment of evidence. We have established a formalism for this structure and
the general formulae for combining pieces of evidence in the form of the
triplet  which have been theoretically justified."
"Separable Bayesian Networks  or the Influence Model  are dynamic Bayesian
Networks in which the conditional probability distribution can be separated
into a function of only the marginal distribution of a node's neighbors 
instead of the joint distributions. In terms of modeling  separable networks
has rendered possible siginificant reduction in complexity  as the state space
is only linear in the number of variables on the network  in contrast to a
typical state space which is exponential. In this work  We describe the
connection between an arbitrary Conditional Probability Table (CPT) and
separable systems using linear algebra. We give an alternate proof on the
equivalence of sufficiency and separability. We present a computational method
for testing whether a given CPT is separable."
"This paper is concerned with graphical criteria that can be used to solve the
problem of identifying casual effects from nonexperimental data in a causal
Bayesian network structure  i.e.  a directed acyclic graph that represents
causal relationships. We first review Pearl's work on this topic [Pearl  1995] 
in which several useful graphical criteria are presented. Then we present a
complete algorithm [Huang and Valtorta  2006b] for the identifiability problem.
By exploiting the completeness of this algorithm  we prove that the three basic
do-calculus rules that Pearl presents are complete  in the sense that  if a
causal effect is identifiable  there exists a sequence of applications of the
rules of the do-calculus that transforms the causal effect formula into a
formula that only includes observational quantities."
"This paper studies a new and more general axiomatization than one presented
previously for preference on likelihood gambles. Likelihood gambles describe
actions in a situation where a decision maker knows multiple probabilistic
models and a random sample generated from one of those models but does not know
prior probability of models. This new axiom system is inspired by Jensen's
axiomatization of probabilistic gambles. Our approach provides a new
perspective to the role of data in decision making under ambiguity. It avoids
one of the most controversial issue of Bayesian methodology namely the
assumption of prior probability."
"Continuous-time Bayesian networks (CTBNs) are graphical representations of
multi-component continuous-time Markov processes as directed graphs. The edges
in the network represent direct influences among components. The joint rate
matrix of the multi-component process is specified by means of conditional rate
matrices for each component separately. This paper addresses the situation
where some of the components evolve on a time scale that is much shorter
compared to the time scale of the other components. In this paper  we prove
that in the limit where the separation of scales is infinite  the Markov
process converges (in distribution  or weakly) to a reduced  or effective
Markov process that only involves the slow components. We also demonstrate that
for reasonable separation of scale (an order of magnitude) the reduced process
is a good approximation of the marginal process over the slow components. We
provide a simple procedure for building a reduced CTBN for this effective
process  with conditional rate matrices that can be directly calculated from
the original CTBN  and discuss the implications for approximate reasoning in
large systems."
"A popular approach to solving large probabilistic systems relies on
aggregating states based on a measure of similarity. Many approaches in the
literature are heuristic. A number of recent methods rely instead on metrics
based on the notion of bisimulation  or behavioral equivalence between states
(Givan et al  2001  2003; Ferns et al  2004). An integral component of such
metrics is the Kantorovich metric between probability distributions. However 
while this metric enables many satisfying theoretical properties  it is costly
to compute in practice. In this paper  we use techniques from network
optimization and statistical sampling to overcome this problem. We obtain in
this manner a variety of distance functions for MDP state aggregation  which
differ in the tradeoff between time and space complexity  as well as the
quality of the aggregation. We provide an empirical evaluation of these
trade-offs."
"Inference for probabilistic graphical models is still very much a practical
challenge in large domains. The commonly used and effective belief propagation
(BP) algorithm and its generalizations often do not converge when applied to
hard  real-life inference tasks. While it is widely recognized that the
scheduling of messages in these algorithms may have significant consequences 
this issue remains largely unexplored. In this work  we address the question of
how to schedule messages for asynchronous propagation so that a fixed point is
reached faster and more often. We first show that any reasonable asynchronous
BP converges to a unique fixed point under conditions similar to those that
guarantee convergence of synchronous BP. In addition  we show that the
convergence rate of a simple round-robin schedule is at least as good as that
of synchronous propagation. We then propose residual belief propagation (RBP) 
a novel  easy-to-implement  asynchronous propagation algorithm that schedules
messages in an informed way  that pushes down a bound on the distance from the
fixed point. Finally  we demonstrate the superiority of RBP over
state-of-the-art methods for a variety of challenging synthetic and real-life
problems. RBP converges significantly more often than other methods; and it
significantly reduces running time until convergence  even when other methods
converge."
"Directed possibly cyclic graphs have been proposed by Didelez (2000) and
Nodelmann et al. (2002) in order to represent the dynamic dependencies among
stochastic processes. These dependencies are based on a generalization of
Granger-causality to continuous time  first developed by Schweder (1970) for
Markov processes  who called them local dependencies. They deserve special
attention as they are asymmetric unlike stochastic (in)dependence. In this
paper we focus on their graphical representation and develop a suitable  i.e.
asymmetric notion of separation  called delta-separation. The properties of
this graph separation as well as of local independence are investigated in
detail within a framework of asymmetric (semi)graphoids allowing a deeper
insight into what information can be read off these graphs."
"There exist several architectures to solve influence diagrams using local
computations  such as the Shenoy-Shafer  the HUGIN  or the Lazy Propagation
architectures. They all extend usual variable elimination algorithms thanks to
the use of so-called 'potentials'. In this paper  we introduce a new
architecture  called the Multi-operator Cluster DAG architecture  which can
produce decompositions with an improved constrained induced-width  and
therefore induce potentially exponential gains. Its principle is to benefit
from the composite nature of influence diagrams  instead of using uniform
potentials  in order to better analyze the problem structure."
"Tasks such as record linkage and multi-target tracking  which involve
reconstructing the set of objects that underlie some observed data  are
particularly challenging for probabilistic inference. Recent work has achieved
efficient and accurate inference on such problems using Markov chain Monte
Carlo (MCMC) techniques with customized proposal distributions. Currently 
implementing such a system requires coding MCMC state representations and
acceptance probability calculations that are specific to a particular
application. An alternative approach  which we pursue in this paper  is to use
a general-purpose probabilistic modeling language (such as BLOG) and a generic
Metropolis-Hastings MCMC algorithm that supports user-supplied proposal
distributions. Our algorithm gains flexibility by using MCMC states that are
only partial descriptions of possible worlds; we provide conditions under which
MCMC over partial worlds yields correct answers to queries. We also show how to
use a context-specific Bayes net to identify the factors in the acceptance
probability that need to be computed for a given proposed move. Experimental
results on a citation matching task show that our general-purpose MCMC engine
compares favorably with an application-specific system."
"In recent years Bayesian networks (BNs) with a mixture of continuous and
discrete variables have received an increasing level of attention. We present
an architecture for exact belief update in Conditional Linear Gaussian BNs (CLG
BNs). The architecture is an extension of lazy propagation using operations of
Lauritzen & Jensen [6] and Cowell [2]. By decomposing clique and separator
potentials into sets of factors  the proposed architecture takes advantage of
independence and irrelevance properties induced by the structure of the graph
and the evidence. The resulting benefits are illustrated by examples. Results
of a preliminary empirical performance evaluation indicate a significant
potential of the proposed architecture."
"We set up a model for reasoning about metric spaces with belief theoretic
measures. The uncertainty in these spaces stems from both probability and
metric. To represent both aspect of uncertainty  we choose an expected distance
function as a measure of uncertainty. A formal logical system is constructed
for the reasoning about expected distance. Soundness and completeness are shown
for this logic. For reasoning on product metric space with uncertainty  a new
metric is defined and shown to have good properties."
"The National Airspace System (NAS) is a large and complex system with
thousands of interrelated components. administration  control centers 
airports  airlines  aircraft  passengers  etc. The complexity of the NAS
creates many difficulties in management and control. One of the most pressing
problems is flight delay. Delay creates high cost to airlines  complaints from
passengers  and difficulties for airport operations. As demand on the system
increases  the delay problem becomes more and more prominent. For this reason 
it is essential for the Federal Aviation Administration to understand the
causes of delay and to find ways to reduce delay. Major contributing factors to
delay are congestion at the origin airport  weather  increasing demand  and air
traffic management (ATM) decisions such as the Ground Delay Programs (GDP).
Delay is an inherently stochastic phenomenon. Even if all known causal factors
could be accounted for  macro-level national airspace system (NAS) delays could
not be predicted with certainty from micro-level aircraft information. This
paper presents a stochastic model that uses Bayesian Networks (BNs) to model
the relationships among different components of aircraft delay and the causal
factors that affect delays. A case study on delays of departure flights from
Chicago O'Hare international airport (ORD) to Hartsfield-Jackson Atlanta
International Airport (ATL) reveals how local and system level environmental
and human-caused factors combine to affect components of delay  and how these
components contribute to the final arrival delay at the destination airport."
"We introduce a new dynamic model with the capability of recognizing both
activities that an individual is performing as well as where that ndividual is
located. Our model is novel in that it utilizes a dynamic graphical model to
jointly estimate both activity and spatial context over time based on the
simultaneous use of asynchronous observations consisting of GPS measurements 
and measurements from a small mountable sensor board. Joint inference is quite
desirable as it has the ability to improve accuracy of the model. A key goal 
however  in designing our overall system is to be able to perform accurate
inference decisions while minimizing the amount of hardware an individual must
wear. This minimization leads to greater comfort and flexibility  decreased
power requirements and therefore increased battery life  and reduced cost. We
show results indicating that our joint measurement model outperforms
measurements from either the sensor board or GPS alone  using two types of
probabilistic inference procedures  namely particle filtering and pruned exact
inference."
"We study the problem of learning the best Bayesian network structure with
respect to a decomposable score such as BDe  BIC or AIC. This problem is known
to be NP-hard  which means that solving it becomes quickly infeasible as the
number of variables increases. Nevertheless  in this paper we show that it is
possible to learn the best Bayesian network structure with over 30 variables 
which covers many practically interesting cases. Our algorithm is less
complicated and more efficient than the techniques presented earlier. It can be
easily parallelized  and offers a possibility for efficient exploration of the
best networks consistent with different variable orderings. In the experimental
part of the paper we compare the performance of the algorithm to the previous
state-of-the-art algorithm. Free source-code and an online-demo can be found at
http.//b-course.hiit.fi/bene."
"Recent work on approximate linear programming (ALP) techniques for
first-order Markov Decision Processes (FOMDPs) represents the value function
linearly w.r.t. a set of first-order basis functions and uses linear
programming techniques to determine suitable weights. This approach offers the
advantage that it does not require simplification of the first-order value
function  and allows one to solve FOMDPs independent of a specific domain
instantiation. In this paper  we address several questions to enhance the
applicability of this work. (1) Can we extend the first-order ALP framework to
approximate policy iteration to address performance deficiencies of previous
approaches? (2) Can we automatically generate basis functions and evaluate
their impact on value function quality? (3) How can we decompose intractable
problems with universally quantified rewards into tractable subproblems? We
propose answers to these questions along with a number of novel optimizations
and provide a comparative empirical evaluation on logistics problems from the
ICAPS 2004 Probabilistic Planning Competition."
"In this paper we promote introducing software verification and control flow
graph similarity measurement in automated evaluation of students' programs. We
present a new grading framework that merges results obtained by combination of
these two approaches with results obtained by automated testing  leading to
improved quality and precision of automated grading. These two approaches are
also useful in providing a comprehensible feedback that can help students to
improve the quality of their programs We also present our corresponding tools
that are publicly available and open source. The tools are based on LLVM
low-level intermediate code representation  so they could be applied to a
number of programming languages. Experimental evaluation of the proposed
grading framework is performed on a corpus of university students' programs
written in programming language C. Results of the experiments show that
automatically generated grades are highly correlated with manually determined
grades suggesting that the presented tools can find real-world applications in
studying and grading."
"Gas Transmission Networks are large-scale complex systems  and corresponding
design and control problems are challenging. In this paper  we consider the
problem of control and management of these systems in crisis situations. We
present these networks by a hybrid systems framework that provides required
analysis models. Further  we discuss decision-making using computational
discrete and hybrid optimization methods. In particular  several reinforcement
learning methods are employed to explore decision space and achieve the best
policy in a specific crisis situation. Simulations are presented to illustrate
the efficiency of the method."
"Fuzzy rule based classification systems are one of the most popular fuzzy
modeling systems used in pattern classification problems. This paper
investigates the effect of applying nine different T-norms in fuzzy rule based
classification systems. In the recent researches  fuzzy versions of confidence
and support merits from the field of data mining have been widely used for both
rules selecting and weighting in the construction of fuzzy rule based
classification systems. For calculating these merits the product has been
usually used as a T-norm. In this paper different T-norms have been used for
calculating the confidence and support measures. Therefore  the calculations in
rule selection and rule weighting steps (in the process of constructing the
fuzzy rule based classification systems) are modified by employing these
T-norms. Consequently  these changes in calculation results in altering the
overall accuracy of rule based classification systems. Experimental results
obtained on some well-known data sets show that the best performance is
produced by employing the Aczel-Alsina operator in terms of the classification
accuracy  the second best operator is Dubois-Prade and the third best operator
is Dombi. In experiments  we have used 12 data sets with numerical attributes
from the University of California  Irvine machine learning repository (UCI)."
"This paper presents a novel fuzzy logic based Adaptive Super-twisting Sliding
Mode Controller for the control of dynamic uncertain systems. The proposed
controller combines the advantages of Second order Sliding Mode Control  Fuzzy
Logic Control and Adaptive Control. The reaching conditions  stability and
robustness of the system with the proposed controller are guaranteed. In
addition  the proposed controller is well suited for simple design and
implementation. The effectiveness of the proposed controller over the first
order Sliding Mode Fuzzy Logic controller is illustrated by Matlab based
simulations performed on a DC-DC Buck converter. Based on this comparison  the
proposed controller is shown to obtain the desired transient response without
causing chattering and error under steady-state conditions. The proposed
controller is able to give robust performance in terms of rejection to input
voltage variations and load variations."
"This paper deals with the implementation of Least Mean Square (LMS) algorithm
in Decision Feedback Equalizer (DFE) for removal of Inter Symbol Interference
(ISI) at the receiver. The channel disrupts the transmitted signal by spreading
it in time. Although  the LMS algorithm is robust and reliable  it is slow in
convergence. In order to increase the speed of convergence  modifications have
been made in the algorithm where the weights get updated depending on the
severity of disturbance."
"The early classifications of the computational complexity of planning under
various restrictions in STRIPS (Bylander) and SAS+ (Baeckstroem and Nebel) have
influenced following research in planning in many ways. We go back and
reanalyse their subclasses  but this time using the more modern tool of
parameterized complexity analysis. This provides new results that together with
the old results give a more detailed picture of the complexity landscape. We
demonstrate separation results not possible with standard complexity theory 
which contributes to explaining why certain cases of planning have seemed
simpler in practice than theory has predicted. In particular  we show that
certain restrictions of practical interest are tractable in the parameterized
sense of the term  and that a simple heuristic is sufficient to make a
well-known partial-order planner exploit this fact."
"Cumulative resource constraints can model scarce resources in scheduling
problems or a dimension in packing and cutting problems. In order to
efficiently solve such problems with a constraint programming solver  it is
important to have strong and fast propagators for cumulative resource
constraints. One such propagator is the recently developed
time-table-edge-finding propagator  which considers the current resource
profile during the edge-finding propagation. Recently  lazy clause generation
solvers  i.e. constraint programming solvers incorporating nogood learning 
have proved to be excellent at solving scheduling and cutting problems. For
such solvers  concise and accurate explanations of the reasons for propagation
are essential for strong nogood learning. In this paper  we develop the first
explaining version of time-table-edge-finding propagation and show preliminary
results on resource-constrained project scheduling problems from various
standard benchmark suites. On the standard benchmark suite PSPLib  we were able
to close one open instance and to improve the lower bound of about 60% of the
remaining open instances. Moreover  6 of those instances were closed."
"In the field of ontology matching  the most systematic evaluation of matching
systems is established by the Ontology Alignment Evaluation Initiative (OAEI) 
which is an annual campaign for evaluating ontology matching systems organized
by different groups of researchers. In this paper  we report on the results of
an intermediary OAEI campaign called OAEI 2011.5. The evaluations of this
campaign are divided in five tracks. Three of these tracks are new or have been
improved compared to previous OAEI campaigns. Overall  we evaluated 18 matching
systems. We discuss lessons learned  in terms of scalability  multilingual
issues and the ability do deal with real world cases from different domains."
"Today  we can find many search engines which provide us with information
which is more operational in nature. None of the search engines provide domain
specific information. This becomes very troublesome to a novice user who wishes
to have information in a particular domain. In this paper  we have developed an
ontology which can be used by a domain specific search engine. We have
developed an ontology on human anatomy  which captures information regarding
cardiovascular system  digestive system  skeleton and nervous system. This
information can be used by people working in medical and health care domain."
"Various methods for lifted probabilistic inference have been proposed  but
our understanding of these methods and the relationships between them is still
limited  compared to their propositional counterparts. The only existing
theoretical characterization of lifting is for weighted first-order model
counting (WFOMC)  which was shown to be complete domain-lifted for the class of
2-logvar models. This paper makes two contributions to lifted variable
elimination (LVE). First  we introduce a novel inference operator called group
inversion. Second  we prove that LVE augmented with this operator is complete
in the same sense as WFOMC."
"The Generalized Traveling Salesman Problem (GTSP) is one of the NP-hard
combinatorial optimization problems. A variant of GTSP is E-GTSP where E 
meaning equality  has the constraint. exactly one node from a cluster of a
graph partition is visited. The main objective of the E-GTSP is to find a
minimum cost tour passing through exactly one node from each cluster of an
undirected graph. Agent-based approaches involving are successfully used
nowadays for solving real life complex problems. The aim of the current paper
is to illustrate some variants of agent-based algorithms including ant-based
models with specific properties for solving E-GTSP."
"The current paper introduces a new parallel computing technique based on ant
colony optimization for a dynamic routing problem. In the dynamic traveling
salesman problem the distances between cities as travel times are no longer
fixed. The new technique uses a parallel model for a problem variant that
allows a slight movement of nodes within their Neighborhoods. The algorithm is
tested with success on several large data sets."
"This is the Proceedings of the Twenty-Fourth Conference on Uncertainty in
Artificial Intelligence  which was held in Helsinki  Finland  July 9 - 12 2008."
"This is the Proceedings of the Twenty-Third Conference on Uncertainty in
Artificial Intelligence  which was held in Vancouver  British Columbia  July 19
- 22 2007."
"This is the Proceedings of the Twenty-First Conference on Uncertainty in
Artificial Intelligence  which was held in Edinburgh  Scotland July 26 - 29
2005."
"This is the Proceedings of the Twenty-Second Conference on Uncertainty in
Artificial Intelligence  which was held in Cambridge  MA  July 13 - 16 2006."
"This is the Proceedings of the Twentieth Conference on Uncertainty in
Artificial Intelligence  which was held in Banff  Canada  July 7 - 11 2004."
"In this paper it is introduced a biobjective ant algorithm for constructing
low cost routing networks. The new algorithm is called the Distributed Pharaoh
System (DPS). DPS is based on AntNet algorithm. The algorithm is using Pharaoh
Ant System (PAS) with an extra-exploration phase and a 'no-entry' condition in
order to improve the solutions for the Low Cost Network Routing problem.
Additionally it is used a cost model for overlay network construction that
includes network traffic demands. The Pharaoh ants (Monomorium pharaonis)
includes negative pheromones with signals concentrated at decision points where
trails fork. The negative pheromones may complement positive pheromone or could
help ants to escape from an unnecessarily long route to food that is being
reinforced by attractive signals. Numerical experiments were made for a random
10-node network. The average node degree of the network tested was 4.0. The
results are encouraging. The algorithm converges to the shortest path while
converging on a low cost overlay routing network topology."
"The Matrix Bandwidth Minimization Problem (MBMP) seeks for a simultaneous
reordering of the rows and the columns of a square matrix such that the nonzero
entries are collected within a band of small width close to the main diagonal.
The MBMP is a NP-complete problem  with applications in many scientific
domains  linear systems  artificial intelligence  and real-life situations in
industry  logistics  information recovery. The complex problems are hard to
solve  that is why any attempt to improve their solutions is beneficent.
Genetic algorithms and ant-based systems are Soft Computing methods used in
this paper in order to solve some MBMP instances. Our approach is based on a
learning agent-based model involving a local search procedure. The algorithm is
compared with the classical Cuthill-McKee algorithm  and with a hybrid genetic
algorithm  using several instances from Matrix Market collection. Computational
experiments confirm a good performance of the proposed algorithms for the
considered set of MBMP instances. On Soft Computing basis  we also propose a
new theoretical Reinforcement Learning model for solving the MBMP problem."
"Rough sets were proposed to deal with the vagueness and incompleteness of
knowledge in information systems. There are may optimization issues in this
field such as attribute reduction. Matroids generalized from matrices are
widely used in optimization. Therefore  it is necessary to connect matroids
with rough sets. In this paper  we take field into consideration and introduce
matrix to study rough sets through vector matroids. First  a matrix
representation of an equivalence relation is proposed  and then a matroidal
structure of rough sets over a field is presented by the matrix. Second  the
properties of the matroidal structure including circuits  bases and so on are
studied through two special matrix solution spaces  especially null space.
Third  over a binary field  we construct an equivalence relation from matrix
null space  and establish an algebra isomorphism from the collection of
equivalence relations to the collection of sets  which any member is a family
of the minimal non-empty sets that are supports of members of null space of a
binary dependence matrix. In a word  matrix provides a new viewpoint to study
rough sets."
"There are few knowledge representation (KR) techniques available for
efficiently representing knowledge. However  with the increase in complexity 
better methods are needed. Some researchers came up with hybrid mechanisms by
combining two or more methods. In an effort to construct an intelligent
computer system  a primary consideration is to represent large amounts of
knowledge in a way that allows effective use and efficiently organizing
information to facilitate making the recommended inferences. There are merits
and demerits of combinations  and standardized method of KR is needed. In this
paper  various hybrid schemes of KR were explored at length and details
presented."
"We describe an inference task in which a set of timestamped event
observations must be clustered into an unknown number of temporal sequences
with independent and varying rates of observations. Various existing approaches
to multi-object tracking assume a fixed number of sources and/or a fixed
observation rate; we develop an approach to inferring structure in timestamped
data produced by a mixture of an unknown and varying number of similar Markov
renewal processes  plus independent clutter noise. The inference simultaneously
distinguishes signal from noise as well as clustering signal observations into
separate source streams. We illustrate the technique via a synthetic experiment
as well as an experiment to track a mixture of singing birds."
"Decision tree is an effective classification approach in data mining and
machine learning. In applications  test costs and misclassification costs
should be considered while inducing decision trees. Recently  some
cost-sensitive learning algorithms based on ID3 such as CS-ID3  IDX 
\lambda-ID3 have been proposed to deal with the issue. These algorithms deal
with only symbolic data. In this paper  we develop a decision tree algorithm
inspired by C4.5 for numeric data. There are two major issues for our
algorithm. First  we develop the test cost weighted information gain ratio as
the heuristic information. According to this heuristic information  our
algorithm is to pick the attribute that provides more gain ratio and costs less
for each selection. Second  we design a post-pruning strategy through
considering the tradeoff between test costs and misclassification costs of the
generated decision tree. In this way  the total cost is reduced. Experimental
results indicate that (1) our algorithm is stable and effective; (2) the
post-pruning technique reduces the total cost significantly; (3) the
competition strategy is effective to obtain a cost-sensitive decision tree with
low cost."
"Case Based Reasoning (CBR) is an intelligent way of thinking based on
experience and capitalization of already solved cases (source cases) to find a
solution to a new problem (target case). Retrieval phase consists on
identifying source cases that are similar to the target case. This phase may
lead to erroneous results if the existing knowledge imperfections are not taken
into account. This work presents a novel solution based on Fuzzy logic
techniques and adaptation measures which aggregate weighted similarities to
improve the retrieval results. To confirm the efficiency of our solution  we
have applied it to the industrial diagnosis domain. The obtained results are
more efficient results than those obtained by applying typical measures."
"This paper advocates the exploration of the full state of recorded real-time
strategy (RTS) games  by human or robotic players  to discover how to reason
about tactics and strategy. We present a dataset of StarCraft games
encompassing the most of the games' state (not only player's orders). We
explain one of the possible usages of this dataset by clustering armies on
their compositions. This reduction of armies compositions to mixtures of
Gaussian allow for strategic reasoning at the level of the components. We
evaluated this clustering method by predicting the outcomes of battles based on
armies compositions' mixtures components"
"Many cognitive systems deploy multiple  closed  individually consistent
models which can represent interpretations of the present state of the world 
moments in the past  possible futures or alternate versions of reality. While
they appear under different names  these structures can be grouped under the
general term of worlds. The Xapagy architecture is a story-oriented cognitive
system which relies exclusively on the autobiographical memory implemented as a
raw collection of events organized into world-type structures called {\em
scenes}. The system performs reasoning by shadowing current events with events
from the autobiography. The shadows are then extrapolated into headless shadows
corresponding to predictions  hidden events or inferred relations."
"This paper argues that the problem of identity is a critical challenge in
agents which are able to reason about stories. The Xapagy architecture has been
built from scratch to perform narrative reasoning and relies on a somewhat
unusual approach to represent instances and identity. We illustrate the
approach by a representation of the story of Little Red Riding Hood in the
architecture  with a focus on the problem of identity raised by the narrative."
"The Xapagy architecture is a story-oriented cognitive system which relies
exclusively on the autobiographical memory implemented as a raw collection of
events. Reasoning is performed by shadowing current events with events from the
autobiography. The shadows are then extrapolated into headless shadows (HLSs).
In a story following mood  HLSs can be used to track the level of surprise of
the agent  to infer hidden actions or relations between the participants  and
to summarize ongoing events. In recall mood  the HLSs can be used to create new
stories ranging from exact recall to free-form confabulation."
"This work uses the L-system to construct a tree structure for the text
sequence and derives its complexity. It serves as a measure of structural
complexity of the text. It is applied to anomaly detection in data
transmission."
"This article describes existing and expected benefits of the ""SP theory of
intelligence""  and some potential applications. The theory aims to simplify and
integrate ideas across artificial intelligence  mainstream computing  and human
perception and cognition  with information compression as a unifying theme. It
combines conceptual simplicity with descriptive and explanatory power across
several areas of computing and cognition. In the ""SP machine"" -- an expression
of the SP theory which is currently realized in the form of a computer model --
there is potential for an overall simplification of computing systems 
including software. The SP theory promises deeper insights and better solutions
in several areas of application including  most notably  unsupervised learning 
natural language processing  autonomous robots  computer vision  intelligent
databases  software engineering  information compression  medical diagnosis and
big data. There is also potential in areas such as the semantic web 
bioinformatics  structuring of documents  the detection of computer viruses 
data fusion  new kinds of computer  and the development of scientific theories.
The theory promises seamless integration of structures and functions within and
between different areas of application. The potential value  worldwide  of
these benefits and applications is at least $190 billion each year. Further
development would be facilitated by the creation of a high-parallel 
open-source version of the SP machine  available to researchers everywhere."
"A famous biologically inspired hierarchical model firstly proposed by
Riesenhuber and Poggio has been successfully applied to multiple visual
recognition tasks. The model is able to achieve a set of position- and
scale-tolerant recognition  which is a central problem in pattern recognition.
In this paper  based on some other biological experimental results  we
introduce the Memory and Association Mechanisms into the above biologically
inspired model. The main motivations of the work are (a) to mimic the active
memory and association mechanism and add the 'top down' adjustment to the above
biologically inspired hierarchical model and (b) to build up an algorithm which
can save the space and keep a good recognition performance. The new model is
also applied to object recognition processes. The primary experimental results
show that our method is efficient with much less memory requirement."