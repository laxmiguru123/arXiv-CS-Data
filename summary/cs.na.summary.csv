summary
"We propose alternative discriminant measures for selecting the best basis
among a large collection of orthonormal bases for classification purposes. A
generalization of the Local Discriminant Basis Algorithm of Saito and Coifman
is constructed. The success of these new methods is evaluated and compared to
earlier methods in experiments."
"This note addresses the input and output of intervals in the sense of
interval arithmetic and interval constraints. The most obvious, and so far most
widely used notation, for intervals has drawbacks that we remedy with a new
notation that we propose to call factored notation. It is more compact and
allows one to find a good trade-off between interval width and ease of reading.
We describe how such a trade-off can be based on the information yield (in the
sense of information theory) of the last decimal shown."
"To analyse the significance of the digits used for interval bounds, we
clarify the philosophical presuppositions of various interval notations. We use
information theory to determine the information content of the last digit of
the numeral used to denote the interval's bounds. This leads to the notion of
efficiency of a decimal digit: the actual value as percentage of the maximal
value of its information content. By taking this efficiency into account, many
presentations of intervals can be made more readable at the expense of
negligible loss of information."
"We show that S.Vavasis' sufficient condition for global invertibility of a
polynomial mapping can be easily generalized to the case of a general Lipschitz
mapping. Keywords: Invertibility conditions, generalized Jacobian, nonsmooth
analysis."
"We consider the prospect of a processor that can perform interval arithmetic
at the same speed as conventional floating-point arithmetic. This makes it
possible for all arithmetic to be performed with the superior security of
interval methods without any penalty in speed. In such a situation the IEEE
floating-point standard needs to be compared with a version of floating-point
arithmetic that is ideal for the purpose of interval arithmetic. Such a
comparison requires a succinct and complete exposition of interval arithmetic
according to its recent developments. We present such an exposition in this
paper. We conclude that the directed roundings toward the infinities and the
definition of division by the signed zeros are valuable features of the
standard. Because the operations of interval arithmetic are always defined,
exceptions do not arise. As a result neither Nans nor exceptions are needed. Of
the status flags, only the inexact flag may be useful. Denormalized numbers
seem to have no use for interval arithmetic; in the use of interval
constraints, they are a handicap."
"There are several numerical methods for computing approximate zeros of a
given univariate polynomial. In this paper, we develop a simple and novel
method for determining sharp upper bounds on errors in approximate zeros of a
given polynomial using Rouche's theorem from complex analysis. We compute the
error bounds using non-linear optimization. Our bounds are scalable in the
sense that we compute sharper error bounds for better approximations of zeros.
We use high precision computations using the LEDA/real floating-point filter
for computing our bounds robustly."
"Numerical analysis has no satisfactory method for the more realistic
optimization models. However, with constraint programming one can compute a
cover for the solution set to arbitrarily close approximation. Because the use
of constraint propagation for composite arithmetic expressions is
computationally expensive, consistency is computed with interval arithmetic. In
this paper we present theorems that support, selective initialization, a simple
modification of constraint propagation that allows composite arithmetic
expressions to be handled efficiently."
"We consider linear systems arising from the use of the finite element method
for solving scalar linear elliptic problems. Our main result is that these
linear systems, which are symmetric and positive semidefinite, are well
approximated by symmetric diagonally dominant matrices. Our framework for
defining matrix approximation is support theory. Significant graph theoretic
work has already been developed in the support framework for preconditioners in
the diagonally dominant case, and in particular it is known that such systems
can be solved with iterative methods in nearly linear time. Thus, our
approximation result implies that these graph theoretic techniques can also
solve a class of finite element problems in nearly linear time. We show that
the support number bounds, which control the number of iterations in the
preconditioned iterative solver, depend on mesh quality measures but not on the
problem size or shape of the domain."
"We consider an algorithm called FEMWARP for warping triangular and
tetrahedral finite element meshes that computes the warping using the finite
element method itself. The algorithm takes as input a two- or three-dimensional
domain defined by a boundary mesh (segments in one dimension or triangles in
two dimensions) that has a volume mesh (triangles in two dimensions or
tetrahedra in three dimensions) in its interior. It also takes as input a
prescribed movement of the boundary mesh. It computes as output updated
positions of the vertices of the volume mesh. The first step of the algorithm
is to determine from the initial mesh a set of local weights for each interior
vertex that describes each interior vertex in terms of the positions of its
neighbors. These weights are computed using a finite element stiffness matrix.
After a boundary transformation is applied, a linear system of equations based
upon the weights is solved to determine the final positions of the interior
vertices. The FEMWARP algorithm has been considered in the previous literature
(e.g., in a 2001 paper by Baker). FEMWARP has been succesful in computing
deformed meshes for certain applications. However, sometimes FEMWARP reverses
elements; this is our main concern in this paper. We analyze the causes for
this undesirable behavior and propose several techniques to make the method
more robust against reversals. The most successful of the proposed methods
includes combining FEMWARP with an optimization-based untangler."
"In this paper, we show a way to exploit sparsity in the problem data in a
primal-dual potential reduction method for solving a class of semidefinite
programs. When the problem data is sparse, the dual variable is also sparse,
but the primal one is not. To avoid working with the dense primal variable, we
apply Fukuda et al.'s theory of partial matrix completion and work with partial
matrices instead. The other place in the algorithm where sparsity should be
exploited is in the computation of the search direction, where the gradient and
the Hessian-matrix product of the primal and dual barrier functions must be
computed in every iteration. By using an idea from automatic differentiation in
backward mode, both the gradient and the Hessian-matrix product can be computed
in time proportional to the time needed to compute the barrier functions of
sparse variables itself. Moreover, the high space complexity that is normally
associated with the use of automatic differentiation in backward mode can be
avoided in this case. In addition, we suggest a technique to efficiently
compute the determinant of the positive definite matrix completion that is
required to compute primal search directions. The method of obtaining one of
the primal search directions that minimizes the number of the evaluations of
the determinant of the positive definite completion is also proposed. We then
implement the algorithm and test it on the problem of finding the maximum cut
of a graph."
"The paper describes two iterative algorithms for solving general systems of M
simultaneous linear algebraic equations (SLAE) with real matrices of
coefficients. The system can be determined, underdetermined, and
overdetermined. Linearly dependent equations are also allowed. Both algorithms
use the method of Lagrange multipliers to transform the original SLAE into a
positively determined function F of real original variables X(i) (i=1,...,N)
and Lagrange multipliers Lambda(i) (i=1,...,M). Function F is differentiated
with respect to variables X(i) and the obtained relationships are used to
express F in terms of Lagrange multipliers Lambda(i). The obtained function is
minimized with respect to variables Lambda(i) with the help of one of two the
following minimization techniques: (1) relaxation method or (2) method of
conjugate gradients by Fletcher and Reeves. Numerical examples are given."
"Mathematical treatment of massive wall systems is a useful tool for
investigation of these solar applications. The objectives of this work are to
develop (and validate) a numerical solution model for predication the thermal
behaviour of passive solar systems with massive wall, to improve knowledge of
using indirect passive solar systems and assess its energy efficiency according
to climatic conditions in Bulgaria. The problem of passive solar systems with
massive walls is modelled by thermal and mass transfer equations. As a boundary
conditions for the mathematical problem are used equations, which describe
influence of weather data and constructive parameters of building on the
thermal performance of the passive system. The mathematical model is solved by
means of finite differences method and improved solution procedure. In article
are presented results of theoretical and experimental study for developing and
validating a numerical solution model for predication the thermal behaviour of
passive solar systems with massive wall."
"In this paper, we investigate the use of compactly supported divergence-free
wavelets for the representation of the Navier-Stokes solution. After reminding
the theoretical construction of divergence-free wavelet vectors, we present in
detail the bases and corresponding fast algorithms for 2D and 3D incompressible
flows. In order to compute the nonlinear term, we propose a new method which
provides in practice with the Hodge decomposition of any flow: this
decomposition enables us to separate the incompressible part of the flow from
its orthogonal complement, which corresponds to the gradient component of the
flow. Finally we show numerical tests to validate our approach."
"In this paper a new method of detection of homogeneous zones and singularity
parts of a 1D signal is proposed. The entropy function is used to transform
signal in piecewise linear one. The multiple regression permits to detect lines
and project them in the Hough parameters space in order to easily recognise
homogeneous zone and abrupt changes of the signal. Two application examples are
analysed, the first is a classical fractal signal and the other is issued from
a dynamic mechanical study."
"A finite-volume method for the one-dimensional shallow-water equations
including topographic source terms is presented. Exploiting an original idea by
Leroux, the system of partial-differential equations is completed by a trivial
equation for the bathymetry. By applying a change of variable, the system is
given a celerity-speed formulation, and linearized. As a result, an approximate
Riemann solver preserving the positivity of the celerity can be constructed,
permitting wetting and drying flow simulations to be performed. Finally, the
simulation of numerical test cases is presented."
"In many global optimization problems motivated by engineering applications,
the number of function evaluations is severely limited by time or cost. To
ensure that each evaluation contributes to the localization of good candidates
for the role of global minimizer, a sequential choice of evaluation points is
usually carried out. In particular, when Kriging is used to interpolate past
evaluations, the uncertainty associated with the lack of information on the
function can be expressed and used to compute a number of criteria accounting
for the interest of an additional evaluation at any given point. This paper
introduces minimizer entropy as a new Kriging-based criterion for the
sequential choice of points at which the function should be evaluated. Based on
\emph{stepwise uncertainty reduction}, it accounts for the informational gain
on the minimizer expected from a new evaluation. The criterion is approximated
using conditional simulations of the Gaussian process model behind Kriging, and
then inserted into an algorithm similar in spirit to the \emph{Efficient Global
Optimization} (EGO) algorithm. An empirical comparison is carried out between
our criterion and \emph{expected improvement}, one of the reference criteria in
the literature. Experimental results indicate major evaluation savings over
EGO. Finally, the method, which we call IAGO (for Informational Approach to
Global Optimization) is extended to robust optimization problems, where both
the factors to be tuned and the function evaluations are corrupted by noise."
"Expressions are not functions. Confusing the two concepts or failing to
define the function that is computed by an expression weakens the rigour of
interval arithmetic. We give such a definition and continue with the required
re-statements and proofs of the fundamental theorems of interval arithmetic and
interval analysis.
  Revision Feb. 10, 2009: added reference to and acknowledgement of P. Taylor."
"Interval arithmetic is hardly feasible without directed rounding as provided,
for example, by the IEEE floating-point standard. Equally essential for
interval methods is directed rounding for conversion between the external
decimal and internal binary numerals. This is not provided by the standard I/O
libraries. Conversion algorithms exist that guarantee identity upon conversion
followed by its inverse. Although it may be possible to adapt these algorithms
for use in decimal interval I/O, we argue that outward rounding in radix
conversion is computationally a simpler problem than guaranteeing identity.
Hence it is preferable to develop decimal interval I/O ab initio, which is what
we do in this paper."
"The fast marching algorithm computes an approximate solution to the eikonal
equation in O(N log N) time, where the factor log N is due to the
administration of a priority queue. Recently, Yatziv, Bartesaghi and Sapiro
have suggested to use an untidy priority queue, reducing the overall complexity
to O(N) at the price of a small error in the computed solution. In this paper,
we give an explicit estimate of the error introduced, which is based on a
discrete comparison principle. This estimates implies in particular that the
choice of an accuracy level that is independent of the speed function F results
in the complexity bound O(Fmax /Fmin N). A numerical experiment illustrates
this robustness problem for large ratios Fmax /Fmin ."
"We use support theory, in particular the fretsaw extensions of Shklarski and
Toledo, to design preconditioners for the stiffness matrices of 2-dimensional
truss structures that are stiffly connected. Provided that all the lengths of
the trusses are within constant factors of each other, that the angles at the
corners of the triangles are bounded away from 0 and $\pi$, and that the
elastic moduli and cross-sectional areas of all the truss elements are within
constant factors of each other, our preconditioners allow us to solve linear
equations in the stiffness matrices to accuracy $\epsilon$ in time $O (n^{5/4}
(\log^{2}n \log \log n)^{3/4} \log (1/\epsilon))$."
"An analysis of traveling wave solutions of partial differential equation
(PDE) systems with cross-diffusion is presented. The systems under study fall
in a general class of the classical Keller-Segel models to describe chemotaxis.
The analysis is conducted using the theory of the phase plane analysis of the
corresponding wave systems without a priory restrictions on the boundary
conditions of the initial PDE. Special attention is paid to families of
traveling wave solutions. Conditions for existence of front-impulse,
impulse-front, and front-front traveling wave solutions are formulated. In
particular, the simplest mathematical model is presented that has an
impulse-impulse solution; we also show that a non-isolated singular point in
the ordinary differential equation (ODE) wave system implies existence of
free-boundary fronts. The results can be used for construction and analysis of
different mathematical models describing systems with chemotaxis."
"An efficient method is introduced in this paper to find the intrinsic mode
function (IMF) components of time series data. This method is faster and more
predictable than the Empirical Mode Decomposition (EMD) method devised by the
author of Hilbert Huang Transform (HHT). The approach is to transforms the
original data function into a piecewise linear sawtooth function (or triangle
wave function), then directly constructs the upper envelope by connecting the
maxima and construct lower envelope by connecting minima with straight line
segments in the sawtooth space, the IMF is calculated as the difference between
the sawtooth function and the mean of the upper and lower envelopes. The
results found in the sawtooth space are reversely transformed into the original
data space as the required IMF and envelopes mean. This decomposition method
process the data in one pass to obtain a unique IMF component without the time
consuming repetitive sifting process of EMD method. An alternative
decomposition method with sawtooth function expansion is also presented."
"To analyze the stability of It\^o stochastic differential equations with
multiplicative noise, we introduce the stochastic logarithmic norm. The
logarithmic norm was originally introduced by G. Dahlquist in 1958 as a tool to
study the growth of solutions to ordinary differential equations and for
estimating the error growth in discretization methods for their approximate
solutions. We extend the concept to the stability analysis of It\^o stochastic
differential equations with multiplicative noise. Stability estimates for
linear It\^o SDEs using the one, two and $\infty$-norms in the $l$-th mean,
where $1 \leq l < \infty $, are derived and the application of the stochastic
logarithmic norm is illustrated with examples."
"In this paper we present the set of intervals as a normed vector space. We
define also a four-dimensional associative algebra whose product gives the
product of intervals in any cases. This approach allows to give a notion of
divisibility and in some cases an euclidian division. We introduce differential
calculus and give some applications."
"We consider the tensorial diffusion equation, and address the discrete
maximum-minimum principle of mixed finite element formulations. In particular,
we address non-negative solutions (which is a special case of the
maximum-minimum principle) of mixed finite element formulations. The discrete
maximum-minimum principle is the discrete version of the maximum-minimum
principle.
  In this paper we present two non-negative mixed finite element formulations
for tensorial diffusion equations based on constrained optimization techniques
(in particular, quadratic programming). These proposed mixed formulations
produce non-negative numerical solutions on arbitrary meshes for low-order
(i.e., linear, bilinear and trilinear) finite elements. The first formulation
is based on the Raviart-Thomas spaces, and is obtained by adding a non-negative
constraint to the variational statement of the Raviart-Thomas formulation. The
second non-negative formulation based on the variational multiscale
formulation.
  For the former formulation we comment on the affect of adding the
non-negative constraint on the local mass balance property of the
Raviart-Thomas formulation. We also study the performance of the active set
strategy for solving the resulting constrained optimization problems. The
overall performance of the proposed formulation is illustrated on three
canonical test problems."
"Let $G_{m \times n}$ be an $m \times n$ real random matrix whose elements are
independent and identically distributed standard normal random variables, and
let $\kappa_2(G_{m \times n})$ be the 2-norm condition number of $G_{m \times
n}$. We prove that, for any $m \geq 2$, $n \geq 2$ and $x \geq |n-m|+1$,
$\kappa_2(G_{m \times n})$ satisfies $
  \frac{1}{\sqrt{2\pi}} ({c}/{x})^{|n-m|+1} < P(\frac{\kappa_2(G_{m \times n})}
{{n}/{(|n-m|+1)}}> x) <
  \frac{1}{\sqrt{2\pi}} ({C}/{x})^{|n-m|+1}, $ where $0.245 \leq c \leq 2.000$
and $ 5.013 \leq C \leq 6.414$ are universal positive constants independent of
$m$, $n$ and $x$. Moreover, for any $m \geq 2$ and $n \geq 2$, $
E(\log\kappa_2(G_{m \times n})) < \log \frac{n}{|n-m|+1} + 2.258. $ A similar
pair of results for complex Gaussian random matrices is also established."
"Proving the existence of a solution to a system of real equations is a
central issue in numerical analysis. In many situations, the system of
equations depend on parameters which are not exactly known. It is then natural
to aim proving the existence of a solution for all values of these parameters
in some given domains. This is the aim of the parametrization of existence
tests. A new parametric existence test based on the Hansen-Sengupta operator is
presented and compared to a similar one based on the Krawczyk operator. It is
used as a basis of a fixed point iteration dedicated to rigorous sensibility
analysis of parametric systems of equations."
"We present a new numerical scheme for one dimensional dynamical systems. This
is a modification of the discrete gradient method and keeps its advantages,
including the stability and the conservation of the energy integral. However,
its accuracy is higher by several orders of magnitude."
"This paper proposes and compares two new sampling schemes for sparse
deconvolution using a Bernoulli-Gaussian model. To tackle such a deconvolution
problem in a blind and unsupervised context, the Markov Chain Monte Carlo
(MCMC) framework is usually adopted, and the chosen sampling scheme is most
often the Gibbs sampler. However, such a sampling scheme fails to explore the
state space efficiently. Our first alternative, the $K$-tuple Gibbs sampler, is
simply a grouped Gibbs sampler. The second one, called partially marginalized
sampler, is obtained by integrating the Gaussian amplitudes out of the target
distribution. While the mathematical validity of the first scheme is obvious as
a particular instance of the Gibbs sampler, a more detailed analysis is
provided to prove the validity of the second scheme.
  For both methods, optimized implementations are proposed in terms of
computation and storage cost. Finally, simulation results validate both schemes
as more efficient in terms of convergence time compared with the plain Gibbs
sampler. Benchmark sequence simulations show that the partially marginalized
sampler takes fewer iterations to converge than the $K$-tuple Gibbs sampler.
However, its computation load per iteration grows almost quadratically with
respect to the data length, while it only grows linearly for the $K$-tuple
Gibbs sampler."
"We show a Condition Number Theorem for the condition number of zero counting
for real polynomial systems. That is, we show that this condition number equals
the inverse of the normalized distance to the set of ill-posed systems (i.e.,
those having multiple real zeros). As a consequence, a smoothed analysis of
this condition number follows."
"This short communication shows that in some cases scalar elliptic finite
element matrices cannot be approximated well by an SDD matrix. We also give a
theoretical analysis of a simple heuristic method for approximating an element
by an SDD matrix."
"In this paper, a linear system of equations with crisp coefficients and fuzzy
right-hand sides is investigated. All possible cases pertaining to the number
of variables, n, and the number of equations, m, are dealt with. A solution is
sought not as a fuzzy vector, as usual, but as a fuzzy set of vectors. Each
vector in the solution set solves the given fuzzy linear system with a certain
possibility. Assuming that the coefficient matrix is a full rank matrix, three
cases are considered: For m = n (square system), the solution set is shown to
be a parallelepiped in coordinate space and is expressed by an explicit
formula. For m > n (overdetermined system), the solution set is proved to be a
convex polyhedron and a novel geometric method is proposed to compute it. For m
< n (underdetermined system), by determining the contribution of free
variables, general solution is computed. From the results of three cases
mentioned above, a method is proposed to handle the general case, in which the
coefficient matrix is not necessarily a full rank matrix. Comprehensive
examples are provided and investigated in depth to illustrate each case and
suggested method."
"In this paper, we present a practical algorithm based on sparsity
regularization to effectively solve nonlinear dynamic inverse problems that are
encountered in subsurface model calibration. We use an iteratively reweighted
algorithm that is widely used to solve linear inverse problems with sparsity
constraint known as compressed sensing to estimate permeability fields from
nonlinear dynamic flow data."
"The computational complexity of different steps of the basic SSA is
discussed. It is shown that the use of the general-purpose ""blackbox"" routines
(e.g. found in packages like LAPACK) leads to huge waste of time resources
since the special Hankel structure of the trajectory matrix is not taken into
account. We outline several state-of-the-art algorithms (for example,
Lanczos-based truncated SVD) which can be modified to exploit the structure of
the trajectory matrix. The key components here are hankel matrix-vector
multiplication and hankelization operator. We show that both can be computed
efficiently by the means of Fast Fourier Transform. The use of these methods
yields the reduction of the worst-case computational complexity from O(N^3) to
O(k N log(N)), where N is series length and k is the number of eigentriples
desired."
"This paper studies and analyzes a preconditioned Krylov solver for Helmholtz
problems that are formulated with absorbing boundary layers based on complex
coordinate stretching. The preconditioner problem is a Helmholtz problem where
not only the coordinates in the absorbing layer have an imaginary part, but
also the coordinates in the interior region. This results into a preconditioner
problem that is invertible with a multigrid cycle. We give a numerical analysis
based on the eigenvalues and evaluate the performance with several numerical
experiments. The method is an alternative to the complex shifted Laplacian and
it gives a comparable performance for the studied model problems."
"We describe an algorithm that, given any full-rank matrix A having fewer rows
than columns, can rapidly compute the orthogonal projection of any vector onto
the null space of A, as well as the orthogonal projection onto the row space of
A, provided that both A and its adjoint can be applied rapidly to arbitrary
vectors. As an intermediate step, the algorithm solves the overdetermined
linear least-squares regression involving the adjoint of A (and so can be used
for this, too). The basis of the algorithm is an obvious but numerically
unstable scheme; suitable use of a preconditioner yields numerical stability.
We generate the preconditioner rapidly via a randomized procedure that succeeds
with extremely high probability. In many circumstances, the method can
accelerate interior-point methods for convex optimization, such as linear
programming (Ming Gu, personal communication)."
"Geometric programming (GP) provides a power tool for solving a variety of
optimization problems. In the real world, many applications of geometric
programming (GP) are engineering design problems in which some of the problem
parameters are estimating of actual values. This paper develops a solution
procedure to solve nonlinear programming problems using GP technique by
splitting the cost coefficients, constraint coefficients and exponents with the
help of binary numbers. The equivalent mathematical programming problems are
formulated to find their corresponding value of the objective function based on
the duality theorem. The ability of calculating the cost coefficients,
constraint coefficients and exponents developed in this paper might help lead
to more realistic modeling efforts in engineering design areas. Standard
nonlinear programming software has been used to solve the proposed optimization
problem. Two numerical examples are presented to illustrate the method."
"In this note, we have shown special case on Routh stability criterion, which
is not discussed, in previous literature. This idea can be useful in computer
science applications."
"Geometric programming is an important class of optimization problems that
enable practitioners to model a large variety of real-world applications,
mostly in the field of engineering design. In many real life optimization
problem multi-objective programming plays a vital role in socio-economical and
industrial optimizing problems. In this paper we have discussed the basic
concepts and principle of multiple objective optimization problems and
developed geometric programming (GP) technique to solve this optimization
problem using weighted method to obtain the non-inferior solutions."
"In this paper, a new approach is presented to determine common eigenvalues of
two matrices. It is based on Gerschgorin theorem and Bisection method. The
proposed approach is simple and can be useful in image processing and noise
estimation."
"In this paper, we consider anisotropic diffusion with decay, and the
diffusivity coefficient to be a second-order symmetric and positive definite
tensor. It is well-known that this particular equation is a second-order
elliptic equation, and satisfies a maximum principle under certain regularity
assumptions. However, the finite element implementation of the classical
Galerkin formulation for both anisotropic and isotropic diffusion with decay
does not respect the maximum principle.
  We first show that the numerical accuracy of the classical Galerkin
formulation deteriorates dramatically with increase in the decay coefficient
for isotropic medium and violates the discrete maximum principle. However, in
the case of isotropic medium, the extent of violation decreases with mesh
refinement. We then show that, in the case of anisotropic medium, the classical
Galerkin formulation for anisotropic diffusion with decay violates the discrete
maximum principle even at lower values of decay coefficient and does not vanish
with mesh refinement. We then present a methodology for enforcing maximum
principles under the classical Galerkin formulation for anisotropic diffusion
with decay on general computational grids using optimization techniques.
Representative numerical results (which take into account anisotropy and
heterogeneity) are presented to illustrate the performance of the proposed
formulation."
"Recent work in theoretical computer science and scientific computing has
focused on nearly-linear-time algorithms for solving systems of linear
equations. While introducing several novel theoretical perspectives, this work
has yet to lead to practical algorithms. In an effort to bridge this gap, we
describe in this paper two related results. Our first and main result is a
simple algorithm to approximate the solution to a set of linear equations
defined by a Laplacian (for a graph $G$ with $n$ nodes and $m \le n^2$ edges)
constraint matrix. The algorithm is a non-recursive algorithm; even though it
runs in $O(n^2 \cdot \polylog(n))$ time rather than $O(m \cdot polylog(n))$
time (given an oracle for the so-called statistical leverage scores), it is
extremely simple; and it can be used to compute an approximate solution with a
direct solver. In light of this result, our second result is a straightforward
connection between the concept of graph resistance (which has proven useful in
recent algorithms for linear equation solvers) and the concept of statistical
leverage (which has proven useful in numerically-implementable randomized
algorithms for large matrix problems and which has a natural data-analytic
interpretation)."
"In this paper, the performances of the quasi-Newton BFGS algorithm, the
NEWUOA derivative free optimizer, the Covariance Matrix Adaptation Evolution
Strategy (CMA-ES), the Differential Evolution (DE) algorithm and Particle Swarm
Optimizers (PSO) are compared experimentally on benchmark functions reflecting
important challenges encountered in real-world optimization problems.
Dependence of the performances in the conditioning of the problem and
rotational invariance of the algorithms are in particular investigated."
"Various classes of stable finite difference schemes can be constructed to
obtain a numerical solution. It is important to select among all stable schemes
such a scheme that is optimal in terms of certain additional criteria. In this
study, we use a simple boundary value problem for a one-dimensional parabolic
equation to discuss the selection of an approximation with respect to time. We
consider the pure diffusion equation, the pure convective transport equation
and combined convection-diffusion phenomena. Requirements for the
unconditionally stable finite difference schemes are formulated that are
related to retaining the main features of the differential problem. The concept
of SM stable finite difference scheme is introduced. The starting point are
difference schemes constructed on the basis of the various Pad$\acute{e}$
approximations."
"Rapid processes of heat transfer are not described by the standard heat
conduction equation. To take into account a finite velocity of heat transfer,
we use the hyperbolic model of heat conduction, which is connected with the
relaxation of heat fluxes. In this case, the mathematical model is based on a
hyperbolic equation of second order or a system of equations for the
temperature and heat fluxes. In this paper we construct for the hyperbolic heat
conduction equation the additive schemes of splitting with respect to
directions. Unconditional stability of locally one-dimensional splitting
schemes is established. New splitting schemes are proposed and studied for a
system of equations written in terms of the temperature and heat fluxes."
"Circulant matrices play a central role in a recently proposed formulation of
three-way data computations. In this setting, a three-way table corresponds to
a matrix where each ""scalar"" is a vector of parameters defining a circulant.
This interpretation provides many generalizations of results from matrix or
vector-space algebra. We derive the power and Arnoldi methods in this algebra.
In the course of our derivation, we define inner products, norms, and other
notions. These extensions are straightforward in an algebraic sense, but the
implications are dramatically different from the standard matrix case. For
example, a matrix of circulants has a polynomial number of eigenvalues in its
dimension; although, these can all be represented by a carefully chosen
canonical set of eigenvalues and vectors. These results and algorithms are
closely related to standard decoupling techniques on block-circulant matrices
using the fast Fourier transform."
"Domain decomposition methods are essential in solving applied problems on
parallel computer systems. For boundary value problems for evolutionary
equations the implicit schemes are in common use to solve problems at a new
time level employing iterative methods of domain decomposition. An alternative
approach is based on constructing iteration-free methods based on special
schemes of splitting into subdomains. Such regionally-additive schemes are
constructed using the general theory of additive operator-difference schemes.
There are employed the analogues of classical schemes of alternating direction
method, locally one-dimensional schemes, factorization methods, vector and
regularized additive schemes. The main results were obtained here for
time-dependent problems with self-adjoint elliptic operators of second order.
  The paper discusses the Cauchy problem for the first order evolutionary
equations with a nonnegative not self-adjoint operator in a finite-dimensional
Hilbert space. Based on the partition of unit, we have constructed the
operators of decomposition which preserve nonnegativity for the individual
operator terms of splitting. Unconditionally stable additive schemes of domain
decomposition were constructed using the regularization principle for
operator-difference schemes. Vector additive schemes were considered, too. The
results of our work are illustrated by a model problem for the two-dimensional
parabolic equation."
"We show how to compactly represent any $n$-dimensional subspace of $R^m$ as a
banded product of Householder reflections using $n(m - n)$ floating point
numbers. This is optimal since these subspaces form a Grassmannian space
$Gr_n(m)$ of dimension $n(m - n)$. The representation is stable and easy to
compute: any matrix can be factored into the product of a banded Householder
matrix and a square matrix using two to three QR decompositions."
"We describe and test an easy-to-implement two-step high-order compact (2SHOC)
scheme for the Laplacian operator and its implementation into an explicit
finite-difference scheme for simulating the nonlinear Schr\""odinger equation
(NLSE). Our method relies on a compact `double-differencing' which is shown to
be computationally equivalent to standard fourth-order non-compact schemes.
Through numerical simulations of the NLSE using fourth-order Runge-Kutta, we
confirm that our scheme shows the desired fourth-order accuracy. A computation
and storage requirement comparison is made between the 2SHOC scheme and the
non-compact equivalent scheme for both the Laplacian operator alone, as well as
when implemented in the NLSE simulations. Stability bounds are also shown in
order to get maximum efficiency out of the method. We conclude that the modest
increase in storage and computation of the 2SHOC schemes are well worth the
advantages of having the schemes compact, and their ease of implementation
makes their use very useful for practical implementations."
"In quasi-static nonlinear time-dependent analysis, the choice of the time
discretization is a complex issue. The most basic strategy consists in
determining a value of the load increment that ensures the convergence of the
solution with respect to time on the base of preliminary simulations. In more
advanced applications, the load increments can be controlled for instance by
prescribing the number of iterations of the nonlinear resolution procedure, or
by using an arc-length algorithm. These techniques usually introduce a
parameter whose correct value is not easy to obtain. In this paper, an
alternative procedure is proposed. It is based on the continuous control of the
residual of the reference problem over time, whose measure is easy to
interpret. This idea is applied in the framework of a multiscale domain
decomposition strategy in order to perform 3D delamination analysis."
"Fast Fourier Transform (FFT) is an efficient algorithm to compute the
Discrete Fourier Transform (DFT) and its inverse. In this paper, we pay special
attention to the description of complex-data FFT. We analyze two common
descriptions of FFT and propose a new presentation. Our heuristic description
is helpful for students and programmers to grasp the algorithm entirely and
deeply."
"On the basis of additive schemes (splitting schemes) we construct efficient
numerical algorithms to solve approximately the initial-boundary value problems
for systems of time-dependent partial differential equations (PDEs). In many
applied problems the individual components of the vector of unknowns are
coupled together and then splitting schemes are applied in order to get a
simple problem for evaluating components at a new time level. Typically, the
additive operator-difference schemes for systems of evolutionary equations are
constructed for operators coupled in space. In this paper we investigate more
general problems where coupling of derivatives in time for components of the
solution vector takes place. Splitting schemes are developed using an additive
representation for both the primary operator of the problem and the operator at
the time derivative. Splitting schemes are based on a triangular two-component
representation of the operators."
"Recently we introduced a class of number representations denoted
RN-representations, allowing an un-biased rounding-to-nearest to take place by
a simple truncation. In this paper we briefly review the binary fixed-point
representation in an encoding which is essentially an ordinary 2's complement
representation with an appended round-bit. Not only is this rounding a constant
time operation, so is also sign inversion, both of which are at best log-time
operations on ordinary 2's complement representations. Addition, multiplication
and division is defined in such a way that rounding information can be carried
along in a meaningful way, at minimal cost. Based on the fixed-point encoding
we here define a floating point representation, and describe to some detail a
possible implementation of a floating point arithmetic unit employing this
representation, including also the directed roundings."
"The study addresses the problem of precision in floating-point (FP)
computations. A method for estimating the errors which affect intermediate and
final results is proposed and a summary of many software simulations is
discussed. The basic idea consists of representing FP numbers by means of a
data structure collecting value and estimated error information. Under certain
constraints, the estimate of the absolute error is accurate and has a compact
statistical distribution. By monitoring the estimated relative error during a
computation (an ad-hoc definition of relative error has been used), the
validity of results can be ensured. The error estimate enables the
implementation of robust algorithms, and the detection of ill-conditioned
problems. A dynamic extension of number precision, under the control of error
estimates, is advocated, in order to compute results within given error bounds.
A reduced time penalty could be achieved by a specialized FP processor. The
realization of a hardwired processor incorporating the method, with current
technology, should not be anymore a problem and would make the practical
adoption of the method feasible for most applications."
"Fast multipole methods have O(N) complexity, are compute bound, and require
very little synchronization, which makes them a favorable algorithm on
next-generation supercomputers. Their most common application is to accelerate
N-body problems, but they can also be used to solve boundary integral
equations. When the particle distribution is irregular and the tree structure
is adaptive, load-balancing becomes a non-trivial question. A common strategy
for load-balancing FMMs is to use the work load from the previous step as
weights to statically repartition the next step. The authors discuss in the
paper another approach based on data-driven execution to efficiently tackle
this challenging load-balancing problem. The core idea consists of breaking the
most time-consuming stages of the FMMs into smaller tasks. The algorithm can
then be represented as a Directed Acyclic Graph (DAG) where nodes represent
tasks, and edges represent dependencies among them. The execution of the
algorithm is performed by asynchronously scheduling the tasks using the QUARK
runtime environment, in a way such that data dependencies are not violated for
numerical correctness purposes. This asynchronous scheduling results in an
out-of-order execution. The performance results of the data-driven FMM
execution outperform the previous strategy and show linear speedup on a
quad-socket quad-core Intel Xeon system."
"Sparse matrix-vector multiplication (SpMxV) is a kernel operation widely used
in iterative linear solvers. The same sparse matrix is multiplied by a dense
vector repeatedly in these solvers. Matrices with irregular sparsity patterns
make it difficult to utilize cache locality effectively in SpMxV computations.
In this work, we investigate single- and multiple-SpMxV frameworks for
exploiting cache locality in SpMxV computations. For the single-SpMxV
framework, we propose two cache-size-aware top-down row/column-reordering
methods based on 1D and 2D sparse matrix partitioning by utilizing the
column-net and enhancing the row-column-net hypergraph models of sparse
matrices. The multiple-SpMxV framework depends on splitting a given matrix into
a sum of multiple nonzero-disjoint matrices so that the SpMxV operation is
performed as a sequence of multiple input- and output-dependent SpMxV
operations. For an effective matrix splitting required in this framework, we
propose a cache-size-aware top-down approach based on 2D sparse matrix
partitioning by utilizing the row-column-net hypergraph model. The primary
objective in all of the three methods is to maximize the exploitation of
temporal locality. We evaluate the validity of our models and methods on a wide
range of sparse matrices by performing actual runs through using OSKI.
Experimental results show that proposed methods and models outperform
state-of-the-art schemes."
"In this paper, we present and analyze a new set of low-rank recovery
algorithms for linear inverse problems within the class of hard thresholding
methods. We provide strategies on how to set up these algorithms via basic
ingredients for different configurations to achieve complexity vs. accuracy
tradeoffs. Moreover, we study acceleration schemes via memory-based techniques
and randomized, $\epsilon$-approximate matrix projections to decrease the
computational costs in the recovery process. For most of the configurations, we
present theoretical analysis that guarantees convergence under mild problem
conditions. Simulation results demonstrate notable performance improvements as
compared to state-of-the-art algorithms both in terms of reconstruction
accuracy and computational complexity."
"Transient diffusion equations arise in many branches of engineering and
applied sciences (e.g., heat transfer and mass transfer), and are parabolic
partial differential equations. It is well-known that, under certain
assumptions on the input data, these equations satisfy important mathematical
properties like maximum principles and the non-negative constraint, which have
implications in mathematical modeling. However, existing numerical formulations
for these types of equations do not, in general, satisfy maximum principles and
the non-negative constraint. In this paper, we present a methodology for
enforcing maximum principles and the non-negative constraint for transient
anisotropic diffusion equation. The method of horizontal lines (also known as
the Rothe method) is applied in which the time is discretized first. This
results in solving steady anisotropic diffusion equation with decay equation at
every discrete time level. The proposed methodology for transient anisotropic
diffusion equation will satisfy maximum principles and the non-negative
constraint on general computational grids, and with no additional restrictions
on the time step. We illustrate the performance and accuracy of the proposed
formulation using representative numerical examples. We also perform numerical
convergence of the proposed methodology. For comparison, we also present the
results from the standard single-field semi-discrete formulation and the
results from a popular software package, which all will violate maximum
principles and the non-negative constraint."
"Operator Axiom produces new real numbers with new operators. New operators
naturally produce new equations and thus extend the traditional mathematical
models which are selected to describe various scientific rules. So new
operators help to describe complex scientific rules which are difficult
described by traditional equations and have an enormous application potential.
As to the equations including new operators, engineering computation often need
the approximate solutions reflecting an intuitive order relation and
equivalence relation. However, the order relation and equivalence relation of
real numbers are not as intuitive as those of base-b expansions. Thus, this
paper introduces numerical computations to approximate all real numbers with
base-b expansions."
"Splines are a popular and attractive way of smoothing noisy data. Computing
splines involves minimizing a functional which is a linear combination of a
fitting term and a regularization term. The former is classically computed
using a (weighted) L2 norm while the latter ensures smoothness. Thus, when
dealing with grid data, the optimization can be solved very efficiently using
the DCT. In this work we propose to replace the L2 norm in the fitting term
with an L1 norm, leading to automatic robustness to outliers. To solve the
resulting minimization problem we propose an extremely simple and efficient
numerical scheme based on split-Bregman iteration combined with DCT.
Experimental validation shows the high-quality results obtained in short
processing times."
"We present the LU decomposition with panel rank revealing pivoting (LU_PRRP),
an LU factorization algorithm based on strong rank revealing QR panel
factorization. LU_PRRP is more stable than Gaussian elimination with partial
pivoting (GEPP). Our extensive numerical experiments show that the new
factorization scheme is as numerically stable as GEPP in practice, but it is
more resistant to pathological cases and easily solves the Wilkinson matrix and
the Foster matrix. We also present CALU_PRRP, a communication avoiding version
of LU_PRRP that minimizes communication. CALU_PRRP is based on tournament
pivoting, with the selection of the pivots at each step of the tournament being
performed via strong rank revealing QR factorization. CALU_PRRP is more stable
than CALU, the communication avoiding version of GEPP. CALU_PRRP is also more
stable in practice and is resistant to pathological cases on which GEPP and
CALU fail."
"Real root finding of polynomial equations is a basic problem in computer
algebra. This task is usually divided into two parts: isolation and refinement.
In this paper, we propose two algorithms LZ1 and LZ2 to refine real roots of
univariate polynomial equations. Our algorithms combine Newton's method and the
secant method to bound the unique solution in an interval of a monotonic convex
isolation (MCI) of a polynomial, and have quadratic and cubic convergence
rates, respectively. To avoid the swell of coefficients and speed up the
computation, we implement the two algorithms by using the floating-point
interval method in Maple15 with the package intpakX. Experiments show that our
methods are effective and much faster than the function RefineBox in the
software Maple15 on benchmark polynomials."
"We apply the holonomic gradient method (HGM) introduced by [9] to the
calculation of orthant probabilities of multivariate normal distribution. The
holonomic gradient method applied to orthant probabilities is found to be a
variant of Plackett's recurrence relation ([14]). However an implementation of
the method yields recurrence relations more suitable for numerical computation
than Plackett's recurrence relation. We derive some theoretical results on the
holonomic system for the orthant probabilities. These results show that
multivariate normal orthant probabilities possess some remarkable properties
from the viewpoint of holonomic systems. Finally we show that numerical
performance of our method is comparable or superior compared to existing
methods."
"The results obtained by analyzing signals with the Square Wave Method (SWM)
introduced previously can be presented in the frequency domain clearly and
precisely by using the Square Wave Transform (SWT) described here. As an
example, the SWT is used to analyze a sequence of samples (that is, of measured
values) taken from an electroencephalographic recording."
"In this paper we revisit stencil methods on GPUs in the context of
exponential integrators. We further discuss boundary conditions, in the same
context, and show that simple boundary conditions (for example, homogeneous
Dirichlet or homogeneous Neumann boundary conditions) do not affect the
performance if implemented directly into the CUDA kernel. In addition, we show
that stencil methods with position-dependent coefficients can be implemented
efficiently as well.
  As an application, we discuss the implementation of exponential integrators
for different classes of problems in a single and multi GPU setup (up to 4
GPUs). We further show that for stencil based methods such parallelization can
be done very efficiently, while for some unstructured matrices the
parallelization to multiple GPUs is severely limited by the throughput of the
PCIe bus."
"The DFLU numerical flux was introduced in order to solve hyperbolic scalar
conservation laws with a flux function discontinuous in space. We show how this
flux can be used to solve certain class of systems of conservation laws such as
systems modeling polymer flooding in oil reservoir engineering. Furthermore,
these results are extended to the case where the flux function is discontinuous
in the space variable. Such a situation arises for example while dealing with
oil reservoirs which are heterogeneous. Numerical experiments are presented to
illustrate the efficiency of this new scheme compared to other standard schemes
like upstream mobility, Lax-Friedrichs and Force schemes."
"We consider a model for flow in a porous medium with a fracture in which the
flow in the fracture is governed by the Darcy-Forchheimer law while that in the
surrounding matrix is governed by Darcy's law. We give an appropriate mixed,
variational formulation and show existence and uniqueness of the solution. To
show existence we give an analogous formulation for the model in which the
Darcy-Forchheimer law is the governing equation throughout the domain. We show
existence and uniqueness of the solution and show that the solution for the
model with Darcy's law in the matrix is the weak limit of solutions of the
model with the Darcy-Forchheimer law in the entire domain when the Forchheimer
coefficient in the matrix tends toward zero."
"Debugging accumulation of floating-point errors is hard; ideally, computer
should track it automatically. Here we consider twofold approximation of an
exact real with value + error pair of floating-point numbers. Normally, value +
error sum is more accurate than value alone, so error can estimate deviation
between value and its exact target. Fast summation algorithm, that provides
twofold sum of x[1]+...+x[N] or dot product x[1]*y[1]+...+x[N]*y[N], can be
same fast as direct summation sometimes if leveraging processor underused
potential. This way, we can hit three goals: improve precision, track
inaccuracy, and do this with little if any loss in performance."
"Domain decomposition (DD) methods for solving time-dependent problems can be
classified by (i) the method of domain decomposition used, (ii) the choice of
decomposition operators (exchange of boundary conditions), and (iii) the
splitting scheme employed. To construct homogeneous numerical algorithms,
overlapping subdomain methods are preferable. Domain decomposition is
associated with the corresponding additive representation of the problem
operator. To solve time-dependent problems with the DD splitting, different
operator-splitting schemes are used. Various variants of decomposition
operators differ by distinct types of data exchanges on interfaces. They ensure
the convergence of the approximate solution in various spaces of grid
functions."
"Can we assure math computations by automatic verifying floating-point
accuracy? We define fast arithmetic (based on Dekker [1971]) over twofold
approximations $z\approx z_0+z_1$, such that $z_0$ is standard result and $z_1$
assesses inaccuracy $\Delta z_0=z-z_0$. We propose on-fly tracking $z_1$,
detecting if $\Delta z_0$ appears too high. We believe permanent tracking is
worth its cost. C++ test code for Intel AVX available via web."
"In this research paper, the problem of optimization of a quadratic form over
the convex hull generated by the corners of hypercube is attempted and solved.
It is reasoned that under some conditions, the optimum occurs at the corners of
hypercube. Results related to the computation of global optimum stable state
(an NP hard problem) are discussed. An algorithm is proposed. It is hoped that
the results shed light on resolving the P not equal to NP problem."
"Recently, a framework for the approximation of the entire set of
$\epsilon$-efficient solutions (denote by $E_\epsilon$) of a multi-objective
optimization problem with stochastic search algorithms has been proposed. It
was proven that such an algorithm produces -- under mild assumptions on the
process to generate new candidate solutions --a sequence of archives which
converges to $E_{\epsilon}$ in the limit and in the probabilistic sense. The
result, though satisfactory for most discrete MOPs, is at least from the
practical viewpoint not sufficient for continuous models: in this case, the set
of approximate solutions typically forms an $n$-dimensional object, where $n$
denotes the dimension of the parameter space, and thus, it may come to
perfomance problems since in practise one has to cope with a finite archive.
Here we focus on obtaining finite and tight approximations of $E_\epsilon$, the
latter measured by the Hausdorff distance. We propose and investigate a novel
archiving strategy theoretically and empirically. For this, we analyze the
convergence behavior of the algorithm, yielding bounds on the obtained
approximation quality as well as on the cardinality of the resulting
approximation, and present some numerical results."
"We present parallel and sequential dense QR factorization algorithms that are
both optimal (up to polylogarithmic factors) in the amount of communication
they perform, and just as stable as Householder QR. Our first algorithm, Tall
Skinny QR (TSQR), factors m-by-n matrices in a one-dimensional (1-D) block
cyclic row layout, and is optimized for m >> n. Our second algorithm, CAQR
(Communication-Avoiding QR), factors general rectangular matrices distributed
in a two-dimensional block cyclic layout. It invokes TSQR for each block column
factorization."
"Numerical methods based on interval arithmetic are efficient means to
reliably solve nonlinear systems of equations. Algorithm bc3revise is an
interval method that tightens variables' domains by enforcing a property called
box consistency. It has been successfully used on difficult problems whose
solving eluded traditional numerical methods. We present a new algorithm to
enforce box consistency that is simpler than bc3revise, faster, and easily data
parallelizable. A parallel implementation with Intel SSE2 SIMD instructions
shows that an increase in performance of up to an order of magnitude and more
is achievable."
"In this paper we investigate the relationship between stabilized and enriched
finite element formulations for the Stokes problem. We also present a new
stabilized mixed formulation for which the stability parameter is derived
purely by the method of weighted residuals. This new formulation allows equal
order interpolation for the velocity and pressure fields. Finally, we show by
counterexample that a direct equivalence between subgrid-based stabilized
finite element methods and Galerkin methods enriched by bubble functions cannot
be constructed for quadrilateral and hexahedral elements using standard bubble
functions."
"The following paper compares a consistent Newton-Raphson and fixed-point
iteration based solution strategy for a variational multiscale finite element
formulation for incompressible Navier-Stokes. The main contributions of this
work include a consistent linearization of the Navier-Stokes equations, which
provides an avenue for advanced algorithms that require origins in a consistent
method. We also present a comparison between formulations that differ only in
their linearization, but maintain all other equivalences. Using the variational
multiscale concept, we construct a stabilized formulation (that may be
considered an extension of the MINI element to nonlinear Navier-Stokes). We
then linearize the problem using fixed-point iteration and by deriving a
consistent tangent matrix for the update equation to obtain the solution via
Newton-Raphson iterations. We show that the consistent formulation converges in
fewer iterations, as expected, for several test problems. We also show that the
consistent formulation converges for problems for which fixed-point iteration
diverges. We present the results of both methods for problems of Reynold's
number up to 5000."
"The following work presents a generalized (extended) finite element
formulation for the advection-diffusion equation. Using enrichment functions
that represent the exponential nature of the exact solution, smooth numerical
solutions are obtained for problems with steep gradients and high Peclet
numbers (up to Pe = 25) in one and two-dimensions. As opposed to traditional
stabilized methods that require the construction of stability parameters and
stabilization terms, the present work avoids numerical instabilities by
improving the classical Galerkin solution with an enrichment function. To
contextualize this method among other stabilized methods, we show by
decomposition of the solution (in a multiscale manner) an equivalence to both
Galerkin/least-squares type methods and those that use bubble functions. This
work also presents a strategy for constructing the enrichment function for
problems with complex geometries by employing a global-local approach."
"We consider Cauchy problem for Fourier transformation of 3-dimensional
Navier-Stokes system with zero external force. Using initial data purposed by
Dong Li and Ya.G.Sinai we implement self-similar regime producing fast growing
behavior of the energy of solution while time tends to critical value."
"For one class of boundary value problem depending on small parameter for
which numerical methods for their solution are actually inapplicable, procedure
of limiting problem acquisition which is much easier and which solution as much
as close to the initial solution is described."
"A systematic construction of higher order splines using two hierarchies of
polynomials is presented. Explicit instructions on how to implement one of
these hierarchies are given. The results are limited to interpolations on
regular, rectangular grids, but an approach to other types of grids is also
discussed."
"We introduce a randomized algorithm for computing the minimal-norm solution
to an underdetermined system of linear equations. Given an arbitrary full-rank
m x n matrix A with m<n, any m x 1 vector b, and any positive real number
epsilon less than 1, the procedure computes an n x 1 vector x approximating to
relative precision epsilon or better the n x 1 vector p of minimal Euclidean
norm satisfying Ap=b. The algorithm typically requires O(mn
log(sqrt(n)/epsilon) + m**3) floating-point operations, generally less than the
O(m**2 n) required by the classical schemes based on QR-decompositions or
bidiagonalization. We present several numerical examples illustrating the
performance of the algorithm."
"The convergence of the projection algorithm for solving the convex
feasibility problem for a family of closed convex sets, is in connection with
the regularity properties of the family. In the paper [18] are pointed out four
cases of such a family depending of the two characteristics: the emptiness and
boudedness of the intersection of the family. The case four (the interior of
the intersection is empty and the intersection itself is bounded) is unsolved.
In this paper we give a (partial) answer for the case four: in the case of two
closed convex sets in R3 the regularity property holds."
"The computational modeling of many engineering problems using the Finite
Element method involves the modeling of two or more bodies that meet through an
interface. The interface can be physical, as in multi-physics and contact
problems, or purely numerical, as in the coupling of non-conforming meshes. The
most critical part of the modeling process is to ensure geometric compatibility
and a complete transfer of surface tractions between the different components
at the connecting interfaces. Popular contact modeling techniques rely on
geometric projections to detect and resolve overlapping or mass
interpenetration between two or more contacting bodies. Such approaches have
been shown to have two major drawbacks: they are not suitable for contact at
highly nonlinear surfaces and sharp corners where smooth normal projections are
not feasible, and they fail to guarantee a complete and accurate transfer of
pressure across the interface. This dissertation presents a novel formulation
for the modeling of contact problems that possesses the ability to resolve
complicated contact scenarios effectively, while being simpler to implement and
more widely applicable than currently available methods. We show that the
formulation boils down to a node-to-surface gap function that works effectively
for non-smooth contact. The numerical implementation using the midpoint rule
shows the need to guarantee the conservation of the total energy during impact,
for which a Lagrange multiplier method is used. We propose a local enrichment
of the interface and a simple stabilization procedure based on the
discontinuous Galerkin method to guarantee an accurate transfer of the pressure
field. The result is a robust interface formulation for contact problems and
the coupling of non-conforming meshes."
"Suppose that $\ff \in \reals^{n}$ is a vector of $n$ error-contaminated
measurements of $n$ smooth values measured at distinct and strictly ascending
abscissae. The following projective technique is proposed for obtaining a
vector of smooth approximations to these values. Find \yy\ minimizing $\| \yy -
\ff \|_{\infty}$ subject to the constraints that the second order consecutive
divided differences of the components of \yy\ change sign at most $q$ times.
This optimization problem (which is also of general geometrical interest) does
not suffer from the disadvantage of the existence of purely local minima and
allows a solution to be constructed in $O(nq)$ operations. A new algorithm for
doing this is developed and its effectiveness is proved. Some of the results of
applying it to undulating and peaky data are presented, showing that it is
economical and can give very good results, particularly for large
densely-packed data, even when the errors are quite large."
"A novel procedure is described for accelerating the convergence of Markov
chain Monte Carlo computations. The algorithm uses an adaptive bootstrap
technique to generate candidate steps in the Markov Chain. It is efficient for
symmetric, convex probability distributions, similar to multivariate Gaussians,
and it can be used for Bayesian estimation or for obtaining maximum likelihood
solutions with confidence limits. As a test case, the Law of Categorical
Judgment (Corrected) was fitted with the algorithm to data sets from simulated
rating scale experiments. The correct parameters were recovered from
practical-sized data sets simulated for Full Signal Detection Theory and its
special cases of standard Signal Detection Theory and Complementary Signal
Detection Theory."
"A numerical explicit method to evaluates transient solutions of linear
partial differential non-homogeneous equation with constant coefficients is
proposed."
"A numerical explicit method to evaluates transient solutions of linear
partial differential inhomogeneous equation with constant coefficients is
proposed. A general form of the scheme for a specific linear inhomogeneous
equation is shown. The method is applied to the wave equation and the diffuse
equation and is investigated by simulating simple models. The numerical
solutions of the proposed method show good agreement to the exact solutions.
Comparing with explicit FDM, FDM shows the instability by the violation of CFL
condition whereas the proposed method is always stable irrespective of any time
step width."
"In this paper, a new efficient computational algorithm is presented for
solving cyclic heptadiagonal linear systems based on using of heptadiagonal
linear solver and Sherman-Morrison-Woodbury formula. The implementation of the
algorithm using computer algebra systems (CAS) such as MAPLE and MATLAB is
straightforward. Numerical example is presented for the sake of illustration."
"Numerical algorithms for solving problems of mathematical physics on modern
parallel computers employ various domain decomposition techniques. Domain
decomposition schemes are developed here to solve numerically initial/boundary
value problems for the Stokes system of equations in the primitive variables
pressure-velocity. Unconditionally stable schemes of domain decomposition are
based on the partition of unit for a computational domain and the corresponding
Hilbert spaces of grid functions."
"The process of rank aggregation is intimately intertwined with the structure
of skew-symmetric matrices. We apply recent advances in the theory and
algorithms of matrix completion to skew-symmetric matrices. This combination of
ideas produces a new method for ranking a set of items. The essence of our idea
is that a rank aggregation describes a partially filled skew-symmetric matrix.
We extend an algorithm for matrix completion to handle skew-symmetric data and
use that to extract ranks for each item. Our algorithm applies to both pairwise
comparison and rating data. Because it is based on matrix completion, it is
robust to both noise and incomplete data. We show a formal recovery result for
the noiseless case and present a detailed study of the algorithm on synthetic
data and Netflix ratings."
"The aim of the paper is to derive for the negative correlation function with
a time parameter an asymptotic disjunction of the numerical generalized
least-squares estimator of an unknown constant mean of random field in fact the
correct classic generalized least-squares estimator of an unknown constant mean
of the field."
"In this study, we present a method to measure changes over time of fractal
dimension. We confirmed that our method can calculate the fractal dimension
with the same precision as conventional methods, and tracking performance of
our method is higher than that of the conventional methods."
"Standard explicit schemes for parabolic equations are not very convenient for
computing practice due to the fact that they have strong restrictions on a time
step. More promising explicit schemes are associated with explicit-implicit
splitting of the problem operator (Saul'yev asymmetric schemes, explicit
alternating direction (ADE) schemes, group explicit method). These schemes
belong to the class of unconditionally stable schemes, but they demonstrate bad
approximation properties. These explicit schemes are treated as schemes of the
alternating triangle method and can be considered as factorized schemes where
the problem operator is splitted into the sum of two operators that are adjoint
to each other. Here we propose a multilevel modification of the alternating
triangle method, which demonstrates better properties in terms of accuracy. We
also consider explicit schemes of the alternating triangle method for the
numerical solution of boundary value problems for hyperbolic equations of
second order. The study is based on the general theory of stability
(well-posedness) for operator-difference schemes."
"In this paper, we consider comparison-based adaptive stochastic algorithms
for solving numerical optimisation problems. We consider a specific subclass of
algorithms that we call comparison-based step-size adaptive randomized search
(CB-SARS), where the state variables at a given iteration are a vector of the
search space and a positive parameter, the step-size, typically controlling the
overall standard deviation of the underlying search distribution.We investigate
the linear convergence of CB-SARS on\emph{scaling-invariant} objective
functions. Scaling-invariantfunctions preserve the ordering of points with
respect to their functionvalue when the points are scaled with the same
positive parameter (thescaling is done w.r.t. a fixed reference point). This
class offunctions includes norms composed with strictly increasing functions
aswell as many non quasi-convex and non-continuousfunctions. On
scaling-invariant functions, we show the existence of ahomogeneous Markov
chain, as a consequence of natural invarianceproperties of CB-SARS (essentially
scale-invariance and invariance tostrictly increasing transformation of the
objective function). We thenderive sufficient conditions for \emph{global
linear convergence} ofCB-SARS, expressed in terms of different stability
conditions of thenormalised homogeneous Markov chain (irreducibility,
positivity, Harrisrecurrence, geometric ergodicity) and thus define a general
methodologyfor proving global linear convergence of CB-SARS algorithms
onscaling-invariant functions. As a by-product we provide aconnexion between
comparison-based adaptive stochasticalgorithms and Markov chain Monte Carlo
algorithms."
"Presented here are algorithms for converting between (decimal)
scientific-notation and (binary) IEEE-754 double-precision floating-point
numbers. By employing a rounding integer quotient operation these algorithms
are much simpler than those previously published. The values are stable under
repeated conversions between the formats. Unlike Java-1.6, the scientific
representations generated use only the minimum number of mantissa digits needed
to convert back to the original binary values.
  Implemented in Java these algorithms execute as fast or faster than Java's
native conversions over nearly all of the IEEE-754 double-precision range."
"In the context of unconstraint numerical optimization, this paper
investigates the global linear convergence of a simple probabilistic
derivative-free optimization algorithm (DFO). The algorithm samples a candidate
solution from a standard multivariate normal distribution scaled by a step-size
and centered in the current solution. This solution is accepted if it has a
better objective function value than the current one. Crucial to the algorithm
is the adaptation of the step-size that is done in order to maintain a certain
probability of success. The algorithm, already proposed in the 60's, is a
generalization of the well-known Rechenberg's $(1+1)$ Evolution Strategy (ES)
with one-fifth success rule which was also proposed by Devroye under the name
compound random search or by Schumer and Steiglitz under the name step-size
adaptive random search. In addition to be derivative-free, the algorithm is
function-value-free: it exploits the objective function only through
comparisons. It belongs to the class of comparison-based step-size adaptive
randomized search (CB-SARS). For the convergence analysis, we follow the
methodology developed in a companion paper for investigating linear convergence
of CB-SARS: by exploiting invariance properties of the algorithm, we turn the
study of global linear convergence on scaling-invariant functions into the
study of the stability of an underlying normalized Markov chain (MC). We hence
prove global linear convergence by studying the stability (irreducibility,
recurrence, positivity, geometric ergodicity) of the normalized MC associated
to the $(1+1)$-ES. More precisely, we prove that starting from any initial
solution and any step-size, linear convergence with probability one and in
expectation occurs. Our proof holds on unimodal functions that are the
composite of strictly increasing functions by positively homogeneous functions
with degree $\alpha$ (assumed also to be continuously differentiable). This
function class includes composite of norm functions but also non-quasi convex
functions. Because of the composition by a strictly increasing function, it
includes non continuous functions. We find that a sufficient condition for
global linear convergence is the step-size increase on linear functions, a
condition typically satisfied for standard parameter choices. While introduced
more than 40 years ago, we provide here the first proof of global linear
convergence for the $(1+1)$-ES with generalized one-fifth success rule and the
first proof of linear convergence for a CB-SARS on such a class of functions
that includes non-quasi convex and non-continuous functions. Our proof also
holds on functions where linear convergence of some CB-SARS was previously
proven, namely convex-quadratic functions (including the well-know sphere
function)."
"Gabor analysis is one of the most common instances of time-frequency signal
analysis. Choosing a suitable window for the Gabor transform of a signal is
often a challenge for practical applications, in particular in audio signal
processing. Many time-frequency (TF) patterns of different shapes may be
present in a signal and they can not all be sparsely represented in the same
spectrogram. We propose several algorithms, which provide optimal windows for a
user-selected TF pattern with respect to different concentration criteria. We
base our optimization algorithm on $l^p$-norms as measure of TF spreading. For
a given number of sampling points in the TF plane we also propose optimal
lattices to be used with the obtained windows. We illustrate the potentiality
of the method on selected numerical examples."
"A new variable-width window is presented and compared with several other
windows, both of variable and fixed widths. The comparison focuses on
sensitivity and dynamic range. The equivalent noise bandwidth or ENBW (or
rather, its reciprocal) is used as a proxy for the first; maximum sidelobe
level and high-frequency roll-off in the Fourier transform, for the second. The
new window can access any value of ENBW by appropriate choice of the width
parameter. At any given value of ENBW below about 3, a setting can be found at
which the sidelobes of the window are lower than those of any other in the
moderate frequency regime below about 100 cycles."
"Solving a quadratic equation $P(x)=ax^2+bx+c=0$ with real coefficients is
known to middle school students. Solving the equation over the quaternions is
not straightforward. Huang and So \cite{Huang} give a complete set of formulas,
breaking it into several cases depending on the coefficients. From a result of
the second author in \cite{kalQ}, zeros of $P(x)$ can be expressed in terms of
the zeros of a real quartic equation. This drastically simplifies solving a
quadratic equation. Here we also consider solving $P(x)=0$ iteratively via
Newton and Halley methods developed in \cite{kalQ}. We prove a property of the
Jacobian of Newton and Halley methods and describe several 2D polynomiography
based on these methods. The images not only encode the outcome of the iterative
process, but by measuring the time taken to render them we find the relative
speed of convergence for the methods."
"This paper presents an algorithm for optimal multi-degree reduction of
rational disk B\'ezier curve in the $L^{2}$ norm. We start by introducing a
novel disk rational B\'ezier based on parallel projection, whose properties are
also discussed. Then we transform multi-degree reduction of the error radius
curve and the weight curve of the disk rational B\'ezier curve into solving a
constrained quadratic programming (QP) problem. Finally, applying weighted
least squares, we provide the optimal multi-degree reduced polynomial
approximation of the center curve of the original disk rational B\'ezier curve.
Also this paper gives error estimation for this algorithm, and shows some
numerical examples to illustrate the correctness and the validity of
theoretical reasoning."
"The intrinsic mode function (IMF) provides adaptive function bases for
nonlinear and non-stationary time series data. A fast convergent iterative
method is introduced in this paper to find the IMF components of the data, the
method is faster and more predictable than the Empirical Mode Decomposition
method devised by the author of Hilbert Huang Transform. The approach is to
iteratively adjust the control points on the data function corresponding to the
extrema of the refining IMF, the control points of the residue function are
calculated as the median of the straight line segments passing through the data
control points, the residue function is then constructed as the cubic spline
function of the median points. The initial residue function is simply
constructed as the straight line segments passing through the extrema of the
first derivative of the data function. The refining IMF is the difference
between the data function and the improved residue function. The IMF found
reveals all the riding waves in the whole data set. A new data filtering method
on frequency and amplitude of IMF is also presented with the similar approach
of finding the residue on the part to be filtered out. The program to
demonstrate the method is distributed under BSD open source license."
"A linear inverse problem is proposed that requires the determination of
multiple unknown signal vectors. Each unknown vector passes through a different
system matrix and the results are added to yield a single observation vector.
Given the matrices and lone observation, the objective is to find a
simultaneously sparse set of unknown vectors that solves the system. We will
refer to this as the multiple-system single-output (MSSO) simultaneous sparsity
problem. This manuscript contrasts the MSSO problem with other simultaneous
sparsity problems and conducts a thorough initial exploration of algorithms
with which to solve it. Seven algorithms are formulated that approximately
solve this NP-Hard problem. Three greedy techniques are developed (matching
pursuit, orthogonal matching pursuit, and least squares matching pursuit) along
with four methods based on a convex relaxation (iteratively reweighted least
squares, two forms of iterative shrinkage, and formulation as a second-order
cone program). The algorithms are evaluated across three experiments: the first
and second involve sparsity profile recovery in noiseless and noisy scenarios,
respectively, while the third deals with magnetic resonance imaging
radio-frequency excitation pulse design."
"In this paper we consider modifications to Darcy's equation wherein the drag
coefficient is a function of pressure, which is a realistic model for
technological applications like enhanced oil recovery and geological carbon
sequestration. We first outline the approximations behind Darcy's equation and
the modifications that we propose to Darcy's equation, and derive the governing
equations through a systematic approach using mixture theory. We then propose a
stabilized mixed finite element formulation for the modified Darcy's equation.
To solve the resulting nonlinear equations we present a solution procedure
based on the consistent Newton-Raphson method. We solve representative test
problems to illustrate the performance of the proposed stabilized formulation.
One of the objectives of this paper is also to show that the dependence of
viscosity on the pressure can have a significant effect both on the qualitative
and quantitative nature of the solution."
"In this paper, systems of linear differential equations with crisp real
coefficients and with initial condition described by a vector of fuzzy numbers
are studied. A new method based on the geometric representations of linear
transformations is proposed to find a solution. The most important difference
between this method and methods offered in previous papers is that the solution
is considered to be a fuzzy set of real vector-functions rather than a fuzzy
vector-function. Each member of the set satisfies the given system with a
certain possibility. It is shown that at any time the solution constitutes a
fuzzy region in the coordinate space, alfa-cuts of which are nested
parallelepipeds. Proposed method is illustrated on examples."
"One of the most used approaches in simulating materials is the tight-binding
approximation. When using this method in a material simulation, it is necessary
to compute the eigenvalues and eigenvectors of the Hamiltonian describing the
system. In general, the system possesses few explicit symmetries. Due to them,
the problem has many degenerate eigenvalues. The ambiguity in choosing a
orthonormal basis of the invariant subspaces, associated with degenerate
eigenvalues, will result in eigenvectors which are not invariant under the
action of the symmetry operators in matrix form. A meaningful computation of
the eigenvectors needs to take those symmetries into account. A natural choice
is a set of eigenvectors, which simultaneously diagonalizes the Hamiltonian and
the symmetry matrices. This is possible because all the matrices commute with
each other. The simultaneous eigenvectors and the corresponding eigenvalues
will be in a parametrized form in terms of the lattice momentum components.
This functional dependence of the eigenvalues is the dispersion relation and
describes the band structure of a material. Therefore it is important to find
this functional dependence in any numerical computation related to material
properties."
"We accelerate the computation of spherical harmonic transforms, using what is
known as the butterfly scheme. This provides a convenient alternative to the
approach taken in the second paper from this series on ""Fast algorithms for
spherical harmonic expansions."" The requisite precomputations become manageable
when organized as a ""depth-first traversal"" of the program's control-flow
graph, rather than as the perhaps more natural ""breadth-first traversal"" that
processes one-by-one each level of the multilevel procedure. We illustrate the
results via several numerical examples."
"The present paper discusses the problem of least-squares over the real
symplectic group of matrices Sp(2n,R)$. The least-squares problem may be
extended from flat spaces to curved spaces by the notion of geodesic distance.
The resulting non-linear minimization problem on manifold may be tackled by
means of a gradient-descent algorithm tailored to the geometry of the space at
hand. In turn, gradient steepest descent on manifold may be implemented through
a geodesic-based stepping method. As the space Sp(2n,R) is a non-compact Lie
group, it is convenient to endow it with a pseudo-Riemannian geometry. Indeed,
a pseudo-Riemannian metric allows the computation of geodesic arcs and geodesic
distances in closed form on Sp(2n,R)."
"Digital architectures for Chebyshev interpolation are explored and a
variation which is word-serial in nature is proposed. These architectures are
contrasted with equispaced system structures. Further, Chebyshev interpolation
scheme is compared to the conventional equispaced interpolation vis-a-vis
reconstruction error and relative number of samples. It is also shown that the
use of a hybrid (or dual) Analog to Digital converter unit can reduce system
power consumption by as much as 1/3rd of the original."
"Since its introduction by Gauss, Matrix Algebra has facilitated understanding
of scientific problems, hiding distracting details and finding more elegant and
efficient ways of computational solving. Today's largest problems, which often
originate from multidimensional data, might profit from even higher levels of
abstraction. We developed a framework for solving tensor structured problems
with tensor algebra that unifies concepts from tensor analysis, multilinear
algebra and multidimensional signal processing. In contrast to the conventional
matrix approach, it allows the formulation of multidimensional problems, in a
multidimensional way, preserving structure and data coherence; and the
implementation of automated optimizations of solving algorithms, based on the
commutativity of all tensor operations. Its ability to handle large scientific
tasks is showcased by a real-world, 4D medical imaging problem, with more than
30 million unknown parameters solved on a current, inexpensive hardware. This
significantly surpassed the best published matrix-based approach."
"Mathematical physics problems are often formulated using differential
oprators of vector analysis - invariant operators of first order, namely,
divergence, gradient and rotor operators. In approximate solution of such
problems it is natural to employ similar operator formulations for grid
problems, too. The VAGO (Vector Analysis Grid Operators) method is based on
such a methodology. In this paper the vector analysis difference operators are
constructed using the Delaunay triangulation and the Voronoi diagrams. Further
the VAGO method is used to solve approximately boundary value problems for the
general elliptic equation of second order. In the convection-diffusion-reaction
equation the diffusion coefficient is a symmetric tensor of second order."
"The Schr\""odinger equation defines the dynamics of quantum particles which
has been an area of unabated interest in physics. We demonstrate how simple
transformations of the Schr\""odinger equation leads to a coupled linear system,
whereby each diagonal block is a high frequency Helmholtz problem. Based on
this model, we derive indefinite Helmholtz model problems with strongly varying
wavenumbers. We employ the iterative approach for their solution. In
particular, we develop a preconditioner that has its spectrum restricted to a
quadrant (of the complex plane) thereby making it easily invertible by
multigrid methods with standard components. This multigrid preconditioner is
used in conjuction with suitable Krylov-subspace methods for solving the
indefinite Helmholtz model problems. The aim of this study is to report the
feasbility of this preconditioner for the model problems. We compare this idea
with the other prevalent preconditioning ideas, and discuss its merits. Results
of numerical experiments are presented, which complement the proposed ideas,
and show that this preconditioner may be used in an automatic setting."
"Multigrid preconditioners and solvers for the indefinite Helmholtz equation
suffer from non-stability of the stationary smoothers due to the indefinite
spectrum of the operator. In this paper we explore GMRES as a replacement for
the stationary smoothers of the standard multigrid method. This results in a
robust and efficient solver for a complex shifted or stretched Helmholtz
problem that can be used as a preconditioner. Very few GMRES iterations are
required on each level to build a good multigrid method. The convergence
behavior is compared to a theoretically derived stable polynomial smoother. We
test this method on some benchmark problems and report on the observed
convergence behavior."
"We derive closed formulas for the condition number of a linear function of
the total least squares solution. Given an over determined linear system Ax=b,
we show that this condition number can be computed using the singular values
and the right singular vectors of [A,b] and A. We also provide an upper bound
that requires the computation of the largest and the smallest singular value of
[A,b] and the smallest singular value of A. In numerical examples, we compare
these values and the resulting forward error bounds with existing error
estimates."
"This paper introduces a new preconditioning technique that is suitable for
matrices arising from the discretization of a system of PDEs on unstructured
grids. The preconditioner satisfies a so-called filtering property, which
ensures that the input matrix is identical with the preconditioner on a given
filtering vector. This vector is chosen to alleviate the effect of low
frequency modes on convergence and so decrease or eliminate the plateau which
is often observed in the convergence of iterative methods. In particular, the
paper presents a general approach that allows to ensure that the filtering
condition is satisfied in a matrix decomposition. The input matrix can have an
arbitrary sparse structure. Hence, it can be reordered using nested dissection,
to allow a parallel computation of the preconditioner and of the iterative
process."
"Domain decomposition methods are used for approximate solving boundary
problems for partial differential equations on parallel computing systems.
Specific features of unsteady problems are taken into account in the most
complete way in iteration-free schemes of domain decomposition.
Regionally-additive schemes are based on different classes of splitting
schemes. In this paper we highlight a class of domain decomposition schemes
which is based on the partition of the initial domain into subdomains with
common boundary nodes. Using the partition of unit we have constructed and
studied unconditionally stable schemes of domain decomposition based on
two-component splitting: the problem within subdomain and the problem at their
boundaries. As an example there is considered the Cauchy problem for
evolutionary equations of first and second order with non-negative self-adjoint
operator in a finite Hilbert space. The theoretical consideration is
supplemented with numerical solving a model problem for the two-dimensional
parabolic equation."
"The Equivalent Effect Function (EEF) is defined as having the identical
integral values on the control points of the original time series data; the EEF
can be obtained from the derivative of the spline function passing through the
integral values on the control points. By choosing control points with
different criteria, the EEF can be used to find the intrinsic mode
function(IMF, fluctuation) and the residue (trend); to fit the curve of the
original data function; and to take samples on original data with equivalent
effect. As examples of application, results of trend and fluctuation on real
stock historical data are calculated on different time scales. A new approach
to extend the EEF to 2D intrinsic mode decomposition is introduced to resolve
the inter slice non continuity problem, some photo image decomposition examples
are presented."
"The paper is devoted to problem of spline approximation. A new method of
nodes location for curves and surfaces computer construction by means of
B-splines and results of simulink-modeling is presented. The advantages of this
paper is that we comprise the basic spline with classical polynomials both on
accuracy, as well as degree of paralleling calculations are also shown."
"Linearized numerical stability bounds for solving the nonlinear
time-dependent Schr\""odinger equation (NLSE) using explicit finite-differencing
are shown. The bounds are computed for the fourth-order Runge-Kutta scheme in
time and both second-order and fourth-order central differencing in space.
Results are given for Dirichlet, modulus-squared Dirichlet, Laplacian-zero, and
periodic boundary conditions for one, two, and three dimensions. Our approach
is to use standard Runge-Kutta linear stability theory, treating the
nonlinearity of the NLSE as a constant. The required bounds on the eigenvalues
of the scheme matrices are found analytically when possible, and otherwise
estimated using the Gershgorin circle theorem."
"The localized radial symmetric function, or blob, is an ideal alternative to
the pixel basis for X-ray computed tomography (CT) image reconstruction. In
this paper we develop image representation models using blob, and propose
reconstruction methods for few projections data. The image is represented in a
shift invariant space generated by a Gaussian blob or a multiscale blob system
of different frequency selectivity, and the reconstruction is done through
minimizing the Total Variation or the 1 norm of blob coefficients. Some 2D
numerical results are presented, where we use GPU platform for accelerating the
X-ray projection and back-projection, the interpolation and the gradient
computations."
"Applied problems of oil and gas recovery are studied numerically using the
mathematical models of multiphase fluid flows in porous media. The basic model
includes the continuity equations and the Darcy laws for each phase, as well as
the algebraic expression for the sum of saturations. Primary computational
algorithms are implemented for such problems using the pressure equation. In
this paper, we highlight the basic properties of the pressure problem and
discuss the necessity of their fulfillment at the discrete level. The resulting
elliptic problem for the pressure equation is characterized by a
non-selfadjoint operator. Possibilities of approximate solving the elliptic
problem are considered using the iterative methods. Special attention is given
to the numerical algorithms for calculating the pressure on parallel computers."
"Reconstruction of multidimensional signals from the samples of their partial
derivatives is known to be a standard problem in inverse theory. Such and
similar problems routinely arise in numerous areas of applied sciences,
including optical imaging, laser interferometry, computer vision, remote
sensing and control. Though being ill-posed in nature, the above problem can be
solved in a unique and stable manner, provided proper regularization and
relevant boundary conditions. In this paper, however, a more challenging setup
is addressed, in which one has to recover an image of interest from its noisy
and blurry version, while the only information available about the imaging
system at hand is the amplitude of the generalized pupil function (GPF) along
with partial observations of the gradient of GPF's phase. In this case, the
phase-related information is collected using a simplified version of the
Shack-Hartmann interferometer, followed by recovering the entire phase by means
of derivative compressed sensing. Subsequently, the estimated phase can be
combined with the amplitude of the GPF to produce an estimate of the point
spread function (PSF), whose knowledge is essential for subsequent image
deconvolution. In summary, the principal contribution of this work is twofold.
First, we demonstrate how to simplify the construction of the Shack-Hartmann
interferometer so as to make it less expensive and hence more accessible.
Second, it is shown by means of numerical experiments that the above
simplification and its associated solution scheme produce image reconstructions
of the quality comparable to those obtained using dense sampling of the GPF
phase."
"We constraint on computer the best linear unbiased generalized statistics of
random field for the best linear unbiased generalized statistics of an unknown
constant mean of random field and derive the numerical generalized
least-squares estimator of an unknown constant mean of random field. We derive
the third constraint of spatial statistics and show that the classic
generalized least-squares estimator of an unknown constant mean of the field is
only an asymptotic disjunction of the numerical one."
"The adoption of hybrid GPU-CPU nodes in traditional supercomputing platforms
opens acceleration opportunities for electronic structure calculations in
materials science and chemistry applications, where medium sized Hermitian
generalized eigenvalue problems must be solved many times. The small size of
the problems limits the scalability on a distributed memory system, hence they
can benefit from the massive computational performance concentrated on a single
node, hybrid GPU-CPU system. However, new algorithms that efficiently exploit
heterogeneity and massive parallelism of not just GPUs, but of multi/many-core
CPUs as well are required. Addressing these demands, we implemented a novel
Hermitian general eigensolver algorithm. This algorithm is based on a standard
eigenvalue solver, and existing algorithms can be used. The resulting
eigensolvers are state-of-the-art in HPC, significantly outperforming existing
libraries. We analyze their performance impact on applications of interest,
when different fractions of eigenvectors are needed by the host electronic
structure code."
"The present work attempts to integrate the independent efforts in the fast
N-body community to create the fastest N-body library for many-core and
heterogenous architectures. Focus is placed on low accuracy optimizations, in
response to the recent interest to use FMM as a preconditioner for sparse
linear solvers. A direct comparison with other state-of-the-art fast N-body
codes demonstrates that orders of magnitude increase in performance can be
achieved by careful selection of the optimal algorithm and low-level
optimization of the code. The current N-body solver uses a fast multipole
method with an efficient strategy for finding the list of cell-cell
interactions by a dual tree traversal. A task-based threading model is used to
maximize thread-level parallelism and intra-node load-balancing. In order to
extract the full potential of the SIMD units on the latest CPUs, the inner
kernels are optimized using AVX instructions. Our code -- exaFMM -- is an order
of magnitude faster than the current state-of-the-art FMM codes, which are
themselves an order of magnitude faster than the average FMM code."
"In this article we show the rough outline of a computer algorithm to generate
lower bounds on the exponential function of (in principle) arbitrary precision.
We implemented this to generate all necessary analytic terms for the Boltzmann
machine partition function thus leading to lower bounds of any order. It turns
out that the extra variational parameters can be optimized analytically. We
show that bounds upto nineth order are still reasonably calculable in practical
situations. The generated terms can also be used as extra correction terms
(beyond TAP) in mean field expansions."
"It is well known that a fixed point iteration for solving a linear equation
system converges if and only if the spectral radius of the iteration matrix is
less than one. A method is presented which guarantees the Fixed Point, even if
this condition is not (""spectral radius <1"") fulfilled and demonstrated through
calculation examples."
"The ILU-based preconditioning methods in previous work have their own
parameters to improve their performances. Although the parameters may degrade
the performance, their determination is left to users. Thus, these previous
methods are not reliable in practical computer-aided engineering use. This
paper proposes a novel ILU-based preconditioner called the auto-accelerated
ILU, or A2ILU. In order to improve the convergence, A2ILU introduces
acceleration parameters which modify the ILU factorized preconditioning matrix.
A$^2$ILU needs no more operations than the original ILU because the
acceleration parameters are optimized automatically by A2ILU itself. Numerical
tests reveal the performance of A2ILU is superior to previous ILU-based methods
with manually optimized parameters. The numerical tests also demonstrate the
ability to apply auto-acceleration to ILU-based methods to improve their
performances and robustness of parameter sensitivities."
"Sequence generators obtained by linear recursions over the two-element field
$\mathbb{F}_2$, i.e., $\mathbb{F}_2$-linear generators, are widely used as
pseudorandom number generators. For example, the Mersenne Twister MT19937 is
one of the most successful applications. An advantage of such generators is
that we can assess them quickly by using theoretical criteria, such as the
dimension of equidistribution with $v$-bit accuracy. To compute these
dimensions, several polynomial-time lattice reduction algorithms have been
proposed in the case of $\mathbb{F}_2$-linear generators.
  In this paper, in order to assess non-random bit patterns in dimensions that
are higher than the dimension of equidistribution with $v$-bit accuracy,we
focus on the relationship between points in the Couture--L'Ecuyer dual lattices
and $\mathbb{F}_2$-linear relations on the most significant $v$ bits of output
sequences, and consider a new figure of merit $N_v$ based on the minimum weight
of $\mathbb{F}_2$-linear relations whose degrees are minimal for $v$. Next, we
numerically show that MT19937 has low-weight $\mathbb{F}_2$-linear relations in
dimensions higher than 623, and show that some output vectors with specific
lags are rejected or have small $p$-values in the birthday spacings tests. We
also report that some variants of Mersenne Twister, such as WELL generators,
are significantly improved from the perspective of $N_v$."
"We present a practical implementation of the ensemble Kalman (EnKF) filter
based on an iterative Sherman-Morrison formula. The new direct method exploits
the special structure of the ensemble-estimated error covariance matrices in
order to efficiently solve the linear systems involved in the analysis step of
the EnKF. The computational complexity of the proposed implementation is
equivalent to that of the best EnKF implementations available in the literature
when the number of observations is much larger than the number of ensemble
members. Even when this conditions is not fulfilled, the proposed method is
expected to perform well since it does not employ matrix decompositions.
Computational experiments using the Lorenz 96 and the oceanic quasi-geostrophic
models are performed in order to compare the proposed algorithm with EnKF
implementations that use matrix decompositions. In terms of accuracy, the
results of all implementations are similar. The proposed method is considerably
faster than other EnKF variants, even when the number of observations is large
relative to the number of ensemble members."
"We consider the error due to a single bit-flip in a floating point number. We
assume IEEE 754 double precision arithmetic, which encodes binary floating
point numbers in a 64-bit word. We assume that the bit-flip happens randomly so
it has equi-probability (1/64) to hit any of the 64 bits. Since we want to
mitigate the assumption on our initial floating-point number, we assume that it
is uniformly picked among all normalized number. With this framework, we can
summarize our findings as follows. The probability for a single bit flip to
cause a relative error less than 10^-11 in a normalized floating-point number
is above 25%; The probability for a single bit flip to cause a relative error
less than 10^-6 in a normalized floating-point number is above 50%; Etc."
"This work considers special types of interval linear systems - overdetermined
systems. Simply said these systems have more equations than variables. The
solution set of an interval linear system is a collection of all solutions of
all instances of an interval system. By the instance we mean a point real
system that emerges when we independently choose a real number from each
interval coefficient of the interval system. Enclosing the solution set of
these systems is in some ways more difficult than for square systems. The main
goal of this work is to present various methods for solving overdetermined
interval linear systems. We would like to present them in an understandable way
even for nonspecialists in a field of linear systems. The second goal is a
numerical comparison of all the methods on random interval linear systems
regarding widths of enclosures, computation times and other special properties
of methods."
"The Nystr\""{o}m method is routinely used for out-of-sample extension of
kernel matrices. We describe how this method can be applied to find the
singular value decomposition (SVD) of general matrices and the eigenvalue
decomposition (EVD) of square matrices. We take as an input a matrix $M\in
\mathbb{R}^{m\times n}$, a user defined integer $s\leq min(m,n)$ and $A_M \in
\mathbb{R}^{s\times s}$, a matrix sampled from the columns and rows of $M$.
These are used to construct an approximate rank-$s$ SVD of $M$ in
$O\left(s^2\left(m+n\right)\right)$ operations. If $M$ is square, the rank-$s$
EVD can be similarly constructed in $O\left(s^2 n\right)$ operations. Thus, the
matrix $A_M$ is a compressed version of $M$. We discuss the choice of $A_M$ and
propose an algorithm that selects a good initial sample for a pivoted version
of $M$. The proposed algorithm performs well for general matrices and kernel
matrices whose spectra exhibit fast decay."
"In this work we present a new simple but efficient scheme - Subsquares
approach - for development of algorithms for enclosing the solution set of
overdetermined interval linear systems. We are going to show two algorithms
based on this scheme and discuss their features. We start with a simple
algorithm as a motivation, then we continue with a sequential algorithm. Both
algorithms can be easily parallelized. The features of both algorithms will be
discussed and numerically tested."
"We study the relation between approximate joint diagonalization of
self-adjoint matrices and the norm of their commutator, and show that almost
commuting self-adjoint matrices are almost jointly diagonalizable by a unitary
matrix."
"Time series are collected and studied extensively for the knowledge about the
data source characteristics such as the trend or the spectral landscape. Some
peaks in the spectral landscape correspond to dominant frequencies. The
approach here is empirical: all time series are discrete and finite. Contents:
Introduction. 1 Examples of periodic phenomena. 2 Algorithms and libraries. 3
Time series analysis. 4 Dominant frequency in ladar data. Conclusion.
References."
"The Darcy model is based on a plethora of assumptions. One of the most
important assumptions is that the Darcy model assumes the drag coefficient to
be constant. However, there is irrefutable experimental evidence that
viscosities of organic liquids and carbon-dioxide depend on the pressure.
Experiments have also shown that the drag varies nonlinearly with respect to
the velocity at high flow rates. In important technological applications like
enhanced oil recovery and geological carbon-dioxide sequestration, one
encounters both high pressures and high flow rates. It should be emphasized
that flow characteristics and pressure variation under varying drag are both
quantitatively and qualitatively different from that of constant drag.
Motivated by experimental evidence, we consider the drag coefficient to depend
on both the pressure and velocity. We consider two major modifications to the
Darcy model based on the Barus formula and Forchheimer approximation. The
proposed modifications to the Darcy model result in nonlinear partial
differential equations, which are not amenable to analytical solutions. To this
end, we present mixed finite element formulations based on least-squares
formalism and variational multiscale formalism for the resulting governing
equations. The proposed modifications to the Darcy model and its associated
finite element formulations are used to solve realistic problems with relevance
to enhanced oil recovery. We also study the competition between the nonlinear
dependence of drag on the velocity and the dependence of viscosity on the
pressure. To the best of the authors' knowledge such a systematic study has not
been performed."
"Estimating the number of eigenvalues located in a given interval of a large
sparse Hermitian matrix is an important problem in certain applications and it
is a prerequisite of eigensolvers based on a divide-and-conquer paradigm. Often
an exact count is not necessary and methods based on stochastic estimates can
be utilized to yield rough approximations. This paper examines a number of
techniques tailored to this specific task. It reviews standard approaches and
explores new ones based on polynomial and rational approximation filtering
combined with a stochastic procedure."
"The paper aims at proposing the first shape identification and classification
algorithm in echolocation. The approach is based on first extracting geometric
features from the reflected waves and then matching them with precomputed ones
associated with a dictionary of targets. The construction of such
frequency-dependent shape descriptors is based on some important properties of
the scattering coefficients and new invariants. The stability and resolution of
the proposed identification algorithm with respect to measurement noise and the
limited-view aspect are analytically and numerically quantified."
"The celebrated Perron--Frobenius (PF) theorem is stated for irreducible
nonnegative square matrices, and provides a simple characterization of their
eigenvectors and eigenvalues. The importance of this theorem stems from the
fact that eigenvalue problems on such matrices arise in many fields of science
and engineering, including dynamical systems theory, economics, statistics and
optimization. However, many real-life scenarios give rise to nonsquare
matrices. A natural question is whether the PF Theorem (along with its
applications) can be generalized to a nonsquare setting. Our paper provides a
generalization of the PF Theorem to nonsquare matrices. The extension can be
interpreted as representing client-server systems with additional degrees of
freedom, where each client may choose between multiple servers that can
cooperate in serving it (while potentially interfering with other clients).
This formulation is motivated by applications to power control in wireless
networks, economics and others, all of which extend known examples for the use
of the original PF Theorem.
  We show that the option of cooperation between servers does not improve the
situation, in the sense that in the optimal solution no cooperation is needed,
and only one server needs to serve each client. Hence, the additional power of
having several potential servers per client translates into \emph{choosing} the
best single server and not into \emph{sharing} the load between the servers in
some way, as one might have expected.
  The two main contributions of the paper are (i) a generalized PF Theorem that
characterizes the optimal solution for a non-convex nonsquare problem, and (ii)
an algorithm for finding the optimal solution in polynomial time."
"In recent studies on sparse modeling, the nonconvex regularization approaches
(particularly, $L_{q}$ regularization with $q\in(0,1)$) have been demonstrated
to possess capability of gaining much benefit in sparsity-inducing and
efficiency. As compared with the convex regularization approaches (say, $L_{1}$
regularization), however, the convergence issue of the corresponding algorithms
are more difficult to tackle. In this paper, we deal with this difficult issue
for a specific but typical nonconvex regularization scheme, the $L_{1/2}$
regularization, which has been successfully used to many applications. More
specifically, we study the convergence of the iterative \textit{half}
thresholding algorithm (the \textit{half} algorithm for short), one of the most
efficient and important algorithms for solution to the $L_{1/2}$
regularization. As the main result, we show that under certain conditions, the
\textit{half} algorithm converges to a local minimizer of the $L_{1/2}$
regularization, with an eventually linear convergence rate. The established
result provides a theoretical guarantee for a wide range of applications of the
\textit{half} algorithm. We provide also a set of simulations to support the
correctness of theoretical assertions and compare the time efficiency of the
\textit{half} algorithm with other known typical algorithms for $L_{1/2}$
regularization like the iteratively reweighted least squares (IRLS) algorithm
and the iteratively reweighted $l_{1}$ minimization (IRL1) algorithm."
"Many problems in machine learning can be solved by rounding the solution of
an appropriate linear program (LP). This paper shows that we can recover
solutions of comparable quality by rounding an approximate LP solution instead
of the ex- act one. These approximate LP solutions can be computed efficiently
by applying a parallel stochastic-coordinate-descent method to a
quadratic-penalty formulation of the LP. We derive worst-case runtime and
solution quality guarantees of this scheme using novel perturbation and
convergence analysis. Our experiments demonstrate that on such combinatorial
problems as vertex cover, independent set and multiway-cut, our approximate
rounding scheme is up to an order of magnitude faster than Cplex (a commercial
LP solver) while producing solutions of similar quality."
"In this paper, a highly efficient fast boundary element method (BEM) for
solving large-scale engineering acoustic problems in a broad frequency range is
developed and implemented. The acoustic problems are modeled by the
Burton-Miller boundary integral equation (BIE), thus the fictitious frequency
issue is completely avoided. The BIE is discretized by using the Nystr\""om
method based on the curved quadratic elements, leading to simple numerical
implementation (no edge or corner problems) and high accuracy in the BEM
analysis. The linear systems are solved iteratively and accelerated by using a
newly developed kernel-independent wideband fast directional algorithm (FDA)
for fast summation of oscillatory kernels. In addition, the computational
efficiency of the FDA is further promoted by exploiting the low-rank features
of the translation matrices, resulting in two- to three-fold reduction in the
computational time of the multipole-to-local translations. The high accuracy
and nearly linear computational complexity of the present method are clearly
demonstrated by typical examples. An acoustic scattering problem with
dimensionless wave number $kD$ (where $k$ is the wave number and $D$ is the
typical length of the obstacle) up to 1000 and the degrees of freedom up to 4
million is successfully solved within 10 hours on a computer with one core and
the memory usage is 24 GB."
"This paper presents a comparison of the quality of randomness of D sequences
based on diehard tests. Since D sequences can model any random sequence, this
comparison is of value beyond this specific class."
"We provide a simplified form of Primal Augmented Lagrange Multiplier
algorithm. We intend to fill the gap in the steps involved in the mathematical
derivations of the algorithm so that an insight into the algorithm is made. The
experiment is focused to show the reconstruction done using this algorithm."
"Building of some isomorphic classes for noncanonical hypercomplex number
systems o dimension 2 is described. In general case, such systems with specific
constraints to structural constants can be isomorphic to complex, dual or
double number system. Isomorphic transition between noncanonical hypercomplex
number systems of the general form and diagonal form is built."
"The widespread use of multi-sensor technology and the emergence of big
datasets has highlighted the limitations of standard flat-view matrix models
and the necessity to move towards more versatile data analysis tools. We show
that higher-order tensors (i.e., multiway arrays) enable such a fundamental
paradigm shift towards models that are essentially polynomial and whose
uniqueness, unlike the matrix methods, is guaranteed under verymild and natural
conditions. Benefiting fromthe power ofmultilinear algebra as theirmathematical
backbone, data analysis techniques using tensor decompositions are shown to
have great flexibility in the choice of constraints that match data properties,
and to find more general latent components in the data than matrix-based
methods. A comprehensive introduction to tensor decompositions is provided from
a signal processing perspective, starting from the algebraic foundations, via
basic Canonical Polyadic and Tucker models, through to advanced cause-effect
and multi-view data analysis schemes. We show that tensor decompositions enable
natural generalizations of some commonly used signal processing paradigms, such
as canonical correlation and subspace techniques, signal separation, linear
regression, feature extraction and classification. We also cover computational
aspects, and point out how ideas from compressed sensing and scientific
computing may be used for addressing the otherwise unmanageable storage and
manipulation problems associated with big datasets. The concepts are supported
by illustrative real world case studies illuminating the benefits of the tensor
framework, as efficient and promising tools for modern signal processing, data
analysis and machine learning applications; these benefits also extend to
vector/matrix data through tensorization. Keywords: ICA, NMF, CPD, Tucker
decomposition, HOSVD, tensor networks, Tensor Train."
"A highly efficient fast boundary element method (BEM) for solving large-scale
engineering acoustic problems in a broad frequency range is developed and
implemented. The acoustic problems are modeled by the Burton-Miller boundary
integral equation (BIE), thus the fictitious frequency issue is completely
avoided. The BIE is discretized by using the collocation method with piecewise
constant elements. The linear systems are solved iteratively and accelerated by
using a newly developed kernel-independent wideband fast directional algorithm
(FDA) for fast summation of oscillatory kernels. In addition, the computational
efficiency of the FDA is further promoted by exploiting the low-rank features
of the translation matrices. The high accuracy and nearly linear computational
complexity of the present method are clearly demonstrated by typical examples.
An acoustic scattering problem with dimensionless wave number $kD$ (where $k$
is the wave number and $D$ is the typical length of the obstacle) up to 1000
and the degrees of freedom up to 4 million is successfully solved within 4
hours on a computer with one core and the memory usage is 24.7 GB."
"We present a hybrid OpenMP/Charm++ framework for solving the $\mathcal{O}
(N)$ Self-Consistent-Field eigenvalue problem with parallelism in the strong
scaling regime, $P\gg{N}$, where $P$ is the number of cores, and $N$ a measure
of system size, i.e. the number of matrix rows/columns, basis functions, atoms,
molecules, etc. This result is achieved with a nested approach to Spectral
Projection and the Sparse Approximate Matrix Multiply [Bock and Challacombe,
SIAM J.~Sci.~Comput. 35 C72, 2013], and involves a recursive, task-parallel
algorithm, often employed by generalized $N$-Body solvers, to occlusion and
culling of negligible products in the case of matrices with decay. Employing
classic technologies associated with generalized $N$-Body solvers, including
over-decomposition, recursive task parallelism, orderings that preserve
locality, and persistence-based load balancing, we obtain scaling beyond
hundreds of cores per molecule for small water clusters ([H${}_2$O]${}_N$, $N
\in \{ 30, 90, 150 \}$, $P/N \approx \{ 819, 273, 164 \}$) and find support for
an increasingly strong scalability with increasing system size $N$."
"This study develops a hybrid ensemble-variational approach for solving data
assimilation problems. The method, called TR-4D-EnKF, is based on a trust
region framework and consists of three computational steps. First an ensemble
of model runs is propagated forward in time and snapshots of the state are
stored. Next, a sequence of basis vectors is built and a low-dimensional
representation of the data assimilation system is obtained by projecting the
model state onto the space spanned by the ensemble deviations from the mean.
Finally, the low-dimensional optimization problem is solved in the
reduced-space using a trust region approach; the size of the trust region is
updated according to the relative decrease of the reduced order surrogate cost
function. The analysis state is projected back onto the full space, and the
process is repeated with the current analysis serving as a new background. A
heuristic approach based on the trust region size is proposed in order to
adjust the background error statistics from one iteration to the next.
Experimental simulations are carried out using the Lorenz and the
quasi-geostrophic models. The results show that TR-4D-EnKF is an efficient
computational approach, and is more accurate than the current state of the art
4D-EnKF implementations such as the POD-4D-EnKF and the Iterative Subspace
Minimization methods."
"Computation of the spherical harmonic rotation coefficients or elements of
Wigner's d-matrix is important in a number of quantum mechanics and
mathematical physics applications. Particularly, this is important for the Fast
Multipole Methods in three dimensions for the Helmholtz, Laplace and related
equations, if rotation-based decomposition of translation operators are used.
In these and related problems related to representation of functions on a
sphere via spherical harmonic expansions computation of the rotation
coefficients of large degree $n$ (of the order of thousands and more) may be
necessary. Existing algorithms for their computation, based on recursions, are
usually unstable, and do not extend to $n$. We develop a new recursion and
study its behavior for large degrees, via computational and asymptotic
analyses. Stability of this recursion was studied based on a novel application
of the Courant-Friedrichs-Lewy condition and the von Neumann method for
stability of finite-difference schemes for solution of PDEs. A recursive
algorithm of minimal complexity $O\left(n^{2}\right)$ for degree $n$ and
FFT-based algorithms of complexity $O\left(n^{2}\log n\right) $ suitable for
computation of rotation coefficients of large degrees are proposed, studied
numerically, and cross-validated. It is shown that the latter algorithm can be
used for $n\lesssim 10^{3}$ in double precision, while the former algorithm was
tested for large $n$ (up to $10^{4}$ in our experiments) and demonstrated
better performance and accuracy compared to the FFT-based algorithm."
"In this paper, we present an overview of constrained PARAFAC models where the
constraints model linear dependencies among columns of the factor matrices of
the tensor decomposition, or alternatively, the pattern of interactions between
different modes of the tensor which are captured by the equivalent core tensor.
Some tensor prerequisites with a particular emphasis on mode combination using
Kronecker products of canonical vectors that makes easier matricization
operations, are first introduced. This Kronecker product based approach is also
formulated in terms of the index notation, which provides an original and
concise formalism for both matricizing tensors and writing tensor models. Then,
after a brief reminder of PARAFAC and Tucker models, two families of
constrained tensor models, the co-called PARALIND/CONFAC and PARATUCK models,
are described in a unified framework, for $N^{th}$ order tensors. New tensor
models, called nested Tucker models and block PARALIND/CONFAC models, are also
introduced. A link between PARATUCK models and constrained PARAFAC models is
then established. Finally, new uniqueness properties of PARATUCK models are
deduced from sufficient conditions for essential uniqueness of their associated
constrained PARAFAC models."
"The Rank Minimization Problem asks to find a matrix of lowest rank inside a
linear variety of the space of n x n matrices. The Low Rank Matrix Completion
problem asks to complete a partially filled matrix such that the resulting
matrix has smallest possible rank.
  The Tensor Rank Problem asks to determine the rank of a tensor. We show that
these three problems are equivalent: each one of the problems can be reduced to
the other two."
"In this work, we present a new approach for the distributed computation of
the PARAFAC decomposition of a third-order tensor across a network of
collaborating nodes. We are interested in the case where the overall data
gathered across the network can be modeled as a data tensor admitting an
essentially unique PARAFAC decomposition, while each node only observes a
sub-tensor with not necessarily enough diversity so that identifiability
conditions are not locally fulfilled at each node. In this situation,
conventional (centralized) tensor based methods cannot be applied individually
at each node. By allowing collaboration between neighboring nodes of the
network, we propose distributed versions of the alternating least squares (ALS)
and Levenberg-Marquardt (LM) algorithms for the in-network estimation of the
factor matrices of a third-order tensor. We assume that one of the factor
matrices contains parameters that are local to each node, while the two
remaining factor matrices contain global parameters that are common to the
whole network. The proposed algorithms combine the estimation of the local
factors with an in-network computation of the global factors of the PARAFAC
decomposition using average consensus over graphs. They emulate their
centralized counterparts in the case of ideal data exchange and ideal consensus
computations. The performance of the proposed algorithms are evaluated in both
ideal and imperfect cases."
"The main properties of hypercomplex generalization of quaternion system as
antiquaternion are presented in this article. Definitions and studied of
antiquaternions conjugation are introduced, their norm and zero divisor, and
how to perform operations on them."
"For boolean quadratic programming (BQP), we will show that there is no
duality gap between the primal and dual problems under some conditions by using
the classical Lagrangian duality. A benchmark generator is given to create
random BQP problems which can be solved in polynomial time. Several numerical
examples are generated to demonstrate the effectiveness of the proposed method."
"Considering high speed following on expressway or highway, an improved
car-following model is developed in this paper by introducing variable safety
headway distance. Stability analysis of the new model is carried out using the
control theory method. Finally, numerical simulations are implemented and the
results show good consistency with theoretical study."
"A primary computational problem in kernel regression is solution of a dense
linear system with the $N\times N$ kernel matrix. Because a direct solution has
an O($N^3$) cost, iterative Krylov methods are often used with fast
matrix-vector products. For poorly conditioned problems, convergence of the
iteration is slow and preconditioning becomes necessary. We investigate
preconditioning from the viewpoint of scalability and efficiency. The problems
that conventional preconditioners face when applied to kernel methods are
demonstrated. A \emph{novel flexible preconditioner }that not only improves
convergence but also allows utilization of fast kernel matrix-vector products
is introduced. The performance of this preconditioner is first illustrated on
synthetic data, and subsequently on a suite of test problems in kernel
regression and geostatistical kriging."
"This paper discusses new simulation algorithms for stochastic chemical
kinetics that exploit the linearity of the chemical master equation and its
matrix exponential exact solution. These algorithms make use of various
approximations of the matrix exponential to evolve probability densities in
time. A sampling of the approximate solutions of the chemical master equation
is used to derive accelerated stochastic simulation algorithms. Numerical
experiments compare the new methods with the established stochastic simulation
algorithm and the tau-leaping method."
"We introduce a new iterative root-finding method for complex polynomials,
dubbed {\it Newton-Ellipsoid} method. It is inspired by the Ellipsoid method, a
classical method in optimization, and a property of Newton's Method derived in
\cite{kalFTA}, according to which at each complex number a half-space can be
found containing a root. Newton-Ellipsoid method combines this property, bounds
on zeros, together with the plane-cutting properties of the Ellipsoid Method.
We present computational results for several examples, as well as corresponding
polynomiography. Polynomiography refers to algorithmic visualization of
root-finding. Newton's method is the first member of the infinite family of
iterations, the {\it basic family}. We also consider general versions of this
ellipsoid approach where Newton's method is replaced by a higher-order member
of the family such as Halley's method."
"This study considers using Metropolis-Hastings algorithm for stochastic
simulation of chemical reactions. The proposed method uses SSA (Stochastic
Simulation Algorithm) distribution which is a standard method for solving
well-stirred chemically reacting systems as a desired distribution. A new
numerical solvers based on exponential form of exact and approximate solutions
of CME (Chemical Master Equation) is employed for obtaining target and proposal
distributions in Metropolis-Hastings algorithm to accelerate the accuracy of
the tau-leap method. Samples generated by this technique have the same
distribution as SSA and the histogram of samples show it's convergence to SSA."
"Non-differentiable and constrained optimization play a key role in machine
learning, signal and image processing, communications, and beyond. For
high-dimensional minimization problems involving large datasets or many
unknowns, the forward-backward splitting method provides a simple, practical
solver. Despite its apparently simplicity, the performance of the
forward-backward splitting is highly sensitive to implementation details.
  This article is an introductory review of forward-backward splitting with a
special emphasis on practical implementation concerns. Issues like stepsize
selection, acceleration, stopping conditions, and initialization are
considered. Numerical experiments are used to compare the effectiveness of
different approaches.
  Many variations of forward-backward splitting are implemented in the solver
FASTA (short for Fast Adaptive Shrinkage/Thresholding Algorithm). FASTA
provides a simple interface for applying forward-backward splitting to a broad
range of problems."
"Joint sparsity has attracted considerable attention in recent years in many
fields including sparse signal recovery in compressed sensing (CS), statistics,
and machine learning. Traditional convex models suffer from the suboptimal
performance though enjoying tractable computation. In this paper, we propose a
new non-convex joint sparsity model, and develop a corresponding multi-stage
adaptive convex relaxation algorithm. This method extends the idea of iterative
support detection (ISD) from the single vector estimation to the multi-vector
estimation by considering the joint sparsity prior. We provide some preliminary
theoretical analysis including convergence analysis and a sufficient recovery
condition. Numerical experiments from both compressive sensing and feature
learning show the better performance of the proposed method in comparison with
several state-of-the-art alternatives. Moreover, we demonstrate that the
extension of ISD from the single vector to multi-vector estimation is not
trivial. In particular, while ISD does not work well for reconstructing the
signal channel sparse Bernoulli signal, it does achieve significantly improved
performance when recovering the multi-channel sparse Bernoulli signal thanks to
its ability of natural incorporation of the joint sparsity structure."
"Simultaneous matrix diagonalization is used as a subroutine in many machine
learning problems, including blind source separation and paramater estimation
in latent variable models. Here, we extend algorithms for performing joint
diagonalization to low-rank and asymmetric matrices, and we also provide
extensions to the perturbation analysis of these methods. Our results allow
joint diagonalization to be applied in several new settings."
"Continued demand for accurate and computationally efficient transport methods
to solve optically thick, fixed-source transport problems has inspired research
on variance-reduction (VR) techniques for Monte Carlo (MC). Methods that use
deterministic results to create VR maps for MC constitute a dominant branch of
this research, with Forward Weighted-Consistent Adjoint Driven Importance
Sampling (FW-CADIS) being a particularly successful example. However, locations
in which energy and spatial self-shielding are combined, such as thin plates
embedded in concrete, challenge FW-CADIS. In these cases the deterministic flux
cannot appropriately capture transport behavior, and the associated VR
parameters result in high variance in and following the plate.
  This work presents a new method that improves performance in transport
calculations that contain regions of combined space and energy self-shielding
without significant impact on the solution quality in other parts of the
problem. This method is based on FW-CADIS and applies a Resonance Factor
correction to the adjoint source. The impact of the Resonance Factor method is
investigated in this work through an example problem. It is clear that this new
method dramatically improves performance in terms of lowering the maximum 95%
confidence interval relative error and reducing the compute time. Based on this
work, we recommend that the Resonance Factor method be used when the accuracy
of the solution in the presence of combined space and energy self-shielding is
important."
"First, we extend the results of approximate matrix multiplication from the
Frobenius norm to the spectral norm. Second, We develop a class of fast
approximate generalized linear regression algorithms with respect to the
spectral norm. Finally, We give a fast approximate SVD."
"Classical reconstruction methods for phase-contrast tomography consist of two
stages: phase retrieval and tomographic reconstruction. A novel algebraic
method combining the two was suggested by Kostenko et al. (Opt. Express, 21,
12185, 2013) and preliminary results demonstrating improved reconstruction
compared to a two-stage method given. Using simulated free-space propagation
experiments with a single sample-detector distance, we thoroughly compare the
novel method with the two-stage method to address limitations of the
preliminary results. We demonstrate that the novel method is substantially more
robust towards noise; our simulations point to a possible reduction in counting
times by an order of magnitude."
"Both sampling a time-varying signal, and its spectral analysis are activities
subjected to theoretically compelling, such as Shannon's theorem and the
objectively limiting of the frequency's resolution. Usually, the signals'
spectra are processed and interpreted by a scientist who, presumably, has
sufficient prior information about the monitored signals to conclude on the
significant frequencies, for example. On the other hand, processing and
interpretation of signals' spectra can be routine tasks that must be automated
using suitable software, i.e. PC application. In the above context, the paper
presents the theoretic bases of an intuitive and practical approach of the
(automatic) detection of the common and non-common frequencies in two or more
congruent spectra."
"Digital filter construction method, which is optimal by parametric
sensitivity, based on using of non-canonical hypercomplex number systems is
proposed and investigated. It is shown that the use of non-canonical
hypercomplex number system with greater number of non-zero structure constants
in multiplication table can significantly improve the sensitivity of the
digital filter."
"Fast algorithms for matrix multiplication, namely those that perform
asymptotically fewer scalar operations than the classical algorithm, have been
considered primarily of theoretical interest. Apart from Strassen's original
algorithm, few fast algorithms have been efficiently implemented or used in
practical applications. However, there exist many practical alternatives to
Strassen's algorithm with varying performance and numerical properties. Fast
algorithms are known to be numerically stable, but because their error bounds
are slightly weaker than the classical algorithm, they are not used even in
cases where they provide a performance benefit.
  We argue in this paper that the numerical sacrifice of fast algorithms,
particularly for the typical use cases of practical algorithms, is not
prohibitive, and we explore ways to improve the accuracy both theoretically and
empirically. The numerical accuracy of fast matrix multiplication depends on
properties of the algorithm and of the input matrices, and we consider both
contributions independently. We generalize and tighten previous error analyses
of fast algorithms and compare their properties. We discuss algorithmic
techniques for improving the error guarantees from two perspectives:
manipulating the algorithms, and reducing input anomalies by various forms of
diagonal scaling. Finally, we benchmark performance and demonstrate our
improved numerical accuracy."
"In recent studies on sparse modeling, $l_q$ ($0<q<1$) regularized least
squares regression ($l_q$LS) has received considerable attention due to its
superiorities on sparsity-inducing and bias-reduction over the convex
counterparts. In this paper, we propose a Gauss-Seidel iterative thresholding
algorithm (called GAITA) for solution to this problem. Different from the
classical iterative thresholding algorithms using the Jacobi updating rule,
GAITA takes advantage of the Gauss-Seidel rule to update the coordinate
coefficients. Under a mild condition, we can justify that the support set and
sign of an arbitrary sequence generated by GAITA will converge within finite
iterations. This convergence property together with the Kurdyka-{\L}ojasiewicz
property of ($l_q$LS) naturally yields the strong convergence of GAITA under
the same condition as above, which is generally weaker than the condition for
the convergence of the classical iterative thresholding algorithms.
Furthermore, we demonstrate that GAITA converges to a local minimizer under
certain additional conditions. A set of numerical experiments are provided to
show the effectiveness, particularly, much faster convergence of GAITA as
compared with the classical iterative thresholding algorithms."
"Roundoff errors cannot be avoided when implementing numerical programs with
finite precision. The ability to reason about rounding is especially important
if one wants to explore a range of potential representations, for instance for
FPGAs or custom hardware implementations. This problem becomes challenging when
the program does not employ solely linear operations, and non-linearities are
inherent to many interesting computational problems in real-world applications.
  Existing solutions to reasoning possibly lead to either inaccurate bounds or
high analysis time in the presence of nonlinear correlations between variables.
Furthermore, while it is easy to implement a straightforward method such as
interval arithmetic, sophisticated techniques are less straightforward to
implement in a formal setting. Thus there is a need for methods which output
certificates that can be formally validated inside a proof assistant.
  We present a framework to provide upper bounds on absolute roundoff errors of
floating-point nonlinear programs. This framework is based on optimization
techniques employing semidefinite programming and sums of squares certificates,
which can be checked inside the Coq theorem prover to provide formal roundoff
error bounds for polynomial programs. Our tool covers a wide range of nonlinear
programs, including polynomials and transcendental operations as well as
conditional statements. We illustrate the efficiency and precision of this tool
on non-trivial programs coming from biology, optimization and space control.
Our tool produces more accurate error bounds for 23% of all programs and yields
better performance in 66% of all programs."
"Because of the attractiveness of the canonical polyadic (CP) tensor
decomposition in various applications, several algorithms have been designed to
compute it, but efficient ones are still lacking. Iterative deflation
algorithms based on successive rank-1 approximations can be used to perform
this task, since the latter are rather easy to compute. We first present an
algebraic rank-1 approximation method that performs better than the standard
higher-order singular value decomposition (HOSVD) for three-way tensors.
Second, we propose a new iterative rank-1 approximation algorithm that improves
any other rank-1 approximation method. Third, we describe a probabilistic
framework allowing to study the convergence of deflation CP decomposition
(DCPD) algorithms based on successive rank-1 approximations. A set of computer
experiments then validates theoretical results and demonstrates the efficiency
of DCPD algorithms compared to other ones."
"It has been the standard teaching of today that backward stability analysis
is taught as absolute, just as in Newtonian physics time is taught absolute
time. We will prove it is not true in general. It depends on algorithms. We
will prove that forward and mixed stability anlaysis are absolutely invalid
stability analysis in the sense that they have absolutely wrong reference
points for detecting huge element growth of any algoritms(if any), even an
""ideal"" or ""desirable"" backward stability analysis is not so ""ideal"" or
""desirable"" in general. Any of forward stable, backward stable and mixed stable
algorihms as in Demmel, Kahan , Parlett and other's papers and text books, see
Demmel(6) and Higham(8)may not be really stable at all because they may fail to
detect and expose any potential instabilities of the algorithm in corresponding
stability analysis. Therefore, it is impossible to prove an algorithm is stable
according to the standard teachin of today, just as it is impossible to prove a
mathematical equuation(Maxwell's) is a law of physics according to the standard
teaching in Newtonian physics."
"This paper presents two novel regularization methods motivated in part by the
geometric significance of biorthogonal bases in signal processing applications.
These methods, in particular, draw upon the structural relevance of
orthogonality and biorthogonality principles and are presented from the
perspectives of signal processing, convex programming, continuation methods and
nonlinear projection operators. Each method is specifically endowed with either
a homotopy or tuning parameter to facilitate tradeoff analysis between accuracy
and numerical stability. An example involving a basis comprised of real
exponential signals illustrates the utility of the proposed methods on an
ill-conditioned inverse problem and the results are compared to standard
regularization techniques from the signal processing literature."
"In geometry processing, numerical optimization methods often involve solving
sparse linear systems of equations. These linear systems have a structure that
strongly resembles to adjacency graphs of the underlying mesh. We observe how
classic linear solvers behave on this specific type of problems. For the sake
of simplicity, we minimise either the squared gradient or the squared
Laplacian, evaluated by finite differences on a regular 1D or 2D grid. We
observed the evolution of the solution for both energies, in 1D and 2D, and
with different solvers: Jacobi, Gauss-Seidel, SSOR (Symmetric successive
over-relaxation) and CG (conjugate gradient [She94]). Plotting results at
different iterations allows to have an intuition of the behavior of these
classic solvers."
"In this paper, we use reduced precision checking (RPC) to detect errors in
floating point arithmetic. Prior work explored RPC for addition and
multiplication. In this work, we extend RPC to a complete floating point unit
(FPU), including division and square root, and we present precise analyses of
the errors undetectable with RPC that show bounds that are smaller than prior
work. We implement RPC for a complete FPU in RTL and experimentally evaluate
its error coverage and cost."
"In this paper, two accelerated divide-and-conquer algorithms are proposed for
the symmetric tridiagonal eigenvalue problem, which cost $O(N^2r)$ {flops} in
the worst case, where $N$ is the dimension of the matrix and $r$ is a modest
number depending on the distribution of eigenvalues. Both of these algorithms
use hierarchically semiseparable (HSS) matrices to approximate some
intermediate eigenvector matrices which are Cauchy-like matrices and are
off-diagonally low-rank. The difference of these two versions lies in using
different HSS construction algorithms, one (denoted by {ADC1}) uses a
structured low-rank approximation method and the other ({ADC2}) uses a
randomized HSS construction algorithm. For the ADC2 algorithm, a method is
proposed to estimate the off-diagonal rank. Numerous experiments have been done
to show their stability and efficiency. These algorithms are implemented in
parallel in a shared memory environment, and some parallel implementation
details are included. Comparing the ADCs with highly optimized multithreaded
libraries such as Intel MKL, we find that ADCs could be more than 6x times
faster for some large matrices with few deflations."
"The Finite Element Method (FEM) is generally unable to accurately predict
natural frequencies and mode shapes of structures (eigenvalues and
eigenvectors). Engineers develop numerical methods and a variety of techniques
to compensate for this misalignment of modal properties, between experimentally
measured data and the computed result from the FEM of structures. In this paper
we compare two indirect methods of updating namely, the Adaptive Metropolis
Hastings and a newly applied algorithm called Monte Carlo Dynamically Weighted
Importance Sampling (MCDWIS). The approximation of a posterior predictive
distribution is based on Bayesian inference of continuous multivariate Gaussian
probability density functions, defining the variability of physical properties
affected by forced vibration. The motivation behind applying MCDWIS is in the
complexity of computing normalizing constants in higher dimensional or
multimodal systems. The MCDWIS accounts for this intractability by analytically
computing importance sampling estimates at each time step of the algorithm. In
addition, a dynamic weighting step with an Adaptive Pruned Enriched Population
Control Scheme (APEPCS) allows for further control over weighted samples and
population size. The performance of the MCDWIS simulation is graphically
illustrated for all algorithm dependent parameters and show unbiased, stable
sample estimates."
"Many common methods for data analysis rely on linear algebra. We provide new
results connecting data analysis error to numerical accuracy, which leads to
the first meaningful stopping criterion for two way spectral partitioning. More
generally, we provide pointwise convergence guarantees so that blends (linear
combinations) of eigenvectors can be employed to solve data analysis problems
with confidence in their accuracy. We demonstrate this theory on an accessible
model problem, the Ring of Cliques, by deriving the relevant eigenpairs and
comparing the predicted results to numerical solutions. These results bridge
the gap between linear algebra based data analysis methods and the convergence
theory of iterative approximation methods."
"In this work, we investigate the interval generalized Sylvester matrix
equation ${\bf{A}}X{\bf{B}}+{\bf{C}}X{\bf{D}}={\bf{F}}$ and develop some
techniques for obtaining outer estimations for the so-called united solution
set of this interval system. First, we propose a modified variant of the
Krawczyk operator which causes reducing computational complexity to cubic,
compared to Kronecker product form. We then propose an iterative technique for
enclosing the solution set. These approaches are based on spectral
decompositions of the midpoints of ${\bf{A}}$, ${\bf{B}}$, ${\bf{C}}$ and
${\bf{D}}$ and in both of them we suppose that the midpoints of ${\bf{A}}$ and
${\bf{C}}$ are simultaneously diagonalizable as well as for the midpoints of
the matrices ${\bf{B}}$ and ${\bf{D}}$. Some numerical experiments are given to
illustrate the performance of the proposed methods."
"This primary purpose of this paper is to succinctly state a number of
verifiable and tractable sufficient conditions under which a particular class
of conservative signal processing structures may be readily used to solve a
companion class of constraint satisfaction problems using both synchronous and
asynchronous implementation protocols. In particular, the mentioned class of
structures is shown to have desirable convergence and robustness properties
with respect to various uncertainties involving communication and processing
delays. Essential ingredients to the arguments herein involve blending together
functional composition methods, conservation principles, asynchronous signal
processing implementation protocols, and methods of homotopy. Numerical
experiments complement the theoretical presentation and connections to
optimization theory are made."
"This paper gives an overview of notations used in multiway array processing.
We redefine the vectorization and matricization operators to comply with some
properties of the Kronecker product. The tensor product and Kronecker product
are also represented with two different symbols, and it is shown how these
notations lead to clearer expressions for multiway array operations. Finally,
the paper recalls the useful yet widely unknown properties of the array normal
law with suggested notations."
"Prior optimal CUR decomposition and near optimal column reconstruction
methods have been established by combining BSS sampling and adaptive sampling.
In this paper, we propose a new approach to the optimal CUR decomposition and
near optimal column reconstruction by just using leverage score sampling. In
our approach, both the BSS sampling and adaptive sampling are not needed.
Moreover, our approach is the first $O(\mathrm{nnz}(\A))$ optimal CUR algorithm
where $\A$ is a data matrix in question. We also extend our approach to the
Nystr{\""o}m method, obtaining a fast algorithm which runs $\tilde{O}(n^{2})$ or
$O(\mathrm{\nnz}(\A))$"
"Kaczmarz algorithm is an efficient iterative algorithm to solve
overdetermined consistent system of linear equations. During each updating
step, Kaczmarz chooses a hyperplane based on an individual equation and
projects the current estimate for the exact solution onto that space to get a
new estimate. Many vairants of Kaczmarz algorithms are proposed on how to
choose better hyperplanes. Using the property of randomly sampled data in
high-dimensional space, we propose an accelerated algorithm based on clustering
information to improve block Kaczmarz and Kaczmarz via Johnson-Lindenstrauss
lemma. Additionally, we theoretically demonstrate convergence improvement on
block Kaczmarz algorithm."
"Significant inaccuracy often occurs during the process of mathematical
calculation due to the digit limitation of floating point, which may lead to
catastrophic loss. Normally, people believe that adjustment of floating-point
precision is an effective way to solve this problem, since high-precision
floating-point has more digits to store information. Thus, it is a prevalent
method to reduce the inaccuracy in much floating-point related research, that
performing all the operations with higher precision. However, we discover that
some operations may lead to larger error in higher precision. In this paper, we
define this kind of operation that generates large error due to precision
adjustment a precision-specific operation. Furthermore, we propose a
light-weight searching algorithm for detecting precision-specific operations
and figure out an automatic processing method to fixing them. In addition, we
conducted an experiment on the scientific mathematical library of GLIBC. The
result shows that there are many precision-specific operations, and our fixing
approach can significantly reduce the inaccuracy."
"We propose a new inertia-revealing factorization for sparse matrices. The
factorization scheme and the method for extracting the inertia from it were
proposed in the 1960s for dense, banded, or tridiagonal matrices, but they have
been abandoned in favor of faster methods. We show that this scheme can be
applied to any sparse matrix and that the fill in the factorization is bounded
by the fill in the sparse QR factorization of the same matrix (but is usually
much smaller). We present experimental results, studying the method's numerical
stability and performance. Our implementation of the method is somewhat naive,
but still demonstrates its potential."
"We consider the line spectral estimation problem which aims to recover a
mixture of complex sinusoids from a small number of randomly observed time
domain samples. Compressed sensing methods formulates line spectral estimation
as a sparse signal recovery problem by discretizing the continuous frequency
parameter space into a finite set of grid points. Discretization, however,
inevitably incurs errors and leads to deteriorated estimation performance. In
this paper, we propose a new method which leverages recent advances in tensor
decomposition. Specifically, we organize the observed data into a structured
tensor and cast line spectral estimation as a CANDECOMP/PARAFAC (CP)
decomposition problem with missing entries. The uniqueness of the CP
decomposition allows the frequency components to be super-resolved with
infinite precision. Simulation results show that the proposed method provides a
competitive estimate accuracy compared with existing state-of-the-art
algorithms."
"Algorithms of control of differential equations solutions are under
investigation in the article. Idealized and real modifications of the
algorithms are distinguished. An equation, which can be the base equation for
investigation of the idealized algorithms properties, is constructed. The
difference appearing for real systems and real algorithms is for separate
investigation. This difference tends to zero under tending to zero of the time
step of control. If the systems of equations satisfy or almost satisfy some
properties for which the algorithms are intended, then the results are similar
numerically as well. One of the algorithms demonstrates high reliability.
Another one is of more complex properties. Bifurcations, periodic solutions and
strange attractors are possible in both algorithms in addition to stable steady
states."
"This paper proposes a novel formulation of the tensor completion problem to
impute missing entries of data represented by tensors. The formulation is
introduced in terms of tensor train (TT) rank which can effectively capture
global information of tensors thanks to its construction by a well-balanced
matricization scheme. Two algorithms are proposed to solve the corresponding
tensor completion problem. The first one called simple low-rank tensor
completion via tensor train (SiLRTC-TT) is intimately related to minimizing the
TT nuclear norm. The second one is based on a multilinear matrix factorization
model to approximate the TT rank of the tensor and called tensor completion by
parallel matrix factorization via tensor train (TMac-TT). These algorithms are
applied to complete both synthetic and real world data tensors. Simulation
results of synthetic data show that the proposed algorithms are efficient in
estimating missing entries for tensors with either low Tucker rank or TT rank
while Tucker-based algorithms are only comparable in the case of low Tucker
rank tensors. When applied to recover color images represented by ninth-order
tensors augmented from third-order ones, the proposed algorithms outperforms
the Tucker-based algorithms."
"The phase separation processes are typically modeled by Cahn-Hilliard
equations. This equation was originally introduced to model phase separation in
binary alloys, where phase stands for concentration of different components in
alloy. When the binary alloy under preparation is subjected to a rapid
reduction in temperature below a critical temperature, it has been
experimentally observed that the concentration changes from a mixed state to a
visibly distinct spatially separated two phase for binary alloy. This rapid
reduction in the temperature, the so-called ""deep quench limit"", is modeled
effectively by obstacle potential. The discretization of Cahn-Hilliard equation
with obstacle potential leads to a block $2 \times 2$ {\em non-linear} system,
where the $(1,1)$ block has a non-linear and non-smooth term. Recently a
globally convergent Newton Schur method was proposed for the non-linear Schur
complement corresponding to this non-linear system. The proposed method is
similar to an inexact active set method in the sense that the active sets are
first approximately identified by solving a quadratic obstacle problem
corresponding to the $(1,1)$ block of the block $2 \times 2$ system, and later
solving a reduced linear system by annihilating the rows and columns
corresponding to identified active sets. For solving the quadratic obstacle
problem, various optimal multigrid like methods have been proposed. In this
paper, we study a non-standard norm that is equivalent to applying a block
diagonal preconditioner to the reduced linear systems. Numerical experiments
confirm the optimality of the solver and convergence independent of problem
parameters on sufficiently fine mesh."
"In the following paper we will consider Navier-Stokes problem and it's
interpretation by hyperbolic waves, focusing on wave propagation. We will begin
with solution for linear waves, then present problem for non-linear waves.
Later we will derive for numerical solution using PDE's. Also we will design a
Matlab program to solve and simulate wave propagation."
"Quadratic programmingis a class of constrained optimization problem with
quadratic objective functions and linear constraints. It has applications in
many areas and is also used to solve nonlinear optimization problems. This
article focuses on the equality constrained quadratic programs whose constraint
matrices are block diagonal. Using the direct solution method, we propose a new
pivot selection algorithm for the factorization of the Karush-Kuhn-Tucker(KKT)
matrix for this problem that maintains the sparsity and stability of the
problem. Our experiments show that our pivot selection algorithm appears to
produce no fill-ins in the factorizationof such matrices. In addition, we
compare our method with MA57 and find that the factors produced by our
algorithm are sparser."
"We propose a new pivot selection technique for symmetric indefinite
factorization of sparse matrices. Such factorization should maintain both
sparsity and numerical stability of the factors, both of which depend solely on
the choices of the pivots. Our method is based on the minimum degree algorithm
and also considers the stability of the factors at the same time. Our
experiments show that our method produces factors that are sparser than the
factors computed by MA57 and are stable."
"There has been a large increase in the amount of work on hierarchical
low-rank approximation methods, where the interest is shared by multiple
communities that previously did not intersect. This objective of this article
is two-fold; to provide a thorough review of the recent advancements in this
field from both analytical and algebraic perspectives, and to present a
comparative benchmark of two highly optimized implementations of contrasting
methods for some simple yet representative test cases. We categorize the recent
advances in this field from the perspective of compute-memory tradeoff, which
has not been considered in much detail in this area. Benchmark tests reveal
that there is a large difference in the memory consumption and performance
between the different methods."
"Toom-Cook multiprecision multiplication is a well-known multiprecision
multiplication method, which can make use of multiprocessor systems. In this
paper the Toom-Cook complexity is derived, some explicit proofs of the
Toom-Cook interpolation method are given, the even-odd method for interpolation
is explained, and certain aspects of a 32-bit C++ and assembler implementation,
which is in development, are discussed. A performance graph of this
implementation is provided. The Toom-Cook method can also be used to
multithread other types of multiplication, which is demonstrated for 32-bit GMP
FFT multiplication."
"We propose an online tensor subspace tracking algorithm based on the CP
decomposition exploiting the recursive least squares (RLS), dubbed OnLine
Low-rank Subspace tracking by TEnsor CP Decomposition (OLSTEC). Numerical
evaluations show that the proposed OLSTEC algorithm gives faster convergence
per iteration comparing with the state-of-the-art online algorithms."
"This article describes a method to accelerate parallel, explicit time
integration of two-dimensional unsteady PDEs. The method is motivated by our
observation that latency, not bandwidth, often limits how fast PDEs can be
solved in parallel. The method is called the swept rule of space-time domain
decomposition. Compared to conventional, space-only domain decomposition, it
communicates similar amount of data, but in fewer messages. The swept rule
achieves this by decomposing space and time among computing nodes in ways that
exploit the domains of influence and the domain of dependency, making it
possible to communicate once per many time steps with no redundant computation.
By communicating less often, the swept rule effectively breaks the latency
barrier, advancing on average more than one time step per ping-pong latency of
the network. The article presents simple theoretical analysis to the
performance of the swept rule in two spatial dimensions, and supports the
analysis with numerical experiments."
"In this paper we consider the matrix structure of arithmetic processors based
on distributed arithmetic in multi-row codes. Scope - development of
supercomputers."
"A parallel fast direct solver for rank-compressible block tridiagonal linear
systems is presented. Algorithmic synergies between Cyclic Reduction and
Hierarchical matrix arithmetic operations result in a solver with $O(N \log^2
N)$ arithmetic complexity and $O(N \log N)$ memory footprint. We provide a
baseline for performance and applicability by comparing with well known
implementations of the $\mathcal{H}$-LU factorization and algebraic multigrid
with a parallel implementation that leverages the concurrency features of the
method. Numerical experiments reveal that this method is comparable with other
fast direct solvers based on Hierarchical Matrices such as $\mathcal{H}$-LU and
that it can tackle problems where algebraic multigrid fails to converge."
"This paper examines the potential role of unit consistency as a system design
principle. Unit-consistent generalized matrix inverses and unit-invariant
matrix decompositions are derived in support of this principle. Applications of
the methods described are illustrated with examples relating to nonlinear
system identification and robustness to multiplicative noise for image database
retrieval."
"A new technique for approximating the entire solution set for a nonlinear
system of relations (nonlinear equations, inequalities, etc. involving
algebraic, smooth, or even continuous functions) is presented. The technique is
to first plot each function as a pixel matrix, and to then perform a sequence
of basic matrix operations, as dictated by how variables are shared by the
relations in the system. The result is a pixel matrix graphing the approximated
simultaneous solution set for the system."
"An important quantity associated with a complex polynomial $p(z)$ is $\Vert p
\Vert_\infty$, the maximum of its modulus over the unit disc $D$. We prove,
$z_* \in D$ is a local maximum of $|p(z)|$ if and only if $a_*$ satisfies,
$z_*=p(z_*)|p'(z_*)|/p'(z_*)|p(z_*)|$, i.e. it is proportional to its
corresponding Newton direction. This explicit formula gives rise to novel
iterative algorithms for computing $\Vert p \Vert_\infty$. We describe two such
algorithms, including a Newton-like method and present some visualization of
their performance."
"The main shortage of principle component analysis (PCA) based anomaly
detection models is their interpretability. In this paper, our goal is to
propose an interpretable PCA-based model for anomaly detection and
interpretation. The propose ASPCA model constructs principal components with
sparse and orthogonal loading vectors to represent the abnormal subspace, and
uses them to interpret detected anomalies. Our experiments on a synthetic
dataset and two real world datasets showed that the proposed ASPCA models
achieved comparable detection accuracies as the PCA model, and can provide
interpretations for individual anomalies."
"A square-root-free matrix QR decomposition (QRD) scheme was rederived in [1]
based on [2] to simplify computations when solving least-squares (LS) problems
on embedded systems. The scheme of [1] aims at eliminating both the square-root
and division operations in the QRD normalization and backward substitution
steps in the LS computations. It is claimed in [1] that the LS solution only
requires finding the directions of the orthogonal basis of the matrix in
question, regardless of the normalization of their Euclidean norms. MIMO
detection problems have been named as potential applications that benefit from
this. While this is true for unconstrained LS problems, we conversely show here
that constrained LS problems such as MIMO detection still require computing the
norms of the orthogonal basis to produce the correct result."
"This note provides a very short proof of a spectral gap independent property
of the simultaneous iterations algorithm for finding the top singular space of
a matrix. See Rokhlin-Szlam-Tygert-2009, Halko-Martinsson-Tropp-2011 and
Musco-Musco-2015. The proof is terse but completely self contained and should
be accessible to the linear algebra savvy reader."
"The traditional Karatsuba algorithm for the multiplication of polynomials and
multi-precision integers has a time complexity of $O(n^{1.59})$ and a space
complexity of $O(n)$. Roche proposed an improved algorithm with the same
$O(n^{1.59})$ time complexity but with a much reduced $O(\log n)$ space
complexity. In Roche's paper details were provided for multiplication of
polynomials, but not for multi-precision integers. Multi-precision integers
differ from polynomials by the presence of carries, which poses difficulties in
implementing Roche's scheme in multi-precision integers. This paper provides a
detailed solution to these difficulties. Finally, numerical comparisons between
the schoolbook, traditional Karatsuba, and space-efficient Karatsuba algorithms
are provided."
"In this paper, we present a Rank Revealing Randomized Singular Value
Decomposition (R3SVD) algorithm to incrementally construct a low-rank
approximation of a potentially large matrix while adaptively estimating the
appropriate rank that can capture most of the actions of the matrix. Starting
from a low-rank approximation with an initial guessed rank, R3SVD adopts an
orthogonal Gaussian sampling approach to obtain the dominant subspace within
the leftover space, which is used to add up to the existing low-rank
approximation. Orthogonal Gaussian sampling is repeated until an appropriate
low-rank approximation with satisfactory accuracy, measured by the overall
energy percentage of the original matrix, is obtained. While being a fast
algorithm, R3SVD is also a memory-aware algorithm where the computational
process can be decomposed into a series of sampling tasks that use constant
amount of memory. Numerical examples in image compression and matrix completion
are used to demonstrate the effectiveness of R3SVD in low-rank approximation."
"In this paper, we propose a novel reordering scheme to improve the
performance of a Laplacian Mesh Smoothing (LMS). While the Laplacian smoothing
algorithm is well optimized and studied, we show how a simple reordering of the
vertices of the mesh can greatly improve the execution time of the smoothing
algorithm. The idea of our reordering is based on (i) the postulate that cache
misses are a very time consuming part of the execution of LMS, and (ii) the
study of the reuse distance patterns of various executions of the LMS
algorithm.
  Our reordering algorithm is very simple but allows for huge performance
improvement. We ran it on a Westmere-EX platform and obtained a speedup of 75
on 32 cores compared to the single core execution without reordering, and a
gain in execution of 32% on 32 cores compared to state of the art reordering.
Finally, we show that we leave little room for a better ordering by reducing
the L2 and L3 cache misses to a bare minimum."
"This paper discusses an efficient parallel implementation of the ensemble
Kalman filter based on the modified Cholesky decomposition. The proposed
implementation starts with decomposing the domain into sub-domains. In each
sub-domain a sparse estimation of the inverse background error covariance
matrix is computed via a modified Cholesky decomposition; the estimates are
computed concurrently on separate processors. The sparsity of this estimator is
dictated by the conditional independence of model components for some radius of
influence. Then, the assimilation step is carried out in parallel without the
need of inter-processor communication. Once the local analysis states are
computed, the analysis sub-domains are mapped back onto the global domain to
obtain the analysis ensemble. Computational experiments are performed using the
Atmospheric General Circulation Model (SPEEDY) with the T-63 resolution on the
Blueridge cluster at Virginia Tech. The number of processors used in the
experiments ranges from 96 to 2,048. The proposed implementation outperforms in
terms of accuracy the well-known local ensemble transform Kalman filter (LETKF)
for all the model variables. The computational time of the proposed
implementation is similar to that of the parallel LETKF method (where no
covariance estimation is performed). Finally, for the largest number of
processors, the proposed parallel implementation is 400 times faster than the
serial version of the proposed method."
"Inaccurate circuits make possible the conservation of limited resources, such
as energy. But effective design of such circuits requires an understanding of
resulting tradeoffs between accuracy and design parameters, such as voltages
and speed of execution. Although studies of tradeoffs have been done on
specific circuits, the applicability of those studies is narrow. This paper
presents a comprehensive and mathematically rigorous method for analyzing a
large class of inaccurate circuits for addition. Furthermore, it presents new,
fast algorithms for the computation of key statistical measures of inaccuracy
in such adders, thus helping hardware architects explore the design space with
greater confidence."
"ADER schemes are numerical methods, which can reach an arbitrary order of
accuracy in both space and time. They are based on a reconstruction procedure
and the solution of generalized Riemann problems. However, for general boundary
conditions, in particular of Dirichlet type, a lack of accuracy might occur if
a suitable treatment of boundaries conditions is not properly carried out. In
this work the treatment of Dirichlet boundary conditions for conservation laws
in the context of ADER schemes, is concerned. The solution of generalized
Riemann problems at the extremes of the computational domain, provides the
correct influence of boundaries. The reconstruction procedure, for data near to
the boundaries, demands for information outside the computational domain, which
is carried out in terms of ghost cells, which are provided by using the
numerical solution of auxiliary problems. These auxiliary problems are
hyperbolic and they are constructed from the conservation laws and the
information at boundaries, which may be partially or totally known in terms of
prescribed functions. The evolution of these problems, unlike to the usual
manner, is done in space rather than in time due to that these problems are
named here, {\it reverse problems}. The methodology can be considered as a
numerical counterpart of the inverse Lax-Wendroff procedure for filling ghost
cells. However, the use of Taylor series expansions, as well as, Lax-Wendroff
procedure, are avoided. For the scalar case is shown that the present procedure
preserve the accuracy of the scheme which is reinforced with some numerical
results. Expected orders of accuracy for solving conservation laws by using the
proposed strategy at boundaries, are obtained up to fifth-order in both space
and time."
"In this work, we propose a nonlinear stabilization technique for scalar
conservation laws with implicit time stepping. The method relies on an
artificial diffusion method, based on a graph-Laplacian operator. It is
nonlinear, since it depends on a shock detector. The same shock detector is
used to gradually lump the mass matrix. The resulting method is LED, positivity
preserving, linearity preserving, and also satisfies a global DMP. Lipschitz
continuity has also been proved. However, the resulting scheme is highly
nonlinear, leading to very poor nonlinear convergence rates. We propose a
smooth version of the scheme, which leads to twice differentiable nonlinear
stabilization schemes. It allows one to straightforwardly use Newton's method
and obtain quadratic convergence. In the numerical experiments, steady and
transient linear transport, and transient Burgers' equation have been
considered in 2D. Using the Newton method with a smooth version of the scheme
we can reduce 10 to 20 times the number of iterations of Anderson acceleration
with the original non-smooth scheme. In any case, these properties are only
true for the converged solution, but not for iterates. In this sense, we have
also proposed the concept of projected nonlinear solvers, where a projection
step is performed at the end of every nonlinear iteration onto a FE space of
admissible solutions. The space of admissible solutions is the one that
satisfies the desired monotonic properties (maximum principle or positivity)."
"Accompanied with the rising popularity of compressed sensing, the Alternating
Direction Method of Multipliers (ADMM) has become the most widely used solver
for linearly constrained convex problems with separable objectives. In this
work, we observe that many previous variants of ADMM update the primal variable
by minimizing different majorant functions with their convergence proofs given
case by case. Inspired by the principle of majorization minimization, we
respectively present the unified frameworks and convergence analysis for the
Gauss-Seidel ADMMs and Jacobian ADMMs, which use different historical
information for the current updating. Our frameworks further generalize
previous ADMMs to the ones capable of solving the problems with non-separable
objectives by minimizing their separable majorant surrogates. We also show that
the bound which measures the convergence speed of ADMMs depends on the
tightness of the used majorant function. Then several techniques are introduced
to improve the efficiency of ADMMs by tightening the majorant functions. In
particular, we propose the Mixed Gauss-Seidel and Jacobian ADMM (M-ADMM) which
alleviates the slow convergence issue of Jacobian ADMMs by absorbing merits of
the Gauss-Seidel ADMMs. M-ADMM can be further improved by using backtracking,
wise variable partition and fully exploiting the structure of the constraint.
Beyond the guarantee in theory, numerical experiments on both synthesized and
real-world data further demonstrate the superiority of our new ADMMs in
practice. Finally, we release a toolbox at https://github.com/canyilu/LibADMM
that implements efficient ADMMs for many problems in compressed sensing."
"Processes to automate the selection of appropriate algorithms for various
matrix computations are described. In particular, processes to check for, and
certify, various matrix properties of black box matrices are presented. These
include sparsity patterns and structural properties that allow ""superfast""
algorithms to be used in place of black-box algorithms. Matrix properties that
hold generically, and allow the use of matrix preconditioning to be reduced or
eliminated, can also be checked for and certified - notably including in the
small-field case, where this presently has the greatest impact on the
efficiency of the computation."
"Wiedemann's paper, introducing his algorithm for sparse and structured matrix
computations over arbitrary fields, also presented a pair of matrix
preconditioners for computations over small fields. The analysis of the second
of these is extended in order to provide more explicit statements of the
expected number of nonzero entries in the matrices obtained as well as bounds
on the probability that such matrices have maximal rank.
  This is part of ongoing work to establish that this matrix preconditioner can
also be used to bound the number of nontrivial nilpotent blocks in the Jordan
normal form of a preconditioned matrix, in such a way that one can also sample
uniformly from the null space of the originally given matrix. If successful
this will result in a black box algorithm for the type of matrix computation
required when using the number field sieve for integer factorization that is
provably reliable and - by a small factor - asymptotically more efficient than
alternative techniques that make use of other matrix preconditioners or require
computations over field extensions."
"The Kaczmarz algorithm is a well known iterative method for solving
overdetermined linear systems. Its randomized version yields provably
exponential convergence in expectation. In this paper, we propose two new
methods to speed up the randomized Kaczmarz algorithm by utilizing the past
estimates in the iterations. The first one utilize the past estimates to get a
preconditioner. The second one combines the stochastic average gradient (SAG)
method with the randomized Kaczmarz algorithm. It takes advantage of past
gradients to improve the convergence speed. Numerical experiments indicate that
the new algorithms can dramatically outperform the standard randomized Kaczmarz
algorithm."
"The Square Wave Method (SWM), previously introduced for the analysis of
signals and images, is presented here as a mathematical tool suitable for the
analysis of time series and signals. To show the potential that the SWM has to
analyze many different types of time series, the results of the analysis of a
time series composed of a sequence of 10,000 numerical values are presented
here. These values were generated by using the Mathematical Random Number
Generator (MRNG)."
"We propose a simple algorithm devoted to locate the ""corner"" of an L-curve, a
function often used to chose the correct regularization parameter for the
solution of ill-posed problems. The algorithm involves the Menger curvature of
a circumcircle and the golden section search method. It efficiently locates the
regularization parameter value corresponding to the maximum positive curvature
region of the L-curve. As an example, the application of the algorithm to the
data processing of an electrical resistance tomography experiment on thin
conductive films is reported."
"Machine learning and data mining algorithms are becoming increasingly
important in analyzing large volume, multi-relational and multi--modal
datasets, which are often conveniently represented as multiway arrays or
tensors. It is therefore timely and valuable for the multidisciplinary research
community to review tensor decompositions and tensor networks as emerging tools
for large-scale data analysis and data mining. We provide the mathematical and
graphical representations and interpretation of tensor networks, with the main
focus on the Tucker and Tensor Train (TT) decompositions and their extensions
or generalizations.
  Keywords: Tensor networks, Function-related tensors, CP decomposition, Tucker
models, tensor train (TT) decompositions, matrix product states (MPS), matrix
product operators (MPO), basic tensor operations, multiway component analysis,
multilinear blind source separation, tensor completion, linear/multilinear
dimensionality reduction, large-scale optimization problems, symmetric
eigenvalue decomposition (EVD), PCA/SVD, huge systems of linear equations,
pseudo-inverse of very large matrices, Lasso and Canonical Correlation Analysis
(CCA) (This is Part 1)"
"Generalized matrix approximation plays a fundamental role in many machine
learning problems, such as CUR decomposition, kernel approximation, and matrix
low rank approximation. Especially with today's applications involved in larger
and larger dataset, more and more efficient generalized matrix approximation
algorithems become a crucially important research issue. In this paper, we find
new sketching techniques to reduce the size of the original data matrix to
develop new matrix approximation algorithms. Our results derive a much tighter
bound for the approximation than previous works: we obtain a $(1+\epsilon)$
approximation ratio with small sketched dimensions which implies a more
efficient generalized matrix approximation."
"This article provides a new type of analysis of a compressed-sensing based
technique for recovering column-sparse matrices, namely minimization of the
$\ell_{1,2}$-norm. Rather than providing conditions on the measurement matrix
which guarantees the solution of the program to be exactly equal to the ground
truth signal (which already has been thoroughly investigated), it presents a
condition which guarantees that the solution is approximately equal to the
ground truth. Soft recovery statements of this kind are to the best knowledge
of the author a novelty in Compressed Sensing. Apart from the theoretical
analysis, we present two heuristic proposes how this property of the
$\ell_{1,2}$-program can be utilized to design algorithms for recovery of
matrices which are sparse and have low rank at the same time."
"Runtime characteristics of sparse matrix computations and related processes
may be often improved by reducing memory footprints of involved matrices. Such
a reduction can be usually achieved when matrices are processed in a block-wise
manner. The presented study analysed memory footprints of 563 representative
benchmark sparse matrices with respect to their partitioning into
uniformly-sized blocks. Different block sizes and different ways of storing
blocks in memory were considered and statistically evaluated. Memory footprints
of partitioned matrices were additionally compared with lower bounds and with
the CSR storage format. The average measured memory savings against CSR in case
of single and double precision were 42.3 and 28.7 percents, the corresponding
worst-case savings 25.5 and 17.1 percents. Moreover, memory footprints of
partitioned matrices were in average 5 times closer to their lower bounds than
CSR. Based on the obtained results, generic suggestions for efficient
partitioning and storage of sparse matrices in a computer memory are provided."
"Certain applications such as Magnetic Resonance Imaging (MRI) require the
reconstruction of functions from Fourier spectral data. When the underlying
functions are piecewise-smooth, standard Fourier approximation methods suffer
from the Gibbs phenomenon - with associated oscillatory artifacts in the
vicinity of edges and an overall reduced order of convergence in the
approximation. This paper proposes an edge-augmented Fourier reconstruction
procedure which uses only the first few Fourier coefficients of an underlying
piecewise-smooth function to accurately estimate jump information and then
incorporate it into a Fourier partial sum approximation. We provide both
theoretical and empirical results showing the improved accuracy of the proposed
method, as well as comparisons demonstrating superior performance over existing
state-of-the-art sparse optimization-based methods. Extensions of the proposed
techniques to functions of several variables are also addressed preliminarily.
All code used to generate the results in this report are made publicly
available."
"In this paper we consider large linear fixed point problems and solution with
proximal algorithms. We show that, under certain assumptions, there is a close
connection between proximal iterations, which are prominent in numerical
analysis and optimization, and multistep methods of the temporal difference
type such as TD(lambda), LSTD(lambda), and LSPE(lambda), which are central in
simulation-based approximate dynamic programming. As an application of this
connection, we show that we may accelerate the standard proximal algorithm by
extrapolation towards the multistep iteration, which generically has a faster
convergence rate. We also use the connection with multistep methods to
integrate into the proximal algorithmic context several new ideas that have
emerged in the approximate dynamic programming context. In particular, we
consider algorithms that project each proximal iterate onto the subspace
spanned by a small number of basis functions, using low-dimensional
calculations and simulation, and we discuss various algorithmic options from
approximate dynamic programming."
"Floating point error is an inevitable drawback of embedded systems
implementation. Computing rigorous upper bounds of roundoff errors is
absolutely necessary to the validation of critical software. This problem is
even more challenging when addressing non-linear programs. In this paper, we
propose and compare two new methods based on Bernstein expansions and sparse
Krivine-Stengle representations, adapted from the field of the global
optimization to compute upper bounds of roundoff errors for programs
implementing polynomial functions. We release two related software package
FPBern and FPKiSten, and compare them with state of the art tools. We show that
these two methods achieve competitive performance, while computing accurate
upper bounds by comparison with other tools."
"In this paper, we revisit the generalized block power methods for
approximating the eigenvector associated with $\lambda_1 = 1$ of a Markov chain
transition matrix. Our analysis of the block power method shows that when $s$
linearly independent probability vectors are used as the initial block, the
convergence of the block power method to the stationary distribution depends on
the magnitude of the $(s+1)$th dominant eigenvalue $\lambda_{s+1}$ of $P$
instead of that of $\lambda_2$ in the power method. Therefore, the block power
method with block size $s$ is particularly effective for transition matrices
where $|\lambda_{s+1}|$ is well separated from $\lambda_1 = 1$ but
$|\lambda_2|$ is not. This approach is particularly useful when visiting the
elements of a large transition matrix is the main computational bottleneck over
matrix--vector multiplications, where the block power method can effectively
reduce the total number of times to pass over the matrix. To further reduce the
overall computational cost, we combine the block power method with a sliding
window scheme, taking advantage of the subsequent vectors of the latest $s$
iterations to assemble the block matrix. The sliding window scheme correlates
vectors in the sliding window to quickly remove the influences from the
eigenvalues whose magnitudes are smaller than $|\lambda_{s}|$ to reduce the
overall number of matrix--vector multiplications to reach convergence. Finally,
we compare the effectiveness of these methods in a Markov chain model
representing a stochastic luminal calcium release site."
"This work proposes a data-driven method for enabling the efficient, stable
time-parallel numerical solution of systems of ordinary differential equations
(ODEs). The method assumes that low-dimensional bases that accurately capture
the time evolution of the state are available. The method adopts the parareal
framework for time parallelism, which is defined by an initialization method, a
coarse propagator, and a fine propagator. Rather than employing usual
approaches for initialization and coarse propagation, we propose novel
data-driven techniques that leverage the available time-evolution bases. The
coarse propagator is defined by a forecast (proposed in Ref. [12]) applied
locally within each coarse time interval, which comprises the following steps:
(1) apply the fine propagator for a small number of time steps, (2) approximate
the state over the entire coarse time interval using gappy POD with the local
time-evolution bases, and (3) select the approximation at the end of the time
interval as the propagated state. We also propose both local-forecast and
global-forecast initialization. The method is particularly well suited for
POD-based reduced-order models (ROMs). In this case, spatial parallelism
quickly saturates, as the ROM dynamical system is low dimensional; thus, time
parallelism is needed to enable lower wall times. Further, the time-evolution
bases can be extracted from the (readily available) right singular vectors
arising during POD computation. In addition to performing analyses related to
the method's accuracy, speedup, stability, and convergence, we also numerically
demonstrate the method's performance. Here, numerical experiments on ROMs for a
nonlinear convection-reaction problem demonstrate the method's ability to
realize near-ideal speedups; global-forecast initialization with a
local-forecast coarse propagator leads to the best performance."
"We study numerical (in)stability of the Method of characteristics (MoC)
applied to a system of non-dissipative hyperbolic partial differential
equations (PDEs) with periodic boundary conditions. We consider three different
solvers along the characteristics: simple Euler (SE), modified Euler (ME), and
Leap-frog (LF). The two former solvers are well known to exhibit a mild, but
unconditional, numerical instability for non-dissipative ordinary differential
equations (ODEs). They are found to have a similar (or stronger, for the
MoC-ME) instability when applied to non-dissipative PDEs. On the other hand,
the LF solver is known to be stable when applied to non-dissipative ODEs.
However, when applied to non-dissipative PDEs within the MoC framework, it was
found to have by far the strongest instability among all three solvers. We also
comment on the use of the fourth-order Runge--Kutta solver within the MoC
framework."
"We show that imposition of non-periodic, in place of periodic, boundary
conditions (BC) can alter stability of modes in the Method of characteristics
(MoC) employing certain ordinary-differential equation (ODE) numerical solvers.
Thus, using non-periodic BC may render some of the MoC schemes stable for most
practical computations, even though they are unstable for periodic BC. This
fact contradicts a statement, found in some literature, that an instability
detected by the von Neumann analysis for a given numerical scheme implies an
instability of that scheme with arbitrary (i.e., non-periodic) BC. We explain
the mechanism behind this contradiction. We also show that, and explain why,
for the MoC employing some other ODE solvers, stability of the modes may be
unaffected by the BC."
"In this paper we consider various splitting schemes for unsteady problems
containing the grad-div operator. The fully implicit discretization of such
problems would yield at each time step a linear problem that couples all
components of the solution vector. In this paper we discuss various
possibilities to decouple the equations for the different components that
result in unconditionally stable schemes. If the spatial discretization uses
Cartesian grids, the resulting schemes are Locally One Dimensional (LOD). The
stability analysis of these schemes is based on the general stability theory of
additive operator-difference schemes developed by Samarskii and his
collaborators. The results of the theoretical analysis are illustrated on a 2D
numerical example with a smooth manufactured solution."
"Non-uniform fast Fourier Transform (NUFFT) and inverse NUFFT (INUFFT)
algorithms, based on the Fast Multipole Method (FMM) are developed and tested.
Our algorithms are based on a novel factorization of the FFT kernel, and are
implemented with attention to data structures and error analysis.
  Note: This unpublished manuscript was available on our web pages and has been
referred to by others in the literature. To provide a proper archival reference
we are placing it on arXiv."
"Computational techniques are extensively applied in nonlinear science.
However, while the use of computers for research has been expressive, the
evaluation of numerical results does not grow in the same pace. Hammel et al.
(Journal of Complexity, 1987, 3(2), 136--145) were pioneers in the numerical
reliability field and have proved a theorem that a pseudo-orbit of a logistic
map is shadowed by a true orbit within a distance of $10^{-8}$ for $10^{7}$
iterates. But the simulation of the logistic map with less than 100 iterates
presents an error greater than $10^{-8}$ in a modern computer, performing a
test based on the concept of multiple pseudo-orbits and symbolic computing."
"The Fast Fourier Transform (FFT) is an algorithm of paramount importance in
signal processing as it allows to apply the Fourier transform in O(n log n)
instead of O(n 2) arithmetic operations. Graph Signal Processing (GSP) is a
recent research domain that generalizes classical signal processing tools, such
as the Fourier transform, to situations where the signal domain is given by any
arbitrary graph instead of a regular grid. Today, there is no method to rapidly
apply graph Fourier transforms. We propose in this paper a method to obtain
approximate graph Fourier transforms that can be applied rapidly and stored
efficiently. It is based on a greedy approximate diagonalization of the graph
Laplacian matrix, carried out using a modified version of the famous Jacobi
eigenvalues algorithm. The method is described and analyzed in detail, and then
applied to both synthetic and real graphs, showing its potential."
"Design of filters for graph signal processing benefits from knowledge of the
spectral decomposition of matrices that encode graphs, such as the adjacency
matrix and the Laplacian matrix, used to define the shift operator. For shift
matrices with real eigenvalues, which arise for symmetric graphs, the empirical
spectral distribution captures the eigenvalue locations. Under realistic
circumstances, stochastic influences often affect the network structure and,
consequently, the shift matrix empirical spectral distribution. Nevertheless,
deterministic functions may often be found to approximate the asymptotic
behavior of empirical spectral distributions of random matrices. This paper
uses stochastic canonical equation methods developed by Girko to derive such
deterministic equivalent distributions for the empirical spectral distributions
of random graphs formed by structured, non-uniform percolation of a
D-dimensional lattice supergraph. Included simulations demonstrate the results
for sample parameters."
"This paper introduces multivariate input-output models to predict the errors
and bases dimensions of local parametric Proper Orthogonal Decomposition
reduced-order models. We refer to these multivariate mappings as the MP-LROM
models. We employ Gaussian Processes and Artificial Neural Networks to
construct approximations of these multivariate mappings. Numerical results with
a viscous Burgers model illustrate the performance and potential of the machine
learning based regression MP-LROM models to approximate the characteristics of
parametric local reduced-order models. The predicted reduced-order models
errors are compared against the multi-fidelity correction and reduced order
model error surrogates methods predictions, whereas the predicted reduced-order
dimensions are tested against the standard method based on the spectrum of
snapshots matrix. Since the MP-LROM models incorporate more features and
elements to construct the probabilistic mappings they achieve more accurate
results. However, for high-dimensional parametric spaces, the MP-LROM models
might suffer from the curse of dimensionality. Scalability challenges of
MP-LROM models and the feasible ways of addressing them are also discussed in
this study."
"Exponential integrators are special time discretization methods where the
traditional linear system solves used by implicit schemes are replaced with
computing the action of matrix exponential-like functions on a vector. A very
general formulation of exponential integrators is offered by the Exponential
Propagation Iterative methods of Runge-Kutta type (EPIRK) family of schemes.
The use of Jacobian approximations is an important strategy to drastically
reduce the overall computational costs of implicit schemes while maintaining
the quality of their solutions. This paper extends the EPIRK class to allow the
use of inexact Jacobians as arguments of the matrix exponential-like functions.
Specifically, we develop two new families of methods: EPIRK-W integrators that
can accommodate any approximation of the Jacobian, and EPIRK-K integrators that
rely on a specific Krylov-subspace projection of the exact Jacobian. Classical
order conditions theories are constructed for these families. A practical
EPIRK-W method of order three and an EPIRK-K method of order four are
developed. Numerical experiments indicate that the methods proposed herein are
computationally favorable when compared to existing exponential integrators."
"The CANDECOMP/PARAFAC (CP) decomposition is a leading method for the analysis
of multiway data. The standard alternating least squares algorithm for the CP
decomposition (CP-ALS) involves a series of highly overdetermined linear least
squares problems. We extend randomized least squares methods to tensors and
show the workload of CP-ALS can be drastically reduced without a sacrifice in
quality. We introduce techniques for efficiently preprocessing, sampling, and
computing randomized least squares on a dense tensor of arbitrary order, as
well as an efficient sampling-based technique for checking the stopping
condition. We also show more generally that the Khatri-Rao product (used within
the CP-ALS iteration) produces conditions favorable for direct sampling. In
numerical results, we see improvements in speed, reductions in memory
requirements, and robustness with respect to initialization."
"Many machine learning models are reformulated as optimization problems. Thus,
it is important to solve a large-scale optimization problem in big data
applications. Recently, subsampled Newton methods have emerged to attract much
attention for optimization due to their efficiency at each iteration, rectified
a weakness in the ordinary Newton method of suffering a high cost in each
iteration while commanding a high convergence rate. Other efficient stochastic
second order methods are also proposed. However, the convergence properties of
these methods are still not well understood. There are also several important
gaps between the current convergence theory and the performance in real
applications. In this paper, we aim to fill these gaps. We propose a unifying
framework to analyze local convergence properties of second order methods.
Based on this framework, our theoretical analysis matches the performance in
real applications."
"The FLAME methodology makes it possible to derive provably correct algorithms
from a formal description of a linear algebra problem. So far, the methodology
has been successfully used to automate the derivation of direct algorithms such
as the Cholesky decomposition and the solution of Sylvester equations. In this
thesis, we present an extension of the FLAME methodology to tackle iterative
methods such as Conjugate Gradient. As a starting point, we use a formal
description of the iterative method in matrix form. The result is a family of
provably correct pseudocode algorithms. We argue that all the intermediate
steps are sufficiently systematic to be fully automated."
"Approximate computing has shown to provide new ways to improve performance
and power consumption of error-resilient applications. While many of these
applications can be found in image processing, data classification or machine
learning, we demonstrate its suitability to a problem from scientific
computing. Utilizing the self-correcting behavior of iterative algorithms, we
show that approximate computing can be applied to the calculation of inverse
matrix p-th roots which are required in many applications in scientific
computing. Results show great opportunities to reduce the computational effort
and bandwidth required for the execution of the discussed algorithm, especially
when targeting special accelerator hardware."
"This work proposes a space-time least-squares Petrov-Galerkin (ST-LSPG)
projection method for model reduction of nonlinear dynamical systems. In
contrast to typical nonlinear model-reduction methods that first apply
(Petrov-)Galerkin projection in the spatial dimension and subsequently apply
time integration to numerically resolve the resulting low-dimensional dynamical
system, the proposed method applies projection in space and time
simultaneously. To accomplish this, the method first introduces a
low-dimensional space-time trial subspace, which can be obtained by computing
tensor decompositions of state-snapshot data. The method then computes
discrete-optimal approximations in this space-time trial subspace by minimizing
the residual arising after time discretization over all space and time in a
weighted $\ell^2$-norm. This norm can be defined to enable complexity reduction
(i.e., hyper-reduction) in time, which leads to space-time collocation and
space-time GNAT variants of the ST-LSPG method. Advantages of the approach
relative to typical spatial-projection-based nonlinear model reduction methods
such as Galerkin projection and least-squares Petrov-Galerkin projection
include: (1) a reduction of both the spatial and temporal dimensions of the
dynamical system, (2) the removal of spurious temporal modes (e.g., unstable
growth) from the state space, and (2) error bounds that exhibit slower growth
in time. Numerical examples performed on model problems in fluid dynamics
demonstrate the ability of the method to generate orders-of-magnitude
computational savings relative to spatial-projection-based reduced-order models
without sacrificing accuracy."
"Matrix and tensor completion aim to recover a low-rank matrix / tensor from
limited observations and have been commonly used in applications such as
recommender systems and multi-relational data mining. A state-of-the-art matrix
completion algorithm is Soft-Impute, which exploits the special ""sparse plus
low-rank"" structure of the matrix iterates to allow efficient SVD in each
iteration. Though Soft-Impute is a proximal algorithm, it is generally believed
that acceleration destroys the special structure and is thus not useful. In
this paper, we show that Soft-Impute can indeed be accelerated without
comprising this structure. To further reduce the iteration time complexity, we
propose an approximate singular value thresholding scheme based on the power
method. Theoretical analysis shows that the proposed algorithm still enjoys the
fast $O(1/T^2)$ convergence rate of accelerated proximal algorithms. We further
extend the proposed algorithm to tensor completion with the scaled latent
nuclear norm regularizer. We show that a similar ""sparse plus low-rank""
structure also exists, leading to low iteration complexity and fast $O(1/T^2)$
convergence rate. Extensive experiments demonstrate that the proposed algorithm
is much faster than Soft-Impute and other state-of-the-art matrix and tensor
completion algorithms."
"In this work, we propose a subspace-based algorithm for direction-of-arrival
(DOA) estimation, referred to as two-step knowledge-aided iterative estimation
of signal parameters via rotational invariance techniques (ESPRIT) method
(Two-Step KAI-ESPRIT), which achieves more accurate estimates than those of
prior art. The proposed Two-Step KAI-ESPRIT improves the estimation of the
covariance matrix of the input data by incorporating prior knowledge of signals
and by exploiting knowledge of the structure of the covariance matrix and its
perturbation terms. Simulation results illustrate the improvement achieved by
the proposed method."
"In this paper, we propose a novel unfitted finite element method for the
simulation of multiple body contact. The computational mesh is generated
independently of the geometry of the interacting solids, which can be
arbitrarily complex. The key novelty of the approach is the combination of
elements of the CutFEM technology, namely the enrichment of the solution field
via the definition of overlapping fictitious domains with a dedicated
penalty-type regularisation of discrete operators, and the LaTIn hybrid-mixed
formulation of complex interface conditions. Furthermore, the novel P1-P1
discretisation scheme that we propose for the unfitted LaTIn solver is shown to
be stable, robust and optimally convergent with mesh refinement. Finally, the
paper introduces a high-performance 3D level-set/CutFEM framework for the
versatile and robust solution of contact problems involving multiple bodies of
complex geometries, with more than two bodies interacting at a single point."
"In this short note, we present a novel method for computing exact lower and
upper bounds of a symmetric tridiagonal interval matrix. Compared to the known
methods, our approach is fast, simple to present and to implement, and avoids
any assumptions Our construction explicitly yields those matrices for which
particular lower and upper bounds are attained."
"In this paper, we present a fast implementation of the Singular Value
Thresholding (SVT) algorithm for matrix completion. A rank-revealing randomized
singular value decomposition (R3SVD) algorithm is used to adaptively carry out
partial singular value decomposition (SVD) to fast approximate the SVT operator
given a desired, fixed precision. We extend the R3SVD algorithm to a recycling
rank revealing randomized singular value decomposition (R4SVD) algorithm by
reusing the left singular vectors obtained from the previous iteration as the
approximate basis in the current iteration, where the computational cost for
partial SVD at each SVT iteration is significantly reduced. A simulated
annealing style cooling mechanism is employed to adaptively adjust the low-rank
approximation precision threshold as SVT progresses. Our fast SVT
implementation is effective in both large and small matrices, which is
demonstrated in matrix completion applications including image recovery and
movie recommendation system."
"We consider a symmetric matrix, the entries of which depend linearly on some
parameters. The domains of the parameters are compact real intervals. We
investigate the problem of checking whether for each (or some) setting of the
parameters, the matrix is positive definite (or positive semidefinite). We
state a characterization in the form of equivalent conditions, and also propose
some computationally cheap sufficient\,/\,necessary conditions. Our results
extend the classical results on positive (semi-)definiteness of interval
matrices. They may be useful for checking convexity or non-convexity in global
optimization methods based on branch and bound framework and using interval
techniques."
"The widespread use of multisensor technology and the emergence of big data
sets have created the necessity to develop more versatile tools to represent
large and multimodal data such as higher-order tensors. Tensor decomposition
based methods have been shown to be flexible in the choice of the constraints
and to extract more general latent components in such data compared to
matrix-based methods. For these reasons, tensor decompositions have found
applications in many different signal processing problems including
dimensionality reduction, signal separation, linear regression, feature
extraction, and classification. However, most of the existing tensor
decomposition methods are founded on the principle of finding a low-rank
approximation in a linear subspace structure, where the definition of the rank
may change depending on the particular decomposition. Since most data are not
necessarily low-rank in a linear subspace, this often results in high
approximation errors or low compression rates. In this paper, we introduce a
new adaptive, multi-scale tensor decomposition method for higher order data
inspired by hybrid linear modeling and subspace clustering techniques. In
particular, we develop a multi-scale higher-order singular value decomposition
(MS-HoSVD) approach where a given tensor is first permuted and then partitioned
into several sub-tensors each of which can be represented as a low-rank tensor
increasing the efficiency of the representation. The proposed approach is
evaluated for two different signal processing applications: dimensionality
reduction and classification."
"An iteration-free method of domain decomposition is considered for
approximate solving a boundary value problem for a second-order parabolic
equation. A standard approach to constructing domain decomposition schemes is
based on a partition of unity for the domain under the consideration. Here a
new general approach is proposed for constructing domain decomposition schemes
with overlapping subdomains based on indicator functions of subdomains. The
basic peculiarity of this method is connected with a representation of the
problem operator as the sum of two operators, which are constructed for two
separate subdomains with the subtraction of the operator that is associated
with the intersection of the subdomains. There is developed a two-component
factorized scheme, which can be treated as a generalization of the standard
Alternating Direction Implicit (ADI) schemes to the case of a special
three-component splitting. There are obtained conditions for the unconditional
stability of regionally additive schemes constructed using indicator functions
of subdomains. Numerical results are presented for a model two-dimensional
problem."
"We present a cache-oblivious adaptation of matrix multiplication to be
incorporated in the parallel TU decomposition for rectangular matrices over
finite fields, based on the Morton-hybrid space-filling curve representation.
To realise this, we introduce the concepts of alignment and containment of
sub-matrices under the Morton-hybrid layout. We redesign the decompositions
within the recursive matrix multiplication to force the base case to avoid all
jumps in address space, at the expense of extra recursive matrix multiplication
(MM) calls. We show that the resulting cache oblivious adaptation has low span,
and our experiments demonstrate that its sequential evaluation order
demonstrates orders of magnitude improvement in run-time, despite the recursion
overhead."
"The problem of increasing the accuracy of an approximate solution is
considered for boundary value problems for parabolic equations. For ordinary
differential equations (ODEs), nonstandard finite difference schemes are in
common use for this problem. They are based on a modification of standard
discretizations of time derivatives and, in some cases, allow to obtain the
exact solution of problems. For multidimensional problems, we can consider the
problem of increasing the accuracy only for the most important components of
the approximate solution. In the present work, new unconditionally stable
schemes for parabolic problems are constructed, which are exact for the
fundamental mode. Such two-level schemes are designed via a modification of
standard schemes with weights using Pad\'{e} approximations. Numerical results
obtained for a model problem demonstrate advantages of the proposed fundamental
mode exact schemes."
"Tensor train (TT) decomposition is a powerful representation for high-order
tensors, which has been successfully applied to various machine learning tasks
in recent years. However, since the tensor product is not commutative,
permutation of data dimensions makes solutions and TT-ranks of TT decomposition
inconsistent. To alleviate this problem, we propose a permutation symmetric
network structure by employing circular multilinear products over a sequence of
low-order core tensors. This network structure can be graphically interpreted
as a cyclic interconnection of tensors, and thus we call it tensor ring (TR)
representation. We develop several efficient algorithms to learn TR
representation with adaptive TR-ranks by employing low-rank approximations.
Furthermore, mathematical properties are investigated, which enables us to
perform basic operations in a computationally efficiently way by using TR
representations. Experimental results on synthetic signals and real-world
datasets demonstrate that the proposed TR network is more expressive and
consistently informative than existing TT networks."
"This work is focused on the application of functional-type a posteriori error
estimates and corresponding indicators to a class of time-dependent problems.
We consider the algorithmic part of their derivation and implementation and
also discuss the numerical properties of these bounds that comply with obtained
numerical results. This paper examines two different methods of approximate
solution reconstruction for evolutionary models, i.e., a time-marching
technique and a space-time approach. The first part of the study presents an
algorithm for global minimization of the majorant on each of discretization
time-cylinders (time-slabs), the effectiveness of this algorithm is confirmed
by extensive numerical tests. In the second part of the publication, the
application of functional error estimates is discussed with respect to a
space-time approach. It is followed by a set of extensive numerical tests that
demonstrate the efficiency of proposed error control method."
"Many idealized problems in signal processing, machine learning and statistics
can be reduced to the problem of finding the symmetric canonical decomposition
of an underlying symmetric and orthogonally decomposable (SOD) tensor. Drawing
inspiration from the matrix case, the successive rank-one approximations (SROA)
scheme has been proposed and shown to yield this tensor decomposition exactly,
and a plethora of numerical methods have thus been developed for the tensor
rank-one approximation problem. In practice, however, the inevitable errors
(say) from estimation, computation, and modeling necessitate that the input
tensor can only be assumed to be a nearly SOD tensor---i.e., a symmetric tensor
slightly perturbed from the underlying SOD tensor. This article shows that even
in the presence of perturbation, SROA can still robustly recover the symmetric
canonical decomposition of the underlying tensor. It is shown that when the
perturbation error is small enough, the approximation errors do not accumulate
with the iteration number. Numerical results are presented to support the
theoretical findings."
"A new concept for the higher-order accurate approximation of partial
differential equations on manifolds is proposed where a surface mesh composed
by higher-order elements is automatically generated based on level-set data.
Thereby, it enables a completely automatic workflow from the geometric
description to the numerical analysis without any user-intervention. A master
level-set function defines the shape of the manifold through its
zero-isosurface which is then restricted to a finite domain by additional
level-set functions. It is ensured that the surface elements are sufficiently
continuous and shape regular which is achieved by manipulating the background
mesh. The numerical results show that optimal convergence rates are obtained
with a moderate increase in the condition number compared to handcrafted
surface meshes."
"A higher-order accurate finite element method is proposed which uses
automatically generated meshes based on implicit level-set data for the
description of boundaries and interfaces in two and three dimensions. The
method is an alternative for fictitious domain and extended finite element
methods. The domain of interest is immersed in a background mesh composed by
higher-order elements. The zero-level sets are identified and meshed followed
by a decomposition of the cut background elements into conforming sub-elements.
Adaptivity is a crucial ingredient of the method to guarantee the success of
the mesh generation. It ensures the successful decomposition of cut elements
and enables improved geometry descriptions and approximations. It is confirmed
that higher-order accurate results with optimal convergence rates are achieved
with the proposed conformal decomposition finite element method (CDFEM)."
"Finding the symmetric and orthogonal decomposition (SOD) of a tensor is a
recurring problem in signal processing, machine learning and statistics. In
this paper, we review, establish and compare the perturbation bounds for two
natural types of incremental rank-one approximation approaches. Numerical
experiments and open questions are also presented and discussed."
"By reducing the number of global synchronization bottlenecks per iteration
and hiding communication behind useful computational work, pipelined Krylov
subspace methods achieve significantly improved parallel scalability on
present-day HPC hardware. However, this typically comes at the cost of a
reduced maximal attainable accuracy. This paper presents and compares several
stabilized versions of the communication-hiding pipelined Conjugate Gradients
method. The main novel contribution of this work is the reformulation of the
multi-term recurrence pipelined CG algorithm by introducing shifts in the
recursions for specific auxiliary variables. These shifts reduce the
amplification of local rounding errors on the residual. The stability analysis
presented in this work provides a rigorous method for selection of the optimal
shift value in practice. It is shown that, given a proper choice for the shift
parameter, the resulting shifted pipelined CG algorithm restores the attainable
accuracy and displays nearly identical robustness to local rounding error
propagation compared to classical CG. Numerical results on a variety of SPD
benchmark problems compare different stabilization techniques for the pipelined
CG algorithm, showing that the shifted pipelined CG algorithm is able to attain
a high accuracy while displaying excellent parallel performance."
"Consider a linear system of equations with interval coefficients, and each
interval coefficient is associated with either a universal or an existential
quantifier. The AE solution set and AE solvability of the system is defined by
$\forall\exists$-quantification.
  Herein, we deal with the problem what properties must the coefficient matrix
have in order that there is guaranteed an existence of an AE solution. Based on
this motivation, we introduce a concept of AE regularity, which implies that
the AE solution set is nonempty and the system is AE solvable for every
right-hand side. We discuss characterization of AE regularity, and we also
focus on various classes of matrices that are implicitly AE regular. Some of
these classes are polynomially decidable, and therefore give an efficient way
for checking AE regularity. We also state open problems related to
computational complexity and characterization."
"Two-step predictor/corrector methods are provided to solve three classes of
problems that present themselves as systems of ordinary differential equations
(ODEs). In the first class, velocities are given from which displacements are
to be solved. In the second class, velocities and accelerations are given from
which displacements are to be solved. And in the third class, accelerations are
given from which velocities and displacements are to be solved. Two-step
methods are not self starting, so compatible one-step methods are provided to
take that first step with. An algorithm is presented for controlling the step
size so that the local truncation error does not exceed a specified tolerance."
"Developed in [Deng and Lin, 2014], Least-Squares Progressive Iterative
Approximation (LSPIA) is an efficient iterative method for solving B-spline
curve and surface least-squares fitting systems. In [Deng and Lin 2014], it was
shown that LSPIA is convergent when the iterative matrix is nonsingular. In
this paper, we will show that LSPIA is still convergent even the iterative
matrix is singular."
"Waveguide Meshes are efficient and versatile models of wave propagation along
a multidimensional ideal medium. The choice of the mesh geometry affects both
the computational cost and the accuracy of simulations. In this paper, we focus
on 2D geometries and use multidimensional sampling theory to compare the
square, triangular, and hexagonal meshes in terms of sampling efficiency and
dispersion error under conditions of critical sampling. The analysis shows that
the triangular geometry exhibits the most desirable tradeoff between accuracy
and computational cost."
"This paper is concerned with a few novel RBF-based numerical schemes
discretizing partial differential equations. For boundary-type methods, we
derive the indirect and direct symmetric boundary knot methods (BKM). The
resulting interpolation matrix of both is always symmetric irrespective of
boundary geometry and conditions. In particular, the direct BKM applies the
practical physical variables rather than expansion coefficients and becomes
very competitive to the boundary element method. On the other hand, based on
the multiple reciprocity principle, we invent the RBF-based boundary particle
method (BPM) for general inhomogeneous problems without a need using inner
nodes. The direct and symmetric BPM schemes are also developed.
  For domain-type RBF discretization schemes, by using the Green integral we
develop a new Hermite RBF scheme called as the modified Kansa method (MKM),
which differs from the symmetric Hermite RBF scheme in that the MKM discretizes
both governing equation and boundary conditions on the same boundary nodes. The
local spline version of the MKM is named as the finite knot method (FKM). Both
MKM and FKM significantly reduce calculation errors at nodes adjacent to
boundary. In addition, the nonsingular high-order fundamental or general
solution is strongly recommended as the RBF in the domain-type methods and dual
reciprocity method approximation of particular solution relating to the BKM.
  It is stressed that all the above discretization methods of boundary-type and
domain-type are symmetric, meshless, and integration-free. The spline-based
schemes will produce desirable symmetric sparse banded interpolation matrix. In
appendix, we present a Hermite scheme to eliminate edge effect on the RBF
geometric modeling and imaging."
"In recent years some attempts have been done to relate the RBF with wavelets
in handling high dimensional multiscale problems. To the author's knowledge,
however, the orthonormal and bi-orthogonal RBF wavelets are still missing in
the literature. By using the nonsingular general solution and singular
fundamental solution of differential operator, recently the present author,
refer. 3, made some substantial headway to derive the orthonormal RBF wavelets
series and transforms. The methodology can be generalized to create the RBF
wavelets by means of the orthogonal convolution kernel function of various
integral operators. In particular, it is stressed that the presented RBF
wavelets does not apply the tensor product to handle multivariate problems at
all.
  This note is to correct some errata in reference 3 and also to supply a few
latest advances in the study of orthornormal RBF wavelet transforms."
"This study applies the RBF wavelet series to the evaluation of analytical
solutions of linear time-dependent wave and diffusion problems of any
dimensionality and geometry. To the best of the author's knowledge, such
analytical solutions have never been achieved before. The RBF wavelets can be
understood an alternative for multidimensional problems to the standard Fourier
series via fundamental and general solutions of partial differential equation.
The present RBF wavelets are infinitely differential, compactly supported,
orthogonal over different scales and very simple. The rigorous mathematical
proof of completeness and convergence is still missing in this study. The
present work may open a new window to numerical solution and theoretical
analysis of many other high-dimensional time-dependent PDE problems under
arbitrary geometry."
"A few novel radial basis function (RBF) discretization schemes for partial
differential equations are developed in this study. For boundary-type methods,
we derive the indirect and direct symmetric boundary knot methods. Based on the
multiple reciprocity principle, the boundary particle method is introduced for
general inhomogeneous problems without using inner nodes. For domain-type
schemes, by using the Green integral we develop a novel Hermite RBF scheme
called the modified Kansa method, which significantly reduces calculation
errors at close-to-boundary nodes. To avoid Gibbs phenomenon, we present the
least square RBF collocation scheme. Finally, five types of the kernel RBF are
also briefly presented."
"This report will add some supplements to the recently finished report series
on the distance function wavelets (DFW). First, we define the general distance
in terms of the Riesz potential, and then, the distance function Abel wavelets
are derived via the fractional integral and Laplacian. Second, the DFW Weyl
transform is found to be a shifted Laplace potential DFW. The DFW Radon
transform is also presented. Third, we present a conjecture on truncation error
formula of the multiple reciprocity Laplace DFW series and discuss its error
distributions in terms of node density distributions. Forth, we point out that
the Hermite distance function interpolation can be used to replace overlapping
in the domain decomposition in order to produce sparse matrix. Fifth, the shape
parameter is explained as a virtual extra axis contribution in terms of the
MQ-type Possion kernel. The report is concluded with some remarks on a range of
other issues."
"It is noted that the standard definition of the fractional Laplacian leads to
a hyper-singular convolution integral and is also obscure about how to
implement the boundary conditions. This purpose of this note is to introduce a
new definition of the fractional Laplacian to overcome these major drawbacks."
"Excellent computer simulations are done for a purpose. The most valid
purposes are to explore uncharted territory, to resolve a well-posed scientific
or technical question, or to make a design choice. Stand-alone modeling can
serve the first purpose. The other two goals need a full integration of the
modeling effort into a scientific or engineering program.
  Some excellent work, much of it related to the Department of Energy
Laboratories, is reviewed. Some less happy stories are recounted.
  In the past, some of the most impressive work has involved complexity and
chaos. Prediction in a complex world requires a first principles understanding
based upon the intersection of theory, experiment and simulation."
"We propose a sufficient condition for invertibility of a polynomial mapping
function defined on a cube or simplex. This condition is applicable to finite
element analysis using curved meshes. The sufficient condition is based on an
analysis of the Bernstein-B\'ezier form of the columns of the derivative."
"Let $\orig{A}$ be any matrix and let $A$ be a slight random perturbation of
$\orig{A}$. We prove that it is unlikely that $A$ has large condition number.
Using this result, we prove it is unlikely that $A$ has large growth factor
under Gaussian elimination without pivoting. By combining these results, we
bound the smoothed precision needed by Gaussian elimination without pivoting.
Our results improve the average-case analysis of Gaussian elimination without
pivoting performed by Yeung and Chan (SIAM J. Matrix Anal. Appl., 1997)."
"The aim of the paper is to derive the numerical least-squares estimator for
mean and variance of random variable. In order to do so the following questions
have to be answered: (i) what is the statistical model for the estimation
procedure? (ii) what are the properties of the estimator, like optimality (in
which class) or asymptotic properties? (iii) how does the estimator work in
practice, how compared to competing estimators?"
"A computational method for numeric resolution of a PDEs system, based on a
Finite Differences schema integrated by interpolations of partial results, and
an estimate of the error of its solution respect to the normal FD solution."
"This paper describes a method of calculating the transforms, currently
obtained via Fourier and reverse Fourier transforms. The method allows
calculating efficiently the transforms of a signal having an arbitrary
dimension of the digital representation by reducing the transform to a
vector-to-circulant matrix multiplying. There is a connection between harmonic
equations in rectangular and polar coordinate systems. The connection
established here and used to create a very robust iterative algorithm for a
conformal mapping calculation. There is also suggested a new ratio (and an
efficient way of computing it) of two oscillative signals."
"The aim of the paper is to derive the complex-valued least-squares estimator
for bias-noise mean and variance."
"Large scale real number computation is an essential ingredient in several
modern mathematical proofs. Because such lengthy computations cannot be
verified by hand, some mathematicians want to use software proof assistants to
verify the correctness of these proofs. This paper develops a new
implementation of the constructive real numbers and elementary functions for
such proofs by using the monad properties of the completion operation on metric
spaces. Bishop and Bridges's notion of regular sequences is generalized to,
what I call, regular functions which form the completion of any metric space.
Using the monad operations, continuous functions on length spaces (a common
subclass of metric spaces) are created by lifting continuous functions on the
original space. A prototype Haskell implementation has been created. I believe
that this approach yields a real number library that is reasonably efficient
for computation, and still simple enough to easily verify its correctness."
"We present a randomized algorithm that, on input a symmetric, weakly
diagonally dominant n-by-n matrix A with m nonzero entries and an n-vector b,
produces a y such that $\norm{y - \pinv{A} b}_{A} \leq \epsilon \norm{\pinv{A}
b}_{A}$ in expected time $O (m \log^{c}n \log (1/\epsilon)),$ for some constant
c. By applying this algorithm inside the inverse power method, we compute
approximate Fiedler vectors in a similar amount of time. The algorithm applies
subgraph preconditioners in a recursive fashion. These preconditioners improve
upon the subgraph preconditioners first introduced by Vaidya (1990).
  For any symmetric, weakly diagonally-dominant matrix A with non-positive
off-diagonal entries and $k \geq 1$, we construct in time $O (m \log^{c} n)$ a
preconditioner B of A with at most $2 (n - 1) + O ((m/k) \log^{39} n)$ nonzero
off-diagonal entries such that the finite generalized condition number
$\kappa_{f} (A,B)$ is at most k, for some other constant c.
  In the special case when the nonzero structure of the matrix is planar the
corresponding linear system solver runs in expected time $ O (n \log^{2} n + n
\log n \ \log \log n \ \log (1/\epsilon))$.
  We hope that our introduction of algorithms of low asymptotic complexity will
lead to the development of algorithms that are also fast in practice."
"We propose an algorithm based on Newton's method and subdivision for finding
all zeros of a polynomial system in a bounded region of the plane. This
algorithm can be used to find the intersections between a line and a surface,
which has applications in graphics and computer-aided geometric design. The
algorithm can operate on polynomials represented in any basis that satisfies a
few conditions. The power basis, the Bernstein basis, and the first-kind
Chebyshev basis are among those compatible with the algorithm. The main novelty
of our algorithm is an analysis showing that its running is bounded only in
terms of the condition number of the polynomial's zeros and a constant
depending on the polynomial basis."
"Compressible Mooney-Rivlin theory has been used to model hyperelastic solids,
such as rubber and porous polymers, and more recently for the modeling of soft
tissues for biomedical tissues, undergoing large elastic deformations. We
propose a solution procedure for Lagrangian finite element discretization of a
static nonlinear compressible Mooney-Rivlin hyperelastic solid. We consider the
case in which the boundary condition is a large prescribed deformation, so that
mesh tangling becomes an obstacle for straightforward algorithms. Our solution
procedure involves a largely geometric procedure to untangle the mesh: solution
of a sequence of linear systems to obtain initial guesses for interior nodal
positions for which no element is inverted. After the mesh is untangled, we
take Newton iterations to converge to a mechanical equilibrium. The Newton
iterations are safeguarded by a line search similar to one used in
optimization. Our computational results indicate that the algorithm is up to 70
times faster than a straightforward Newton continuation procedure and is also
more robust (i.e., able to tolerate much larger deformations). For a few
extremely large deformations, the deformed mesh could only be computed through
the use of an expensive Newton continuation method while using a tight
convergence tolerance and taking very small steps."
"We present statistics (S-statistics) based only on random variable (not
random value) with a mean squared error of mean estimation as a concept of
error."
"The Doppler spectrum estimation of a weather radar signal in a classic way
can be made by two methods, temporal one based in the autocorrelation of the
successful signals, whereas the other one uses the estimation of the power
spectral density PSD by using Fourier transforms. We introduces a new tool of
signal processing based on Ramanujan sums cq(n), adapted to the analysis of
arithmetical sequences with several resonances p/q. These sums are almost
periodic according to time n of resonances and aperiodic according to the order
q of resonances. New results will be supplied by the use of Ramanujan Fourier
Transform (RFT) for the estimation of the Doppler spectrum for the weather
radar signal."
"This paper presents two sufficient conditions to ensure a faithful evaluation
of polynomial in IEEE-754 floating point arithmetic. Faithfulness means that
the computed value is one of the two floating point neighbours of the exact
result; it can be satisfied using a more accurate algorithm than the classic
Horner scheme. One condition here provided is an apriori bound of the
polynomial condition number derived from the error analysis of the compensated
Horner algorithm. The second condition is both dynamic and validated to check
at the running time the faithfulness of a given evaluation. Numerical
experiments illustrate the behavior of these two conditions and that associated
running time over-cost is really interesting."
"We introduce two algorithms for accurately evaluating powers to a positive
integer in floating-point arithmetic, assuming a fused multiply-add (fma)
instruction is available. We show that our log-time algorithm always produce
faithfully-rounded results, discuss the possibility of getting correctly
rounded results, and show that results correctly rounded in double precision
can be obtained if extended-precision is available with the possibility to
round into double precision (with a single rounding)."
"In [5], Srijuntongsiri and Vavasis propose the ""Kantorovich-Test Subdivision
algorithm"", or KTS, which is an algorithm for finding all zeros of a polynomial
system in a bounded region of the plane. This algorithm can be used to find the
intersections between a line and a surface. The main features of KTS are that
it can operate on polynomials represented in any basis that satisfies certain
conditions and that its efficiency has an upper bound that depends only on the
conditioning of the problem and the choice of the basis representing the
polynomial system.
  This article explores in detail the dependence of the efficiency of the KTS
algorithm on the choice of basis. Three bases are considered: the power, the
Bernstein, and the Chebyshev bases. These three bases satisfy the basis
properties required by KTS. Theoretically, Chebyshev case has the smallest
upper bound on its running time. The computational results, however, do not
show that Chebyshev case performs better than the other two."
"The vast use of computers on scientific numerical computation makes the
awareness of the limited precision that these machines are able to provide us
an essential matter. A limited and insufficient precision allied to the
truncation and rounding errors may induce the user to incorrect interpretation
of his/hers answer. In this work, we have developed a computational package to
minimize this kind of error by offering arbitrary precision numbers and
calculation. This is very important in Physics where we can work with numbers
too small and too big simultaneously."
"In various areas of applied numerics, the problem of calculating the
logarithm of a matrix A emerges. Since series expansions of the logarithm
usually do not converge well for matrices far away from the identity, the
standard numerical method calculates successive square roots. In this article,
a new algorithm is presented that relies on the computation of successive
matrix exponentials. Convergence of the method is demonstrated for a large
class of initial matrices and favorable choices of the initial matrix are
discussed."
"Nonnegative matrix factorization (NMF) has become a prominent technique for
the analysis of image databases, text databases and other information retrieval
and clustering applications. In this report, we define an exact version of NMF.
Then we establish several results about exact NMF: (1) that it is equivalent to
a problem in polyhedral combinatorics; (2) that it is NP-hard; and (3) that a
polynomial-time local search heuristic exists."
"High confidence in floating-point programs requires proving numerical
properties of final and intermediate values. One may need to guarantee that a
value stays within some range, or that the error relative to some ideal value
is well bounded. Such work may require several lines of proof for each line of
code, and will usually be broken by the smallest change to the code (e.g. for
maintenance or optimization purpose). Certifying these programs by hand is
therefore very tedious and error-prone. This article discusses the use of the
Gappa proof assistant in this context. Gappa has two main advantages over
previous approaches: Its input format is very close to the actual C code to
validate, and it automates error evaluation and propagation using interval
arithmetic. Besides, it can be used to incrementally prove complex mathematical
properties pertaining to the C code. Yet it does not require any specific
knowledge about automatic theorem proving, and thus is accessible to a wide
community. Moreover, Gappa may generate a formal proof of the results that can
be checked independently by a lower-level proof assistant like Coq, hence
providing an even higher confidence in the certification of the numerical code.
The article demonstrates the use of this tool on a real-size example, an
elementary function with correctly rounded output."
"We present six Theorems on the univariate real Polynomial, using which we
develop a new algorithm for deciding the existence of atleast one real root for
univariate integer Polynomials. Our algorithm outputs that no positive real
root exists, if and only if, the given Polynomial is a factor of a real
Polynomial with positive coefficients. Next, we define a transformation that
transforms any instance of 3-SAT into a multivariate real Polynomial with
positive coefficients, if and only if, the instance is not satisfiable."
"The floating-point implementation of a function on an interval often reduces
to polynomial approximation, the polynomial being typically provided by Remez
algorithm. However, the floating-point evaluation of a Remez polynomial
sometimes leads to catastrophic cancellations. This happens when some of the
polynomial coefficients are very small in magnitude with respects to others. In
this case, it is better to force these coefficients to zero, which also reduces
the operation count. This technique, classically used for odd or even
functions, may be generalized to a much larger class of functions. An algorithm
is presented that forces to zero the smaller coefficients of the initial
polynomial thanks to a modified Remez algorithm targeting an incomplete
monomial basis. One advantage of this technique is that it is purely numerical,
the function being used as a numerical black box. This algorithm is implemented
within a larger polynomial implementation tool that is demonstrated on a range
of examples, resulting in polynomials with less coefficients than those
obtained the usual way."
"This paper addresses some numerical and theoretical aspects of dual Schur
domain decomposition methods for linear first-order transient partial
differential equations. In this work, we consider the trapezoidal family of
schemes for integrating the ordinary differential equations (ODEs) for each
subdomain and present four different coupling methods, corresponding to
different algebraic constraints, for enforcing kinematic continuity on the
interface between the subdomains.
  Method 1 (d-continuity) is based on the conventional approach using
continuity of the primary variable and we show that this method is unstable for
a lot of commonly used time integrators including the mid-point rule. To
alleviate this difficulty, we propose a new Method 2 (Modified d-continuity)
and prove its stability for coupling all time integrators in the trapezoidal
family (except the forward Euler). Method 3 (v-continuity) is based on
enforcing the continuity of the time derivative of the primary variable.
However, this constraint introduces a drift in the primary variable on the
interface. We present Method 4 (Baumgarte stabilized) which uses Baumgarte
stabilization to limit this drift and we derive bounds for the stabilization
parameter to ensure stability.
  Our stability analysis is based on the ``energy'' method, and one of the main
contributions of this paper is the extension of the energy method (which was
previously introduced in the context of numerical methods for ODEs) to assess
the stability of numerical formulations for index-2 differential-algebraic
equations (DAEs)."
"This paper introduces a new algorithm for solving a sub-class of quantified
constraint satisfaction problems (QCSP) where existential quantifiers precede
universally quantified inequalities on continuous domains. This class of QCSPs
has numerous applications in engineering and design. We propose here a new
generic branch and prune algorithm for solving such continuous QCSPs. Standard
pruning operators and solution identification operators are specialized for
universally quantified inequalities. Special rules are also proposed for
handling the parameters of the constraints. First experimentation show that our
algorithm outperforms the state of the art methods."
"In this paper, we examine the CE method in the broad context of Monte Carlo
Optimization (MCO) and Parametric Learning (PL), a type of machine learning. A
well-known overarching principle used to improve the performance of many PL
algorithms is the bias-variance tradeoff. This tradeoff has been used to
improve PL algorithms ranging from Monte Carlo estimation of integrals, to
linear estimation, to general statistical estimation. Moreover, as described
by, MCO is very closely related to PL. Owing to this similarity, the
bias-variance tradeoff affects MCO performance, just as it does PL performance.
  In this article, we exploit the bias-variance tradeoff to enhance the
performance of MCO algorithms. We use the technique of cross-validation, a
technique based on the bias-variance tradeoff, to significantly improve the
performance of the Cross Entropy (CE) method, which is an MCO algorithm. In
previous work we have confirmed that other PL techniques improve the perfomance
of other MCO algorithms. We conclude that the many techniques pioneered in PL
could be investigated as ways to improve MCO algorithms in general, and the CE
method in particular."
"If the non-zero finite floating-point numbers are interpreted as point
intervals, then the effect of rounding can be interpreted as computing one of
the bounds of the result according to interval arithmetic. We give an interval
interpretation for the signed zeros and infinities, so that the undefined
operations 0*inf, inf - inf, inf/inf, and 0/0 become defined.
  In this way no operation remains that gives rise to an error condition.
Mathematically questionable features of the floating-point standard become
well-defined sets of reals. Interval semantics provides a basis for the
verification of numerical algorithms. We derive the results of the newly
defined operations and consider the implications for hardware implementation."
"Many challenging tasks in sensor networks, including sensor calibration,
ranking of nodes, monitoring, event region detection, collaborative filtering,
collaborative signal processing, {\em etc.}, can be formulated as a problem of
solving a linear system of equations. Several recent works propose different
distributed algorithms for solving these problems, usually by using linear
iterative numerical methods.
  The main problem with previous approaches is that once the problem inputs
change during the process of computation, the computation may output unexpected
results. In real life settings, sensor measurements are subject to varying
environmental conditions and to measurement noise.
  We present a simple iterative scheme called SS-Iterative for solving systems
of linear equations, and examine its properties in the self-stabilizing
perspective. We analyze the behavior of the proposed scheme under changing
input sequences using two different assumptions on the input: a box bound, and
a probabilistic distribution.
  As a case study, we discuss the sensor calibration problem and provide
simulation results to support the applicability of our approach."
"In this paper, we consider the vibratory motions of lumped parameter systems
wherein the components of the system cannot be described by constitutive
expressions for the force in terms of appropriate kinematical quantities. Such
physical systems reduce to a system of differential-algebraic equations, which
invariably need to be solved numerically. To illustrate the issues with
clarity, we consider a simple system in which the dashpot is assumed to contain
a ""Bingham"" fluid for which one cannot describe the force in the dashpot as a
function of the velocity. On the other hand, one can express the velocity as a
function of the force."
"Elementary techniques from operational calculus, differential algebra, and
noncommutative algebra lead to a new approach for change-point detection, which
is an important field of investigation in various areas of applied sciences and
engineering. Several successful numerical experiments are presented."
"The combinatorial optimization problem is one of the important applications
in neural network computation. The solutions of linearly constrained continuous
optimization problems are difficult with an exact algorithm, but the algorithm
for the solution of such problems is derived by using logarithm barrier
function. In this paper we have made an attempt to solve the linear constrained
optimization problem by using general logarithm barrier function to get an
approximate solution. In this case the barrier parameters behave as temperature
decreasing to zero from sufficiently large positive number satisfying convexity
of the barrier function. We have developed an algorithm to generate decreasing
sequence of solution converging to zero limit."
This paper has been withdrawn by the author.
"The linear complementarity problem is to find vector $z$ in $\mathrm{IR}^{n}$
satisfying $z^{T}(Mz+q)=0$, $Mz+q\geqslant0,$ $z\geqslant0$, where $M$ as a
matrix and $q$ as a vector, are given data; this problem becomes in present the
subject of much important research because it arises in many areas and it
includes important fields, we cite for example the linear and nonlinear
programming, the convex quadratic programming and the variational inequalities
problems, ... It is known that the linear complementarity problem is completely
equivalent to solving nonlinear equation $F(x)=0$ with $F$ is a function from
$\mathrm{IR}^{n}$ into itself defined by $F(x)=(M+I)x+(M-I)|x|+q$. In this
paper we propose a globally convergent hybrid algorithm for solving this
equation; this method is based on an algorithm given by Shi \cite{Y. Shi}, he
uses vector divisions with the secant method; but for using this method we must
have a function continuous with partial derivatives on an open set of
$\mathrm{IR}^{n}$; so we built a sequence of functions $\tilde{F}(p,x)\in
C^{\infty}$ which converges uniformly to the function $F(x)$; and we show that
finding the zero of the function $F$ is completely equivalent to finding the
zero of the sequence of the functions $\tilde{F}(p,x)$. We close our paper with
some numerical simulation examples to illustrate our theoretical results."
"An exact, one-to-one transform is presented that not only allows digital
circular convolutions, but is free from multiplications and quantisation errors
for transform lengths of arbitrary powers of two. The transform is analogous to
the Discrete Fourier Transform, with the canonical harmonics replaced by a set
of cyclic integers computed using only bit-shifts and additions modulo a prime
number. The prime number may be selected to occupy contemporary word sizes or
to be very large for cryptographic or data hiding applications. The transform
is an extension of the Rader Transforms via Carmichael's Theorem. These
properties allow for exact convolutions that are impervious to numerical
overflow and to utilise Fast Fourier Transform algorithms."
"Many applied time-dependent problems are characterized by an additive
representation of the problem operator. Additive schemes are constructed using
such a splitting and associated with the transition to a new time level on the
basis of the solution of more simple problems for the individual operators in
the additive decomposition. We consider a new class of additive schemes for
problems with additive representation of the operator at the time derivative.
In this paper we construct and study the vector operator-difference schemes,
which are characterized by a transition from one initial the evolution equation
to a system of such equations."
"We present two new adaptive quadrature routines. Both routines differ from
previously published algorithms in many aspects, most significantly in how they
represent the integrand, how they treat non-numerical values of the integrand,
how they deal with improper divergent integrals and how they estimate the
integration error. The main focus of these improvements is to increase the
reliability of the algorithms without significantly impacting their efficiency.
Both algorithms are implemented in Matlab and tested using both the ""families""
suggested by Lyness and Kaganove and the battery test used by Gander and
Gautschi and Kahaner. They are shown to be more reliable, albeit in some cases
less efficient, than other commonly-used adaptive integrators."
"Among the algorithms that are likely to play a major role in future exascale
computing, the fast multipole method (FMM) appears as a rising star. Our
previous recent work showed scaling of an FMM on GPU clusters, with problem
sizes in the order of billions of unknowns. That work led to an extremely
parallel FMM, scaling to thousands of GPUs or tens of thousands of CPUs. This
paper reports on a a campaign of performance tuning and scalability studies
using multi-core CPUs, on the Kraken supercomputer. All kernels in the FMM were
parallelized using OpenMP, and a test using 10^7 particles randomly distributed
in a cube showed 78% efficiency on 8 threads. Tuning of the
particle-to-particle kernel using SIMD instructions resulted in 4x speed-up of
the overall algorithm on single-core tests with 10^3 - 10^7 particles. Parallel
scalability was studied in both strong and weak scaling. The strong scaling
test used 10^8 particles and resulted in 93% parallel efficiency on 2048
processes for the non-SIMD code and 54% for the SIMD-optimized code (which was
still 2x faster). The weak scaling test used 10^6 particles per process, and
resulted in 72% efficiency on 32,768 processes, with the largest calculation
taking about 40 seconds to evaluate more than 32 billion unknowns. This work
builds up evidence for our view that FMM is poised to play a leading role in
exascale computing, and we end the paper with a discussion of the features that
make it a particularly favorable algorithm for the emerging heterogeneous and
massively parallel architectural landscape."
"This paper deals with the formulation and numerical implementation of a fully
coupled continuum model for deformation-diffusion in linearized elastic solids.
The mathematical model takes into account the effect of the deformation on the
diffusion process, and the affect of the transport of an inert chemical species
on the deformation of the solid. We then present a robust computational
framework for solving the proposed mathematical model, which consists of
coupled non-linear partial differential equations. It should be noted that many
popular numerical formulations may produce unphysical negative values for the
concentration, particularly, when the diffusion process is anisotropic. The
violation of the non-negative constraint by these numerical formulations is not
mere numerical noise. In the proposed computational framework we employ a novel
numerical formulation that will ensure that the concentration of the diffusant
be always non-negative, which is one of the main contributions of this paper.
Representative numerical examples are presented to show the robustness,
convergence, and performance of the proposed computational framework. Another
contribution of this paper is to systematically study the affect of transport
of the diffusant on the deformation of the solid and vice-versa, and their
implication in modeling degradation/healing of materials. We show that the
coupled response is both qualitatively and quantitatively different from the
uncoupled response."
"We introduce a time-parallel algorithm for solving numerically almost
integrable Hamiltonian systems in action-angle coordinates. This algorithm is a
refinement of that introduced by Saha, Stadel and Tremaine in 1997 (SST97) for
the same type of problems. Our refined algorithm has a better convergence
obtained from the use of derivatives of the perturbing term not considered in
the original SST97 algorithm. An advantage of this algorithm is its
independence of the step-size for the parallelized procedures which can be
consider as a particular case of the parareal scheme."
"The main aim of this paper is to document the performance of $p$-refinement
with respect to maximum principles and the non-negative constraint. The model
problem is (steady-state) anisotropic diffusion with decay (which is a
second-order elliptic partial differential equation). We considered the
standard single-field formulation (which is based on the Galerkin formalism)
and two least-squares-based mixed formulations. We have employed non-uniform
Lagrange polynomials for altering the polynomial order in each element, and we
have used $p = 1, ..., 10$.
  It will be shown that the violation of the non-negative constraint will not
vanish with $p$-refinement for anisotropic diffusion. We shall illustrate the
performance of $p$-refinement using several representative problems. The
intended outcome of the paper is twofold. Firstly, this study will caution the
users of high-order approximations about its performance with respect to
maximum principles and the non-negative constraint. Secondly, this study will
help researchers to develop new methodologies for enforcing maximum principles
and the non-negative constraint under high-order approximations."
"The implementation of global optimization algorithms, using the arithmetic of
infinity, is considered. A relatively simple version of implementation is
proposed for the algorithms that possess the introduced property of strong
homogeneity. It is shown that the P-algorithm and the one-step Bayesian
algorithm are strongly homogeneous."
"In the past decades, exactly recovering the intrinsic data structure from
corrupted observations, which is known as robust principal component analysis
(RPCA), has attracted tremendous interests and found many applications in
computer vision. Recently, this problem has been formulated as recovering a
low-rank component and a sparse component from the observed data matrix. It is
proved that under some suitable conditions, this problem can be exactly solved
by principal component pursuit (PCP), i.e., minimizing a combination of nuclear
norm and $l_1$ norm. Most of the existing methods for solving PCP require
singular value decompositions (SVD) of the data matrix, resulting in a high
computational complexity, hence preventing the applications of RPCA to very
large scale computer vision problems. In this paper, we propose a novel
algorithm, called $l_1$ filtering, for \emph{exactly} solving PCP with an
$O(r^2(m+n))$ complexity, where $m\times n$ is the size of data matrix and $r$
is the rank of the matrix to recover, which is supposed to be much smaller than
$m$ and $n$. Moreover, $l_1$ filtering is \emph{highly parallelizable}. It is
the first algorithm that can \emph{exactly} solve a nuclear norm minimization
problem in \emph{linear time} (with respect to the data size). Experiments on
both synthetic data and real applications testify to the great advantage of
$l_1$ filtering in speed over state-of-the-art algorithms."
"With the current hybridization of treecodes and FMMs, combined with
auto-tuning capabilities on heterogeneous architectures, the flexibility of
fast N-body methods has been greatly enhanced. These features are a requirement
to developing a black-box software library for fast N-body algorithms on
heterogeneous systems, which is our immediate goal."
"In this paper we propose an algorithm to classify tensor data. Our
methodology is built on recent studies about matrix classification with the
trace norm constrained weight matrix and the tensor trace norm. Similar to
matrix classification, the tensor classification is formulated as a convex
optimization problem which can be solved by using the off-the-shelf accelerated
proximal gradient (APG) method. However, there are no analytic solutions as the
matrix case for the updating of the weight tensors via the proximal gradient.
To tackle this problem, the Douglas-Rachford splitting technique and the
alternating direction method of multipliers (ADM) used in tensor completion are
adapted to update the weight tensors. Further more, due to the demand of real
applications, we also propose its online learning approaches. Experiments
demonstrate the efficiency of the methods."
"We demonstrate the applicability of a new PAINT method to speed up iterations
of interactive methods in multiobjective optimization. As our test case, we
solve a computationally expensive non-linear, five-objective problem of
designing and operating a wastewater treatment plant. The PAINT method
interpolates between a given set of Pareto optimal outcomes and constructs a
computationally inexpensive mixed integer linear surrogate problem for the
original problem. We develop an IND-NIMBUS(R) PAINT module to combine the
interactive NIMBUS method and the PAINT method and to find a preferred solution
to the original problem. With the PAINT method, the solution process with the
NIMBUS method take a comparatively short time even though the original problem
is computationally expensive."
"An easy to implement modulus-squared Dirichlet (MSD) boundary condition is
formulated for numerical simulations of time-dependent complex partial
differential equations in multidimensional settings. The MSD boundary condition
approximates a constant modulus-square value of the solution at the boundaries.
Application of the MSD boundary condition to the nonlinear Schr\""odinger
equation is shown, and numerical simulations are performed to demonstrate its
usefulness and advantages over other simple boundary conditions."
"We define the notion of effective stiffness and show that it can used to
build sparsifiers, algorithms that sparsify linear systems arising from
finite-element discretizations of PDEs. In particular, we show that sampling
$O(n\log n)$ elements according to probabilities derived from effective
stiffnesses yields a high quality preconditioner that can be used to solve the
linear system in a small number of iterations. Effective stiffness generalizes
the notion of effective resistance, a key ingredient of recent progress in
developing nearly linear symmetric diagonally dominant (SDD) linear solvers.
Solving finite elements problems is of considerably more interest than the
solution of SDD linear systems, since the finite element method is frequently
used to numerically solve PDEs arising in scientific and engineering
applications. Unlike SDD systems, which are relatively easy to solve, there has
been limited success in designing fast solvers for finite element systems, and
previous algorithms usually target discretization of limited class of PDEs like
scalar elliptic or 2D trusses. Our sparsifier is general; it applies to a wide
range of finite-element discretizations. A sparsifier does not constitute a
complete linear solver. To construct a solver, one needs additional components
(e.g., an efficient elimination or multilevel scheme for the sparsified
system). Still, sparsifiers have been a critical tools in efficient SDD
solvers, and we believe that our sparsifier will become a key ingredient in
future fast finite-element solvers."
"We present a numerical scheme for the solution of a class of atmospheric
models where high horizontal resolution is required while a coarser vertical
structure is allowed. The proposed scheme considers a layering procedure for
the original set of equations, and the use of high-order ADER finite volume
schemes for the solution of the system of balance laws arising from the
dimensional reduction procedure. We present several types of layering based
upon Galerkin discretizations of the vertical structure, and we study the
effect of incrementing the order of horizontal approximation. Numerical
experiments for the computational validation of the convergence of the scheme
together with the study of physical phenomena are performed over 2D linear
advective models, including a set of equations for an isothermal atmosphere."
"Vertex-centroid schemes are cell-centered finite volume schemes for
conservation laws which make use of vertex values to construct high resolution
schemes. The vertex values must be obtained through a consistent averaging
(interpolation) procedure. A modified interpolation scheme is proposed which is
better than existing schemes in giving positive weights in the interpolation
formula. A simplified reconstruction scheme is also proposed which is also more
accurate and efficient. For scalar conservation laws, we develop limited
versions of the schemes which are stable in maximum norm by constructing
suitable limiters. The schemes are applied to compressible flows governed by
the Euler equations of inviscid gas dynamics."
"Parameter identification problems are formulated in a probabilistic language,
where the randomness reflects the uncertainty about the knowledge of the true
values. This setting allows conceptually easily to incorporate new information,
e.g. through a measurement, by connecting it to Bayes's theorem. The unknown
quantity is modelled as a (may be high-dimensional) random variable. Such a
description has two constituents, the measurable function and the measure. One
group of methods is identified as updating the measure, the other group changes
the measurable function. We connect both groups with the relatively recent
methods of functional approximation of stochastic problems, and introduce
especially in combination with the second group of methods a new procedure
which does not need any sampling, hence works completely deterministically. It
also seems to be the fastest and more reliable when compared with other
methods. We show by example that it also works for highly nonlinear non-smooth
problems with non-Gaussian measures."
"Several widely-used textbooks lead the reader to believe that solving a
linear system of equations Ax = b by multiplying the vector b by a computed
inverse inv(A) is inaccurate. Virtually all other textbooks on numerical
analysis and numerical linear algebra advise against using computed inverses
without stating whether this is accurate or not. In fact, under reasonable
assumptions on how the inverse is computed, x = inv(A)*b is as accurate as the
solution computed by the best backward-stable solvers. This fact is not new,
but obviously obscure. We review the literature on the accuracy of this
computation and present a self-contained numerical analysis of it."
"Power Series Solution Method has been traditionally used to solve Ordinary
and Partial Linear Differential Equations. However, despite their usefulness
the application of this method has been limited to this particular kind of
equations. In this work we use the method of power series to solve nonlinear
partial differential equations. The method is applied to solve three versions
of nonlinear time-dependent Burgers-Type differential equations in order to
demonstrate its scope and applicability."
"A positive real interval, [a, b], can be partitioned into sub-intervals such
that sub-interval widths divided by sub-interval ""average"" values remains
constant. That both Arithmetic Mean and Geometric Mean ""average"" values produce
constant ratios for the same log scale is the stated invariance proved in this
short note. The continuous analog is briefly considered and shown to have
similar properties."
"In this paper, we revisit the D-iteration algorithm in order to better
explain its connection to the Gauss-Seidel method and different performance
results that were observed. In particular, we study here the practical
computation cost based on the execution runtime compared to the theoretical
number of iterations. We also propose an exact formula of the error for
PageRank class of equations."
"A parallelization of a sweeping preconditioner for 3D Helmholtz equations
without large cavities is introduced and benchmarked for several challenging
velocity models. The setup and application costs of the sequential
preconditioner are shown to be O({\gamma}^2 N^{4/3}) and O({\gamma} N log N),
where {\gamma}({\omega}) denotes the modestly frequency-dependent number of
grid points per Perfectly Matched Layer. Several computational and memory
improvements are introduced relative to using black-box sparse-direct solvers
for the auxiliary problems, and competitive runtimes and iteration counts are
reported for high-frequency problems distributed over thousands of cores. Two
open-source packages are released along with this paper: ""Parallel Sweeping
Preconditioner (PSP)"" and the underlying distributed multifrontal solver,
""Clique""."
"In this paper, we study how the D-iteration algorithm can be applied to
numerically solve the differential equations such as heat equation in 2D or 3D.
The method can be applied on the class of problems that can be addressed by the
Gauss-Seidel iteration, based on the linear approximation of the differential
equations."
"Product between mode-$n$ unfolding $\bY_{(n)}$ of an $N$-D tensor $\tY$ and
Khatri-Rao products of $(N-1)$ factor matrices $\bA^{(m)}$, $m = 1,..., n-1,
n+1, ..., N$ exists in algorithms for CANDECOMP/PARAFAC (CP). If $\tY$ is an
error tensor of a tensor approximation, this product is the gradient of a cost
function with respect to factors, and has the largest workload in most CP
algorithms. In this paper, a fast method to compute this product is proposed.
Experimental verification shows that the fast CP gradient can accelerate the
CP_ALS algorithm 2 times and 8 times faster for factorizations of 3-D and 4-D
tensors, and the speed-up ratios can be 20-30 times for higher dimensional
tensors."
"The multi-frontal direct solver is the state-of-the-art algorithm for the
direct solution of sparse linear systems. This paper provides computational
complexity and memory usage estimates for the application of the multi-frontal
direct solver algorithm on linear systems resulting from B-spline-based
isogeometric finite elements, where the mesh is a structured grid. Specifically
we provide the estimates for systems resulting from $C^{p-1}$ polynomial
B-spline spaces and compare them to those obtained using $C^0$ spaces."
"We analyze the FEAST method for computing selected eigenvalues and
eigenvectors of large sparse matrix pencils. After establishing the close
connection between FEAST and the well-known Rayleigh-Ritz method, we identify
several critical issues that influence convergence and accuracy of the solver:
the choice of the starting vector space, the stopping criterion, how the inner
linear systems impact the quality of the solution, and the use of FEAST for
computing eigenpairs from multiple intervals. We complement the study with
numerical examples, and hint at possible improvements to overcome the existing
problems."
"The fast multipole method (FMM) performs fast approximate kernel summation to
a specified tolerance $\epsilon$ by using a hierarchical division of the
domain, which groups source and receiver points into regions that satisfy local
separation and the well-separated pair decomposition properties. While square
tilings and quadtrees are commonly used in 2D, we investigate alternative
tilings and associated spatial data structures: regular hexagons (septree) and
triangles (triangle-quadtree). We show that both structures satisfy separation
properties for the FMM and prove their theoretical error bounds and
computational costs. Empirical runtime and error analysis of our
implementations are provided."
"The well-known Turing machine is an example of a theoretical digital
computer, and it was the logical basis of constructing real electronic
computers. In the present paper we propose an alternative, namely, by
formalising arithmetic operations in the ordinary computing device, we attempt
to go to the analytical procedure (for calculations). The method creates
possibilities for solving nonlinear differential equations and systems. Our
theoretical computer model requires retaining a finite number of terms to
represent numbers, and utilizes digit carry procedure. The solution is
represented in the form of a segment of a series in the powers of the step size
of the independent variable in the finite-difference scheme. The algorithm
generates a schematic representation that approximates the convergent
finite-difference scheme, which, in turn, approximates the equation under
consideration. The use of probabilistic methods allows us to average the
recurrent calculations and exclude intermediate levels of computation. All the
stages of formalizing operations of the classical computer result in ""the
method of the computer analogy"". The proposed method leads to an explicit
analytical representation of the solution. We present the general features of
the algorithm which are illustrated by an example of solutions for a system of
nonlinear equations."
"In this paper, we propose a new adaptation of the D-iteration algorithm to
numerically solve the differential equations. This problem can be reinterpreted
in 2D or 3D (or higher dimensions) as a limit of a diffusion process where the
boundary or initial conditions are replaced by fluid catalysts. Pre-computing
the diffusion process for an elementary catalyst case as a fundamental block of
a class of differential equations, we show that the computation efficiency can
be greatly improved. The method can be applied on the class of problems that
can be addressed by the Gauss-Seidel iteration, based on the linear
approximation of the differential equations."
"In this paper, we revisit the D-iteration algorithm in order to better
explain different performance results that were observed for the numerical
computation of the eigenvector associated to the PageRank score. We revisit
here the practical computation cost based on the execution runtime compared to
the theoretical number of iterations."
"Mukherjee (Pattern Recognition Letters, vol. 32, pp. 824-831, 2011) recently
introduced a class of distance functions called weighted t-cost distances that
generalize m-neighbor, octagonal, and t-cost distances. He proved that weighted
t-cost distances form a family of metrics and derived an approximation for the
Euclidean norm in $\mathbb{Z}^n$. In this note we compare this approximation to
two previously proposed Euclidean norm approximations and demonstrate that the
empirical average errors given by Mukherjee are significantly optimistic in
$\mathbb{R}^n$. We also propose a simple normalization scheme that improves the
accuracy of his approximation substantially with respect to both average and
maximum relative errors."
"In this paper we study how the use of a more continuous set of basis
functions affects the cost of solving systems of linear equations resulting
from a discretized Galerkin weak form. Specifically, we compare performance of
linear solvers when discretizing using $C^0$ B-splines, which span traditional
finite element spaces, and $C^{p-1}$ B-splines, which represent maximum
continuity. We provide theoretical estimates for the increase in cost of the
matrix-vector product as well as for the construction and application of
black-box preconditioners. We accompany these estimates with numerical results
and study their sensitivity to various grid parameters such as element size $h$
and polynomial order of approximation $p$. Finally, we present timing results
for a range of preconditioning options for the Laplace problem. We conclude
that the matrix-vector product operation is at most $\slfrac{33p^2}{8}$ times
more expensive for the more continuous space, although for moderately low $p$,
this number is significantly reduced. Moreover, if static condensation is not
employed, this number further reduces to at most a value of 8, even for high
$p$. Preconditioning options can be up to $p^3$ times more expensive to setup,
although this difference significantly decreases for some popular
preconditioners such as Incomplete LU factorization."
"In this paper, we apply the ideas of the matrix column based diffusion
approach to define a new eigenvector computation algorithm of a stationary
probability of a Markov chain."
"The classification of high dimensional data with kernel methods is considered
in this article. Exploit- ing the emptiness property of high dimensional
spaces, a kernel based on the Mahalanobis distance is proposed. The computation
of the Mahalanobis distance requires the inversion of a covariance matrix. In
high dimensional spaces, the estimated covariance matrix is ill-conditioned and
its inversion is unstable or impossible. Using a parsimonious statistical
model, namely the High Dimensional Discriminant Analysis model, the specific
signal and noise subspaces are estimated for each considered class making the
inverse of the class specific covariance matrix explicit and stable, leading to
the definition of a parsimonious Mahalanobis kernel. A SVM based framework is
used for selecting the hyperparameters of the parsimonious Mahalanobis kernel
by optimizing the so-called radius-margin bound. Experimental results on three
high dimensional data sets show that the proposed kernel is suitable for
classifying high dimensional data, providing better classification accuracies
than the conventional Gaussian kernel."
"Convection-diffusion equations provide the basis for describing heat and mass
transfer phenomena as well as processes of continuum mechanics. To handle flows
in porous media, the fundamental issue is to model correctly the convective
transport of individual phases. Moreover, for compressible media, the pressure
equation itself is just a time-dependent convection-diffusion equation.
  For different problems, a convection-diffusion equation may be be written in
various forms. The most popular formulation of convective transport employs the
divergent (conservative) form. In some cases, the nondivergent (characteristic)
form seems to be preferable. The so-called skew-symmetric form of convective
transport operators that is the half-sum of the operators in the divergent and
nondivergent forms is of great interest in some applications.
  Here we discuss the basic classes of discretization in space: finite
difference schemes on rectangular grids, approximations on general polyhedra
(the finite volume method), and finite element procedures. The key properties
of discrete operators are studied for convective and diffusive transport. We
emphasize the problems of constructing approximations for convection and
diffusion operators that satisfy the maximum principle at the discrete level
--- they are called monotone approximations.
  Two- and three-level schemes are investigated for transient problems.
Unconditionally stable explicit-implicit schemes are developed for
convection-diffusion problems. Stability conditions are obtained both in
finite-dimensional Hilbert spaces and in Banach spaces depending on the form in
which the convection-diffusion equation is written."
"Convection-diffusion problem are the base for continuum mechanics. The main
features of these problems are associated with an indefinite operator the
problem. In this work we construct unconditionally stable scheme for
non-stationary convection-diffusion equations, which are based on use of new
variables. Also, we consider these equations in the form of
convection-diffusion-reaction and construct unconditionally stable schemes when
explicit-implicit approximations are used with splitting of the reaction
operator."
"The kernel-independent fast multipole method (KIFMM) proposed in [1] is of
almost linear complexity. In the original KIFMM the time-consuming M2L
translations are accelerated by FFT. However, when more equivalent points are
used to achieve higher accuracy, the efficiency of the FFT approach tends to be
lower because more auxiliary volume grid points have to be added. In this
paper, all the translations of the KIFMM are accelerated by using the singular
value decomposition (SVD) based on the low-rank property of the translating
matrices. The acceleration of M2L is realized by first transforming the
associated translating matrices into more compact form, and then using low-rank
approximations. By using the transform matrices for M2L, the orders of the
translating matrices in upward and downward passes are also reduced. The
improved KIFMM is then applied to accelerate BEM. The performance of the
proposed algorithms are demonstrated by three examples. Numerical results show
that, compared with the original KIFMM, the present method can reduce about 40%
of the iterating time and 25% of the memory requirement."
"The recovery type error estimators introduced by Zienkiewicz and Zhu use a
recovered stress field evaluated from the Finite Element (FE) solution. Their
accuracy depends on the quality of the recovered field. In this sense, accurate
results are obtained using recovery procedures based on the Superconvergent
Patch recovery technique (SPR). These error estimators can be easily
implemented and provide accurate estimates. Another important feature is that
the recovered solution is of a better quality than the FE solution and can
therefore be used as an enhanced solution. We have developed an SPR-type
recovery technique that considers equilibrium and displacements constraints to
obtain a very accurate recovered displacements field from which a recovered
stress field can also be evaluated. We propose the use of these recovered
fields as the standard output of the FE code instead of the raw FE solution.
Techniques to quantify the error of the recovered solution are therefore
needed. In this report we present an error estimation technique that accurately
evaluates the error of the recovered solution both at global and local levels
in the FEM and XFEM frameworks. We have also developed an h-adaptive mesh
refinement strategy based on the error of the recovered solution. As the
converge rate of the error of the recovered solution is higher than that of the
FE one, the computational cost required to obtain a solution with a prescribed
accuracy is smaller than for traditional h-adaptive processes."
"Factorization of compact wavelet matrices into primitive ones has been known
for more than 20 years. This method makes it possible to generate wavelet
matrix coefficients and also to specify them by their first row. Recently, a
new parametrization of compact wavelet matrices of the same order and degree
has been introduced by the last author. This method also enables us to fulfill
the above mentioned tasks of matrix constructions. In the present paper, we
briefly describe the corresponding algorithms based on two different methods,
and numerically compare their performance"
"A novel Mathematical Random Number Generator (MRNG) is presented here. In
this case, ""mathematical"" refers to the fact that to construct that generator
it is not necessary to resort to a physical phenomenon, such as the thermal
noise of an electronic device, but rather to a mathematical procedure. The MRNG
generates binary strings - in principle, as long as desired - which may be
considered genuinely random in the sense that they pass the statistical tests
currently accepted to evaluate the randomness of those strings. From those
strings, the MRNG also generates random numbers expressed in base 10. An MRNG
has been installed as a facility on the following web page:
http://www.appliedmathgroup.org. This generator may be used for applications in
tasks in: a) computational simulation of probabilistic-type systems, and b) the
random selection of samples of different populations. Users interested in
applications in cryptography can build another MRNG, but they would have to
withhold information - specified in section 5 - from people who are not
authorized to decode messages encrypted using that resource."
"We propose a novel method for a solution of a system of linear equations with
the non-negativity condition. The method is based on the Tikhonov functional
and has better accuracy and stability than other well-known algorithms."
"The aim of this paper is to find the numerical solutions of the second order
linear and nonlinear differential equations with Dirichlet, Neumann and Robin
boundary conditions. We use the Bernoulli polynomials as linear combination to
the approximate solutions of 2nd order boundary value problems. Here the
Bernoulli polynomials over the interval [0, 1] are chosen as trial functions so
that care has been taken to satisfy the corresponding homogeneous form of the
Dirichlet boundary conditions in the Galerkin weighted residual method. In
addition to that the given differential equation over arbitrary finite domain
[a, b] and the boundary conditions are converted into its equivalent form over
the interval [0, 1]. All the formulas are verified by considering numerical
examples. The approximate solutions are compared with the exact solutions, and
also with the solutions of the existing methods. A reliable good accuracy is
obtained in all cases."
"In this paper we present the Large Inverse Cholesky (LIC) method, an
efficient method for computing the coefficient matrices of a Structural Vector
Autoregressive (SVAR) model."
"In this paper, Bernstein piecewise polynomials are used to solve the integral
equations numerically. A matrix formulation is given for a non-singular linear
Fredholm Integral Equation by the technique of Galerkin method. In the Galerkin
method, the Bernstein polynomials are used as the approximation of basis
functions. Examples are considered to verify the effectiveness of the proposed
derivations, and the numerical solutions guarantee the desired accuracy."
"The Kaczmarz method is an iterative method for solving large systems of
equations that projects iterates orthogonally onto the solution space of each
equation. In contrast to direct methods such as Gaussian elimination or
QR-factorization, this algorithm is efficient for problems with sparse
matrices, as they appear in constraint-based user interface (UI) layout
specifications. However, the Kaczmarz method as described in the literature has
its limitations: it considers only equality constraints and does not support
soft constraints, which makes it inapplicable to the UI layout problem.
  In this paper we extend the Kaczmarz method for solving specifications
containing soft constraints, using the prioritized IIS detection algorithm.
Furthermore, the performance and convergence of the proposed algorithms are
evaluated empirically using randomly generated UI layout specifications of
various sizes. The results show that these methods offer improvements in
performance over standard methods like Matlab's LINPROG, a well-known efficient
linear programming solver."
"This is an overview paper written in style of research proposal. In recent
years we introduced a general framework for large-scale unconstrained
optimization -- Sequential Subspace Optimization (SESOP) and demonstrated its
usefulness for sparsity-based signal/image denoising, deconvolution,
compressive sensing, computed tomography, diffraction imaging, support vector
machines. We explored its combination with Parallel Coordinate Descent and
Separable Surrogate Function methods, obtaining state of the art results in
above-mentioned areas. There are several methods, that are faster than plain
SESOP under specific conditions: Trust region Newton method - for problems with
easily invertible Hessian matrix; Truncated Newton method - when fast
multiplication by Hessian is available; Stochastic optimization methods - for
problems with large stochastic-type data; Multigrid methods - for problems with
nested multilevel structure. Each of these methods can be further improved by
merge with SESOP. One can also accelerate Augmented Lagrangian method for
constrained optimization problems and Alternating Direction Method of
Multipliers for problems with separable objective function and non-separable
constraints."
"This paper develops a new class of exponential-type integrators where all the
matrix exponentiations are performed in a single Krylov space of low dimension.
The new family, called Lightly Implicit Krylov-Exponential (LIKE), is well
suited for solving large scale systems of ODEs or semi-discrete PDEs. The time
discretization and the Krylov space approximation are treated as a single
computational process, and the Krylov space properties are an integral part of
the new LIKE order condition theory developed herein. Consequently, LIKE
methods require a small number of basis vectors determined solely by the
temporal order of accuracy. The subspace size is independent of the ODE under
consideration, and there is no need to monitor the errors in linear system
solutions at each stage. Numerical results illustrate the favorable properties
of new family of methods."
"The method of obtaining the set of noncanonical hypercomplex number systems
by conversion of infinite hypercomplex number system to finite hypercomplex
number system depending on multiplication rules and factorization method is
described. Systems obtained by this method starting from the 3rddimension are
noncanonical. The obtained systems of even dimension can be re-factorized. As a
result of it hypercomplex number system of two times less dimension are got."
"Let $p(z)$ be a monic cubic complex polynomial with distinct roots and
distinct critical points. We say a critical point has the {\it Voronoi
property} if it lies in the Voronoi cell of a root $\theta$, $V(\theta)$, i.e.
the set of points that are closer to $\theta$ than to the other roots. We prove
at least one critical point has the Voronoi property and characterize the cases
when both satisfy this property. It is known that for any $\xi \in V(\theta)$,
the sequence $B_m(\xi) =\xi - p(\xi) d_{m-2}/d_{m-1}$ converges to $\theta$,
where $d_m$ satisfies the recurrence $d_m =p'(\xi)d_{m-1}-0.5
p(\xi)p''(\xi)d_{m-2} +p^2(\xi)d_{m-3}$, $d_0 =1, d_{-1}=d_{-2}=0$. Thus by the
Voronoi property, there is a solution $c$ of $p'(z)=0$ where $B_m(c)$ converges
to a root of $p(z)$. The speed of convergence is dependent on the ratio of the
distances between $c$ and the closest and the second closest roots of $p(z)$.
This results in a different algorithm for solving a cubic equation than the
classical methods. We give polynomiography for an example."
"The famous Fourier theorem states that, under some restrictions, any periodic
function (or real world signal) can be obtained as a sum of sinusoids, and
hence, a technique exists for decomposing a signal into its sinusoidal
components. From this theory an entire branch of research has flourished: from
the Short-Time or Windowed Fourier Transform to the Wavelets, the Frames, and
lately the Generic Frequency Analysis. The aim of this paper is to take the
Frequency Analysis a step further. It will be shown that keeping the same
reconstruction algorithm as the Fourier Theorem but changing to a new computing
method for the analysis phase allows the generalization of the Fourier Theorem
to a large class of nonorthogonal bases. New methods and algorithms can be
employed in function decomposition on such generic bases. It will be shown that
these algorithms are a generalization of the Fourier analysis, i.e. they are
reduced to the familiar Fourier tools when using orthogonal bases. The
differences between this tool and the wavelets and frames theories will be
discussed. Examples of analysis and reconstruction of functions using the given
algorithms and nonorthogonal bases will be given. In this first part the focus
will be on vectorial decomposition, while the second part will be on phased
decomposition. The phased decomposition thanks to a single function basis has
many interesting consequences and applications."
"A new method based on Shannon-Happ formula to calculate transfer function
from Signal Flow Graph (SFG) is presented. The algorithm provides an explicit
approach to get the transfer function in a format with both numerical and
symbolic expressions. The adoption of the symbolic variable in SFG, which could
represent the nonlinear item or the independent sub-system, is achieved by
variable separation approach. An investigation is given for the solutions of
several special conditions of SFG. To improve the efficiency of the algorithm,
a new technique combined with Johnson method for generating the combinations of
the non-touching loops is developed. It uses the previous combinations in lower
order to get the ones in higher order. There is an introduction about the
visualization of SFG and the subroutines for system performance analysis in the
software, AVANT."
"Non-negative matrix factorization (NMF) has previously been shown to be a
useful decomposition for multivariate data. We interpret the factorization in a
new way and use it to generate missing attributes from test data. We provide a
joint optimization scheme for the missing attributes as well as the NMF
factors. We prove the monotonic convergence of our algorithms. We present
classification results for cases with missing attributes."
"We describe two main classes of one-sided trigonometric and hyperbolic
Jacobi-type algorithms for computing eigenvalues and eigenvectors of Hermitian
matrices. These types of algorithms exhibit significant advantages over many
other eigenvalue algorithms. If the matrices permit, both types of algorithms
compute the eigenvalues and eigenvectors with high relative accuracy.
  We present novel parallelization techniques for both trigonometric and
hyperbolic classes of algorithms, as well as some new ideas on how pivoting in
each cycle of the algorithm can improve the speed of the parallel one-sided
algorithms. These parallelization approaches are applicable to both
distributed-memory and shared-memory machines.
  The numerical testing performed indicates that the hyperbolic algorithms may
be superior to the trigonometric ones, although, in theory, the latter seem
more natural."
"A one-sided Jacobi hyperbolic singular value decomposition (HSVD) algorithm,
using a massively parallel graphics processing unit (GPU), is developed. The
algorithm also serves as the final stage of solving a symmetric indefinite
eigenvalue problem. Numerical testing demonstrates the gains in speed and
accuracy over sequential and MPI-parallelized variants of similar Jacobi-type
HSVD algorithms. Finally, possibilities of hybrid CPU--GPU parallelism are
discussed."
"The solution of large sparse linear systems is often the most time-consuming
part of many science and engineering applications. Computational fluid
dynamics, circuit simulation, power network analysis, and material science are
just a few examples of the application areas in which large sparse linear
systems need to be solved effectively. In this paper we introduce a new
parallel hybrid sparse linear system solver for distributed memory
architectures that contains both direct and iterative components. We show that
by using our solver one can alleviate the drawbacks of direct and iterative
solvers, achieving better scalability than with direct solvers and more
robustness than with classical preconditioned iterative solvers. Comparisons to
well-known direct and iterative solvers on a parallel architecture are
provided."
"The presented article contains a 2D mesh generation routine optimized with
the Metropolis algorithm. The procedure enables to produce meshes with a
prescribed size h of elements. These finite element meshes can serve as
standard discrete patterns for the Finite Element Method (FEM). Appropriate
meshes together with the FEM approach constitute an effective tool to deal with
differential problems. Thus, having them both one can solve the 2D Poisson
problem. It can be done for different domains being either of a regular
(circle, square) or of a non--regular type. The proposed routine is even
capable to deal with non--convex shapes."
"In this paper, we present a stabilized mixed formulation for unsteady
Brinkman equation. The formulation is systematically derived based on the
variational multiscale formalism and the method of horizontal lines. The
derivation does not need the assumption that the fine-scale variables do not
depend on the time, which is the case with the conventional derivation of
multiscale stabilized formulations for transient mixed problems. An expression
for the stabilization parameter is obtained in terms of a bubble function, and
appropriate bubble functions for various finite elements are also presented.
Under the proposed formulation, equal-order interpolation for the velocity and
pressure (which is computationally the most convenient) is stable.
Representative numerical results are presented to illustrate the performance of
the proposed formulation. Spatial and temporal convergence studies are also
performed, and the proposed formulation performed well."
"The paper describes several efficient parallel implementations of the
one-sided hyperbolic Jacobi-type algorithm for computing eigenvalues and
eigenvectors of Hermitian matrices. By appropriate blocking of the algorithms
an almost ideal load balancing between all available processors/cores is
obtained. A similar blocking technique can be used to exploit local cache
memory of each processor to further speed up the process. Due to diversity of
modern computer architectures, each of the algorithms described here may be the
method of choice for a particular hardware and a given matrix size. All
proposed block algorithms compute the eigenvalues with relative accuracy
similar to the original non-blocked Jacobi algorithm."
"Euclidean norm calculations arise frequently in scientific and engineering
applications. Several approximations for this norm with differing complexity
and accuracy have been proposed in the literature. Earlier approaches were
based on minimizing the maximum error. Recently, Seol and Cheun proposed an
approximation based on minimizing the average error. In this paper, we first
examine these approximations in detail, show that they fit into a single
mathematical formulation, and compare their average and maximum errors. We then
show that the maximum errors given by Seol and Cheun are significantly
optimistic."
"To assess the durability of structures, heat and moisture transport need to
be analyzed. To provide a reliable estimation of heat and moisture distribution
in a certain structure, one needs to include all available information about
the loading conditions and material parameters. Moreover, the information
should be accompanied by a corresponding evaluation of its credibility. Here,
the Bayesian inference is applied to combine different sources of information,
so as to provide a more accurate estimation of heat and moisture fields [1].
The procedure is demonstrated on the probabilistic description of heterogeneous
material where the uncertainties consist of a particular value of individual
material characteristic and spatial fluctuations. As for the heat and moisture
transfer, it is modelled in coupled setting [2]."
"With the explosion of the size of digital dataset, the limiting factor for
decomposition algorithms is the \emph{number of passes} over the input, as the
input is often stored out-of-core or even off-site. Moreover, we're only
interested in algorithms that operate in \emph{constant memory} w.r.t. to the
input size, so that arbitrarily large input can be processed. In this paper, we
present a practical comparison of two such algorithms: a distributed method
that operates in a single pass over the input vs. a streamed two-pass
stochastic algorithm. The experiments track the effect of distributed
computing, oversampling and memory trade-offs on the accuracy and performance
of the two algorithms. To ensure meaningful results, we choose the input to be
a real dataset, namely the whole of the English Wikipedia, in the application
settings of Latent Semantic Analysis."
"Several ways to accelerate the solution of 2D/3D linear min-max problems in
$n$ constraints are discussed. We also present an algorithm for solving such
problems in the 2D case, which is superior to CGAL's linear programming solver,
both in performance and in stability."
"This paper addresses the problem of evaluating a subset of the range of a
vector-valued function. It is based on a work by Gold- sztejn and Jaulin which
provides methods based on interval analysis to address this problem when the
dimension of the domain and co-domain of the function are equal. This paper
extends this result to vector-valued functions with domain and co-domain of
different dimensions. This ex- tension requires the knowledge of the rank of
the Jacobian function on the whole domain. This leads to the sub-problem of
extracting an in- terval sub-matrix of maximum rank from a given interval
matrix. Three different techniques leading to approximate solutions of this
extraction are proposed and compared."
"We propose in this paper a novel inverse tangent transverse shear deformation
formulation for functionally graded material (FGM) plates. The isogeometric
finite element analysis (IGA) of static, free vibration and buckling problems
of FGM plates is then addressed using a refined plate theory (RPT). The RPT
enables us to describe the non-linear distribution of shear stresses through
the plate thickness without any requirement of shear correction factors (SCF).
IGA utilizes basis functions, namely B-splines or non-uniform rational
B-splines (NURBS), which achieve easily the smoothness of any arbitrary order.
It hence satisfies the C1 requirement of the RPT model. The present method
approximates the displacement field of four degrees of freedom per each control
point and retains the computational efficiency while ensuring the high accuracy
in solution."
"The objective of this paper is to apply the well-known exact operational
matrices (EOMs) idea for solving the Emden-Fowler equations, illustrating the
superiority of EOMs versus ordinary operational matrices (OOMs). Up to now, a
few studies have been conducted on EOMs and the differential equations solved
by them do not have high-degree nonlinearity and the reported results are not
regarded as appropriate criteria for the excellence of the new method. So, we
chose Emden-Fowler type differential equations and solved them by this method.
To confirm the accuracy of the new method and to show the preeminence of EOMs
versus OOMs, the norm1 of the residual and error function of both methods are
evaluated for multiple $m$ values, where $m$ is the degree of the Bernstein
polynomials. We reported the results in form of plots to illustrate the error
convergence of both methods to zero and also to show the primacy of the new
method versus OOMs. The obtained results have demonstrated the increased
accuracy of the new method."
"Graphics Processing Units (GPUs) are high performance co-processors
originally intended to improve the use and quality of computer graphics
applications. Once, researchers and practitioners noticed the potential of
using GPU for general purposes, GPUs applications have been extended from
graphics applications to other fields. The main objective of this paper is to
evaluate the impact of using GPU in solution of the transient diffusion type
equation by parallel and stable group explicit finite difference method and
encourage the researchers in this field to immigrate from implementing their
algorithms in CPU to the GPU emerging world. For comparing them, we implemented
the method in both GPU and CPU (multi-core) programming context. Moreover, we
proposed an optimal synchronization arrangement for the implementation
pseudo-code. Also, the interrelation of GPU parallel programming and
initializing the algorithm variables were discussed, taking advantage of
numerical experiences. The GPU-approach results are faster than those obtained
from a much expensive parallel 8-thread CPU-based programming. The GPU used in
this paper, is an ordinary old laptop GPU (GT 335M, launched at 2010) and is
accessible for everyone and the newer generations of GPU (as discussed in
paper) have even more performance priority over the similar-price GPUs. Then,
the results are expected to encourage the entire research society to take
advantage of GPUs and improve the time efficiency of their studies."
"This work generalizes the additively partitioned Runge-Kutta methods by
allowing for different stage values as arguments of different components of the
right hand side. An order conditions theory is developed for the new family of
generalized additive methods, and stability and monotonicity investigations are
carried out. The paper discusses the construction and properties of
implicit-explicit and implicit-implicit,methods in the new framework. The new
family, named GARK, introduces additional flexibility when compared to
traditional partitioned Runge-Kutta methods, and therefore offers additional
opportunities for the development of flexible solvers for systems with multiple
scales, or driven by multiple physical processes."
"This work constructs a new class of multirate schemes based on the recently
developed generalized additive Runge-Kutta (GARK) methods (Sandu and Guenther,
2013). Multirate schemes use different step sizes for different components and
for different partitions of the right-hand side based on the local activity
levels. We show that the new multirate GARK family includes many well-known
multirate schemes as special cases. The order conditions theory follows
directly from the GARK accuracy theory. Nonlinear stability and monotonicity
investigations show that these properties are inherited from the base schemes
provided that additional coupling conditions hold."
"This work presents a method to adaptively refine reduced-order models \emph{a
posteriori} without requiring additional full-order-model solves. The technique
is analogous to mesh-adaptive $h$-refinement: it enriches the reduced-basis
space online by `splitting' a given basis vector into several vectors with
disjoint support. The splitting scheme is defined by a tree structure
constructed offline via recursive $k$-means clustering of the state variables
using snapshot data. The method identifies the vectors to split online using a
dual-weighted-residual approach that aims to reduce error in an output quantity
of interest. The resulting method generates a hierarchy of subspaces online
without requiring large-scale operations or full-order-model solves. Further,
it enables the reduced-order model to satisfy \emph{any prescribed error
tolerance} regardless of its original fidelity, as a completely refined
reduced-order model is mathematically equivalent to the original full-order
model. Experiments on a parameterized inviscid Burgers equation highlight the
ability of the method to capture phenomena (e.g., moving shocks) not contained
in the span of the original reduced basis."
"We consider concave minimization problems over non-convex sets.Optimization
problems with this structure arise in sparse principal component analysis. We
analyze both a gradient projection algorithm and an approximate Newton
algorithm where the Hessian approximation is a multiple of the identity.
Convergence results are established. In numerical experiments arising in sparse
principal component analysis, it is seen that the performance of the gradient
projection algorithm is very similar to that of the truncated power method and
the generalized power method. In some cases, the approximate Newton algorithm
with a Barzilai-Borwein (BB) Hessian approximation can be substantially faster
than the other algorithms, and can converge to a better solution."
"In this paper, we study iterative methods on the coefficients of the rational
univariate representation (RUR) of a given algebraic set, called global Newton
iteration. We compare two natural approaches to define locally quadratically
convergent iterations: the first one involves Newton iteration applied to the
approximate roots individually and then interpolation to find the RUR of these
approximate roots; the second one considers the coefficients in the exact RUR
as zeroes of a high dimensional map defined by polynomial reduction, and
applies Newton iteration on this map. We prove that over fields with a p-adic
valuation these two approaches give the same iteration function, but over
fields equipped with the usual Archimedean absolute value, they are not
equivalent. In the latter case, we give explicitly the iteration function for
both approaches. Finally, we analyze the parallel complexity of the different
versions of the global Newton iteration, compare them, and demonstrate that
they can be efficiently computed. The motivation for this study comes from the
certification of approximate roots of overdetermined and singular polynomial
systems via the recovery of an exact RUR from approximate numerical data."
"This paper proposes the Proximal Iteratively REweighted (PIRE) algorithm for
solving a general problem, which involves a large body of nonconvex sparse and
structured sparse related problems. Comparing with previous iterative solvers
for nonconvex sparse problem, PIRE is much more general and efficient. The
computational cost of PIRE in each iteration is usually as low as the
state-of-the-art convex solvers. We further propose the PIRE algorithm with
Parallel Splitting (PIRE-PS) and PIRE algorithm with Alternative Updating
(PIRE-AU) to handle the multi-variable problems. In theory, we prove that our
proposed methods converge and any limit solution is a stationary point.
Extensive experiments on both synthesis and real data sets demonstrate that our
methods achieve comparative learning performance, but are much more efficient,
by comparing with previous nonconvex solvers."
"In this paper, we first extend the celebrated PageRank modification to a
higher-order Markov chain. Although this system has attractive theoretical
properties, it is computationally intractable for many interesting problems. We
next study a computationally tractable approximation to the higher-order
PageRank vector that involves a system of polynomial equations called
multilinear PageRank, which is a type of tensor PageRank vector. It is
motivated by a novel ""spacey random surfer"" model, where the surfer remembers
bits and pieces of history and is influenced by this information. The
underlying stochastic process is an instance of a vertex-reinforced random
walk. We develop convergence theory for a simple fixed-point method, a shifted
fixed-point method, and a Newton iteration in a particular parameter regime. In
marked contrast to the case of the PageRank vector of a Markov chain where the
solution is always unique and easy to compute, there are parameter regimes of
multilinear PageRank where solutions are not unique and simple algorithms do
not converge. We provide a repository of these non-convergent cases that we
encountered through exhaustive enumeration and randomly sampling that we
believe is useful for future study of the problem."
"Many proofs of the fundamental theorem of algebra rely on the fact that the
minimum of the modulus of a complex polynomial over the complex plane is
attained at some complex number. The proof then follows by arguing the minimum
value is zero. This can be done by proving that at any complex number that is
not a zero of the polynomial we can exhibit a direction of descent for the
modulus. In this note we present a very short and simple proof of the existence
of such descent direction. In particular, our descent direction gives rise to
Newton's method for solving a polynomial equation via modulus minimization and
also makes the iterates definable at any critical point."
"The Hildreth's algorithm is a row action method for solving large systems of
inequalities. This algorithm is efficient for problems with sparse matrices, as
opposed to direct methods such as Gaussian elimination or QR-factorization. We
apply the Hildreth's algorithm, as well as a randomized version, along with
prioritized selection of the inequalities, to efficiently detect the highest
priority feasible subsystem of equations. We prove convergence results and
feasibility criteria for both cyclic and randomized Hildreth's algorithm, as
well as a mixed algorithm which uses Hildreth's algorithm for inequalities and
Kaczmarz algorithm for equalities. These prioritized, sparse systems of
inequalities commonly appear in constraint-based user interface (UI) layout
specifications. The performance and convergence of these proposed algorithms
are evaluated empirically using randomly generated UI layout specifications of
various sizes. The results show that these methods offer improvements in
performance over standard methods like Matlab's LINPROG, a well-known efficient
linear programming solver, and the recent developed Kaczmarz algorithm with
prioritized IIS detection."
"Boman and Hendrickson observed that one can solve linear systems in Laplacian
matrices in time $\bigO{m^{3/2 + o (1)} \ln (1/\epsilon)}$ by preconditioning
with the Laplacian of a low-stretch spanning tree. By examining the
distribution of eigenvalues of the preconditioned linear system, we prove that
the preconditioned conjugate gradient will actually solve the linear system in
time $\softO{m^{4/3} \ln (1/\epsilon)}$."
"We study the decomposition of a nonnegative tensor into a minimal sum of
outer product of nonnegative vectors and the associated parsimonious naive
Bayes probabilistic model. We show that the corresponding approximation
problem, which is central to nonnegative PARAFAC, will always have optimal
solutions. The result holds for any choice of norms and, under a mild
assumption, even Bregman divergences."
"In this paper, linear systems with a crisp real coefficient matrix and with a
vector of fuzzy triangular numbers on the right-hand side are studied. A new
method, which is based on the geometric representations of linear
transformations, is proposed to find solutions. The method uses the fact that a
vector of fuzzy triangular numbers forms a rectangular prism in n-dimensional
space and that the image of a parallelepiped is also a parallelepiped under a
linear transformation. The suggested method clarifies why in general case
different approaches do not generate solutions as fuzzy numbers. It is
geometrically proved that if the coefficient matrix is a generalized
permutation matrix, then the solution of a fuzzy linear system (FLS) is a
vector of fuzzy numbers irrespective of the vector on the right-hand side. The
most important difference between this and previous papers on FLS is that the
solution is sought as a fuzzy set of vectors (with real components) rather than
a vector of fuzzy numbers. Each vector in the solution set solves the given FLS
with a certain possibility. The suggested method can also be applied in the
case when the right-hand side is a vector of fuzzy numbers in parametric form.
However, in this case, -cuts of the solution can not be determined by geometric
similarity and additional computations are needed."
"We consider the problem of reconstructing a low-rank matrix from a small
subset of its entries. In this paper, we describe the implementation of an
efficient algorithm called OptSpace, based on singular value decomposition
followed by local manifold optimization, for solving the low-rank matrix
completion problem. It has been shown that if the number of revealed entries is
large enough, the output of singular value decomposition gives a good estimate
for the original matrix, so that local optimization reconstructs the correct
matrix with high probability. We present numerical results which show that this
algorithm can reconstruct the low rank matrix exactly from a very small subset
of its entries. We further study the robustness of the algorithm with respect
to noise, and its performance on actual collaborative filtering datasets."
"We consider equation systems of the form X_1 = f_1(X_1, ..., X_n), ..., X_n =
f_n(X_1, ..., X_n) where f_1, ..., f_n are polynomials with positive real
coefficients. In vector form we denote such an equation system by X = f(X) and
call f a system of positive polynomials, short SPP. Equation systems of this
kind appear naturally in the analysis of stochastic models like stochastic
context-free grammars (with numerous applications to natural language
processing and computational biology), probabilistic programs with procedures,
web-surfing models with back buttons, and branching processes. The least
nonnegative solution mu f of an SPP equation X = f(X) is of central interest
for these models. Etessami and Yannakakis have suggested a particular version
of Newton's method to approximate mu f.
  We extend a result of Etessami and Yannakakis and show that Newton's method
starting at 0 always converges to mu f. We obtain lower bounds on the
convergence speed of the method. For so-called strongly connected SPPs we prove
the existence of a threshold k_f such that for every i >= 0 the (k_f+i)-th
iteration of Newton's method has at least i valid bits of mu f. The proof
yields an explicit bound for k_f depending only on syntactic parameters of f.
We further show that for arbitrary SPP equations Newton's method still
converges linearly: there are k_f>=0 and alpha_f>0 such that for every i>=0 the
(k_f+alpha_f i)-th iteration of Newton's method has at least i valid bits of mu
f. The proof yields an explicit bound for alpha_f; the bound is exponential in
the number of equations, but we also show that it is essentially optimal.
Constructing a bound for k_f is still an open problem. Finally, we also provide
a geometric interpretation of Newton's method for SPPs."
"For scientific computations on a digital computer the set of real number is
usually approximated by a finite set F of ""floating-point"" numbers. We compare
the numerical accuracy possible with difference choices of F having
approximately the same range and requiring the same word length. In particular,
we compare different choices of base (or radix) in the usual floating-point
systems. The emphasis is on the choice of F, not on the details of the number
representation or the arithmetic, but both rounded and truncated arithmetic are
considered. Theoretical results are given, and some simulations of typical
floating-point computations (forming sums, solving systems of linear equations,
finding eigenvalues) are described. If the leading fraction bit of a normalized
base 2 number is not stored explicitly (saving a bit), and the criterion is to
minimize the mean square roundoff error, then base 2 is best. If unnormalized
numbers are allowed, so the first bit must be stored explicitly, then base 4
(or sometimes base 8) is the best of the usual systems."
"Finding the sparsest solution $\alpha$ for an under-determined linear system
of equations $D\alpha=s$ is of interest in many applications. This problem is
known to be NP-hard. Recent work studied conditions on the support size of
$\alpha$ that allow its recovery using L1-minimization, via the Basis Pursuit
algorithm. These conditions are often relying on a scalar property of $D$
called the mutual-coherence. In this work we introduce an alternative set of
features of an arbitrarily given $D$, called the ""capacity sets"". We show how
those could be used to analyze the performance of the basis pursuit, leading to
improved bounds and predictions of performance. Both theoretical and numerical
methods are presented, all using the capacity values, and shown to lead to
improved assessments of the basis pursuit success in finding the sparest
solution of $D\alpha=s$."
"Scaling up the sparse matrix-vector multiplication kernel on modern Graphics
Processing Units (GPU) has been at the heart of numerous studies in both
academia and industry. In this article we present a novel non-parametric,
self-tunable, approach to data representation for computing this kernel,
particularly targeting sparse matrices representing power-law graphs. Using
real data, we show how our representation scheme, coupled with a novel tiling
algorithm, can yield significant benefits over the current state of the art GPU
efforts on a number of core data mining algorithms such as PageRank, HITS and
Random Walk with Restart."
"In this paper we consider a modification to Darcy equation by taking into
account the dependence of viscosity on the pressure. We present a stabilized
mixed formulation for the resulting governing equations. Equal-order
interpolation for the velocity and pressure is considered, and shown to be
stable (which is not the case under the classical mixed formulation). The
proposed mixed formulation is tested using a wide variety of numerical
examples. The proposed formulation is also implemented in a parallel setting,
and the performance of the formulation for large-scale problems is illustrated
using a representative problem. Two practical and technologically important
problems, one each on enhanced oil recovery and geological carbon-dioxide
sequestration, are solved using the proposed formulation. The numerical
examples show that the predictions based on Darcy model are qualitatively and
quantitatively different from that of the predictions based on the modified
Darcy model, which takes into account the dependence of the viscosity on the
pressure. In particular, the numerical example on the geological carbon-dioxide
sequestration shows that Darcy model over-predicts the leakage into an
abandoned well when compared to that of the modified Darcy model. On the other
hand, the modified Darcy model predicts higher pressures and higher pressure
gradients near the injection well. These predictions have dire consequences in
predicting damage and fracture zones, and designing the seal, whose integrity
is crucial to the safety of a geological carbon-dioxide sequestration
geosystem."
"In this paper we propose some very promissing results in interval arithmetics
which permit to build well-defined arithmetics including distributivity of
multiplication and division according addition and substraction. Thus, it
allows to build all algebraic operations and functions on intervals. This will
avoid completely the wrapping effects and data dependance. Some simple
applications for matrix eigenvalues calculations, inversion of symmetric
matrices and finally optimization are exhibited in the object-oriented
programming language python."
"This letter aims at resolving the issues raised in the recent short
communication [1] and answered by [2] by proposing a systematic approximation
scheme based on non-mapped shape functions, which both allows to fully exploit
the unique advantages of the smoothed finite element method (SFEM) [3, 4, 5, 6,
7, 8, 9] and resolve the existence, linearity and positivity deficiencies
pointed out in [1]. We show that Wachspress interpolants [10] computed in the
physical coordinate system are very well suited to the SFEM, especially when
elements are heavily distorted (obtuse interior angles). The proposed
approximation leads to results which are almost identical to those of the SFEM
initially proposed in [3]. These results that the proposed approximation scheme
forms a strong and rigorous basis for construction of smoothed finite element
methods."
"Markov population models (MPMs) are a widely used modelling formalism in the
area of computational biology and related areas. The semantics of a MPM is an
infinite-state continuous-time Markov chain. In this paper, we use the
established continuous stochastic logic (CSL) to express properties of Markov
population models. This allows us to express important measures of biological
systems, such as probabilistic reachability, survivability, oscillations,
switching times between attractor regions, and various others. Because of the
infinite state space, available analysis techniques only apply to a very
restricted subset of CSL properties. We present a full algorithm for model
checking CSL for MPMs, and provide experimental evidence showing that our
method is effective."
"In this notes we describe an algorithm for non-linear fitting which
incorporates some of the features of linear least squares into a general
minimum $\chi^2$ fit and provide a pure Python implementation of the algorithm.
It consists of the variable projection method (varpro), combined with a Newton
optimizer and stabilized using the steepest descent with an adaptative step.
The algorithm includes a term to account for Bayesian priors. We performed
tests of the algorithm using simulated data. This method is suitable, for
example, for fitting with sums of exponentials as often needed in Lattice
Quantum Chromodynamics."
"The sparse matrix-vector multiplication (SpMxV) is a kernel operation widely
used in iterative linear solvers. The same sparse matrix is multiplied by a
dense vector repeatedly in these solvers. Matrices with irregular sparsity
patterns make it difficult to utilize cache locality effectively in SpMxV
computations. In this work, we investigate single- and multiple-SpMxV
frameworks for exploiting cache locality in SpMxV computations. For the
single-SpMxV framework, we propose two cache-size-aware top-down
row/column-reordering methods based on 1D and 2D sparse matrix partitioning by
utilizing the column-net and enhancing the row-column-net hypergraph models of
sparse matrices. The multiple-SpMxV framework depends on splitting a given
matrix into a sum of multiple nonzero-disjoint matrices so that the SpMxV
operation is performed as a sequence of multiple input- and output- dependent
SpMxV operations. For an effective matrix splitting required in this framework,
we propose a cache- size-aware top-down approach based on 2D sparse matrix
partitioning by utilizing the row-column-net hypergraph model. For this
framework, we also propose two methods for effective ordering of individual
SpMxV operations. The primary objective in all of the three methods is to
maximize the exploitation of temporal locality. We evaluate the validity of our
models and methods on a wide range of sparse matrices using both cache-miss
simulations and actual runs by using OSKI. Experimental results show that
proposed methods and models outperform state-of-the-art schemes."
"In this paper we prove that computing the solution of an initial-value
problem $\dot{y}=p(y)$ with initial condition $y(t_0)=y_0\in\R^d$ at time
$t_0+T$ with precision $e^{-\mu}$ where $p$ is a vector of polynomials can be
done in time polynomial in the value of $T$, $\mu$ and $Y=\sup_{t_0\leqslant
u\leqslant T}\infnorm{y(u)}$. Contrary to existing results, our algorithm works
for any vector of polynomials $p$ over any bounded or unbounded domain and has
a guaranteed complexity and precision. In particular we do not assume $p$ to be
fixed, nor the solution to lie in a compact domain, nor we assume that $p$ has
a Lipschitz constant."
"The low-rank matrix factorization as a L1 norm minimization problem has
recently attracted much attention due to its intrinsic robustness to the
presence of outliers and missing data. In this paper, we propose a new method,
called the divide-and-conquer method, for solving this problem. The main idea
is to break the original problem into a series of smallest possible
sub-problems, each involving only unique scalar parameter. Each of these
subproblems is proved to be convex and has closed-form solution. By recursively
optimizing these small problems in an analytical way, efficient algorithm,
entirely avoiding the time-consuming numerical optimization as an inner loop,
for solving the original problem can naturally be constructed. The
computational complexity of the proposed algorithm is approximately linear in
both data size and dimensionality, making it possible to handle large-scale L1
norm matrix factorization problems. The algorithm is also theoretically proved
to be convergent. Based on a series of experiment results, it is substantiated
that our method always achieves better results than the current
state-of-the-art methods on $L1$ matrix factorization calculation in both
computational time and accuracy, especially on large-scale applications such as
face recognition and structure from motion."
"To solve numerically boundary value problems for parabolic equations with
mixed derivatives, the construction of difference schemes with prescribed
quality faces essential difficulties. In parabolic problems, some possibilities
are associated with the transition to a new formulation of the problem, where
the fluxes (derivatives with respect to a spatial direction) are treated as
unknown quantities. In this case, the original problem is rewritten in the form
of a boundary value problem for the system of equations in the fluxes. This
work deals with studying schemes with weights for parabolic equations written
in the flux coordinates. Unconditionally stable flux locally one-dimensional
schemes of the first and second order of approximation in time are constructed
for parabolic equations without mixed derivatives. A peculiarity of the system
of equations written in flux variables for equations with mixed derivatives is
that there do exist coupled terms with time derivatives."
"Polynomial approximations of computationally intensive models are central to
uncertainty quantification. This paper describes an adaptive method for
non-intrusive pseudospectral approximation, based on Smolyak's algorithm with
generalized sparse grids. We rigorously analyze and extend the non-adaptive
method proposed in [6], and compare it to a common alternative approach for
using sparse grids to construct polynomial approximations, direct quadrature.
Analysis of direct quadrature shows that O(1) errors are an intrinsic property
of some configurations of the method, as a consequence of internal aliasing. We
provide precise conditions, based on the chosen polynomial basis and quadrature
rules, under which this aliasing error occurs. We then establish theoretical
results on the accuracy of Smolyak pseudospectral approximation, and show that
the Smolyak approximation avoids internal aliasing and makes far more effective
use of sparse function evaluations. These results are applicable to broad
choices of quadrature rule and generalized sparse grids. Exploiting this
flexibility, we introduce a greedy heuristic for adaptive refinement of the
pseudospectral approximation. We numerically demonstrate convergence of the
algorithm on the Genz test functions, and illustrate the accuracy and
efficiency of the adaptive approach on a realistic chemical kinetics problem."
"A new inverse iteration algorithm that can be used to compute all the
eigenvectors of a real symmetric tri-diagonal matrix on parallel computers is
developed. The modified Gram-Schmidt orthogonalization is used in the classical
inverse iteration. This algorithm is sequential and causes a bottleneck in
parallel computing. In this paper, the use of the compact WY representation is
proposed in the orthogonalization process of the inverse iteration with the
Householder transformation. This change results in drastically reduced
synchronization cost in parallel computing. The new algorithm is evaluated on
both an 8-core and a 32-core parallel computer, and it is shown that the new
algorithm is greatly faster than the classical inverse iteration algorithm in
computing all the eigenvectors of matrices with several thousand dimensions."
"We present a novel computational framework for diffusive-reactive systems
that satisfies the non-negative constraint and maximum principles on general
computational grids. The governing equations for the concentration of reactants
and product are written in terms of tensorial diffusion-reaction equations. %
We restrict our studies to fast irreversible bimolecular reactions. If one
assumes that the reaction is diffusion-limited and all chemical species have
the same diffusion coefficient, one can employ a linear transformation to
rewrite the governing equations in terms of invariants, which are unaffected by
the reaction. This results in two uncoupled tensorial diffusion equations in
terms of these invariants, which are solved using a novel non-negative solver
for tensorial diffusion-type equations. The concentrations of the reactants and
the product are then calculated from invariants using algebraic manipulations.
The novel aspect of the proposed computational framework is that it will always
produce physically meaningful non-negative values for the concentrations of all
chemical species. Several representative numerical examples are presented to
illustrate the robustness, convergence, and the numerical performance of the
proposed computational framework. We will also compare the proposed framework
with other popular formulations. In particular, we will show that the Galerkin
formulation (which is the standard single-field formulation) does not produce
reliable solutions, and the reason can be attributed to the fact that the
single-field formulation does not guarantee non-negative solutions. We will
also show that the clipping procedure (which produces non-negative solutions
but is considered as a variational crime) does not give accurate results when
compared with the proposed computational framework."
"We propose a proximal approach to deal with a class of convex variational
problems involving nonlinear constraints. A large family of constraints, proven
to be effective in the solution of inverse problems, can be expressed as the
lower level set of a sum of convex functions evaluated over different, but
possibly overlapping, blocks of the signal. For such constraints, the
associated projection operator generally does not have a simple form. We
circumvent this difficulty by splitting the lower level set into as many
epigraphs as functions involved in the sum. A closed half-space constraint is
also enforced, in order to limit the sum of the introduced epigraphical
variables to the upper bound of the original lower level set. In this paper, we
focus on a family of constraints involving linear transforms of distance
functions to a convex set or $\ell_{1,p}$ norms with $p\in \{1,2,\infty\}$. In
these cases, the projection onto the epigraph of the involved function has a
closed form expression.
  The proposed approach is validated in the context of image restoration with
missing samples, by making use of constraints based on Non-Local Total
Variation. Experiments show that our method leads to significant improvements
in term of convergence speed over existing algorithms for solving similar
constrained problems. A second application to a pulse shape design problem is
provided in order to illustrate the flexibility of the proposed approach."
"In this article we show and implement a simple and effcient method to
strictly locate eigenvectors and eigenvalues of a given matrix, based on the
modified cone condition. As a consequence we can also effectively localize
zeros of complex polynomials."
"Consider a symmetric matrix $A(v)\in\RR^{n\times n}$ depending on a vector
$v\in\RR^n$ and satisfying the property $A(\alpha v)=A(v)$ for any
$\alpha\in\RR\backslash{0}$. We will here study the problem of finding
$(\lambda,v)\in\RR\times \RR^n\backslash\{0\}$ such that $(\lambda,v)$ is an
eigenpair of the matrix $A(v)$ and we propose a generalization of inverse
iteration for eigenvalue problems with this type of eigenvector nonlinearity.
The convergence of the proposed method is studied and several convergence
properties are shown to be analogous to inverse iteration for standard
eigenvalue problems, including local convergence properties. The algorithm is
also shown to be equivalent to a particular discretization of an associated
ordinary differential equation, if the shift is chosen in a particular way. The
algorithm is adapted to a variant of the Schr\""odinger equation known as the
Gross-Pitaevskii equation. We use numerical simulations toillustrate the
convergence properties, as well as the efficiency of the algorithm and the
adaption."
"This paper describes a direct solver algorithm for a sequence of finite
element meshes that are h-refined towards one or several point singularities.
For such a sequence of grids, the solver delivers linear computational cost
O(N) in terms of CPU time and memory with respect to the number of unknowns N.
The linear computational cost is achieved by utilizing the recursive structure
provided by the sequence of h-adaptive grids with a special construction of the
elimination tree that allows for reutilization of previously computed partial
LU factorizations over the entire unrefined part of the computational mesh. The
reutilization technique reduces the computational cost of the entire sequence
of h-refined grids from O(N^2) down to O(N). Theoretical estimates are
illustrated with numerical results on two- and three-dimensional model problems
exhibiting one or several point singularities."
"We numerically analyze the possibility of turning off post-smoothing
(relaxation) in geometric multigrid when used as a preconditioner in conjugate
gradient linear and eigenvalue solvers for the 3D Laplacian. The geometric
Semicoarsening Multigrid (SMG) method is provided by the hypre parallel
software package. We solve linear systems using two variants (standard and
flexible) of the preconditioned conjugate gradient (PCG) and preconditioned
steepest descent (PSD) methods. The eigenvalue problems are solved using the
locally optimal block preconditioned conjugate gradient (LOBPCG) method
available in hypre through BLOPEX software. We observe that turning off the
post-smoothing in SMG dramatically slows down the standard PCG-SMG. For
flexible PCG and LOBPCG, our numerical results show that post-smoothing can be
avoided, resulting in overall acceleration, due to the high costs of smoothing
and relatively insignificant decrease in convergence speed. We numerically
demonstrate for linear systems that PSD-SMG and flexible PCG-SMG converge
similarly if SMG post-smoothing is off. We experimentally show that the effect
of acceleration is independent of memory interconnection. A theoretical
justification is provided."
"We describe a Krylov-subspace method for estimating the spectral condition
number of a real matrix A or indicating that it is numerically rank deficient.
The main difficulty in estimating the condition number is the estimation of the
smallest singular value \sigma_{\min} of A. Our method estimates this value by
solving a consistent linear least-squares problem with a known solution using a
specific Krylov-subspace method called LSQR. In this method, the forward error
tends to concentrate in the direction of a singular vector corresponding to
\sigma_{\min}. Extensive experiments show that the method is reliable. It is
often much faster than a dense SVD and it can sometimes estimate the condition
number when running a dense SVD would be impractical due to the computational
cost or the memory requirements. The method uses very little memory (it
inherits this property from LSQR) and it works equally well on square and
rectangular matrices."
"Non-negative matrix factorization (NMF) has become a popular machine learning
approach to many problems in text mining, speech and image processing,
bio-informatics and seismic data analysis to name a few. In NMF, a matrix of
non-negative data is approximated by the low-rank product of two matrices with
non-negative entries. In this paper, the approximation quality is measured by
the Kullback-Leibler divergence between the data and its low-rank
reconstruction. The existence of the simple multiplicative update (MU)
algorithm for computing the matrix factors has contributed to the success of
NMF. Despite the availability of algorithms showing faster convergence, MU
remains popular due to its simplicity. In this paper, a diagonalized Newton
algorithm (DNA) is proposed showing faster convergence while the implementation
remains simple and suitable for high-rank problems. The DNA algorithm is
applied to various publicly available data sets, showing a substantial speed-up
on modern hardware."
"Gaussian elimination with full pivoting generates a PLUQ matrix
decomposition. Depending on the strategy used in the search for pivots, the
permutation matrices can reveal some information about the row or the column
rank profiles of the matrix. We propose a new pivoting strategy that makes it
possible to recover at the same time both row and column rank profiles of the
input matrix and of any of its leading sub-matrices. We propose a
rank-sensitive and quad-recursive algorithm that computes the latter PLUQ
triangular decomposition of an m \times n matrix of rank r in O(mnr^{\omega-2})
field operations, with \omega the exponent of matrix multiplication. Compared
to the LEU decomposition by Malashonock, sharing a similar recursive structure,
its time complexity is rank sensitive and has a lower leading constant. Over a
word size finite field, this algorithm also improveLs the practical efficiency
of previously known implementations."
"In conjugate gradient method, it is well known that the recursively computed
residual differs from true one as the iteration proceeds in finite arithmetic.
Some work have been devoted to analyze this be-havior and to evaluate the lower
and the upper bounds of the difference. This paper focuses on the behavior of
these two kinds of residuals, especially their lower bounds caused by the loss
of trailing digit, respectively."
"In this paper, we introduce a new iterative method which we call one step
back approach: the main idea is to anticipate the consequence of the iterative
computation per coordinate and to optimize on the choice of the sequence of the
coordinates on which the iterative update computations are done. The method
requires the increase of the size of the state vectors and one iteration step
loss from the initial vector. We illustrate the approach in linear and non
linear iterative equations."
"The fast Fourier transform (FFT) is undoubtedly an essential primitive that
has been applied in various fields of science and engineering. In this paper,
we present a decomposition method for parallelization of multi-dimensional FFTs
with smallest communication amount for all ranges of the number of processes
compared to previously proposed methods. This is achieved by two distinguishing
features: adaptive decomposition and transpose order awareness. In the proposed
method, the FFT data are decomposed based on a row-wise basis that maps the
multi-dimensional data into one-dimensional data, and translates the
corresponding coordinates from multi-dimensions into one-dimension so that the
resultant one-dimensional data can be divided and allocated equally to the
processes. As a result, differently from previous works that have the
dimensions of decomposition pre-defined, our method can adaptively decompose
the FFT data on the lowest possible dimensions depending on the number of
processes. In addition, this row-wise decomposition provides plenty of
alternatives in data transpose, and different transpose order results in
different amount of communication. We identify the best transpose orders with
smallest communication amounts for the 3-D, 4-D, and 5-D FFTs by analyzing all
possible cases. Given both communication efficiency and scalability, our method
is promising in development of highly efficient parallel packages for the FFT."
"The problem of optimizing a linear objective function,given a number of
linear constraints has been a long standing problem ever since the times of
Kantorovich, Dantzig and von Neuman. These developments have been followed by a
different approach pioneered by Khachiyan and Karmarkar.
  In this paper we present an entirely new method for solving an old
optimization problem in a novel manner, a technique that reduces the dimension
of the problem step by step and interestingly is recursive. A theorem which
proves the correctness of the approach is given.
  The method can be extended to other types of optimization problems in convex
space, e.g. for solving a linear optimization problem subject to nonlinear
constraints in a convex region."
"In this paper, we explain the convergence speed of different iteration
schemes with the fluid diffusion view when solving a linear fixed point
problem. This interpretation allows one to better understand why power
iteration or Jacobi iteration may converge faster or slower than Gauss-Seidel
iteration."
"The real symmetric tridiagonal eigenproblem is of outstanding importance in
numerical computations; it arises frequently as part of eigensolvers for
standard and generalized dense Hermitian eigenproblems that are based on a
reduction to tridiagonal form. For its solution, the algorithm of Multiple
Relatively Robust Representations (MRRR) is among the fastest methods. Although
fast, the solvers based on MRRR do not deliver the same accuracy as competing
methods like Divide & Conquer or the QR algorithm. In this paper, we
demonstrate that the use of mixed precisions leads to improved accuracy of
MRRR-based eigensolvers with limited or no performance penalty. As a result, we
obtain eigensolvers that are not only equally or more accurate than the best
available methods, but also -in most circumstances- faster and more scalable
than the competition."
"For many systems of differential equations modeling problems in science and
engineering, there are natural splittings of the right hand side into two
parts, one non-stiff or mildly stiff, and the other one stiff. For such systems
implicit-explicit (IMEX) integration combines an explicit scheme for the
non-stiff part with an implicit scheme for the stiff part.
  In a recent series of papers two of the authors (Sandu and Zhang) have
developed IMEX GLMs, a family of implicit-explicit schemes based on general
linear methods. It has been shown that, due to their high stage order, IMEX
GLMs require no additional coupling order conditions, and are not marred by
order reduction.
  This work develops a new extrapolation-based approach to construct practical
IMEX GLM pairs of high order. We look for methods with large absolute stability
region, assuming that the implicit part of the method is A- or L-stable. We
provide examples of IMEX GLMs with optimal stability properties. Their
application to a two dimensional test problem confirms the theoretical
findings."
"In the theory and practice of inverse problems for partial differential
equations (PDEs) much attention is paid to the problem of the identification of
coefficients from some additional information. This work deals with the problem
of determining in a multidimensional parabolic equation the lower coefficient
that depends on time only. To solve numerically a nonlinear inverse problem,
linearized approximations in time are constructed using standard finite element
procedures in space. The computational algorithm is based on a special
decomposition, where the transition to a new time level is implemented via
solving two standard elliptic problems. The numerical results presented here
for a model 2D problem demonstrate capabilities of the proposed computational
algorithms for approximate solving inverse problems."
"We show that the norm of the commutator defines ""almost a metric"" on the
quotient space of commuting matrices, in the sense that it is a semi-metric
satisfying the triangle inequality asymptotically for large matrices drawn from
a ""good"" distribution."
"We discuss possibilities of application of Numerical Analysis methods to
proving computability, in the sense of the TTE approach, of solution operators
of boundary-value problems for systems of PDEs. We prove computability of the
solution operator for a symmetric hyperbolic system with computable real
coefficients and dissipative boundary conditions, and of the Cauchy problem for
the same system (in this case we also prove computable dependence on the
coefficients) in a cube $Q\subseteq\mathbb R^m$. Such systems describe a wide
variety of physical processes (e.g. elasticity, acoustics, Maxwell equations).
Moreover, many boundary-value problems for the wave equation also can be
reduced to this case, thus we partially answer a question raised in
\cite{wz02}. Compared with most of other existing methods of proving
computability for PDEs, this method does not require existence of explicit
solution formulas and is thus applicable to a broader class of (systems of)
equations."
"The efficiency of exact simulation methods for the reaction-diffusion master
equation (RDME) is severely limited by the large number of diffusion events if
the mesh is fine or if diffusion constants are large. Furthermore, inherent
properties of exact kinetic-Monte Carlo simulation methods limit the efficiency
of parallel implementations. Several approximate and hybrid methods have
appeared that enable more efficient simulation of the RDME. A common feature to
most of them is that they rely on splitting the system into its reaction and
diffusion parts and updating them sequentially over a discrete timestep. This
use of operator splitting enables more efficient simulation but it comes at the
price of a temporal discretization error that depends on the size of the
timestep. So far, existing methods have not attempted to estimate or control
this error in a systematic manner. This makes the solvers hard to use for
practitioners since they must guess an appropriate timestep. It also makes the
solvers potentially less efficient than if the timesteps are adapted to control
the error. Here, we derive estimates of the local error and propose a strategy
to adaptively select the timestep when the RDME is simulated via a first order
operator splitting. While the strategy is general and applicable to a wide
range of approximate and hybrid methods, we exemplify it here by extending a
previously published approximate method, the Diffusive Finite-State Projection
(DFSP) method, to incorporate temporal adaptivity."
"We present a way of constructing multi-time-step monolithic coupling methods
for elastodynamics. The governing equations for constrained multiple subdomains
are written in dual Schur form and enforce the continuity of velocities at
system time levels. The resulting equations will be in the form of
differential-algebraic equations. To crystallize the ideas we shall employ
Newmark family of time-stepping schemes. The proposed method can handle
multiple subdomains, and allows different time-steps as well as different time
stepping schemes from the Newmark family in different subdomains. We shall use
the energy method to assess the numerical stability, and quantify the influence
of perturbations under the proposed coupling method. We also discuss the
conditions under which the proposed method will be energy preserving, and the
conditions under which the method will be energy conserving. Several numerical
examples are presented to illustrate the accuracy and stability properties of
the proposed method. We shall also compare the proposed multi-time-step
coupling method with some other similar methods available in the literature."
"This work introduces a time-adaptive strategy that uses a refinement
estimator based on the first Frenet curvature. In dynamics, a time-adaptive
strategy is a mechanism that interactively proposes changes to the time step
used in iterative methods of solution. These changes aim to improve the
relation between quality of response and computational cost. The method here
proposed is suitable for a variety of numerical time integration problems,
e.g., in the study of bodies subjected to dynamical loads. The motion equation
in its space-discrete form is used as reference to derive the formulation
presented in this paper. Our method is contrasted with other ones based on
local error estimator and apparent frequencies. We check the performance of our
proposal when employed with the central difference, the explicit
generalized-alpha and the Chung-Lee integration methods. The proposed
refinement estimator demands low computational resources, being easily applied
to several direct integration methods."
"GROUSE (Grassmannian Rank-One Update Subspace Estimation) is an iterative
algorithm for identifying a linear subspace of R^n from data consisting of
partial observations of random vectors from that subspace. This paper examines
local convergence properties of GROUSE, under assumptions on the randomness of
the observed vectors, the randomness of the subset of elements observed at each
iteration, and incoherence of the subspace with the coordinate directions.
Convergence at an expected linear rate is demonstrated under certain
assumptions. The case in which the full random vector is revealed at each
iteration allows for much simpler analysis, and is also described. GROUSE is
related to incremental SVD methods and to gradient projection algorithms in
optimization."
"We present new results on Boolean matrix factorization and a new algorithm
based on these results. The results emphasize the significance of
factorizations that provide from-below approximations of the input matrix.
While the previously proposed algorithms do not consider the possibly different
significance of different matrix entries, our results help measure such
significance and suggest where to focus when computing factors. An experimental
evaluation of the new algorithm on both synthetic and real data demonstrates
its good performance in terms of good coverage by the first k factors as well
as a small number of factors needed for exact decomposition and indicates that
the algorithm outperforms the available ones in these terms. We also propose
future research topics."
"This article is concerned with Monte-Carlo methods for the estimation of the
trace of an implicitly given matrix $A$ whose information is only available
through matrix-vector products. Such a method approximates the trace by an
average of $N$ expressions of the form $\ww^t (A\ww)$, with random vectors
$\ww$ drawn from an appropriate distribution. We prove, discuss and experiment
with bounds on the number of realizations $N$ required in order to guarantee a
probabilistic bound on the relative error of the trace estimation upon
employing Rademacher (Hutchinson), Gaussian and uniform unit vector (with and
without replacement) probability distributions.
  In total, one necessary bound and six sufficient bounds are proved, improving
upon and extending similar estimates obtained in the seminal work of Avron and
Toledo (2011) in several dimensions. We first improve their bound on $N$ for
the Hutchinson method, dropping a term that relates to $rank(A)$ and making the
bound comparable with that for the Gaussian estimator.
  We further prove new sufficient bounds for the Hutchinson, Gaussian and the
unit vector estimators, as well as a necessary bound for the Gaussian
estimator, which depend more specifically on properties of the matrix $A$. As
such they may suggest for what type of matrices one distribution or another
provides a particularly effective or relatively ineffective stochastic
estimation method."
"Among optimal hierarchical algorithms for the computational solution of
elliptic problems, the Fast Multipole Method (FMM) stands out for its
adaptability to emerging architectures, having high arithmetic intensity,
tunable accuracy, and relaxable global synchronization requirements. We
demonstrate that, beyond its traditional use as a solver in problems for which
explicit free-space kernel representations are available, the FMM has
applicability as a preconditioner in finite domain elliptic boundary value
problems, by equipping it with boundary integral capability for satisfying
conditions at finite boundaries and by wrapping it in a Krylov method for
extensibility to more general operators. Here, we do not discuss the well
developed applications of FMM to implement matrix-vector multiplications within
Krylov solvers of boundary element methods. Instead, we propose using FMM for
the volume-to-volume contribution of inhomogeneous Poisson-like problems, where
the boundary integral is a small part of the overall computation. Our method
may be used to precondition sparse matrices arising from finite
difference/element discretizations, and can handle a broader range of
scientific applications. Compared with multigrid methods, it is capable of
comparable algebraic convergence rates down to the truncation error of the
discretized PDE, and it offers potentially superior multicore and distributed
memory scalability properties on commodity architecture supercomputers.
Compared with other methods exploiting the low rank character of off-diagonal
blocks of the dense resolvent operator, FMM-preconditioned Krylov iteration may
reduce the amount of communication because it is matrix-free and exploits the
tree structure of FMM. We describe our tests in reproducible detail with freely
available codes and outline directions for further extensibility."
"The development of randomized algorithms for numerical linear algebra, e.g.
for computing approximate QR and SVD factorizations, has recently become an
intense area of research. This paper studies one of the most frequently
discussed algorithms in the literature for dimensionality
reduction---specifically for approximating an input matrix with a low-rank
element. We introduce a novel and rather intuitive analysis of the algorithm in
Martinsson et al. (2008), which allows us to derive sharp estimates and give
new insights about its performance. This analysis yields theoretical guarantees
about the approximation error and at the same time, ultimate limits of
performance (lower bounds) showing that our upper bounds are tight. Numerical
experiments complement our study and show the tightness of our predictions
compared with empirical observations."
"This work deals with the problem of choosing a time step for the numerical
solution of boundary value problems for parabolic equations. The problem
solution is derived using the fully implicit scheme, whereas a time step is
selected via explicit calculations. The selection strategy consists of the
following steps. First, using the explicit scheme, we calculate the solution at
a new time level. Next, we employ this solution in order to obtain the solution
at the previous time level (the implicit scheme, explicit calculations). This
solution should be close to the solution of our problem at this time level with
a prescribed accuracy. Such an algorithm leads to explicit formulas for the
calculation of the time step and takes into account both the dynamics of the
problem solution and changes in coefficients of the equation and in its
right-hand side. The same formulas for the evaluation of the time step we get
using a comparison of two approximate solutions, which are obtained using the
explicit scheme with the primary time step and the step that is reduced by
half. Numerical results are presented for a model parabolic boundary value
problem, which demonstrate the robustness of the developed algorithm for the
time step selection."
"We present the first parallel algorithm for solving systems of linear
equations in symmetric, diagonally dominant (SDD) matrices that runs in
polylogarithmic time and nearly-linear work. The heart of our algorithm is a
construction of a sparse approximate inverse chain for the input matrix: a
sequence of sparse matrices whose product approximates its inverse. Whereas
other fast algorithms for solving systems of equations in SDD matrices exploit
low-stretch spanning trees, our algorithm only requires spectral graph
sparsifiers."
"In this work, we consider the coupled systems of linear unsteady partial
differential equations, which arise in the modeling of poroelasticity
processes. Stability estimates of weighted difference schemes for the coupled
system of equations are presented. Approximation in space is based on the
finite element method. We construct splitting schemes and give some numerical
comparisons for typical poroelasticity problems. The results of numerical
simulation of a 3D problem are presented. Special attention is given to using
hight performance computing systems."
"Inverse problems involving systems of partial differential equations (PDEs)
with many measurements or experiments can be very expensive to solve
numerically. In a recent paper we examined dimensionality reduction methods,
both stochastic and deterministic, to reduce this computational burden,
assuming that all experiments share the same set of receivers. In the present
article we consider the more general and practically important case where
receivers are not shared across experiments. We propose a data completion
approach to alleviate this problem. This is done by means of an approximation
using an appropriately restricted gradient or Laplacian regularization,
extending existing data for each experiment to the union of all receiver
locations. Results using the method of simultaneous sources (SS) with the
completed data are then compared to those obtained by a more general but slower
random subset (RS) method which requires no modifications."
"We explore the link between data representation and soft errors in dot
products. We present an analytic model for the absolute error introduced should
a soft error corrupt a bit in an IEEE-754 floating-point number. We show how
this finding relates to the fundamental linear algebra concepts of
normalization and matrix equilibration. We present a case study illustrating
that the probability of experiencing a large error in a dot product is
minimized when both vectors are normalized. Furthermore, when data is
normalized we show that the absolute error is less than one or very large,
which allows us to detect large errors. We demonstrate how this finding can be
used by instrumenting the GMRES iterative solver. We count all possible errors
that can be introduced through faults in arithmetic in the computationally
intensive orthogonalization phase, and show that when scaling is used the
absolute error can be bounded above by one."
"What is called ""numerical reproducibility"" is the problem of getting the same
result when the scientific computation is run several times, either on the same
machine or on different machines, with different types and numbers of
processing units, execution environments, computational loads etc. This problem
is especially stringent for HPC numerical simulations. In what follows, the
focus is on parallel implementations of interval arithmetic using
floating-point arithmetic. For interval computations, numerical reproducibility
is of course an issue for testing and debugging purposes. However, as long as
the computed result encloses the exact and unknown result, the inclusion
property, which is the main property of interval arithmetic, is satisfied and
getting bit for bit identical results may not be crucial. Still, implementation
issues may invalidate the inclusion property. Several ways to preserve the
inclusion property are presented, on the example of the product of matrices
with interval coefficients."
"In this paper we address the problem of recovering a matrix, with inherent
low rank structure, from its lower dimensional projections. This problem is
frequently encountered in wide range of areas including pattern recognition,
wireless sensor networks, control systems, recommender systems, image/video
reconstruction etc. Both in theory and practice, the most optimal way to solve
the low rank matrix recovery problem is via nuclear norm minimization. In this
paper, we propose a Split Bregman algorithm for nuclear norm minimization. The
use of Bregman technique improves the convergence speed of our algorithm and
gives a higher success rate. Also, the accuracy of reconstruction is much
better even for cases where small number of linear measurements are available.
Our claim is supported by empirical results obtained using our algorithm and
its comparison to other existing methods for matrix recovery. The algorithms
are compared on the basis of NMSE, execution time and success rate for varying
ranks and sampling ratios."
"A boundary value problem for a fractional power of the second-order elliptic
operator is considered. It is solved numerically using a time-dependent problem
for a pseudo-parabolic equation. For the auxiliary Cauchy problem, the standard
two-level schemes with weights are applied. Stability conditions are obtained
for the fully discrete schemes under the consideration. The numerical results
are presented for a model two-dimensional boundary value problem wit a
fractional power of an elliptic operator. The dependence of accuracy on grids
in time and in space is studied."
"Tensor diagonalization means transforming a given tensor to an exactly or
nearly diagonal form through multiplying the tensor by non-orthogonal
invertible matrices along selected dimensions of the tensor. It is
generalization of approximate joint diagonalization (AJD) of a set of matrices.
In particular, we derive (1) a new algorithm for symmetric AJD, which is called
two-sided symmetric diagonalization of order-three tensor, (2) a similar
algorithm for non-symmetric AJD, also called general two-sided diagonalization
of an order-3 tensor, and (3) an algorithm for three-sided diagonalization of
order-3 or order-4 tensors. The latter two algorithms may serve for canonical
polyadic (CP) tensor decomposition, and they can outperform other CP tensor
decomposition methods in terms of computational speed under the restriction
that the tensor rank does not exceed the tensor multilinear rank. Finally, we
propose (4) similar algorithms for tensor block diagonalization, which is
related to the tensor block-term decomposition."
"This paper introduces tensorial calculus techniques in the framework of
Proper Orthogonal Decomposition (POD) to reduce the computational complexity of
the reduced nonlinear terms. The resulting method, named tensorial POD, can be
applied to polynomial nonlinearities of any degree $p$. Such nonlinear terms
have an on-line complexity of $\mathcal{O}(k^{p+1})$, where $k$ is the
dimension of POD basis, and therefore is independent of full space dimension.
However it is efficient only for quadratic nonlinear terms since for higher
nonlinearities standard POD proves to be less time consuming once the POD basis
dimension $k$ is increased. Numerical experiments are carried out with a two
dimensional shallow water equation (SWE) test problem to compare the
performance of tensorial POD, standard POD, and POD/Discrete Empirical
Interpolation Method (DEIM). Numerical results show that tensorial POD
decreases by $76\times$ times the computational cost of the on-line stage of
standard POD for configurations using more than $300,000$ model variables. The
tensorial POD SWE model was only $2-8\times$ slower than the POD/DEIM SWE model
but the implementation effort is considerably increased. Tensorial calculus was
again employed to construct a new algorithm allowing POD/DEIM shallow water
equation model to compute its off-line stage faster than the standard and
tensorial POD approaches."
"In this paper, we improve the usual relative error bound for the computation
of x^n through iterated multiplications by x in binary floating-point
arithmetic. The obtained error bound is only slightly better than the usual
one, but it is simpler. We also discuss the more general problem of computing
the product of n terms."
"Implant placement under soft tissues operation is described. In this
operation tissues can reach such deformations that nonlinear properties are
appeared. A mass-spring model modification for modeling nonlinear tissue
operation is developed. A method for creating elasticity module using splines
is described. For Poisson ratio different stiffness for different types of
springs in cubic grid is used. For stiffness finding an equation system that
described material tension is solved. The model is verified with quadratic
sample tension experiment. These tests show that sample tension under external
forces is equal to defined nonlinear elasticity module. The accuracy of Poisson
ratio modeling is thirty five percent that is better the results of available
ratio modeling method."
"The method of monotonization of difference schemes is being considered in the
paper. The method was earlier proposed by the author for stationary problems.
It is investigated in the paper more profoundly. The idea of the method is to
build the monotonizing operators into the schemes so that the balance relations
from point to point are not violated. Different monotonizing operators can be
used to be installed in the schemes. Propositions concerning approximation and
stability of the monotonized schemes are formulated and proved. Also a
proposition significant for practical use of the schemes is formulated and
proved. The idea is to use the monotonized schemes in the cases when the
proposition conditions are fulfilled. The proposition is based on closeness of
solutions of the initial and auxiliary schemes. Constructions for solving of
time dependent problems are also written in the paper. One dimensional example
and three-dimensional hydrodynamic example are considered. The method allows to
considerably decrease value of calculations in many cases."
"We investigate the methods that simultaneously enforce sparsity and low-rank
structure in a matrix as often employed for sparse phase retrieval problems or
phase calibration problems in compressive sensing. We propose a new approach
for analyzing the trade off between the sparsity and low rank constraints in
these approaches which not only helps to provide guidelines to adjust the
weights between the aforementioned constraints, but also enables new simulation
strategies for evaluating performance. We then provide simulation results for
phase retrieval and phase calibration cases both to demonstrate the consistency
of the proposed method with other approaches and to evaluate the change of
performance with different weights for the sparsity and low rank structure
constraints."
"This article presents a fast solver for the dense ""frontal"" matrices that
arise from the multifrontal sparse elimination process of 3D elliptic PDEs. The
solver relies on the fact that these matrices can be efficiently represented as
a hierarchically off-diagonal low-rank (HODLR) matrix. To construct the
low-rank approximation of the off-diagonal blocks, we propose a new
pseudo-skeleton scheme, the boundary distance low-rank approximation, that
picks rows and columns based on the location of their corresponding vertices in
the sparse matrix graph. We compare this new low-rank approximation method to
the adaptive cross approximation (ACA) algorithm and show that it achieves
betters speedup specially for unstructured meshes. Using the HODLR direct
solver as a preconditioner (with a low tolerance) to the GMRES iterative
scheme, we can reach machine accuracy much faster than a conventional LU
solver. Numerical benchmarks are provided for frontal matrices arising from 3D
finite element problems corresponding to a wide range of applications."
"A basic issue in optimization, inverse theory,neural networks, computational
chemistry and many other problems is the geometrical characterization of high
dimensional functions. In inverse calculations one aims to characterize the set
of models that fit the data (among other constraints). If the data misfit
function is unimodal then one can find its peak by local optimization methods
and characterize its width (related to the range of data-fitting models) by
estimating derivatives at this peak. On the other hand, if there are local
extrema, then a number of interesting and difficult problems arise. Are the
local extrema important compared to the global or can they be eliminated (e.g.,
by smoothing) without significant loss of information? Is there a sufficiently
small number of local extrema that they can be enumerated via local
optimization? What are the basins of attraction of these local extrema? Can two
extrema be joined by a path that never goes uphill? Can the whole problem be
reduced to one of enumerating the local extrema and their basins of attraction?
For locally ill-conditioned functions, premature convergence of local
optimization can be confused with the presence of local extrema. Addressing any
of these issues requires topographic information about the functions under
study. But in many applications these functions may have hundreds or thousands
of variables and can only be evaluated pointwise (by some numerical method for
instance). In this paper we describe systematic (but generic) methods of
analysing the topography of high dimensional functions using local optimization
methods applied to randomly chosen starting models. We provide a number of
quantitative measures of function topography that have proven to be useful in
practical problems along with error estimates."
"Developing robust simulation tools for problems involving multiple
mathematical scales has been a subject of great interest in computational
mathematics and engineering. A desirable feature to have in a numerical
formulation for multiscale transient problems is to be able to employ different
time-steps (multi-time-step coupling), and different time integrators and
different numerical formulations (mixed methods) in different regions of the
computational domain. We present two new monolithic multi-time-step mixed
coupling methods for first-order transient systems. We shall employ unsteady
advection-diffusion-reaction equation with linear decay as the model problem,
which offers several unique challenges in terms of non-self-adjoint spatial
operator and rich features in the solutions. We shall employ the dual Schur
domain decomposition technique to handle the decomposition of domain into
subdomains. Two different methods of enforcing compatibility along the
subdomain interface will be used in the time discrete setting. A systematic
theoretical analysis (which includes numerical stability, influence of
perturbations, bounds on drift along the subdomain interface) will be
performed. The first coupling method ensures that there is no drift along the
subdomain interface but does not facilitate explicit/implicit coupling. The
second coupling method allows explicit/implicit coupling with controlled (but
non-zero) drift in the solution along the subdomain interface. Several
canonical problems will be solved to numerically verify the theoretical
predictions, and to illustrate the overall performance of the proposed coupling
methods. Finally, we shall illustrate the robustness of the proposed coupling
methods using a multi-time-step transient simulation of a fast bimolecular
advective-diffusive-reactive system."
"This work concerns the use of the iterative algorithm (KMF algorithm)
proposed by Kozlov, Mazya and Fomin to solve the Cauchy problem for Laplaces
equation. This problem consists to recovering the lacking data on some part of
the boundary using the over specified conditions on the other part of the
boundary. We describe an alternating formulation of the KMF algorithm and its
relationship with a classical formulation. The implementation of this algorithm
for a regular domain is performed by the finite element method using the
software Freefem. The numerical tests developed show the effectiveness of the
proposed algorithm since it allows to have more accurate results as well as
reducing the number of iterations needed for convergence."
"Computational kernel of the three-dimensional variational data assimilation
(3D-Var) problem is a linear system, generally solved by means of an iterative
method. The most costly part of each iterative step is a matrix-vector product
with a very large covariance matrix having Gaussian correlation structure. This
operation may be interpreted as a Gaussian convolution, that is a very
expensive numerical kernel. Recursive Filters (RFs) are a well known way to
approximate the Gaussian convolution and are intensively applied in the
meteorology, in the oceanography and in forecast models. In this paper, we deal
with an oceanographic 3D-Var data assimilation scheme, named OceanVar, where
the linear system is solved by using the Conjugate Gradient (GC) method by
replacing, at each step, the Gaussian convolution with RFs. Here we give
theoretical issues on the discrete convolution approximation with a first order
(1st-RF) and a third order (3rd-RF) recursive filters. Numerical experiments
confirm given error bounds and show the benefits, in terms of accuracy and
performance, of the 3-rd RF."
"A method of representation of a solution as segments of the series in powers
of the step of the independent variable is expanded for solving complex systems
of ordinary differential equations (ODE): the Lorenz system and other systems.
A new procedure of reduction of the representation of the solution to a sum of
two parts (regular and random) is performed. A shifting procedure is applied in
each level of the independent variable to the random part and it acts as the
filter that extracts the values to the regular part. In certain cases it is
possible to omit the random part and construct the approximation which does not
converge but still provides the qualitative information about the full solution
(a linear approximation provides a simple exact solution). Evaluation of the
error for this case is performed. Constructing the analytical representation of
the solutions for these systems by the developed method is presented."
"An isogeometric boundary element method for problems in elasticity is
presented, which is based on an independent approximation for the geometry,
traction and displacement field. This enables a flexible choice of refinement
strategies, permits an efficient evaluation of geometry related information, a
mixed collocation scheme which deals with discontinuous tractions along
non-smooth boundaries and a significant reduction of the right hand side of the
system of equations for common boundary conditions. All these benefits are
achieved without any loss of accuracy compared to conventional isogeometric
formulations. The system matrices are approximated by means of hierarchical
matrices to reduce the computational complexity for large scale analysis. For
the required geometrical bisection of the domain, a strategy for the evaluation
of bounding boxes containing the supports of NURBS basis functions is
presented. The versatility and accuracy of the proposed methodology is
demonstrated by convergence studies showing optimal rates and real world
examples in two and three dimensions."
"In this work we address the complexity problem of the isogeometric Boundary
Element Method by proposing a collocation scheme for practical problems in
linear elasticity and the application of hierarchical matrices. For mixed
boundary value problems, a block system of matrices similar to Galerkin
formulations is constructed allowing an effective application of that matrix
format. We introduce a strategy for the geometric bisection of surfaces based
on NURBS patches. The approximation of system matrices is carried out by means
of kernel interpolation. Numerical results are shown that prove the success of
the formulation."
"A novel approach to the simulation with the boundary element method using
trimmed NURBS patches is presented. The advantage of this approach is its
efficiency and easy implementation. The analysis with trimmed NURBS is achieved
by double mapping. The variation of the unknowns on the boundary is specified
in a local coordinate system and is completely independent of the description
of the geometry. The method is tested on a branched tunnel and the results
compared with those obtained from a conventional analysis. The conclusion is
that the proposed approach is superior in terms of number of unknowns and
effort required."
"Motivated by Pryce's structural index reduction method for differential
algebraic equations (DAEs), we show the complexity of the fixed-point iteration
algorithm and propose a fixed-point iteration method with parameters. It leads
to a block fixed-point iteration method which can be applied to large-scale
DAEs with block upper triangular structure. Moreover, its complexity analysis
is also given in this paper."
"Sparse signal restoration is usually formulated as the minimization of a
quadratic cost function $\|y-Ax\|_2^2$, where A is a dictionary and x is an
unknown sparse vector. It is well-known that imposing an $\ell_0$ constraint
leads to an NP-hard minimization problem. The convex relaxation approach has
received considerable attention, where the $\ell_0$-norm is replaced by the
$\ell_1$-norm. Among the many efficient $\ell_1$ solvers, the homotopy
algorithm minimizes $\|y-Ax\|_2^2+\lambda\|x\|_1$ with respect to x for a
continuum of $\lambda$'s. It is inspired by the piecewise regularity of the
$\ell_1$-regularization path, also referred to as the homotopy path. In this
paper, we address the minimization problem $\|y-Ax\|_2^2+\lambda\|x\|_0$ for a
continuum of $\lambda$'s and propose two heuristic search algorithms for
$\ell_0$-homotopy. Continuation Single Best Replacement is a forward-backward
greedy strategy extending the Single Best Replacement algorithm, previously
proposed for $\ell_0$-minimization at a given $\lambda$. The adaptive search of
the $\lambda$-values is inspired by $\ell_1$-homotopy. $\ell_0$ Regularization
Path Descent is a more complex algorithm exploiting the structural properties
of the $\ell_0$-regularization path, which is piecewise constant with respect
to $\lambda$. Both algorithms are empirically evaluated for difficult inverse
problems involving ill-conditioned dictionaries. Finally, we show that they can
be easily coupled with usual methods of model order selection."
"In this paper we present an algorithm for adaptive sparse grid approximations
of quantities of interest computed from discretized partial differential
equations. We use adjoint-based a posteriori error estimates of the physical
discretization error and the interpolation error in the sparse grid to enhance
the sparse grid approximation and to drive adaptivity of the sparse grid.
Utilizing these error estimates provides significantly more accurate functional
values for random samples of the sparse grid approximation. We also demonstrate
that alternative refinement strategies based upon a posteriori error estimates
can lead to further increases in accuracy in the approximation over traditional
hierarchical surplus based strategies. Throughout this paper we also provide
and test a framework for balancing the physical discretization error with the
stochastic interpolation error of the enhanced sparse grid approximation."
"Higher-order tensors are becoming prevalent in many scientific areas such as
computer vision, social network analysis, data mining and neuroscience.
Traditional tensor decomposition approaches face three major challenges: model
selecting, gross corruptions and computational efficiency. To address these
problems, we first propose a parallel trace norm regularized tensor
decomposition method, and formulate it as a convex optimization problem. This
method does not require the rank of each mode to be specified beforehand, and
can automatically determine the number of factors in each mode through our
optimization scheme. By considering the low-rank structure of the observed
tensor, we analyze the equivalent relationship of the trace norm between a
low-rank tensor and its core tensor. Then, we cast a non-convex tensor
decomposition model into a weighted combination of multiple much smaller-scale
matrix trace norm minimization. Finally, we develop two parallel alternating
direction methods of multipliers (ADMM) to solve our problems. Experimental
results verify that our regularized formulation is effective, and our methods
are robust to noise or outliers."
"In the numerical solution of partial differential equations using a
method-of-lines approach, the availability of high order spatial discretization
schemes motivates the development of sophisticated high order time integration
methods. For multiphysics problems with both stiff and non-stiff terms
implicit-explicit (IMEX) time stepping methods attempt to combine the lower
cost advantage of explicit schemes with the favorable stability properties of
implicit schemes. Existing high order IMEX Runge Kutta or linear multistep
methods, however, suffer from accuracy or stability reduction.
  This work shows that IMEX general linear methods (GLMs) are competitive
alternatives to classic IMEX schemes for large problems arising in practice.
High order IMEX-GLMs are constructed in the framework developed by the authors
[34]. The stability regions of the new schemes are optimized numerically. The
resulting IMEX-GLMs have similar stability properties as IMEX Runge-Kutta
methods, but they do not suffer from order reduction, and are superior in terms
of accuracy and efficiency. Numerical experiments with two and three
dimensional test problems illustrate the potential of the new schemes to speed
up complex applications."
"In this paper we review basic and emerging models and associated algorithms
for large-scale tensor networks, especially Tensor Train (TT) decompositions
using novel mathematical and graphical representations. We discus the concept
of tensorization (i.e., creating very high-order tensors from lower-order
original data) and super compression of data achieved via quantized tensor
train (QTT) networks. The purpose of a tensorization and quantization is to
achieve, via low-rank tensor approximations ""super"" compression, and
meaningful, compact representation of structured data. The main objective of
this paper is to show how tensor networks can be used to solve a wide class of
big data optimization problems (that are far from tractable by classical
numerical methods) by applying tensorization and performing all operations
using relatively small size matrices and tensors and applying iteratively
optimized and approximative tensor contractions.
  Keywords: Tensor networks, tensor train (TT) decompositions, matrix product
states (MPS), matrix product operators (MPO), basic tensor operations,
tensorization, distributed representation od data optimization problems for
very large-scale problems: generalized eigenvalue decomposition (GEVD),
PCA/SVD, canonical correlation analysis (CCA)."
"In this paper we describe the implementation of our C++ resistive
magnetohydrodynamics solver. The framework developed facilitates the separation
of the code implementing the specific numerical method and the physical model,
on the one hand, from the handling of boundary conditions and the management of
the computational domain, on the other hand. In particular, this will allow us
to use finite difference stencils which are only defined in the interior of the
domain (the boundary conditions are handled automatically). We will discuss
this and other design considerations and their impact on performance in some
detail. In addition, we provide a documentation of the code developed and
demonstrate that a performance comparable to Fortran can be achieved, while
still maintaining a maximum of code readability and extensibility."
"Kaczmarz's alternating projection method has been widely used for solving a
consistent (mostly over-determined) linear system of equations Ax=b. Because of
its simple iterative nature with light computation, this method was
successfully applied in computerized tomography. Since tomography generates a
matrix A with highly coherent rows, randomized Kaczmarz algorithm is expected
to provide faster convergence as it picks a row for each iteration at random,
based on a certain probability distribution. It was recently shown that picking
a row at random, proportional with its norm, makes the iteration converge
exponentially in expectation with a decay constant that depends on the scaled
condition number of A and not the number of equations. Since Kaczmarz's method
is a subspace projection method, the convergence rate for simple Kaczmarz
algorithm was developed in terms of subspace angles. This paper provides
analyses of simple and randomized Kaczmarz algorithms and explain the link
between them. It also propose new versions of randomization that may speed up
convergence."
"In this paper we present a basis selection method that can be used with
$\ell_1$-minimization to adaptively determine the large coefficients of
polynomial chaos expansions (PCE). The adaptive construction produces
anisotropic basis sets that have more terms in important dimensions and limits
the number of unimportant terms that increase mutual coherence and thus degrade
the performance of $\ell_1$-minimization. The important features and the
accuracy of basis selection are demonstrated with a number of numerical
examples. Specifically, we show that for a given computational budget, basis
selection produces a more accurate PCE than would be obtained if the basis is
fixed a priori. We also demonstrate that basis selection can be applied with
non-uniform random variables and can leverage gradient information."
"We present new algorithms for computing the log-determinant of symmetric,
diagonally dominant matrices. Existing algorithms run with cubic complexity
with respect to the size of the matrix in the worst case. Our algorithm
computes an approximation of the log-determinant in time near-linear with
respect to the number of non-zero entries and with high probability. This
algorithm builds upon the utra-sparsifiers introduced by Spielman and Teng for
Laplacian matrices and ultimately uses their refined versions introduced by
Koutis, Miller and Peng in the context of solving linear systems. We also
present simpler algorithms that compute upper and lower bounds and that may be
of more immediate practical interest."
"In this research, the Bernoulli polynomials are introduced. The properties of
these polynomials are employed to construct the operational matrices of
integration together with the derivative and product. These properties are then
utilized to transform the differential equation to a matrix equation which
corresponds to a system of algebraic equations with unknown Bernoulli
coefficients. This method can be used for many problems such as differential
equations, integral equations and so on. Numerical examples show the method is
computationally simple and also illustrate the efficiency and accuracy of the
method."
"In this paper a numerical meshless method for solving the radiative transfer
equations in a slab medium with an isotropic scattering is considered. The
method is based on radial basis functions to approximate the solution of an
integral-partial differential equation by using collocation method. For this
purpose different applications of RBFs are used. To this end the numerical
solutions are obtained without any mesh generation into the domain of the
problems. The results of numerical experiments are compared with the existing
results in illustrative examples to confirm the accuracy and efficiency of the
presented scheme. Also the norm of the residual functions are obtained to show
the convergence of the method."
"The computation of a few singular triplets of large, sparse matrices is a
challenging task, especially when the smallest magnitude singular values are
needed in high accuracy. Most recent efforts try to address this problem
through variations of the Lanczos bidiagonalization method, but they are still
challenged even for medium matrix sizes due to the difficulty of the problem.
We propose a novel SVD approach that can take advantage of preconditioning and
of any well designed eigensolver to compute both largest and smallest singular
triplets. Accuracy and efficiency is achieved through a hybrid, two-stage
meta-method, PHSVDS. In the first stage, PHSVDS solves the normal equations up
to the best achievable accuracy. If further accuracy is required, the method
switches automatically to an eigenvalue problem with the augmented matrix. Thus
it combines the advantages of the two stages, faster convergence and accuracy,
respectively. For the augmented matrix, solving the interior eigenvalue is
facilitated by a proper use of the good initial guesses from the first stage
and an efficient implementation of the refined projection method. We also
discuss how to precondition PHSVDS and to cope with some issues that arise.
Numerical experiments illustrate the efficiency and robustness of the method."
"Iterative numerical algorithms are typically equipped with a stopping
criterion, where the iteration process is terminated when some error or misfit
measure is deemed to be below a given tolerance. This is a useful setting for
comparing algorithm performance, among other purposes. However, in practical
applications a precise value for such a tolerance is rarely known; rather, only
some possibly vague idea of the desired quality of the numerical approximation
is at hand. We discuss four case studies from different areas of numerical
computation, where uncertainty in the error tolerance value and in the stopping
criterion is revealed in different ways. This leads us to think of approaches
to relax the notion of exactly satisfying a tolerance value. We then
concentrate on a {\em probabilistic} relaxation of the given tolerance. This
allows, for instance, derivation of proven bounds on the sample size of certain
Monte Carlo methods. We describe an algorithm that becomes more efficient in a
controlled way as the uncertainty in the tolerance increases, and demonstrate
this in the context of some particular applications of inverse problems."
"We briefly review the basic ideas behind archetypal analysis for matrix
factorization and discuss its behavior in approximating the convex hull of a
data sample. We then ask how good such approximations can be and consider
different cases. Understanding archetypal analysis as the problem of computing
a convexity constrained low-rank approximation of the identity matrix provides
estimates for archetypal analysis and the SiVM heuristic."
"In this article, we introduce a fast and memory efficient solver for sparse
matrices arising from the finite element discretization of elliptic partial
differential equations (PDEs). We use a fast direct (but approximate)
multifrontal solver as a preconditioner, and use an iterative solver to achieve
a desired accuracy. This approach combines the advantages of direct and
iterative schemes to arrive at a fast, robust and accurate solver. We will show
that this solver is faster ($\sim$ 2x) and more memory efficient ($\sim$ 2--3x)
than a conventional direct multifrontal solver. Furthermore, we will
demonstrate that the solver is both a faster and more effective preconditioner
than other preconditioners such as the incomplete LU preconditioner. Specific
speed-ups depend on the matrix size and improve as the size of the matrix
increases. The solver can be applied to both structured and unstructured meshes
in a similar manner. We build on our previous work and utilize the fact that
dense frontal and update matrices, in the multifrontal algorithm, can be
represented as hierarchically off-diagonal low-rank (HODLR) matrices. Using
this idea, we replace all large dense matrix operations in the multifrontal
elimination process with $O(N)$ HODLR operations to arrive at a faster and more
memory efficient solver."
"The comprehensive generalization of summation-by-parts of Del Rey Fern\'andez
et al.\ (J. Comput. Phys., 266, 2014) is extended to approximations of second
derivatives with variable coefficients. This enables the construction of
second-derivative operators with one or more of the following characteristics:
i) non-repeating interior stencil, ii) nonuniform nodal distributions, and iii)
exclusion of one or both boundary nodes. Definitions are proposed that give
rise to generalized SBP operators that result in consistent, conservative, and
stable discretizations of PDEs with or without mixed derivatives. It is proven
that such operators can be constructed using a correction to the application of
the first-derivative operator twice that is the same as used for the
constant-coefficient operator. Moreover, for operators with a repeating
interior stencil, a decomposition is proposed that makes the application of
such operators particularly simple. A number of novel operators are
constructed, including operators on pseudo-spectral nodal distributions and
operators that have a repeating interior stencil, but unequal nodal spacing
near boundaries. The various operators are compared to the application of the
first-derivative operator twice in the context of the linear
convection-diffusion equation with constant and variable coefficients."
"This paper describes a node relocation algorithm based on nonlinear
optimization which delivers excellent results for both unstructured and
structured plane triangle meshes over convex as well as non-convex domains with
high curvature. The local optimization scheme is a damped Newton's method in
which the gradient and Hessian of the objective function are evaluated exactly.
The algorithm has been developed in order to continuously rezone the mesh in
arbitrary Lagrangian-Eulerian (ALE) methods for large deformation penetration
problems, but it is also suitable for initial mesh improvement. Numerical
examples highlight the capabilities of the algorithm."
"In this paper a local Fourier analysis for multigrid methods on tetrahedral
grids is presented. Different smoothers for the discretization of the Laplace
operator by linear finite elements on such grids are analyzed. A four-color
smoother is presented as an efficient choice for regular tetrahedral grids,
whereas line and plane relaxations are needed for poorly shaped tetrahedra. A
novel partitioning of the Fourier space is proposed to analyze the four-color
smoother. Numerical test calculations validate the theoretical predictions. A
multigrid method is constructed in a block-wise form, by using different
smoothers and different numbers of pre- and post-smoothing steps in each
tetrahedron of the coarsest grid of the domain. Some numerical experiments are
presented to illustrate the efficiency of this multigrid algorithm."
"The p-norm often used in stress constrained topology optimisation supposedly
mimics a delta function and it is thus characterised by a small length scale
and ideally one would also prefer to have the solid-void transition occur over
a small length scale, since the material in this transition does not have a
clear physical interpretation. We propose to resolve these small length scales
using anisotropic mesh adaptation. We use the method of moving asymptotes with
interpolation of sensitivities, asymptotes and design variables between
iterations. We demonstrate this combination for the portal and L-bracket
problems with p=10, and we are able to investigate mesh dependence. Finally, we
suggest relaxing the L-bracket problem statement by introducing a rounded
corner."
"A Random SubMatrix method (RSM) is proposed to calculate the low-rank
decomposition of large-scale matrices with known entry percentage \rho. RSM is
very fast as the floating-point operations (flops) required are compared
favorably with the state-of-the-art algorithms. Meanwhile RSM is very
memory-saving. With known entries homogeneously distributed in the given
matrix, sub-matrices formed by known entries are randomly selected. According
to the just proved theorem that subspace related to smaller singular values is
less perturbed by noise, the null vectors or the right singular vectors
associated with the minor singular values are calculated for each submatrix.
The vectors are the null vectors of the corresponding submatrix in the ground
truth of the given large-scale matrix. If enough sub-matrices are randomly
chosen, the low-rank decomposition is estimated. The experimental results on
random synthetical matrices with sizes such as 131072X1024 and on real data
sets indicate that RSM is much faster and memory-saving, and, meanwhile, has
considerable high precision achieving or approximating to the best."
"Tensor transpose is a higher order generalization of matrix transpose. In
this paper, we use permutations and symmetry group to define? the tensor
transpose. Then we discuss the classification and composition of tensor
transposes. Properties of tensor transpose are studied in relation to tensor
multiplication, tensor eigenvalues, tensor decompositions and tensor rank."
"Multilinear and tensor decompositions are a popular tool in linear and
multilinear algebra and have a wide range of important applications to modern
computing. Our paper of 1972 presented the first nontrivial application of such
decompositions to fundamental matrix computations and was also a landmark in
the history of the acceleration of matrix multiplication. Published in 1972 in
Russian, it has never been translated into English. It has been very rarely
cited in the Western literature on matrix multiplication and never in the works
on multilinear and tensor decompositions. This motivates us to present its
translation into English, together with our brief comments on its impact on the
two fields."
"If a tensor with various symmetries is properly unfolded, then the resulting
matrix inherits those symmetries. As tensor computations become increasingly
important it is imperative that we develop efficient structure preserving
methods for matrices with multiple symmetries. In this paper we consider how to
exploit and preserve structure in the pivoted Cholesky factorization when
approximating a matrix $A$ that is both symmetric ($A=A^T$) and what we call
{\em perfect shuffle symmetric}, or {\em perf-symmetric}. The latter property
means that $A = \Pi A\Pi$ where $\Pi$ is a permutation with the property that
$\Pi v = v$ if $v$ is the vec of a symmetric matrix and $\Pi v = -v$ if $v$ is
the vec of a skew-symmetric matrix. Matrices with this structure can arise when
an order-4 tensor $\cal A$ is unfolded and its elements satisfy ${\cal
A}(i_{1},i_{2},i_{3},i_{4}) = {\cal A}(i_{2},i_{1},i_{3},i_{4}) ={\cal
A}(i_{1},i_{2},i_{4},i_{3}) ={\cal A}(i_{3},i_{4},i_{1},i_{2}).$ This is the
case in certain quantum chemistry applications where the tensor entries are
electronic repulsion integrals. Our technique involves a closed-form block
diagonalization followed by one or two half-sized pivoted Cholesky
factorizations. This framework allows for a lazy evaluation feature that is
important if the entries in $\cal A$ are expensive to compute. In addition to
being a structure preserving rank reduction technique, we find that this
approach for obtaining the Cholesky factorization reduces the work by up to a
factor of 4."
"A two step mesh deformation approach for large nodal deformations, typically
arising from non-parametric shape optimization, fluid-structure interaction or
computer graphics, is considered. Two major difficulties, collapsed cells and
an undesirable parameterization, are overcome by considering a special form of
ray tracing paired with a centroid Voronoi reparameterization. The ray
direction is computed by solving an Eikonal equation. With respect to the
Hadamard form of the shape derivative, both steps are within the kernel of the
objective and have no negative impact on the minimizer. The paper concludes
with applications in 2D and 3D fluid dynamics and automatic code generation and
manages to solve these problems without any remeshing. The methodology is
available as a FEniCS shape optimization add-on at
http://www.mathematik.uni-wuerzburg.de/~schmidt/femorph."
"Tensor decompositions are promising tools for big data analytics as they
bring multiple modes and aspects of data to a unified framework, which allows
us to discover complex internal structures and correlations of data.
Unfortunately most existing approaches are not designed to meet the major
challenges posed by big data analytics. This paper attempts to improve the
scalability of tensor decompositions and provides two contributions: A flexible
and fast algorithm for the CP decomposition (FFCP) of tensors based on their
Tucker compression; A distributed randomized Tucker decomposition approach for
arbitrarily big tensors but with relatively low multilinear rank. These two
algorithms can deal with huge tensors, even if they are dense. Extensive
simulations provide empirical evidence of the validity and efficiency of the
proposed algorithms."
"We consider the recovery of a low rank and jointly sparse matrix from under
sampled measurements of its columns. This problem is highly relevant in the
recovery of dynamic MRI data with high spatio-temporal resolution, where each
column of the matrix corresponds to a frame in the image time series; the
matrix is highly low-rank since the frames are highly correlated. Similarly the
non-zero locations of the matrix in appropriate transform/frame domains (e.g.
wavelet, gradient) are roughly the same in different frame. The superset of the
support can be safely assumed to be jointly sparse. Unlike the classical
multiple measurement vector (MMV) setup that measures all the snapshots using
the same matrix, we consider each snapshot to be measured using a different
measurement matrix. We show that this approach reduces the total number of
measurements, especially when the rank of the matrix is much smaller than than
its sparsity. Our experiments in the context of dynamic imaging shows that this
approach is very useful in realizing free breathing cardiac MRI."
"Beginning with the projectively invariant method for linear programming,
interior point methods have led to powerful algorithms for many difficult
computing problems, in combinatorial optimization, logic, number theory and
non-convex optimization. Algorithms for convex optimization benefitted from
many pre-established ideas from classical mathematics, but non-convex problems
require new concepts. Lecture series I am presenting at the conference on
Foundations of Computational Mathematics, 2014, outlines some of these
concepts{computational models based on the concept of the continuum, algorithms
invariant w.r.t. projective, bi-rational, and bi-holomorphic transformations on
co-ordinate representation, extended proof systems for more efficient
certificates of optimality, extensions of Grassmanns extension theory,
efficient evaluation methods for the effect of exponential number of
constraints, theory of connected sets based on graded connectivity, theory of
curved spaces adapted to the problem data, and concept of relatively algebraic
sets in curved space. Since this conference does not have a proceedings, the
purpose of this article is to provide the material being presented at the
conference in more widely accessible form."
"An unsteady problem is considered for a space-fractional diffusion equation
in a bounded domain. A first-order evolutionary equation containing a
fractional power of an elliptic operator of second order is studied for general
boundary conditions of Robin type. Finite element approximation in space is
employed. To construct approximation in time, regularized two-level schemes are
used. The numerical implementation is based on solving the equation with the
fractional power of the elliptic operator using an auxiliary Cauchy problem for
a pseudo-parabolic equation. The results of numerical experiments are presented
for a model two-dimensional problem."
"This work concerns the Perona-Malik equation, which plays essential role in
image processing. The first part gives a survey of results on existance,
uniqueness and stability of solutions, the second part introduces
discretisations of equation and deals with an analysis of discrete problem. In
the last part I present some numerical results, in particular with algorithms
applied to real images."
"We propose a novel stochastic gradient method---semi-stochastic coordinate
descent (S2CD)---for the problem of minimizing a strongly convex function
represented as the average of a large number of smooth convex functions:
$f(x)=\tfrac{1}{n}\sum_i f_i(x)$. Our method first performs a deterministic
step (computation of the gradient of $f$ at the starting point), followed by a
large number of stochastic steps. The process is repeated a few times, with the
last stochastic iterate becoming the new starting point where the deterministic
step is taken. The novelty of our method is in how the stochastic steps are
performed. In each such step, we pick a random function $f_i$ and a random
coordinate $j$---both using nonuniform distributions---and update a single
coordinate of the decision vector only, based on the computation of the
$j^{th}$ partial derivative of $f_i$ at two different points. Each random step
of the method constitutes an unbiased estimate of the gradient of $f$ and
moreover, the squared norm of the steps goes to zero in expectation, meaning
that the stochastic estimate of the gradient progressively improves. The
complexity of the method is the sum of two terms: $O(n\log(1/\epsilon))$
evaluations of gradients $\nabla f_i$ and $O(\hat{\kappa}\log(1/\epsilon))$
evaluations of partial derivatives $\nabla_j f_i$, where $\hat{\kappa}$ is a
novel condition number."
"Dealing with hardware and software faults is an important problem as parallel
and distributed systems scale to millions of processing cores and wide area
networks. Traditional methods for dealing with faults include
checkpoint-restart, active replicas, and deterministic replay. Each of these
techniques has associated resource overheads and constraints. In this paper, we
propose an alternate approach to dealing with faults, based on input
augmentation. This approach, which is an algorithmic analog of erasure coded
storage, applies a minimally modified algorithm on the augmented input to
produce an augmented output. The execution of such an algorithm proceeds
completely oblivious to faults in the system. In the event of one or more
faults, the real solution is recovered using a rapid reconstruction method from
the augmented output. We demonstrate this approach on the problem of solving
sparse linear systems using a conjugate gradient solver. We present input
augmentation and output recovery techniques. Through detailed experiments, we
show that our approach can be made oblivious to a large number of faults with
low computational overhead. Specifically, we demonstrate cases where a single
fault can be corrected with less than 10% overhead in time, and even in extreme
cases (fault rates of 20%), our approach is able to compute a solution with
reasonable overhead. These results represent a significant improvement over the
state of the art."
"The class of non-commutative hypercomplex number systems (HNS) of
4-dimension, constructed by using of non-commutative Grassmann-Clifford
procedure of doubling of 2-dimensional systems is investigated in the article
and established here are their relationships with the generalized quaternions.
Algorithms of performance of operations and methods of algebraic
characteristics calculation in them, such as conjugation, normalization, a type
of zero divisors are investigated. The considered arithmetic and algebraic
operations and procedures in this class HNS allow to use these HNS in
mathematical modeling."
"Tensor factorizations are computationally hard problems, and in particular,
are often significantly harder than their matrix counterparts. In case of
Boolean tensor factorizations -- where the input tensor and all the factors are
required to be binary and we use Boolean algebra -- much of that hardness comes
from the possibility of overlapping components. Yet, in many applications we
are perfectly happy to partition at least one of the modes. In this paper we
investigate what consequences does this partitioning have on the computational
complexity of the Boolean tensor factorizations and present a new algorithm for
the resulting clustering problem. This algorithm can alternatively be seen as a
particularly regularized clustering algorithm that can handle extremely
high-dimensional observations. We analyse our algorithms with the goal of
maximizing the similarity and argue that this is more meaningful than
minimizing the dissimilarity. As a by-product we obtain a PTAS and an efficient
0.828-approximation algorithm for rank-1 binary factorizations. Our algorithm
for Boolean tensor clustering achieves high scalability, high similarity, and
good generalization to unseen data with both synthetic and real-world data
sets."
"This paper presents a new approach to verify accuracy of computational
simulations. We develop mathematical theorems which can serve as robust a
posteriori error estimation techniques to identify numerical pollution, check
the performance of adaptive meshes, and verify numerical solutions. We
demonstrate performance of this methodology on problems from flow thorough
porous media. However, one can extend it to other models. We construct
mathematical properties such that the solutions to Darcy and Darcy-Brinkman
equations satisfy them. The mathematical properties include the total minimum
mechanical power, minimum dissipation theorem, reciprocal relation, and maximum
principle for the vorticity. All the developed theorems have firm mechanical
bases and are independent of numerical methods. So, these can be utilized for
solution verification of finite element, finite volume, finite difference,
lattice Boltzmann methods and so forth. In particular, we show that, for a
given set of boundary conditions, Darcy velocity has the minimum total
mechanical power of all the kinematically admissible vector fields. We also
show that a similar result holds for Darcy-Brinkman velocity. We then show for
a conservative body force, the Darcy and Darcy-Brinkman velocities have the
minimum total dissipation among their respective kinematically admissible
vector fields. Using numerical examples, we show that the minimum dissipation
and total mechanical power theorems can be utilized to identify pollution
errors in numerical solutions. The solutions to Darcy and Darcy-Brinkman
equations are shown to satisfy a reciprocal relation, which has the potential
to identify errors in the numerical implementation of boundary conditions."
"In this work a novel method for the analysis with trimmed CAD surfaces is
presented. The method involves an additional mapping step and the attraction
stems from its sim- plicity and ease of implementation into existing Finite
Element (FEM) or Boundary Element (BEM) software. The method is first verified
with classical test examples in structural mechanics. Then two practical
applications are presented one using the FEM, the other the BEM, that show the
applicability of the method."
"This paper offers a new regard on compactly supported wavelets derived from
FIR filters. Although being continuous wavelets, analytical formulation are
lacking for such wavelets. Close approximations for daublets (Daubechies
wavelets) and their spectra are introduced here. The frequency detection
properties of daublets are investigated through scalograms derived from these
new analytical expressions. These near-daublets have been implemented on the
Matlab wavelet toolbox and a few scalograms presented. This approach can be
valuable for wavelet synthesis from hardware or for application involving
continuous wavelet-based systems, such as wavelet OFDM."
"In this paper we study the impact of two types of preconditioning on the
numerical solution of large sparse augmented linear systems. The first
preconditioning matrix is the lower triangular part whereas the second is the
product of the lower triangular part with the upper triangular part of the
augmented system's coefficient matrix. For the first preconditioning matrix we
form the Generalized Modified Extrapolated Successive Overrelaxation (GMESOR)
method, whereas the second preconditioning matrix yields the Generalized
Modified Preconditioned Simultaneous Displacement (GMPSD) method, which is an
extrapolated form of the Symmetric Successive Overrelaxation method. We find
sufficient conditions for each aforementioned iterative method to converge. In
addition, we develop a geometric approach, for determining the optimum values
of their parameters and corresponding spectral radii. It is shown that both
iterative methods studied (GMESOR and GMPSD) attain the same rate of
convergence. Numerical results confirm our theoretical expectations."
"Here we present an implementation of Primal-Dual Affine scaling method to
solve linear optimization problem on GPU based systems. Strategies to convert
the system generated by complementary slackness theorem into a symmetric system
are given. A new CUDA friendly technique to solve the resulting symmetric
positive definite subsystem is also developed. Various strategies to reduce the
memory transfer and storage requirements were also explored."
"This paper describes a flexible architecture for implementing a new fast
computation of the discrete Fourier and Hartley transforms, which is based on a
matrix Laurent series. The device calculates the transforms based on a single
bit selection operator. The hardware structure and synthesis are presented,
which handled a 16-point fast transform in 65 nsec, with a Xilinx SPARTAN 3E
device."
"This paper concerns with mesh restrictions that are needed to satisfy several
important mathematical properties -- maximum principles, comparison principles,
and the non-negative constraint -- for a general linear second-order elliptic
partial differential equation. We critically review some recent developments in
the field of discrete maximum principles, derive new results, and discuss some
possible future research directions in this area. In particular, we derive
restrictions for a three-node triangular (T3) element and a four-node
quadrilateral (Q4) element to satisfy comparison principles, maximum
principles, and the non-negative constraint under the standard single-field
Galerkin formulation. Analysis is restricted to uniformly elliptic linear
differential operators in divergence form with Dirichlet boundary conditions
specified on the entire boundary of the domain. Various versions of maximum
principles and comparison principles are discussed in both continuous and
discrete settings. In the literature, it is well-known that an acute-angled
triangle is sufficient to satisfy the discrete weak maximum principle for pure
isotropic diffusion. An iterative algorithm is developed to construct
simplicial meshes that preserves discrete maximum principles using existing
open source mesh generators. Various numerical examples based on different
types of triangulations are presented to show the pros and cons of placing
restrictions on a computational mesh. We also quantify local and global mass
conservation errors using representative numerical examples, and illustrate the
performance of metric-based meshes with respect to mass conservation."
"Hyper spectral imaging is a remote sensing technology, providing variety of
applications such as material identification, space object identification,
planetary exploitation etc. It deals with capturing continuum of images of the
earth surface from different angles. Due to the multidimensional nature of the
image, multi-way arrays are one of the possible solutions for analyzing hyper
spectral data. This multi-way array is called tensor. Our approach deals with
implementing three decomposition models LMLRA, BTD and CPD to the sample data
for choosing the best decomposition of the data set. The results have proved
that Block Term Decomposition (BTD) is the best tensor model for decomposing
the hyper spectral image in to resultant factor matrices."
"We introduce a fast mesh-based method for computing N-body interactions that
is both scalable and accurate. The method is founded on a
particle-particle--particle-mesh P3M approach, which decomposes a potential
into rapidly decaying short-range interactions and smooth, mesh-resolvable
long-range interactions. However, in contrast to the traditional approach of
using Gaussian screen functions to accomplish this decomposition, our method
employs specially designed polynomial bases to construct the screened
potentials. Because of this form of the screen, the long-range component of the
potential is then solved exactly with a finite element method, leading
ultimately to a sparse matrix problem that is solved efficiently with standard
multigrid methods. Moreover, since this system represents an exact
discretization, the optimal resolution properties of the FFT are unnecessary,
though the short-range calculation is now more involved than P3M/PME methods.
We introduce the method, analyze its key properties, and demonstrate the
accuracy of the algorithm."
"For a given matrix subspace, how can we find a basis that consists of
low-rank matrices? This is a generalization of the sparse vector problem. It
turns out that when the subspace is spanned by rank-1 matrices, the matrices
can be obtained by the tensor CP decomposition. For the higher rank case, the
situation is not as straightforward. In this work we present an algorithm based
on a greedy process applicable to higher rank problems. Our algorithm first
estimates the minimum rank by applying soft singular value thresholding to a
nuclear norm relaxation, and then computes a matrix with that rank using the
method of alternating projections. We provide local convergence results, and
compare our algorithm with several alternative approaches. Applications include
data compression beyond the classical truncated SVD, computing accurate
eigenvectors of a near-multiple eigenvalue, image separation and graph
Laplacian eigenproblems."
"We present an algebraic method for constructing a highly effective coarse
grid correction to accelerate domain decomposition. The coarse problem is
constructed from the original matrix and a small set of input vectors that span
a low-degree polynomial space, but no further knowledge of meshes or continuous
functionals is used. We construct a coarse basis by partitioning the problem
into subdomains and using the restriction of each input vector to each
subdomain as its own basis function. This basis resembles a Discontinuous
Galerkin basis on subdomain-sized elements. Constructing the coarse problem by
Galerkin projection, we prove a high-order convergent error bound for the
coarse solutions. Used in a two-level symmetric multiplicative overlapping
Schwarz preconditioner, the resulting conjugate gradient solver shows optimal
scaling. Convergence requires a constant number of iterations, independent of
fine problem size, on a range of scalar and vector-valued second-order and
fourth-order PDEs."
"In this paper, we review the parallel and distributed optimization algorithms
based on the alternating direction method of multipliers (ADMM) for solving
""big data"" optimization problems in modern communication networks. We first
introduce the canonical formulation of the large-scale optimization problem.
Next, we describe the general form of ADMM and then focus on several direct
extensions and sophisticated modifications of ADMM from $2$-block to $N$-block
settings to deal with the optimization problem. The iterative schemes and
convergence properties of each extension/modification are given, and the
implementation on large-scale computing facilities is also illustrated.
Finally, we numerate several applications in communication networks, such as
the security constrained optimal power flow problem in smart grid networks and
mobile data offloading problem in software defined networks (SDNs)."
"Least-squares Petrov--Galerkin (LSPG) model-reduction techniques such as the
Gauss--Newton with Approximated Tensors (GNAT) method have shown promise, as
they have generated stable, accurate solutions for large-scale turbulent,
compressible flow problems where standard Galerkin techniques have failed.
However, there has been limited comparative analysis of the two approaches.
This is due in part to difficulties arising from the fact that Galerkin
techniques perform optimal projection associated with residual minimization at
the time-continuous level, while LSPG techniques do so at the time-discrete
level. This work provides a detailed theoretical and computational comparison
of the two techniques for two common classes of time integrators: linear
multistep schemes and Runge--Kutta schemes. We present a number of new
findings, including conditions under which the LSPG ROM has a time-continuous
representation, conditions under which the two techniques are equivalent, and
time-discrete error bounds for the two approaches. Perhaps most surprisingly,
we demonstrate both theoretically and computationally that decreasing the time
step does not necessarily decrease the error for the LSPG ROM; instead, the
time step should be `matched' to the spectral content of the reduced basis. In
numerical experiments carried out on a turbulent compressible-flow problem with
over one million unknowns, we show that increasing the time step to an
intermediate value decreases both the error and the simulation time of the LSPG
reduced-order model by an order of magnitude."
"Schemes with the second-order approximation in time are considered for
numerical solving the Cauchy problem for an evolutionary equation of first
order with a self-adjoint operator. The implicit two-level scheme based on the
Pad\'{e} polynomial approximation is unconditionally stable. It demonstrates
good asymptotic properties in time and provides an adequate evolution in time
for individual harmonics of the solution (has spectral mimetic stability). In
fact, the only drawback of this scheme is the necessity to solve an equation
with an operator polynomial of second degree at each time level. We consider
modifications of these schemes, which are based on solving equations with
operator polynomials of first degree. Such computational implementations occur,
for example, if we apply the fully implicit two-level scheme (the backward
Euler scheme). A three-level modification of the SM-stable scheme is proposed.
Its unconditional stability is established in the corresponding norms. The
emphasis is on the scheme, where the numerical algorithm involves two stages,
namely, the backward Euler scheme of first order at the first (prediction)
stage and the following correction of the approximate solution using a
factorized operator. The SM-stability is established for the proposed scheme.
To illustrate the theoretical results of the work, a model problem is solved
numerically."
"Mean field type models describing the limiting behavior of stochastic
differential games as the number of players tends to +$\infty$, have been
recently introduced by J-M. Lasry and P-L. Lions. Under suitable assumptions,
they lead to a system of two coupled partial differential equations, a forward
Bellman equation and a backward Fokker-Planck equations. Finite difference
schemes for the approximation of such systems have been proposed in previous
works. Here, we prove the convergence of these schemes towards a weak solution
of the system of partial differential equations."
"In this paper, we study the global convergence of majorization minimization
(MM) algorithms for solving nonconvex regularized optimization problems. MM
algorithms have received great attention in machine learning. However, when
applied to nonconvex optimization problems, the convergence of MM algorithms is
a challenging issue. We introduce theory of the Kurdyka- Lojasiewicz inequality
to address this issue. In particular, we show that many nonconvex problems
enjoy the Kurdyka- Lojasiewicz property and establish the global convergence
result of the corresponding MM procedure. We also extend our result to a well
known method that called CCCP (concave-convex procedure)."
"The models that are based of fractional derivatives should be highlighted
among promising new models to describe turbulent fluid flows. In the present
work, a steady-state flow in a duct is considered under the condition that the
turbulent diffusion is governed by a fractional power of the Laplace operator.
To study numerically flows in rectangular channels, finite-difference
approximations are employed. For approximate solving the corresponding boundary
value problem, the iterative method of conjugate gradients is used. At each
iteration, the problem with a fractional power of the grid Laplace operator is
solved. Predictions of turbulent flows in ducts at different Reynolds numbers
are presented via mean velocity fields."
"An equation containing a fractional power of an elliptic operator of second
order is studied for Dirichlet boundary conditions. Finite difference
approximations in space are employed. The proposed numerical algorithm is based
on solving an auxiliary Cauchy problem for a pseudo-parabolic equation.
Unconditionally stable vector additive schemes (splitting schemes) are
constructed. Numerical results for a model problem in a rectangle calculated
using the splitting with respect to spatial variables are presented."
"This paper introduces a robust preconditioner for general sparse symmetric
matrices, that is based on low-rank approximations of the Schur complement in a
Domain Decomposition (DD) framework. In this ""Schur Low Rank"" (SLR)
preconditioning approach, the coefficient matrix is first decoupled by DD, and
then a low-rank correction is exploited to compute an approximate inverse of
the Schur complement associated with the interface points. The method avoids
explicit formation of the Schur complement matrix. We show the feasibility of
this strategy for a model problem, and conduct a detailed spectral analysis for
the relationship between the low-rank correction and the quality of the
preconditioning. Numerical experiments on general matrices illustrate the
robustness and efficiency of the proposed approach."
"This paper presents a parallel preconditioning method for distributed sparse
linear systems, based on an approximate inverse of the original matrix, that
adopts a general framework of distributed sparse matrices and exploits the
domain decomposition method and low-rank corrections. The domain decomposition
approach decouples the matrix and once inverted, a low-rank approximation is
applied by exploiting the Sherman-Morrison-Woodbury formula, which yields two
variants of the preconditioning methods. The low-rank expansion is computed by
the Lanczos procedure with reorthogonalizations. Numerical experiments indicate
that, when combined with Krylov subspace accelerators, this preconditioner can
be efficient and robust for solving symmetric sparse linear systems.
Comparisons with other distributed-memory preconditioning methods are
presented."
"A parallel-in-time algorithm based on an augmented Lagrangian approach is
proposed to solve four-dimensional variational (4D-Var) data assimilation
problems. The assimilation window is divided into multiple sub-intervals that
allows to parallelize cost function and gradient computations. Solution
continuity equations across interval boundaries are added as constraints. The
augmented Lagrangian approach leads to a different formulation of the
variational data assimilation problem than weakly constrained 4D-Var. A
combination of serial and parallel 4D-Vars to increase performance is also
explored. The methodology is illustrated on data assimilation problems with
Lorenz-96 and the shallow water models."
"This paper constructs an ensemble-based sampling smoother for
four-dimensional data assimilation using a Hybrid/Hamiltonian Monte-Carlo
approach. The smoother samples efficiently from the posterior probability
density of the solution at the initial time. Unlike the well-known ensemble
Kalman smoother, which is optimal only in the linear Gaussian case, the
proposed methodology naturally accommodates non-Gaussian errors and non-linear
model dynamics and observation operators. Unlike the four-dimensional
variational met\-hod, which only finds a mode of the posterior distribution,
the smoother provides an estimate of the posterior uncertainty. One can use the
ensemble mean as the minimum variance estimate of the state, or can use the
ensemble in conjunction with the variational approach to estimate the
background errors for subsequent assimilation windows. Numerical results
demonstrate the advantages of the proposed method compared to the traditional
variational and ensemble-based smoothing methods."
"Nonconvex optimization problems such as the ones in training deep neural
networks suffer from a phenomenon called saddle point proliferation. This means
that there are a vast number of high error saddle points present in the loss
function. Second order methods have been tremendously successful and widely
adopted in the convex optimization community, while their usefulness in deep
learning remains limited. This is due to two problems: computational complexity
and the methods being driven towards the high error saddle points. We introduce
a novel algorithm specially designed to solve these two issues, providing a
crucial first step to take the widely known advantages of Newton's method to
the nonconvex optimization community, especially in high dimensional settings."
"We propose a novel method for speeding up stochastic optimization algorithms
via sketching methods, which recently became a powerful tool for accelerating
algorithms for numerical linear algebra. We revisit the method of conditioning
for accelerating first-order methods and suggest the use of sketching methods
for constructing a cheap conditioner that attains a significant speedup with
respect to the Stochastic Gradient Descent (SGD) algorithm. While our
theoretical guarantees assume convexity, we discuss the applicability of our
method to deep neural networks, and experimentally demonstrate its merits."
"The Fast Marching Method is a very popular algorithm to compute
times-of-arrival maps (distances map measured in time units). Since their
proposal in 1995, it has been applied to many different applications such as
robotics, medical computer vision, fluid simulation, etc. Many alternatives
have been proposed with two main objectives: to reduce its computational time
and to improve its accuracy. In this paper, we collect the main approaches
which improve the computational time of the standard Fast Marching Method,
focusing on single-threaded methods and isotropic environments. 9 different
methods are studied under a common mathematical framework and experimentally in
representative environments: Fast Marching Method with binary heap, Fast
Marching Method with Fibonacci Heap, Simplified Fast Marching Method, Untidy
Fast Marching Method, Fast Iterative Method, Group Marching Method, Fast
Sweeping Method, Lock Sweeping Method and Double Dynamic Queue Method."
"In this paper the isogeometric Nystr\""om method is presented. It's
outstanding features are: it allows the analysis of domains described by many
different geometrical mapping methods in computer aided geometric design and it
requires only pointwise function evaluations just like isogeometric collocation
methods. The analysis of the computational domain is carried out by means of
boundary integral equations, therefor only the boundary representation is
required. The method is thoroughly integrated into the isogeometric framework.
For example, the regularization of the arising singular integrals performed
with local correction as well as the interpolation of the pointwise existing
results are carried out by means of Bezier elements.
  The presented isogeometric Nystr\""om method is applied to practical problems
solved by the Laplace and the Lame-Navier equation. Numerical tests show higher
order convergence in two and three dimensions. It is concluded that the
presented approach provides a simple and flexible alternative to currently used
methods for solving boundary integral equations, but has some limitations."
"In many mathematical models of physical phenomenons and engineering fields,
such as electrical circuits or mechanical multibody systems, which generate the
differential algebraic equations (DAEs) systems naturally. In general, the
feature of DAEs is a sparse large scale system of fully nonlinear and high
index. To make use of its sparsity, this paper provides a simple and efficient
algorithm for computing the large scale DAEs system. We exploit the shortest
augmenting path algorithm for finding maximum value transversal (MVT) as well
as block triangular forms (BTF). We also present the extended signature matrix
method with the block fixed point iteration and its complexity results.
Furthermore, a range of nontrivial problems are demonstrated by our algorithm."
"In this work we discuss the problem of selecting suitable approximators from
families of parameterized elementary functions that are known to be dense in a
Hilbert space of functions. We consider and analyze published procedures, both
randomized and deterministic, for selecting elements from these families that
have been shown to ensure the rate of convergence in $L_2$ norm of order
$O(1/N)$, where $N$ is the number of elements. We show that both randomized and
deterministic procedures are successful if additional information about the
families of functions to be approximated is provided. In the absence of such
additional information one may observe exponential growth of the number of
terms needed to approximate the function and/or extreme sensitivity of the
outcome of the approximation to parameters. Implications of our analysis for
applications of neural networks in modeling and control are illustrated with
examples."
"CANDECOMP/PARAFAC (CPD) approximates multiway data by sum of rank-1 tensors.
Our recent study has presented a method to rank-1 tensor deflation, i.e.
sequential extraction of the rank-1 components. In this paper, we extend the
method to block deflation problem. When at least two factor matrices have full
column rank, one can extract two rank-1 tensors simultaneously, and rank of the
data tensor is reduced by 2. For decomposition of order-3 tensors of size R x R
x R and rank-R, the block deflation has a complexity of O(R^3) per iteration
which is lower than the cost O(R^4) of the ALS algorithm for the overall CPD."
"We consider Newton methods for common types of single commodity and
multi-commodity network flow problems. Despite the potentially very large
dimension of the problem, they can be implemented using the conjugate gradient
method and low-dimensional network operations, as shown nearly thirty years
ago. We revisit these methods, compare them to more recent proposals, and
describe how they can be implemented in a distributed computing system. We also
discuss generalizations, including the treatment of arc gains, linear side
constraints, and related special structures."
"We consider a quadrilateral 'mini' finite element for approximating the
solution of Stokes equations using a quadrilateral mesh. We use the standard
bilinear finite element space enriched with element-wise defined bubble
functions for the velocity and the standard bilinear finite element space for
the pressure space. With a simple modification of the standard bubble function
we show that a single bubble function is sufficient to ensure the inf-sup
condition. We have thus improved an earlier result on the quadrilateral 'mini'
element, where more than one bubble function are used to get the stability."
"This article shows that the unconditional stability of the Dual-Finite Volume
Method, which is at least valid for linear problems, is not true for generic
nonlinear differential equations including the PMEs unless the coefficient
appearing in the numerical fluxes are appropriately evaluated. This article
provides a theoretically truly isotone numerical fluxes specialized for solving
the PMEs presented, which is still as simple as the conventional fully-upwind
counterpart."
"Direct factorization methods for the solution of large, sparse linear systems
that arise from PDE discretizations are robust, but typically show poor time
and memory scalability for large systems. In this paper, we describe an
efficient sparse, rank-structured Cholesky algorithm for solution of the
positive definite linear system $A x = b$ when $A$ comes from a discretized
partial-differential equation. Our approach combines the efficient memory
access patterns of conventional supernodal Cholesky algorithms with the memory
efficiency of rank-structured direct solvers. For several test problems arising
from PDE discretizations, our method takes less memory than standard sparse
Cholesky solvers and less wall-clock time than standard preconditioned
iterations."
"We develop the Sparse Approximate Matrix Multiply ($\tt SpAMM$) $n$-body
solver for first order Newton Schulz iteration of the matrix square root and
inverse square root. The solver performs recursive two-sided metric queries on
a modified Cauchy-Schwarz criterion, culling negligible sub-volumes of the
product-tensor for problems with structured decay in the sub-space metric.
These sub-structures are shown to bound the relative error in the matrix-matrix
product, and in favorable cases, to enjoy a reduced computational complexity
governed by dimensionality reduction of the product volume. A main contribution
is demonstration of a new, algebraic locality that develops under contractive
identity iteration, with collapse of the metric-subspace onto the identity's
plane diagonal, resulting in a stronger $\tt SpAMM$ bound. Also, we carry out a
first order {Fr\'{e}chet} analyses for single and dual channel instances of the
square root iteration, and look at bifurcations due to ill-conditioning and a
too aggressive $\tt SpAMM$ approximation. Then, we show that extreme $\tt
SpAMM$ approximation and contractive identity iteration can be achieved for
ill-conditioned systems through regularization, and we demonstrate the
potential for acceleration with a scoping, product representation of the
inverse factor."
"Some system identification problems impose nonnegativity constraints on the
parameters to estimate due to inherent physical characteristics of the unknown
system. The nonnegative least-mean-square (NNLMS) algorithm and its variants
allow to address this problem in an online manner. A nonnegative least mean
fourth (NNLMF) algorithm has been recently proposed to improve the performance
of these algorithms in cases where the measurement noise is not Gaussian. This
paper provides a first theoretical analysis of the stochastic behavior of the
NNLMF algorithm for stationary Gaussian inputs and slow learning. Simulation
results illustrate the accuracy of the proposed analysis."
"As the number of processor cores on supercomputers becomes larger and larger,
algorithms with high degree of parallelism attract more attention. In this
work, we propose a novel space-time coupled algorithm for solving an inverse
problem associated with the time-dependent convection-diffusion equation in
three dimensions. We introduce a mixed finite element/finite difference method
and a one-level and a two-level space-time parallel domain decomposition
preconditioner for the Karush-Kuhn-Tucker (KKT) system induced from
reformulating the inverse problem as an output least-squares optimization
problem in the space-time domain. The new full space approach eliminates the
sequential steps of the optimization outer loop and the inner forward and
backward time marching processes, thus achieves high degree of parallelism.
Numerical experiments validate that this approach is effective and robust for
recovering unsteady moving sources. We report strong scalability results
obtained on a supercomputer with more than 1,000 processors."
"The power method and block Lanczos method are popular numerical algorithms
for computing the truncated singular value decomposition (SVD) and eigenvalue
decomposition problems. Especially in the literature of randomized numerical
linear algebra, the power method is widely applied to improve the quality of
randomized sketching, and relative-error bounds have been well established.
Recently, Musco & Musco (2015) proposed a block Krylov subspace method that
fully exploits the intermediate results of the power iteration to accelerate
convergence. They showed spectral gap-independent bounds which are stronger
than the power method by order-of-magnitude. This paper offers novel error
analysis techniques and significantly improves the bounds of both the
randomized power method and the block Lanczos method. This paper also
establishes the first gap-independent bound for the warm-start block Lanczos
method."
"The Lane-Emden equation has been used to model several phenomenas in
theoretical physics, mathematical physics and astrophysics such as the theory
of stellar structure. This study is an attempt to utilize the collocation
method with the Rational Chebyshev of Second Kind function (RSC) to solve the
Lane-Emden equation over the semi-infinit interval [0; +infinity). According to
well-known results and comparing with previous methods, it can be said that
this method is efficient and applicable."
"Fourier series of smooth, non-periodic functions on $[-1,1]$ are known to
exhibit the Gibbs phenomenon, and exhibit overall slow convergence. One way of
overcoming these problems is by using a Fourier series on a larger domain, say
$[-T,T]$ with $T>1$, a technique called Fourier extension or Fourier
continuation. When constructed as the discrete least squares minimizer in
equidistant points, the Fourier extension has been shown shown to converge
geometrically in the truncation parameter $N$. A fast ${\mathcal O}(N \log^2
N)$ algorithm has been described to compute Fourier extensions for the case
where $T=2$, compared to ${\mathcal O}(N^3)$ for solving the dense discrete
least squares problem. We present two ${\mathcal O}(N\log^2 N )$ algorithms for
the computation of these approximations for the case of general $T$, made
possible by exploiting the connection between Fourier extensions and Prolate
Spheroidal Wave theory. The first algorithm is based on the explicit
computation of so-called periodic discrete prolate spheroidal sequences, while
the second algorithm is purely algebraic and only implicitly based on the
theory."
"In this article, we consider a simple representation for real numbers and
propose top-down procedures to approximate various algebraic and transcendental
operations with arbitrary precision. Detailed algorithms and proofs are
provided to guarantee the correctness of the approximations. Moreover, we
develop and apply a perturbation analysis method to show that our approximation
procedures only recompute expressions when unavoidable.
  In the last decade, various theories have been developed and implemented to
realize real computations with arbitrary precision. Proof of correctness for
existing approaches typically consider basic algebraic operations, whereas
detailed arguments about transcendental operations are not available. Another
important observation is that in each approach some expressions might require
iterative computations to guarantee the desired precision. However, no formal
reasoning is provided to prove that such iterative calculations are essential
in the approximation procedures. In our approximations of real functions, we
explicitly relate the precision of the inputs to the guaranteed precision of
the output, provide full proofs and a precise analysis of the necessity of
iterations."
"We make an in-depth study of the known border rank (i.e. approximate)
algorithms for the matrix multiplication tensor encoding the multiplication of
an n x 2 matrix by a 2 x 2 matrix."
"As parallel computing trends towards the exascale, scientific data produced
by high-fidelity simulations are growing increasingly massive. For instance, a
simulation on a three-dimensional spatial grid with 512 points per dimension
that tracks 64 variables per grid point for 128 time steps yields 8~TB of data,
assuming double precision. By viewing the data as a dense five-way tensor, we
can compute a Tucker decomposition to find inherent low-dimensional multilinear
structure, achieving compression ratios of up to 5000 on real-world data sets
with negligible loss in accuracy. So that we can operate on such massive data,
we present the first-ever distributed-memory parallel implementation for the
Tucker decomposition, whose key computations correspond to parallel linear
algebra operations, albeit with nonstandard data layouts. Our approach
specifies a data distribution for tensors that avoids any tensor data
redistribution, either locally or in parallel. We provide accompanying analysis
of the computation and communication costs of the algorithms. To demonstrate
the compression and accuracy of the method, we apply our approach to real-world
data sets from combustion science simulations. We also provide detailed
performance results, including parallel performance in both weak and strong
scaling experiments."
"This paper presents an interesting property of the matrices that may be
obtained with the use of direct Trefftz method. It is proved analytically for
2D Laplace problem that values of the elements of matrices describing the
capacitance of two scaled domains are inversely proportional to the scalability
factor. As an example of the application the capacitance extraction problem is
chosen. Concise description of the algorithm in which the scalability property
can be utilized is given. Furthermore some numerical results of the algorithm
are presented."
"We consider the problem of low-rank decomposition of incomplete multiway
tensors. Since many real-world data lie on an intrinsically low dimensional
subspace, tensor low-rank decomposition with missing entries has applications
in many data analysis problems such as recommender systems and image
inpainting. In this paper, we focus on Tucker decomposition which represents an
Nth-order tensor in terms of N factor matrices and a core tensor via
multilinear operations. To exploit the underlying multilinear low-rank
structure in high-dimensional datasets, we propose a group-based log-sum
penalty functional to place structural sparsity over the core tensor, which
leads to a compact representation with smallest core tensor. The method for
Tucker decomposition is developed by iteratively minimizing a surrogate
function that majorizes the original objective function, which results in an
iterative reweighted process. In addition, to reduce the computational
complexity, an over-relaxed monotone fast iterative shrinkage-thresholding
technique is adapted and embedded in the iterative reweighted process. The
proposed method is able to determine the model complexity (i.e. multilinear
rank) in an automatic way. Simulation results show that the proposed algorithm
offers competitive performance compared with other existing algorithms."
"The matrix low-rank approximation problem with additional convex constraints
can find many applications and has been extensively studied before. However,
this problem is shown to be nonconvex and NP-hard; most of the existing
solutions are heuristic and application-dependent. In this paper, we show that,
other than tons of application in current literature, this problem can be used
to recover a feasible solution for SDP relaxation. By some sophisticated
tricks, it can be equivalently posed in an appropriate form for the Alternating
Direction Method of Multipliers (ADMM) to solve. The two updates of ADMM
include the basic matrix low-rank approximation and projection onto a convex
set. Different from the general non-convex problems, the sub-problems in each
step of ADMM can be solved exactly and efficiently in spite of their
non-convexity. Moreover, the algorithm will converge exponentially under proper
conditions. The simulation results confirm its superiority over existing
solutions. We believe that the results in this paper provide a useful tool for
this important problem and will help to extend the application of ADMM to the
non-convex regime."
"A hybrid Schwarz/multigrid method for spectral element solvers to the Poisson
equation in $\mathbb R^2$ is presented. It extends the additive Schwarz method
studied by J. Lottes and P. Fischer (J. Sci. Comput. 24:45--78, 2005) by
introducing nonuniform weight distributions based on the smoothed sign
function. Using a V-cycle with only one pre-smoothing, the new method attains
logarithmic convergence rates in the range from 1.2 to 1.9, which corresponds
to residual reductions of almost two orders of magnitude. Compared to the
original method, it reduces the iteration count by a factor of 1.5 to 3,
leading to runtime savings of about 50 percent. In numerical experiments the
method proved robust with respect to the mesh size and polynomial orders up to
32. Used as a preconditioner for the (inexact) CG method it is also suited for
anisotropic meshes and easily extended to diffusion problems with variable
coefficients."
"In this article we propose a method of performing arithmetic operations on
varia-bles with unknown distribution. The approach to the evaluation results of
arithme-tic operations can select probability intervals of the algebraic
equations and their systems solutions, of differential equations and their
systems in case of histogram evaluation of the empirical density distributions
of random parameters."
"We introduce a new Adaptive Integration Approach (AIA) to be used in a wide
range of molecular simulations. Given a simulation problem and a step size, the
method automatically chooses the optimal scheme out of an available family of
numerical integrators. Although we focus on two-stage splitting integrators,
the idea may be used with more general families. In each instance, the
system-specific integrating scheme identified by our approach is optimal in the
sense that it provides the best conservation of energy for harmonic forces. The
AIA method has been implemented in the BCAM-modified GROMACS software package.
Numerical tests in molecular dynamics and hybrid Monte Carlo simulations of
constrained and unconstrained physical systems show that the method
successfully realises the fail-safe strategy. In all experiments, and for each
of the criteria employed, the AIA is at least as good as, and often
significantly outperforms the standard Verlet scheme, as well as fixed
parameter, optimized two-stage integrators. In particular, the sampling
efficiency found in simulations using the AIA is up to 5 times better than the
one achieved with other tested schemes."
"Approximating a definite integral of product of cosines to within an accuracy
of n binary digits where the integrand depends on input integers x[k] given in
binary radix, is equivalent to counting the number of equal-sum partitions of
the integers and is thus a #P problem. Similarly, integrating this function
from zero to infinity and deciding whether the result is either zero or
infinity is an NP-Complete problem. Efficient numerical integration methods
such as the double exponential formula and the sinc approximation have been
around since the mid 70's. Noting the hardness of approximating the integral we
argue that the proven rates of convergence of such methods cannot possibly be
correct since they give rise to an anomalous result as P=#P."
"Implicitly described domains are a well established tool in the simulation of
time dependent problems, e.g. using level-set methods. In order to solve
partial differential equations on such domains, a range of numerical methods
was developed, e.g. the Immersed Boundary method, Unfitted Finite Element or
Unfitted discontinuous Galerkin methods, eXtended or Generalised Finite Element
methods, just to name a few. Many of these methods involve integration over
cut-cells or their boundaries, as they are described by sub-domains of the
original level-set mesh. We present a new algorithm to geometrically evaluate
the integrals over domains described by a first-order, conforming level-set
function. The integration is based on a polyhedral reconstruction of the
implicit geometry, following the concepts of the Marching Cubes algorithm. The
algorithm preserves various topological properties of the implicit geometry in
its polyhedral reconstruction, making it suitable for Finite Element
computations. Numerical experiments show second order accuracy of the
integration. An implementation of the algorithm is available as free software,
which allows for an easy incorporation into other projects. The software is in
productive use within the DUNE framework."
"We study distributed low rank approximation in which the matrix to be
approximated is only implicitly represented across the different servers. For
example, each of $s$ servers may have an $n \times d$ matrix $A^t$, and we may
be interested in computing a low rank approximation to $A = f(\sum_{t=1}^s
A^t)$, where $f$ is a function which is applied entrywise to the matrix
$\sum_{t=1}^s A^t$. We show for a wide class of functions $f$ it is possible to
efficiently compute a $d \times d$ rank-$k$ projection matrix $P$ for which
$\|A - AP\|_F^2 \leq \|A - [A]_k\|_F^2 + \varepsilon \|A\|_F^2$, where $AP$
denotes the projection of $A$ onto the row span of $P$, and $[A]_k$ denotes the
best rank-$k$ approximation to $A$ given by the singular value decomposition.
The communication cost of our protocols is $d \cdot (sk/\varepsilon)^{O(1)}$,
and they succeed with high probability. Our framework allows us to efficiently
compute a low rank approximation to an entry-wise softmax, to a Gaussian kernel
expansion, and to $M$-Estimators applied entrywise (i.e., forms of robust low
rank approximation). We also show that our additive error approximation is best
possible, in the sense that any protocol achieving relative error for these
problems requires significantly more communication. Finally, we experimentally
validate our algorithms on real datasets."
"High-order methods gain more and more attention in computational fluid
dynamics. However, the potential advantage of these methods depends critically
on the availability of efficient elliptic solvers. With spectral-element
methods, static condensation is a common approach to reduce the number of
degree of freedoms and to improve the condition of the algebraic equations. The
resulting system is block-structured and the face-based operator well suited
for matrix-matrix multiplications. However, a straight-forward implementation
scales super-linearly with the number of unknowns and, therefore, prohibits the
application to high polynomial degrees. This paper proposes a novel
factorization technique, which yields a linear operation count of just 13N
multiplications, where N is the total number of unknowns. In comparison to
previous work it saves a factor larger than 3 and clearly outpaces unfactored
variants for all polynomial degrees. Using the new technique as a building
block for a preconditioned conjugate gradient method resulted in a runtime
scaling linearly with N for polynomial degrees $2 \leq p \leq 32$ . Moreover
the solver proved remarkably robust for aspect ratios up to 128."
"The Singular Value Decomposition (SVD) is a longstanding standard for data
approximation because it is optimal in the 2 and Frobenius norms. The SVD,
nevertheless, suffers from many setbacks, including computational cost, loss of
sparsity in the decomposition, and the inability to be updated easily when new
information arrives. Additionally, the SVD provides limited information on data
features and variables that best represent the data. In this work, we present a
truncated LU factorization called {\bf Spectrum-Revealing LU} (SRLU) for
effective low-rank matrix approximation, and develop the first algorithm to
compute an SRLU factorization, which is both efficient and reliable. Our
algorithm uses randomization and a novel LU updating technique with partial
pivoting, which is more stable than any other known LU updating algorithm. We
provide both approximation error bounds and singular value bounds for the SRLU
approximation computed by our algorithm. Our analysis suggests that SRLU is
competitive with the best low-rank matrix approximation methods, deterministic
or randomized, in both computational complexity and approximation quality.
Numeric experiments illustrate that SRLU preserves sparsity, highlights
important data features and variables, can be efficiently updated, and
calculates data approximations nearly as accurately as the SVD. To the best of
our knowledge this is the first practical variant of the LU decomposition for
efficient and effective low-rank matrix approximation."
"We propose a new instruction (FPADDRE) that computes the round-off error in
floating-point addition. We explain how this instruction benefits
high-precision arithmetic operations in applications where double precision is
not sufficient. Performance estimates on Intel Haswell, Intel Skylake, and AMD
Steamroller processors, as well as Intel Knights Corner co-processor,
demonstrate that such an instruction would improve the latency of double-double
addition by up to 55% and increase double-double addition throughput by up to
103%, with smaller, but non-negligible benefits for double-double
multiplication. The new instruction delivers up to 2x speedups on three
benchmarks that use high-precision floating-point arithmetic: double-double
matrix-matrix multiplication, compensated dot product, and polynomial
evaluation via the compensated Horner scheme."
"A randomized misfit approach is presented for the efficient solution of
large-scale PDE-constrained inverse problems with high-dimensional data. The
purpose of this paper is to offer a theory-based framework for random
projections in this inverse problem setting. The stochastic approximation to
the misfit is analyzed using random projection theory. By expanding beyond mean
estimator convergence, a practical characterization of randomized misfit
convergence can be achieved. The theoretical results developed hold with any
valid random projection in the literature. The class of feasible distributions
is broad yet simple to characterize compared to previous stochastic misfit
methods. This class includes very sparse random projections which provide
additional computational benefit. A different proof for a variant of the
Johnson-Lindenstrauss lemma is also provided. This leads to a different
intuition for the $O(\epsilon^{-2})$ factor in bounds for Johnson-Lindenstrauss
results. The main contribution of this paper is a theoretical result showing
the method guarantees a valid solution for small reduced misfit dimensions. The
interplay between Johnson-Lindenstrauss theory and Morozov's discrepancy
principle is shown to be essential to the result. The computational cost
savings for large-scale PDE-constrained problems with high- dimensional data is
discussed. Numerical verification of the developed theory is presented for
model problems of estimating a distributed parameter in an elliptic partial
differential equation. Results with different random projections are presented
to demonstrate the viability and accuracy of the proposed approach."
"Recently, a combined approach of CFIE--BAE has been proposed by authors for
solving external scattering problems in acoustics. CFIE stands for
combined-field integral equations, and BAE is the method of boundary
algebraical equation. The combined method is, essentially, a discrete analogue
of the boundary element method (BEM), having none of its disadvantages. Namely,
due to the discrete nature of BAE one should not compute quadratures of
oversingular integrals. Moreover, due to CFIE formulation, the method does not
possess spurious resonances.
  However, the CFIE--BAE method has an important drawback. Since the modelling
is performed in a regular discrete space, the shape of the obstacle should be
assembled of elementary ""bricks"", so smooth scatterers (like spheres,
cylinders, etc) are approximated with a poor accuracy. This loss of accuracy
becomes the bottleneck of the method. Here this disadvantage is overcome. The
CFIE--BAE method developed for regular meshing of the outer space is coupled in
a standard way with a relatively small irregular mesh enabling one to describe
the shape of the obstacle accurately enough."
"We introduce a new weakly-convex penalty function for signals with a group
behavior. The penalty promotes signals with a few number of active groups,
where within each group, only a few high magnitude coefficients are active. We
derive the threshold function associated with the proposed penalty and study
its properties. We discuss how the proposed penalty/threshold function can be
useful for signals with isolated non-zeros, such as audio with isolated
harmonics along the frequency axis, or reflection functions in exploration
seismology where the non-zeros occur on the boundaries of subsoil layers. We
demonstrate the use of the proposed penalty/threshold functions in a convex
denoising and a non-convex deconvolution formulation. We provide convergent
algorithms for both formulations and compare the performance with
state-of-the-art methods."
"We consider the algorithm for verified integration of piecewise analytic
functions given by Petras. The analysis of the algorithm contained in Patras'
paper is limited to a narrow class of functions and gives upper bounds only. We
present an estimation of the complexity (measured by a number of evaluations of
an integrand) of the algorithm, both upper and lower bounds, for a wider class
of functions. We show examples with complexity $\Theta(|\ln\eps|/\eps^{p-1})$,
for any $p >1$, where $\eps$ is the desired accuracy of the computed integral."
"In this note, we consider the line search for a class of abstract nonconvex
algorithm which have been deeply studied in the Kurdyka-Lojasiewicz theory. We
provide a weak convergence result of the line search in general. When the
objective function satisfies the Kurdyka-Lojasiewicz property and some certain
assumption, a global convergence result can be derived. An application is
presented for the L0-regularized least square minimization in the end of the
paper."
"We explore extended B-splines as a stable basis for isogeometric analysis
with trimmed parameter spaces. The stabilization is accomplished by an
appropriate substitution of B-splines that may lead to ill-conditioned system
matrices. The construction for non-uniform knot vectors is presented. The
properties of extended B-splines are examined in the context of interpolation,
potential, and linear elasticity problems and excellent results are attained.
The analysis is performed by an isogeometric boundary element formulation using
collocation. It is argued that extended B-splines provide a flexible and simple
stabilization scheme which ideally suits the isogeometric paradigm."
"A new higher-order accurate method is proposed that combines the advantages
of the classical $p$-version of the FEM on body-fitted meshes with embedded
domain methods. A background mesh composed by higher-order Lagrange elements is
used. Boundaries and interfaces are described implicitly by the level set
method and are within elements. In the elements cut by the boundaries or
interfaces, an automatic decomposition into higher-order accurate sub-elements
is realized. Therefore, the zero level sets are detected and meshed in a first
step which is called reconstruction. Then, based on the topological situation
in the cut element, higher-order sub-elements are mapped to the two sides of
the boundary or interface. The quality of the reconstruction and the mapping
largely determines the properties of the resulting, automatically generated
conforming mesh. It is found that optimal convergence rates are possible
although the resulting sub-elements are not always well-shaped."
"Many theoretical results in the machine learning domain stand only for
functions that are Lipschitz continuous. Lipschitz continuity is a strong form
of continuity that linearly bounds the variations of a function. In this paper,
we derive tight Lipschitz constants for two families of metrics: Mahalanobis
distances and bounded-space bilinear forms. To our knowledge, this is the first
time the Mahalanobis distance is formally proved to be Lipschitz continuous and
that such tight Lipschitz constants are derived."
"A boundary value problem for a fractional power $0 < \varepsilon < 1$ of the
second-order elliptic operator is considered. The boundary value problem is
singularly perturbed when $\varepsilon \rightarrow 0$. It is solved numerically
using a time-dependent problem for a pseudo-parabolic equation. For the
auxiliary Cauchy problem, the standard two-level schemes with weights are
applied. The numerical results are presented for a model two-dimen\-sional
boundary value problem with a fractional power of an elliptic operator. Our
work focuses on the solution of the boundary value problem with $0 <
\varepsilon \ll 1$."
"Coefficient inverse problems related to identifying the right-hand side of an
equation with use of additional information is of interest among inverse
problems for partial differential equations. When considering non-stationary
problems, tasks of recovering the dependence of the right-hand side on time and
spatial variables can be treated as independent. These tasks relate to a class
of linear inverse problems, which sufficiently simplifies their study. This
work is devoted to a finding the dependence of right-hand side of
multidimensional parabolic equation on spatial variables using additional
observations of the solution at the final point of time - the final
overdetermination. More general problems are associated with some integral
observation of the solution on time - the integral overdetermination. The first
method of numerical solution of inverse problems is based on iterative solution
of boundary value problem for time derivative with non-local acceleration. The
second method is based on the known approach with iterative refinement of
desired dependence of the right-hand side on spacial variables. Capabilities of
proposed methods are illustrated by numerical examples for model
two-dimensional problem of identifying the right-hand side of a parabolic
equation. The standard finite-element approximation on space is used, the time
discretization is based on fully implicit two-level schemes."
"We prove a complex polynomial of degree $n$ has at most $\lceil n/2 \rceil$
attractive fixed points lying on a line. We also consider the general case."
"We present a scheme to automatically set the precision of floating point
variables in an application. We design a framework that profiles applications
to measure undesirable numerical behavior at the floating point operation
level. We use this framework to perform mixed precision analysis to
heuristically set the precision of all variables in an application based on
their numerical profiles. We experimentally evaluate the mixed precision
analysis to show that it can generate a range of results with different
accuracy and performance characteristics."
"This paper proposes a novel approach to tensor completion, which recovers
missing entries of data represented by tensors. The approach is based on the
tensor train (TT) rank, which is able to capture hidden information from
tensors thanks to its definition from a well-balanced matricization scheme.
Accordingly, new optimization formulations for tensor completion are proposed
as well as two new algorithms for their solution. The first one called simple
low-rank tensor completion via tensor train (SiLRTC-TT) is intimately related
to minimizing a nuclear norm based on TT rank. The second one is from a
multilinear matrix factorization model to approximate the TT rank of a tensor,
and is called tensor completion by parallel matrix factorization via tensor
train (TMac-TT). A tensor augmentation scheme of transforming a low-order
tensor to higher-orders is also proposed to enhance the effectiveness of
SiLRTC-TT and TMac-TT. Simulation results for color image and video recovery
show the clear advantage of our method over all other methods."
"In order to approximate transandental functions, several algorithms were
proposed.Historically, polynomial interpolation, infinite series, $\cdots$ and
other$+,\times, -$ and $/$ based algorithms were studied for this purpose.The
CORDIC (COordinate Rotation DIgital Computer)introduced by Jack E. Volder in
1959, and generalized by J. S. Walther a few years later, is a hardware based
algorithmfor the approximation of trigonometric, hyperbolic andlogarithmic
functions.As a consequence, CORDIC is used for applications indiverse areas
such as signal and image processing.For these reasons, several modified
versions were proposed.In this article, we present anoverview of the CORDIC
algorithm for the computation of the circular functions, essentially the
scaling free version,and we will give a substential improvement to the commonly
used one."
"Volumetric spline parameterization and computational efficiency are two main
challenges in isogeometric analysis (IGA). To tackle this problem, we propose a
framework of computation reuse in IGA on a set of three-dimensional models with
similar semantic features. Given a template domain, B-spline based consistent
volumetric parameterization is first constructed for a set of models with
similar semantic features. An efficient quadrature-free method is investigated
in our framework to compute the entries of stiffness matrix by Bezier
extraction and polynomial approximation. In our approach, evaluation on the
stiffness matrix and imposition of the boundary conditions can be pre-computed
and reused during IGA on a set of CAD models. Examples with complex geometry
are presented to show the effectiveness of our methods, and efficiency similar
to the computation in linear finite element analysis can be achieved for IGA
taken on a set of models."
"Finite volume methods (FVMs) constitute a popular class of methods for the
numerical simulation of fluid flows. Among the various components of these
methods, the discretisation of the gradient operator has received less
attention despite its fundamental importance with regards to the accuracy of
the FVM. The most popular gradient schemes are the divergence theorem (DT) (or
Green-Gauss) scheme, and the least-squares (LS) scheme. Both are widely
believed to be second-order accurate, but the present study shows that in fact
the common variant of the DT gradient is second-order accurate only on
structured meshes whereas it is zeroth-order accurate on general unstructured
meshes, and the LS gradient is second-order and first-order accurate,
respectively. This is explained through a theoretical analysis and is confirmed
by numerical tests. The schemes are then used within a FVM to solve a simple
diffusion equation on unstructured grids generated by several methods; the
results reveal that the zeroth-order accuracy of the DT gradient is inherited
by the FVM as a whole, and the discretisation error does not decrease with grid
refinement. On the other hand, use of the LS gradient leads to second-order
accurate results, as does the use of alternative, consistent, DT gradient
schemes, including a new iterative scheme that makes the common DT gradient
consistent at almost no extra cost. The numerical tests are performed using
both an in-house code and the popular public domain PDE solver OpenFOAM."
"We present a family of spacetree-based multigrid realizations using the
tree's multiscale nature to derive coarse grids. They align with matrix-free
geometric multigrid solvers as they never assemble the system matrices which is
cumbersome for dynamically adaptive grids and full multigrid. The most
sophisticated realizations use BoxMG to construct operator-dependent
prolongation and restriction in combination with Galerkin/Petrov-Galerkin
coarse-grid operators. This yields robust solvers for nontrivial elliptic
problems. We embed the algebraic, problem- and grid-dependent multigrid
operators as stencils into the grid and evaluate all matrix-vector products
in-situ throughout the grid traversals. While such an approach is not literally
matrix-free---the grid carries the matrix---we propose to switch to a
hierarchical representation of all operators. Only differences of algebraic
operators to their geometric counterparts are held. These hierarchical
differences can be stored and exchanged with small memory footprint. Our
realizations support arbitrary dynamically adaptive grids while they vertically
integrate the multilevel operations through spacetree linearization. This
yields good memory access characteristics, while standard colouring of mesh
entities with domain decomposition allows us to use parallel manycore clusters.
All realization ingredients are detailed such that they can be used by other
codes."
"In this article, we investigate the debated Instantaneous Frequency (IF)
topic. Here, we show that IF is non-unique inherently. We explain how this
non-uniqueness can be quantified and explained from a mathematical perspective.
The non-uniqueness of the IF can also be observed if different methods of
adaptive signal processing are used. We will also show that even if we know the
physical origin of an oscillatory signal, e.g. linear second order ordinary
differential equation, the non-uniqueness is still present. All in all, we will
end up with the conclusion that, without any a priori assumption about the
relationship of the envelope and phase function of an oscillatory signal, there
is not any preferred neither best representation of the IF of such oscillatory
signal."
"Symmetric nonnegative matrix factorization (SNMF) is equivalent to computing
a symmetric nonnegative low rank approximation of a data similarity matrix. It
inherits the good data interpretability of the well-known nonnegative matrix
factorization technique and have better ability of clustering nonlinearly
separable data. In this paper, we focus on the algorithmic aspect of the SNMF
problem and propose simple inexact block coordinate decent methods to address
the problem, leading to both serial and parallel algorithms. The proposed
algorithms have guaranteed stationary convergence and can efficiently handle
large-scale and/or sparse SNMF problems. Extensive simulations verify the
effectiveness of the proposed algorithms compared to recent state-of-the-art
algorithms."
"The growing use of neuroimaging technologies generates a massive amount of
biomedical data that exhibit high dimensionality. Tensor-based analysis of
brain imaging data has been proved quite effective in exploiting their multiway
nature. The advantages of tensorial methods over matrix-based approaches have
also been demonstrated in the characterization of functional magnetic resonance
imaging (fMRI) data, where the spatial (voxel) dimensions are commonly grouped
(unfolded) as a single way/mode of the 3-rd order array, the other two ways
corresponding to time and subjects. However, such methods are known to be
ineffective in more demanding scenarios, such as the ones with strong noise
and/or significant overlapping of activated regions. This paper aims at
investigating the possible gains from a better exploitation of the spatial
dimension, through a higher- (4 or 5) order tensor modeling of the fMRI signal.
In this context, and in order to increase the degrees of freedom of the
modeling process, a higher-order Block Term Decomposition (BTD) is applied, for
the first time in fMRI analysis. Its effectiveness is demonstrated via
extensive simulation results."
"In this paper, we propose a nonlinear dimensionality reduction algorithm for
the manifold of Symmetric Positive Definite (SPD) matrices that considers the
geometry of SPD matrices and provides a low dimensional representation of the
manifold with high class discrimination. The proposed algorithm, tries to
preserve the local structure of the data by preserving distance to local mean
(DPLM) and also provides an implicit projection matrix. DPLM is linear in terms
of the number of training samples and may use the label information when they
are available in order to performance improvement in classification tasks. We
performed several experiments on the multi-class dataset IIa from BCI
competition IV. The results show that our approach as dimensionality reduction
technique - leads to superior results in comparison with other competitor in
the related literature because of its robustness against outliers. The
experiments confirm that the combination of DPLM with FGMDM as the classifier
leads to the state of the art performance on this dataset."
"We study the Monte Carlo method for solving a linear system of the form $x =
H x + b$. A sufficient condition for the method to work is $\| H \| < 1$, which
greatly limits the usability of this method. We improve this condition by
proposing a new multi-way Markov random walk, which is a generalization of the
standard Markov random walk. Under our new framework we prove that the
necessary and sufficient condition for our method to work is the spectral
radius $\rho(H^{+}) < 1$, which is a weaker requirement than $\| H \| < 1$. In
addition to solving more problems, our new method can work faster than the
standard algorithm. In numerical experiments on both synthetic and real world
matrices, we demonstrate the effectiveness of our new method."
"Using fundamental ideas from [Brattka&Hertling'98] and by means of
object-oriented overloading of operators, the iRRAM library supports imperative
programming over the reals with a both sound and computable, multivalued
semantics of tests. We extend Floyd-Hoare Logic to formally verify the
correctness of symbolic-numerical algorithms employing such primitives for
three example problems: truncated binary logarithm, 1D simple root finding, and
solving systems of linear equations. This is to be generalized to other hybrid
(i.e. discrete and continuous) abstract data types."
"This paper is concerned with approximating the dominant left singular vector
space of a real matrix $A$ of arbitrary dimension, from block Krylov spaces
generated by the matrix $AA^T$ and the block vector $AX$. Two classes of
results are presented. First are bounds on the distance, in the two and
Frobenius norms, between the Krylov space and the target space. The distance is
expressed in terms of principal angles. Second are quality of approximation
bounds, relative to the best approximation in the Frobenius norm. For starting
guesses $X$ of full column-rank, the bounds depend on the tangent of the
principal angles between $X$ and the dominant right singular vector space of
$A$. The results presented here form the structural foundation for the analysis
of randomized Krylov space methods. The innovative feature is a combination of
traditional Lanczos convergence analysis with optimal approximations via least
squares problems."
"The L1 norm has been tremendously popular in signal and image processing in
the past two decades due to its sparsity-promoting properties. More recently,
its generalization to non-Euclidean domains has been found useful in shape
analysis applications. For example, in conjunction with the minimization of the
Dirichlet energy, it was shown to produce a compactly supported quasi-harmonic
orthonormal basis, dubbed as compressed manifold modes. The continuous L1 norm
on the manifold is often replaced by the vector l1 norm applied to sampled
functions. We show that such an approach is incorrect in the sense that it does
not consistently discretize the continuous norm and warn against its
sensitivity to the specific sampling. We propose two alternative
discretizations resulting in an iteratively-reweighed l2 norm. We demonstrate
the proposed strategy on the compressed modes problem, which reduces to a
sequence of simple eigendecomposition problems not requiring non-convex
optimization on Stiefel manifolds and producing more stable and accurate
results."
"Fourier domain structured low-rank matrix priors are emerging as powerful
alternatives to traditional image recovery methods such as total variation and
wavelet regularization. These priors specify that a convolutional structured
matrix, i.e., Toeplitz, Hankel, or their multi-level generalizations, built
from Fourier data of the image should be low-rank. The main challenge in
applying these schemes to large-scale problems is the computational complexity
and memory demand resulting from lifting the image data to a large scale
matrix. We introduce a fast and memory efficient approach called the Generic
Iterative Reweighted Annihilation Filter (GIRAF) algorithm that exploits the
convolutional structure of the lifted matrix to work in the original un-lifted
domain, thus considerably reducing the complexity. Our experiments on the
recovery of images from undersampled Fourier measurements show that the
resulting algorithm is considerably faster than previously proposed algorithms,
and can accommodate much larger problem sizes than previously studied."
"Providing flexibility and user-interpretability in nonlinear system
identification can be achieved by means of block-oriented methods. One of such
block-oriented system structures is the parallel Wiener-Hammerstein system,
which is a sum of Wiener-Hammerstein branches, consisting of static
nonlinearities sandwiched between linear dynamical blocks. Parallel
Wiener-Hammerstein models have more descriptive power than their single-branch
counterparts, but their identification is a non-trivial task that requires
tailored system identification methods. In this work, we will tackle the
identification problem by performing a tensor decomposition of the Volterra
kernels obtained from the nonlinear system. We illustrate how the parallel
Wiener-Hammerstein block-structure gives rise to a joint tensor decomposition
of the Volterra kernels with block-circulant structured factors. The
combination of Volterra kernels and tensor methods is a fruitful way to tackle
the parallel Wiener-Hammerstein system identification task. In simulation
experiments, we were able to reconstruct very accurately the underlying blocks
under noisy conditions."
"Decompositions of tensors into factor matrices, which interact through a core
tensor, have found numerous applications in signal processing and machine
learning. A more general tensor model which represents data as an ordered
network of sub-tensors of order-2 or order-3 has, so far, not been widely
considered in these fields, although this so-called tensor network
decomposition has been long studied in quantum physics and scientific
computing. In this study, we present novel algorithms and applications of
tensor network decompositions, with a particular focus on the tensor train
decomposition and its variants. The novel algorithms developed for the tensor
train decomposition update, in an alternating way, one or several core tensors
at each iteration, and exhibit enhanced mathematical tractability and
scalability to exceedingly large-scale data tensors. The proposed algorithms
are tested in classic paradigms of blind source separation from a single
mixture, denoising, and feature extraction, and achieve superior performance
over the widely used truncated algorithms for tensor train decomposition."
"Objective: To present the first real-time a posteriori error-driven adaptive
finite element approach for real-time simulation and to demonstrate the method
on a needle insertion problem. Methods: We use corotational elasticity and a
frictional needle/tissue interaction model. The problem is solved using finite
elements within SOFA. The refinement strategy relies upon a hexahedron-based
finite element method, combined with a posteriori error estimation driven local
$h$-refinement, for simulating soft tissue deformation. Results: We control the
local and global error level in the mechanical fields (e.g. displacement or
stresses) during the simulation. We show the convergence of the algorithm on
academic examples, and demonstrate its practical usability on a percutaneous
procedure involving needle insertion in a liver. For the latter case, we
compare the force displacement curves obtained from the proposed adaptive
algorithm with that obtained from a uniform refinement approach. Conclusions:
Error control guarantees that a tolerable error level is not exceeded during
the simulations. Local mesh refinement accelerates simulations. Significance:
Our work provides a first step to discriminate between discretization error and
modeling error by providing a robust quantification of discretization error
during simulations."
"The integration of surface normals for the purpose of computing the shape of
a surface in 3D space is a classic problem in computer vision. However, even
nowadays it is still a challenging task to devise a method that combines the
flexibility to work on non-trivial computational domains with high accuracy,
robustness and computational efficiency. By uniting a classic approach for
surface normal integration with modern computational techniques we construct a
solver that fulfils these requirements. Building upon the Poisson integration
model we propose to use an iterative Krylov subspace solver as a core step in
tackling the task. While such a method can be very efficient, it may only show
its full potential when combined with a suitable numerical preconditioning and
a problem-specific initialisation. We perform a thorough numerical study in
order to identify an appropriate preconditioner for our purpose. To address the
issue of a suitable initialisation we propose to compute this initial state via
a recently developed fast marching integrator. Detailed numerical experiments
illuminate the benefits of this novel combination. In addition, we show on
real-world photometric stereo datasets that the developed numerical framework
is flexible enough to tackle modern computer vision applications."
"We present a method to approximate functionals $\text{Tr} \, f(A)$ of very
high-dimensional hermitian matrices $A$ represented as Matrix Product Operators
(MPOs). Our method is based on a reformulation of a block Lanczos algorithm in
tensor network format. We state main properties of the method and show how to
adapt the basic Lanczos algorithm to the tensor network formalism to allow for
high-dimensional computations. Additionally, we give an analysis of the
complexity of our method and provide numerical evidence that it yields good
approximations of the entropy of density matrices represented by MPOs while
being robust against truncations."
"The Sinc approximation is a function approximation formula that attains
exponential convergence for rapidly decaying functions defined on the whole
real axis. Even for other functions, the Sinc approximation works accurately
when combined with a proper variable transformation. The convergence rate has
been analyzed for typical cases including finite, semi-infinite, and infinite
intervals. Recently, for verified numerical computations, a more explicit,
""computable"" error bound has been given in the case of a finite interval. In
this paper, such explicit error bounds are derived for other cases."
"A longstanding problem related to floating-point implementation of numerical
programs is to provide efficient yet precise analysis of output errors.
  We present a framework to compute lower bounds of absolute roundoff errors
for numerical programs implementing polynomial functions with box constrained
input variables. Our study relies on semidefinite programming (SDP) relaxations
and is complementary of over-approximation frameworks, consisting of obtaining
upper bounds for the absolute roundoff error.
  Our method is based on a new hierarchy of convergent robust SDP
approximations for certain classes of polynomial optimization problems. Each
problem in this hierarchy can be exactly solved via SDP. By using this
hierarchy, one can provide a monotone non-decreasing sequence of lower bounds
converging to the absolute roundoff error of a program implementing a
polynomial function.
  We investigate the efficiency and precision of our method on non-trivial
polynomial programs coming from space control, optimization and computational
biology."
"Alternating Direction Method of Multipliers (ADMM) is a popular method in
solving Machine Learning problems. Stochastic ADMM was firstly proposed in
order to reduce the per iteration computational complexity, which is more
suitable for big data problems. Recently, variance reduction techniques have
been integrated with stochastic ADMM in order to get a fast convergence rate,
such as SAG-ADMM and SVRG-ADMM,but the convergence is still suboptimal w.r.t
the smoothness constant. In this paper, we propose a new accelerated stochastic
ADMM algorithm with variance reduction, which enjoys a faster convergence than
all the other stochastic ADMM algorithms. We theoretically analyze its
convergence rate and show its dependence on the smoothness constant is optimal.
We also empirically validate its effectiveness and show its priority over other
stochastic ADMM algorithms."
"This manuscript contains some thoughts on the discretization of the classical
heat equation. Namely, we discuss the advantages and disadvantages of explicit
and implicit schemes. Then, we show how to overcome some disadvantages while
preserving some advantages. However, since there is no free lunch, there is a
price to pay for any improvement in the numerical scheme. This price will be
thoroughly discussed below.In particular, we like explicit discretizations for
the ease of their implementation even for nonlinear problems. Unfortunately,
when these schemes are applied to parabolic equations, severe stability limits
appear for the time step magnitude making the explicit simulations
prohibitively expensive. Implicit schemes remove the stability limit, but each
time step requires now the solution of linear (at best) or even nonlinear
systems of equations. However, there exists a number of tricks to overcome (or
at least to relax) severe stability limitations of explicit schemes without
going into the trouble of fully implicit ones. The purpose of this manuscript
is just to inform the readers about these alternative techniques to extend the
stability limits. It was not written for classical scientific publication
purposes."
"We present a polynomial multigrid method for the nodal interior penalty
formulation of the Poisson equation on three-dimensional Cartesian grids. Its
key ingredient is a weighted overlapping Schwarz smoother operating on
element-centered subdomains. The MG method reaches superior convergence rates
corresponding to residual reductions of about two orders of magnitude within a
single V(1,1) cycle. It is robust with respect to the mesh size and the ansatz
order, at least up to ${P=32}$. Rigorous exploitation of tensor-product
factorization yields a computational complexity of $O(PN)$ for $N$ unknowns,
whereas numerical experiments indicate even linear runtime scaling. Moreover,
by allowing adjustable subdomain overlaps and adding Krylov acceleration, the
method proved feasible for anisotropic grids with element aspect ratios up to
48."
"Laguerre and Laguerre-type polynomials are orthogonal polynomials on the
interval $[0,\infty)$ with respect to a weight function of the form $w(x) =
x^{\alpha} e^{-Q(x)}, Q(x) = \sum_{k=0}^m q_k x^k, \alpha > -1, q_m > 0$. The
classical Laguerre polynomials correspond to $Q(x)=x$. The computation of
higher-order terms of the asymptotic expansions of these polynomials for large
degree becomes quite complicated, and a full description seems to be lacking in
literature. However, this information is implicitly available in the work of
Vanlessen, based on a non-linear steepest descent analysis of an associated
so-called Riemann--Hilbert problem. We will extend this work and show how to
efficiently compute an arbitrary number of higher-order terms in the asymptotic
expansions of Laguerre and Laguerre-type polynomials. This effort is similar to
the case of Jacobi and Jacobi-type polynomials in a previous paper. We supply
an implementation with explicit expansions in four different regions of the
complex plane. These expansions can also be extended to Hermite-type weights of
the form $\exp(-\sum_{k=0}^m q_k x^{2k})$ on $(-\infty,\infty)$, and to general
non-polynomial functions $Q(x)$ using contour integrals. The expansions may be
used, e.g., to compute Gauss-Laguerre quadrature rules in a lower computational
complexity than based on the recurrence relation, and with improved accuracy
for large degree. They are also of interest in random matrix theory."
"This work is devoted to the design of interior penalty discontinuous Galerkin
(dG) schemes that preserve maximum principles at the discrete level for the
steady transport and convection-diffusion problems and the respective transient
problems with implicit time integration. Monotonic schemes that combine
explicit time stepping with dG space discretization are very common, but the
design of such schemes for implicit time stepping is rare, and it had only been
attained so far for 1D problems. The proposed scheme is based on an artificial
diffusion that linearly depends on a shock detector that identifies the
troublesome areas. In order to define the new shock detector, we have
introduced the concept of discrete local extrema. The diffusion operator is a
graph-Laplacian, instead of the more common finite element discretization of
the Laplacian operator, which is essential to keep monotonicity on general
meshes and in multi-dimension. The resulting nonlinear stabilization is
non-smooth and nonlinear solvers can fail to converge. As a result, we propose
a smoothed (twice differentiable) version of the nonlinear stabilization, which
allows us to use Newton with line search nonlinear solvers and dramatically
improve nonlinear convergence. A theoretical numerical analysis of the proposed
schemes show that they satisfy the desired monotonicity properties. Further,
the resulting operator is Lipschitz continuous and there exists at least one
solution of the discrete problem, even in the non-smooth version. We provide a
set of numerical results to support our findings."
"This report describes the computation of gradients by algorithmic
differentiation for statistically optimum beamforming operations. Especially
the derivation of complex-valued functions is a key component of this approach.
Therefore the real-valued algorithmic differentiation is extended via the
complex-valued chain rule. In addition to the basic mathematic operations the
derivative of the eigenvalue problem with complex-valued eigenvectors is one of
the key results of this report. The potential of this approach is shown with
experimental results on the CHiME-3 challenge database. There, the beamforming
task is used as a front-end for an ASR system. With the developed derivatives a
joint optimization of a speech enhancement and speech recognition system w.r.t.
the recognition optimization criterion is possible."
"This thesis examines a modern concept for machine numbers based on interval
arithmetic called 'Unums' and compares it to IEEE 754 floating-point
arithmetic, evaluating possible uses of this format where floating-point
numbers are inadequate. In the course of this examination, this thesis builds
theoretical foundations for IEEE 754 floating-point numbers, interval
arithmetic based on the projectively extended real numbers and Unums."
"The paper covers a formulation of the inverse quadratic programming problem
in terms of unconstrained optimization where it is required to find the unknown
parameters (the matrix of the quadratic form and the vector of the quasi-linear
part of the quadratic form) provided that approximate estimates of the optimal
solution of the direct problem and those of the target function to be minimized
in the form of pairs of values lying in the corresponding neighborhoods are
only known. The formulation of the inverse problem and its solution are based
on the least squares method. In the explicit form the inverse problem solution
has been derived in the form a system of linear equations. The parameters
obtained can be used for reconstruction of the direct quadratic programming
problem and determination of the optimal solution and the extreme value of the
target function, which were not known formerly. It is possible this approach
opens new ways in over applications, for example, in neurocomputing and quadric
surfaces fitting. Simple numerical examples have been demonstrated. A scenario
in the Octave/MATLAB programming language has been proposed for practical
implementation of the method."
"A machine-learning-based framework for modeling the error introduced by
surrogate models of parameterized dynamical systems is proposed. The framework
entails the use of high-dimensional regression techniques (e.g., random
forests, LASSO) to map a large set of inexpensively computed `error indicators'
(i.e., features) produced by the surrogate model at a given time instance to a
prediction of the surrogate-model error in a quantity of interest (QoI). This
eliminates the need for the user to hand-select a small number of informative
features. The methodology requires a training set of parameter instances at
which the time-dependent surrogate-model error is computed by simulating both
the high-fidelity and surrogate models. Using these training data, the method
first determines regression-model locality (via classification or clustering),
and subsequently constructs a `local' regression model to predict the
time-instantaneous error within each identified region of feature space. We
consider two uses for the resulting error model: (1) as a correction to the
surrogate-model QoI prediction at each time instance, and (2) as a way to
statistically model arbitrary functions of the time-dependent surrogate-model
error (e.g., time-integrated errors). We apply the proposed framework to model
errors in reduced-order models of nonlinear oil--water subsurface flow
simulations. The reduced-order models used in this work entail application of
trajectory piecewise linearization with proper orthogonal decomposition. When
the first use of the method is considered, numerical experiments demonstrate
consistent improvement in accuracy in the time-instantaneous QoI prediction
relative to the original surrogate model, across a large number of test cases.
When the second use is considered, results show that the proposed method
provides accurate statistical predictions of the time- and well-averaged
errors."
"In this work, we propose two-level space-time domain decomposition
preconditioners for parabolic problems discretized using finite elements. They
are motivated as an extension to space-time of balancing domain decomposition
by constraints preconditioners. The key ingredients to be defined are the
sub-assembled space and operator, the coarse degrees of freedom (DOFs) in which
we want to enforce continuity among subdomains at the preconditioner level, and
the transfer operator from the sub-assembled to the original finite element
space. With regard to the sub-assembled operator, a perturbation of the time
derivative is needed to end up with a well-posed preconditioner. The set of
coarse DOFs includes the time average (at the space-time subdomain) of
classical space constraints plus new constraints between consecutive subdomains
in time. Numerical experiments show that the proposed schemes are weakly
scalable in time, i.e., we can efficiently exploit increasing computational
resources to solve more time steps in the same {total elapsed} time. Further,
the scheme is also weakly space-time scalable, since it leads to asymptotically
constant iterations when solving larger problems both in space and time.
Excellent {wall clock} time weak scalability is achieved for space-time
parallel solvers on some thousands of cores."
"On modern large-scale parallel computers, the performance of Krylov subspace
iterative methods is limited by global synchronization. This has inspired the
development of $s$-step Krylov subspace method variants, in which iterations
are computed in blocks of $s$, which can reduce the number of global
synchronizations per iteration by a factor of $O(s)$.
  Although the $s$-step variants are mathematically equivalent to their
classical counterparts, they can behave quite differently in finite precision
depending on the parameter $s$. If $s$ is chosen too large, the $s$-step method
can suffer a convergence delay and a decrease in attainable accuracy relative
to the classical method. This makes it difficult for a potential user of such
methods - the $s$ value that minimizes the time per iteration may not be the
best $s$ for minimizing the overall time-to-solution, and further may cause an
unacceptable decrease in accuracy.
  Towards improving the reliability and usability of $s$-step Krylov subspace
methods, in this work we derive the \emph{adaptive $s$-step CG method}, a
variable $s$-step CG method where in block $k$, the parameter $s_k$ is
determined automatically such that a user-specified accuracy is attainable. The
method for determining $s_k$ is based on a bound on growth of the residual gap
within block $k$, from which we derive a constraint on the condition numbers of
the computed $O(s_k)$-dimensional Krylov subspace bases. The computations
required for determining the block size $s_k$ can be performed without
increasing the number of global synchronizations per block. Our numerical
experiments demonstrate that the adaptive $s$-step CG method is able to attain
up to the same accuracy as classical CG while still significantly reducing the
total number of global synchronizations."
"In this paper we introduce a novel algorithm of calculating arbitrary order
cumulants of multidimensional data. Since the n th order cumulant can be
presented in the form of an n-dimensional tensor, the algorithm is presented
using the tensor network notation. The presented algorithm exploits the
super--symmetry of cumulant and moment tensors. We show, that proposed
algorithm highly decreases the computational complexity of cumulants
calculation, compared to the naive algorithm"
"In this paper a new hp-adaptive strategy for elliptic problems based on
refinement history is proposed, which chooses h-, p- or hp-refinement on
individual elements according to a posteriori error estimate, as well as
smoothness estimate of the solution obtained by comparing the actual and
expected error reduction rate. Numerical experiments show that exponential
convergence can be achieved with this strategy."
"Lower bounds on the smallest eigenvalue of a symmetric positive definite
matrices $A\in\mathbb{R}^{m\times m}$ play an important role in condition
number estimation and in iterative methods for singular value computation. In
particular, the bounds based on ${\rm Tr}(A^{-1})$ and ${\rm Tr}(A^{-2})$
attract attention recently because they can be computed in $O(m)$ work when $A$
is tridiagonal. In this paper, we focus on these bounds and investigate their
properties in detail. First, we consider the problem of finding the optimal
bound that can be computed solely from ${\rm Tr}(A^{-1})$ and ${\rm
Tr}(A^{-2})$ and show that so called Laguerre's lower bound is the optimal one
in terms of sharpness. Next, we study the gap between the Laguerre bound and
the smallest eigenvalue. We characterize the situation in which the gap becomes
largest in terms of the eigenvalue distribution of $A$ and show that the gap
becomes smallest when ${\rm Tr}(A^{-2})/\{{\rm Tr}(A^{-1})\}^2$ approaches 1 or
$\frac{1}{m}$. These results will be useful, for example, in designing
efficient shift strategies for singular value computation algorithms."
"For a wide variety of problems, creating detailed continuous models of
(continuous) physical systems is, at the very least, impractical. Hybrid models
can abstract away short transient behaviour (thus introducing discontinuities)
in order to simplify the study of such systems. For example, when modelling a
bouncing ball, the bounce can be abstracted as a discontinuous change of the
velocity, instead of resorting to the physics of the ball (de-)compression to
keep the velocity signal continuous. Impulsive differential equations can be
used to model and simulate hybrid systems such as the bouncing ball. In this
approach, the force acted on the ball by the floor is abstracted as an
infinitely large function in an infinitely small interval of time, that is, an
impulse. Current simulators cannot handle such approximations well due to the
limitations of machine precision.
  In this paper, we explore the simulation of impulsive differential equations,
where impulses are first class citizens. We present two approaches for the
simulation of impulses: symbolic and numerical. Our contribution is a
theoretically founded description of the implementation of both approaches in a
Causal Block Diagram modelling and simulation tool. Furthermore, we investigate
the conditions for which one approach is better than the other."
"This research investigates the implementation mechanism of block-wise ILU(k)
preconditioner on GPU. The block-wise ILU(k) algorithm requires both the level
k and the block size to be designed as variables. A decoupled ILU(k) algorithm
consists of a symbolic phase and a factorization phase. In the symbolic phase,
a ILU(k) nonzero pattern is established from the point-wise structure extracted
from a block-wise matrix. In the factorization phase, the block-wise matrix
with a variable block size is factorized into a block lower triangular matrix
and a block upper triangular matrix. And a further diagonal factorization is
required to perform on the block upper triangular matrix for adapting a
parallel triangular solver on GPU.We also present the numerical experiments to
study the preconditioner actions on different k levels and block sizes."
"In this letter, we propose an algorithm for recovery of sparse and low rank
components of matrices using an iterative method with adaptive thresholding. In
each iteration, the low rank and sparse components are obtained using a
thresholding operator. This algorithm is fast and can be implemented easily. We
compare it with one of the most common fast methods in which the rank and
sparsity are approximated by $\ell_1$ norm. We also apply it to some real
applications where the noise is not so sparse. The simulation results show that
it has a suitable performance with low run-time."
"The CANDECOMP/PARAFAC (CP) tensor decomposition is a popular
dimensionality-reduction method for multiway data. Dimensionality reduction is
often sought since many high-dimensional tensors have low intrinsic rank
relative to the dimension of the ambient measurement space. However, the
emergence of `big data' poses significant computational challenges for
computing this fundamental tensor decomposition. Leveraging modern randomized
algorithms, we demonstrate that the coherent structure can be learned from a
smaller representation of the tensor in a fraction of the time. Moreover, the
high-dimensional signal can be faithfully approximated from the compressed
measurements. Thus, this simple but powerful algorithm enables one to compute
the approximate CP decomposition even for massive tensors. The approximation
error can thereby be controlled via oversampling and the computation of power
iterations. In addition to theoretical results, several empirical results
demonstrate the performance of the proposed algorithm."
"Convolution with Green's function of a differential operator appears in a lot
of applications e.g. Lippmann-Schwinger integral equation. Algorithms for
computing such are usually non-trivial and require non-uniform mesh. However,
recently Vico, Greengard and Ferrando developed method for computing
convolution with smooth functions with compact support with spectral accuracy,
requiring nothing more than Fast Fourier Transform (FFT). Their approach is
very suitable for the low-rank tensor implementation which we develop using
Quantized Tensor Train (QTT) decomposition."