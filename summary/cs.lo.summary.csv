summary
"Manna and Waldinger's theory of substitutions and unification has been
verified using the Cambridge LCF theorem prover. A proof of the monotonicity of
substitution is presented in detail, as an example of interaction with LCF.
Translating the theory into LCF's domain-theoretic logic is largely
straightforward. Well-founded induction on a complex ordering is translated
into nested structural inductions. Correctness of unification is expressed
using predicates for such properties as idempotence and most-generality. The
verification is presented as a series of lemmas. The LCF proofs are compared
with the original ones, and with other approaches. It appears difficult to find
a logic that is both simple and flexible, especially for proving termination."
"Martin-L\""of's Intuitionistic Theory of Types is becoming popular for formal
reasoning about computer programs. To handle recursion schemes other than
primitive recursion, a theory of well-founded relations is presented. Using
primitive recursion over higher types, induction and recursion are formally
derived for a large class of well-founded relations. Included are < on natural
numbers, and relations formed by inverse images, addition, multiplication, and
exponentiation of other relations. The constructions are given in full detail
to allow their use in theorem provers for Type Theory, such as Nuprl. The
theory is compared with work in the field of ordinal recursion over higher
types."
"Boyer and Moore have discussed a recursive function that puts conditional
expressions into normal form [1]. It is difficult to prove that this function
terminates on all inputs. Three termination proofs are compared: (1) using a
measure function, (2) in domain theory using LCF, (3) showing that its
recursion relation, defined by the pattern of recursive calls, is well-founded.
The last two proofs are essentially the same though conducted in markedly
different logical frameworks. An obviously total variant of the normalize
function is presented as the `computational meaning' of those two proofs. A
related function makes nested recursive calls. The three termination proofs
become more complex: termination and correctness must be proved simultaneously.
The recursion relation approach seems flexible enough to handle subtle
termination proofs where previously domain theory seemed essential."
"An interactive theorem prover, Isabelle, is under development. In LCF, each
inference rule is represented by one function for forwards proof and another (a
tactic) for backwards proof. In Isabelle, each inference rule is represented by
a Horn clause. Resolution gives both forwards and backwards proof, supporting a
large class of logics. Isabelle has been used to prove theorems in
Martin-L\""of's Constructive Type Theory. Quantifiers pose several difficulties:
substitution, bound variables, Skolemization. Isabelle's representation of
logical syntax is the typed lambda-calculus, requiring higher- order
unification. It may have potential for logic programming. Depth-first
subgoaling along inference rules constitutes a higher-order Prolog."
"Isabelle is an interactive theorem prover that supports a variety of logics.
It represents rules as propositions (not as functions) and builds proofs by
combining rules. These operations constitute a meta-logic (or `logical
framework') in which the object-logics are formalized. Isabelle is now based on
higher-order logic -- a precise and well-understood foundation. Examples
illustrate use of this meta-logic to formalize logics and proofs. Axioms for
first-order logic are shown sound and complete. Backwards proof is formalized
by meta-reasoning about object-level entailment. Higher-order logic has several
practical advantages over other meta-logics. Many proof techniques are known,
such as Huet's higher-order unification procedure."
"Isabelle is a generic theorem prover, designed for interactive reasoning in a
variety of formal theories. At present it provides useful proof procedures for
Constructive Type Theory, various first-order logics, Zermelo-Fraenkel set
theory, and higher-order logic. This survey of Isabelle serves as an
introduction to the literature. It explains why generic theorem proving is
beneficial. It gives a thorough history of Isabelle, beginning with its origins
in the LCF system. It presents an account of how logics are represented,
illustrated using classical logic. The approach is compared with the Edinburgh
Logical Framework. Several of the Isabelle object-logics are presented."
"Simple type theory is formulated for use with the generic theorem prover
Isabelle. This requires explicit type inference rules. There are function,
product, and subset types, which may be empty. Descriptions (the eta-operator)
introduce the Axiom of Choice. Higher-order logic is obtained through
reflection between formulae and terms of type bool. Recursive types and
functions can be formally constructed. Isabelle proof procedures are described.
The logic appears suitable for general mathematics as well as computational
problems."
"Many automatic theorem-provers rely on rewriting. Using theorems as rewrite
rules helps to simplify the subgoals that arise during a proof.
  LCF is an interactive theorem-prover intended for reasoning about
computation. Its implementation of rewriting is presented in detail. LCF
provides a family of rewriting functions, and operators to combine them. A
succession of functions is described, from pattern matching primitives to the
rewriting tool that performs most inferences in LCF proofs.
  The design is highly modular. Each function performs a basic, specific task,
such as recognizing a certain form of tautology. Each operator implements one
method of building a rewriting function from simpler ones. These pieces can be
put together in numerous ways, yielding a variety of rewrit- ing strategies.
  The approach involves programming with higher-order functions. Rewriting
functions are data values, produced by computation on other rewriting
functions. The code is in daily use at Cambridge, demonstrating the practical
use of functional programming."
"An attempt at unifying logic and functional programming is reported. As a
starting point, we take the view that ""logic programs"" are not about logic but
constitute inductive definitions of sets and relations. A skeletal language
design based on these considerations is sketched and a prototype implementation
discussed."
"A step-by-step presentation of the code for a small theorem prover introduces
theorem-proving techniques. The programming language used is Standard ML. The
prover operates on a sequent calculus formulation of first-order logic, which
is briefly explained. The implementation of unification and logical inference
is shown. The prover is demonstrated on several small examples, including one
that shows its limitations. The final part of the paper is a survey of
contemporary research on interactive theorem proving."
"A logic for specification and verification is derived from the axioms of
Zermelo-Fraenkel set theory. The proofs are performed using the proof assistant
Isabelle. Isabelle is generic, supporting several different logics. Isabelle
has the flexibility to adapt to variants of set theory. Its higher-order syntax
supports the definition of new binding operators. Unknowns in subgoals can be
instantiated incrementally. The paper describes the derivation of rules for
descriptions, relations and functions, and discusses interactive proofs of
Cantor's Theorem, the Composition of Homomorphisms challenge [9], and Ramsey's
Theorem [5]. A generic proof assistant can stand up against provers dedicated
to particular logics."
"A theory of recursive definitions has been mechanized in Isabelle's
Zermelo-Fraenkel (ZF) set theory. The objective is to support the formalization
of particular recursive definitions for use in verification, semantics proofs
and other computational reasoning. Inductively defined sets are expressed as
least fixedpoints, applying the Knaster-Tarski Theorem over a suitable set.
Recursive functions are defined by well-founded recursion and its derivatives,
such as transfinite recursion. Recursive data structures are expressed by
applying the Knaster-Tarski Theorem to a set, such as V[omega], that is closed
under Cartesian product and disjoint sum. Worked examples include the
transitive closure of a relation, lists, variable-branching trees and mutually
recursive trees and forests. The Schr\""oder-Bernstein Theorem and the soundness
of propositional logic are proved in Isabelle sessions."
"A special final coalgebra theorem, in the style of Aczel's, is proved within
standard Zermelo-Fraenkel set theory. Aczel's Anti-Foundation Axiom is replaced
by a variant definition of function that admits non-well-founded constructions.
Variant ordered pairs and tuples, of possibly infinite length, are special
cases of variant functions. Analogues of Aczel's Solution and Substitution
Lemmas are proved in the style of Rutten and Turi. The approach is less general
than Aczel's, but the treatment of non-well-founded objects is simple and
concrete. The final coalgebra of a functor is its greatest fixedpoint. The
theory is intended for machine implementation and a simple case of it is
already implemented using the theorem prover Isabelle."
"Fairly deep results of Zermelo-Frenkel (ZF) set theory have been mechanized
using the proof assistant Isabelle. The results concern cardinal arithmetic and
the Axiom of Choice (AC). A key result about cardinal multiplication is K*K =
K, where K is any infinite cardinal. Proving this result required developing
theories of orders, order-isomorphisms, order types, ordinal arithmetic,
cardinals, etc.; this covers most of Kunen, Set Theory, Chapter I. Furthermore,
we have proved the equivalence of 7 formulations of the Well-ordering Theorem
and 20 formulations of AC; this covers the first two chapters of Rubin and
Rubin, Equivalents of the Axiom of Choice, and involves highly technical
material. The definitions used in the proofs are largely faithful in style to
the original mathematics."
"A theory of recursive and corecursive definitions has been developed in
higher-order logic (HOL) and mechanized using Isabelle. Least fixedpoints
express inductive data types such as strict lists; greatest fixedpoints express
coinductive data types, such as lazy lists. Well-founded recursion expresses
recursive functions over inductive data types; corecursion expresses functions
that yield elements of coinductive data types. The theory rests on a
traditional formalization of infinite trees. The theory is intended for use in
specification and verification. It supports reasoning about a wide range of
computable functions, but it does not formalize their operational semantics and
can express noncomputable functions also. The theory is illustrated using
finite and infinite lists. Corecursion expresses functions over infinite lists;
coinduction reasons about such functions."
"This book chapter establishes connections between the interactive proof tool
Isabelle and classical tableau and resolution technology. Isabelle's classical
reasoner is described and demonstrated by an extended case study: the
Church-Rosser theorem for combinators. Compared with other interactive theorem
provers, Isabelle's classical reasoner achieves a high degree of automation."
"Uniform proofs are sequent calculus proofs with the following characteristic:
the last step in the derivation of a complex formula at any stage in the proof
is always the introduction of the top-level logical symbol of that formula. We
investigate the relevance of this uniform proof notion to structuring proof
search in classical logic. A logical language in whose context provability is
equivalent to uniform provability admits of a goal-directed proof procedure
that interprets logical symbols as search directives whose meanings are given
by the corresponding inference rules. While this uniform provability property
does not hold directly of classical logic, we show that it holds of a fragment
of it that only excludes essentially positive occurrences of universal
quantifiers under a modest, sound, modification to the set of assumptions: the
addition to them of the negation of the formula being proved. We further note
that all uses of the added formula can be factored into certain derived rules.
The resulting proof system and the uniform provability property that holds of
it are used to outline a proof procedure for classical logic. An interesting
aspect of this proof procedure is that it incorporates within it previously
proposed mechanisms for dealing with disjunctive information in assumptions and
for handling hypotheticals. Our analysis sheds light on the relationship
between these mechanisms and the notion of uniform proofs."
"Based on an analysis of the inference rules used, we provide a
characterization of the situations in which classical provability entails
intuitionistic provability. We then examine the relationship of these
derivability notions to uniform provability, a restriction of intuitionistic
provability that embodies a special form of goal-directedness. We determine,
first, the circumstances in which the former relations imply the latter. Using
this result, we identify the richest versions of the so-called abstract logic
programming languages in classical and intuitionistic logic. We then study the
reduction of classical and, derivatively, intuitionistic provability to uniform
provability via the addition to the assumption set of the negation of the
formula to be proved. Our focus here is on understanding the situations in
which this reduction is achieved. However, our discussions indicate the
structure of a proof procedure based on the reduction, a matter also considered
explicitly elsewhere."
"In this paper, we present a formalization of Kozen's propositional modal
$\mu$-calculus, in the Calculus of Inductive Constructions. We address several
problematic issues, such as the use of higher-order abstract syntax in
inductive sets in presence of recursive constructors, the encoding of modal
(``proof'') rules and of context sensitive grammars. The encoding can be used
in the \Coq system, providing an experimental computer-aided proof environment
for the interactive development of error-free proofs in the $\mu$-calculus. The
techniques we adopted can be readily ported to other languages and proof
systems featuring similar problematic issues."
"Flat iteration is a variation on the original binary version of the Kleene
star operation P*Q, obtained by restricting the first argument to be a sum of
atomic actions. It generalizes prefix iteration, in which the first argument is
a single action. Complete finite equational axiomatizations are given for five
notions of bisimulation congruence over basic CCS with flat iteration, viz.
strong congruence, branching congruence, eta-congruence, delay congruence and
weak congruence. Such axiomatizations were already known for prefix iteration
and are known not to exist for general iteration. The use of flat iteration has
two main advantages over prefix iteration: 1.The current axiomatizations
generalize to full CCS, whereas the prefix iteration approach does not allow an
elimination theorem for an asynchronous parallel composition operator. 2.The
greater expressiveness of flat iteration allows for much shorter completeness
proofs.
  In the setting of prefix iteration, the most convenient way to obtain the
completeness theorems for eta-, delay, and weak congruence was by reduction to
the completeness theorem for branching congruence. In the case of weak
congruence this turned out to be much simpler than the only direct proof found.
In the setting of flat iteration on the other hand, the completeness theorems
for delay and weak (but not eta-) congruence can equally well be obtained by
reduction to the one for strong congruence, without using branching congruence
as an intermediate step. Moreover, the completeness results for prefix
iteration can be retrieved from those for flat iteration, thus obtaining a
second indirect approach for proving completeness for delay and weak congruence
in the setting of prefix iteration."
"For arbitrary undirected graph $G$, we are designing SATISFIABILITY problem
(SAT) for HCP, using tools of Boolean algebra only. The obtained SAT be the
logic formulation of conditions for Hamiltonian cycle existence, and use $m$
Boolean variables, where $m$ is the number of graph edges. This Boolean
expression is true if and only if an initial graph is Hamiltonian. That is,
each satisfying assignment of the Boolean variables determines a Hamiltonian
cycle of $G$, and each Hamiltonian cycle of $G$ corresponds to a satisfying
assignment of the Boolean variables. In common case, the obtained Boolean
expression may has an exponential length (the number of Boolean literals)."
"We define a hierarchy of circuit complexity classes LD^i, whose depth are the
inverse of a function in Ackermann hierarchy. Then we introduce extremely weak
versions of length induction and construct a bounded arithmetic theory L^i_2
whose provably total functions exactly correspond to functions computable by
LD^i circuits. Finally, we prove a non-conservation result between L^i_2 and a
weaker theory AC^0CA which corresponds to the class AC^0. Our proof utilizes
KPT witnessing theorem."
"This paper introduces a logical system, called BV, which extends
multiplicative linear logic by a non-commutative self-dual logical operator.
This extension is particularly challenging for the sequent calculus, and so far
it is not achieved therein. It becomes very natural in a new formalism, called
the calculus of structures, which is the main contribution of this work.
Structures are formulae submitted to certain equational laws typical of
sequents. The calculus of structures is obtained by generalising the sequent
calculus in such a way that a new top-down symmetry of derivations is observed,
and it employs inference rules that rewrite inside structures at any depth.
These properties, in addition to allow the design of BV, yield a modular proof
of cut elimination."
"Many logic programming based approaches can be used to describe and solve
combinatorial search problems. On the one hand there are definite programs and
constraint logic programs that compute a solution as an answer substitution to
a query containing the variables of the constraint satisfaction problem. On the
other hand there are approaches based on stable model semantics, abduction, and
first-order logic model generation that compute solutions as models of some
theory. This paper compares these different approaches from point of view of
knowledge representation (how declarative are the programs) and from point of
view of performance (how good are they at solving typical problems)."
"Tabled logic programming is receiving increasing attention in the Logic
Programming community. It avoids many of the shortcomings of SLD execution and
provides a more flexible and often extremely efficient execution mechanism for
logic programs. In particular, tabled execution of logic programs terminates
more often than execution based on SLD-resolution. In this article, we
introduce two notions of universal termination of logic programming with
Tabling: quasi-termination and (the stronger notion of) LG-termination. We
present sufficient conditions for these two notions of termination, namely
quasi-acceptability and LG-acceptability, and we show that these conditions are
also necessary in case the tabling is well-chosen. Starting from these
conditions, we give modular termination proofs, i.e., proofs capable of
combining termination proofs of separate programs to obtain termination proofs
of combined programs. Finally, in the presence of mode information, we state
sufficient conditions which form the basis for automatically proving
termination in a constraint-based way."
"Proving failure of queries for definite logic programs can be done by
constructing a finite model of the program in which the query is false. A
general purpose model generator for first order logic can be used for this. A
recent paper presented at PLILP98 shows how the peculiarities of definite
programs can be exploited to obtain a better solution. There a procedure is
described which combines abduction with tabulation and uses a meta-interpreter
for heuristic control of the search. The current paper shows how similar
results can be obtained by direct execution under the standard tabulation of
the XSB-Prolog system. The loss of control is compensated for by better
intelligent backtracking and more accurate failure analysis."
"We introduce a set of eight universal Rules of Inference by which computer
programs with known properties (axioms) are transformed into new programs with
known properties (theorems). Axioms are presented to formalize a segment of
Number Theory, DataBase retrieval and Computability Theory. The resulting
Program Calculus is used to generate programs to (1) Determine if one number is
a factor of another. (2) List all employees who earn more than their manager.
(3) List the set of programs that halt no on themselves, thus proving that it
is recursively enumerable. The well-known fact that the set of programs that do
not halt yes on themselves is not recursively enumerable is formalized as a
program requirement that has no solution, an Incompleteness Axiom. Thus, any
axioms (programs) which could be used to generate this program are themselves
unattainable. Such proofs are presented to formally generate several additional
theorems, including (4) The halting problem is unsolvable.
  Open problems and future research is discussed, including the use of
temporary sort files, programs that calculate statistics (such as counts and
sums), the synthesis of programs to solve other well-known problems from Number
Theory, Logic, DataBase retrieval and Computability Theory, application to
Programming Language Semantics, and the formalization of incompleteness results
from Logic and the semantic paradoxes."
"This paper is a structured introduction to Light Affine Logic, and to its
intuitionistic fragment. Light Affine Logic has a polynomially costing cut
elimination (P-Time correctness), and encodes all P-Time Turing machines
(P-Time completeness). P-Time correctness is proved by introducing the Proof
nets for Intuitionistic Light Affine Logic. P-Time completeness is demonstrated
in full details thanks to a very compact program notation. On one side, the
proof of P-Time correctness describes how the complexity of cut elimination is
controlled, thanks to a suitable cut elimination strategy that exploits
structural properties of the Proof nets. This allows to have a good catch on
the meaning of the ``paragraph'' modality, which is a peculiarity of light
logics. On the other side, the proof of P-Time completeness, together with a
lot of programming examples, gives a flavor of the non trivial task of
programming with resource limitations, using Intuitionistic Light Affine Logic
derivations as programs."
"In existing simulation proof techniques, a single step in a lower-level
specification may be simulated by an extended execution fragment in a
higher-level one. As a result, it is cumbersome to mechanize these techniques
using general purpose theorem provers. Moreover, it is undecidable whether a
given relation is a simulation, even if tautology checking is decidable for the
underlying specification logic. This paper introduces various types of normed
simulations. In a normed simulation, each step in a lower-level specification
can be simulated by at most one step in the higher-level one, for any related
pair of states. In earlier work we demonstrated that normed simulations are
quite useful as a vehicle for the formalization of refinement proofs via
theorem provers. Here we show that normed simulations also have pleasant
theoretical properties: (1) under some reasonable assumptions, it is decidable
whether a given relation is a normed forward simulation, provided tautology
checking is decidable for the underlying logic; (2) at the semantic level,
normed forward and backward simulations together form a complete proof method
for establishing behavior inclusion, provided that the higher-level
specification has finite invisible nondeterminism."
"We study the topological models of a logic of knowledge for topological
reasoning, introduced by Larry Moss and Rohit Parikh. Among our results is a
solution of a conjecture by the formentioned authors, finite satisfiability
property and decidability for the theory of topological models."
"We consider a variant of the Boolean satisfiability problem where a subset E
of the propositional variables appearing in formula Fsat encode a symmetric,
transitive, binary relation over N elements. Each of these relational
variables, e[i,j], for 1 <= i < j <= N, expresses whether or not the relation
holds between elements i and j. The task is to either find a satisfying
assignment to Fsat that also satisfies all transitivity constraints over the
relational variables (e.g., e[1,2] & e[2,3] ==> e[1,3]), or to prove that no
such assignment exists. Solving this satisfiability problem is the final and
most difficult step in our decision procedure for a logic of equality with
uninterpreted functions. This procedure forms the core of our tool for
verifying pipelined microprocessors.
  To use a conventional Boolean satisfiability checker, we augment the set of
clauses expressing Fsat with clauses expressing the transitivity constraints.
We consider methods to reduce the number of such clauses based on the sparse
structure of the relational variables.
  To use Ordered Binary Decision Diagrams (OBDDs), we show that for some sets
E, the OBDD representation of the transitivity constraints has exponential size
for all possible variable orderings. By considering only those relational
variables that occur in the OBDD representation of Fsat, our experiments show
that we can readily construct an OBDD representation of the relevant
transitivity constraints and thus solve the constrained satisfiability problem."
"We consider a general prescriptive type system with parametric polymorphism
and subtyping for logic programs. The property of subject reduction expresses
the consistency of the type system w.r.t. the execution model: if a program is
""well-typed"", then all derivations starting in a ""well-typed"" goal are again
""well-typed"". It is well-established that without subtyping, this property is
readily obtained for logic programs w.r.t. their standard (untyped) execution
model. Here we give syntactic conditions that ensure subject reduction also in
the presence of general subtyping relations between type constructors. The idea
is to consider logic programs with a fixed dataflow, given by modes."
"We present a framework for expressing bottom-up algorithms to compute the
well-founded model of non-disjunctive logic programs. Our method is based on
the notion of conditional facts and elementary program transformations studied
by Brass and Dix for disjunctive programs. However, even if we restrict their
framework to nondisjunctive programs, their residual program can grow to
exponential size, whereas for function-free programs our program remainder is
always polynomial in the size of the extensional database (EDB).
  We show that particular orderings of our transformations (we call them
strategies) correspond to well-known computational methods like the alternating
fixpoint approach, the well-founded magic sets method and the magic alternating
fixpoint procedure. However, due to the confluence of our calculi, we come up
with computations of the well-founded model that are provably better than these
methods.
  In contrast to other approaches, our transformation method treats magic set
transformed programs correctly, i.e. it always computes a relevant part of the
well-founded model of the original program."
"A syntactical proof is given that all functions definable in a certain affine
linear typed lambda-calculus with iteration in all types are polynomial time
computable. The proof provides explicit polynomial bounds that can easily be
calculated."
"We characterize those intersection-type theories which yield complete
intersection-type assignment systems for lambda-calculi, with respect to the
three canonical set-theoretical semantics for intersection-types: the inference
semantics, the simple semantics and the F-semantics. These semantics arise by
taking as interpretation of types subsets of applicative structures, as
interpretation of the intersection constructor set-theoretic inclusion, and by
taking the interpretation of the arrow constructor a' la Scott, with respect to
either any possible functionality set, or the largest one, or the least one.
  These results strengthen and generalize significantly all earlier results in
the literature, to our knowledge, in at least three respects. First of all the
inference semantics had not been considered before. Secondly, the
characterizations are all given just in terms of simple closure conditions on
the preorder relation on the types, rather than on the typing judgments
themselves. The task of checking the condition is made therefore considerably
more tractable. Lastly, we do not restrict attention just to lambda-models, but
to arbitrary applicative structures which admit an interpretation function.
Thus we allow also for the treatment of models of restricted lambda-calculi.
Nevertheless the characterizations we give can be tailored just to the case of
lambda-models."
"We consider prescriptive type systems for logic programs (as in Goedel or
Mercury). In such systems, the typing is static, but it guarantees an
operational property: if a program is ""well-typed"", then all derivations
starting in a ""well-typed"" query are again ""well-typed"". This property has been
called subject reduction. We show that this property can also be phrased as a
property of the proof-theoretic semantics of logic programs, thus abstracting
from the usual operational (top-down) semantics. This proof-theoretic view
leads us to questioning a condition which is usually considered necessary for
subject reduction, namely the head condition. It states that the head of each
clause must have a type which is a variant (and not a proper instance) of the
declared type. We provide a more general condition, thus reestablishing a
certain symmetry between heads and body atoms. The condition ensures that in a
derivation, the types of two unified terms are themselves unifiable. We discuss
possible implications of this result. We also discuss the relationship between
the head condition and polymorphic recursion, a concept known in functional
programming."
"We consider the problem of searching for proofs in sequential presentations
of logics with multiplicative (or intensional) connectives. Specifically, we
start with the multiplicative fragment of linear logic and extend, on the one
hand, to linear logic with its additives and, on the other, to the additives of
the logic of bunched implications, BI. We give an algebraic method for
calculating the distribution of the side-formulae in multiplicative rules which
allows the occurrence or non-occurrence of a formula on a branch of a proof to
be determined once sufficient information is available. Each formula in the
conclusion of such a rule is assigned a Boolean expression. As a search
proceeds, a set of Boolean constraint equations is generated. We show that a
solution to such a set of equations determines a proof corresponding to the
given search. We explain a range of strategies, from the lazy to the eager, for
solving sets of constraint equations. We indicate how to apply our methods
systematically to large family of relevant systems."
"We define five increasingly comprehensive classes of infinite-state systems,
called STS1--5, whose state spaces have finitary structure. For four of these
classes, we provide examples from hybrid systems."
"This paper is motivated by the fact that verifying liveness properties under
a fairness condition is often problematic, especially when abstraction is used.
It shows that using a more abstract notion than truth under fairness,
specifically the concept of a property being satisfied within fairness can lead
to interesting possibilities. Technically, it is first established that
deciding satisfaction within fairness is a PSPACE-complete problem and it is
shown that properties satisfied within fairness can always be satisfied by some
fair implementation. Thereafter, the interaction between behavior abstraction
and satisfaction within fairness is studied and it is proved that satisfaction
of properties within fairness can be verified on behavior abstractions, if the
abstraction homomorphism is weakly continuation-closed."
"This paper explores goal-directed proof search in first-order multi-modal
logic. The key issue is to design a proof system that respects the modularity
and locality of assumptions of many modal logics. By forcing ambiguities to be
considered independently, modular disjunctions in particular can be used to
construct efficiently executable specifications in reasoning tasks involving
partial information that otherwise might require prohibitive search. To achieve
this behavior requires prior proof-theoretic justifications of logic
programming to be extended, strengthened, and combined with proof-theoretic
analyses of modal deduction in a novel way."
"Analysis of (partial) groundness is an important application of abstract
interpretation. There are several proposals for improving the precision of such
an analysis by exploiting type information, icluding our own work with Hill and
King, where we had shown how the information present in the type declarations
of a program can be used to characterise the degree of instantiation of a term
in a precise and yet inherently finite way. This approach worked for
polymorphically typed programs as in Goedel or HAL. Here, we recast this
approach following works by Codish, Lagoon and Stuckey. To formalise which
properties of terms we want to characterise, we use labelling functions, which
are functions that extract subterms from a term along certain paths. An
abstract term collects the results of all labelling functions of a term. For
the analysis, programs are executed on abstract terms instead of the concrete
ones, and usual unification is replaced by unification modulo an equality
theory which includes the well-known ACI-theory. Thus we generalise the works
by Codish, Lagoon and Stuckey w.r.t. the type systems considered and relate the
works among each other."
"A model of computation is abstract if, when applied to any algebra, the
resulting programs for computable functions and sets on that algebra are
invariant under isomorphisms, and hence do not depend on a representation for
the algebra. Otherwise it is concrete. Intuitively, concrete models depend on
the implementation of the algebra.
  The difference is particularly striking in the case of topological partial
algebras, and notably in algebras over the reals. We investigate the
relationship between abstract and concrete models of partial metric algebras.
In the course of this investigation, interesting aspects of continuity,
extensionality and non-determinism are uncovered."
"computable functions are defined by abstract finite deterministic algorithms
on many-sorted algebras. We show that there exist finite universal algebraic
specifications that specify uniquely (up to isomorphism) (i) all abstract
computable functions on any many-sorted algebra; and (ii) all functions
effectively approximable by abstract computable functions on any metric
algebra.
  We show that there exist universal algebraic specifications for all the
classically computable functions on the set R of real numbers. The algebraic
specifications used are mainly bounded universal equations and conditional
equations. We investigate the initial algebra semantics of these
specifications, and derive situations where algebraic specifications define
precisely the computable functions."
"We consider pushdown timed automata (PTAs) that are timed automata (with
dense clocks) augmented with a pushdown stack. A configuration of a PTA
includes a control state, dense clock values and a stack word. By using the
pattern technique, we give a decidable characterization of the binary
reachability (i.e., the set of all pairs of configurations such that one can
reach the other) of a PTA. Since a timed automaton can be treated as a PTA
without the pushdown stack, we can show that the binary reachability of a timed
automaton is definable in the additive theory of reals and integers. The
results can be used to verify a class of properties containing linear relations
over both dense variables and unbounded discrete variables. The properties
previously could not be verified using the classic region technique nor
expressed by timed temporal logics for timed automata and CTL$^*$ for pushdown
systems. The results are also extended to other generalizations of timed
automata."
"Decidability of definitional equality and conversion of terms into canonical
form play a central role in the meta-theory of a type-theoretic logical
framework. Most studies of definitional equality are based on a confluent,
strongly-normalizing notion of reduction. Coquand has considered a different
approach, directly proving the correctness of a practical equivalance algorithm
based on the shape of terms. Neither approach appears to scale well to richer
languages with unit types or subtyping, and neither directly addresses the
problem of conversion to canonical.
  In this paper we present a new, type-directed equivalence algorithm for the
LF type theory that overcomes the weaknesses of previous approaches. The
algorithm is practical, scales to richer languages, and yields a new notion of
canonical form sufficient for adequate encodings of logical systems. The
algorithm is proved complete by a Kripke-style logical relations argument
similar to that suggested by Coquand. Crucially, both the algorithm itself and
the logical relations rely only on the shapes of types, ignoring dependencies
on terms."
"The paper is concerned with defining the electrical signals and their models.
The delays are discussed, the asynchronous automata - which are the models of
the asynchronous circuits - and the examples of the clock generator and of the
R-S latch are given. We write the equations of the asynchronous automata, which
combine the pure delay model and the inertial delay model; the simple gate
model and the complex gate model; the fixed, bounded and unbounded delay model.
We give the solutions of these equations, which are written on R->{0,1}
functions, where R is the time set. The connection between the real time and
the discrete time is discussed. The stability, the fundamental mode of
operation, the combinational automata, the semi-modularity are defined and
characterized. Some connections are suggested with the linear time and the
branching time temporal logic of the propositions."
"The paper presents the differential equations that characterize an
asynchronous automaton and gives their solution x:R->{0,1}x...x{0,1}. Remarks
are made on the connection between the continuous time and the discrete time of
the approach. The continuous and the discrete time, the linear and the
branching temporal logics have the semantics depending on x and their formulas
give the properties of the automaton."
"The paper studies some important properties of the asynchronous (=timed)
automata: the delay-insensitivity, the hazard-freedom, the semi-modularity and
the technical condition of good running. Time is discrete."
"We show that it is decidable whether a transitive mixed linear relation has
an $\omega$-chain. Using this result, we study a number of liveness
verification problems for generalized timed automata within a unified
framework. More precisely, we prove that (1) the mixed linear liveness problem
for a timed automaton with dense clocks, reversal-bounded counters, and a free
counter is decidable, and (2) the Presburger liveness problem for a timed
automaton with discrete clocks, reversal-bounded counters, and a pushdown stack
is decidable."
"We write the relations that characterize the simpliest timed automaton, the
inertial delay buffer, in two versions: the non-deterministic and the
deterministic one, by making use of the derivatives of the R->{0,1} functions."
"Abduction, first proposed in the setting of classical logics, has been
studied with growing interest in the logic programming area during the last
years.
  In this paper we study {\em abduction with penalization} in logic
programming. This form of abductive reasoning, which has not been previously
analyzed in logic programming, turns out to represent several relevant
problems, including optimization problems, very naturally. We define a formal
model for abduction with penalization from logic programs, which extends the
abductive framework proposed by Kakas and Mancarella. We show the high
expressiveness of this formalism, by encoding a couple of relevant problems,
including the well-know Traveling Salesman Problem from optimization theory, in
this abductive framework. The resulting encodings are very simple and elegant.
We analyze the complexity of the main decisional problems arising in this
framework. An interesting result in this course is that ``negation comes for
free.'' Indeed, the addition of (even unstratified) negation does not cause any
further increase to the complexity of the abductive reasoning tasks (which
remains the same as for not-free programs)."
"This paper describes the semantics and ideas about SKY, a logic programming
language intended in order to specify algorithmic strategies for the evaluation
of problems."
"We address the problem of integrating information coming from different
sources. The information consists of facts that a central server collects and
tries to combine using (a) a set of logical rules, i.e. a logic program, and
(b) a hypothesis representing the server's own estimates. In such a setting
incomplete information from a source or contradictory information from
different sources necessitate the use of many-valued logics in which programs
can be evaluated and hypotheses can be tested. To carry out such activities we
propose a formal framework based on bilattices such as Belnap's four-valued
logics. In this framework we work with the class of programs defined by Fitting
and we develop a theory for information integration.
  We also establish an intuitively appealing connection between our hypothesis
testing mechanism on the one hand, and the well-founded semantics and
Kripke-Kleene semantics of Datalog programs with negation, on the other hand."
"This paper explores the connection between semantic equivalences and
preorders for concrete sequential processes, represented by means of labelled
transition systems, and formats of transition system specifications using
Plotkin's structural approach. For several preorders in the linear time -
branching time spectrum a format is given, as general as possible, such that
this preorder is a precongruence for all operators specifiable in that format.
The formats are derived using the modal characterizations of the corresponding
preorders."
"In this paper we demonstrate that the class of basic feasible functionals has
recursion theoretic properties which naturally generalize the corresponding
properties of the class of feasible functions. We also improve the Kapron -
Cook result on mashine representation of basic feasible functionals. Our proofs
are based on essential applications of logic. We introduce a weak fragment of
second order arithmetic with second order variables ranging over functions from
N into N which suitably characterizes basic feasible functionals, and show that
it is a useful tool for investigating the properties of basic feasible
functionals. In particular, we provide an example how one can extract feasible
""programs"" from mathematical proofs which use non-feasible functionals (like
second order polynomials)."
"The four authors present their speculations about the future developments of
mathematical logic in the twenty-first century. The areas of recursion theory,
proof theory and logic for computer science, model theory, and set theory are
discussed independently."
"The non-classical, nonmonotonic inference relation associated with the answer
set semantics for logic programs gives rise to a relationship of 'strong
equivalence' between logical programs that can be verified in 3-valued Goedel
logic, G3, the strongest non-classical intermediate propositional logic
(Lifschitz, Pearce and Valverde, 2001). In this paper we will show that KC (the
logic obtained by adding axiom ~A v ~~A to intuitionistic logic), is the
weakest intermediate logic for which strongly equivalent logic programs, in a
language allowing negations, are logically equivalent."
"We show the NP-completeness of the existential theory of term algebras with
the Knuth-Bendix order by giving a nondeterministic polynomial-time algorithm
for solving Knuth-Bendix ordering constraints."
"The goal of computational logic is to allow us to model computation as well
as to reason about it. We argue that a computational logic must be able to
model interactive computation. We show that first-order logic cannot model
interactive computation due to the incompleteness of interaction. We show that
interactive computation is necessarily paraconsistent, able to model both a
fact and its negation, due to the role of the world (environment) in
determining the course of the computation. We conclude that paraconsistency is
a necessary property for a logic that can model interactive computation."
"Defeasible logic is an efficient logic for defeasible reasoning. It is
defined through a proof theory and, until now, has had no model theory. In this
paper a model-theoretic semantics is given for defeasible logic. The logic is
sound and complete with respect to the semantics. We also briefly outline how
this approach extends to a wide range of defeasible logics."
"This paper studies axioms for nonmonotonic consequences from a
semantics-based point of view, focusing on a class of mathematical structures
for reasoning about partial information without a predefined syntax/logic. This
structure is called a default structure. We study axioms for the nonmonotonic
consequence relation derived from extensions as in Reiter's default logic,
using skeptical reasoning, but extensions are now used for the construction of
possible worlds in a default information structure.
  In previous work we showed that skeptical reasoning arising from
default-extensions obeys a well-behaved set of axioms including the axiom of
cautious cut. We show here that, remarkably, the converse is also true: any
consequence relation obeying this set of axioms can be represented as one
constructed from skeptical reasoning. We provide representation theorems to
relate axioms for nonmonotonic consequence relation and properties about
extensions, and provide one-to-one correspondence between nonmonotonic systems
which satisfies the law of cautious monotony and default structures with unique
extensions. Our results give a theoretical justification for a set of basic
rules governing the update of nonmonotonic knowledge bases, demonstrating the
derivation of them from the more concrete and primitive construction of
extensions. It is also striking to note that proofs of the representation
theorems show that only shallow extensions are necessary, in the sense that the
number of iterations needed to achieve an extension is at most three. All of
these developments are made possible by taking a more liberal view of
consistency: consistency is a user defined predicate, satisfying some basic
properties."
"This paper concerns a goal directed proof procedure for the propositional
fragment of the adaptive logic ACLuN1. At the propositional level, it forms an
algorithm for final derivability. If extended to the predicative level, it
provides a criterion for final derivability. This is essential in view of the
absence of a positive test. The procedure may be generalized to all flat
adaptive logics."
"The model theory of a first-order logic called N^4 is introduced. N^4 does
not eliminate double negations, as classical logic does, but instead reduces
fourfold negations. N^4 is very close to classical logic: N^4 has two truth
values; implications in N^4 are material, like in classical logic; and negation
distributes over compound formulas in N^4 as it does in classical logic.
Results suggest that the semantics of normal logic programs is conveniently
formalized in N^4: Classical logic Herbrand interpretations generalize
straightforwardly to N^4; the classical minimal Herbrand model of a positive
logic program coincides with its unique minimal N^4 Herbrand model; the stable
models of a normal logic program and its so-called complete minimal N^4
Herbrand models coincide."
"We provide a denotational semantics for first-order logic that captures the
two-level view of the computation process typical for constraint programming.
At one level we have the usual program execution. At the other level an
automatic maintenance of the constraint store takes place. We prove that the
resulting semantics is sound with respect to the truth definition. By
instantiating it by specific forms of constraint management policies we obtain
several sound evaluation policies of first-order formulas. This semantics can
also be used a basis for sound implementation of constraint maintenance in
presence of block declarations and conditionals."
"A fragment of second-order lambda calculus (System F) is defined that
characterizes the elementary recursive functions. Type quantification is
restricted to be non-interleaved and stratified, i.e., the types are assigned
levels, and a quantified variable can only be instantiated by a type of smaller
level, with a slightly liberalized treatment of the level zero."
"We illustrate the use of intersection types as a semantic tool for showing
properties of the lattice of lambda theories. Relying on the notion of easy
intersection type theory we successfully build a filter model in which the
interpretation of an arbitrary simple easy term is any filter which can be
described in an uniform way by a predicate. This allows us to prove the
consistency of a well-know lambda theory: this consistency has interesting
consequences on the algebraic structure of the lattice of lambda theories."
"We present two embeddings of infinite-valued Lukasiewicz logic L into Meyer
and Slaney's abelian logic A, the logic of lattice-ordered abelian groups. We
give new analytic proof systems for A and use the embeddings to derive
corresponding systems for L. These include: hypersequent calculi for A and L
and terminating versions of these calculi; labelled single sequent calculi for
A and L of complexity co-NP; unlabelled single sequent calculi for A and L."
"This paper gives a thorough overview of what is known about first-order logic
with counting quantifiers and with arithmetic predicates. As a main theorem we
show that Presburger arithmetic is closed under unary counting quantifiers.
Precisely, this means that for every first-order formula phi(y,z_1,...,z_k)
over the signature {<,+} there is a first-order formula psi(x,z_1,...,z_k)
which expresses over the structure <Nat,<,+> (respectively, over initial
segments of this structure) that the variable x is interpreted exactly by the
number of possible interpretations of the variable y for which the formula
phi(y,z_1,...,z_k) is satisfied. Applying this theorem, we obtain an easy proof
of Ruhl's result that reachability (and similarly, connectivity) in finite
graphs is not expressible in first-order logic with unary counting quantifiers
and addition. Furthermore, the above result on Presburger arithmetic helps to
show the failure of a particular version of the Crane Beach conjecture."
"We define a sound and complete proof system for affine beta-eta-retractions
in simple types built over many atoms, and we state simple necessary conditions
for arbitrary beta-eta-retractions in simple and polymorphic types."
"Since the seminal work of J. A. Robinson on resolution, many lifting lemmas
for simplifying proofs of completeness of resolution have been proposed in the
literature. In the logic programming framework, they may also help to detect
some infinite derivations while proving goals under the SLD-resolution. In this
paper, we first generalize a version of the lifting lemma, by extending the
relation ""is more general than"" so that it takes into account only some
arguments of the atoms. The other arguments, which we call neutral arguments,
are disregarded. Then we propose two syntactic conditions of increasing power
for identifying neutral arguments from mere inspection of the text of a logic
program."
"This article answers two questions (posed in the literature), each concerning
the guaranteed existence of proofs free of double negation. A proof is free of
double negation if none of its deduced steps contains a term of the form
n(n(t)) for some term t, where n denotes negation. The first question asks for
conditions on the hypotheses that, if satisfied, guarantee the existence of a
double-negation-free proof when the conclusion is free of double negation. The
second question asks about the existence of an axiom system for classical
propositional calculus whose use, for theorems with a conclusion free of double
negation, guarantees the existence of a double-negation-free proof. After
giving conditions that answer the first question, we answer the second question
by focusing on the Lukasiewicz three-axiom system. We then extend our studies
to infinite-valued sentential calculus and to intuitionistic logic and
generalize the notion of being double-negation free. The double-negation proofs
of interest rely exclusively on the inference rule condensed detachment, a rule
that combines modus ponens with an appropriately general rule of substitution.
The automated reasoning program OTTER played an indispensable role in this
study."
"This paper considers finite-automata based algorithms for handling linear
arithmetic with both real and integer variables. Previous work has shown that
this theory can be dealt with by using finite automata on infinite words, but
this involves some difficult and delicate to implement algorithms. The
contribution of this paper is to show, using topological arguments, that only a
restricted class of automata on infinite words are necessary for handling real
and integer linear arithmetic. This allows the use of substantially simpler
algorithms, which have been successfully implemented."
"We explore how different proof orderings induce different notions of
saturation. We relate completion, paramodulation, saturation, redundancy
elimination, and rewrite system reduction to proof orderings."
"A large number of different model checking approaches has been proposed
during the last decade. The different approaches are applicable to different
model types including untimed, timed, probabilistic and stochastic models. This
paper presents a new framework for model checking techniques which includes
some of the known approaches, but enlarges the class of models for which model
checking can be applied to the general class of weighted automata. The approach
allows an easy adaption of model checking to models which have not been
considered yet for this purpose. Examples for those new model types for which
model checking can be applied are max/plus or min/plus automata which are well
established models to describe different forms of dynamic systems and
optimization problems. In this context, model checking can be used to verify
temporal or quantitative properties of a system. The paper first presents
briefly our class of weighted automata, as a very general model type. Then
Valued Computational Tree Logic (CTL$) is introduced as a natural extension of
the well known branching time logic CTL. Afterwards, algorithms to check a
weighted automaton according to a CTL$ formula are presented. As a last result,
a bisimulation is presented for weighted automata and for CTL$."
"We introduce a temporal logic to reason on global applications in an
asynchronous setting. First, we define the Distributed States Logic (DSL), a
modal logic for localities that embeds the local theories of each component
into a theory of the distributed states of the system. We provide the logic
with a sound and complete axiomatization. The contribution is that it is
possible to reason about properties that involve several components, even in
the absence of a global clock. Then, we define the Distributed States Temporal
Logic (DSTL) by introducing temporal operators a' la Unity. We support our
proposal by working out a pair of examples: a simple secure communication
system, and an algorithm for distributed leader election.
  The motivation for this work is that the existing logics for distributed
systems do not have the right expressive power to reason on the systems
behaviour, when the communication is based on asynchronous message passing. On
the other side, asynchronous communication is the most used abstraction when
modelling global applications."
"A method is presented for computing minimal answers in disjunctive deductive
databases under the disjunctive stable model semantics. Such answers are
constructed by repeatedly extending partial answers. Our method is complete (in
that every minimal answer can be computed) and does not admit redundancy (in
the sense that every partial answer generated can be extended to a minimal
answer), whence no non-minimal answer is generated. For stratified databases,
the method does not (necessarily) require the computation of models of the
database in their entirety. Compilation is proposed as a tool by which problems
relating to computational efficiency and the non-existence of disjunctive
stable models can be overcome. The extension of our method to other semantics
is also considered."
"We present a type inference algorithm for lambda-terms in Elementary Affine
Logic using linear constraints. We prove that the algorithm is correct and
complete."
"We present some applications of intermediate logics in the field of Answer
Set Programming (ASP). A brief, but comprehensive introduction to the answer
set semantics, intuitionistic and other intermediate logics is given. Some
equivalence notions and their applications are discussed. Some results on
intermediate logics are shown, and applied later to prove properties of answer
sets. A characterization of answer sets for logic programs with nested
expressions is provided in terms of intuitionistic provability, generalizing a
recent result given by Pearce.
  It is known that the answer set semantics for logic programs with nested
expressions may select non-minimal models. Minimal models can be very important
in some applications, therefore we studied them; in particular we obtain a
characterization, in terms of intuitionistic logic, of answer sets which are
also minimal models. We show that the logic G3 characterizes the notion of
strong equivalence between programs under the semantic induced by these models.
Finally we discuss possible applications and consequences of our results. They
clearly state interesting links between ASP and intermediate logics, which
might bring research in these two areas together."
"Until recently, First-Order Temporal Logic (FOTL) has been little understood.
While it is well known that the full logic has no finite axiomatisation, a more
detailed analysis of fragments of the logic was not previously available.
However, a breakthrough by Hodkinson et.al., identifying a finitely
axiomatisable fragment, termed the monodic fragment, has led to improved
understanding of FOTL. Yet, in order to utilise these theoretical advances, it
is important to have appropriate proof techniques for the monodic fragment.
  In this paper, we modify and extend the clausal temporal resolution
technique, originally developed for propositional temporal logics, to enable
its use in such monodic fragments. We develop a specific normal form for
formulae in FOTL, and provide a complete resolution calculus for formulae in
this form. Not only is this clausal resolution technique useful as a practical
proof technique for certain monodic classes, but the use of this approach
provides us with increased understanding of the monodic fragment. In
particular, we here show how several features of monodic FOTL are established
as corollaries of the completeness result for the clausal temporal resolution
method. These include definitions of new decidable monodic classes,
simplification of existing monodic classes by reductions, and completeness of
clausal temporal resolution in the case of monodic logics with expanding
domains, a case with much significance in both theory and practice."
"We provide a simple translation of the satisfiability problem for regular
grammar logics with converse into GF2, which is the intersection of the guarded
fragment and the 2-variable fragment of first-order logic. This translation is
theoretically interesting because it translates modal logics with certain frame
conditions into first-order logic, without explicitly expressing the frame
conditions.
  A consequence of the translation is that the general satisfiability problem
for regular grammar logics with converse is in EXPTIME. This extends a previous
result of the first author for grammar logics without converse. Using the same
method, we show how some other modal logics can be naturally translated into
GF2, including nominal tense logics and intuitionistic logic.
  In our view, the results in this paper show that the natural first-order
fragment corresponding to regular grammar logics is simply GF2 without extra
machinery such as fixed point-operators."
"Every endofunctor of the category of classes is proved to be set-based in the
sense of Aczel and Mendler, therefore, it has a final coalgebra. Other basic
properties of these endofunctors are proved, e.g. the existence of a free
completely iterative theory."
"In this paper we adapt the definitions and results from Apt and Vermeulen on
`First order logic as a constraint programming language' (in: Proceedings of
LPAR2001, Baaz and Voronkov (eds.), Springer LNAI 2514) to include important
ideas about search and choice into the system. We give motivating examples.
Then we set up denotational semantics for first order logic as follows: the
semantic universe includes states that consist of two components: a
substitution, which can be seen as the computed answer; and a constraint
satisfaction problem, which can be seen as the residue of the original problem,
yet to be handled by constraint programming. The interaction between these
components is regulated by an operator called: infer. In this paper we regard
infer as an operator on sets of states to enable us to analyze ideas about
search among states and choice between states.
  The precise adaptations of definitions and results are able to deal with the
examples and we show that, given several reasonable conditions, the new
definitions ensure soundness of the system with respect to the standard
interpretation of first order logic. In this way the `reasonable conditions'
can be read as conditions for sound search.
  We indicate briefly how to investigate efficiency of search in future
research."
"Description Logics are knowledge representation formalisms which have been
used in a wide range of application domains. Owing to their appealing
expressiveness, we consider in this paper extensions of the well-known concept
language ALC allowing for number restrictions on complex role expressions.
These have been first introduced by Baader and Sattler as ALCN(M) languages,
with the adoption of role constructors M subset-of {o,-,And,Or}. In particular,
they showed in 1999 that, although ALCN(o) is decidable, the addition of other
operators may easily lead to undecidability: in fact, ALCN(o,And) and
ALCN(o,-,Or) were proved undecidable.
  In this work, we further investigate the computational properties of the ALCN
family, aiming at narrowing the decidability gap left open by Baader and
Sattler's results. In particular, we will show that ALCN(o) extended with
inverse roles both in number and in value restrictions becomes undecidable,
whereas it can be safely extended with qualified number restrictions without
losing decidability."
"A formal model of the structure of information is presented in five axioms
which define identity, containment, and joins of infons. Joins are shown to be
commutative, associative, provide inverses of infons, and, potentially, have
many identity elements, two of which are multiplicative and additive. Those two
types of join are distributive. The other identity elements are for operators
on entwined states. Multiplicative joins correspond to adding or removing new
bits to a system while additive joins correspond to a change of state. The
order or size of an infon is defined. This groundwork is intended to be used to
model continuous and discreet information structures through time, especially
in closed systems."
"We study self-referential sentences of the type related to the Liar paradox.
In particular, we consider the problem of assigning consistent fuzzy truth
values to collections of self-referential sentences. We show that the problem
can be reduced to the solution of a system of nonlinear equations. Furthermore,
we prove that, under mild conditions, such a system always has a solution (i.e.
a consistent truth value assignment) and that, for a particular implementation
of logical ``and'', ``or'' and ``negation'', the ``mid-point'' solution is
always consistent. Next we turn to computational issues and present several
truth-value assignment algorithms; we argue that these algorithms can be
understood as generalized sequential reasoning. In an Appendix we present a
large number of examples of self-referential collections (including the Liar
and the Strengthened Liar), we formulate the corresponding truth value
equations and solve them analytically and/ or numerically."
"We propose Kleene algebra with domain (KAD), an extension of Kleene algebra
with two equational axioms for a domain and a codomain operation, respectively.
KAD considerably augments the expressiveness of Kleene algebra, in particular
for the specification and analysis of state transition systems. We develop the
basic calculus, discuss some related theories and present the most important
models of KAD. We demonstrate applicability by two examples: First, an
algebraic reconstruction of Noethericity and well-foundedness; second, an
algebraic reconstruction of propositional Hoare logic."
"We investigate mca-programs, that is, logic programs with clauses built of
monotone cardinality atoms of the form kX, where k is a non-negative integer
and X is a finite set of propositional atoms. We develop a theory of
mca-programs. We demonstrate that the operational concept of the one-step
provability operator generalizes to mca-programs, but the generalization
involves nondeterminism. Our main results show that the formalism of
mca-programs is a common generalization of (1) normal logic programming with
its semantics of models, supported models and stable models, (2) logic
programming with cardinality atoms and with the semantics of stable models, as
defined by Niemela, Simons and Soininen, and (3) of disjunctive logic
programming with the possible-model semantics of Sakama and Inoue."
"In this paper we bring together the areas of combinatorics and propositional
satisfiability. Many combinatorial theorems establish, often constructively,
the existence of positive integer functions, without actually providing their
closed algebraic form or tight lower and upper bounds. The area of Ramsey
theory is especially rich in such results. Using the problem of computing van
der Waerden numbers as an example, we show that these problems can be
represented by parameterized propositional theories in such a way that
decisions concerning their satisfiability determine the numbers (function) in
question. We show that by using general-purpose complete and local-search
techniques for testing propositional satisfiability, this approach becomes
effective -- competitive with specialized approaches. By following it, we were
able to obtain several new results pertaining to the problem of computing van
der Waerden numbers. We also note that due to their properties, especially
their structural simplicity and computational hardness, propositional theories
that arise in this research can be of use in development, testing and
benchmarking of SAT solvers."
"Suitable extensions of the monadic second-order theory of k successors have
been proposed in the literature to capture the notion of time granularity. In
this paper, we provide the monadic second-order theories of downward unbounded
layered structures, which are infinitely refinable structures consisting of a
coarsest domain and an infinite number of finer and finer domains, and of
upward unbounded layered structures, which consist of a finest domain and an
infinite number of coarser and coarser domains, with expressively complete and
elementarily decidable temporal logic counterparts.
  We obtain such a result in two steps. First, we define a new class of
combined automata, called temporalized automata, which can be proved to be the
automata-theoretic counterpart of temporalized logics, and show that relevant
properties, such as closure under Boolean operations, decidability, and
expressive equivalence with respect to temporal logics, transfer from component
automata to temporalized ones. Then, we exploit the correspondence between
temporalized logics and automata to reduce the task of finding the temporal
logic counterparts of the given theories of time granularity to the easier one
of finding temporalized automata counterparts of them."
"An abstract architecture for idealized multi-agent systems whose behaviour is
regulated by normative systems is developed and discussed. Agent choices are
determined partially by the preference ordering of possible states and
partially by normative considerations: The agent chooses that act which leads
to the best outcome of all permissible actions. If an action is non-permissible
depends on if the result of performing that action leads to a state satisfying
a condition which is forbidden, according to the norms regulating the
multi-agent system. This idea is formalized by defining set-theoretic
predicates characterizing multi-agent systems. The definition of the predicate
uses decision theory, the Kanger-Lindahl theory of normative positions, and an
algebraic representation of normative systems."
"Full first order linear logic can be presented as an abstract logic
programming language in Miller's system Forum, which yields a sensible
operational interpretation in the 'proof search as computation' paradigm.
However, Forum still has to deal with syntactic details that would normally be
ignored by a reasonable operational semantics. In this respect, Forum improves
on Gentzen systems for linear logic by restricting the language and the form of
inference rules. We further improve on Forum by restricting the class of
formulae allowed, in a system we call G-Forum, which is still equivalent to
full first order linear logic. The only formulae allowed in G-Forum have the
same shape as Forum sequents: the restriction does not diminish expressiveness
and makes G-Forum amenable to proof theoretic analysis. G-Forum consists of two
(big) inference rules, for which we show a cut elimination procedure. This does
not need to appeal to finer detail in formulae and sequents than is provided by
G-Forum, thus successfully testing the internal symmetries of our system."
"A construction of a fuzzy logic controller based on an analogy between fuzzy
conditional rule of inference and marginal probability in terms of the
conditional probability function has been proposed."
"Shape analysis concerns the problem of determining ""shape invariants"" for
programs that perform destructive updating on dynamically allocated storage. In
recent work, we have shown how shape analysis can be performed, using an
abstract interpretation based on 3-valued first-order logic. In that work,
concrete stores are finite 2-valued logical structures, and the sets of stores
that can possibly arise during execution are represented (conservatively) using
a certain family of finite 3-valued logical structures. In this paper, we show
how 3-valued structures that arise in shape analysis can be characterized using
formulas in first-order logic with transitive closure.
  We also define a non-standard (""supervaluational"") semantics for 3-valued
first-order logic that is more precise than a conventional 3-valued semantics,
and demonstrate that the supervaluational semantics can be effectively
implemented using existing theorem provers."
"In this paper we consider the problem of proving properties of infinite
behaviour of formalisms suitable to describe (infinite state) systems with
recursion and parallelism. As a formal setting, we consider the framework of
Process Rewriting Systems (PRSs). For a meaningfull fragment of PRSs, allowing
to accommodate both Pushdown Automata and Petri Nets, we state decidability
results for a class of properties about infinite derivations (infinite term
rewritings). The given results can be exploited for the automatic verification
of some classes of linear time properties of infinite state systems described
by PRSs. In order to exemplify the assessed results, we introduce a meaningful
automaton based formalism which allows to express both recursion and
multi--treading."
"This paper describes learning in a compiler for algorithms solving classes of
the logic minimization problem MINSAT, where the underlying propositional
formula is in conjunctive normal form (CNF) and where costs are associated with
the True/False values of the variables. Each class consists of all instances
that may be derived from a given propositional formula and costs for True/False
values by fixing or deleting variables, and by deleting clauses. The learning
step begins once the compiler has constructed a solution algorithm for a given
class. The step applies that algorithm to comparatively few instances of the
class, analyses the performance of the algorithm on these instances, and
modifies the underlying propositional formula, with the goal that the algorithm
will perform much better on all instances of the class."
"Disjunctive Linear Arithmetic (DLA) is a major decidable theory that is
supported by almost all existing theorem provers. The theory consists of
Boolean combinations of predicates of the form $\Sigma_{j=1}^{n}a_j\cdot x_j
\le b$, where the coefficients $a_j$, the bound $b$ and the variables $x_1 >...
x_n$ are of type Real ($\mathbb{R}$). We show a reduction to propositional
logic from disjunctive linear arithmetic based on Fourier-Motzkin elimination.
While the complexity of this procedure is not better than competing techniques,
it has practical advantages in solving verification problems. It also promotes
the option of deciding a combination of theories by reducing them to this
logic. Results from experiments show that this method has a strong advantage
over existing techniques when there are many disjunctions in the formula."
"ACL2 was used to prove properties of two simplification procedures. The
procedures differ in complexity but solve the same programming problem that
arises in the context of a resolution/paramodulation theorem proving system.
Term rewriting is at the core of the two procedures, but details of the
rewriting procedure itself are irrelevant. The ACL2 encapsulate construct was
used to assert the existence of the rewriting function and to state some of its
properties. Termination, irreducibility, and soundness properties were
established for each procedure. The availability of the encapsulation mechanism
in ACL2 is considered essential to rapid and efficient verification of this
kind of algorithm."
"The inequations of the delays of the asynchronous circuits are written, by
making use of pseudo-Boolean differential calculus. We consider these efforts
to be a possible starting point in the semi-formalized reconstruction of the
digital electrical engineering (which is a non-formalized theory)."
"We present the bounded delays, the absolute inertia and the relative inertia."
"We define the delays of a circuit, as well as the properties of determinism,
order, time invariance, constancy, symmetry and the serial connection."
"We define the delays of a circuit, as well as the properties of determinism,
order, time invariance, constancy, symmetry and the serial connection."
"We propose a new type system for lambda-calculus ensuring that well-typed
programs can be executed in polynomial time: Dual light affine logic (DLAL).
  DLAL has a simple type language with a linear and an intuitionistic type
arrow, and one modality. It corresponds to a fragment of Light affine logic
(LAL). We show that contrarily to LAL, DLAL ensures good properties on
lambda-terms: subject reduction is satisfied and a well-typed term admits a
polynomial bound on the reduction by any strategy. We establish that as LAL,
DLAL allows to represent all polytime functions. Finally we give a type
inference procedure for propositional DLAL."
"The (meta)logic underlying classical theory of computation is Boolean
(two-valued) logic. Quantum logic was proposed by Birkhoff and von Neumann as a
logic of quantum mechanics more than sixty years ago. The major difference
between Boolean logic and quantum logic is that the latter does not enjoy
distributivity in general. The rapid development of quantum computation in
recent years stimulates us to establish a theory of computation based on
quantum logic. The present paper is the first step toward such a new theory and
it focuses on the simplest models of computation, namely finite automata. It is
found that the universal validity of many properties of automata depend heavily
upon the distributivity of the underlying logic. This indicates that these
properties does not universally hold in the realm of quantum logic. On the
other hand, we show that a local validity of them can be recovered by imposing
a certain commutativity to the (atomic) statements about the automata under
consideration. This reveals an essential difference between the classical
theory of computation and the computation theory based on quantum logic."
"Cousot and Cousot introduced and studied a general past/future-time
specification language, called mu*-calculus, featuring a natural time-symmetric
trace-based semantics. The standard state-based semantics of the mu*-calculus
is an abstract interpretation of its trace-based semantics, which turns out to
be incomplete (i.e., trace-incomplete), even for finite systems. As a
consequence, standard state-based model checking of the mu*-calculus is
incomplete w.r.t. trace-based model checking. This paper shows that any
refinement or abstraction of the domain of sets of states induces a
corresponding semantics which is still trace-incomplete for any propositional
fragment of the mu*-calculus. This derives from a number of results, one for
each incomplete logical/temporal connective of the mu*-calculus, that
characterize the structure of models, i.e. transition systems, whose
corresponding state-based semantics of the mu*-calculus is trace-complete."
"The objective of this paper is to develop a functional programming language
for quantum computers. We develop a lambda calculus for the classical control
model, following the first author's work on quantum flow-charts. We define a
call-by-value operational semantics, and we give a type system using affine
intuitionistic linear logic. The main results of this paper are the safety
properties of the language and the development of a type inference algorithm."
"Lambek's production machines may be used to generate and recognize sentences
in a subset of the language described by a production grammar. We determine in
this paper the subset of the language of a grammar generated and recognized by
such machines."
"A system is data-independent with respect to a data type X iff the operations
it can perform on values of type X are restricted to just equality testing. The
system may also store, input and output values of type X. We study model
checking of systems which are data-independent with respect to two distinct
type variables X and Y, and may in addition use arrays with indices from X and
values from Y . Our main interest is the following parameterised model-checking
problem: whether a given program satisfies a given temporal-logic formula for
all non-empty nite instances of X and Y . Initially, we consider instead the
abstraction where X and Y are infinite and where partial functions with finite
domains are used to model arrays. Using a translation to data-independent
systems without arrays, we show that the u-calculus model-checking problem is
decidable for these systems. From this result, we can deduce properties of all
systems with finite instances of X and Y . We show that there is a procedure
for the above parameterised model-checking problem of the universal fragment of
the u-calculus, such that it always terminates but may give false negatives. We
also deduce that the parameterised model-checking problem of the universal
disjunction-free fragment of the u-calculus is decidable. Practical motivations
for model checking data-independent systems with arrays include verification of
memory and cache systems, where X is the type of memory addresses, and Y the
type of storable values. As an example we verify a fault-tolerant memory
interface over a set of unreliable memories."
"""To Appear in Theory and Practice of Logic Programming (TPLP)"" This paper
presents a technique for the optimization of bound queries over disjunctive
deductive databases with constraints. The proposed approach is an extension of
the well-known Magic-Set technique and is well-suited for being integrated in
current bottom-up (stable) model inference engines. More specifically, it is
based on the exploitation of binding propagation techniques which reduce the
size of the data relevant to answer the query and, consequently, reduces both
the complexity of computing a single model and the number of models to be
considered. The motivation of this work stems from the observation that
traditional binding propagation optimization techniques for bottom-up model
generator systems, simulating the goal driven evaluation of top-down engines,
are only suitable for positive (disjunctive) queries, while hard problems are
expressed using unstratified negation. The main contribution of the paper
consists in the extension of a previous technique, defined for positive
disjunctive queries, to queries containing both disjunctive heads and
constraints (a simple and expressive form of unstratified negation). As the
usual way of expressing declaratively hard problems is based on the
guess-and-check technique, where the guess part is expressed by means of
disjunctive rules and the check part is expressed by means of constraints, the
technique proposed here is highly relevant for the optimization of queries
expressing hard problems. The value of the technique has been proved by several
experiments."
"Predicate abstraction provides a powerful tool for verifying properties of
infinite-state systems using a combination of a decision procedure for a subset
of first-order logic and symbolic methods originally developed for finite-state
model checking. We consider models containing first-order state variables,
where the system state includes mutable functions and predicates. Such a model
can describe systems containing arbitrarily large memories, buffers, and arrays
of identical processes. We describe a form of predicate abstraction that
constructs a formula over a set of universally quantified variables to describe
invariant properties of the first-order state variables. We provide a formal
justification of the soundness of our approach and describe how it has been
used to verify several hardware and software designs, including a
directory-based cache coherence protocol."
"The classical propositional logic is known to be sound and complete with
respect to the set semantics that interprets connectives as set operations. The
paper extends propositional language by a new binary modality that corresponds
to partial recursive function type constructor under the above interpretation.
The cases of deterministic and non-deterministic functions are considered and
for both of them semantically complete modal logics are described and
decidability of these logics is established."
"Many existing algorithms for model checking of infinite-state systems operate
on constraints which are used to represent (potentially infinite) sets of
states. A general powerful technique which can be employed for proving
termination of these algorithms is that of well quasi-orderings. Several
methodologies have been proposed for derivation of new well quasi-ordered
constraint systems. However, many of these constraint systems suffer from a
""constraint explosion problem"", as the number of the generated constraints
grows exponentially with the size of the problem. In this paper, we demonstrate
that a refinement of the theory of well quasi-orderings, called the theory of
better quasi-orderings, is more appropriate for symbolic model checking, since
it allows inventing constraint systems which are both well quasi-ordered and
compact. As a main application, we introduce existential zones, a constraint
system for verification of systems with unboundedly many clocks and use our
methodology to prove that existential zones are better quasi-ordered. We show
how to use existential zones in verification of timed Petri nets and present
some experimental results. Also, we apply our methodology to derive new
constraint systems for verification of broadcast protocols, lossy channel
systems, and integral relational automata. The new constraint systems are
exponentially more succinct than existing ones, and their well quasi-ordering
cannot be shown by previous methods in the literature."
"In this paper, we introduce Linear Logic with a nondeterministic facility,
which has a self-dual additive connective. In the system the proof net
technology is available in a natural way. The important point is that
nondeterminism in the system is expressed by the process of normalization, not
by proof search. Moreover we can incorporate the system into Light Linear Logic
and Elementary Linear Logic developed by J.-Y.Girard recently: Nondeterministic
Light Linear Logic and Nondeterministic Elementary Linear Logic are defined in
a very natural way."
"In the Boehm theorem workshop on Crete island, Zoran Petric called Statman's
``Typical Ambiguity theorem'' typed Boehm theorem. Moreover, he gave a new
proof of the theorem based on set-theoretical models of the simply typed lambda
calculus. In this paper, we study the linear version of the typed Boehm theorem
on a fragment of Intuitionistic Linear Logic. We show that in the
multiplicative fragment of intuitionistic linear logic without the
multiplicative unit 1 (for short IMLL) weak typed Boehm theorem holds. The
system IMLL exactly corresponds to the linear lambda calculus without
exponentials, additives and logical constants. The system IMLL also exactly
corresponds to the free symmetric monoidal closed category without the unit
object. As far as we know, our separation result is the first one with regard
to these systems in a purely syntactical manner."
"In CSL'99 Roversi pointed out that the Turing machine encoding of Girard's
seminal paper ""Light Linear Logic"" has a flaw. Moreover he presented a working
version of the encoding in Light Affine Logic, but not in Light Linear Logic.
In this paper we present a working version of the encoding in Light Linear
Logic. The idea of the encoding is based on a remark of Girard's tutorial paper
on Linear Logic. The encoding is also an example which shows usefulness of
additive connectives. Moreover we also consider a nondeterministic extension of
Light Linear Logic. We show that the extended system is NP-complete in the same
meaning as P-completeness of Light Linear Logic."
"A Turing machine with faults, failures and recovery (TMF) is described. TMF
is (weakly) non-deterministic Turing machine consisting of five semi-infinite
tapes (Master Tape, Synchro Tape, Backup Tape, Backup Synchro Tape, User Tape)
and four controlling components (Program, Daemon, Apparatus, User).
Computational process consists of three phases (Program Phase, Failure Phase,
Repair Phase). C++ Simulator of a Turing machine with faults, failures and
recovery has been developed."
"In this paper, we present the interval neutrosophic logics which generalizes
the fuzzy logic, paraconsistent logic, intuitionistic fuzzy logic and many
other non-classical and non-standard logics. We will give the formal definition
of interval neutrosophic propositional calculus and interval neutrosophic
predicate calculus. Then we give one application of interval neutrosophic
logics to do approximate reasoning."
"Formal concept analysis has grown from a new branch of the mathematical field
of lattice theory to a widely recognized tool in Computer Science and
elsewhere. In order to fully benefit from this theory, we believe that it can
be enriched with notions such as approximation by computation or
representability. The latter are commonly studied in denotational semantics and
domain theory and captured most prominently by the notion of algebraicity, e.g.
of lattices. In this paper, we explore the notion of algebraicity in formal
concept analysis from a category-theoretical perspective. To this end, we build
on the the notion of approximable concept with a suitable category and show
that the latter is equivalent to the category of algebraic lattices. At the
same time, the paper provides a relatively comprehensive account of the
representation theory of algebraic lattices in the framework of Stone duality,
relating well-known structures such as Scott information systems with further
formalisms from logic, topology, domains and lattice theory."
"We study conditions for a concurrent construction of proof-nets in the
framework developed by Andreoli in recent papers. We define specific
correctness criteria for that purpose. We first study closed modules (i.e.
validity of the execution of a logic program), then extend the criterion to
open modules (i.e. validity during the execution) distinguishing criteria for
acyclicity and connectability in order to allow incremental verification."
"We show that the satisfiability and finite satisfiability problems for the
two-variable fragment of first-order logic with counting quantifiers are both
in NEXPTIME, even when counting quantifiers are coded succinctly."
"This article illustrates the use of a logical specification language to
capture various forms of confidentiality properties used in the security
literature."
"In this paper we extend the predicate logic introduced in [Beauquier et al.
2002] in order to deal with Semi-Markov Processes. We prove that with respect
to qualitative probabilistic properties, model checking is decidable for this
logic applied to Semi-Markov Processes. Furthermore we apply our logic to
Probabilistic Timed Automata considering classical and urgent semantics, and
considering also predicates on clock. We prove that results on Semi Markov
Processes hold also for Probabilistic Timed Automata for both the two semantics
considered. Moreover, we prove that results for Markov Processes shown in
[Beauquier et al. 2002] are extensible to Probabilistic Timed Automata where
urgent semantics is considered."
"We give a new type inference algorithm for typing lambda-terms in Elementary
Affine Logic (EAL), which is motivated by applications to complexity and
optimal reduction. Following previous references on this topic, the variant of
EAL type system we consider (denoted EAL*) is a variant without sharing and
without polymorphism. Our algorithm improves over the ones already known in
that it offers a better complexity bound: if a simple type derivation for the
term t is given our algorithm performs EAL* type inference in polynomial time."
"A fully abstract and universal domain model for modal transition systems and
refinement is shown to be a maximal-points space model for the bisimulation
quotient of labelled transition systems over a finite set of events. In this
domain model we prove that this quotient is a Stone space whose compact,
zero-dimensional, and ultra-metrizable Hausdorff topology measures the degree
of bisimilarity such that image-finite labelled transition systems are dense.
Using this compactness we show that the set of labelled transition systems that
refine a modal transition system, its ''set of implementations'', is compact
and derive a compactness theorem for Hennessy-Milner logic on such
implementation sets. These results extend to systems that also have partially
specified state propositions, unify existing denotational, operational, and
metric semantics on partial processes, render robust consistency measures for
modal transition systems, and yield an abstract interpretation of compact sets
of labelled transition systems as Scott-closed sets of modal transition
systems."
"Tableaux-based decision procedures for satisfiability of modal and
description logics behave quite well in practice, but it is sometimes hard to
obtain exact worst-case complexity results using these approaches, especially
for EXPTIME-complete logics. In contrast, automata-based approaches often yield
algorithms for which optimal worst-case complexity can easily be proved.
However, the algorithms obtained this way are usually not only worst-case, but
also best-case exponential: they first construct an automaton that is always
exponential in the size of the input, and then apply the (polynomial) emptiness
test to this large automaton. To overcome this problem, one must try to
construct the automaton ""on-the-fly"" while performing the emptiness test.
  In this paper we will show that Voronkov's inverse method for the modal logic
K can be seen as an on-the-fly realization of the emptiness test done by the
automata approach for K. The benefits of this result are two-fold. First, it
shows that Voronkov's implementation of the inverse method, which behaves quite
well in practice, is an optimized on-the-fly implementation of the
automata-based satisfiability procedure for K. Second, it can be used to give a
simpler proof of the fact that Voronkov's optimizations do not destroy
completeness of the procedure. We will also show that the inverse method can
easily be extended to handle global axioms, and that the correspondence to the
automata approach still holds in this setting. In particular, the inverse
method yields an EXPTIME-algorithm for satisfiability in K w.r.t. global
axioms."
"A function f is continuous iff the PRE-image f^{-1}[V] of any open set V is
open again. Dual to this topological property, f is called OPEN iff the IMAGE
f[U] of any open set U is open again. Several classical Open Mapping Theorems
in Analysis provide a variety of sufficient conditions for openness.
  By the Main Theorem of Recursive Analysis, computable real functions are
necessarily continuous. In fact they admit a well-known characterization in
terms of the mapping V+->f^{-1}[V] being EFFECTIVE: Given a list of open
rational balls exhausting V, a Turing Machine can generate a corresponding list
for f^{-1}[V]. Analogously, EFFECTIVE OPENNESS requires the mapping U+->f[U] on
open real subsets to be effective.
  By effectivizing classical Open Mapping Theorems as well as from application
of Tarski's Quantifier Elimination, the present work reveals several rich
classes of functions to be effectively open."
"We define the notion of a partially additive Kleene algebra, which is a
Kleene algebra where the + operation need only be partially defined. These
structures formalize a number of examples that cannot be handled directly by
Kleene algebras. We relate partially additive Kleene algebras to existing
algebraic structures, by exhibiting categorical connections with Kleene
algebras, partially additive categories, and closed semirings."
"We offer a short tour into the interactive interpretation of sequential
programs. We emphasize streamlike computation -- that is, computation of
successive bits of information upon request. The core of the approach surveyed
here dates back to the work of Berry and the author on sequential algorithms on
concrete data structures in the late seventies, culminating in the design of
the programming language CDS, in which the semantics of programs of any type
can be explored interactively. Around one decade later, two major insights of
Cartwright and Felleisen on one hand, and of Lamarche on the other hand gave
new, decisive impulses to the study of sequentiality. Cartwright and Felleisen
observed that sequential algorithms give a direct semantics to control
operators like \""call-cc\"" and proposed to include explicit errors both in the
syntax and in the semantics of the language PCF. Lamarche (unpublished)
connected sequential algorithms to linear logic and games. The successful
program of games semantics has spanned over the nineties until now, starting
with syntax-independent characterizations of the term model of PCF by Abramsky,
Jagadeesan, and Malacaria on one hand, and by Hyland and Ong on the other hand."
"We recall some of the early occurrences of the notions of interactivity and
symmetry in the operational and denotational semantics of programming
languages. We suggest some connections with ludics."
"This two-parts paper offers a survey of linear logic and ludics, which were
introduced by Girard in 1986 and 2001, respectively. Both theories revisit
mathematical logic from first principles, with inspiration from and
applications to computer science. The present part I covers an introduction to
the connectives and proof rules of linear logic, to its decidability
properties, and to its models. Part II will deal with proof nets, a graph-like
representation of proofs which is one of the major innovations of linear logic,
and will present an introduction to ludics."
"This paper is the second part of an introduction to linear logic and ludics,
both due to Girard. It is devoted to proof nets, in the limited, yet central,
framework of multiplicative linear logic and to ludics, which has been recently
developped in an aim of further unveiling the fundamental interactive nature of
computation and logic. We hope to offer a few computer science insights into
this new theory."
"This note shows that split-2 bisimulation equivalence (also known as timed
equivalence) affords a finite equational axiomatization over the process
algebra obtained by adding an auxiliary operation proposed by Hennessy in 1981
to the recursion, relabelling and restriction free fragment of Milner's
Calculus of Communicating Systems. Thus the addition of a single binary
operation, viz. Hennessy's merge, is sufficient for the finite equational
axiomatization of parallel composition modulo this non-interleaving
equivalence. This result is in sharp contrast to a theorem previously obtained
by the same authors to the effect that the same language is not finitely based
modulo bisimulation equivalence."
"This article examines the interpretation of the LTL temporal operators over
finite and infinite sequences. This is used as the basis for deriving a sound
and complete axiomatization for Caret, a recent temporal logic for reasoning
about programs with nested procedure calls and returns."
"In this report, we present a formal model of fair iteration of events for B
event systems. The model is used to justify proof obligations for basic
liveness properties and preservation under refinement of general liveness
properties. The model of fair iteration of events uses the dovetail operator,
an operator proposed by Broy and Nelson to model fair choice. The proofs are
mainly founded in fixpoint calculations of fair iteration of events and weakest
precondition calculus."
"Succinctness is a natural measure for comparing the strength of different
logics. Intuitively, a logic L_1 is more succinct than another logic L_2 if all
properties that can be expressed in L_2 can be expressed in L_1 by formulas of
(approximately) the same size, but some properties can be expressed in L_1 by
(significantly) smaller formulas.
  We study the succinctness of logics on linear orders. Our first theorem is
concerned with the finite variable fragments of first-order logic. We prove
that:
  (i) Up to a polynomial factor, the 2- and the 3-variable fragments of
first-order logic on linear orders have the same succinctness. (ii) The
4-variable fragment is exponentially more succinct than the 3-variable
fragment. Our second main result compares the succinctness of first-order logic
on linear orders with that of monadic second-order logic. We prove that the
fragment of monadic second-order logic that has the same expressiveness as
first-order logic on linear orders is non-elementarily more succinct than
first-order logic."
"This book presents the advancements and applications of neutrosophics.
Chapter 1 first introduces the interval neutrosophic sets which is an instance
of neutrosophic sets. In this chapter, the definition of interval neutrosophic
sets and set-theoretic operators are given and various properties of interval
neutrosophic set are proved. Chapter 2 defines the interval neutrosophic logic
based on interval neutrosophic sets including the syntax and semantics of first
order interval neutrosophic propositional logic and first order interval
neutrosophic predicate logic. The interval neutrosophic logic can reason and
model fuzzy, incomplete and inconsistent information. In this chapter, we also
design an interval neutrosophic inference system based on first order interval
neutrosophic predicate logic. The interval neutrosophic inference system can be
applied to decision making. Chapter 3 gives one application of interval
neutrosophic sets and logic in the field of relational databases. Neutrosophic
data model is the generalization of fuzzy data model and paraconsistent data
model. Here, we generalize various set-theoretic and relation-theoretic
operations of fuzzy data model to neutrosophic data model. Chapter 4 gives
another application of interval neutrosophic logic. A soft semantic Web
Services agent framework is proposed to faciliate the registration and
discovery of high quality semantic Web Services agent. The intelligent
inference engine module of soft Semantic Web Services agent is implemented
using interval neutrosophic logic."
"The theory of Petri Nets provides a general framework to specify the
behaviors of real-time reactive systems and Time Petri Nets were introduced to
take also temporal specifications into account. We present in this paper a
forward zone-based algorithm to compute the state space of a bounded Time Petri
Net: the method is different and more efficient than the classical State Class
Graph. We prove the algorithm to be exact with respect to the reachability
problem. Furthermore, we propose a translation of the computed state space into
a Timed Automaton, proved to be timed bisimilar to the original Time Petri Net.
As the method produce a single Timed Automaton, syntactical clocks reduction
methods (Daws and Yovine for instance) may be applied to produce an automaton
with fewer clocks. Then, our method allows to model-check TTPN by the use of
efficient Timed Automata tools.
  To appear in Theory and Practice of Logic Programming (TPLP)."
"This article examines two approaches to verification, one based on using a
logic for expressing properties of a system, and one based on showing the
system equivalent to a simpler system that obviously has whatever property is
of interest. Using examples such as process calculi and regular programs, the
relationship between these two approaches is explored."
"The paper presents a selection of recently developed and/or used techniques
for equivalence-checking on infinite-state systems, and an up-to-date overview
of existing results (as of September 2004)."
"The language Timed Concurrent Constraint (tccp) is the extension over time of
the Concurrent Constraint Programming (cc) paradigm that allows us to specify
concurrent systems where timing is critical, for example reactive systems.
Systems which may have an infinite number of states can be specified in tccp.
Model checking is a technique which is able to verify finite-state systems with
a huge number of states in an automatic way. In the last years several studies
have investigated how to extend model checking techniques to systems with an
infinite number of states. In this paper we propose an approach which exploits
the computation model of tccp. Constraint based computations allow us to define
a methodology for applying a model checking algorithm to (a class of)
infinite-state systems. We extend the classical algorithm of model checking for
LTL to a specific logic defined for the verification of tccp and to the tccp
Structure which we define in this work for modeling the program behavior. We
define a restriction on the time in order to get a finite model and then we
develop some illustrative examples. To the best of our knowledge this is the
first approach that defines a model checking methodology for tccp."
"We address the problem of verifying clique avoidance in the TTP protocol. TTP
allows several stations embedded in a car to communicate. It has many
mechanisms to ensure robustness to faults. In particular, it has an algorithm
that allows a station to recognize itself as faulty and leave the
communication. This algorithm must satisfy the crucial 'non-clique' property:
it is impossible to have two or more disjoint groups of stations communicating
exclusively with stations in their own group.
  In this paper, we propose an automatic verification method for an arbitrary
number of stations $N$ and a given number of faults $k$. We give an abstraction
that allows to model the algorithm by means of unbounded (parametric) counter
automata. We have checked the non-clique property on this model in the case of
one fault, using the ALV tool as well as the LASH tool."
"A constructive proof of the Goedel-Rosser incompleteness theorem has been
completed using the Coq proof assistant. Some theory of classical first-order
logic over an arbitrary language is formalized. A development of primitive
recursive functions is given, and all primitive recursive functions are proved
to be representable in a weak axiom system. Formulas and proofs are encoded as
natural numbers, and functions operating on these codes are proved to be
primitive recursive. The weak axiom system is proved to be essentially
incomplete. In particular, Peano arithmetic is proved to be consistent in Coq's
type theory and therefore is incomplete."
"A fertile field of research in theoretical computer science investigates the
representation of general recursive functions in intensional type theories.
Among the most successful approaches are: the use of wellfounded relations,
implementation of operational semantics, formalization of domain theory, and
inductive definition of domain predicates. Here, a different solution is
proposed: exploiting coinductive types to model infinite computations. To every
type A we associate a type of partial elements Partial(A), coinductively
generated by two constructors: the first, return(a) just returns an element
a:A; the second, step(x), adds a computation step to a recursive element
x:Partial(A). We show how this simple device is sufficient to formalize all
recursive functions between two given types. It allows the definition of fixed
points of finitary, that is, continuous, operators. We will compare this
approach to different ones from the literature. Finally, we mention that the
formalization, with appropriate structural maps, defines a strong monad."
"We develop a pseudo-metric analogue of bisimulation for generalized
semi-Markov processes. The kernel of this pseudo-metric corresponds to
bisimulation; thus we have extended bisimulation for continuous-time
probabilistic processes to a much broader class of distributions than
exponential distributions. This pseudo-metric gives a useful handle on
approximate reasoning in the presence of numerical information -- such as
probabilities and time -- in the model. We give a fixed point characterization
of the pseudo-metric. This makes available coinductive reasoning principles for
reasoning about distances. We demonstrate that our approach is insensitive to
potentially ad hoc articulations of distance by showing that it is intrinsic to
an underlying uniformity. We provide a logical characterization of this
uniformity using a real-valued modal logic. We show that several quantitative
properties of interest are continuous with respect to the pseudo-metric. Thus,
if two processes are metrically close, then observable quantitative properties
of interest are indeed close."
"We show that Csanky's fast parallel algorithm for computing the
characteristic polynomial of a matrix can be formalized in the logical theory
LAP, and can be proved correct in LAP from the principle of linear
independence. LAP is a natural theory for reasoning about linear algebra
introduced by Cook and Soltys. Further, we show that several principles of
matrix algebra, such as linear independence or the Cayley-Hamilton Theorem, can
be shown equivalent in the logical theory QLA. Applying the separation between
complexity classes AC^0[2] contained in DET(GF(2)), we show that these
principles are in fact not provable in QLA. In a nutshell, we show that linear
independence is ``all there is'' to elementary linear algebra (from a proof
complexity point of view), and furthermore, linear independence cannot be
proved trivially (again, from a proof complexity point of view)."
"Automata provide a decision procedure for Presburger arithmetic. However,
until now only crude lower and upper bounds were known on the sizes of the
automata produced by this approach. In this paper, we prove an upper bound on
the the number of states of the minimal deterministic automaton for a
Presburger arithmetic formula. This bound depends on the length of the formula
and the quantifiers occurring in the formula. The upper bound is established by
comparing the automata for Presburger arithmetic formulas with the formulas
produced by a quantifier elimination method. We also show that our bound is
tight, even for nondeterministic automata. Moreover, we provide optimal
automata constructions for linear equations and inequations."
"It is decidable for deterministic MSO definable graph-to-string or
graph-to-tree transducers whether they are equivalent on a context-free set of
graphs."
"In this paper, we prove the strong normalisation for Martin-L\""{o}f's Logical
Framework, and suggest that {}``correct arity'', a condition weaker than
well-typedness, will also guarantee the strong normalisation."
"We show how standard IPC mechanisms can be used with the fork() system call
to perform explicit state model checking on all interleavings of a
multithreaded application. We specifically show how to check for deadlock and
race conditions in programs with two threads. Our techniques are easy to apply
to other languages, and require only the most rudimentary parsing of the target
language. Our fundamental system fits in one page of C code."
"We propose a generic termination proof method for rewriting under strategies,
based on an explicit induction on the termination property. Rewriting trees on
ground terms are modeled by proof trees, generated by alternatively applying
narrowing and abstracting steps. The induction principle is applied through the
abstraction mechanism, where terms are replaced by variables representing any
of their normal forms. The induction ordering is not given a priori, but
defined with ordering constraints, incrementally set during the proof.
Abstraction constraints can be used to control the narrowing mechanism, well
known to easily diverge. The generic method is then instantiated for the
innermost, outermost and local strategies."
"We consider the model checking problem for probabilistic pushdown automata
(pPDA) and properties expressible in various probabilistic logics. We start
with properties that can be formulated as instances of a generalized random
walk problem. We prove that both qualitative and quantitative model checking
for this class of properties and pPDA is decidable. Then we show that model
checking for the qualitative fragment of the logic PCTL and pPDA is also
decidable. Moreover, we develop an error-tolerant model checking algorithm for
PCTL and the subclass of stateless pPDA. Finally, we consider the class of
omega-regular properties and show that both qualitative and quantitative model
checking for pPDA is decidable."
"This paper describes a simpler way for programmers to reason about the
correctness of their code. The study of semantics of logic programs has shown
strong links between the model theoretic semantics (truth and falsity of atoms
in the programmer's interpretation of a program), procedural semantics (for
example, SLD resolution) and fixpoint semantics (which is useful for program
analysis and alternative execution mechanisms). Most of this work assumes that
intended interpretations are two-valued: a ground atom is true (and should
succeed according to the procedural semantics) or false (and should not
succeed). In reality, intended interpretations are less precise. Programmers
consider that some atoms ""should not occur"" or are ""ill-typed"" or
""inadmissible"". Programmers don't know and don't care whether such atoms
succeed. In this paper we propose a three-valued semantics for (essentially)
pure Prolog programs with (ground) negation as failure which reflects this. The
semantics of Fitting is similar but only associates the third truth value with
non-termination. We provide tools to reason about correctness of programs
without the need for unnatural precision or undue restrictions on programming
style. As well as theoretical results, we provide a programmer-oriented
synopsis. This work has come out of work on declarative debugging, where it has
been recognised that inadmissible calls are important. This paper has been
accepted to appear in Theory and Practice of Logic Programming."
"This article is intended as an introduction to the subject of quantum logic,
and as a brief survey of the relevant literature. Also discussed here are
logics for specification and analysis of quantum information systems, in
particular, recent work by P. Mateus and A. Sernadas, and also by R. van der
Meyden and M. Patra. Overall, our objective is to provide a high-level
presentation of the logical aspects of quantum theory. Mateus' and Sernadas'
EQPL logic is illustrated with a small example, namely the state of an
entangled pair of qubits. The ""KT"" logic of van der Meyden and Patra is
demonstrated briefly in the context of the B92 protocol for quantum key
distribution."
"Given a formula in quantifier-free Presburger arithmetic, if it has a
satisfying solution, there is one whose size, measured in bits, is polynomially
bounded in the size of the formula. In this paper, we consider a special class
of quantifier-free Presburger formulas in which most linear constraints are
difference (separation) constraints, and the non-difference constraints are
sparse. This class has been observed to commonly occur in software
verification. We derive a new solution bound in terms of parameters
characterizing the sparseness of linear constraints and the number of
non-difference constraints, in addition to traditional measures of formula
size. In particular, we show that the number of bits needed per integer
variable is linear in the number of non-difference constraints and logarithmic
in the number and size of non-zero coefficients in them, but is otherwise
independent of the total number of linear constraints in the formula. The
derived bound can be used in a decision procedure based on instantiating
integer variables over a finite domain and translating the input
quantifier-free Presburger formula to an equi-satisfiable Boolean formula,
which is then checked using a Boolean satisfiability solver. In addition to our
main theoretical result, we discuss several optimizations for deriving tighter
bounds in practice. Empirical evidence indicates that our decision procedure
can greatly outperform other decision procedures."
"We study the problem of efficient, scalable set-sharing analysis of logic
programs. We use the idea of representing sharing information as a pair of
abstract substitutions, one of which is a worst-case sharing representation
called a clique set, which was previously proposed for the case of inferring
pair-sharing. We use the clique-set representation for (1) inferring actual
set-sharing information, and (2) analysis within a top-down framework. In
particular, we define the abstract functions required by standard top-down
analyses, both for sharing alone and also for the case of including freeness in
addition to sharing. Our experimental evaluation supports the conclusion that,
for inferring set-sharing, as it was the case for inferring pair-sharing,
precision losses are limited, while useful efficiency gains are obtained. At
the limit, the clique-set representation allowed analyzing some programs that
exceeded memory capacity using classical sharing representations."
"In this paper we consider two hierarchies of hereditarily total and
continuous functionals over the reals based on one extensional and one
intensional representation of real numbers, and we discuss under which
asumptions these hierarchies coincide. This coincidense problem is equivalent
to a statement about the topology of the Kleene-Kreisel continuous functionals.
As a tool of independent interest, we show that the Kleene-Kreisel functionals
may be embedded into both these hierarchies."
"In this paper, we present a framework for the semantics and the computation
of aggregates in the context of logic programming. In our study, an aggregate
can be an arbitrary interpreted second order predicate or function. We define
extensions of the Kripke-Kleene, the well-founded and the stable semantics for
aggregate programs. The semantics is based on the concept of a three-valued
immediate consequence operator of an aggregate program. Such an operator
approximates the standard two-valued immediate consequence operator of the
program, and induces a unique Kripke-Kleene model, a unique well-founded model
and a collection of stable models. We study different ways of defining such
operators and thus obtain a framework of semantics, offering different
trade-offs between precision and tractability. In particular, we investigate
conditions on the operator that guarantee that the computation of the three
types of semantics remains on the same level as for logic programs without
aggregates. Other results show that, in practice, even efficient three-valued
immediate consequence operators which are very low in the precision hierarchy,
still provide optimal precision."
"The Ambient Logic (AL) has been proposed for expressing properties of process
mobility in the calculus of Mobile Ambients (MA), and as a basis for query
languages on semistructured data. In this paper, we study the expressiveness of
AL. We define formulas for capabilities and for communication in MA. We also
derive some formulas that capture finitess of a term, name occurrences and
persistence. We study extensions of the calculus involving more complex forms
of communications, and we define characteristic formulas for the equivalence
induced by the logic on a subcalculus of MA. This subcalculus is defined by
imposing an image-finiteness condition on the reducts of a MA process."
"CTL is the dominant temporal specification language in practice mainly due to
the fact that it admits model checking in linear time. Logic programming and
the database query language Datalog are often used as an implementation
platform for logic languages. In this paper we present the exact relation
between CTL and Datalog and moreover we build on this relation and known
efficient algorithms for CTL to obtain efficient algorithms for fragments of
stratified Datalog. The contributions of this paper are: a) We embed CTL into
STD which is a proper fragment of stratified Datalog. Moreover we show that STD
expresses exactly CTL -- we prove that by embedding STD into CTL. Both
embeddings are linear. b) CTL can also be embedded to fragments of Datalog
without negation. We define a fragment of Datalog with the successor build-in
predicate that we call TDS and we embed CTL into TDS in linear time. We build
on the above relations to answer open problems of stratified Datalog. We prove
that query evaluation is linear and that containment and satisfiability
problems are both decidable. The results presented in this paper are the first
for fragments of stratified Datalog that are more general than those containing
only unary EDBs."
"Community decisions about access control in virtual communities are
non-monotonic in nature. This means that they cannot be expressed in current,
monotonic trust management languages such as the family of Role Based Trust
Management languages (RT). To solve this problem we propose RT-, which adds a
restricted form of negation to the standard RT language, thus admitting a
controlled form of non-monotonicity. The semantics of RT- is discussed and
presented in terms of the well-founded semantics for Logic Programs. Finally we
discuss how chain discovery can be accomplished for RT-."
"This article establishes that the split decomposition of graphs introduced by
Cunnigham, is definable in Monadic Second-Order Logic.This result is actually
an instance of a more general result covering canonical graph decompositions
like the modular decomposition and the Tutte decomposition of 2-connected
graphs into 3-connected components. As an application, we prove that the set of
graphs having the same cycle matroid as a given 2-connected graph can be
defined from this graph by Monadic Second-Order formulas."
"It is common practice to compare the computational power of different models
of computation. For example, the recursive functions are strictly more powerful
than the primitive recursive functions, because the latter are a proper subset
of the former (which includes Ackermann's function). Side-by-side with this
""containment"" method of measuring power, it is standard to use an approach
based on ""simulation"". For example, one says that the (untyped) lambda calculus
is as powerful--computationally speaking--as the partial recursive functions,
because the lambda calculus can simulate all partial recursive functions by
encoding the natural numbers as Church numerals.
  The problem is that unbridled use of these two ways of comparing power allows
one to show that some computational models are strictly stronger than
themselves! We argue that a better definition is that model A is strictly
stronger than B if A can simulate B via some encoding, whereas B cannot
simulate A under any encoding. We then show that the recursive functions are
strictly stronger in this sense than the primitive recursive. We also prove
that the recursive functions, partial recursive functions, and Turing machines
are ""complete"", in the sense that no injective encoding can make them
equivalent to any ""hypercomputational"" model."
"We present an embedding of Petri nets into B abstract systems. The embedding
is achieved by translating both the static structure (modelling aspect) and the
evolution semantics of Petri nets. The static structure of a Petri-net is
captured within a B abstract system through a graph structure. This abstract
system is then included in another abstract system which captures the evolution
semantics of Petri-nets. The evolution semantics results in some B events
depending on the chosen policies: basic nets or high level Petri nets. The
current embedding enables one to use conjointly Petri nets and Event-B in the
same system development, but at different steps and for various analysis."
"Logical relations and their generalizations are a fundamental tool in proving
properties of lambda-calculi, e.g., yielding sound principles for observational
equivalence. We propose a natural notion of logical relations able to deal with
the monadic types of Moggi's computational lambda-calculus. The treatment is
categorical, and is based on notions of subsconing, mono factorization systems,
and monad morphisms. Our approach has a number of interesting applications,
including cases for lambda-calculi with non-determinism (where being in logical
relation means being bisimilar), dynamic name creation, and probabilistic
systems."
"Lossy channel systems (LCSs) are systems of finite state automata that
communicate via unreliable unbounded fifo channels. In order to circumvent the
undecidability of model checking for nondeterministic
  LCSs, probabilistic models have been introduced, where it can be decided
whether a linear-time property holds almost surely. However, such fully
probabilistic systems are not a faithful model of nondeterministic protocols.
  We study a hybrid model for LCSs where losses of messages are seen as faults
occurring with some given probability, and where the internal behavior of the
system remains nondeterministic. Thus the semantics is in terms of
infinite-state Markov decision processes. The purpose of this article is to
discuss the decidability of linear-time properties formalized by formulas of
linear temporal logic (LTL). Our focus is on the qualitative setting where one
asks, e.g., whether a LTL-formula holds almost surely or with zero probability
(in case the formula describes the bad behaviors). Surprisingly, it turns out
that -- in contrast to finite-state Markov decision processes -- the
satisfaction relation for LTL formulas depends on the chosen type of schedulers
that resolve the nondeterminism. While all variants of the qualitative LTL
model checking problem for the full class of history-dependent schedulers are
undecidable, the same questions for finite-memory scheduler can be solved
algorithmically. However, the restriction to reachability properties and
special kinds of recurrent reachability properties yields decidable
verification problems for the full class of schedulers, which -- for this
restricted class of properties -- are as powerful as finite-memory schedulers,
or even a subclass of them."
"Formalizing syntactic proofs of properties of logics, programming languages,
security protocols, and other formal systems is a significant challenge, in
large part because of the obligation to handle name-binding correctly. We
present an approach called nominal abstract syntax that has attracted
considerable interest since its introduction approximately six years ago. After
an overview of other approaches, we describe nominal abstract syntax and
nominal logic, a logic for reasoning about nominal abstract syntax. We also
discuss applications of nominal techniques to programming, automated reasoning,
and identify some future directions."
"This paper treats logic programming with three kinds of negation: default,
weak and strict negations. A 3-valued logic model theory is discussed for logic
programs with three kinds of negation. The procedure is constructed for
negations so that a soundness of the procedure is guaranteed in terms of
3-valued logic model theory."
"Defeasible reasoning is a simple but efficient approach to nonmonotonic
reasoning that has recently attracted considerable interest and that has found
various applications. Defeasible logic and its variants are an important family
of defeasible reasoning methods. So far no relationship has been established
between defeasible logic and mainstream nonmonotonic reasoning approaches.
  In this paper we establish close links to known semantics of logic programs.
In particular, we give a translation of a defeasible theory D into a
meta-program P(D). We show that under a condition of decisiveness, the
defeasible consequences of D correspond exactly to the sceptical conclusions of
P(D) under the stable model semantics. Without decisiveness, the result holds
only in one direction (all defeasible consequences of D are included in all
stable models of P(D)). If we wish a complete embedding for the general case,
we need to use the Kunen semantics of P(D), instead."
"We propose a novel algorithm for automata-based LTL model checking that
interleaves the construction of the generalized B\""{u}chi automaton for the
negation of the formula and the emptiness check. Our algorithm first converts
the LTL formula into a linear weak alternating automaton; configurations of the
alternating automaton correspond to the locations of a generalized B\""{u}chi
automaton, and a variant of Tarjan's algorithm is used to decide the existence
of an accepting run of the product of the transition system and the automaton.
Because we avoid an explicit construction of the B\""{u}chi automaton, our
approach can yield significant improvements in runtime and memory, for large
LTL formulas. The algorithm has been implemented within the SPIN model checker,
and we present experimental results for some benchmark examples."
"Given a universal Horn formula of Kleene algebra with hypotheses of the form
r = 0, it is already known that we can efficiently construct an equation which
is valid if and only if the Horn formula is valid. This is an example of
<i>elimination of hypotheses</i>, which is useful because the equational theory
of Kleene algebra is decidable while the universal Horn theory is not. We show
that hypotheses of the form r = 0 can still be eliminated in the presence of
other hypotheses. This lets us extend any technique for eliminating hypotheses
to include hypotheses of the form r = 0."
"The almost periodic functions form a natural example of a non-separable
normed space. As such, it has been a challenge for constructive mathematicians
to find a natural treatment of them. Here we present a simple proof of Bohr's
fundamental theorem for almost periodic functions which we then generalize to
almost periodic functions on general topological groups."
"This paper describes a logic of progress for concurrent programs. The logic
is based on that of UNITY, molded to fit a sequential programming model.
Integration of the two is achieved by using auxiliary variables in a systematic
way that incorporates program counters into the program text. The rules for
progress in UNITY are then modified to suit this new system. This modification
is however subtle enough to allow the theory of Owicki and Gries to be used
without change."
"A notion of alternating timed automata is proposed. It is shown that such
automata with only one clock have decidable emptiness problem over finite
words. This gives a new class of timed languages which is closed under boolean
operations and which has an effective presentation. We prove that the
complexity of the emptiness problem for alternating timed automata with one
clock is non-primitive recursive. The proof gives also the same lower bound for
the universality problem for nondeterministic timed automata with one clock. We
investigate extension of the model with epsilon-transitions and prove that
emptiness is undecidable. Over infinite words, we show undecidability of the
universality problem."
"This paper studies properties of the logic BV, which is an extension of
multiplicative linear logic (MLL) with a self-dual non-commutative operator. BV
is presented in the calculus of structures, a proof theoretic formalism that
supports deep inference, in which inference rules can be applied anywhere
inside logical expressions. The use of deep inference results in a simple
logical system for MLL extended with the self-dual non-commutative operator,
which has been to date not known to be expressible in sequent calculus. In this
paper, deep inference is shown to be crucial for the logic BV, that is, any
restriction on the ``depth'' of the inference rules of BV would result in a
strictly less expressive logical system."
"We present a fixpoint semantics of event systems. The semantics is presented
in a general framework without concerns of fairness. Soundness and completeness
of rules for deriving ""leads-to"" properties are proved in this general
framework. The general framework is instantiated to minimal progress and weak
fairness assumptions and similar results are obtained. We show the power of
these results by deriving sufficient conditions for ""leads-to"" under minimal
progress proving soundness of proof obligations without reasoning over
state-traces."
"The term ``Boolean category'' should be used for describing an object that is
to categories what a Boolean algebra is to posets. More specifically, a Boolean
category should provide the abstract algebraic structure underlying the proofs
in Boolean Logic, in the same sense as a Cartesian closed category captures the
proofs in intuitionistic logic and a *-autonomous category captures the proofs
in linear logic. However, recent work has shown that there is no canonical
axiomatisation of a Boolean category. In this work, we will see a series (with
increasing strength) of possible such axiomatisations, all based on the notion
of *-autonomous category. We will particularly focus on the medial map, which
has its origin in an inference rule in KS, a cut-free deductive system for
Boolean logic in the calculus of structures. Finally, we will present a
category of proof nets as a particularly well-behaved example of a Boolean
category."
"We present a hierarchical framework for analysing propositional linear-time
temporal logic (PTL) to obtain standard results such as a small model property,
decision procedures and axiomatic completeness. Both finite time and infinite
time are considered and one consequent benefit of the framework is the ability
to systematically reduce infinite-time reasoning to finite-time reasoning. The
treatment of PTL with both the operator Until and past time naturally reduces
to that for PTL without either one. Our method utilises a low-level normal form
for PTL called a ""transition configuration"". In addition, we employ reasoning
about intervals of time. Besides being hierarchical and interval-based, the
approach differs from other analyses of PTL typically based on sets of formulas
and sequences of such sets. Instead we describe models using time intervals
represented as finite and infinite sequences of states. The analysis relates
larger intervals with smaller ones. Steps involved are expressed in
Propositional Interval Temporal Logic (PITL) which is better suited than PTL
for sequentially combining and decomposing formulas. Consequently, we can
articulate issues in PTL model construction of equal relevance in more
conventional analyses but normally only considered at the metalevel. We also
describe a decision procedure based on Binary Decision Diagrams."
"Meseguer's rewriting logic and the rewriting logic CRWL are two well-known
approaches to rewriting as logical deduction that, despite some clear
similarities, were designed with different objectives. Here we study the
relationships between them, both at a syntactic and at a semantic level. Even
though it is not possible to establish an entailment system map between them,
both can be naturally simulated in each other. Semantically, there is no
embedding between the corresponding institutions. Along the way, the notions of
entailment and satisfaction in Meseguer's rewriting logic are generalized. We
also use the syntactic results to prove reflective properties of CRWL."
"We consider a general notion of timed automata with input-determined guards
and show that they admit a robust logical framework along the lines of [D
'Souza03], in terms of a monadic second order logic characterisation and an
expressively complete timed temporal logic. We then generalize these automata
using the notion of recursive operators introduced by Henzinger, Raskin, and
Schobbens, and show that they admit a similar logical framework. These results
hold in the ``pointwise'' semantics. We finally use this framework to show that
the real-time logic MITL of Alur et al is expressively complete with respect to
an MSO corresponding to an appropriate input-determined operator."
"<p>We address the general problem of determining the validity of boolean
combinations of equalities and inequalities between real-valued expressions. In
particular, we consider methods of establishing such assertions using only
restricted forms of distributivity. At the same time, we explore ways in which
""local"" decision or heuristic procedures for fragments of the theory of the
reals can be amalgamated into global ones. </p> <p>Let <em>Tadd[Q]</em> be the
first-order theory of the real numbers in the language of ordered groups, with
negation, a constant <em>1</em>, and function symbols for multiplication by
rational constants. Let <em>Tmult[Q]</em> be the analogous theory for the
multiplicative structure, and let <em>T[Q]</em> be the union of the two. We
show that although <em>T[Q]</em> is undecidable, the universal fragment of
<em>T[Q]</em> is decidable. We also show that terms of <em>T[Q]</em>can
fruitfully be put in a normal form. We prove analogous results for theories in
which <em>Q</em> is replaced, more generally, by suitable subfields <em>F</em>
of the reals. Finally, we consider practical methods of establishing
quantifier-free validities that approximate our (impractical) decidability
results.</p>"
"Modal formulae express monadic second-order properties on Kripke frames, but
in many important cases these have first-order equivalents. Computing such
equivalents is important for both logical and computational reasons. On the
other hand, canonicity of modal formulae is important, too, because it implies
frame-completeness of logics axiomatized with canonical formulae.
  Computing a first-order equivalent of a modal formula amounts to elimination
of second-order quantifiers. Two algorithms have been developed for
second-order quantifier elimination: SCAN, based on constraint resolution, and
DLS, based on a logical equivalence established by Ackermann.
  In this paper we introduce a new algorithm, SQEMA, for computing first-order
equivalents (using a modal version of Ackermann's lemma) and, moreover, for
proving canonicity of modal formulae. Unlike SCAN and DLS, it works directly on
modal formulae, thus avoiding Skolemization and the subsequent problem of
unskolemization. We present the core algorithm and illustrate it with some
examples. We then prove its correctness and the canonicity of all formulae on
which the algorithm succeeds. We show that it succeeds not only on all
Sahlqvist formulae, but also on the larger class of inductive formulae,
introduced in our earlier papers. Thus, we develop a purely algorithmic
approach to proving canonical completeness in modal logic and, in particular,
establish one of the most general completeness results in modal logic so far."
"We are interested in verifying dynamic properties of finite state reactive
systems under fairness assumptions by model checking. The systems we want to
verify are specified through a top-down refinement process. In order to deal
with the state explosion problem, we have proposed in previous works to
partition the reachability graph, and to perform the verification on each part
separately. Moreover, we have defined a class, called Bmod, of dynamic
properties that are verifiable by parts, whatever the partition. We decide if a
property P belongs to Bmod by looking at the form of the Buchi automaton that
accepts the negation of P. However, when a property P belongs to Bmod, the
property f => P, where f is a fairness assumption, does not necessarily belong
to Bmod. In this paper, we propose to use the refinement process in order to
build the parts on which the verification has to be performed. We then show
that with such a partition, if a property P is verifiable by parts and if f is
the expression of the fairness assumptions on a system, then the property f =>
P is still verifiable by parts. This approach is illustrated by its application
to the chip card protocol T=1 using the B engineering design language."
"In this paper we show that classical notions from automata theory such as
simulation and bisimulation can be lifted to the context of enriched
categories. The usual properties of bisimulation are nearly all preserved in
this new context. The class of enriched functors that correspond to functionnal
bisimulations surjective on objects is investigated and appears ""nearly"" open
in the sense of Joyal and Moerdijk. Seeing the change of base techniques as a
convenient means to define process refinement/abstractions, we give sufficient
conditions for the change of base categories to preserve bisimularity. We apply
these concepts to Betti's generalized automata, categorical transition systems,
and other exotic categories."
"In the present paper, we investigate consequence relations that are both
paraconsistent and plausible (but still monotonic). More precisely, we put the
focus on pivotal consequence relations, i.e. those relations that can be
defined by a pivot (in the style of e.g. D. Makinson). A pivot is a fixed
subset of valuations which are considered to be the important ones in the
absolute sense. We worked with a general notion of valuation that covers e.g.
the classical valuations as well as certain kinds of many-valued valuations. In
the many-valued cases, pivotal consequence relations are paraconsistant (in
addition to be plausible), i.e. they are capable of drawing reasonable
conclusions which contain contradictions. We will provide in our general
framework syntactic characterizations of several families of pivotal relations.
In addition, we will provide, again in our general framework, characterizations
of several families of pivotal discriminative consequence relations. The latter
are defined exactly as the plain version, but contradictory conclusions are
rejected. We will also answer negatively a representation problem that was left
open by Makinson. Finally, we will put in evidence a connexion with X-logics
from Forget, Risch, and Siegel. The motivations and the framework of the
present paper are very close to those of a previous paper of the author which
is about preferential consequence relations."
"In a previous work we introduced Dual Light Affine Logic (DLAL)
([BaillotTerui04]) as a variant of Light Linear Logic suitable for guaranteeing
complexity properties on lambda-calculus terms: all typable terms can be
evaluated in polynomial time and all Ptime functions can be represented. In the
present work we address the problem of typing lambda-terms in second-order
DLAL. For that we give a procedure which, starting with a term typed in system
F, finds all possible ways to decorate it into a DLAL typed term. We show that
our procedure can be run in time polynomial in the size of the original Church
typed system F term."
"We extend the work of A. Ciaffaglione and P. Di Gianantonio on mechanical
verification of algorithms for exact computation on real numbers, using
infinite streams of digits implemented as co-inductive types. Four aspects are
studied: the first aspect concerns the proof that digit streams can be related
to the axiomatized real numbers that are already axiomatized in the proof
system (axiomatized, but with no fixed representation). The second aspect
re-visits the definition of an addition function, looking at techniques to let
the proof search mechanism perform the effective construction of an algorithm
that is correct by construction. The third aspect concerns the definition of a
function to compute affine formulas with positive rational coefficients. This
should be understood as a testbed to describe a technique to combine
co-recursion and recursion to obtain a model for an algorithm that appears at
first sight to be outside the expressive power allowed by the proof system. The
fourth aspect concerns the definition of a function to compute series, with an
application on the series that is used to compute Euler's number e. All these
experiments should be reproducible in any proof system that supports
co-inductive types, co-recursion and general forms of terminating recursion,
but we performed with the Coq system [12, 3, 14]."
"These notes provide a quick introduction to the Coq system and show how it
can be used to define logical concepts and functions and reason about them. It
is designed as a tutorial, so that readers can quickly start their own
experiments, learning only a few of the capabilities of the system. A much more
comprehensive study is provided in [1], which also provides an extensive
collection of exercises to train on."
"We describe the basic notions of co-induction as they are available in the
coq system. As an application, we describe arithmetic properties for simple
representations of real numbers."
"In this paper, we consider first-order logic over unary functions and study
the complexity of the evaluation problem for conjunctive queries described by
such kind of formulas. A natural notion of query acyclicity for this language
is introduced and we study the complexity of a large number of variants or
generalizations of acyclic query problems in that context (Boolean or not
Boolean, with or without inequalities, comparisons, etc...). Our main results
show that all those problems are \textit{fixed-parameter linear} i.e. they can
be evaluated in time $f(|Q|).|\textbf{db}|.|Q(\textbf{db})|$ where $|Q|$ is the
size of the query $Q$, $|\textbf{db}|$ the database size, $|Q(\textbf{db})|$ is
the size of the output and $f$ is some function whose value depends on the
specific variant of the query problem (in some cases, $f$ is the identity
function). Our results have two kinds of consequences. First, they can be
easily translated in the relational (i.e., classical) setting. Previously known
bounds for some query problems are improved and new tractable cases are then
exhibited. Among others, as an immediate corollary, we improve a result of
\~\cite{PapadimitriouY-99} by showing that any (relational) acyclic conjunctive
query with inequalities can be evaluated in time
$f(|Q|).|\textbf{db}|.|Q(\textbf{db})|$. A second consequence of our method is
that it provides a very natural descriptive approach to the complexity of
well-known algorithmic problems. A number of examples (such as acyclic subgraph
problems, multidimensional matching, etc...) are considered for which new
insights of their complexity are given."
"In the first part of this paper we present a theory of proof nets for full
multiplicative linear logic, including the two units. It naturally extends the
well-known theory of unit-free multiplicative proof nets. A linking is no
longer a set of axiom links but a tree in which the axiom links are subtrees.
These trees will be identified according to an equivalence relation based on a
simple form of graph rewriting. We show the standard results of
sequentialization and strong normalization of cut elimination. In the second
part of the paper we show that the identifications enforced on proofs are such
that the class of two-conclusion proof nets defines the free *-autonomous
category."
"This paper introduces a propositional encoding for lexicographic path orders
in connection with dependency pairs. This facilitates the application of SAT
solvers for termination analysis of term rewrite systems based on the
dependency pair method. We address two main inter-related issues and encode
them as satisfiability problems of propositional formulas that can be
efficiently handled by SAT solving: (1) the combined search for a lexicographic
path order together with an \emph{argument filtering} to orient a set of
inequalities; and (2) how the choice of the argument filtering influences the
set of inequalities that have to be oriented. We have implemented our
contributions in the termination prover AProVE. Extensive experiments show that
by our encoding and the application of SAT solvers one obtains speedups in
orders of magnitude as well as increased termination proving power."
"We propose a new algorithm for minimal unsatisfiable core extraction, based
on a deeper exploration of resolution-refutation properties. We provide
experimental results on formal verification benchmarks confirming that our
algorithm finds smaller cores than suboptimal algorithms; and that it runs
faster than those algorithms that guarantee minimality of the core."
"In order to more effectively cope with the real-world problems of vagueness,
impreciseness, and subjectivity, fuzzy discrete event systems (FDESs) were
proposed recently. Notably, FDESs have been applied to biomedical control for
HIV/AIDS treatment planning and sensory information processing for robotic
control. Qiu, Cao and Ying independently developed supervisory control theory
of FDESs. We note that the controllability of events in Qiu's work is fuzzy but
the observability of events is crisp, and, the observability of events in Cao
and Ying's work is also crisp although the controllability is not completely
crisp since the controllable events can be disabled with any degrees. Motivated
by the necessity to consider the situation that the events may be observed or
controlled with some membership degrees, in this paper, we establish the
supervisory control theory of FDESs with partial observations, in which both
the observability and controllability of events are fuzzy instead. We formalize
the notions of fuzzy controllability condition and fuzzy observability
condition. And Controllability and Observability Theorem of FDESs is set up in
a more generic framework. In particular, we present a detailed computing flow
to verify whether the controllability and observability conditions hold. Thus,
this result can decide the existence of supervisors. Also, we use this
computing method to check the existence of supervisors in the Controllability
and Observability Theorem of classical discrete event systems (DESs), which is
a new method and different from classical case. A number of examples are
elaborated on to illustrate the presented results."
"This article describes recent work on the topic of specifying properties of
transition systems. By giving a suitably abstract description of transition
systems as coalgebras, it is possible to derive logics for capturing properties
of these transition systems in an elegant way."
"We investigate families of infinite automata for context-sensitive languages.
An infinite automaton is an infinite labeled graph with two sets of initial and
final vertices. Its language is the set of all words labelling a path from an
initial vertex to a final vertex. In 2001, Morvan and Stirling proved that
rational graphs accept the context-sensitive languages between rational sets of
initial and final vertices. This result was later extended to sub-families of
rational graphs defined by more restricted classes of transducers.
languages.<br><br>
  Our contribution is to provide syntactical and self-contained proofs of the
above results, when earlier constructions relied on a non-trivial normal form
of context-sensitive grammars defined by Penttonen in the 1970's. These new
proof techniques enable us to summarize and refine these results by considering
several sub-families defined by restrictions on the type of transducers, the
degree of the graph or the size of the set of initial vertices."
"Lehmann, Magidor, and Schlechta developed an approach to belief revision
based on distances between any two valuations. Suppose we are given such a
distance D. This defines an operator |D, called a distance operator, which
transforms any two sets of valuations V and W into the set V |D W of all
elements of W that are closest to V. This operator |D defines naturally the
revision of K by A as the set of all formulas satisfied in M(K) |D M(A) (i.e.
those models of A that are closest to the models of K). This constitutes a
distance-based revision operator. Lehmann et al. characterized families of them
using a loop condition of arbitrarily big size. An interesting question is
whether this loop condition can be replaced by a finite one. Extending the
results of Schlechta, we will provide elements of negative answer. In fact, we
will show that for families of distance operators, there is no ""normal""
characterization. Approximatively, a normal characterization contains only
finite and universally quantified conditions. These results have an interest of
their own for they help to understand the limits of what is possible in this
area. Now, we are quite confident that this work can be continued to show
similar impossibility results for distance-based revision operators, which
suggests that the big loop condition cannot be simplified."
"This paper presents some first results on how to perform uniform random walks
(where every trace has the same probability to occur) in very large models. The
models considered here are described in a succinct way as a set of
communicating reactive modules. The method relies upon techniques for counting
and drawing uniformly at random words in regular languages. Each module is
considered as an automaton defining such a language. It is shown how it is
possible to combine local uniform drawings of traces, and to obtain some global
uniform random sampling, without construction of the global model."
"We formalise and mechanise a construtive, proof theoretic proof of Craig's
Interpolation Theorem in Isabelle/HOL. We give all the definitions and lemma
statements both formally and informally. We also transcribe informally the
formal proofs. We detail the main features of our mechanisation, such as the
formalisation of binding for first order formulae. We also give some
applications of Craig's Interpolation Theorem."
"This article responds to a critique of higher-order abstract syntax appearing
in Logic Column 14, ``Nominal Logic and Abstract Syntax'', cs.LO/0511025."
"Using the left merge and communication merge from ACP, we present an
equational base (i.e., a ground-complete and $\omega$-complete set of valid
equations) for the fragment of CCS without recursion, restriction and
relabelling. Our equational base is finite if the set of actions is finite."
"It is well known that S^1_2 cannot prove the injective weak pigeonhole
principle for polynomial time functions unless RSA is insecure. In this note we
investigate the provability of the surjective (dual) weak pigeonhole principle
in S^1_2 for provably weaker function classes."
"This paper brings together two lines of research: implicit characterization
of complexity classes by Linear Logic (LL) on the one hand, and computation
over an arbitrary ring in the Blum-Shub-Smale (BSS) model on the other. Given a
fixed ring structure K we define an extension of Terui's light affine
lambda-calculus typed in LAL (Light Affine Logic) with a basic type for K. We
show that this calculus captures the polynomial time function class FP(K):
every typed term can be evaluated in polynomial time and conversely every
polynomial time BSS machine over K can be simulated in this calculus."
"We give an axiomatisation of strong bisimilarity on a small fragment of CCS
that does not feature the sum operator. This axiomatisation is then used to
derive congruence of strong bisimilarity in the finite pi-calculus in absence
of sum. To our knowledge, this is the only nontrivial subcalculus of the
pi-calculus that includes the full output prefix and for which strong
bisimilarity is a congruence."
"The refinement calculus for logic programs is a framework for deriving logic
programs from specifications. It is based on a wide-spectrum language that can
express both specifications and code, and a refinement relation that models the
notion of correct implementation. In this paper we extend and generalise
earlier work on contextual refinement. Contextual refinement simplifies the
refinement process by abstractly capturing the context of a subcomponent of a
program, which typically includes information about the values of the free
variables. This paper also extends and generalises module refinement. A module
is a collection of procedures that operate on a common data type; module
refinement between a specification module A and an implementation module C
allows calls to the procedures of A to be systematically replaced with calls to
the corresponding procedures of C. Based on the conditions for module
refinement, we present a method for calculating an implementation module from a
specification module. Both contextual and module refinement within the
refinement calculus have been generalised from earlier work and the results are
presented in a unified framework."
"In a previous work, the first author extended to higher-order rewriting and
dependent types the use of size annotations in types, a termination proof
technique called type or size based termination and initially developed for
ML-like programs. Here, we go one step further by considering conditional
rewriting and explicit quantifications and constraints on size annotations.
This allows to describe more precisely how the size of the output of a function
depends on the size of its inputs. Hence, we can check the termination of more
functions. We first give a general type-checking algorithm based on constraint
solving. Then, we give a termination criterion with constraints in Presburger
arithmetic. To our knowledge, this is the first termination criterion for
higher-order conditional rewriting taking into account the conditions in
termination."
"In (hyper)coherence semantics, proofs/terms are cliques in (hyper)graphs.
Intuitively, vertices represent results of computations and the edge relation
witnesses the ability of being assembled into a same piece of data or a same
(strongly) stable function, at arrow types. In (hyper)coherence semantics, the
argument of a (strongly) stable functional is always a (strongly) stable
function. As a consequence, comparatively to the relational semantics, where
there is no edge relation, some vertices are missing. Recovering these vertices
is essential for the purpose of reconstructing proofs/terms from their
interpretations. It shall also be useful for the comparison with other
semantics, like game semantics. In [BE01], Bucciarelli and Ehrhard introduced a
so called non uniform coherence space semantics where no vertex is missing. By
constructing the co-free exponential we set a new version of this last
semantics, together with non uniform versions of hypercoherences and
multicoherences, a new semantics where an edge is a finite multiset. Thanks to
the co-free construction, these non uniform semantics are deterministic in the
sense that the intersection of a clique and of an anti-clique contains at most
one vertex, a result of interaction, and extensionally collapse onto the
corresponding uniform semantics."
"The notion of computability closure has been introduced for proving the
termination of the combination of higher-order rewriting and beta-reduction. It
is also used for strengthening the higher-order recursive path ordering. In the
present paper, we study in more details the relations between the computability
closure and the (higher-order) recursive path ordering. We show that the
first-order recursive path ordering is equal to an ordering naturally defined
from the computability closure. In the higher-order case, we get an ordering
containing the higher-order recursive path ordering whose well-foundedness
relies on the correctness of the computability closure. This provides a simple
way to extend the higher-order recursive path ordering to richer type systems."
"Termination is a major question in both logic and computer science. In logic,
termination is at the heart of proof theory where it is usually called strong
normalization (of cut elimination). In computer science, termination has always
been an important issue for showing programs correct. In the early days of
logic, strong normalization was usually shown by assigning ordinals to
expressions in such a way that eliminating a cut would yield an expression with
a smaller ordinal. In the early days of verification, computer scientists used
similar ideas, interpreting the arguments of a program call by a natural
number, such as their size. Showing the size of the arguments to decrease for
each recursive call gives a termination proof of the program, which is however
rather weak since it can only yield quite small ordinals. In the sixties, Tait
invented a new method for showing cut elimination of natural deduction, based
on a predicate over the set of terms, such that the membership of an expression
to the predicate implied the strong normalization property for that expression.
The predicate being defined by induction on types, or even as a fixpoint, this
method could yield much larger ordinals. Later generalized by Girard under the
name of reducibility or computability candidates, it showed very effective in
proving the strong normalization property of typed lambda-calculi..."
"We show that it is equivalent, for certain sets of finite graphs, to be
definable in CMS (counting monadic second-order logic, a natural extension of
monadic second-order logic), and to be recognizable in an algebraic framework
induced by the notion of modular decomposition of a finite graph. More
precisely, we consider the set $F\_\infty$ of composition operations on graphs
which occur in the modular decomposition of finite graphs. If $F$ is a subset
of $F\_{\infty}$, we say that a graph is an $\calF$-graph if it can be
decomposed using only operations in $F$. A set of $F$-graphs is recognizable if
it is a union of classes in a finite-index equivalence relation which is
preserved by the operations in $F$. We show that if $F$ is finite and its
elements enjoy only a limited amount of commutativity -- a property which we
call weak rigidity, then recognizability is equivalent to CMS-definability.
This requirement is weak enough to be satisfied whenever all $F$-graphs are
posets, that is, transitive dags. In particular, our result generalizes Kuske's
recent result on series-parallel poset languages."
"<i>H</i> is the theory extending &#946;-conversion by identifying all closed
unsolvables. <i>H</i>&#969; is the closure of this theory under the &#969;-rule
(and &#946;-conversion). A long-standing conjecture of H. Barendregt states
that the provable equations of <i>H</i>&#969; form
&#928;<sub>1</sub><sup>1</sup>-complete set. Here we prove that conjecture."
"Bipolar synchronization systems (BP-systems) constitute a class of coloured
Petri nets, well suited for modeling the control flow of discrete, dynamical
systems. Every BP-system has an underlying ordinary Petri net, which is a
T-system. Moreover, it has a second ordinary net attached, which is a
free-choice system. We prove that a BP-system is live and safe if the T-system
and the free-choice system are live and safe and if the free-choice system has
no frozen tokens. This result is the converse of a theorem of Genrich and
Thiagarajan and proves an elder conjecture. The proof compares the different
Petri nets by Petri net morphisms and makes use of the classical theory of
free-choice systems"
"We describe several technical tools that prove to be efficient for
investigating the rewrite systems associated with a family of algebraic laws,
and might be useful for more general rewrite systems. These tools consist in
introducing a monoid of partial operators, listing the monoid relations
expressing the possible local confluence of the rewrite system, then
introducing the group presented by these relations, and finally replacing the
initial rewrite system with a internal process entirely sitting in the latter
group. When the approach can be completed, one typically obtains a practical
method for constructing algebras satisfying prescribed laws and for solving the
associated word problem."
"Recognizable languages of finite words are part of every computer science
cursus, and they are routinely described as a cornerstone for applications and
for theory. We would like to briefly explore why that is, and how this
word-related notion extends to more complex models, such as those developed for
modeling distributed or timed behaviors."
"We deal with the distribution of N points placed consecutively around the
circle by a fixed angle of a. From the proof of Tony van Ravenstein, we propose
a detailed proof of the Steinhaus conjecture whose result is the following: the
N points partition the circle into gaps of at most three different lengths. We
study the mathematical notions required for the proof of this theorem revealed
during a formal proof carried out in Coq."
"We have studied the update operator defined for update sequences by Eiter et
al. without tautologies and we have observed that it satisfies an interesting
property This property, which we call Weak Independence of Syntax (WIS), is
similar to one of the postulates proposed by Alchourron, Gardenfors, and
Makinson (AGM); only that in this case it applies to nonmonotonic logic. In
addition, we consider other five additional basic properties about update
programs and we show that the operator of Eiter et al. satisfies them. This
work continues the analysis of the AGM postulates under a refined view that
considers nelson logic as a monotonic logic which allows us to expand our
understanding of answer sets. Moreover, nelson logic helped us to derive an
alternative definition of the operator defined by Eiter et al. avoiding the use
of unnecessary extra atoms."
"Church's thesis claims that all effecticely calculable functions are
recursive. A shortcoming of the various definitions of recursive functions lies
in the fact that it is not a matter of a syntactical check to find out if an
entity gives rise to a function. Eight new ideas for a precise setup of
arithmetical logic and its metalanguage give the proper environment for the
construction of a special computer, the ARBACUS computer. Computers do not come
to a necessary halt; it is requested that calculators are constructed on the
basis of computers in a way that they always come to a halt, then all
calculations are effective. The ARBATOR is defined as a calculator with
two-layer-computation. It allows for the calculation of all primitive recursive
functions, but multi-level-arbation also allows for the calculation of other
arbative functions that are not primitive recursive. The new paradigm of
calculation does not have the above mentioned shortcoming. The defenders of
Church's thesis are challenged to show that exotic arbative functions are
recursive and to put forward a recursive function that is not arbative. A
construction with three-tier-multi-level-arbation that includes a
diagonalisation leads to the extravagant yet calculable Snark-function that is
not arbative. As long as it is not shown that all exotic arbative functions and
particularily the Snark-function are arithmetically representable Goedel's
first incompleteness sentence is in limbo."
"We propose to use Tarski's least fixpoint theorem as a basis to define
recursive functions in the calculus of inductive constructions. This widens the
class of functions that can be modeled in type-theory based theorem proving
tool to potentially non-terminating functions. This is only possible if we
extend the logical framework by adding the axioms that correspond to classical
logic. We claim that the extended framework makes it possible to reason about
terminating and non-terminating computations and we show that common facilities
of the calculus of inductive construction, like program extraction can be
extended to also handle the new functions."
"This paper is concerned with the foundations of the Calculus of Algebraic
Constructions (CAC), an extension of the Calculus of Constructions by inductive
data types. CAC generalizes inductive types equipped with higher-order
primitive recursion, by providing definitions of functions by pattern-matching
which capture recursor definitions for arbitrary non-dependent and
non-polymorphic inductive types satisfying a strictly positivity condition. CAC
also generalizes the first-order framework of abstract data types by providing
dependent types and higher-order rewrite rules."
"In the last twenty years, several approaches to higher-order rewriting have
been proposed, among which Klop's Combinatory Rewrite Systems (CRSs), Nipkow's
Higher-order Rewrite Systems (HRSs) and Jouannaud and Okada's higher-order
algebraic specification languages, of which only the last one considers typed
terms. The later approach has been extended by Jouannaud, Okada and the present
author into Inductive Data Type Systems (IDTSs). In this paper, we extend IDTSs
with the CRS higher-order pattern-matching mechanism, resulting in simply-typed
CRSs. Then, we show how the termination criterion developed for IDTSs with
first-order pattern-matching, called the General Schema, can be extended so as
to prove the strong normalization of IDTSs with higher-order pattern-matching.
Next, we compare the unified approach with HRSs. We first prove that the
extended General Schema can also be applied to HRSs. Second, we show how
Nipkow's higher-order critical pair analysis technique for proving local
confluence can be applied to IDTSs."
"The main novelty of this paper is to consider an extension of the Calculus of
Constructions where predicates can be defined with a general form of rewrite
rules. We prove the strong normalization of the reduction relation generated by
the beta-rule and the user-defined rules under some general syntactic
conditions including confluence. As examples, we show that two important
systems satisfy these conditions: a sub-system of the Calculus of Inductive
Constructions which is the basis of the proof assistant Coq, and the Natural
Deduction Modulo a large class of equational theories."
"In a previous work (""Abstract Data Type Systems"", TCS 173(2), 1997), the last
two authors presented a combined language made of a (strongly normalizing)
algebraic rewrite system and a typed lambda-calculus enriched by
pattern-matching definitions following a certain format, called the ""General
Schema"", which generalizes the usual recursor definitions for natural numbers
and similar ""basic inductive types"". This combined language was shown to be
strongly normalizing. The purpose of this paper is to reformulate and extend
the General Schema in order to make it easily extensible, to capture a more
general class of inductive types, called ""strictly positive"", and to ease the
strong normalization proof of the resulting system. This result provides a
computation model for the combination of an algebraic specification language
based on abstract data types and of a strongly typed functional language with
strictly positive inductive types."
"We study the properties, in particular termination, of dependent types
systems for lambda calculus and rewriting."
"A protocol-independent secrecy theorem is established and applied to several
non-trivial protocols. In particular, it is applied to protocols proposed for
protecting the computation results of free-roaming mobile agents doing
comparison shopping. All the results presented here have been formally proved
in Isabelle by building on Larry Paulson's inductive approach. This therefore
provides a library of general theorems that can be applied to other protocols."
"In a previous work, we proved that almost all of the Calculus of Inductive
Constructions (CIC), which is the basis of the proof assistant Coq, can be seen
as a Calculus of Algebraic Constructions (CAC), an extension of the Calculus of
Constructions with functions and predicates defined by higher-order rewrite
rules. In this paper, we not only prove that CIC as a whole can be seen as a
CAC, but also that it can be extended with non-free constructors,
pattern-matching on defined symbols, non-strictly positive types and
inductive-recursive types."
"We study the termination of rewriting modulo a set of equations in the
Calculus of Algebraic Constructions, an extension of the Calculus of
Constructions with functions and predicates defined by higher-order rewrite
rules. In a previous work, we defined general syntactic conditions based on the
notion of computable closure for ensuring the termination of the combination of
rewriting and beta-reduction. Here, we show that this result is preserved when
considering rewriting modulo a set of equations if the equivalence classes
generated by these equations are finite, the equations are linear and satisfy
general syntactic conditions also based on the notion of computable closure.
This includes equations like associativity and commutativity, and provides an
original treatment of termination modulo equations."
"This paper presents general syntactic conditions ensuring the strong
normalization and the logical consistency of the Calculus of Algebraic
Constructions, an extension of the Calculus of Constructions with functions and
predicates defined by higher-order rewrite rules. On the one hand, the Calculus
of Constructions is a powerful type system in which one can formalize the
propositions and natural deduction proofs of higher-order logic. On the other
hand, rewriting is a simple and powerful computation paradigm. The combination
of both allows, among other things, to develop formal proofs with a reduced
size and more automation compared with more traditional proof assistants. The
main novelty is to consider a general form of rewriting at the predicate-level
which generalizes the strong elimination of the Calculus of Inductive
Constructions."
"In a previous work, we proved that an important part of the Calculus of
Inductive Constructions (CIC), the basis of the Coq proof assistant, can be
seen as a Calculus of Algebraic Constructions (CAC), an extension of the
Calculus of Constructions with functions and predicates defined by higher-order
rewrite rules. In this paper, we prove that almost all CIC can be seen as a
CAC, and that it can be further extended with non-strictly positive types and
inductive-recursive types together with non-free constructors and
pattern-matching on defined symbols."
"We show how to give a coherent semantics to programs that are well-specified
in a version of separation logic for a language with higher types: idealized
algol extended with heaps (but with immutable stack variables). In particular,
we provide simple sound rules for deriving higher-order frame rules, allowing
for local reasoning."
"In 1985, van den Dries showed that the theory of the reals with a predicate
for the integer powers of two admits quantifier elimination in an expanded
language, and is hence decidable. He gave a model-theoretic argument, which
provides no apparent bounds on the complexity of a decision procedure. We
provide a syntactic argument that yields a procedure that is primitive
recursive, although not elementary. In particular, we show that it is possible
to eliminate a single block of existential quantifiers in time $2^0_{O(n)}$,
where $n$ is the length of the input formula and $2_k^x$ denotes $k$-fold
iterated exponentiation."
"These are the notes for a 5-lecture-course given at ESSLLI 2006 in Malaga,
Spain. The URL of the school is http://esslli2006.lcc.uma.es/ . This version
slightly differs from the one which has been distributed at the school because
typos have been removed and comments and suggestions by students have been
worked in. The course is intended to be introductory. That means no prior
knowledge of proof nets is required. However, the student should be familiar
with the basics of propositional logic, and should have seen formal proofs in
some formal deductive system (e.g., sequent calculus, natural deduction,
resolution, tableaux, calculus of structures, Frege-Hilbert-systems, ...). It
is probably helpful if the student knows already what cut elimination is, but
this is not strictly necessary. In these notes, I will introduce the concept of
``proof nets'' from the viewpoint of the problem of the identity of proofs. I
will proceed in a rather informal way. The focus will be more on presenting
ideas than on presenting technical details. The goal of the course is to give
the student an overview of the theory of proof nets and make the vast amount of
literature on the topic easier accessible to the beginner. For introducing the
basic concepts of the theory, I will in the first part of the course stick to
the unit-free multiplicative fragment of linear logic because of its rather
simple notion of proof nets. In the second part of the course we will see proof
nets for more sophisticated logics. This is a basic introduction into proof
nets from the perspective of the identity of proofs. We discuss how deductive
proofs can be translated into proof nets and what a correctness criterion is."
"According to a previous result by S. V. Avgustinovich and the author, each
factorial language admits a unique canonical decomposition to a catenation of
factorial languages. In this paper, we analyze the appearance of the canonical
decomposition of a catenation of two factorial languages whose canonical
decompositions are given."
"We present a formalization of a version of Abadi and
  Plotkin's logic for parametricity for a polymorphic dual
intuitionistic/linear type theory with fixed points, and show, following
Plotkin's suggestions, that it can be used to define a wide collection of
types, including existential types, inductive types, coinductive types and
general recursive types. We show that the recursive types satisfy a universal
property called dinaturality, and we develop reasoning principles for the
constructed types. In the case of recursive types, the reasoning principle is a
mixed induction/coinduction principle, with the curious property that
coinduction holds for general relations, but induction only for a limited
collection of ``admissible'' relations. A similar property was observed in
Pitts' 1995 analysis of recursive types in domain theory. In a future paper we
will develop a category theoretic notion of models of the logic presented here,
and show how the results developed in the logic can be transferred to the
models."
"This article surveys recent advances in applying algebraic techniques to
constraint satisfaction problems."
"We consider the problem of bounded model checking (BMC) for linear temporal
logic (LTL). We present several efficient encodings that have size linear in
the bound. Furthermore, we show how the encodings can be extended to LTL with
past operators (PLTL). The generalised encoding is still of linear size, but
cannot detect minimal length counterexamples. By using the virtual unrolling
technique minimal length counterexamples can be captured, however, the size of
the encoding is quadratic in the specification. We also extend virtual
unrolling to Buchi automata, enabling them to accept minimal length
counterexamples.
  Our BMC encodings can be made incremental in order to benefit from
incremental SAT technology. With fairly small modifications the incremental
encoding can be further enhanced with a termination check, allowing us to prove
properties with BMC. Experiments clearly show that our new encodings improve
performance of BMC considerably, particularly in the case of the incremental
encoding, and that they are very competitive for finding bugs. An analysis of
the liveness-to-safety transformation reveals many similarities to the BMC
encodings in this paper. Using the liveness-to-safety translation with
BDD-based invariant checking results in an efficient method to find shortest
counterexamples that complements the BMC-based approach."
"The formal system lambda-delta is a typed lambda calculus that pursues the
unification of terms, types, environments and contexts as the main goal.
lambda-delta takes some features from the Automath-related lambda calculi and
some from the pure type systems, but differs from both in that it does not
include the Pi construction while it provides for an abbreviation mechanism at
the level of terms. lambda-delta enjoys some important desirable properties
such as the confluence of reduction, the correctness of types, the uniqueness
of types up to conversion, the subject reduction of the type assignment, the
strong normalization of the typed terms and, as a corollary, the decidability
of type inference problem."
"We consider Dense-Timed Petri Nets (TPN), an extension of Petri nets in which
each token is equipped with a real-valued clock and where the semantics is lazy
(i.e., enabled transitions need not fire; time can pass and disable
transitions). We consider the following verification problems for TPNs. (i)
Zenoness: whether there exists a zeno-computation from a given marking, i.e.,
an infinite computation which takes only a finite amount of time. We show
decidability of zenoness for TPNs, thus solving an open problem from [Escrig et
al.]. Furthermore, the related question if there exist arbitrarily fast
computations from a given marking is also decidable. On the other hand,
universal zenoness, i.e., the question if all infinite computations from a
given marking are zeno, is undecidable. (ii) Token liveness: whether a token is
alive in a marking, i.e., whether there is a computation from the marking which
eventually consumes the token. We show decidability of the problem by reducing
it to the coverability problem, which is decidable for TPNs. (iii) Boundedness:
whether the size of the reachable markings is bounded. We consider two versions
of the problem; namely semantic boundedness where only live tokens are taken
into consideration in the markings, and syntactic boundedness where also dead
tokens are considered. We show undecidability of semantic boundedness, while we
prove that syntactic boundedness is decidable through an extension of the
Karp-Miller algorithm."
"This draft suggests a new counterexample guided abstraction refinement
(CEGAR) framework that uses the combination of numerical simulation for
nonlinear differential equations with linear programming for linear hybrid
automata (LHA) to perform reachability analysis on nonlinear hybrid automata. A
notion of $\epsilon-$ structural robustness is also introduced which allows the
algorithm to validate counterexamples using numerical simulations.
  Keywords: verification, model checking, hybrid systems, hybrid automata,
robustness, robust hybrid systems, numerical simulation, cegar, abstraction
refinement."
"This report presents a formalisation of Sylow's theorems done in {\sc Coq}.
The formalisation has been done in a couple of weeks on top of Georges
Gonthier's {\sc ssreflect} \cite{ssreflect}. There were two ideas behind
formalising Sylow's theorems. The first one was to get familiar with Georges
way of doing proofs. The second one was to contribute to the collective effort
to formalise a large subset of group theory in {\sc Coq} with some non-trivial
proofs.}"
"This article is the first of an intended series of works on the model theory
of Ultrafinitism. It is roughly divided into two parts. The first one addresses
some of the issues related to ultrafinitistic programs, as well as some of the
core ideas proposed thus far. The second part of the paper presents a model of
ultrafinitistic arithmetics based on the notion of fuzzy initial segments of
the standard natural numbers series. We also introduce a proof theory and a
semantics for ultrafinitism through which feasibly consistent theories can be
treated on the same footing as their classically consistent counterparts. We
conclude with a brief sketch of a foundational program, that aims at
reproducing the transfinite within the finite realm."
"We prove a conjecture by A. Pnueli and strengthen it showing a sequence of
""counting modalities"" none of which is expressible in the temporal logic
generated by the previous modalities, over the real line, or over the positive
reals. Moreover, there is no finite temporal logic that can express all of them
over the real line, so that no finite metric temporal logic is expressively
complete."
"A relational structure is a core, if all its endomorphisms are embeddings.
This notion is important for computational complexity classification of
constraint satisfaction problems. It is a fundamental fact that every finite
structure has a core, i.e., has an endomorphism such that the structure induced
by its image is a core; moreover, the core is unique up to isomorphism. Weprove
that every \omega -categorical structure has a core. Moreover, every
\omega-categorical structure is homomorphically equivalent to a model-complete
core, which is unique up to isomorphism, and which is finite or \omega
-categorical. We discuss consequences for constraint satisfaction with \omega
-categorical templates."
"The Paige and Tarjan algorithm (PT) for computing the coarsest refinement of
a state partition which is a bisimulation on some Kripke structure is well
known. It is also well known in model checking that bisimulation is equivalent
to strong preservation of CTL, or, equivalently, of Hennessy-Milner logic.
Drawing on these observations, we analyze the basic steps of the PT algorithm
from an abstract interpretation perspective, which allows us to reason on
strong preservation in the context of generic inductively defined (temporal)
languages and of possibly non-partitioning abstract models specified by
abstract interpretation. This leads us to design a generalized Paige-Tarjan
algorithm, called GPT, for computing the minimal refinement of an abstract
interpretation-based model that strongly preserves some given language. It
turns out that PT is a straight instance of GPT on the domain of state
partitions for the case of strong preservation of Hennessy-Milner logic. We
provide a number of examples showing that GPT is of general use. We first show
how a well-known efficient algorithm for computing stuttering equivalence can
be viewed as a simple instance of GPT. We then instantiate GPT in order to
design a new efficient algorithm for computing simulation equivalence that is
competitive with the best available algorithms. Finally, we show how GPT allows
to compute new strongly preserving abstract models by providing an efficient
algorithm that computes the coarsest refinement of a given partition that
strongly preserves the language generated by the reachability operator."
"It is a common knowledge that the integer functions definable in simply typed
lambda-calculus are exactly the extended polynomials. This is indeed the case
when one interprets integers over the type (p->p)->p->p where p is a base type
and/or equality is taken as beta-conversion. It is commonly believed that the
same holds for beta-eta equality and for integers represented over any fixed
type of the form (t->t)->t->t. In this paper we show that this opinion is not
quite true.
  We prove that the class of functions strictly definable in simply typed
lambda-calculus is considerably larger than the extended polynomials. Namely,
we define F as the class of strictly definable functions and G as a class that
contains extended polynomials and two additional functions, or more precisely,
two function schemas, and is closed under composition. We prove that G is a
subset of F.
  We conjecture that G exactly characterizes strictly definable functions, i.e.
G=F, and we gather some evidence for this conjecture proving, for example, that
every skewly representable finite range function is strictly representable over
(t->t)->t->t for some t."
"We prove that the inhabitation problem for rank two intersection types is
decidable, but (contrary to common belief) EXPTIME-hard. The exponential time
hardness is shown by reduction from the in-place acceptance problem for
alternating Turing machines."
"Let $F$ be the set of functions from an infinite set, $S$, to an ordered
ring, $R$. For $f$, $g$, and $h$ in $F$, the assertion $f = g + O(h)$ means
that for some constant $C$, $|f(x) - g(x)| \leq C |h(x)|$ for every $x$ in $S$.
Let $L$ be the first-order language with variables ranging over such functions,
symbols for $0, +, -, \min, \max$, and absolute value, and a ternary relation
$f = g + O(h)$. We show that the set of quantifier-free formulas in this
language that are valid in the intended class of interpretations is decidable,
and does not depend on the underlying set, $S$, or the ordered ring, $R$. If
$R$ is a subfield of the real numbers, we can add a constant 1 function, as
well as multiplication by constants from any computable subfield. We obtain
further decidability results for certain situations in which one adds symbols
denoting the elements of a fixed sequence of functions of strictly increasing
rates of growth."
"The authors' ATR programming formalism is a version of call-by-value PCF
under a complexity-theoretically motivated type system. ATR programs run in
type-2 polynomial-time and all standard type-2 basic feasible functionals are
ATR-definable (ATR types are confined to levels 0, 1, and 2). A limitation of
the original version of ATR is that the only directly expressible recursions
are tail-recursions. Here we extend ATR so that a broad range of affine
recursions are directly expressible. In particular, the revised ATR can fairly
naturally express the classic insertion- and selection-sort algorithms, thus
overcoming a sticking point of most prior implicit-complexity-based formalisms.
The paper's main work is in extending and simplifying the original
time-complexity semantics for ATR to develop a set of tools for extracting and
solving the higher-type recurrences arising from feasible affine recursions."
"The theorem of factorisation forests shows the existence of nested
factorisations -- a la Ramsey -- for finite words. This theorem has important
applications in semigroup theory, and beyond. The purpose of this paper is to
illustrate the importance of this approach in the context of automata over
infinite words and trees. We extend the theorem of factorisation forest in two
directions: we show that it is still valid for any word indexed by a linear
ordering; and we show that it admits a deterministic variant for words indexed
by well-orderings. A byproduct of this work is also an improvement on the known
bounds for the original result. We apply the first variant for giving a
simplified proof of the closure under complementation of rational sets of words
indexed by countable scattered linear orderings. We apply the second variant in
the analysis of monadic second-order logic over trees, yielding new results on
monadic interpretations over trees. Consequences of it are new caracterisations
of prefix-recognizable structures and of the Caucal hierarchy."
"In this paper, we study the model-checking and parameter synthesis problems
of the logic TCTL over discrete-timed automata where parameters are allowed
both in the model (timed automaton) and in the property (temporal formula). Our
results are as follows. On the negative side, we show that the model-checking
problem of TCTL extended with parameters is undecidable over discrete-timed
automata with only one parametric clock. The undecidability result needs
equality in the logic. On the positive side, we show that the model-checking
and the parameter synthesis problems become decidable for a fragment of the
logic where equality is not allowed. Our method is based on automata theoretic
principles and an extension of our method to express durations of runs in timed
automata using Presburger arithmetic."
"The study of finite automata and regular languages is a privileged meeting
point of algebra and logic. Since the work of Buchi, regular languages have
been classified according to their descriptive complexity, i.e. the type of
logical formalism required to define them. The algebraic point of view on
automata is an essential complement of this classification: by providing
alternative, algebraic characterizations for the classes, it often yields the
only opportunity for the design of algorithms that decide expressibility in
some logical fragment.
  We survey the existing results relating the expressibility of regular
languages in logical fragments of MSO[S] with algebraic properties of their
minimal automata. In particular, we show that many of the best known results in
this area share the same underlying mechanics and rely on a very strong
relation between logical substitutions and block-products of pseudovarieties of
monoid. We also explain the impact of these connections on circuit complexity
theory."
"In this paper we consider the specification and verification of
infinite-state systems using temporal logic. In particular, we describe
parameterised systems using a new variety of first-order temporal logic that is
both powerful enough for this form of specification and tractable enough for
practical deductive verification. Importantly, the power of the temporal
language allows us to describe (and verify) asynchronous systems, communication
delays and more complex properties such as liveness and fairness properties.
These aspects appear difficult for many other approaches to infinite-state
verification."
"The biological process of gene assembly has been modeled based on three types
of string rewriting rules, called string pointer rules, defined on so-called
legal strings. It has been shown that reduction graphs, graphs that are based
on the notion of breakpoint graph in the theory of sorting by reversal, for
legal strings provide valuable insights into the gene assembly process. We
characterize which legal strings obtain the same reduction graph (up to
isomorphism), and moreover we characterize which graphs are (isomorphic to)
reduction graphs."
"Reactivity is an essential property of a synchronous program. Informally, it
guarantees that at each instant the program fed with an input will `react'
producing an output. In the present work, we consider a refined property that
we call ` feasible reactivity'. Beyond reactivity, this property guarantees
that at each instant both the size of the program and its reaction time are
bounded by a polynomial in the size of the parameters at the beginning of the
computation and the size of the largest input. We propose a method to annotate
programs and we develop related static analysis techniques that guarantee
feasible reactivity for programs expressed in the S-pi-calculus. The latter is
a synchronous version of the pi-calculus based on the SL synchronous
programming model."
"In recent years, several efforts have been made to enhance conceptual data
modelling with automated reasoning to improve the model's quality and derive
implicit information. One approach to achieve this in implementations, is to
constrain the language. Advances in Description Logics can help choosing the
right language to have greatest expressiveness yet to remain within the
decidable fragment of first order logic to realise a workable implementation
with good performance using DL reasoners. The best fit DL language appears to
be the ExpTime-complete DLRifd. To illustrate trade-offs and highlight features
of the modelling languages, we present a precise transformation of the mappable
features of the very expressive (undecidable) ORM/ORM2 conceptual data
modelling languages to exactly DLRifd. Although not all ORM2 features can be
mapped, this is an interesting fragment because it has been shown that DLRifd
can also encode UML Class Diagrams and EER, and therefore can foster
interoperation between conceptual data models and research into ontological
aspects of the modelling languages."
"Bedwyr is a generalization of logic programming that allows model checking
directly on syntactic expressions possibly containing bindings. This system,
written in OCaml, is a direct implementation of two recent advances in the
theory of proof search. The first is centered on the fact that both finite
success and finite failure can be captured in the sequent calculus by
incorporating inference rules for definitions that allow fixed points to be
explored. As a result, proof search in such a sequent calculus can capture
simple model checking problems as well as may and must behavior in operational
semantics. The second is that higher-order abstract syntax is directly
supported using term-level $\lambda$-binders and the $\nabla$ quantifier. These
features allow reasoning directly on expressions containing bound variables."
"This paper concerns the explicit treatment of substitutions in the lambda
calculus. One of its contributions is the simplification and rationalization of
the suspension calculus that embodies such a treatment. The earlier version of
this calculus provides a cumbersome encoding of substitution composition, an
operation that is important to the efficient realization of reduction. This
encoding is simplified here, resulting in a treatment that is easy to use
directly in applications. The rationalization consists of the elimination of a
practically inconsequential flexibility in the unravelling of substitutions
that has the inadvertent side effect of losing contextual information in terms;
the modified calculus now has a structure that naturally supports logical
analyses, such as ones related to the assignment of types, over lambda terms.
The overall calculus is shown to have pleasing theoretical properties such as a
strongly terminating sub-calculus for substitution and confluence even in the
presence of term meta variables that are accorded a grafting interpretation.
Another contribution of the paper is the identification of a broad set of
properties that are desirable for explicit substitution calculi to support and
a classification of a variety of proposed systems based on these. The
suspension calculus is used as a tool in this study. In particular, mappings
are described between it and the other calculi towards understanding the
characteristics of the latter."
"Formal models for gene assembly in ciliates have been developed, in
particular the string pointer reduction system (SPRS) and the graph pointer
reduction system (GPRS). The reduction graph is a valuable tool within the
SPRS, revealing much information about how gene assembly is performed for a
given gene. The GPRS is more abstract than the SPRS and not all information
present in the SPRS is retained in the GPRS. As a consequence the reduction
graph cannot be defined for the GPRS in general, but we show that it can be
defined (in an equivalent manner as defined for the SPRS) if we restrict
ourselves to so-called realistic overlap graphs. Fortunately, only these graphs
correspond to genes occurring in nature. Defining the reduction graph within
the GPRS allows one to carry over several results within the SPRS that rely on
the reduction graph."
"In modern mathematics, graphs figure as one of the better-investigated class
of mathematical objects. Various properties of graphs, as well as
graph-processing algorithms, can be useful if graphs of a certain kind are used
as denotations for CF-grammars. Furthermore, graph are well adapted to various
extensions (one kind of such extensions being attributes)."
"We consider a new kind of interpretation over relational structures: finite
sets interpretations. Those interpretations are defined by weak monadic
second-order (WMSO) formulas with free set variables. They transform a given
structure into a structure with a domain consisting of finite sets of elements
of the orignal structure. The definition of these interpretations directly
implies that they send structures with a decidable WMSO theory to structures
with a decidable first-order theory. In this paper, we investigate the
expressive power of such interpretations applied to infinite deterministic
trees. The results can be used in the study of automatic and tree-automatic
structures."
"A worst-case ExpTime tableau-based decision procedure is outlined for the
satisfiability problem in $\mathcal{ALCQI}$ w.r.t. general axioms."
"String languages recognizable in (deterministic) log-space are characterized
either by two-way (deterministic) multi-head automata, or following Immerman,
by first-order logic with (deterministic) transitive closure. Here we elaborate
this result, and match the number of heads to the arity of the transitive
closure. More precisely, first-order logic with k-ary deterministic transitive
closure has the same power as deterministic automata walking on their input
with k heads, additionally using a finite set of nested pebbles. This result is
valid for strings, ordered trees, and in general for families of graphs having
a fixed automaton that can be used to traverse the nodes of each of the graphs
in the family. Other examples of such families are grids, toruses, and
rectangular mazes. For nondeterministic automata, the logic is restricted to
positive occurrences of transitive closure.
  The special case of k=1 for trees, shows that single-head deterministic
tree-walking automata with nested pebbles are characterized by first-order
logic with unary deterministic transitive closure. This refines our earlier
result that placed these automata between first-order and monadic second-order
logic on trees."
"We study an untyped lambda calculus with quantum data and classical control.
This work stems from previous proposals by Selinger and Valiron and by Van
Tonder. We focus on syntax and expressiveness, rather than (denotational)
semantics. We prove subject reduction, confluence and a standardization
theorem. Moreover, we prove the computational equivalence of the proposed
calculus with a suitable class of quantum circuit families."
"Bi-intuitionistic logic is the extension of intuitionistic logic with a
connective dual to implication. Bi-intuitionistic logic was introduced by
Rauszer as a Hilbert calculus with algebraic and Kripke semantics. But her
subsequent ``cut-free'' sequent calculus for BiInt has recently been shown by
Uustalu to fail cut-elimination. We present a new cut-free sequent calculus for
BiInt, and prove it sound and complete with respect to its Kripke semantics.
Ensuring completeness is complicated by the interaction between implication and
its dual, similarly to future and past modalities in tense logic. Our calculus
handles this interaction using extended sequents which pass information from
premises to conclusions using variables instantiated at the leaves of failed
derivation trees. Our simple termination argument allows our calculus to be
used for automated deduction, although this is not its main purpose."
"We extend our approach to abstract syntax (with binding constructions)
through modules and linearity. First we give a new general definition of arity,
yielding the companion notion of signature. Then we obtain a modularity result
as requested by Ghani and Uustalu (2003): in our setting, merging two
extensions of syntax corresponds to building an amalgamated sum. Finally we
define a natural notion of equation concerning a signature and prove the
existence of an initial semantics for a so-called representable signature
equipped with a set of equations."
"We propose two alternatives to Xu's axiomatization of the Chellas STIT. The
first one also provides an alternative axiomatization of the deliberative STIT.
The second one starts from the idea that the historic necessity operator can be
defined as an abbreviation of operators of agency, and can thus be eliminated
from the logic of the Chellas STIT. The second axiomatization also allows us to
establish that the problem of deciding the satisfiability of a STIT formula
without temporal operators is NP-complete in the single-agent case, and is
NEXPTIME-complete in the multiagent case, both for the deliberative and the
Chellas' STIT."
"Higher-Order Fixpoint Logic (HFL) is a hybrid of the simply typed
\lambda-calculus and the modal \lambda-calculus. This makes it a highly
expressive temporal logic that is capable of expressing various interesting
correctness properties of programs that are not expressible in the modal
\lambda-calculus.
  This paper provides complexity results for its model checking problem. In
particular we consider those fragments of HFL built by using only types of
bounded order k and arity m. We establish k-fold exponential time completeness
for model checking each such fragment. For the upper bound we use fixpoint
elimination to obtain reachability games that are singly-exponential in the
size of the formula and k-fold exponential in the size of the underlying
transition system. These games can be solved in deterministic linear time. As a
simple consequence, we obtain an exponential time upper bound on the expression
complexity of each such fragment.
  The lower bound is established by a reduction from the word problem for
alternating (k-1)-fold exponential space bounded Turing Machines. Since there
are fixed machines of that type whose word problems are already hard with
respect to k-fold exponential time, we obtain, as a corollary, k-fold
exponential time completeness for the data complexity of our fragments of HFL,
provided m exceeds 3. This also yields a hierarchy result in expressive power."
"This article discusses two books on the topic of alternative logics in
science: ""Deviant Logic"", by Susan Haack, and ""Alternative Logics: Do Sciences
Need Them?"", edited by Paul Weingartner."
"Linearly bounded Turing machines have been mainly studied as acceptors for
context-sensitive languages. We define a natural class of infinite automata
representing their observable computational behavior, called linearly bounded
graphs. These automata naturally accept the same languages as the linearly
bounded machines defining them. We present some of their structural properties
as well as alternative characterizations in terms of rewriting systems and
context-sensitive transductions. Finally, we compare these graphs to rational
graphs, which are another class of automata accepting the context-sensitive
languages, and prove that in the bounded-degree case, rational graphs are a
strict sub-class of linearly bounded graphs."
"We define a new decidable logic for expressing and checking invariants of
programs that manipulate dynamically-allocated objects via pointers and
destructive pointer updates. The main feature of this logic is the ability to
limit the neighborhood of a node that is reachable via a regular expression
from a designated node. The logic is closed under boolean operations
(entailment, negation) and has a finite model property. The key technical
result is the proof of decidability. We show how to express precondition,
postconditions, and loop invariants for some interesting programs. It is also
possible to express properties such as disjointness of data-structures, and
low-level heap mutations. Moreover, our logic can express properties of
arbitrary data-structures and of an arbitrary number of pointer fields. The
latter provides a way to naturally specify postconditions that relate the
fields on entry to a procedure to the fields on exit. Therefore, it is possible
to use the logic to automatically prove partial correctness of programs
performing low-level heap mutations."
"We consider the problem of symbolic reachability analysis of higher-order
context-free processes. These models are generalizations of the context-free
processes (also called BPA processes) where each process manipulates a data
structure which can be seen as a nested stack of stacks. Our main result is
that, for any higher-order context-free process, the set of all predecessors of
a given regular set of configurations is regular and effectively constructible.
This result generalizes the analogous result which is known for level 1
context-free processes. We show that this result holds also in the case of
backward reachability analysis under a regular constraint on configurations. As
a corollary, we obtain a symbolic model checking algorithm for the temporal
logic E(U,X) with regular atomic predicates, i.e., the fragment of CTL
restricted to the EU and EX modalities."
"Several types of term rewriting systems can be distinguished by the way their
rules overlap. In particular, we define the classes of prefix, suffix,
bottom-up and top-down systems, which generalize similar classes on words. Our
aim is to study the derivation relation of such systems (i.e. the reflexive and
transitive closure of their rewriting relation) and, if possible, to provide a
finite mechanism characterizing it. Using a notion of rational relations based
on finite graph grammars, we show that the derivation of any bottom-up,
top-down or suffix systems is rational, while it can be non recursive for
prefix systems."
"The characterization of second-order type isomorphisms is a purely
syntactical problem that we propose to study under the enlightenment of game
semantics. We study this question in the case of second-order
&#955;$\mu$-calculus, which can be seen as an extension of system F to
classical logic, and for which we de&#64257;ne a categorical framework: control
hyperdoctrines. Our game model of &#955;$\mu$-calculus is based on polymorphic
arenas (closely related to Hughes' hyperforests) which evolve during the play
(following the ideas of Murawski-Ong). We show that type isomorphisms coincide
with the ""equality"" on arenas associated with types. Finally we deduce the
equational characterization of type isomorphisms from this equality. We also
recover from the same model Roberto Di Cosmo's characterization of type
isomorphisms for system F. This approach leads to a geometrical comprehension
on the question of second order type isomorphisms, which can be easily extended
to some other polymorphic calculi including additional programming features."
"Curry-style system F, ie. system F with no explicit types in terms, can be
seen as a core presentation of polymorphism from the point of view of
programming languages. This paper gives a characterisation of type isomorphisms
for this language, by using a game model whose intuitions come both from the
syntax and from the game semantics universe. The model is composed of: an
untyped part to interpret terms, a notion of game to interpret types, and a
typed part to express the fact that an untyped strategy plays on a game. By
analysing isomorphisms in the model, we prove that the equational system
corresponding to type isomorphisms for Curry-style system F is the extension of
the equational system for Church-style isomorphisms with a new, non-trivial
equation: forall X.A = A[forall Y.Y/X] if X appears only positively in A."
"In this paper we present an algorithm for performing runtime verification of
a bounded temporal logic over timed runs. The algorithm consists of three
elements. First, the bounded temporal formula to be verified is translated into
a monadic first-order logic over difference inequalities, which we call monadic
difference logic. Second, at each step of the timed run, the monadic difference
formula is modified by computing a quotient with the state and time of that
step. Third, the resulting formula is checked for being a tautology or being
unsatisfiable by a decision procedure for monadic difference logic.
  We further provide a simple decision procedure for monadic difference logic
based on the data structure Difference Decision Diagrams. The algorithm is
complete in a very strong sense on a subclass of temporal formulae
characterized as homogeneously monadic and it is approximate on other formulae.
The approximation comes from the fact that not all unsatisfiable or
tautological formulae are recognised at the earliest possible time of the
runtime verification.
  Contrary to existing approaches, the presented algorithms do not work by
syntactic rewriting but employ efficient decision structures which make them
applicable in real applications within for instance business software."
"The paper presents probabilistic extensions of interval temporal logic (ITL)
and duration calculus (DC) with infinite intervals and complete Hilbert-style
proof systems for them. The completeness results are a strong completeness
theorem for the system of probabilistic ITL with respect to an abstract
semantics and a relative completeness theorem for the system of probabilistic
DC with respect to real-time semantics. The proposed systems subsume
probabilistic real-time DC as known from the literature. A correspondence
between the proposed systems and a system of probabilistic interval temporal
logic with finite intervals and expanding modalities is established too."
"The notion of innocent strategy was introduced by Hyland and Ong in order to
capture the interactive behaviour of lambda-terms and PCF programs. An innocent
strategy is defined as an alternating strategy with partial memory, in which
the strategy plays according to its view. Extending the definition to
non-alternating strategies is problematic, because the traditional definition
of views is based on the hypothesis that Opponent and Proponent alternate
during the interaction. Here, we take advantage of the diagrammatic
reformulation of alternating innocence in asynchronous games, in order to
provide a tentative definition of innocence in non-alternating games. The task
is interesting, and far from easy. It requires the combination of true
concurrency and game semantics in a clean and organic way, clarifying the
relationship between asynchronous games and concurrent games in the sense of
Abramsky and Melli\`es. It also requires an interactive reformulation of the
usual acyclicity criterion of linear logic, as well as a directed variant, as a
scheduling criterion."
"Model checking properties are often described by means of finite automata.
Any particular such automaton divides the set of infinite trees into finitely
many classes, according to which state has an infinite run. Building the full
type hierarchy upon this interpretation of the base type gives a finite
semantics for simply-typed lambda-trees.
  A calculus based on this semantics is proven sound and complete. In
particular, for regular infinite lambda-trees it is decidable whether a given
automaton has a run or not. As regular lambda-trees are precisely recursion
schemes, this decidability result holds for arbitrary recursion schemes of
arbitrary level, without any syntactical restriction."
"The notion of abstract Boehm tree has arisen as an operationally-oriented
distillation of works on game semantics, and has been investigated in two
papers. This paper revisits the notion, providing more syntactic support and
more examples (like call-by-value evaluation) illustrating the generality of
the underlying computing device. Precise correspondences between various
formulations of the evaluation mechanism of abstract Boehm trees are
established."
"A logic calculus is presented that is a conservative extension of linear
logic. The motivation beneath this work concerns lazy evaluation, true
concurrency and interferences in proof search. The calculus includes two new
connectives to deal with multisequent structures and has the cut-elimination
property. Extensions are proposed that give first results concerning our
objectives."
"We compare the expressiveness of two extensions of monadic second-order logic
(MSO) over the class of finite structures. The first, counting monadic
second-order logic (CMSO), extends MSO with first-order modulo-counting
quantifiers, allowing the expression of queries like ``the number of elements
in the structure is even''. The second extension allows the use of an
additional binary predicate, not contained in the signature of the queried
structure, that must be interpreted as an arbitrary linear order on its
universe, obtaining order-invariant MSO.
  While it is straightforward that every CMSO formula can be translated into an
equivalent order-invariant MSO formula, the converse had not yet been settled.
Courcelle showed that for restricted classes of structures both order-invariant
MSO and CMSO are equally expressive, but conjectured that, in general,
order-invariant MSO is stronger than CMSO.
  We affirm this conjecture by presenting a class of structures that is
order-invariantly definable in MSO but not definable in CMSO."
"The S-pi-calculus is a synchronous pi-calculus which is based on the SL
model. The latter is a relaxation of the Esterel model where the reaction to
the absence of a signal within an instant can only happen at the next instant.
In the present work, we present and characterise a compositional semantics of
the S-pi-calculus based on suitable notions of labelled transition system and
bisimulation. Based on this semantic framework, we explore the notion of
determinacy and the related one of (local) confluence."
"Over the last 25 years, a lot of work has been done on seeking for decidable
non-regular extensions of Propositional Dynamic Logic (PDL). Only recently, an
expressive extension of PDL, allowing visibly pushdown automata (VPAs) as a
formalism to describe programs, was introduced and proven to have a
satisfiability problem complete for deterministic double exponential time.
Lately, the VPA formalism was extended to so called k-phase multi-stack visibly
pushdown automata (k-MVPAs). Similarly to VPAs, it has been shown that the
language of k-MVPAs have desirable effective closure properties and that the
emptiness problem is decidable. On the occasion of introducing k-MVPAs, it has
been asked whether the extension of PDL with k-MVPAs still leads to a decidable
logic. This question is answered negatively here. We prove that already for the
extension of PDL with 2-phase MVPAs with two stacks satisfiability becomes
\Sigma_1^1-complete."
"We present a process algebra based approach to formalize the interactions of
computing devices such as the representation of policies and the resolution of
conflicts. As an example we specify how promises may be used in coming to an
agreement regarding a simple though practical transportation problem."
"For many a natural deduction style logic there is a Hilbert-style logic that
is equivalent to it in that it has the same theorems (i.e. valid judgements
with empty contexts). For intuitionistic logic, the axioms of the equivalent
Hilbert-style logic can be propositions which are also known as the types of
the combinators I, K and S. Hilbert-style versions of illative combinatory
logic have formulations with axioms that are actual type statements for I, K
and S. As pure type systems (PTSs)are, in a sense, equivalent to systems of
illative combinatory logic, it might be thought that Hilbert-style PTSs (HPTSs)
could be based in a similar way. This paper shows that some PTSs have very
trivial equivalent HPTSs, with only the axioms as theorems and that for many
PTSs no equivalent HPTS can exist. Most commonly used PTSs belong to these two
classes. For some PTSs however, including lambda* and the PTS at the basis of
the proof assistant Coq, there is a nontrivial equivalent HPTS, with axioms
that are type statements for I, K and S."
"It is commonly agreed that the success of future proof assistants will rely
on their ability to incorporate computations within deduction in order to mimic
the mathematician when replacing the proof of a proposition P by the proof of
an equivalent proposition P' obtained from P thanks to possibly complex
calculations. In this paper, we investigate a new version of the calculus of
inductive constructions which incorporates arbitrary decision procedures into
deduction via the conversion rule of the calculus. The novelty of the problem
in the context of the calculus of inductive constructions lies in the fact that
the computation mechanism varies along proof-checking: goals are sent to the
decision procedure together with the set of user hypotheses available from the
current context. Our main result shows that this extension of the calculus of
constructions does not compromise its main properties: confluence, subject
reduction, strong normalization and consistency are all preserved."
"The notion of computability closure has been introduced for proving the
termination of higher-order rewriting with first-order matching by Jean-Pierre
Jouannaud and Mitsuhiro Okada in a 1997 draft which later served as a basis for
the author's PhD. In this paper, we show how this notion can also be used for
dealing with beta-normalized rewriting with matching modulo beta-eta (on
patterns \`a la Miller), rewriting with matching modulo some equational theory,
and higher-order data types (types with constructors having functional
recursive arguments). Finally, we show how the computability closure can easily
be turned into a reduction ordering which, in the higher-order case, contains
Jean-Pierre Jouannaud and Albert Rubio's higher-order recursive path ordering
and, in the first-order case, is equal to the usual first-order recursive path
ordering."
"We propose a set theory strong enough to interpret powerful type theories
underlying proof assistants such as LEGO and also possibly Coq, which at the
same time enables program extraction from its constructive proofs. For this
purpose, we axiomatize an impredicative constructive version of
Zermelo-Fraenkel set theory IZF with Replacement and $\omega$-many
inaccessibles, which we call \izfio. Our axiomatization utilizes set terms, an
inductive definition of inaccessible sets and the mutually recursive nature of
equality and membership relations. It allows us to define a weakly-normalizing
typed lambda calculus corresponding to proofs in \izfio according to the
Curry-Howard isomorphism principle. We use realizability to prove the
normalization theorem, which provides a basis for program extraction
capability."
"A construction of fully abstract typed models for PCF and PCF^+ (i.e., PCF +
""parallel conditional function""), respectively, is presented. It is based on
general notions of sequential computational strategies and wittingly consistent
non-deterministic strategies introduced by the author in the seventies.
Although these notions of strategies are old, the definition of the fully
abstract models is new, in that it is given level-by-level in the finite type
hierarchy. To prove full abstraction and non-dcpo domain theoretic properties
of these models, a theory of computational strategies is developed. This is
also an alternative and, in a sense, an analogue to the later game strategy
semantics approaches of Abramsky, Jagadeesan, and Malacaria; Hyland and Ong;
and Nickau. In both cases of PCF and PCF^+ there are definable universal
(surjective) functionals from numerical functions to any given type,
respectively, which also makes each of these models unique up to isomorphism.
Although such models are non-omega-complete and therefore not continuous in the
traditional terminology, they are also proved to be sequentially complete (a
weakened form of omega-completeness), ""naturally"" continuous (with respect to
existing directed ""pointwise"", or ""natural"" lubs) and also ""naturally""
omega-algebraic and ""naturally"" bounded complete -- appropriate generalisation
of the ordinary notions of domain theory to the case of non-dcpos."
"In earlier work, the Abstract State Machine Thesis -- that arbitrary
algorithms are behaviorally equivalent to abstract state machines -- was
established for several classes of algorithms, including ordinary, interactive,
small-step algorithms. This was accomplished on the basis of axiomatizations of
these classes of algorithms. Here we extend the axiomatization and, in a
companion paper, the proof, to cover interactive small-step algorithms that are
not necessarily ordinary. This means that the algorithms (1) can complete a
step without necessarily waiting for replies to all queries from that step and
(2) can use not only the environment's replies but also the order in which the
replies were received."
"In earlier work, the Abstract State Machine Thesis -- that arbitrary
algorithms are behaviorally equivalent to abstract state machines -- was
established for several classes of algorithms, including ordinary, interactive,
small-step algorithms. This was accomplished on the basis of axiomatizations of
these classes of algorithms. In Part I (Interactive Small-Step Algorithms I:
Axiomatization), the axiomatization was extended to cover interactive
small-step algorithms that are not necessarily ordinary. This means that the
algorithms (1) can complete a step without necessarily waiting for replies to
all queries from that step and (2) can use not only the environment's replies
but also the order in which the replies were received. In order to prove the
thesis for algorithms of this generality, we extend here the definition of
abstract state machines to incorporate explicit attention to the relative
timing of replies and to the possible absence of replies. We prove the
characterization theorem for extended abstract state machines with respect to
general algorithms as axiomatized in Part I."
"De Vrijer has presented a proof of the finite developments theorem which, in
addition to showing that all developments are finite, gives an effective
reduction strategy computing longest developments as well as a simple formula
computing their length.
  We show that by applying a rather simple and intuitive principle of duality
to de Vrijer's approach one arrives at a proof that some developments are
finite which in addition yields an effective reduction strategy computing
shortest developments as well as a simple formula computing their length. The
duality fails for general beta-reduction.
  Our results simplify previous work by Khasidashvili."
"Automated theorem provers are used in extended static checking, where they
are the performance bottleneck. Extended static checkers are run typically
after incremental changes to the code. We propose to exploit this usage pattern
to improve performance. We present two approaches of how to do so and a full
solution."
"We describe a remarkable relation between the notion of valid formula of
predicate logic and the specification of network protocols. We give several
examples such as the acknowledgement of one packet or of a sequence of packets.
We show how to specify the composition of protocols."
"We consider approximating data structures with collections of the items that
they contain. For examples, lists, binary trees, tuples, etc, can be
approximated by sets or multisets of the items within them. Such approximations
can be used to provide partial correctness properties of logic programs. For
example, one might wish to specify than whenever the atom $sort(t,s)$ is proved
then the two lists $t$ and $s$ contain the same multiset of items (that is, $s$
is a permutation of $t$). If sorting removes duplicates, then one would like to
infer that the sets of items underlying $t$ and $s$ are the same. Such results
could be useful to have if they can be determined statically and automatically.
We present a scheme by which such collection analysis can be structured and
automated. Central to this scheme is the use of linear logic as a omputational
logic underlying the logic of Horn clauses."
"A focused proof system provides a normal form to cut-free proofs that
structures the application of invertible and non-invertible inference rules.
The focused proof system of Andreoli for linear logic has been applied to both
the proof search and the proof normalization approaches to computation. Various
proof systems in literature exhibit characteristics of focusing to one degree
or another. We present a new, focused proof system for intuitionistic logic,
called LJF, and show how other proof systems can be mapped into the new system
by inserting logical connectives that prematurely stop focusing. We also use
LJF to design a focused proof system for classical logic. Our approach to the
design and analysis of these systems is based on the completeness of focusing
in linear logic and on the notion of polarity that appears in Girard's LC and
LU proof systems."
"For a two-variable formula &psi;(X,Y) of Monadic Logic of Order (MLO) the
Church Synthesis Problem concerns the existence and construction of an operator
Y=F(X) such that &psi;(X,F(X)) is universally valid over Nat.
  B\""{u}chi and Landweber proved that the Church synthesis problem is
decidable; moreover, they showed that if there is an operator F that solves the
Church Synthesis Problem, then it can also be solved by an operator defined by
a finite state automaton or equivalently by an MLO formula. We investigate a
parameterized version of the Church synthesis problem. In this version &psi;
might contain as a parameter a unary predicate P. We show that the Church
synthesis problem for P is computable if and only if the monadic theory of
<Nat,<,P> is decidable. We prove that the B\""{u}chi-Landweber theorem can be
extended only to ultimately periodic parameters. However, the MLO-definability
part of the B\""{u}chi-Landweber theorem holds for the parameterized version of
the Church synthesis problem."
"This paper provides a new, decidable definition of the higher- order
recursive path ordering in which type comparisons are made only when needed,
therefore eliminating the need for the computability clo- sure, and bound
variables are handled explicitly, making it possible to handle recursors for
arbitrary strictly positive inductive types."
"Trace semantics has been defined for various kinds of state-based systems,
notably with different forms of branching such as non-determinism vs.
probability. In this paper we claim to identify one underlying mathematical
structure behind these ""trace semantics,"" namely coinduction in a Kleisli
category. This claim is based on our technical result that, under a suitably
order-enriched setting, a final coalgebra in a Kleisli category is given by an
initial algebra in the category Sets. Formerly the theory of coalgebras has
been employed mostly in Sets where coinduction yields a finer process semantics
of bisimilarity. Therefore this paper extends the application field of
coalgebras, providing a new instance of the principle ""process semantics via
coinduction."""
"We consider the following \emph{model repair problem}: given a finite Kripke
structure $M$ and a specification formula $\eta$ in some modal or temporal
logic, determine if $M$ contains a substructure $M'$ (with the same initial
state) that satisfies $\eta$. Thus, $M$ can be ``repaired'' to satisfy the
specification $\eta$ by deleting some transitions.
  We map an instance $(M, \eta)$ of model repair to a boolean formula
$\repfor(M,\eta)$ such that $(M, \eta)$ has a solution iff $\repfor(M,\eta)$ is
satisfiable. Furthermore, a satisfying assignment determines which transitions
must be removed from $M$ to generate a model $M'$ of $\eta$. Thus, we can use
any SAT solver to repair Kripke structures. Furthermore, using a complete SAT
solver yields a complete algorithm: it always finds a repair if one exists.
  We extend our method to repair finite-state shared memory concurrent
programs, to solve the discrete event supervisory control problem
\cite{RW87,RW89}, to check for the existence of symmettric solutions
\cite{ES93}, and to accomodate any boolean constraint on the existence of
states and transitions in the repaired model.
  Finally, we show that model repair is NP-complete for CTL, and logics with
polynomial model checking algorithms to which CTL can be reduced in polynomial
time. A notable example of such a logic is Alternating-Time Temporal Logic
(ATL)."
"This paper presents the design of a novel distributed algorithm d-IRA for the
reachability analysis of linear hybrid automata. Recent work on iterative
relaxation abstraction (IRA) is leveraged to distribute the computational
problem among multiple computational nodes in a non-redundant manner by
performing careful infeasibility analysis of linear programs corresponding to
spurious counterexamples. The d-IRA algorithm is resistant to failure of
multiple computational nodes. The experimental results provide promising
evidence for the possible successful application of this technique."
"In 2002 Jurdzinski and Lorys settled a long-standing conjecture that
palindromes are not a Church-Rosser language. Their proof required a
sophisticated theory about computation graphs of 2-stack automata. We present
their proof in terms of 1-tape Turing machines.We also provide an alternative
proof of Buntrock and Otto's result that the set of non-square bitstrings,
which is context-free, is not Church-Rosser."
"Current algorithms for bounded model checking use SAT methods for checking
satisfiability of Boolean formulae. These methods suffer from the potential
memory explosion problem. Methods based on the validity of Quantified Boolean
Formulae (QBF) allow an exponentially more succinct representation of formulae
to be checked, because no ""unrolling"" of the transition relation is required.
These methods have not been widely used, because of the lack of an efficient
decision procedure for QBF. We evaluate the usage of QBF in bounded model
checking (BMC), using general-purpose SAT and QBF solvers. We develop a
special-purpose decision procedure for QBF used in BMC, and compare our
technique with the methods using general-purpose SAT and QBF solvers on
real-life industrial benchmarks."
"We describe verification techniques for embedded memory systems using
efficient memory modeling (EMM), without explicitly modeling each memory bit.
We extend our previously proposed approach of EMM in Bounded Model Checking
(BMC) for a single read/write port single memory system, to more commonly
occurring systems with multiple memories, having multiple read and write ports.
More importantly, we augment such EMM to providing correctness proofs, in
addition to finding real bugs as before. The novelties of our verification
approach are in a) combining EMM with proof-based abstraction that preserves
the correctness of a property up to a certain analysis depth of SAT-based BMC,
and b) modeling arbitrary initial memory state precisely and thereby, providing
inductive proofs using SAT-based BMC for embedded memory systems. Similar to
the previous approach, we construct a verification model by eliminating memory
arrays, but retaining the memory interface signals with their control logic and
adding constraints on those signals at every analysis depth to preserve the
data forwarding semantics. The size of these EMM constraints depends
quadratically on the number of memory accesses and the number of read and write
ports; and linearly on the address and data widths and the number of memories.
We show the effectiveness of our approach on several industry designs and
software programs."
"Development of energy and performance-efficient embedded software is
increasingly relying on application of complex transformations on the critical
parts of the source code. Designers applying such nontrivial source code
transformations are often faced with the problem of ensuring functional
equivalence of the original and transformed programs. Currently they have to
rely on incomplete and time-consuming simulation. Formal automatic verification
of the transformed program against the original is instead desirable. This
calls for equivalence checking tools similar to the ones available for
comparing digital circuits. We present such a tool to compare array-intensive
programs related through a combination of important global transformations like
expression propagations, loop and algebraic transformations. When the
transformed program fails to pass the equivalence check, the tool provides
specific feedback on the possible locations of errors."
"We propose an approach to optimally synthesize quantum circuits from
non-permutative quantum gates such as Controlled-Square-Root-of-Not (i.e.
Controlled-V). Our approach reduces the synthesis problem to multiple-valued
optimization and uses group theory. We devise a novel technique that transforms
the quantum logic synthesis problem from a multi-valued constrained
optimization problem to a group permutation problem. The transformation enables
us to utilize group theory to exploit the properties of the synthesis problem.
Assuming a cost of one for each two-qubit gate, we found all reversible
circuits with quantum costs of 4, 5, 6, etc, and give another algorithm to
realize these reversible circuits with quantum gates."
"This paper describes an improved approach to Boolean network optimization
using internal don't-cares. The improvements concern the type of don't-cares
computed, their scope, and the computation method. Instead of the traditionally
used compatible observability don't-cares (CODCs), we introduce and justify the
use of complete don't-cares (CDC). To ensure the robustness of the don't-care
computation for very large industrial networks, a optional windowing scheme is
implemented that computes substantial subsets of the CDCs in reasonable time.
Finally, we give a SAT-based don't-care computation algorithm that is more
efficient than BDD-based algorithms. Experimental results confirm that these
improvements work well in practice. Complete don't-cares allow for a reduction
in the number of literals compared to the CODCs. Windowing guarantees
robustness, even for very large benchmarks on which previous methods could not
be applied. SAT reduces the runtime and enhances robustness, making don't-cares
affordable for a variety of other Boolean methods applied to the network."
"Automated synthesis of monitors from high-level properties plays a
significant role in assertion-based verification. We present here a methodology
to synthesize assertion monitors from visual specifications given in CESC
(Clocked Event Sequence Chart). CESC is a visual language designed for
specifying system level interactions involving single and multiple clock
domains. It has well-defined graphical and textual syntax and formal semantics
based on synchronous language paradigm enabling formal analysis of
specifications. In this paper we provide an overview of CESC language with few
illustrative examples. The algorithm for automated synthesis of assertion
monitors from CESC specifications is described. A few examples from standard
bus protocols (OCP-IP and AMBA) are presented to demonstrate the application of
monitor synthesis algorithm."
"A class of discrete event synthesis problems can be reduced to solving
language equations f . X &sube; S, where F is the fixed component and S the
specification. Sequential synthesis deals with FSMs when the automata for F and
S are prefix closed, and are naturally represented by multi-level networks with
latches. For this special case, we present an efficient computation, using
partitioned representations, of the most general prefix-closed solution of the
above class of language equations. The transition and the output relations of
the FSMs for F and S in their partitioned form are represented by the sets of
output and next state functions of the corresponding networks. Experimentally,
we show that using partitioned representations is much faster than using
monolithic representations, as well as applicable to larger problem instances."
"Static program analysis by abstract interpretation is an efficient method to
determine properties of embedded software. One example is value analysis, which
determines the values stored in the processor registers. Its results are used
as input to more advanced analyses, which ultimately yield information about
the stack usage and the timing behavior of embedded software."
"Formal verification techniques have been playing an important role in
pre-silicon validation processes. One of the most important points considered
in performing formal verification is to define good verification scopes; we
should define clearly what to be verified formally upon designs under tests. We
considered the following three practical requirements when we defined the scope
of formal verification. They are (a) hard to verify (b) small to handle, and
(c) easy to understand. Our novel approach is to break down generic properties
for system into stereotype properties in block level and to define requirements
for Verifiable RTL. Consequently, each designer instead of verification experts
can describe properties of the design easily, and formal model checking can be
applied systematically and thoroughly to all the leaf modules. During the
development of a component chip for server platforms, we focused on RAS
(Reliability, Availability, and Serviceability) features and described more
than 2000 properties in PSL. As a result of the formal verification, we found
several critical logic bugs in a short time with limited resources, and
successfully verified all of them. This paper presents a study of the
functional verification methodology."
"This paper deals with a common verification methodology and environment for
SystemC BCA and RTL models. The aim is to save effort by avoiding the same work
done twice by different people and to reuse the same environment for the two
design views. Applying this methodology the verification task starts as soon as
the functional specification is signed off and it runs in parallel to the
models and design development. The verification environment is modeled with the
aid of dedicated verification languages and it is applied to both the models.
The test suite is exactly the same and thus it's possible to verify the
alignment between the two models. In fact the final step is to check the
cycle-by-cycle match of the interface behavior. A regression tool and a bus
analyzer have been developed to help the verification and the alignment
process. The former is used to automate the testbench generation and to run the
two test suites. The latter is used to verify the alignment between the two
models comparing the waveforms obtained in each run. The quality metrics used
to validate the flow are full functional coverage and full alignment at each IP
port."
"We show that for every homomorphism $\Gamma^+ \to S$ where $S$ is a finite
semigroup there exists a factorization forest of height $\leq 3 \abs{S}$. The
proof is based on Green's relations."
"Formal verification using the model checking paradigm has to deal with two
aspects: The system models are structured, often as products of components, and
the specification logic has to be expressive enough to allow the formalization
of reachability properties. The present paper is a study on what can be
achieved for infinite transition systems under these premises. As models we
consider products of infinite transition systems with different synchronization
constraints. We introduce finitely synchronized transition systems, i.e.
product systems which contain only finitely many (parameterized) synchronized
transitions, and show that the decidability of FO(R), first-order logic
extended by reachability predicates, of the product system can be reduced to
the decidability of FO(R) of the components. This result is optimal in the
following sense: (1) If we allow semifinite synchronization, i.e. just in one
component infinitely many transitions are synchronized, the FO(R)-theory of the
product system is in general undecidable. (2) We cannot extend the expressive
power of the logic under consideration. Already a weak extension of first-order
logic with transitive closure, where we restrict the transitive closure
operators to arity one and nesting depth two, is undecidable for an
asynchronous (and hence finitely synchronized) product, namely for the infinite
grid."
"We give an explicit coinduction principle for recursively-defined stochastic
processes. The principle applies to any closed property, not just equality, and
works even when solutions are not unique. The rule encapsulates low-level
analytic arguments, allowing reasoning about such processes at a higher
algebraic level. We illustrate the use of the rule in deriving properties of a
simple coin-flip process."
"We take a process component as a pair of an interface and a behaviour. We
study the composition of interacting process components in the setting of
process algebra. We formalize the interfaces of interacting process components
by means of an interface group. An interesting feature of the interface group
is that it allows for distinguishing between expectations and promises in
interfaces of process components. This distinction comes into play in case
components with both client and server behaviour are involved."
"We present a theory of threads, interleaving of threads, and interaction
between threads and services with features of molecular dynamics, a model of
computation that bears on computations in which dynamic data structures are
involved. Threads can interact with services of which the states consist of
structured data objects and computations take place by means of actions which
may change the structure of the data objects. The features introduced include
restriction of the scope of names used in threads to refer to data objects.
Because that feature makes it troublesome to provide a model based on
structural operational semantics and bisimulation, we construct a projective
limit model for the theory."
"We present a tableau-based algorithm for deciding satisfiability for
propositional dynamic logic (PDL) which builds a finite rooted tree with
ancestor loops and passes extra information from children to parents to
separate good loops from bad loops during backtracking. It is easy to
implement, with potential for parallelisation, because it constructs a
pseudo-model ``on the fly'' by exploring each tableau branch independently. But
its worst-case behaviour is 2EXPTIME rather than EXPTIME. A prototype
implementation in the TWB (http://twb.rsise.anu.edu.au) is available."
"Recently, there has been a lot of interest in the integration of Description
Logics and rules on the Semantic Web.We define guarded hybrid knowledge bases
(or g-hybrid knowledge bases) as knowledge bases that consist of a Description
Logic knowledge base and a guarded logic program, similar to the DL+log
knowledge bases from (Rosati 2006). G-hybrid knowledge bases enable an
integration of Description Logics and Logic Programming where, unlike in other
approaches, variables in the rules of a guarded program do not need to appear
in positive non-DL atoms of the body, i.e. DL atoms can act as guards as well.
Decidability of satisfiability checking of g-hybrid knowledge bases is shown
for the particular DL DLRO, which is close to OWL DL, by a reduction to guarded
programs under the open answer set semantics. Moreover, we show
2-EXPTIME-completeness for satisfiability checking of such g-hybrid knowledge
bases. Finally, we discuss advantages and disadvantages of our approach
compared with DL+log knowledge bases."
"ZF is a well investigated impredicative constructive version of
Zermelo-Fraenkel set theory. Using set terms, we axiomatize IZF with
Replacement, which we call \izfr, along with its intensional counterpart
\iizfr. We define a typed lambda calculus $\li$ corresponding to proofs in
\iizfr according to the Curry-Howard isomorphism principle. Using realizability
for \iizfr, we show weak normalization of $\li$. We use normalization to prove
the disjunction, numerical existence and term existence properties. An inner
extensional model is used to show these properties, along with the set
existence property, for full, extensional \izfr."
"We present a logical separability analysis for a functional quantum
computation language. This logic is inspired by previous works on logical
analysis of aliasing for imperative functional programs. Both analyses share
similarities notably because they are highly non-compositional. Quantum setting
is harder to deal with since it introduces non determinism and thus
considerably modifies semantics and validity of logical assertions. This logic
is the first proposal of entanglement/separability analysis dealing with a
functional quantum programming language with higher-order functions."
"We present a \emph{pairwise normal form} for finite-state shared memory
concurrent programs: all variables are shared between exactly two processes,
and the guards on transitions are conjunctions of conditions over this pairwise
shared state. This representation has been used to efficiently (in polynomial
time) synthesize and model-check correctness properties of concurrent programs.
Our main result is that any finite state concurrent program can be transformed
into pairwise normal form. Specifically, if $Q$ is an arbitrary finite-state
shared memory concurrent program, then there exists a finite-state shared
memory concurrent program $P$ expressed in pairwise normal form such that $P$
is strongly bisimilar to $Q$. Our result is constructive: we give an algorithm
for producing $P$, given $Q$."
"We give a categorical semantics for a call-by-value linear lambda calculus.
Such a lambda calculus was used by Selinger and Valiron as the backbone of a
functional programming language for quantum computation. One feature of this
lambda calculus is its linear type system, which includes a duplicability
operator ""!"" as in linear logic. Another main feature is its call-by-value
reduction strategy, together with a side-effect to model probabilistic
measurements. The ""!"" operator gives rise to a comonad, as in the linear logic
models of Seely, Bierman, and Benton. The side-effects give rise to a monad, as
in Moggi's computational lambda calculus. It is this combination of a monad and
a comonad that makes the present paper interesting. We show that our
categorical semantics is sound and complete."
"We present a new approach for reasoning about liveness properties of
distributed systems, represented as automata. Our approach is based on
simulation relations, and requires reasoning only over finite execution
fragments. Current simulation-relation based methods for reasoning about
liveness properties of automata require reasoning over entire executions, since
they involve a proof obligation of the form: if a concrete and abstract
execution ``correspond'' via the simulation, and the concrete execution is
live, then so is the abstract execution.
  Our contribution consists of (1) a formalism for defining liveness
properties, (2) a proof method for liveness properties based on that formalism,
and (3) two expressive completeness results: firstly, our formalism can express
any liveness property which satisfies a natural ``robustness'' condition, and
secondly, our formalism can express any liveness property at all, provided that
history variables can be used"
"We present a tractable method for synthesizing arbitrarily large concurrent
programs, for a shared memory model with common hardware-available primitives
such as atomic registers, compare-and-swap, load-linked/store conditional, etc.
The programs we synthesize are dynamic: new processes can be created and added
at run-time, and so our programs are not finite-state, in general.
Nevertheless, we successfully exploit automatic synthesis and model-checking
methods based on propositional temporal logic. Our method is algorithmically
efficient, with complexity polynomial in the number of component processes (of
the program) that are ``alive'' at any time. Our method does not explicitly
construct the automata-theoretic product of all processes that are alive,
thereby avoiding \intr{state explosion}. Instead, for each pair of processes
which interact, our method constructs an automata-theoretic product
(\intr{pair-machine}) which embodies all the possible interactions of these two
processes. From each pair-machine, we can synthesize a correct
\intr{pair-program} which coordinates the two involved processes as needed. We
allow such pair-programs to be added dynamically at run-time. They are then
``composed conjunctively'' with the currently alive pair-programs to
re-synthesize the program as it results after addition of the new pair-program.
We are thus able to add new behaviors, which result in new properties being
satisfied, at run-time. We establish a ``large model'' theorem which shows that
the synthesized large program inherits correctness properties from the
pair-programs."
"We show that a special case of the Feferman-Vaught composition theorem gives
rise to a natural notion of automata for finite words over an infinite
alphabet, with good closure and decidability properties, as well as several
logical characterizations. We also consider a slight extension of the
Feferman-Vaught formalism which allows to express more relations between
component values (such as equality), and prove related decidability results.
  From this result we get new classes of decidable logics for words over an
infinite alphabet."
"This paper presents a cut-elimination proof for the logic $LG^\omega$, which
is an extension of a proof system for encoding generic judgments, the logic
$\FOLDNb$ of Miller and Tiu, with an induction principle. The logic
$LG^\omega$, just as $\FOLDNb$, features extensions of first-order
intuitionistic logic with fixed points and a ``generic quantifier'', $\nabla$,
which is used to reason about the dynamics of bindings in object systems
encoded in the logic. A previous attempt to extend $\FOLDNb$ with an induction
principle has been unsuccessful in modeling some behaviours of bindings in
inductive specifications. It turns out that this problem can be solved by
relaxing some restrictions on $\nabla$, in particular by adding the axiom $B
\equiv \nabla x. B$, where $x$ is not free in $B$. We show that by adopting the
equivariance principle, the presentation of the extended logic can be much
simplified. This paper contains the technical proofs for the results stated in
\cite{tiu07entcs}; readers are encouraged to consult \cite{tiu07entcs} for
motivations and examples for $LG^\omega.$"
"The $\pi$-calculus is a process algebra where agents interact by sending
communication links to each other via noiseless communication channels. Taking
into account the reality of noisy channels, an extension of the $\pi$-calculus,
called the $\pi_N$-calculus, has been introduced recently. In this paper, we
present an early transitional semantics of the $\pi_N$-calculus, which is not a
directly translated version of the late semantics of $\pi_N$, and then extend
six kinds of behavioral equivalences consisting of reduction bisimilarity,
barbed bisimilarity, barbed equivalence, barbed congruence, bisimilarity, and
full bisimilarity into the $\pi_N$-calculus. Such behavioral equivalences are
cast in a hierarchy, which is helpful to verify behavioral equivalence of two
agents. In particular, we show that due to the noisy nature of channels, the
coincidence of bisimilarity and barbed equivalence, as well as the coincidence
of full bisimilarity and barbed congruence, in the $\pi$-calculus does not hold
in $\pi_N$."
"Many semantical aspects of programming languages, such as their operational
semantics and their type assignment calculi, are specified by describing
appropriate proof systems. Recent research has identified two proof-theoretic
features that allow direct, logic-based reasoning about such descriptions: the
treatment of atomic judgments as fixed points (recursive definitions) and an
encoding of binding constructs via generic judgments. However, the logics
encompassing these two features have thus far treated them orthogonally: that
is, they do not provide the ability to define object-logic properties that
themselves depend on an intrinsic treatment of binding. We propose a new and
simple integration of these features within an intuitionistic logic enhanced
with induction over natural numbers and we show that the resulting logic is
consistent. The pivotal benefit of the integration is that it allows recursive
definitions to not just encode simple, traditional forms of atomic judgments
but also to capture generic properties pertaining to such judgments. The
usefulness of this logic is illustrated by showing how it can provide elegant
treatments of object-logic contexts that appear in proofs involving typing
calculi and of arbitrarily cascading substitutions that play a role in
reducibility arguments."
"In this paper, we first introduce a lower bound technique for the state
complexity of transformations of automata. Namely we suggest first considering
the class of full automata in lower bound analysis, and later reducing the size
of the large alphabet via alphabet substitutions. Then we apply such technique
to the complementation of nondeterministic \omega-automata, and obtain several
lower bound results. Particularly, we prove an \omega((0.76n)^n) lower bound
for B\""uchi complementation, which also holds for almost every complementation
or determinization transformation of nondeterministic omega-automata, and prove
an optimal (\omega(nk))^n lower bound for the complementation of generalized
B\""uchi automata, which holds for Streett automata as well."
"We investigate the relation between the theory of the iterations in the sense
of Shelah-Stupp and of Muchnik, resp., and the theory of the base structure for
several logics. These logics are obtained from the restriction of set
quantification in monadic second order logic to certain subsets like, e.g.,
finite sets, chains, and finite unions of chains. We show that these theories
of the Shelah-Stupp iteration can be reduced to corresponding theories of the
base structure. This fails for Muchnik's iteration."
"We investigate structures that can be represented by omega-automata, so
called omega-automatic structures, and prove that relations defined over such
structures in first-order logic expanded by the first-order quantifiers `there
exist at most $\aleph_0$ many', 'there exist finitely many' and 'there exist
$k$ modulo $m$ many' are omega-regular. The proof identifies certain algebraic
properties of omega-semigroups. As a consequence an omega-regular equivalence
relation of countable index has an omega-regular set of representatives. This
implies Blumensath's conjecture that a countable structure with an
$\omega$-automatic presentation can be represented using automata on finite
words. This also complements a very recent result of Hj\""orth, Khoussainov,
Montalban and Nies showing that there is an omega-automatic structure which has
no injective presentation."
"We go into the need for, and the requirements on, a formal theory of budgets.
We present a simple algebraic theory of rational budgets, i.e., budgets in
which amounts of money are specified by functions on the rational numbers. This
theory is based on the tuplix calculus. We go into the importance of using
totalized models for the rational numbers. We present a case study on the
educational budget of a university department offering master programs."
"Two-dimensional nine neighbor hood rectangular Cellular Automata rules can be
modeled using many different techniques like Rule matrices, State Transition
Diagrams, Boolean functions, Algebraic Normal Form etc. In this paper, a new
model is introduced using color graphs to model all the 512 linear rules. The
graph theoretic properties therefore studied in this paper simplifies the
analysis of all linear rules in comparison with other ways of its study."
"The new approach to representation of syntax of formal languages-- a
formalism of syntax diagrams is offered. Syntax diagrams look a convenient
language for the description of syntactic relations in the languages having
nonlinear representation of texts, for example, for representation of syntax
lows of the language of structural chemical formulas. The formalism of
neighbourhood grammar is used to describe the set of correct syntax constructs.
The neighbourhood the grammar consists of a set of families of
""neighbourhoods""-- the diagrams defined for each symbol of the language's
alphabet. The syntax diagram is correct if each symbol is included into this
diagram together with some neighbourhood. In other words, correct diagrams are
needed to be covered by elements of the neighbourhood grammar. Thus, the
grammar of formal language can be represented as system of the covers defined
for each correct syntax diagram."
"We introduce two modal natural deduction systems that are suitable to
represent and reason about transformations of quantum registers in an abstract,
qualitative, way. Quantum registers represent quantum systems, and can be
viewed as the structure of quantum data for quantum operations. Our systems
provide a modal framework for reasoning about operations on quantum registers
(unitary transformations and measurements), in terms of possible worlds (as
abstractions of quantum registers) and accessibility relations between these
worlds. We give a Kripke--style semantics that formally describes quantum
register transformations and prove the soundness and completeness of our
systems with respect to this semantics."
"In this paper an algorithm is designed which generates in-equivalent Boolean
functions of any number of variables from the four Boolean functions of single
variable. The grammar for such set of Boolean function is provided. The Turing
Machine that accepts such set is constructed."
"A data word is a sequence of pairs of a letter from a finite alphabet and an
element from an infinite set, where the latter can only be compared for
equality. Safety one-way alternating automata with one register on infinite
data words are considered, their nonemptiness is shown EXPSPACE-complete, and
their inclusion decidable but not primitive recursive. The same complexity
bounds are obtained for satisfiability and refinement, respectively, for the
safety fragment of linear temporal logic with freeze quantification. Dropping
the safety restriction, adding past temporal operators, or adding one more
register, each causes undecidability."
"Threads as considered in basic thread algebra are primarily looked upon as
behaviours exhibited by sequential programs on execution. It is a fact of life
that sequential programs are often fragmented. Consequently, fragmented program
behaviours are frequently found. In this paper, we consider this phenomenon. We
extend basic thread algebra with the barest mechanism for sequencing of threads
that are taken for fragments. This mechanism, called poly-threading, supports
both autonomous and non-autonomous thread selection in sequencing. We relate
the resulting theory to the algebraic theory of processes known as ACP and use
it to describe analytic execution architectures suited for fragmented programs.
We also consider the case where the steps of fragmented program behaviours are
interleaved in the ways of non-distributed and distributed multi-threading."
"We propose a new quantifier elimination algorithm for the theory of linear
real arithmetic. This algorithm uses as subroutine satisfiability modulo this
theory, a problem for which there are several implementations available. The
quantifier elimination algorithm presented in the paper is compared, on
examples arising from program analysis problems, to several other
implementations, all of which cannot solve some of the examples that our
algorithm solves easily."
"We give labeled natural deduction systems for a family of tense logics
extending the basic linear tense logic Kl. We prove that our systems are sound
and complete with respect to the usual Kripke semantics, and that they possess
a number of useful normalization properties (in particular, derivations reduce
to a normal form that enjoys a subformula property). We also discuss how to
extend our systems to capture richer logics like (fragments of) LTL."
"Desharnais, Gupta, Jagadeesan and Panangaden introduced a family of
behavioural pseudometrics for probabilistic transition systems. These
pseudometrics are a quantitative analogue of probabilistic bisimilarity.
Distance zero captures probabilistic bisimilarity. Each pseudometric has a
discount factor, a real number in the interval (0, 1]. The smaller the discount
factor, the more the future is discounted. If the discount factor is one, then
the future is not discounted at all. Desharnais et al. showed that the
behavioural distances can be calculated up to any desired degree of accuracy if
the discount factor is smaller than one. In this paper, we show that the
distances can also be approximated if the future is not discounted. A key
ingredient of our algorithm is Tarski's decision procedure for the first order
theory over real closed fields. By exploiting the Kantorovich-Rubinstein
duality theorem we can restrict to the existential fragment for which more
efficient decision procedures exist."
"The recent advances in knowledge base research and the growing importance of
effective knowledge management raised an important question of knowledge base
equivalence verification. This problem has not been stated earlier, at least in
a way that allows speaking about algorithms for verification of informational
equivalence, because the informal definition of knowledge bases makes formal
solution of this problem impossible. In this paper we provide an implementable
formal algorithm for knowledge base equivalence verification based on the
formal definition of knowledge base proposed by Plotkin B. and Plotkin T., and
study some important properties of automorphic equivalence of models. We also
describe the concept of equivalence and formulate the criterion for the
equivalence of knowledge bases defined over finite models. Further we define
multi-models and automorphic equivalence of models and multi-models, that is
generalization of automorphic equivalence of algebras."
"In Constructive Type Theory, recursive and corecursive definitions are
subject to syntactic restrictions which guarantee termination for recursive
functions and productivity for corecursive functions. However, many terminating
and productive functions do not pass the syntactic tests. Bove proposed in her
thesis an elegant reformulation of the method of accessibility predicates that
widens the range of terminative recursive functions formalisable in
Constructive Type Theory. In this paper, we pursue the same goal for productive
corecursive functions. Notably, our method of formalisation of coinductive
definitions of productive functions in Coq requires not only the use of ad-hoc
predicates, but also a systematic algorithm that separates the inductive and
coinductive parts of functions."
"In this article we present a method for formally proving the correctness of
the lazy algorithms for computing homographic and quadratic transformations --
of which field operations are special cases-- on a representation of real
numbers by coinductive streams. The algorithms work on coinductive stream of
M\""{o}bius maps and form the basis of the Edalat--Potts exact real arithmetic.
We use the machinery of the Coq proof assistant for the coinductive types to
present the formalisation. The formalised algorithms are only partially
productive, i.e., they do not output provably infinite streams for all possible
inputs. We show how to deal with this partiality in the presence of syntactic
restrictions posed by the constructive type theory of Coq. Furthermore we show
that the type theoretic techniques that we develop are compatible with the
semantics of the algorithms as continuous maps on real numbers. The resulting
Coq formalisation is available for public download."
"Two different paradoxes of the fuzzy logic programming system of [29] are
presented. The first paradox is due to two distinct (contradictory) truth
values for every ground atom of FLP, one is syntactical, the other is
semantical. The second paradox concerns the cardinality of the valid FLP
formulas which is found to have contradictory values: both $\aleph_0$ the
cardinality of the natural numbers, and $c$, the cardinality of the continuum.
The result is that CH=""False"" and Axiom of Choice=""False"". Hence, ZFC is
inconsistent."
"As an attempt to uncover the topological nature of composition of strategies
in game semantics, we present a ``topological'' game for Multiplicative
Additive Linear Logic without propositional variables, including cut moves. We
recast the notion of (winning) strategy and the question of cut elimination in
this context, and prove a cut elimination theorem. Finally, we prove soundness
and completeness. The topology plays a crucial role, in particular through the
fact that strategies form a sheaf."
"Sequential propositional logic deviates from ordinary propositional logic by
taking into account that during the sequential evaluation of a propositional
statement,atomic propositions may yield different Boolean values at repeated
occurrences. We introduce `free valuations' to capture this dynamics of a
propositional statement's environment. The resulting logic is phrased as an
equationally specified algebra rather than in the form of proof rules, and is
named `proposition algebra'. It is strictly more general than Boolean algebra
to the extent that the classical connectives fail to be expressively complete
in the sequential case. The four axioms for free valuation congruence are then
combined with other axioms in order define a few more valuation congruences
that gradually identify more propositional statements, up to static valuation
congruence (which is the setting of conventional propositional logic).
  Proposition algebra is developed in a fashion similar to the process algebra
ACP and the program algebra PGA, via an algebraic specification which has a
meaningful initial algebra for which a range of coarser congruences are
considered important as well. In addition infinite objects (that is
propositional statements, processes and programs respectively) are dealt with
by means of an inverse limit construction which allows the transfer of
knowledge concerning finite objects to facts about infinite ones while reducing
all facts about infinite objects to an infinity of facts about finite ones in
return."
"We study rational streams (over a field) from a coalgebraic perspective.
Exploiting the finality of the set of streams, we present an elementary and
uniform proof of the equivalence of four notions of representability of
rational streams: by finite dimensional linear systems; by finite stream
circuits; by finite weighted stream automata; and by finite dimensional
subsystems of the set of streams."
"Probabilistic timed automata are an extension of timed automata with discrete
probability distributions. We consider model-checking algorithms for the
subclasses of probabilistic timed automata which have one or two clocks.
Firstly, we show that PCTL probabilistic model-checking problems (such as
determining whether a set of target states can be reached with probability at
least 0.99 regardless of how nondeterminism is resolved) are PTIME-complete for
one-clock probabilistic timed automata, and are EXPTIME-complete for
probabilistic timed automata with two clocks. Secondly, we show that, for
one-clock probabilistic timed automata, the model-checking problem for the
probabilistic timed temporal logic PCTL is EXPTIME-complete. However, the
model-checking problem for the subclass of PCTL which does not permit both
punctual timing bounds, which require the occurrence of an event at an exact
time point, and comparisons with probability bounds other than 0 or 1, is
PTIME-complete for one-clock probabilistic timed automata."
"The so-called light logics have been introduced as logical systems enjoying
quite remarkable normalization properties. Designing a type assignment system
for pure lambda calculus from these logics, however, is problematic. In this
paper we show that shifting from usual call-by-name to call-by-value lambda
calculus allows regaining strong connections with the underlying logic. This
will be done in the context of Elementary Affine Logic (EAL), designing a type
system in natural deduction style assigning EAL formulae to lambda terms."
"Interaction Grammar (IG) is a grammatical formalism based on the notion of
polarity. Polarities express the resource sensitivity of natural languages by
modelling the distinction between saturated and unsaturated syntactic
structures. Syntactic composition is represented as a chemical reaction guided
by the saturation of polarities. It is expressed in a model-theoretic framework
where grammars are constraint systems using the notion of tree description and
parsing appears as a process of building tree description models satisfying
criteria of saturation and minimality."
"We show a new and constructive proof of the following language-theoretic
result: for every context-free language L, there is a bounded context-free
language L' included in L which has the same Parikh (commutative) image as L.
Bounded languages, introduced by Ginsburg and Spanier, are subsets of regular
languages of the form w1*w2*...wk* for some finite words w1,...,wk. In
particular bounded subsets of context-free languages have nice structural and
decidability properties. Our proof proceeds in two parts. First, using Newton's
iterations on the language semiring, we construct a context-free subset Ls of L
that can be represented as a sequence of substitutions on a linear language and
has the same Parikh image as L. Second, we inductively construct a
Parikh-equivalent bounded context-free subset of Ls.
  We show two applications of this result in model checking: to
underapproximate the reachable state space of multithreaded procedural programs
and to underapproximate the reachable state space of recursive counter
programs. The bounded language constructed above provides a decidable
underapproximation for the original problems. By iterating the construction, we
get a semi-algorithm for the original problems that constructs a sequence of
underapproximations such that no two underapproximations of the sequence can be
compared. This provides a progress guarantee: every word w in L is in some
underapproximation of the sequence. In addition, we show that our approach
subsumes context-bounded reachability for multithreaded programs."
"There are two incompatible Coq libraries that have a theory of the real
numbers; the Coq standard library gives an axiomatic treatment of classical
real numbers, while the CoRN library from Nijmegen defines constructively valid
real numbers. Unfortunately, this means results about one structure cannot
easily be used in the other structure. We present a way interfacing these two
libraries by showing that their real number structures are isomorphic assuming
the classical axioms already present in the standard library reals. This allows
us to use O'Connor's decision procedure for solving ground inequalities present
in CoRN to solve inequalities about the reals from the Coq standard library,
and it allows theorems from the Coq standard library to apply to problem about
the CoRN reals."
"We construct a logic-enriched type theory LTTW that corresponds closely to
the predicative system of foundations presented by Hermann Weyl in Das
Kontinuum. We formalise many results from that book in LTTW, including Weyl's
definition of the cardinality of a set and several results from real analysis,
using the proof assistant Plastic that implements the logical framework LF.
This case study shows how type theory can be used to represent a
non-constructive foundation for mathematics."
"Checking infinite-state systems is frequently done by encoding infinite sets
of states as regular languages. Computing such a regular representation of,
say, the set of reachable states of a system requires acceleration techniques
that can finitely compute the effect of an unbounded number of transitions.
Among the acceleration techniques that have been proposed, one finds both
specific and generic techniques. Specific techniques exploit the particular
type of system being analyzed, e.g. a system manipulating queues or integers,
whereas generic techniques only assume that the transition relation is
represented by a finite-state transducer, which has to be iterated. In this
paper, we investigate the possibility of using generic techniques in cases
where only specific techniques have been exploited so far. Finding that
existing generic techniques are often not applicable in cases easily handled by
specific techniques, we have developed a new approach to iterating transducers.
This new approach builds on earlier work, but exploits a number of new
conceptual and algorithmic ideas, often induced with the help of experiments,
that give it a broad scope, as well as good performances."
"We show how to transform into programs the proofs in classical Analysis which
use the existence of an ultrafilter on the integers. The method mixes the
classical realizability introduced by the author, with the ""forcing"" of P.
Cohen. The programs we obtain, use read and write instructions in random access
memory."
"We formalise the pi-calculus using the nominal datatype package, based on
ideas from the nominal logic by Pitts et al., and demonstrate an implementation
in Isabelle/HOL. The purpose is to derive powerful induction rules for the
semantics in order to conduct machine checkable proofs, closely following the
intuitive arguments found in manual proofs. In this way we have covered many of
the standard theorems of bisimulation equivalence and congruence, both late and
early, and both strong and weak in a uniform manner. We thus provide one of the
most extensive formalisations of a process calculus ever done inside a theorem
prover.
  A significant gain in our formulation is that agents are identified up to
alpha-equivalence, thereby greatly reducing the arguments about bound names.
This is a normal strategy for manual proofs about the pi-calculus, but that
kind of hand waving has previously been difficult to incorporate smoothly in an
interactive theorem prover. We show how the nominal logic formalism and its
support in Isabelle accomplishes this and thus significantly reduces the tedium
of conducting completely formal proofs. This improves on previous work using
weak higher order abstract syntax since we do not need extra assumptions to
filter out exotic terms and can keep all arguments within a familiar
first-order logic."
"We propose a framework for the specification of behaviour-preserving
reconfigurations of systems modelled as Petri nets. The framework is based on
open nets, a mild generalisation of ordinary Place/Transition nets suited to
model open systems which might interact with the surrounding environment and
endowed with a colimit-based composition operation. We show that natural
notions of bisimilarity over open nets are congruences with respect to the
composition operation. The considered behavioural equivalences differ for the
choice of the observations, which can be single firings or parallel steps.
Additionally, we consider weak forms of such equivalences, arising in the
presence of unobservable actions. We also provide an up-to technique for
facilitating bisimilarity proofs. The theory is used to identify suitable
classes of reconfiguration rules (in the double-pushout approach to rewriting)
whose application preserves the observational semantics of the net."
"We provide a characterisation of strong bisimilarity in a fragment of CCS
that contains only prefix, parallel composition, synchronisation and a limited
form of replication. The characterisation is not an axiomatisation, but is
instead presented as a rewriting system. We discuss how our method allows us to
derive a new congruence result in the $\pi$-calculus: congruence holds in the
sub-calculus that does not include restriction nor sum, and features a limited
form of replication. We have not formalised the latter result in all details."
"interpreters are tools to compute approximations for behaviors of a program.
These approximations can then be used for optimisation or for error detection.
In this paper, we show how to describe an abstract interpreter using the
type-theory based theorem prover Coq, using inductive types for syntax and
structural recursive programming for the abstract interpreter's kernel. The
abstract interpreter can then be proved correct with respect to a Hoare logic
for the programming language."
"In this paper we show that states, transitions and behavior of concurrent
systems can often be modeled as sheaves over a suitable topological space. In
this context, geometric logic can be used to describe which local properties
(i.e. properties of individual systems) are preserved, at a global level, when
interconnecting the systems. The main area of application is to modular
verification of complex systems. We illustrate the ideas by means of an example
involving a family of interacting controllers for trains on a rail track."
"Pure, or type-free, Linear Logic proof nets are Turing complete once
cut-elimination is considered as computation. We introduce modal
impredicativity as a new form of impredicativity causing the complexity of
cut-elimination to be problematic from a complexity point of view. Modal
impredicativity occurs when, during reduction, the conclusion of a residual of
a box b interacts with a node that belongs to the proof net inside another
residual of b. Technically speaking, superlazy reduction is a new notion of
reduction that allows to control modal impredicativity. More specifically,
superlazy reduction replicates a box only when all its copies are opened. This
makes the overall cost of reducing a proof net finite and predictable.
Specifically, superlazy reduction applied to any pure proof nets takes
primitive recursive time. Moreover, any primitive recursive function can be
computed by a pure proof net via superlazy reduction."
"The primary goal of this paper is to present a unified way to transform the
syntax of a logic system into certain initial algebraic structure so that it
can be studied algebraically. The algebraic structures which one may choose for
this purpose are various clones over a full subcategory of a category. We show
that the syntax of equational logic, lambda calculus and first order logic can
be represented as clones or right algebras of clones over the set of positive
integers. The semantics is then represented by structures derived from left
algebras of these clones."
"In 1992 Wang & Larsen extended the may- and must preorders of De Nicola and
Hennessy to processes featuring probabilistic as well as nondeterministic
choice. They concluded with two problems that have remained open throughout the
years, namely to find complete axiomatisations and alternative
characterisations for these preorders. This paper solves both problems for
finite processes with silent moves. It characterises the may preorder in terms
of simulation, and the must preorder in terms of failure simulation. It also
gives a characterisation of both preorders using a modal logic. Finally it
axiomatises both preorders over a probabilistic version of CSP."
"We provide a finite basis for the (in)equational theory of the process
algebra BCCS modulo the weak failures preorder and equivalence. We also give
positive and negative results regarding the axiomatizability of BCCS modulo
weak impossible futures semantics."
"We introduce the class of rational Kripke models and study symbolic model
checking of the basic tense logic Kt and some extensions of it in models from
that class. Rational Kripke models are based on (generally infinite) rational
graphs, with vertices labeled by the words in some regular language and
transitions recognized by asynchronous two-head finite automata, also known as
rational transducers. Every atomic proposition in a rational Kripke model is
evaluated in a regular set of states. We show that every formula of Kt has an
effectively computable regular extension in every rational Kripke model, and
therefore local model checking and global model checking of Kt in rational
Kripke models are decidable. These results are lifted to a number of extensions
of Kt. We study and partly determine the complexity of the model checking
procedures."
"We study complexity of the model-checking problems for LTL with registers
(also known as freeze LTL) and for first-order logic with data equality tests
over one-counter automata. We consider several classes of one-counter automata
(mainly deterministic vs. nondeterministic) and several logical fragments
(restriction on the number of registers or variables and on the use of
propositional variables for control locations). The logics have the ability to
store a counter value and to test it later against the current counter value.
We show that model checking over deterministic one-counter automata is
PSPACE-complete with infinite and finite accepting runs. By constrast, we prove
that model checking freeze LTL in which the until operator is restricted to the
eventually operator over nondeterministic one-counter automata is undecidable
even if only one register is used and with no propositional variable. As a
corollary of our proof, this also holds for first-order logic with data
equality tests restricted to two variables. This makes a difference with the
facts that several verification problems for one-counter automata are known to
be decidable with relatively low complexity, and that finitary satisfiability
for the two logics are decidable. Our results pave the way for model-checking
memoryful (linear-time) logics over other classes of operational models, such
as reversal-bounded counter machines."
"Nested words are a structured model of execution paths in procedural
programs, reflecting their call and return nesting structure. Finite nested
words also capture the structure of parse trees and other tree-structured data,
such as XML. We provide new temporal logics for finite and infinite nested
words, which are natural extensions of LTL, and prove that these logics are
first-order expressively-complete. One of them is based on adding a ""within""
modality, evaluating a formula on a subword, to a logic CaRet previously
studied in the context of verifying properties of recursive state machines
(RSMs). The other logic, NWTL, is based on the notion of a summary path that
uses both the linear and nesting structures. For NWTL we show that
satisfiability is EXPTIME-complete, and that model-checking can be done in time
polynomial in the size of the RSM model and exponential in the size of the NWTL
formula (and is also EXPTIME-complete). Finally, we prove that first-order
logic over nested words has the three-variable property, and we present a
temporal logic for nested words which is complete for the two-variable fragment
of first-order."
"Existential fixed point logic (EFPL) is a natural fit for some applications,
and the purpose of this talk is to attract attention to EFPL. The logic is also
interesting in its own right as it has attractive properties. One of those
properties is rather unusual: truth of formulas can be defined (given
appropriate syntactic apparatus) in the logic. We mentioned that property
elsewhere, and we use this opportunity to provide the proof."
"A natural liberalization of Datalog is used in the Distributed Knowledge
Authorization Language (DKAL). We show that the expressive power of this
liberal Datalog is that of existential fixed-point logic. The exposition is
self-contained."
"We describe an extension to the TLA+ specification language with constructs
for writing proofs and a proof environment, called the Proof Manager (PM), to
checks those proofs. The language and the PM support the incremental
development and checking of hierarchically structured proofs. The PM translates
a proof into a set of independent proof obligations and calls upon a collection
of back-end provers to verify them. Different provers can be used to verify
different obligations. The currently supported back-ends are the tableau prover
Zenon and Isabelle/TLA+, an axiomatisation of TLA+ in Isabelle/Pure. The proof
obligations for a complete TLA+ proof can also be used to certify the theorem
in Isabelle/TLA+."
"We generalize some of the central results in automata theory to the
abstraction level of coalgebras and thus lay out the foundations of a universal
theory of automata operating on infinite objects.
  Let F be any set functor that preserves weak pullbacks. We show that the
class of recognizable languages of F-coalgebras is closed under taking unions,
intersections, and projections. We also prove that if a nondeterministic
F-automaton accepts some coalgebra it accepts a finite one of the size of the
automaton. Our main technical result concerns an explicit construction which
transforms a given alternating F-automaton into an equivalent nondeterministic
one, whose size is exponentially bound by the size of the original automaton."
"A fundamental theorem of Buchi and Landweber shows that the Church synthesis
problem is computable. Buchi and Landweber reduced the Church Problem to
problems about &#969;-games and used the determinacy of such games as one of
the main tools to show its computability. We consider a natural generalization
of the Church problem to countable ordinals and investigate games of arbitrary
countable length. We prove that determinacy and decidability parts of the
Bu}chi and Landweber theorem hold for all countable ordinals and that its full
extension holds for all ordinals < \omega\^\omega."
"We address the problem of cyclic termgraph rewriting. We propose a new
framework where rewrite rules are tuples of the form $(L,R,\tau,\sigma)$ such
that $L$ and $R$ are termgraphs representing the left-hand and the right-hand
sides of the rule, $\tau$ is a mapping from the nodes of $L$ to those of $R$
and $\sigma$ is a partial function from nodes of $R$ to nodes of $L$. $\tau$
describes how incident edges of the nodes in $L$ are connected in $R$. $\tau$
is not required to be a graph morphism as in classical algebraic approaches of
graph transformation. The role of $\sigma$ is to indicate the parts of $L$ to
be cloned (copied). Furthermore, we introduce a new notion of \emph{cloning
pushout} and define rewrite steps as cloning pushouts in a given category.
Among the features of the proposed rewrite systems, we quote the ability to
perform local and global redirection of pointers, addition and deletion of
nodes as well as cloning and collapsing substructures."
"Combining higher-order abstract syntax and (co)induction in a logical
framework is well known to be problematic. Previous work described the
implementation of a tool called Hybrid, within Isabelle HOL, which aims to
address many of these difficulties. It allows object logics to be represented
using higher-order abstract syntax, and reasoned about using tactical theorem
proving and principles of (co)induction. In this paper we describe how to use
it in a multi-level reasoning fashion, similar in spirit to other meta-logics
such as Twelf. By explicitly referencing provability in a middle layer called a
specification logic, we solve the problem of reasoning by (co)induction in the
presence of non-stratifiable hypothetical judgments, which allow very elegant
and succinct specifications of object logic inference rules."
"A class of structures is said to have the homomorphism-preservation property
just in case every first-order formula that is preserved by homomorphisms on
this class is equivalent to an existential-positive formula. It is known by a
result of Rossman that the class of finite structures has this property and by
previous work of Atserias et al. that various of its subclasses do. We extend
the latter results by introducing the notion of a quasi-wide class and showing
that any quasi-wide class that is closed under taking substructures and
disjoint unions has the homomorphism-preservation property. We show, in
particular, that classes of structures of bounded expansion and that locally
exclude minors are quasi-wide. We also construct an example of a class of
finite structures which is closed under substructures and disjoint unions but
does not admit the homomorphism-preservation property."
"Generalised Symbolic Trajectory Evaluation (GSTE) is a high-capacity formal
verification technique for hardware. GSTE uses abstraction, meaning that
details of the circuit behaviour are removed from the circuit model. A
semantics for GSTE can be used to predict and understand why certain circuit
properties can or cannot be proven by GSTE. Several semantics have been
described for GSTE. These semantics, however, are not faithful to the proving
power of GSTE-algorithms, that is, the GSTE-algorithms are incomplete with
respect to the semantics.
  The abstraction used in GSTE makes it hard to understand why a specific
property can, or cannot, be proven by GSTE. The semantics mentioned above
cannot help the user in doing so. The contribution of this paper is a faithful
semantics for GSTE. That is, we give a simple formal theory that deems a
property to be true if-and-only-if the property can be proven by a GSTE-model
checker. We prove that the GSTE algorithm is sound and complete with respect to
this semantics."
"Since the topic emerged several years ago, work on regular model checking has
mostly been devoted to the verification of state reachability and safety
properties. Though it was known that linear temporal properties could also be
checked within this framework, little has been done about working out the
corresponding details. This paper addresses this issue in the context of
regular model checking based on the encoding of states by finite or infinite
words. It works out the exact constructions to be used in both cases, and
proposes a partial solution to the problem resulting from the fact that
infinite computations of unbounded configurations might never contain the same
configuration twice, thus making cycle detection problematic."
"Neighbourhood structures are the standard semantic tool used to reason about
non-normal modal logics. The logic of all neighbourhood models is called
classical modal logic. In coalgebraic terms, a neighbourhood frame is a
coalgebra for the contravariant powerset functor composed with itself, denoted
by 2^2. We use this coalgebraic modelling to derive notions of equivalence
between neighbourhood structures. 2^2-bisimilarity and behavioural equivalence
are well known coalgebraic concepts, and they are distinct, since 2^2 does not
preserve weak pullbacks. We introduce a third, intermediate notion whose
witnessing relations we call precocongruences (based on pushouts). We give
back-and-forth style characterisations for 2^2-bisimulations and
precocongruences, we show that on a single coalgebra, precocongruences capture
behavioural equivalence, and that between neighbourhood structures,
precocongruences are a better approximation of behavioural equivalence than
2^2-bisimulations. We also introduce a notion of modal saturation for
neighbourhood models, and investigate its relationship with definability and
image-finiteness. We prove a Hennessy-Milner theorem for modally saturated and
for image-finite neighbourhood models. Our main results are an analogue of Van
Benthem's characterisation theorem and a model-theoretic proof of Craig
interpolation for classical modal logic."
"Let Q_0 denote the rational numbers expanded to a meadow by totalizing
inversion such that 0^{-1}=0. Q_0 can be expanded by a total sign function s
that extracts the sign of a rational number. In this paper we discuss an
extension Q_0(s ,\sqrt) of the signed rationals in which every number has a
unique square root."
"Well-structured transition systems provide the right foundation to compute a
finite basis of the set of predecessors of the upward closure of a state. The
dual problem, to compute a finite representation of the set of successors of
the downward closure of a state, is harder: Until now, the theoretical
framework for manipulating downward-closed sets was missing. We answer this
problem, using insights from domain theory (dcpos and ideal completions), from
topology (sobrifications), and shed new light on the notion of adequate domains
of limits."
"Canonical models are of central importance in modal logic, in particular as
they witness strong completeness and hence compactness. While the canonical
model construction is well understood for Kripke semantics, non-normal modal
logics often present subtle difficulties - up to the point that canonical
models may fail to exist, as is the case e.g. in most probabilistic logics.
Here, we present a generic canonical model construction in the semantic
framework of coalgebraic modal logic, which pinpoints coherence conditions
between syntax and semantics of modal logics that guarantee strong
completeness. We apply this method to reconstruct canonical model theorems that
are either known or folklore, and moreover instantiate our method to obtain new
strong completeness results. In particular, we prove strong completeness of
graded modal logic with finite multiplicities, and of the modal logic of exact
probabilities."
"Algorithmic meta-theorems are general algorithmic results applying to a whole
range of problems, rather than just to a single problem alone. They often have
a ""logical"" and a ""structural"" component, that is they are results of the form:
every computational problem that can be formalised in a given logic L can be
solved efficiently on every class C of structures satisfying certain
conditions. This paper gives a survey of algorithmic meta-theorems obtained in
recent years and the methods used to prove them. As many meta-theorems use
results from graph minor theory, we give a brief introduction to the theory
developed by Robertson and Seymour for their proof of the graph minor theorem
and state the main algorithmic consequences of this theory as far as they are
needed in the theory of algorithmic meta-theorems."
"The use of formal methods provides confidence in the correctness of
developments. Yet one may argue about the actual level of confidence obtained
when the method itself -- or its implementation -- is not formally checked. We
address this question for the B, a widely used formal method that allows for
the derivation of correct programs from specifications. Through a deep
embedding of the B logic in Coq, we check the B theory but also implement B
tools. Both aspects are illustrated by the description of a proved prover for
the B logic."
"Formal methods provide remarkable tools allowing for high levels of
confidence in the correctness of developments. Their use is therefore
encouraged, when not required, for the development of systems in which safety
or security is mandatory. But effectively specifying a secure system or
deriving a secure implementation can be tricky. We propose a review of some
classical `gotchas' and other possible sources of concerns with the objective
to improve the confidence in formal developments, or at least to better assess
the actual confidence level."
"We present Bicoq3, a deep embedding of the B system in Coq, focusing on the
technical aspects of the development. The main subjects discussed are related
to the representation of sets and maps, the use of induction principles, and
the introduction of a new de Bruijn notation providing solutions to various
problems related to the mechanisation of languages and logics."
"We propose and evaluate antichain algorithms to solve the universality and
language inclusion problems for nondeterministic Buechi automata, and the
emptiness problem for alternating Buechi automata. To obtain those algorithms,
we establish the existence of simulation pre-orders that can be exploited to
efficiently evaluate fixed points on the automata defined during the
complementation step (that we keep implicit in our approach). We evaluate the
performance of the algorithm to check the universality of Buechi automata using
the random automaton model recently proposed by Tabakov and Vardi. We show that
on the difficult instances of this probabilistic model, our algorithm
outperforms the standard ones by several orders of magnitude."
"We give semi-decision procedures for the ground word problem of variable
preserving term equation systems and term equation systems. They are natural
improvements of two well known trivial semi-decision procedures. We show the
correctness of our procedures."
"The canonical extension of a lattice is in an essential way a two-sided
completion. Domain theory, on the contrary, is primarily concerned with
one-sided completeness. In this paper, we show two things. Firstly, that the
canonical extension of a lattice can be given an asymmetric description in two
stages: a free co-directed meet completion, followed by a completion by
\emph{selected} directed joins. Secondly, we show that the general techniques
for dcpo presentations of dcpo algebras used in the second stage of the
construction immediately give us the well-known canonicity result for bounded
lattices with operators."
"We are considering typed hierarchies of total, continuous functionals using
complete, separable metric spaces at the base types. We pay special attention
to the so called Urysohn space constructed by P. Urysohn. One of the properties
of the Urysohn space is that every other separable metric space can be
isometrically embedded into it. We discuss why the Urysohn space may be
considered as the universal model of possibly infinitary outputs of algorithms.
The main result is that all our typed hierarchies may be topologically
embedded, type by type, into the corresponding hierarchy over the Urysohn
space. As a preparation for this, we prove an effective density theorem that is
also of independent interest."
"We study observation-based strategies for partially-observable Markov
decision processes (POMDPs) with omega-regular objectives. An observation-based
strategy relies on partial information about the history of a play, namely, on
the past sequence of observations. We consider the qualitative analysis
problem: given a POMDP with an omega-regular objective, whether there is an
observation-based strategy to achieve the objective with probability~1
(almost-sure winning), or with positive probability (positive winning). Our
main results are twofold. First, we present a complete picture of the
computational complexity of the qualitative analysis of POMDP s with parity
objectives (a canonical form to express omega-regular objectives) and its
subclasses. Our contribution consists in establishing several upper and lower
bounds that were not known in literature. Second, we present optimal bounds
(matching upper and lower bounds) on the memory required by pure and randomized
observation-based strategies for the qualitative analysis of POMDP s with
parity objectives and its subclasses."
"Nondeterministic weighted automata are finite automata with numerical weights
on transitions. They define quantitative languages L that assign to each word w
a real number L(w). The value of an infinite word w is computed as the maximal
value of all runs over w, and the value of a run as the maximum, limsup,
liminf, limit average, or discounted sum of the transition weights. We
introduce probabilistic weighted automata, in which the transitions are chosen
in a randomized (rather than nondeterministic) fashion. Under almost-sure
semantics (resp. positive semantics), the value of a word w is the largest real
v such that the runs over w have value at least v with probability 1 (resp.
positive probability).
  We study the classical questions of automata theory for probabilistic
weighted automata: emptiness and universality, expressiveness, and closure
under various operations on languages. For quantitative languages, emptiness
and universality are defined as whether the value of some (resp. every) word
exceeds a given threshold. We prove some of these questions to be decidable,
and others undecidable. Regarding expressive power, we show that probabilities
allow us to define a wide variety of new classes of quantitative languages,
except for discounted-sum automata, where probabilistic choice is no more
expressive than nondeterminism. Finally, we give an almost complete picture of
the closure of various classes of probabilistic weighted automata for the
following pointwise operations on quantitative languages: max, min, sum, and
numerical complement."
"When verifying a concurrent program, it is usual to assume that memory is
sequentially consistent. However, most modern multiprocessors depend on store
buffering for efficiency, and provide native sequential consistency only at a
substantial performance penalty. To regain sequential consistency, a programmer
has to follow an appropriate programming discipline. However, na\""ive
disciplines, such as protecting all shared accesses with locks, are not
flexible enough for building high-performance multiprocessor software.
  We present a new discipline for concurrent programming under TSO (total store
order, with store buffer forwarding). It does not depend on concurrency
primitives, such as locks. Instead, threads use ghost operations to acquire and
release ownership of memory addresses. A thread can write to an address only if
no other thread owns it, and can read from an address only if it owns it or it
is shared and the thread has flushed its store buffer since it last wrote to an
address it did not own. This discipline covers both coarse-grained concurrency
(where data is protected by locks) as well as fine-grained concurrency (where
atomic operations race to memory).
  We formalize this discipline in Isabelle/HOL, and prove that if every
execution of a program in a system without store buffers follows the
discipline, then every execution of the program with store buffers is
sequentially consistent. Thus, we can show sequential consistency under TSO by
ordinary assertional reasoning about the program, without having to consider
store buffers at all."
"In the framework of explicit substitutions there is two termination
properties: preservation of strong normalization (PSN), and strong
normalization (SN). Since there are not easily proved, only one of them is
usually established (and sometimes none). We propose here a connection between
them which helps to get SN when one already has PSN. For this purpose, we
formalize a general proof technique of SN which consists in expanding
substitutions into ""pure"" lambda-terms and to inherit SN of the whole calculus
by SN of the ""pure"" calculus and by PSN. We apply it successfully to a large
set of calculi with explicit substitutions, allowing us to establish SN, or, at
least, to trace back the failure of SN to that of PSN."
"We present a case study of formal verification of full-wave rectifier for
analog and mixed signal designs. We have used the Checkmate tool from CMU [1],
which is a public domain formal verification tool for hybrid systems. Due to
the restriction imposed by Checkmate it necessitates to make the changes in the
Checkmate implementation to implement the complex and non-linear system.
Full-wave rectifier has been implemented by using the Checkmate custom blocks
and the Simulink blocks from MATLAB from Math works. After establishing the
required changes in the Checkmate implementation we are able to efficiently
verify the safety properties of the full-wave rectifier."
"A ""numerical set-expression"" is a term specifying a cascade of arithmetic and
logical operations to be performed on sets of non-negative integers. If these
operations are confined to the usual Boolean operations together with the
result of lifting addition to the level of sets, we speak of ""additive
circuits"". If they are confined to the usual Boolean operations together with
the result of lifting addition and multiplication to the level of sets, we
speak of ""arithmetic circuits"". In this paper, we investigate the definability
of sets and functions by means of additive and arithmetic circuits,
occasionally augmented with additional operations."
"The theory of finite and infinitary term rewriting is extensively developed
for orthogonal rewrite systems, but to a lesser degree for weakly orthogonal
rewrite systems. In this note we present some contributions to the latter case
of weak orthogonality, where critial pairs are admitted provided they are
trivial.
  We start with a refinement of the by now classical Compression Lemma, as a
tool for establishing infinitary confluence, and hence the property of unique
infinitary normal forms, for the case of weakly orthogonal TRSs that do not
contain collapsing rewrite rules.
  That this restriction of collapse-freeness is crucial, is shown in a
elaboration of a simple TRS which is weakly orthogonal, but has two collapsing
rules. It turns out that all the usual theory breaks down dramatically.
  We conclude with establishing a positive fact: the diamond property for
infinitary developments for weakly orthogonal TRSs, by means of a detailed
analysis initiated by van Oostrom for the finite case."
"The literature on concurrency theory offers a wealth of examples of
characteristic-formula constructions for various behavioural relations over
finite labelled transition systems and Kripke structures that are defined in
terms of fixed points of suitable functions. Such constructions and their
proofs of correctness have been developed independently, but have a common
underlying structure. This study provides a general view of characteristic
formulae that are expressed in terms of logics with a facility for the
recursive definition of formulae. It is shown how several examples of
characteristic-formula constructions from the literature can be recovered as
instances of the proposed general framework, and how the framework can be used
to yield novel constructions."
"We prove a compactness theorem in the context of Hennessy-Milner logic. It is
used to derive a sufficient condition on modal characterizations for the
Approximation Induction Principle to be sound modulo the corresponding process
equivalence. We show that this condition is necessary when the equivalence in
question is compositional with respect to the projection operators."
"We apply to locally finite partially ordered sets a construction which
associates a complete lattice to a given poset; the elements of the lattice are
the closed subsets of a closure operator, defined starting from the concurrency
relation. We show that, if the partially ordered set satisfies a property of
local density, i.e.: N-density, then the associated lattice is also
orthomodular. We then consider occurrence nets, introduced by C.A. Petri as
models of concurrent computations, and define a family of subsets of the
elements of an occurrence net; we call those subsets ""causally closed"" because
they can be seen as subprocesses of the whole net which are, intuitively,
closed with respect to the forward and backward local state changes. We show
that, when the net is K-dense, the causally closed sets coincide with the
closed sets induced by the closure operator defined starting from the
concurrency relation. K-density is a property of partially ordered sets
introduced by Petri, on the basis of former axiomatizations of special
relativity theory."
"TACS is an extension of CCS where upper time bounds for delays can be
specified. Luettgen and Vogler defined three variants of bismulation-type
faster-than relations and showed that they all three lead to the same preorder,
demonstrating the robustness of their approach. In the present paper, the
operational semantics of TACS is extended; it is shown that two of the variants
still give the same preorder as before, underlining robustness. An explanation
is given why this result fails for the third variant. It is also shown that
another variant, which mixes old and new operational semantics, can lead to
smaller relations that prove the same preorder."
"This paper presents a logic language for expressing NP search and
optimization problems. Specifically, first a language obtained by extending
(positive) Datalog with intuitive and efficient constructs (namely, stratified
negation, constraints and exclusive disjunction) is introduced. Next, a further
restricted language only using a restricted form of disjunction to define
(non-deterministically) subsets (or partitions) of relations is investigated.
This language, called NP Datalog, captures the power of Datalog with
unstratified negation in expressing search and optimization problems. A system
prototype implementing NP Datalog is presented. The system translates NP
Datalog queries into OPL programs which are executed by the ILOG OPL
Development Studio. Our proposal combines easy formulation of problems,
expressed by means of a declarative logic language, with the efficiency of the
ILOG System. Several experiments show the effectiveness of this approach."
"This volume contains the proceedings of the 16th International Workshop on
Expressiveness in Concurrency (EXPRESS'09), which took place on 5th September
2009 in Bologna, co-located with CONCUR'09. The EXPRESS workshop series aim at
bringing together researchers who are interested in the expressiveness and
comparison of formal models that broadly relate to concurrency. In particular,
this also includes emergent fields such as logic and interaction,
game-theoretic models, and service-oriented computing."
"In computer science, there is a distinction between closed systems, whose
behavior is totally determined in advance, and open systems, that are systems
maintaining a constant interaction with an unspecified environment. Closed
systems are naturally modeled by transitions systems. Open systems have been
modeled in various ways, including process algebras, I/O automata, ``modules'',
and interfaces. Games provide a uniform setting in which all these models can
be cast and compared. In this paper, we discuss the features and costs related
to the game-based approach to open systems, referring to some of the existing
models. Finally, we describe a new model of interface, called sociable
interface, which is geared towards easier specification, improved reusability
of models, and efficient symbolic implementation."
"This paper clarifies the picture about Dense-choice Counter Machines, which
have been less studied than (discrete) Counter Machines. We revisit the
definition of ""Dense Counter Machines"" so that it now extends (discrete)
Counter Machines, and we provide new undecidability and decidability results.
Using the first-order additive mixed theory of reals and integers, we give a
logical characterization of the sets of configurations reachable by
reversal-bounded Dense-choice Counter Machines."
"We tackle the problem of graph transformation with a particular focus on node
cloning. We propose a new approach to graph rewriting where nodes can be cloned
zero, one or more times. A node can be cloned together with all its incident
edges, with only its outgoing edges, with only its incoming edges or with none
of its incident edges. We thus subsume previous works such as the
sesqui-pushout, the heterogeneous pushout and the adaptive star grammars
approaches. A rewrite rule is defined as a span where the right-hand and
left-hand sides are graphs while the interface is a polarized graph. A
polarized graph is a graph endowed with some annotations on nodes. The way a
node is cloned is indicated by its polarization annotation. We use these
annotations for designing graph transformation with polarized cloning. We show
how a clone of a node can be built according to the different possible
polarizations and define a rewrite step as a final pullback complement followed
by a pushout. This is called the polarized sesqui-pushout approach. We also
provide an algorithmic presentation of the proposed graph transformation with
polarized cloning."
"We prove a characterization of all polynomial-time computable queries on the
class of interval graphs by sentences of fixed-point logic with counting. More
precisely, it is shown that on the class of unordered interval graphs, any
query is polynomial-time computable if and only if it is definable in
fixed-point logic with counting. This result is one of the first establishing
the capturing of polynomial time on a graph class which is defined by forbidden
induced subgraphs. For this, we define a canonical form of interval graphs
using a type of modular decomposition, which is different from the method of
tree decomposition that is used in most known capturing results for other graph
classes, specifically those defined by forbidden minors. The method might also
be of independent interest for its conceptual simplicity. Furthermore, it is
shown that fixed-point logic with counting is not expressive enough to capture
polynomial time on the classes of chordal graphs or incomparability graphs."
"The paper focuses on the structure of fundamental sequences of ordinals
smaller than $\epsilon_0$. A first result is the construction of a monadic
second-order formula identifying a given structure, whereas such a formula
cannot exist for ordinals themselves. The structures are precisely classified
in the pushdown hierarchy. Ordinals are also located in the hierarchy, and a
direct presentation is given."
"An 'arithmetic circuit' is a labeled, acyclic directed graph specifying a
sequence of arithmetic and logical operations to be performed on sets of
natural numbers. Arithmetic circuits can also be viewed as the elements of the
smallest subalgebra of the complex algebra of the semiring of natural numbers.
In the present paper, we investigate the algebraic structure of complex
algebras of natural numbers, and make some observations regarding the
complexity of various theories of such algebras."
"Wegner describes coordination as constrained interaction. We take this
approach literally and define a coordination model based on interaction
constraints and partial, iterative and interactive constraint satisfaction. Our
model captures behaviour described in terms of synchronisation and data flow
constraints, plus various modes of interaction with the outside world provided
by external constraint symbols, on-the-fly constraint generation, and
coordination variables. Underlying our approach is an engine performing
(partial) constraint satisfaction of the sets of constraints. Our model extends
previous work on three counts: firstly, a more advanced notion of external
interaction is offered; secondly, our approach enables local satisfaction of
constraints with appropriate partial solutions, avoiding global synchronisation
over the entire constraints set; and, as a consequence, constraint satisfaction
can finally occur concurrently, and multiple parts of a set of constraints can
be solved and interact with the outside world in an asynchronous manner, unless
synchronisation is required by the constraints. This paper describes the
underlying logic, which enables a notion of local solution, and relates this
logic to the more global approach of our previous work based on classical
logic."
"In this paper, we present an integrated structural and behavioral model of
Reo connectors and Petri nets, allowing a direct comparison of the two
concurrency models. For this purpose, we introduce a notion of connectors which
consist of a number of interconnected, user-defined primitives with fixed
behavior. While the structure of connectors resembles hypergraphs, their
semantics is given in terms of so-called port automata. We define both models
in a categorical setting where composition operations can be elegantly defined
and integrated. Specifically, we formalize structural gluings of connectors as
pushouts, and joins of port automata as pullbacks. We then define a semantical
functor from the connector to the port automata category which preserves this
composition. We further show how to encode Reo connectors and Petri nets into
this model and indicate applications to dynamic reconfigurations modeled using
double pushout graph transformation."
"Linear logics have been shown to be able to embed both rewriting-based
approaches and process calculi in a single, declarative framework. In this
paper we are exploring the embedding of double-pushout graph transformations
into quantified linear logic, leading to a Curry-Howard style isomorphism
between graphs and transformations on one hand, formulas and proof terms on the
other. With linear implication representing rules and reachability of graphs,
and the tensor modelling parallel composition of graphs and transformations, we
obtain a language able to encode graph transformation systems and their
computations as well as reason about their properties."
"This paper revisits the classical notion of sampling in the setting of
real-time temporal logics for the modeling and analysis of systems. The
relationship between the satisfiability of Metric Temporal Logic (MTL) formulas
over continuous-time models and over discrete-time models is studied. It is
shown to what extent discrete-time sequences obtained by sampling
continuous-time signals capture the semantics of MTL formulas over the two time
domains. The main results apply to ""flat"" formulas that do not nest temporal
operators and can be applied to the problem of reducing the verification
problem for MTL over continuous-time models to the same problem over
discrete-time, resulting in an automated partial practically-efficient
discretization technique."
"In 1891 a paper by Georg Cantor was published in which he addressed the
relative cardinality of two sets, the set of integers and the set of real
numbers, in effort to demonstrate that the two sets were of unequal
cardinality. This paper offers a contrary conclusion to Cantor's argument,
together with implication of such to the theory of computation."
"We present an affine-intuitionistic system of types and effects which can be
regarded as an extension of Barber-Plotkin Dual Intuitionistic Linear Logic to
multi-threaded programs with effects. In the system, dynamically generated
values such as references or channels are abstracted into a finite set of
regions. We introduce a discipline of region usage that entails the confluence
(and hence determinacy) of the typable programs. Further, we show that a
discipline of region stratification guarantees termination."
"This paper is a sequel to arXiv:0902.2355 and continues the study of quantum
logic via dagger kernel categories. It develops the relation between these
categories and both orthomodular lattices and Foulis semigroups. The relation
between the latter two notions has been uncovered in the 1960s. The current
categorical perspective gives a broader context and reconstructs this
relationship between orthomodular lattices and Foulis semigroups as special
instance."
"This paper presents a novel approach for augmenting proof-based verification
with performance-style analysis of the kind employed in state-of-the-art model
checking tools for probabilistic systems. Quantitative safety properties
usually specified as probabilistic system invariants and modeled in proof-based
environments are evaluated using bounded model checking techniques.
  Our specific contributions include the statement of a theorem that is central
to model checking safety properties of proof-based systems, the establishment
of a procedure; and its full implementation in a prototype system (YAGA) which
readily transforms a probabilistic model specified in a proof-based environment
to its equivalent verifiable PRISM model equipped with reward structures. The
reward structures capture the exact interpretation of the probabilistic
invariants and can reveal succinct information about the model during
experimental investigations. Finally, we demonstrate the novelty of the
technique on a probabilistic library case study."
"We study the equivalence relation on states of labelled transition systems of
satisfying the same formulas in Computation Tree Logic without the next state
modality (CTL-X). This relation is obtained by De Nicola & Vaandrager by
translating labelled transition systems to Kripke structures, while lifting the
totality restriction on the latter. They characterised it as divergence
sensitive branching bisimulation equivalence.
  We find that this equivalence fails to be a congruence for interleaving
parallel composition. The reason is that the proposed application of CTL-X to
non-total Kripke structures lacks the expressiveness to cope with deadlock
properties that are important in the context of parallel composition. We
propose an extension of CTL-X, or an alternative treatment of non-totality,
that fills this hiatus. The equivalence induced by our extension is
characterised as branching bisimulation equivalence with explicit divergence,
which is, moreover, shown to be the coarsest congruence contained in divergence
sensitive branching bisimulation equivalence."
"In this paper, we focus our attention on the interval temporal logic of the
Allen's relations ""meets"", ""begins"", and ""begun by"" (ABBar for short),
interpreted over natural numbers. We first introduce the logic and we show that
it is expressive enough to model distinctive interval properties,such as
accomplishment conditions, to capture basic modalities of point-based temporal
logic, such as the until operator, and to encode relevant metric constraints.
Then, we prove that the satisfiability problem for ABBar over natural numbers
is decidable by providing a small model theorem based on an original
contraction method. Finally, we prove the EXPSPACE-completeness of the problem"
"In this paper the correspondence between safe Petri nets and event
structures, due to Nielsen, Plotkin and Winskel, is extended to arbitrary nets
without self-loops, under the collective token interpretation. To this end we
propose a more general form of event structure, matching the expressive power
of such nets. These new event structures and nets are connected by relating
both notions with configuration structures, which can be regarded as
representations of either event structures or nets that capture their behaviour
in terms of action occurrences and the causal relationships between them, but
abstract from any auxiliary structure.
  A configuration structure can also be considered logically, as a class of
propositional models, or - equivalently - as a propositional theory in
disjunctive normal from. Converting this theory to conjunctive normal form is
the key idea in the translation of such a structure into a net.
  For a variety of classes of event structures we characterise the associated
classes of configuration structures in terms of their closure properties, as
well as in terms of the axiomatisability of the associated propositional
theories by formulae of simple prescribed forms, and in terms of structural
properties of the associated Petri nets."
"This paper presents an extension to Hoare logic for pointer program
verification. First, the Logic for Partial Function (LPF) used by VDM is
extended to specify memory access using pointers and memory layout of composite
types. Then, the concepts of data-retrieve functions (DRF) and memory-scope
functions (MSF) are introduced in this paper. People can define DRFs to
retrieve abstract values from interconnected concrete data objects. The
definition of the corresponding MSF of a DRF can be derived syntactically from
the definition of the DRF. This MSF computes the set of memory units accessed
when the DRF retrieves an abstract value. This memory unit set is called the
memory scope of the abstract value. Finally, the proof rule of assignment
statements in Hoare's logic is modified to deal with pointers. The basic idea
is that a virtual value keeps unmodified as long as no memory unit in its scope
is over-written. Another proof rule is added for memory allocation statements.
The consequence rule and the rules for control-flow statements are slightly
modified. They are essentially same as their original version in Hoare logic.
  An example is presented to show the efficacy of this logic. We also give some
heuristics on how to verify pointer programs."
"We study normalising reduction strategies for infinitary Combinatory
Reduction Systems (iCRSs). We prove that all fair, outermost-fair, and
needed-fair strategies are normalising for orthogonal, fully-extended iCRSs.
These facts properly generalise a number of results on normalising strategies
in first-order infinitary rewriting and provide the first examples of
normalising strategies for infinitary lambda calculus."
"Zot is an agile and easily extendible bounded model checker, which can be
downloaded at http://home.dei.polimi.it/pradella/. The tool supports different
logic languages through a multi-layered approach: its core uses PLTL, and on
top of it a decidable predicative fragment of TRIO is defined. An interesting
feature of Zot is its ability to support different encodings of temporal logic
as SAT problems by means of plug-ins. This approach encourages experimentation,
as plug-ins are expected to be quite simple, compact (usually around 500 lines
of code), easily modifiable, and extendible."
"This document describes the implementation in SML of the LoopW language, an
imperative language with higher-order procedural variables and non-local jumps
equiped with a program logic. It includes the user manual along with some
implementation notes and many examples of certified imperative programs. As a
concluding example, we show the certification of an imperative program encoding
shift/reset using callcc/throw and a global meta-continuation."
"We give an optimal (EXPTIME), sound and complete tableau-based algorithm for
deciding satisfiability for propositional dynamic logic with converse (CPDL)
which does not require the use of analytic cut. Our main contribution is a
sound methodto combine our previous optimal method for tracking least
fix-points in PDL with our previous optimal method for handling converse in the
description logic ALCI. The extension is non-trivial as the two methods cannot
be combined naively. We give sufficient details to enable an implementation by
others. Our OCaml implementation seems to be the first theorem prover for CPDL."
"This paper discusses highly general mechanisms for specifying the refinement
of a real-time system as a collection of lower level parallel components that
preserve the timing and functional requirements of the upper level
specification. These mechanisms are discussed in the context of ASTRAL, which
is a formal specification language for real-time systems. Refinement is
accomplished by mapping all of the elements of an upper level specification
into lower level elements that may be split among several parallel components.
In addition, actions that can occur in the upper level are mapped to actions of
components operating at the lower level. This allows several types of
implementation strategies to be specified in a natural way, while the price for
generality (in terms of complexity) is paid only when necessary. The refinement
mechanisms are first illustrated using a simple digital circuit; then, through
a highly complex phone system; finally, design guidelines gleaned from these
specifications are presented."
"Fixed point combinators (and their generalization: looping combinators) are
classic notions belonging to the heart of lambda-calculus and logic. We start
with an exploration of the structure of fixed point combinators (fpc's), vastly
generalizing the well-known fact that if Y is an fpc, Y(SI) is again an fpc,
generating the Boehm sequence of fpc's. Using the infinitary lambda-calculus we
devise infinitely many other generation schemes for fpc's. In this way we find
schemes and building blocks to construct new fpc's in a modular way.
  Having created a plethora of new fixed point combinators, the task is to
prove that they are indeed new. That is, we have to prove their
beta-inconvertibility. Known techniques via Boehm Trees do not apply, because
all fpc's have the same Boehm Tree (BT). Therefore, we employ `clocked BT's',
with annotations that convey information of the tempo in which the data in the
BT are produced. BT's are thus enriched with an intrinsic clock behaviour,
leading to a refined discrimination method for lambda-terms. The corresponding
equality is strictly intermediate between beta-convertibility and BT-equality,
the equality in the classical models of lambda-calculus. An analogous approach
pertains to Levy-Longo Berarducci trees. Finally, we increase the
discrimination power by a precision of the clock notion that we call `atomic
clock'."
"This paper presents a bisimulation-based method for establishing the
soundness of equations between terms constructed using operations whose
semantics is specified by rules in the GSOS format of Bloom, Istrail and Meyer.
The method is inspired by de Simone's FH-bisimilarity and uses transition rules
as schematic transitions in a bisimulation-like relation between open terms.
The soundness of the method is proven and examples showing its applicability
are provided. The proposed bisimulation-based proof method is incomplete, but
the article offers some completeness results for restricted classes of GSOS
specifications."
"We present a symbolic transition system and bisimulation equivalence for
psi-calculi, and show that it is fully abstract with respect to bisimulation
congruence in the non-symbolic semantics.
  A psi-calculus is an extension of the pi-calculus with nominal data types for
data structures and for logical assertions representing facts about data. These
can be transmitted between processes and their names can be statically scoped
using the standard pi-calculus mechanism to allow for scope migrations.
Psi-calculi can be more general than other proposed extensions of the
pi-calculus such as the applied pi-calculus, the spi-calculus, the fusion
calculus, or the concurrent constraint pi-calculus.
  Symbolic semantics are necessary for an efficient implementation of the
calculus in automated tools exploring state spaces, and the full abstraction
property means the semantics of a process does not change from the original."
"Reactive systems (RSs) represent a meta-framework aimed at deriving
behavioral congruences for those computational formalisms whose operational
semantics is provided by reduction rules. RSs proved a flexible specification
device, yet so far most of the efforts dealing with their behavioural semantics
focused on idem pushouts (IPOs) and saturated (also known as dynamic)
bisimulations. In this paper we introduce a novel, intermediate behavioural
equivalence: L-bisimilarity, which is able to recast both its IPO and saturated
counterparts. The equivalence is parametric with respect to a set L of RSs
labels, and it is shown that under mild conditions on L it is indeed a
congruence. Furthermore, L-bisimilarity can also recast the notion of barbed
semantics for RSs, proposed by the same authors in a previous paper. In order
to provide a suitable test-bed, we instantiate our proposal by addressing the
semantics of (asynchronous) CCS and of the calculus of mobile ambients."
"The relationships between various equivalences on configuration structures,
including interleaving bisimulation (IB), step bisimulation (SB) and hereditary
history-preserving (HH) bisimulation, have been investigated by van Glabbeek
and Goltz (and later Fecher). Since HH bisimulation may be characterised by the
use of reverse as well as forward transitions, it is of interest to investigate
forms of IB and SB where both forward and reverse transitions are allowed. We
give various characterisations of reverse SB, showing that forward steps do not
add extra power. We strengthen Bednarczyk's result that, in the absence of
auto-concurrency, reverse IB is as strong as HH bisimulation, by showing that
we need only exclude auto-concurrent events at the same depth in the
configuration."
"Live sequence charts (LSCs) have been proposed as an inter-object
scenario-based specification and visual programming language for reactive
systems. In this paper, we introduce a logic-based framework to check the
consistency of an LSC specification. An LSC simulator has been implemented in
logic programming, utilizing a memoized depth-first search strategy, to show
how a reactive system in LSCs would response to a set of external event
sequences. A formal notation is defined to specify external event sequences,
extending the regular expression with a parallel operator and a testing
control. The parallel operator allows interleaved parallel external events to
be tested in LSCs simultaneously; while the testing control provides users to a
new approach to specify and test certain temporal properties (e.g., CTL
formula) in a form of LSC. Our framework further provides either a state
transition graph or a failure trace to justify the consistency checking
results."
"We show that for Multiplicative Exponential Linear Logic (without weakenings)
the syntactical equivalence relation on proofs induced by cut-elimination
coincides with the semantic equivalence relation on proofs induced by the
multiset based relational model: one says that the interpretation in the model
(or the semantics) is injective. We actually prove a stronger result: two
cut-free proofs of the full multiplicative and exponential fragment of linear
logic whose interpretations coincide in the multiset based relational model are
the same ""up to the connections between the doors of exponential boxes""."
"We analyse the problem of solving Boolean equation systems through the use of
structure graphs. The latter are obtained through an elegant set of
Plotkin-style deduction rules. Our main contribution is that we show that
equation systems with bisimilar structure graphs have the same solution. We
show that our work conservatively extends earlier work, conducted by Keiren and
Willemse, in which dependency graphs were used to analyse a subclass of Boolean
equation systems, viz., equation systems in standard recursive form. We
illustrate our approach by a small example, demonstrating the effect of
simplifying an equation system through minimisation of its structure graph."
"Motivated by model-theoretic properties of the BSR class, we present a family
of semantic classes of FO formulae with finite or co-finite spectra over a
relational vocabulary \Sigma. A class in this family is denoted
EBS_\Sigma(\sigma), where \sigma is a subset of \Sigma. Formulae in
EBS_\Sigma(\sigma) are preserved under substructures modulo a bounded core and
modulo re-interpretation of predicates outside \sigma. We study properties of
the family EBS_\Sigma = {EBS_\Sigma(\sigma) | \sigma \subseteq \Sigma}, e.g.
classes in EBS_\Sigma are spectrally indistinguishable, EBS_\Sigma(\Sigma) is
semantically equivalent to BSR over \Sigma, and EBS_\Sigma(\emptyset) is the
set of all FO formulae over \Sigma with finite or co-finite spectra.
Furthermore, (EBS_\Sigma, \subseteq) forms a lattice isomorphic to the powerset
lattice (\wp({\Sigma}), \subseteq). This gives a natural semantic
generalization of BSR as ascending chains in (EBS_\Sigma, \subseteq). Many
well-known FO classes are semantically subsumed by EBS_\Sigma(\Sigma) or
EBS_\Sigma(\emptyset). Our study provides alternative proofs of interesting
results like the Lo\'s-Tarski Theorem and the semantic subsumption of the
L\""owenheim class with equality by BSR. We also present a syntactic sub-class
of EBS_\Sigma(\sigma) called EDP_\Sigma(\sigma) and give an expression for the
size of the bounded cores of models of EDP_\Sigma(\sigma) formulae. We show
that the EDP_\Sigma(\sigma) classes also form a lattice structure. Finally, we
study some closure properties and applications of the classes presented."
"This volume contains the papers presented at the 19th Workshop on Logic-
based methods in Programming Environments (WLPE'09), which was held in
Pasadena, USA, on July 14th, 2009.
  WLPE aims at providing an informal meeting for researchers working on
logic-based methods and tools which support program development and analy- sis.
This year, we have continued and consolidated the shift in focus from en-
vironmental tools for logic programming to logic-based environmental tools for
programming in general, so that this workshop can be possibly interesting for a
wider scientific community.
  All the papers submitted to WLPE'09 have gone through a careful process of
peer reviewing, with at least three reviews for each paper and a subsequent
in-depth discussion in the Program Committee."
"There is a growing interest in techniques for detecting whether a logic
specification is satisfied too easily, or vacuously. For example, the
specification ""every request is eventually followed by an acknowledgment"" is
satisfied vacuously by a system that never generates any requests. Vacuous
satisfaction misleads users of model-checking into thinking that a system is
correct.
  There are several existing definitions of vacuity. Originally, Beer et al.
formalized vacuity as insensitivity to syntactic perturbation. However, this
definition is only reasonable for vacuity in a single occurrence. Armoni et al.
argued that vacuity must be robust -- not affected by semantically invariant
changes, such as extending a model with additional atomic propositions. They
show that syntactic vacuity is not robust for LTL, and propose an alternative
definition -- trace vacuity.
  In this article, we continue this line of research. We show that trace
vacuity is not robust for branching time logic. We refine it to apply uniformly
to linear and branching time logic and to not suffer from the common pitfalls
of prior definitions. Our new definition -- bisimulation vacuity -- is a proper
non-trivial extension of both syntactic and trace vacuity. We discuss the
complexity of detecting bisimulation vacuity, and give efficient algorithms to
detect vacuity for several practically-relevant subsets of CTL*."
"The full abstraction result for PCF using game semantics requires one to
identify all innocent strategies that are innocently indistinguishable. This
involves a quantification over all innocent tests, cf. quantification over all
innocent contexts. Here we present a representation of innocent strategies that
equates innocently indistinguishable ones, yielding a representation of PCF
terms that equates precisely those terms that are observational equivalent."
"The timed automata formalism is an important model for specifying and
analysing real-time systems. Robustness is the correctness of the model in the
presence of small drifts on clocks or imprecision in testing guards. A symbolic
algorithm for the analysis of the robustness of timed automata has been
implemented. In this paper, we re-analyse an industrial case lip
synchronization protocol using the new robust reachability algorithm. This lip
synchronization protocol is an interesting case because timing aspects are
crucial for the correctness of the protocol. Several versions of the model are
considered: with an ideal video stream, with anchored jitter, and with
non-anchored jitter."
"Term unification plays an important role in many areas of computer science,
especially in those related to logic. The universal mechanism of grammar-based
compression for terms, in particular the so-called Singleton Tree Grammars
(STG), have recently drawn considerable attention. Using STGs, terms of
exponential size and height can be represented in linear space. Furthermore,
the term representation by directed acyclic graphs (dags) can be efficiently
simulated. The present paper is the result of an investigation on term
unification and matching when the terms given as input are represented using
different compression mechanisms for terms such as dags and Singleton Tree
Grammars. We describe a polynomial time algorithm for context matching with
dags, when the number of different context variables is fixed for the problem.
For the same problem, NP-completeness is obtained when the terms are
represented using the more general formalism of Singleton Tree Grammars. For
first-order unification and matching polynomial time algorithms are presented,
each of them improving previous results for those problems."
"We present a novel method for the synthesis of finite state systems that is a
generalisation of the generalised reactivity(1) synthesis approach by Piterman,
Pnueli and Sa'ar. In particular, we describe an efficient method to synthesize
systems from linear-time temporal logic specifications for which all
assumptions and guarantees have a Rabin index of one. We show how to build a
parity game with at most five colours that captures all solutions to the
synthesis problem from such a specification. This parity game has a structure
that is amenable to symbolic implementations. We furthermore show that the
results obtained are in some sense tight, i.e., that there does not exist a
similar synthesis method for assumptions and specifications of higher Rabin
index, unless P=NP."
"Classical logics of knowledge and belief are usually interpreted on Kripke
models, for which a mathematically well-developed model theory is available.
However, such models are inadequate to capture dynamic phenomena. Therefore,
epistemic plausibility models have been introduced. Because these are much
richer structures than Kripke models, they do not straightforwardly inherit the
model-theoretical results of modal logic. Therefore, while epistemic
plausibility structures are well-suited for modeling purposes, an extensive
investigation of their model theory has been lacking so far. The aim of the
present paper is to fill exactly this gap, by initiating a systematic
exploration of the model theory of epistemic plausibility models. Like in
'ordinary' modal logic, the focus will be on the notion of bisimulation. We
define various notions of bisimulations (parametrized by a language L) and show
that L-bisimilarity implies L-equivalence. We prove a Hennesy-Milner type
result, and also two undefinability results. However, our main point is a
negative one, viz. that bisimulations cannot straightforwardly be generalized
to epistemic plausibility models if conditional belief is taken into account.
We present two ways of coping with this issue: (i) adding a modality to the
language, and (ii) putting extra constraints on the models. Finally, we make
some remarks about the interaction between bisimulation and dynamic model
changes."
"A number of flexible tactic-based logical frameworks are nowadays available
that can implement a wide range of mathematical theories using a common
higher-order metalanguage. Used as proof assistants, one of the advantages of
such powerful systems resides in their responsiveness to extensibility of their
reasoning capabilities, being designed over rule-based programming languages
that allow the user to build her own `programs to construct proofs' - the
so-called proof tactics.
  The present contribution discusses the implementation of an algorithm that
generates sound and complete tableau systems for a very inclusive class of
sufficiently expressive finite-valued propositional logics, and then
illustrates some of the challenges and difficulties related to the algorithmic
formation of automated theorem proving tactics for such logics. The procedure
on whose implementation we will report is based on a generalized notion of
analyticity of proof systems that is intended to guarantee termination of the
corresponding automated tactics on what concerns theoremhood in our targeted
logics."
"The tree automaton completion is an algorithm used for proving safety
properties of systems that can be modeled by a term rewriting system. This
representation and verification technique works well for proving properties of
infinite systems like cryptographic protocols or more recently on Java Bytecode
programs. This algorithm computes a tree automaton which represents a (regular)
over approximation of the set of reachable terms by rewriting initial terms.
This approach is limited by the lack of information about rewriting relation
between terms. Actually, terms in relation by rewriting are in the same
equivalence class: there are recognized by the same state in the tree
automaton.
  Our objective is to produce an automaton embedding an abstraction of the
rewriting relation sufficient to prove temporal properties of the term
rewriting system.
  We propose to extend the algorithm to produce an automaton having more
equivalence classes to distinguish a term or a subterm from its successors
w.r.t. rewriting. While ground transitions are used to recognize equivalence
classes of terms, epsilon-transitions represent the rewriting relation between
terms. From the completed automaton, it is possible to automatically build a
Kripke structure abstracting the rewriting sequence. States of the Kripke
structure are states of the tree automaton and the transition relation is given
by the set of epsilon-transitions. States of the Kripke structure are labelled
by the set of terms recognized using ground transitions. On this Kripke
structure, we define the Regular Linear Temporal Logic (R-LTL) for expressing
properties. Such properties can then be checked using standard model checking
algorithms. The only difference between LTL and R-LTL is that predicates are
replaced by regular sets of acceptable terms."
"Two formal stochastic models are said to be bisimilar if their solutions as a
stochastic process are probabilistically equivalent. Bisimilarity between two
stochastic model formalisms means that the strengths of one stochastic model
formalism can be used by the other stochastic model formalism. The aim of this
paper is to explain bisimilarity relations between stochastic hybrid automata,
stochastic differential equations on hybrid space and stochastic hybrid Petri
nets. These bisimilarity relations make it possible to combine the formal
verification power of automata with the analysis power of stochastic
differential equations and the compositional specification power of Petri nets.
The relations and their combined strengths are illustrated for an air traffic
example."
"This paper addresses the problem of forbidden states of non safe Petri Net
(PN) modelling discrete events systems. To prevent the forbidden states, it is
possible to use conditions or predicates associated with transitions.
Generally, there are many forbidden states, thus many complex conditions are
associated with the transitions. A new idea for computing predicates in non
safe Petri nets will be presented. Using this method, we can construct a
maximally permissive controller if it exists."
"We consider quantifier-free spatial logics, designed for qualitative spatial
representation and reasoning in AI, and extend them with the means to represent
topological connectedness of regions and restrict the number of their connected
components. We investigate the computational complexity of these logics and
show that the connectedness constraints can increase complexity from NP to
PSpace, ExpTime and, if component counting is allowed, to NExpTime."
"Nominal abstract syntax and higher-order abstract syntax provide a means for
describing binding structure which is higher-level than traditional techniques.
These approaches have spawned two different communities which have developed
along similar lines but with subtle differences that make them difficult to
relate. The nominal abstract syntax community has devices like names,
freshness, name-abstractions with variable capture, and the new-quantifier,
whereas the higher-order abstract syntax community has devices like
lambda-binders, lambda-conversion, raising, and the nabla-quantifier. This
paper aims to unify these communities and provide a concrete correspondence
between their different devices. In particular, we develop a
semantics-preserving translation from alpha-Prolog, a nominal abstract syntax
based logic programming language, to G-, a higher-order abstract syntax based
logic programming language. We also discuss higher-order judgments, a common
and powerful tool for specifications with higher-order abstract syntax, and we
show how these can be incorporated into G-. This establishes G- as a language
with the power of higher-order abstract syntax, the fine-grained variable
control of nominal specifications, and the desirable properties of higher-order
judgments."
"This paper is about a categorical approach to model a very simple
Semantically Linear lambda calculus, named Sll-calculus. This is a core
calculus underlying the programming language SlPCF. In particular, in this
work, we introduce the notion of Sll-Category, which is able to describe a very
large class of sound models of Sll-calculus. Sll-Category extends in the
natural way Benton, Bierman, Hyland and de Paiva's Linear Category, in order to
soundly interpret all the constructs of Sll-calculus. This category is general
enough to catch interesting models in Scott Domains and Coherence Spaces."
"Graph transformation has been used to model concurrent systems in software
engineering, as well as in biochemistry and life sciences. The application of a
transformation rule can be characterised algebraically as construction of a
double-pushout (DPO) diagram in the category of graphs. We show how
intuitionistic linear logic can be extended with resource-bound quantification,
allowing for an implicit handling of the DPO conditions, and how resource logic
can be used to reason about graph transformation systems."
"We present two rewriting systems that define labelled explicit substitution
lambda-calculi. Our work is motivated by the close correspondence between
Levy's labelled lambda-calculus and paths in proof-nets, which played an
important role in the understanding of the Geometry of Interaction. The
structure of the labels in Levy's labelled lambda-calculus relates to the
multiplicative information of paths; the novelty of our work is that we design
labelled explicit substitution calculi that also keep track of exponential
information present in call-by-value and call-by-name translations of the
lambda-calculus into linear logic proof-nets."
"This volume contains the proceedings of LINEARITY 2009: the first
International Workshop on Linearity, which took place 12th September 2009 in
Coimbra, Portugal. The workshop was a satellite event of CSL 2009, the 18th
EACSL Annual Conference on Computer Science Logic."
"Shape types are a general concept of process types which work for many
process calculi. We extend the previously published Poly* system of shape types
to support name restriction. We evaluate the expressiveness of the extended
system by showing that shape types are more expressive than an implicitly typed
pi-calculus and an explicitly typed Mobile Ambients. We demonstrate that the
extended system makes it easier to enjoy advantages of shape types which
include polymorphism, principal typings, and a type inference implementation."
"Size-Change Termination (SCT) is a method of proving program termination
based on the impossibility of infinite descent. To this end we may use a
program abstraction in which transitions are described by monotonicity
constraints over (abstract) variables. When only constraints of the form x>y'
and x>=y' are allowed, we have size-change graphs. Both theory and practice are
now more evolved in this restricted framework then in the general framework of
monotonicity constraints. This paper shows that it is possible to extend and
adapt some theory from the domain of size-change graphs to the general case,
thus complementing previous work on monotonicity constraints. In particular, we
present precise decision procedures for termination; and we provide a procedure
to construct explicit global ranking functions from monotonicity constraints in
singly-exponential time, which is better than what has been published so far
even for size-change graphs."
"One of the most annoying aspects in the formalization of mathematics is the
need of transforming notions to match a given, existing result. This kind of
transformations, often based on a conspicuous background knowledge in the given
scientific domain (mostly expressed in the form of equalities or isomorphisms),
are usually implicit in the mathematical discourse, and it would be highly
desirable to obtain a similar behavior in interactive provers. The paper
describes the superposition-based implementation of this feature inside the
Matita interactive theorem prover, focusing in particular on the so called
smart application tactic, supporting smart matching between a goal and a given
result."
"We introduce a variant of linear logic with second order quantifiers and type
fixpoints, both restricted to purely linear formulas. The Church encodings of
binary words are typed by a standard non-linear type `Church,' while the Scott
encodings (purely linear representations of words) are by a linear type
`Scott.' We give a characterization of polynomial time functions, which is
derived from (Leivant and Marion 93): a function is computable in polynomial
time if and only if it can be represented by a term of type Church => Scott.
  To prove soundness, we employ a resource sensitive realizability technique
developed by Hofmann and Dal Lago."
"We present an affine-intuitionistic system of types and effects which can be
regarded as an extension of Barber-Plotkin Dual Intuitionistic Linear Logic to
multi-threaded programs with effects. In the system, dynamically generated
values such as references or channels are abstracted into a finite set of
regions. We introduce a discipline of region usage that entails the confluence
(and hence determinacy) of the typable programs. Further, we show that a
discipline of region stratification guarantees termination."
"Quantitative properties of stochastic systems are usually specified in logics
that allow one to compare the measure of executions satisfying certain temporal
properties with thresholds. The model checking problem for stochastic systems
with respect to such logics is typically solved by a numerical approach that
iteratively computes (or approximates) the exact measure of paths satisfying
relevant subformulas; the algorithms themselves depend on the class of systems
being analyzed as well as the logic used for specifying the properties. Another
approach to solve the model checking problem is to \emph{simulate} the system
for finitely many runs, and use \emph{hypothesis testing} to infer whether the
samples provide a \emph{statistical} evidence for the satisfaction or violation
of the specification. In this short paper, we survey the statistical approach,
and outline its main advantages in terms of efficiency, uniformity, and
simplicity."
"We present Classical BI (CBI), a new addition to the family of bunched logics
which originates in O'Hearn and Pym's logic of bunched implications BI. CBI
differs from existing bunched logics in that its multiplicative connectives
behave classically rather than intuitionistically (including in particular a
multiplicative version of classical negation). At the semantic level,
CBI-formulas have the normal bunched logic reading as declarative statements
about resources, but its resource models necessarily feature more structure
than those for other bunched logics; principally, they satisfy the requirement
that every resource has a unique dual. At the proof-theoretic level, a very
natural formalism for CBI is provided by a display calculus \`a la Belnap,
which can be seen as a generalisation of the bunched sequent calculus for BI.
In this paper we formulate the aforementioned model theory and proof theory for
CBI, and prove some fundamental results about the logic, most notably
completeness of the proof theory with respect to the semantics."
"The theory of classical realizability is a framework in which we can develop
the proof-program correspondence. Using this framework, we show how to
transform into programs the proofs in classical analysis with dependent choice
and the existence of a well ordering of the real line. The principal tools are:
The notion of realizability algebra, which is a three-sorted variant of the
well known combinatory algebra of Curry. An adaptation of the method of forcing
used in set theory to prove consistency results. Here, it is used in another
way, to obtain programs associated with a well ordering of R and the existence
of a non trivial ultrafilter on N."
"We examine the relationship between the algebraic lambda-calculus, a fragment
of the differential lambda-calculus and the linear-algebraic lambda-calculus, a
candidate lambda-calculus for quantum computation. Both calculi are algebraic:
each one is equipped with an additive and a scalar-multiplicative structure,
and their set of terms is closed under linear combinations. However, the two
languages were built using different approaches: the former is a call-by-name
language whereas the latter is call-by-value; the former considers algebraic
equalities whereas the latter approaches them through rewrite rules. In this
paper, we analyse how these different approaches relate to one another. To this
end, we propose four canonical languages based on each of the possible choices:
call-by-name versus call-by-value, algebraic equality versus algebraic
rewriting. We show that the various languages simulate one another. Due to
subtle interaction between beta-reduction and algebraic rewriting, to make the
languages consistent some additional hypotheses such as confluence or
normalisation might be required. We carefully devise the required properties
for each proof, making them general enough to be valid for any sub-language
satisfying the corresponding properties."
"We propose a realizability interpretation of a system for quantifier free
arithmetic which is equivalent to the fragment of classical arithmetic without
""nested"" quantifiers, called here EM1-arithmetic. We interpret classical proofs
as interactive learning strategies, namely as processes going through several
stages of knowledge and learning by interacting with the ""environment"" and with
each other. We give a categorical presentation of the interpretation through
the construction of two suitable monads."
"This paper surveys main and recent studies on temporal logics in a broad
sense by presenting various logic systems, dealing with various time
structures, and discussing important features, such as decidability (or
undecidability) results, expressiveness and proof systems."
"Over the last two decades, there has been an extensive study on logical
formalisms for specifying and verifying real-time systems. Temporal logics have
been an important research subject within this direction. Although numerous
logics have been introduced for the formal specification of real-time and
complex systems, an up to date comprehensive analysis of these logics does not
exist in the literature. In this paper we analyse real-time and probabilistic
temporal logics which have been widely used in this field. We extrapolate the
notions of decidability, axiomatizability, expressiveness, model checking, etc.
for each logic analysed. We also provide a comparison of features of the
temporal logics discussed."
"Dependently typed lambda calculi such as the Logical Framework (LF) can
encode relationships between terms in types and can naturally capture
correspondences between formulas and their proofs. Such calculi can also be
given a logic programming interpretation: the Twelf system is based on such an
interpretation of LF. We consider here whether a conventional logic programming
language can provide the benefits of a Twelf-like system for encoding type and
proof-and-formula dependencies. In particular, we present a simple mapping from
LF specifications to a set of formulas in the higher-order hereditary Harrop
(hohh) language, that relates derivations and proof-search between the two
frameworks. We then show that this encoding can be improved by exploiting
knowledge of the well-formedness of the original LF specifications to elide
much redundant type-checking information. The resulting logic program has a
structure that closely resembles the original specification, thereby allowing
LF specifications to be viewed as hohh meta-programs. Using the Teyjus
implementation of lambdaProlog, we show that our translation provides an
efficient means for executing LF specifications, complementing the ability that
the Twelf system provides for reasoning about them."
"We quickly review labelled Markov processes (LMP) and provide a
counterexample showing that in general measurable spaces, event bisimilarity
and state bisimilarity differ in LMP. This shows that the logic in Desharnais
[*] does not characterize state bisimulation in non-analytic measurable spaces.
Furthermore we show that, under current foundations of Mathematics, such
logical characterization is unprovable for spaces that are projections of a
coanalytic set. Underlying this construction there is a proof that stationary
Markov processes over general measurable spaces do not have semi-pullbacks.
([*] J. Desharnais, Labelled Markov Processes. School of Computer Science.
McGill University, Montr\'eal (1999))"
"Regular tree grammars and regular path expressions constitute core constructs
widely used in programming languages and type systems. Nevertheless, there has
been little research so far on reasoning frameworks for path expressions where
node cardinality constraints occur along a path in a tree. We present a logic
capable of expressing deep counting along paths which may include arbitrary
recursive forward and backward navigation. The counting extensions can be seen
as a generalization of graded modalities that count immediate successor nodes.
While the combination of graded modalities, nominals, and inverse modalities
yields undecidable logics over graphs, we show that these features can be
combined in a tree logic decidable in exponential time."
"We define two transformations from term rewriting systems (TRSs) to
context-sensitive TRSs in such a way that termination of the target system
implies outermost termination of the original system. In the transformation
based on 'context extension', each outermost rewrite step is modeled by exactly
one step in the transformed system. This transformation turns out to be
complete for the class of left-linear TRSs. The second transformation is called
`dynamic labeling' and results in smaller sized context-sensitive TRSs. Here
each modeled step is adjoined with a small number of auxiliary steps. As a
result state-of-the-art termination methods for context-sensitive rewriting
become available for proving termination of outermost rewriting. Both
transformations have been implemented in Jambox, making it the most successful
tool in the category of outermost rewriting of the last edition of the annual
termination competition."
"We investigate the expressiveness of backward jumps in a framework of
formalized sequential programming called program algebra. We show that - if
expressiveness is measured in terms of the computability of partial Boolean
functions - then backward jumps are superfluous. If we, however, want to
prevent explosion of the length of programs, then backward jumps are essential."
"Computation with advice is suggested as generalization of both computation
with discrete advice and Type-2 Nondeterminism. Several embodiments of the
generic concept are discussed, and the close connection to Weihrauch
reducibility is pointed out. As a novel concept, computability with random
advice is studied; which corresponds to correct solutions being guessable with
positive probability. In the framework of computation with advice, it is
possible to define computational complexity for certain concepts of
hypercomputation. Finally, some examples are given which illuminate the
interplay of uniform and non-uniform techniques in order to investigate both
computability with advice and the Weihrauch lattice."
"We examine the relation of BSS-reducibility on subsets of the real numbers.
The question was asked recently (and anonymously) whether it is possible for
the halting problem H in BSS-computation to be BSS-reducible to a countable
set. Intuitively, it seems that a countable set ought not to contain enough
information to decide membership in a reasonably complex (uncountable) set such
as H. We confirm this intuition, and prove a more general theorem linking the
cardinality of the oracle set to the cardinality, in a local sense, of the set
which it computes. We also mention other recent results on BSS-computation and
algebraic real numbers."
"We present decidability results for termination of classes of term rewriting
systems modulo permutative theories. Termination and innermost termination
modulo permutative theories are shown to be decidable for term rewrite systems
(TRS) whose right-hand side terms are restricted to be shallow (variables occur
at depth at most one) and linear (each variable occurs at most once). Innermost
termination modulo permutative theories is also shown to be decidable for
shallow TRS. We first show that a shallow TRS can be transformed into a flat
(only variables and constants occur at depth one) TRS while preserving
termination and innermost termination. The decidability results are then proved
by showing that (a) for right-flat right-linear (flat) TRS, non-termination
(respectively, innermost non-termination) implies non-termination starting from
flat terms, and (b) for right-flat TRS, the existence of non-terminating
derivations starting from a given term is decidable. On the negative side, we
show PSPACE-hardness of termination and innermost termination for shallow
right-linear TRS, and undecidability of termination for flat TRS."
"In distributed environments, access control decisions depend on statements of
multiple agents rather than only one central trusted party. However, existing
policy languages put few emphasis on authorization provenances. The capability
of managing these provenances is important and useful in various security areas
such as computer auditing and authorization recycling. Based on our previously
proposed logic, we present several case studies of this logic. By doing this,
we show its expressiveness and usefulness in security arena."
"The classic approaches to synthesize a reactive system from a linear temporal
logic (LTL) specification first translate the given LTL formula to an
equivalent omega-automaton and then compute a winning strategy for the
corresponding omega-regular game. To this end, the obtained omega-automata have
to be (pseudo)-determinized where typically a variant of Safra's
determinization procedure is used. In this paper, we show that this
determinization step can be significantly improved for tool implementations by
replacing Safra's determinization by simpler determinization procedures. In
particular, we exploit (1) the temporal logic hierarchy that corresponds to the
well-known automata hierarchy consisting of safety, liveness, Buechi, and
co-Buechi automata as well as their boolean closures, (2) the non-confluence
property of omega-automata that result from certain translations of LTL
formulas, and (3) symbolic implementations of determinization procedures for
the Rabin-Scott and the Miyano-Hayashi breakpoint construction. In particular,
we present convincing experimental results that demonstrate the practical
applicability of our new synthesis procedure."
"Several Markovian process calculi have been proposed in the literature, which
differ from each other for various aspects. With regard to the action
representation, we distinguish between integrated-time Markovian process
calculi, in which every action has an exponentially distributed duration
associated with it, and orthogonal-time Markovian process calculi, in which
action execution is separated from time passing. Similar to deterministically
timed process calculi, we show that these two options are not irreconcilable by
exhibiting three mappings from an integrated-time Markovian process calculus to
an orthogonal-time Markovian process calculus that preserve the behavioral
equivalence of process terms under different interpretations of action
execution: eagerness, laziness, and maximal progress. The mappings are limited
to classes of process terms of the integrated-time Markovian process calculus
with restrictions on parallel composition and do not involve the full
capability of the orthogonal-time Markovian process calculus of expressing
nondeterministic choices, thus elucidating the only two important differences
between the two calculi: their synchronization disciplines and their ways of
solving choices."
"One technique to reduce the state-space explosion problem in temporal logic
model checking is symmetry reduction. The combination of symmetry reduction and
symbolic model checking by using BDDs suffered a long time from the
prohibitively large BDD for the orbit relation. Dynamic symmetry reduction
calculates representatives of equivalence classes of states dynamically and
thus avoids the construction of the orbit relation. In this paper, we present a
new efficient model checking algorithm based on dynamic symmetry reduction. Our
experiments show that the algorithm is very fast and allows the verification of
larger systems. We additionally implemented the use of state symmetries for
symbolic symmetry reduction. To our knowledge we are the first who investigated
state symmetries in combination with BDD based symbolic model checking."
"We present a reduction of the termination problem for a Turing machine (in
the simplified form of the Post correspondence problem) to the problem of
determining whether a continuous-time Markov chain presented as a set of Kappa
graph-rewriting rules has an equilibrium. It follows that the problem of
whether a computable CTMC is dissipative (ie does not have an equilibrium) is
undecidable."
"Bohrification defines a locale of hidden variables internal in a topos. We
find that externally this is the space of partial measurement outcomes. By
considering the double negation sheafification, we obtain the space of
measurement outcomes which coincides with the spectrum for commutative
C*-algebras."
"Algebraic lambda-calculi have been studied in various ways, but their
semantics remain mostly untouched. In this paper we propose a semantic analysis
of a general simply-typed lambda-calculus endowed with a structure of vector
space. We sketch the relation with two established vectorial lambda-calculi.
Then we study the problems arising from the addition of a fixed point
combinator and how to modify the equational theory to solve them. We sketch an
algebraic vectorial PCF and its possible denotational interpretations."
"Quantitative languages are an extension of boolean languages that assign to
each word a real number. Mean-payoff automata are finite automata with
numerical weights on transitions that assign to each infinite path the long-run
average of the transition weights. When the mode of branching of the automaton
is deterministic, nondeterministic, or alternating, the corresponding class of
quantitative languages is not robust as it is not closed under the pointwise
operations of max, min, sum, and numerical complement. Nondeterministic and
alternating mean-payoff automata are not decidable either, as the quantitative
generalization of the problems of universality and language inclusion is
undecidable.
  We introduce a new class of quantitative languages, defined by mean-payoff
automaton expressions, which is robust and decidable: it is closed under the
four pointwise operations, and we show that all decision problems are decidable
for this class. Mean-payoff automaton expressions subsume deterministic
mean-payoff automata, and we show that they have expressive power incomparable
to nondeterministic and alternating mean-payoff automata. We also present for
the first time an algorithm to compute distance between two quantitative
languages, and in our case the quantitative languages are given as mean-payoff
automaton expressions."
"We review the close relationship between abstract machines for (call-by-name
or call-by-value) lambda-calculi (extended with Felleisen's C) and sequent
calculus, reintroducing on the way Curien-Herbelin's syntactic kit expressing
the duality of computation. We use this kit to provide a term language for a
presentation of LK (with conjunction, disjunction, and negation), and to
transcribe cut elimination as (non confluent) rewriting. A key slogan here,
which may appear here in print for the first time, is that commutative cut
elimination rules are explicit substitution propagation rules. We then describe
the focalised proof search discipline (in the classical setting), and narrow
down the language and the rewriting rules to a confluent calculus (a variant of
the second author's focalising system L). We then define a game of patterns and
counterpatterns, leading us to a fully focalised finitary syntax for a
synthetic presentation of classical logic, that provides a quotient on
(focalised) proofs, abstracting out the order of decomposition of negative
connectives."
"We present an adaptation, based on program extraction in elementary linear
logic, of Krivine & Leivant's system FA_2. This system allows to write
higher-order equations in order to specify the computational content of
extracted programs. The user can then prove a generic formula, using these
equations as axioms, whose proof can be extracted into programs that normalize
in elementary time and satisfy the specifications. Finally, we show that every
elementary recursive functions can be implemented in this system."
"Let P be any pure type system, we are going to show how we can extend P into
a PTS P' which will be used as a proof system whose formulas express properties
about sets of terms of P. We will show that P' is strongly normalizable if and
only if P is. Given a term t in P and a formula F in P', P' is expressive
enough to construct a formula ""t ||- F"" that is interpreted as ""t is a realizer
of F"". We then prove the following adequacy theorem: if F is provable then by
projecting its proof back to a term t in P we obtain a proof that ""t ||- F""."
"Many decision procedures for SMT problems rely more or less implicitly on an
instantiation of the axioms of the theories under consideration, and differ by
making use of the additional properties of each theory, in order to increase
efficiency. We present a new technique for devising complete instantiation
schemes on SMT problems over a combination of linear arithmetic with another
theory T. The method consists in first instantiating the arithmetic part of the
formula, and then getting rid of the remaining variables in the problem by
using an instantiation strategy which is complete for T. We provide examples
evidencing that not only is this technique generic (in the sense that it
applies to a wide range of theories) but it is also efficient, even compared to
state-of-the-art instantiation schemes for specific theories."
"It is standard to regard the intuitionistic restriction of a classical logic
as increasing the expressivity of the logic because the classical logic can be
adequately represented in the intuitionistic logic by double-negation, while
the other direction has no truth-preserving propositional encodings. We show
here that subexponential logic, which is a family of substructural refinements
of classical logic, each parametric over a preorder over the subexponential
connectives, does not suffer from this asymmetry if the preorder is
systematically modified as part of the encoding. Precisely, we show a bijection
between synthetic (i.e., focused) partial sequent derivations modulo a given
encoding. Particular instances of our encoding for particular subexponential
preorders give rise to both known and novel adequacy theorems for substructural
logics."
"We introduce a generic extension of the popular branching-time logic CTL
which refines the temporal until and release operators with formal languages.
For instance, a language may determine the moments along a path that an until
property may be fulfilled. We consider several classes of languages leading to
logics with different expressive power and complexity, whose importance is
motivated by their use in model checking, synthesis, abstract interpretation,
etc.
  We show that even with context-free languages on the until operator the logic
still allows for polynomial time model-checking despite the significant
increase in expressive power. This makes the logic a promising candidate for
applications in verification.
  In addition, we analyse the complexity of satisfiability and compare the
expressive power of these logics to CTL* and extensions of PDL."
"We study the problem of generating referring expressions modulo different
notions of expressive power. We define the notion of $\+L$-referring
expression, for a formal language $\+L$ equipped with a semantics in terms of
relational models. We show that the approach is independent of the particular
algorithm used to generate the referring expression by providing examples using
the frameworks of \cite{AKS08} and \cite{Krahmer2003}. We provide some new
complexity bounds, discuss the issue of the length of the generated
descriptions, and propose ways in which the two approaches can be combined."
"We consider an extension of bi-intuitionistic logic with the traditional
modalities from tense logic Kt. Proof theoretically, this extension is obtained
simply by extending an existing sequent calculus for bi-intuitionistic logic
with typical inference rules for the modalities used in display logics. As it
turns out, the resulting calculus, LBiKt, seems to be more basic than most
intuitionistic tense or modal logics considered in the literature, in
particular, those studied by Ewald and Simpson, as it does not assume any a
priori relationship between the diamond and the box modal operators. We recover
Ewald's intuitionistic tense logic and Simpson's intuitionistic modal logic by
modularly extending LBiKt with additional structural rules. The calculus LBiKt
is formulated in a variant of display calculus, using a form of sequents called
nested sequents. Cut elimination is proved for LBiKt, using a technique similar
to that used in display calculi. As in display calculi, the inference rules of
LBiKt are ``shallow'' rules, in the sense that they act on top-level formulae
in a nested sequent. The calculus LBiKt is ill-suited for backward proof search
due to the presence of certain structural rules called ``display postulates''
and the contraction rules on arbitrary structures. We show that these
structural rules can be made redundant in another calculus, DBiKt, which uses
deep inference, allowing one to apply inference rules at an arbitrary depth in
a nested sequent. We prove the equivalence between LBiKt and DBiKt and outline
a proof search strategy for DBiKt. We also give a Kripke semantics and prove
that LBiKt is sound with respect to the semantics, but completeness is still an
open problem. We then discuss various extensions of LBiKt."
"The characterisation of termination using well-founded monotone algebras has
been a milestone on the way to automated termination techniques, of which we
have seen an extensive development over the past years. Both the semantic
characterisation and most known termination methods are concerned with global
termination, uniformly of all the terms of a term rewriting system (TRS). In
this paper we consider local termination, of specific sets of terms within a
given TRS. The principal goal of this paper is generalising the semantic
characterisation of global termination to local termination. This is made
possible by admitting the well-founded monotone algebras to be partial. We also
extend our approach to local relative termination. The interest in local
termination naturally arises in program verification, where one is probably
interested only in sensible inputs, or just wants to characterise the set of
inputs for which a program terminates. Local termination will be also be of
interest when dealing with a specific class of terms within a TRS that is known
to be non-terminating, such as combinatory logic (CL) or a TRS encoding
recursive program schemes or Turing machines. We show how some of the
well-known techniques for proving global termination, such as stepwise removal
of rewrite rules and semantic labelling, can be adapted to the local case. We
also describe transformations reducing local to global termination problems.
The resulting techniques for proving local termination have in some cases
already been automated. One of our applications concerns the characterisation
of the terminating S-terms in CL as regular language. Previously this language
had already been found via a tedious analysis of the reduction behaviour of
S-terms. These findings have now been vindicated by a fully automated and
verified proof."
"Several application domains require formal but flexible approaches to the
comparison problem. Different process models that cannot be related by
behavioral equivalences should be compared via a quantitative notion of
similarity, which is usually achieved through approximation of some
equivalence. While in the literature the classical equivalence subject to
approximation is bisimulation, in this paper we propose a novel approach based
on testing equivalence. As a step towards flexibility and usability, we study
different relaxations taking into account orthogonal aspects of the process
observations: execution time, event probability, and observed behavior. In this
unifying framework, both interpretation of the measures and decidability of the
verification algorithms are discussed."
"We define a testing equivalence in the spirit of De Nicola and Hennessy for
reactive probabilistic processes, i.e. for processes where the internal
nondeterminism is due to random behaviour. We characterize the testing
equivalence in terms of ready-traces. From the characterization it follows that
the equivalence is insensitive to the exact moment in time in which an internal
probabilistic choice occurs, which is inherent from the original testing
equivalence of De Nicola and Hennessy. We also show decidability of the testing
equivalence for finite systems for which the complete model may not be known."
"Model-based safety analysis approaches aim at finding critical failure
combinations by analysis of models of the whole system (i.e. software,
hardware, failure modes and environment). The advantage of these methods
compared to traditional approaches is that the analysis of the whole system
gives more precise results. Only few model-based approaches have been applied
to answer quantitative questions in safety analysis, often limited to analysis
of specific failure propagation models, limited types of failure modes or
without system dynamics and behavior, as direct quantitative analysis is uses
large amounts of computing resources. New achievements in the domain of
(probabilistic) model-checking now allow for overcoming this problem.
  This paper shows how functional models based on synchronous parallel
semantics, which can be used for system design, implementation and qualitative
safety analysis, can be directly re-used for (model-based) quantitative safety
analysis. Accurate modeling of different types of probabilistic failure
occurrence is shown as well as accurate interpretation of the results of the
analysis. This allows for reliable and expressive assessment of the safety of a
system in early design stages."
"In this paper we revisit the well-known technique of predicate abstraction to
characterise performance attributes of system models incorporating probability.
We recast the theory using expectation transformers, and identify transformer
properties which correspond to abstractions that yield nevertheless exact bound
on the performance of infinite state probabilistic systems. In addition, we
extend the developed technique to the special case of ""data independent""
programs incorporating probability. Finally, we demonstrate the subtleness of
the extended technique by using the PRISM model checking tool to analyse an
infinite state protocol, obtaining exact bounds on its performance."
"Recursive domain equations have natural solutions. In particular there are
domains defined by strictly positive induction. The class of countably based
domains gives a computability theory for possibly non-countably based
topological spaces. A $ qcb_{0} $ space is a topological space characterized by
its strong representability over domains. In this paper, we study strictly
positive inductive definitions for $ qcb_{0} $ spaces by means of domain
representations, i.e. we show that there exists a canonical fixed point of
every strictly positive operation on $qcb_{0} $ spaces."
"The reachability problem for Vector Addition Systems (VASs) is a central
problem of net theory. The general problem is known to be decidable by
algorithms exclusively based on the classical
Kosaraju-Lambert-Mayr-Sacerdote-Tenney decomposition. This decomposition is
used in this paper to prove that the Parikh images of languages recognized by
VASs are semi-pseudo-linear; a class that extends the semi-linear sets, a.k.a.
the sets definable in Presburger arithmetic. We provide an application of this
result; we prove that a final configuration is not reachable from an initial
one if and only if there exists a semi-linear inductive invariant that contains
the initial configuration but not the final one. Since we can decide if a
Presburger formula denotes an inductive invariant, we deduce that there exist
checkable certificates of non-reachability. In particular, there exists a
simple algorithm for deciding the general VAS reachability problem based on two
semi-algorithms. A first one that tries to prove the reachability by
enumerating finite sequences of actions and a second one that tries to prove
the non-reachability by enumerating Presburger formulas."
"The book gives a detailed exposition of basic concepts and results of a
theory of processes. The presentation of theoretical concepts and results is
accompanied with illustrations of their application to solving various problems
of verification of processes. Along with well-known results there are presented
author's results related to verification of processes with message passing, and
there are given examples of an application of these results."
"A provably correct bijection between higher-order abstract syntax (HOAS) and
the natural numbers enables one to define a ""not equals"" relationship between
terms and also to have an adequate encoding of sets of terms, and maps from one
term family to another. Sets and maps are useful in many situations and are
preferably provided in a library of some sort. I have released a map and set
library for use with Twelf which can be used with any type for which a
bijection to the natural numbers exists.
  Since creating such bijections is tedious and error-prone, I have created a
""bijection generator"" that generates such bijections automatically together
with proofs of correctness, all in the context of Twelf."
"We present an approach to type theory in which the typing judgments do not
have explicit contexts. Instead of judgments of shape ""Gamma |- A : B"", our
systems just have judgments of shape ""A : B"". A key feature is that we
distinguish free and bound variables even in pseudo-terms.
  Specifically we give the rules of the ""Pure Type System"" class of type
theories in this style. We prove that the typing judgments of these systems
correspond in a natural way with those of Pure Type Systems as traditionally
formulated. I.e., our systems have exactly the same well-typed terms as
traditional presentations of type theory.
  Our system can be seen as a type theory in which all type judgments share an
identical, infinite, typing context that has infinitely many variables for each
possible type. For this reason we call our system ""Gamma_infinity"". This name
means to suggest that our type judgment ""A : B"" should be read as
""Gamma_infinity |- A : B"", with a fixed infinite type context called
""Gamma_infinity""."
"LF has been designed and successfully used as a meta-logical framework to
represent and reason about object logics. Here we design a representation of
the Isabelle logical framework in LF using the recently introduced module
system for LF. The major novelty of our approach is that we can naturally
represent the advanced Isabelle features of type classes and locales.
  Our representation of type classes relies on a feature so far lacking in the
LF module system: morphism variables and abstraction over them. While
conservative over the present system in terms of expressivity, this feature is
needed for a representation of type classes that preserves the modular
structure. Therefore, we also design the necessary extension of the LF module
system."
"We define the pattern fragment for higher-order unification problems in
linear and affine type theory and give a deterministic unification algorithm
that computes most general unifiers."
"We study logics defined in terms of second-order monadic monoidal and
groupoidal quantifiers. These are generalized quantifiers defined by monoid and
groupoid word-problems, equivalently, by regular and context-free languages. We
give a computational classification of the expressive power of these logics
over strings with varying built-in predicates. In particular, we show that
ATIME(n) can be logically characterized in terms of second-order monadic
monoidal quantifiers."
"The need to deal with vague information in Semantic Web languages is rising
in importance and, thus, calls for a standard way to represent such
information. We may address this issue by either extending current Semantic Web
languages to cope with vagueness, or by providing a procedure to represent such
information within current standard languages and tools. In this work, we
follow the latter approach, by identifying the syntactic differences that a
fuzzy ontology language has to cope with, and by proposing a concrete
methodology to represent fuzzy ontologies using OWL 2 annotation properties. We
also report on the prototypical implementations."
"We present a Curry-style second-order type system with union and intersection
types for the lambda-calculus with constructors of Arbiser, Miquel and Rios, an
extension of lambda-calculus with a pattern matching mechanism for variadic
constructors. We then prove the strong normalisation and the absence of match
failure for a restriction of this system, by adapting the standard reducibility
method."
"This paper presents a test automation framework for Mercury programs. We
developed a method that generates runnable Mercury code from a formalized test
suite, and which code provides a report on execution about the success of test
cases. We also developed a coverage tool for the framework, which identifies
and provide a visualization of the reached parts of the program when executing
a given test suite."
"Inference in probabilistic logic languages such as ProbLog, an extension of
Prolog with probabilistic facts, is often based on a reduction to a
propositional formula in DNF. Calculating the probability of such a formula
involves the disjoint-sum-problem, which is computationally hard. In this work
we introduce a new approximation method for ProbLog inference which exploits
the DNF to focus sampling. While this DNF sampling technique has been applied
to a variety of tasks before, to the best of our knowledge it has not been used
for inference in probabilistic logic systems. The paper also presents an
experimental comparison with another sampling based inference method previously
introduced for ProbLog."
"This volume contains the proceedings of the First International Workshop on
Rewriting Techniques for Real-Time Systems (RTRTS 2010), held in Longyearbyen,
Spitsbergen, on April 6-9, 2010.
  The aim of the workshop is to bring together researchers with an interest in
the use of rewriting-based techniques (including rewriting logic) and tools for
the modeling, analysis, and/or implementation of real-time and hybrid systems,
and to give them the opportunity to present their recent works, discuss future
research directions, and exchange ideas. The topics of the workshop comprise,
but are not limited to: methods and tools supporting rewriting-based modeling
and analysis of real-time and hybrid systems, and extensions of such systems;
use of rewriting techniques to provide rigorous support for model-based
software engineering of timed systems; applications and case studies; and
comparison with other formalisms and tools."
"This paper presents a transformational approach for model checking two
important classes of metric temporal logic (MTL) properties, namely, bounded
response and minimum separation, for nonhierarchical object-oriented Real-Time
Maude specifications. We prove the correctness of our model checking
algorithms, which terminate under reasonable non-Zeno-ness assumptions when the
reachable state space is finite. These new model checking features have been
integrated into Real-Time Maude, and are used to analyze a network of medical
devices and a 4-way traffic intersection system."
"Distributed embedded systems (DESs) are no longer the exception; they are the
rule in many application areas such as avionics, the automotive industry,
traffic systems, sensor networks, and medical devices. Formal DES specification
and verification is challenging due to state space explosion and the need to
support real-time features. This paper reports on an extensive industry-based
case study involving a DES product family for a pedestrian and car 4-way
traffic intersection in which autonomous devices communicate by asynchronous
message passing without a centralized controller. All the safety requirements
and a liveness requirement informally specified in the requirements document
have been formally verified using Real-Time Maude and its model checking
features."
"We refine HO/N game semantics with an additional notion of pointer
(mu-pointers) and extend it to first-order classical logic with completeness
results. We use a Church style extension of Parigot's lambda-mu-calculus to
represent proofs of first-order classical logic. We present some relations with
Krivine's classical realizability and applications to type isomorphisms."
"The PALS architecture reduces distributed, real-time asynchronous system
design to the design of a synchronous system under reasonable requirements.
Assuming logical synchrony leads to fewer system behaviors and provides a
conceptually simpler paradigm for engineering purposes. One of the current
limitations of the framework is that from a set of independent ""synchronous
machines"", one must compose the entire synchronous system by hand, which is
tedious and error-prone. We use Maude's meta-level to automatically generate a
synchronous composition from user-provided component machines and a description
of how the machines communicate with each other. We then use the new
capabilities to verify the correctness of a distributed topology control
protocol for wireless networks in the presence of nodes that may fail."
"We consider the temporal logic with since and until modalities. This temporal
logic is expressively equivalent over the class of ordinals to first-order
logic by Kamp's theorem. We show that it has a PSPACE-complete satisfiability
problem over the class of ordinals. Among the consequences of our proof, we
show that given the code of some countable ordinal alpha and a formula, we can
decide in PSPACE whether the formula has a model over alpha. In order to show
these results, we introduce a class of simple ordinal automata, as expressive
as B\""uchi ordinal automata. The PSPACE upper bound for the satisfiability
problem of the temporal logic is obtained through a reduction to the
nonemptiness problem for the simple ordinal automata."
"Proof search has been used to specify a wide range of computation systems. In
order to build a framework for reasoning about such specifications, we make use
of a sequent calculus involving induction and co-induction. These proof
principles are based on a proof theoretic (rather than set-theoretic) notion of
definition. Definitions are akin to logic programs, where the left and right
rules for defined atoms allow one to view theories as ""closed"" or defining
fixed points. The use of definitions and free equality makes it possible to
reason intentionally about syntax. We add in a consistent way rules for pre and
post fixed points, thus allowing the user to reason inductively and
co-inductively about properties of computational system making full use of
higher-order abstract syntax. Consistency is guaranteed via cut-elimination,
where we give the first, to our knowledge, cut-elimination procedure in the
presence of general inductive and co-inductive definitions."
"In a constraint satisfaction problem (CSP) the goal is to find an assignment
of a given set of variables subject to specified constraints. A global
cardinality constraint is an additional requirement that prescribes how many
variables must be assigned a certain value. We study the complexity of the
problem CCSP(G), the constraint satisfaction problem with global cardinality
constraints that allows only relations from the set G. The main result of this
paper characterizes sets G that give rise to problems solvable in polynomial
time, and states that the remaining such problems are NP-complete."
"We review the notion of perfect recall in the literature on interpreted
systems, game theory, and epistemic logic. In the context of Epistemic Temporal
Logic (ETL), we give a (to our knowledge) novel frame condition for perfect
recall, which is local and can straightforwardly be translated to a defining
formula in a language that only has next-step temporal operators. This frame
condition also gives rise to a complete axiomatization for S5 ETL frames with
perfect recall. We then consider how to extend and consolidate the notion of
perfect recall in sub-S5 settings, where the various notions discussed are no
longer equivalent."
"Interaction nets are a graphical formalism inspired by Linear Logic
proof-nets often used for studying higher order rewriting e.g. \Beta-reduction.
Traditional presentations of interaction nets are based on graph theory and
rely on elementary properties of graph theory. We give here a more explicit
presentation based on notions borrowed from Girard's Geometry of Interaction:
interaction nets are presented as partial permutations and a composition of
nets, the gluing, is derived from the execution formula. We then define
contexts and reduction as the context closure of rules. We prove strong
confluence of the reduction within our framework and show how interaction nets
can be viewed as the quotient of some generalized proof-nets."
"The paper proposes and studies temporal logics for attributed words, that is,
data words with a (finite) set of (attribute,value)-pairs at each position. It
considers a basic logic which is a semantical fragment of the logic
$LTL^\downarrow_1$ of Demri and Lazic with operators for navigation into the
future and the past. By reduction to the emptiness problem for data automata it
is shown that this basic logic is decidable. Whereas the basic logic only
allows navigation to positions where a fixed data value occurs, extensions are
studied that also allow navigation to positions with different data values.
Besides some undecidable results it is shown that the extension by a certain
UNTIL-operator with an inequality target condition remains decidable."
"The safety of infinite state systems can be checked by a backward
reachability procedure. For certain classes of systems, it is possible to prove
the termination of the procedure and hence conclude the decidability of the
safety problem. Although backward reachability is property-directed, it can
unnecessarily explore (large) portions of the state space of a system which are
not required to verify the safety property under consideration. To avoid this,
invariants can be used to dramatically prune the search space. Indeed, the
problem is to guess such appropriate invariants. In this paper, we present a
fully declarative and symbolic approach to the mechanization of backward
reachability of infinite state systems manipulating arrays by Satisfiability
Modulo Theories solving. Theories are used to specify the topology and the data
manipulated by the system. We identify sufficient conditions on the theories to
ensure the termination of backward reachability and we show the completeness of
a method for invariant synthesis (obtained as the dual of backward
reachability), again, under suitable hypotheses on the theories. We also
present a pragmatic approach to interleave invariant synthesis and backward
reachability so that a fix-point for the set of backward reachable states is
more easily obtained. Finally, we discuss heuristics that allow us to derive an
implementation of the techniques in the model checker MCMT, showing remarkable
speed-ups on a significant set of safety problems extracted from a variety of
sources."
"The problem of computing Craig interpolants in SAT and SMT has recently
received a lot of interest, mainly for its applications in formal verification.
Efficient algorithms for interpolant generation have been presented for some
theories of interest ---including that of equality and uninterpreted functions,
linear arithmetic over the rationals, and their combination--- and they are
successfully used within model checking tools. For the theory of linear
arithmetic over the integers (LA(Z)), however, the problem of finding an
interpolant is more challenging, and the task of developing efficient
interpolant generators for the full theory LA(Z) is still the objective of
ongoing research. In this paper we try to close this gap. We build on previous
work and present a novel interpolation algorithm for SMT(LA(Z)), which exploits
the full power of current state-of-the-art SMT(LA(Z)) solvers. We demonstrate
the potential of our approach with an extensive experimental evaluation of our
implementation of the proposed algorithm in the MathSAT SMT solver."
"The Halpern-Shoham logic is a modal logic of time intervals. Some effort has
been put in last ten years to classify fragments of this beautiful logic with
respect to decidability of its satisfiability problem. We contribute to this
effort by showing - what we believe is quite an unexpected result - that the
logic of subintervals, the fragment of the Halpern-Shoham where only the
operator ""during"", or D, is allowed, is undecidable over discrete structures.
This is surprising as this logic is decidable over dense orders and its
reflexive variant is known to be decidable over discrete structures."
"In component-based development, approaches for property verification exist
that avoid building the global system behavior of the component model.
Typically, these approaches rely on the analysis of the local behavior of fixed
sized subsystems of components. In our approach, we want to avoid not only the
analysis of the global behavior but also of the local behaviors of the
components. Instead, we consider very small parts of the local behaviors called
port protocols that suffice to verify properties."
"The task of implementing a supervisory controller is non-trivial, even though
different theories exist that allow automatic synthesis of these controllers in
the form of automata. One of the reasons for this discord is due to the
asynchronous interaction between a plant and its controller in implementations,
whereas the existing supervisory control theories assume synchronous
interaction. As a consequence the implementation suffer from the so-called
inexact synchronisation problem. In this paper we address the issue of inexact
synchronisation in a process algebraic setting, by solving a more general
problem of refinement. We construct an asynchronous closed loop system by
introducing a communication medium in a given synchronous closed loop system.
Our goal is to find sufficient conditions under which a synchronous closed loop
system is branching bisimilar to its corresponding asynchronous closed loop
system."
"The verification and validation of cyber-physical systems is known to be a
difficult problem due to the different modeling abstractions used for control
components and for software components. A recent trend to address this
difficulty is to reduce the need for verification by adopting correct-by-design
methodologies. According to the correct-by-design paradigm, one seeks to
automatically synthesize a controller that can be refined into code and that
enforces temporal specifications on the cyber-physical system. In this paper we
consider an instance of this problem where the specifications are given by a
fragment of Linear Temporal Logic (LTL) and the physical environment is
described by a smooth differential equation. The contribution of this paper is
to show that synthesis for cyber-physical systems is viable by considering a
fragment of LTL that is expressive enough to describe interesting properties
but simple enough to avoid Safra's construction. We report on two examples
illustrating a preliminary implementation of these techniques on the tool
PESSOALTL."
"The purpose of this paper is to develop further the main concepts of
Phenomena Dynamic Logic (P-DL) and Cognitive Dynamic Logic (C-DL), presented in
the previous paper. The specific character of these logics is in matching
vagueness or fuzziness of similarity measures to the uncertainty of models.
These logics are based on the following fundamental notions: generality
relation, uncertainty relation, simplicity relation, similarity maximization
problem with empirical content and enhancement (learning) operator. We develop
these notions in terms of logic and probability and developed a Probabilistic
Dynamic Logic of Phenomena and Cognition (P-DL-PC) that relates to the scope of
probabilistic models of brain. In our research the effectiveness of suggested
formalization is demonstrated by approximation of the expert model of breast
cancer diagnostic decisions. The P-DL-PC logic was previously successfully
applied to solving many practical tasks and also for modelling of some
cognitive processes."
"We answer a question by Vasco Brattka and Guido Gherardi by proving that the
Weihrauch-lattice is not a Brouwer algebra. The computable Weihrauch-lattice is
also not a Heyting algebra, but the continuous Weihrauch-lattice is. We further
investigate the existence of infinite infima and suprema, as well as embeddings
of the Medvedev-degrees into the Weihrauch-degrees."
"Refinement types are a well-studied manner of performing in-depth analysis on
functional programs. The dependency pair method is a very powerful method used
to prove termination of rewrite systems; however its extension to higher order
rewrite systems is still the object of active research. We observe that a
variant of refinement types allow us to express a form of higher-order
dependency pair criterion that only uses information at the type level, and we
prove the correctness of this criterion."
"In this paper, we propose Probabilistic discrete-time Projection Temporal
Logic (PrPTL), which extends Projection Temporal Logic (PTL) with probability.
To this end, some useful formulas are derived and some logic laws are given.
Further, we define Time Normal Form (TNF) for PrPTL as the standard form and
prove that any PrPTL formulas can be rewritten to TNF. According to the TNF, we
construct the time normal form graph which can be used for the probabilistic
model checking on PrPTL."
"We introduce two-sorted theories in the style of [CN10] for the complexity
classes \oplusL and DET, whose complete problems include determinants over Z2
and Z, respectively. We then describe interpretations of Soltys' linear algebra
theory LAp over arbitrary integral domains, into each of our new theories. The
result shows equivalences of standard theorems of linear algebra over Z2 and Z
can be proved in the corresponding theory, but leaves open the interesting
question of whether the theorems themselves can be proved."
"Based on a new coinductive characterization of continuous functions we
extract certified programs for exact real number computation from constructive
proofs. The extracted programs construct and combine exact real number
algorithms with respect to the binary signed digit representation of real
numbers. The data type corresponding to the coinductive definition of
continuous functions consists of finitely branching non-wellfounded trees
describing when the algorithm writes and reads digits. We discuss several
examples including the extraction of programs for polynomials up to degree two
and the definite integral of continuous maps."
"In semantics and in programming practice, algebraic concepts such as monads
or, essentially equivalently, (large) Lawvere theories are a well-established
tool for modelling generic side-effects. An important issue in this context are
combination mechanisms for such algebraic effects, which allow for the modular
design of programming languages and verification logics. The most basic
combination operators are sum and tensor: while the sum of effects is just
their non-interacting union, the tensor imposes commutation of effects.
However, for effects with unbounded arity, such as continuations or unbounded
nondeterminism, it is not a priori clear whether these combinations actually
exist in all cases. Here, we introduce the class of uniform effects, which
includes unbounded nondeterminism and continuations, and prove that the tensor
does always exist if one of the component effects is uniform, thus in
particular improving on previous results on tensoring with continuations. We
then treat the case of nondeterminism in more detail, and give an
order-theoretic characterization of effects for which tensoring with
nondeterminism is conservative, thus enabling nondeterministic arguments such
as a generic version of the Fischer-Ladner encoding of control operators."
"Basic arithmetic is the cornerstone of mathematics and computer sciences. In
arithmetic, 'division by zero' is an undefined operation and any attempt at
extending logic for algebraic division to incorporate division by zero has
resulted in paradoxes and fallacies. However, there is no proven theorem or
mathematical logic that suggests that, defining logic for division by zero
would result in break-down of theory. Basing on this motivation, in this paper,
we attempt at logically defining a solution for 'division by zero' problem."
"By using the representational power of Chu spaces we define the notion of a
generalized topological space (or GTS, for short), i.e., a mathematical
structure that generalizes the notion of a topological space. We demonstrate
that these topological spaces have as special cases known topological spaces.
Furthermore, we develop the various topological notions and concepts for GTS.
Moreover, since the logic of Chu spaces is linear logic, we give an
interpretation of most linear logic connectives as operators that yield
topological spaces."
"In sequential logic there is an order in which the atomic propositions in an
expression are evaluated. This order allows the same atomic proposition to have
different values depending on which atomic propositions have already been
evaluated. In the sequential propositional logic discussed in this thesis, such
valuations are called ""reactive"" valuations, in contrast to ""static"" valuations
as are common in e.g. ordinary propositional logic. There are many classes of
these reactive valuations e.g., we can define a class of reactive valuations
such that the value for each atomic proposition remains the same until another
atomic proposition is evaluated. This Master of Logic thesis consists of a
study of some of the properties of this logic. We take a closer look at some of
the classes of reactive valuations. We particularly focus on the relation
between the axiomatization and the semantics. Consequently, the main part of
this thesis focuses on proving soundness and completeness. Furthermore, we show
that the axioms in the provided axiomatizations are independent i.e., there are
no redundant axioms present. Finally, we show {\omega}-completeness for two
classes of reactive valuations."
"The framework of psi-calculi extends the pi-calculus with nominal datatypes
for data structures and for logical assertions and conditions. These can be
transmitted between processes and their names can be statically scoped as in
the standard pi-calculus. Psi-calculi can capture the same phenomena as other
proposed extensions of the pi-calculus such as the applied pi-calculus, the
spi-calculus, the fusion calculus, the concurrent constraint pi-calculus, and
calculi with polyadic communication channels or pattern matching. Psi-calculi
can be even more general, for example by allowing structured channels,
higher-order formalisms such as the lambda calculus for data structures, and
predicate logic for assertions. We provide ample comparisons to related calculi
and discuss a few significant applications. Our labelled operational semantics
and definition of bisimulation is straightforward, without a structural
congruence. We establish minimal requirements on the nominal data and logic in
order to prove general algebraic properties of psi-calculi, all of which have
been checked in the interactive theorem prover Isabelle. Expressiveness of
psi-calculi significantly exceeds that of other formalisms, while the purity of
the semantics is on par with the original pi-calculus."
"Rewriting systems are often defined as binary relations over a given set of
objects. This simple definition is used to describe various properties of
rewriting such as termination, confluence, normal forms etc. In this paper, we
introduce a new notion of abstract rewriting in the framework of categories.
Then, we define the functoriality property of rewriting systems. This property
is sometimes called vertical composition. We show that most of graph
transformation systems are functorial and provide a counter-example of graph
transformation systems which is not functorial."
"We study the verification of a finite continuous-time Markov chain (CTMC) C
against a linear real-time specification given as a deterministic timed
automaton (DTA) A with finite or Muller acceptance conditions. The central
question that we address is: what is the probability of the set of paths of C
that are accepted by A, i.e., the likelihood that C satisfies A? It is shown
that under finite acceptance criteria this equals the reachability probability
in a finite piecewise deterministic Markov process (PDP), whereas for Muller
acceptance criteria it coincides with the reachability probability of terminal
strongly connected components in such a PDP. Qualitative verification is shown
to amount to a graph analysis of the PDP. Reachability probabilities in our
PDPs are then characterized as the least solution of a system of Volterra
integral equations of the second type and are shown to be approximated by the
solution of a system of partial differential equations. For single-clock DTA,
this integral equation system can be transformed into a system of linear
equations where the coefficients are solutions of ordinary differential
equations. As the coefficients are in fact transient probabilities in CTMCs,
this result implies that standard algorithms for CTMC analysis suffice to
verify single-clock DTA specifications."
"The theory of coalgebras, for an endofunctor on a category, has been proposed
as a general theory of transition systems. We investigate and relate four
generalizations of bisimulation to this setting, providing conditions under
which the four different generalizations coincide. We study transfinite
sequences whose limits are the greatest bisimulations."
"We show how to extract existential witnesses from classical proofs using
Krivine's classical realizability---where classical proofs are interpreted as
lambda-terms with the call/cc control operator. We first recall the basic
framework of classical realizability (in classical second-order arithmetic) and
show how to extend it with primitive numerals for faster computations. Then we
show how to perform witness extraction in this framework, by discussing several
techniques depending on the shape of the existential formula. In particular, we
show that in the Sigma01-case, Krivine's witness extraction method reduces to
Friedman's through a well-suited negative translation to intuitionistic
second-order arithmetic. Finally we discuss the advantages of using call/cc
rather than a negative translation, especially from the point of view of an
implementation."
"In this paper we define intersection and union type assignment for Parigot's
calculus lambda-mu. We show that this notion is complete (i.e. closed under
subject-expansion), and show also that it is sound (i.e. closed under
subject-reduction). This implies that this notion of intersection-union type
assignment is suitable to define a semantics."
"We consider a simple model of higher order, functional computation over the
booleans. Then, we enrich the model in order to encompass non-termination and
unrecoverable errors, taken separately or jointly. We show that the models so
defined form a lattice when ordered by the extensional collapse situation
relation, introduced in order to compare models with respect to the amount of
""intensional information"" that they provide on computation. The proofs are
carried out by exhibiting suitable applied {\lambda}-calculi, and by exploiting
the fundamental lemma of logical relations."
"(To appear in Theory and Practice of Logic Programming (TPLP).)
  The seaport of Gioia Tauro is the largest transshipment terminal of the
Mediterranean coast. A crucial management task for the companies operating in
the seaport is team building: the problem of properly allocating the available
personnel for serving the incoming ships. Teams have to be carefully arranged
in order to meet several constraints, such as allocation of the employees with
the appropriate skills, fair distribution of the working load, and turnover of
the heavy/dangerous roles. This makes team building a hard and expensive task
requiring several hours per day of manual preparation.
  In this paper we present a system based on Answer Set Programming (ASP) for
the automatic generation of the teams of employees in the seaport of Gioia
Tauro. The system is currently exploited in the Gioia Tauro seaport by ICO BLG,
a company specialized in automobile logistics."
"Over the last 20 years a large number of automata-based specification
theories have been proposed for modeling of discrete,real-time and
probabilistic systems. We have observed a lot of shared algebraic structure
between these formalisms. In this short abstract, we collect results of our
work in progress on describing and systematizing the algebraic assumptions in
specification theories."
"The fact that classical mathematical proofs of simply existential statements
can be read as programs was established by Goedel and Kreisel half a century
ago. But the possibility of extracting useful computational content from
classical proofs was taken seriously only from the 1990s on when it was
discovered that proof interpretations based on Goedel's and Kreisel's ideas can
provide new nontrivial algorithms and numerical results, and the Curry-Howard
correspondence can be extended to classical logic via programming concepts such
as continuations and control operators.
  The workshop series ""Classical Logic and Computation"" aims to support a
fruitful exchange of ideas between the various lines of research on
computational aspects of classical logic. This volume contains the abstracts of
the invited lectures and the accepted contributed papers of the third CL&C
workshop which was held jointly with the workshop ""Program Extraction and
Constructive Mathematics"" at the University of Brno in August 21-22, 2010, as a
satellite of CSL and MFCS. The workshops were held in honour of Helmut
Schwichtenberg who became ""professor emeritus"" in September 2010.
  The topics of the papers include the foundations, optimizations and
applications of proof interpretations such as Hilbert's epsilon substitution
method, Goedel's functional interpretation, learning based realizability and
negative translations as well as special calculi and theories capturing
computational and complexity-theoretic aspects of classical logic such as the
lambda-mu-calculus, applicative theories, sequent-calculi, resolution and
cut-elimination"
"We prove that interactive learning based classical realizability (introduced
by Aschieri and Berardi for first order arithmetic) is sound with respect to
Coquand game semantics. In particular, any realizer of an
implication-and-negation-free arithmetical formula embodies a winning recursive
strategy for the 1-Backtracking version of Tarski games. We also give examples
of realizer and winning strategy extraction for some classical proofs. We also
sketch some ongoing work about how to extend our notion of realizability in
order to obtain completeness with respect to Coquand semantics, when it is
restricted to 1-Backtracking games."
"Several proof translations of classical mathematics into intuitionistic
mathematics have been proposed in the literature over the past century. These
are normally referred to as negative translations or double-negation
translations. Among those, the most commonly cited are translations due to
Kolmogorov, Godel, Gentzen, Kuroda and Krivine (in chronological order). In
this paper we propose a framework for explaining how these different
translations are related to each other. More precisely, we define a notion of a
(modular) simplification starting from Kolmogorov translation, which leads to a
partial order between different negative translations. In this derived
ordering, Kuroda and Krivine are minimal elements. Two new minimal translations
are introduced, with Godel and Gentzen translations sitting in between
Kolmogorov and one of these new translations."
"Superdeduction is a method specially designed to ease the use of first-order
theories in predicate logic. The theory is used to enrich the deduction system
with new deduction rules in a systematic, correct and complete way.
  A proof-term language and a cut-elimination reduction already exist for
superdeduction, both based on Christian Urban's work on classical sequent
calculus. However the computational content of Christian Urban's calculus is
not directly related to the (lambda-calculus based) Curry-Howard
correspondence. In contrast the Lambda bar mu mu tilde calculus is a
lambda-calculus for classical sequent calculus.
  This short paper is a first step towards a further exploration of the
computational content of superdeduction proofs, for we extend the Lambda bar mu
mu tilde calculus in order to obtain a proofterm langage together with a
cut-elimination reduction for superdeduction. We also prove strong
normalisation for this extension of the Lambda bar mu mu tilde calculus."
"Bi-intuitionistic logic is the conservative extension of intuitionistic logic
with a connective dual to implication. It is sometimes presented as a symmetric
constructive subsystem of classical logic.
  In this paper, we compare three sequent calculi for bi-intuitionistic
propositional logic: (1) a basic standard-style sequent calculus that restricts
the premises of implication-right and exclusion-left inferences to be
single-conclusion resp. single-assumption and is incomplete without the cut
rule, (2) the calculus with nested sequents by Gore et al., where a complete
class of cuts is encapsulated into special ""unnest"" rules and (3) a cut-free
labelled sequent calculus derived from the Kripke semantics of the logic. We
show that these calculi can be translated into each other and discuss the
ineliminable cuts of the standard-style sequent calculus."
"Goedel's functional ""Dialectica"" interpretation can be used to extract
functional programs from non-constructive proofs in arithmetic by employing two
sorts of higher-order witnessing terms: positive realisers and negative
counterexamples. In the original interpretation decidability of atoms is
required to compute the correct counterexample from a set of candidates. When
combined with recursion, this choice needs to be made for every step in the
extracted program, however, in some special cases the decision on negative
witnesses can be calculated only once. We present a variant of the
interpretation in which the time complexity of extracted programs can be
improved by marking the chosen witness and thus avoiding recomputation. The
achieved effect is similar to using an abortive control operator to interpret
computational content of non-constructive principles."
"We investigate (quantifier-free) spatial constraint languages with equality,
contact and connectedness predicates as well as Boolean operations on regions,
interpreted over low-dimensional Euclidean spaces. We show that the complexity
of reasoning varies dramatically depending on the dimension of the space and on
the type of regions considered. For example, the logic with the
interior-connectedness predicate (and without contact) is undecidable over
polygons or regular closed sets in the Euclidean plane, NP-complete over
regular closed sets in three-dimensional Euclidean space, and ExpTime-complete
over polyhedra in three-dimensional Euclidean space."
"We give the first cut-free ExpTime (optimal) tableau decision procedure for
the logic CPDLreg, which extends Converse-PDL with regular inclusion axioms
characterized by finite automata. The logic CPDLreg is the combination of
Converse-PDL and regular grammar logic with converse. Our tableau decision
procedure uses global state caching and has been designed to increase
efficiency and allow various optimization techniques, including on-the-fly
propagation of local and global (in)consistency."
"Ludics is peculiar in the panorama of game semantics: we first have the
definition of interaction-composition and then we have semantical types, as a
set of strategies which ""behave well"" and react in the same way to a set of
tests. The semantical types which are interpretations of logical formulas enjoy
a fundamental property, called internal completeness, which characterizes
ludics and sets it apart also from realizability. Internal completeness entails
standard full completeness as a consequence. A growing body of work start to
explore the potential of this specific interactive approach. However, ludics
has some limitations, which are consequence of the fact that in the original
formulation, strategies are abstractions of MALL proofs. On one side, no
repetitions are allowed. On the other side, the proofs tend to rely on the very
specific properties of the MALL proof-like strategies, making it difficult to
transfer the approach to semantical types into different settings. In this
paper, we provide an extension of ludics which allows repetitions and show that
one can still have interactive types and internal completeness. From this, we
obtain full completeness w.r.t. a polarized version of MELL. In our extension,
we use less properties than in the original formulation, which we believe is of
independent interest. We hope this may open the way to applications of ludics
approach to larger domains and different settings."
"We consider timed Petri nets, i.e., unbounded Petri nets where each token
carries a real-valued clock. Transition arcs are labeled with time intervals,
which specify constraints on the ages of tokens. Our cost model assigns token
storage costs per time unit to places, and firing costs to transitions. We
study the cost to reach a given control-state. In general, a cost-optimal run
may not exist. However, we show that the infimum of the costs is computable."
"The B\""uchi non-emptiness problem for timed automata refers to deciding if a
given automaton has an infinite non-Zeno run satisfying the B\""uchi accepting
condition. The standard solution to this problem involves adding an auxiliary
clock to take care of the non-Zenoness. In this paper, it is shown that this
simple transformation may sometimes result in an exponential blowup. A
construction avoiding this blowup is proposed. It is also shown that in many
cases, non-Zenoness can be ascertained without extra construction. An
on-the-fly algorithm for the non-emptiness problem, using non-Zenoness
construction only when required, is proposed. Experiments carried out with a
prototype implementation of the algorithm are reported."
"We study bisimulations for useful description logics. The simplest among the
considered logics is $\mathcal{ALC}_{reg}$ (a variant of PDL). The others
extend that logic with inverse roles, nominals, quantified number restrictions,
the universal role, and/or the concept constructor for expressing the local
reflexivity of a role. They also allow role axioms. We give results about
invariance of concepts, TBoxes and ABoxes, preservation of RBoxes and knowledge
bases, and the Hennessy-Milner property w.r.t. bisimulations in the considered
description logics. Using the invariance results we compare the expressiveness
of the considered description logics w.r.t. concepts, TBoxes and ABoxes. Our
results about separating the expressiveness of description logics are naturally
extended to the case when instead of $\mathcal{ALC}_{reg}$ we have any sublogic
of $\mathcal{ALC}_{reg}$ that extends $\mathcal{ALC}$. We also provide results
on the largest auto-bisimulations and quotient interpretations w.r.t. such
equivalence relations. Such results are useful for minimizing interpretations
and concept learning in description logics. To deal with minimizing
interpretations for the case when the considered logic allows quantified number
restrictions and/or the constructor for the local reflexivity of a role, we
introduce a new notion called QS-interpretation, which is needed for obtaining
expected results. By adapting Hopcroft's automaton minimization algorithm and
the Paige-Tarjan algorithm, we give efficient algorithms for computing the
partition corresponding to the largest auto-bisimulation of a finite
interpretation."
"Type-based amortised resource analysis following Hofmann and Jost---where
resources are associated with individual elements of data structures and doled
out to the programmer under a linear typing discipline---have been successful
in providing concrete resource bounds for functional programs, with good
support for inference. In this work we translate the idea of amortised resource
analysis to imperative pointer-manipulating languages by embedding a logic of
resources, based on the affine intuitionistic Logic of Bunched Implications,
within Separation Logic. The Separation Logic component allows us to assert the
presence and shape of mutable data structures on the heap, while the resource
component allows us to state the consumable resources associated with each
member of the structure. We present the logic on a small imperative language,
based on Java bytecode, with procedures and mutable heap. We have formalised
the logic and its soundness property within the Coq proof assistant and
extracted a certified verification condition generator. We also describe an
proof search procedure that allows generated verification conditions to be
discharged while using linear programming to infer consumable resource
annotations. We demonstrate the logic on some examples, including proving the
termination of in-place list reversal on lists with cyclic tails."
"We present an abstract framework for concurrent processes in which atomic
steps have generic side effects, handled according to the principle of monadic
encapsulation of effects. Processes in this framework are potentially infinite
resumptions, modelled using final coalgebras over the monadic base. As a
calculus for such processes, we introduce a concurrent extension of Moggi's
monadic metalanguage of effects. We establish soundness and completeness of a
natural equational axiomatisation of this calculus. Moreover, we identify a
corecursion scheme that is explicitly definable over the base language and
provides flexible expressive means for the definition of new operators on
processes, such as parallel composition. As a worked example, we prove the
safety of a generic mutual exclusion scheme using a verification logic built on
top of the equational calculus."
"Heterogeneous nonmonotonic multi-context systems (MCS) permit different
logics to be used in different contexts, and link them via bridge rules. We
investigate the role of symmetry detection and symmetry breaking in such
systems to eliminate symmetric parts of the search space and, thereby, simplify
the evaluation process. We propose a distributed algorithm that takes a local
stance, i.e., computes independently the partial symmetries of a context and,
in order to construct potential symmetries of the whole, combines them with
those partial symmetries returned by neighbouring contexts. We prove the
correctness of our methods. We instantiate such symmetry detection and symmetry
breaking in a multi-context system with contexts that use answer set programs,
and demonstrate computational benefit on some recently proposed benchmarks."
"For continuous-time Markov chains, the model-checking problem with respect to
continuous-time stochastic logic (CSL) has been introduced and shown to be
decidable by Aziz, Sanwal, Singhal and Brayton in 1996. Their proof can be
turned into an approximation algorithm with worse than exponential complexity.
In 2000, Baier, Haverkort, Hermanns and Katoen presented an efficient
polynomial-time approximation algorithm for the sublogic in which only binary
until is allowed. In this paper, we propose such an efficient polynomial-time
approximation algorithm for full CSL. The key to our method is the notion of
stratified CTMCs with respect to the CSL property to be checked. On a
stratified CTMC, the probability to satisfy a CSL path formula can be
approximated by a transient analysis in polynomial time (using uniformization).
We present a measure-preserving, linear-time and -space transformation of any
CTMC into an equivalent, stratified one. This makes the present work the
centerpiece of a broadly applicable full CSL model checker. Recently, the
decision algorithm by Aziz et al. was shown to work only for stratified CTMCs.
As an additional contribution, our measure-preserving transformation can be
used to ensure the decidability for general CTMCs."
"This paper investigates the time-bounded version of the reachability problem
for hybrid automata. This problem asks whether a given hybrid automaton can
reach a given target location within T time units, where T is a constant
rational value. We show that, in contrast to the classical (unbounded)
reachability problem, the timed-bounded version is decidable for rectangular
hybrid automata provided only non-negative rates are allowed. This class of
systems is of practical interest and subsumes, among others, the class of
stopwatch automata. We also show that the problem becomes undecidable if either
diagonal constraints or both negative and positive rates are allowed."
"Lambek's non-associative syntactic calculus (NL) excels in its resource
consciousness: the usual structural rules for weakening, contraction, exchange
and even associativity are all dropped. Recently, there have been proposals for
conservative extensions dispensing with NL's intuitionistic bias towards
sequents with single conclusions: De Groote and Lamarche's classical
non-associative Lambek calculus (CNL) and the Lambek-Grishin calculus (LG) of
Moortgat and associates. We demonstrate Andreoli's focalization property for
said proposals: a normalization result for Cut-free sequent derivations
identifying to a large extent those differing only by trivial rule
permutations. In doing so, we proceed from a `uniform' sequent presentation,
deriving CNL from LG through the addition of structural rules. The
normalization proof proceeds by the construction of syntactic phase models
wherein every `truth' has a focused proof, similar to work of Okada and of
Herbelin and Lee."
"This volume contains the Proceedings of the Second International Symposium on
Games, Automata, Languages, and Formal Verification (GandALF 2011). The
conference was held in Minori (Amalfi Coast, Italy), from the 15th to the 17th
of June 2011. The aim of the GandALF Symposium is to provide a forum for
researchers from different areas and with different background, that share a
common interest in game theory, mathematical logic, automata theory, and their
applications to the specification, design, and verification of complex systems.
This proceedings contain the abstracts of three invited talks and nineteen
regular papers that have been selected through a rigorous reviewing process
according to originality, quality, and relevance to the topics of the
symposium."
"Symbolic model checking by using BDDs has greatly improved the applicability
of model checking. Nevertheless, BDD based symbolic model checking can still be
very memory and time consuming. One main reason is the complex transition
relation of systems. Sometimes, it is even not possible to generate the
transition relation, due to its exhaustive memory requirements. To diminish
this problem, the use of partitioned transition relations has been proposed.
However, there are still systems which can not be verified at all. Furthermore,
if the granularity of the partitions is too fine, the time required for
verification may increase. In this paper we target the symbolic verification of
asynchronous concurrent systems. For such systems we present an approach which
uses similarities in the transition relation to get further memory reductions
and runtime improvements. By applying our approach, even the verification of
systems with an previously intractable transition relation becomes feasible."
"Petri net unfoldings are a useful tool to tackle state-space explosion in
verification and related tasks. Moreover, their structure allows to access
directly the relations of causal precedence, concurrency, and conflict between
events. Here, we explore the data structure further, to determine the following
relation: event a is said to reveal event b iff the occurrence of a implies
that b inevitably occurs, too, be it before, after, or concurrently with a.
Knowledge of reveals facilitates in particular the analysis of partially
observable systems, in the context of diagnosis, testing or verification; it
can also be used to generate more concise representations of behaviours via
abstractions. The reveals relation was previously introduced in the context of
fault diagnosis, where it was shown that the reveals relation was decidable:
for a given pair a,b in the unfolding U of a safe Petri net N, a finite prefix
P of U is sufficient to decide whether or not a reveals b. In this paper, we
first considerably improve the bound on |P|. We then show that there exists an
efficient algorithm for computing the relation on a given prefix. We have
implemented the algorithm and report on experiments."
"In this paper we study the liveness of several MUTEX solutions by
representing them as processes in PAFAS s, a CCS-like process algebra with a
specific operator for modelling non-blocking reading behaviours. Verification
is carried out using the tool FASE, exploiting a correspondence between
violations of the liveness property and a special kind of cycles (called
catastrophic cycles) in some transition system. We also compare our approach
with others in the literature. The aim of this paper is twofold: on the one
hand, we want to demonstrate the applicability of FASE to some concrete,
meaningful examples; on the other hand, we want to study the impact of
introducing non-blocking behaviours in modelling concurrent systems."
"Model checking of open pushdown systems (OPD) w.r.t. standard branching
temporal logics (pushdown module checking or PMC) has been recently
investigated in the literature, both in the context of environments with
perfect and imperfect information about the system (in the last case, the
environment has only a partial view of the system's control states and stack
content). For standard CTL, PMC with imperfect information is known to be
undecidable. If the stack content is assumed to be visible, then the problem is
decidable and 2EXPTIME-complete (matching the complexity of PMC with perfect
information against CTL). The decidability status of PMC with imperfect
information against CTL restricted to the case where the depth of the stack
content is visible is open. In this paper, we show that with this restriction,
PMC with imperfect information against CTL remains undecidable. On the other
hand, we individuate an interesting subclass of OPDS with visible stack content
depth such that PMC with imperfect information against the existential fragment
of CTL is decidable and in 2EXPTIME. Moreover, we show that the program
complexity of PMC with imperfect information and visible stack content against
CTL is 2EXPTIME-complete (hence, exponentially harder than the program
complexity of PMC with perfect information, which is known to be
EXPTIME-complete)."
"For a class L of languages let PDL[L] be an extension of Propositional
Dynamic Logic which allows programs to be in a language of L rather than just
to be regular. If L contains a non-regular language, PDL[L] can express
non-regular properties, in contrast to pure PDL.
  For regular, visibly pushdown and deterministic context-free languages, the
separation of the respective PDLs can be proven by automata-theoretic
techniques. However, these techniques introduce non-determinism on the automata
side. As non-determinism is also the difference between DCFL and CFL, these
techniques seem to be inappropriate to separate PDL[DCFL] from PDL[CFL].
Nevertheless, this separation is shown but for programs without test operators."
"We introduce a new class of graphs which we call P-transitive graphs, lying
between transitive and 3-transitive graphs. First we show that the analogue of
de Jongh-Sambin Theorem is false for wellfounded P-transitive graphs; then we
show that the mu-calculus fixpoint hierarchy is infinite for P-transitive
graphs. Both results contrast with the case of transitive graphs. We give also
an undecidability result for an enriched mu-calculus on P-transitive graphs.
Finally, we consider a polynomial time reduction from the model checking
problem on arbitrary graphs to the model checking problem on P-transitive
graphs. All these results carry over to 3-transitive graphs."
"This paper deals with the problem of point-to-point reachability in
multi-linear systems. These systems consist of a partition of the Euclidean
space into a finite number of regions and a constant derivative assigned to
each region in the partition, which governs the dynamical behavior of the
system within it. The reachability problem for multi-linear systems has been
proven to be decidable for the two-dimensional case and undecidable for the
dimension three and higher.
  Multi-linear systems however exhibit certain properties that make them very
suitable for topological analysis. We prove that reachability can be decided
exactly in the 3-dimensional case when systems satisfy certain conditions. We
show with experiments that our approach can be orders of magnitude more
efficient than simulation."
"We study the problem of automatically computing the controllable region of a
Linear Hybrid Automaton, with respect to a safety objective. We describe the
techniques that are needed to effectively and efficiently implement a
recently-proposed solution procedure, based on polyhedral abstractions of the
state space. Supporting experimental results are presented, based on an
implementation of the proposed techniques on top of the tool PHAVer."
"An infinite run of a timed automaton is Zeno if it spans only a finite amount
of time. Such runs are considered unfeasible and hence it is important to
detect them, or dually, find runs that are non-Zeno. Over the years important
improvements have been obtained in checking reachability properties for timed
automata. We show that some of these very efficient optimizations make testing
for Zeno runs costly. In particular we show NP-completeness for the
LU-extrapolation of Behrmann et al. We analyze the source of this complexity in
detail and give general conditions on extrapolation operators that guarantee a
(low) polynomial complexity of Zenoness checking. We propose a slight weakening
of the LU-extrapolation that satisfies these conditions."
"We prove the decidability of Ticket Entailment. Raised by Anderson and Belnap
within the framework of relevance logic, this question is equivalent to the
question of the decidability of type inhabitation in simply-typed combinatory
logic with the partial basis BB'IW. We solve the equivalent problem of type
inhabitation for the restriction of simply-typed lambda-calculus to
hereditarily right-maximal terms."
"Probabilistic automata (PAs) have been successfully applied in formal
verification of concurrent and stochastic systems. Efficient model checking
algorithms have been studied, where the most often used logics for expressing
properties are based on probabilistic computation tree logic (PCTL) and its
extension PCTL^*. Various behavioral equivalences are proposed, as a powerful
tool for abstraction and compositional minimization for PAs. Unfortunately, the
equivalences are well-known to be sound, but not complete with respect to the
logical equivalences induced by PCTL or PCTL*. The desire of a both sound and
complete behavioral equivalence has been pointed out by Segala in 1995, but
remains open throughout the years. In this paper we introduce novel notions of
strong bisimulation relations, which characterize PCTL and PCTL* exactly. We
extend weak bisimulations that characterize PCTL and PCTL* without next
operator, respectively. Further, we also extend the framework to simulation
preorders. Thus, our paper bridges the gap between logical and behavioral
equivalences and preorders in this setting."
"Computability logic is a formal theory of computability. The earlier article
""Introduction to cirquent calculus and abstract resource semantics"" by
Japaridze proved soundness and completeness for the basic fragment CL5 of
computability logic. The present article extends that result to the more
expressive cirquent calculus system CL6, which is a conservative extension of
both CL5 and classical propositional logic."
"We give the first cut-free ExpTime (optimal) tableau decision procedure for
checking satisfiability of a knowledge base in the description logic SHI, which
extends the description logic ALC with transitive roles, inverse roles and role
hierarchies."
"Embedded systems are everywhere, from home appliances to critical systems
such as medical devices. They usually have associated timing constraints that
need to be verified for the implementation. Here, we use an untimed bounded
model checker to verify timing properties of embedded C programs. We propose an
approach to specify discrete time timing constraints using code annotations.
The annotated code is then automatically translated to code that manipulates
auxiliary timer variables and is thus suitable as input to conventional,
untimed software model checker such as ESBMC. Thus, we can check timing
constraints in the same way and at the same time as untimed system
requirements, and even allow for interaction between them. We applied the
proposed method in a case study, and verified timing constraints of a pulse
oximeter, a noninvasive medical device that measures the oxygen saturation of
arterial blood."
"The paper discusses the role of interpretations, understood as multifunctions
that preserve and reflect logical consequence, as refinement witnesses in the
general setting of pi-institutions. This leads to a smooth generalization of
the refinement-by-interpretation approach, recently introduced by the authors
in more specific contexts. As a second, yet related contribution a basis is
provided to build up a refinement calculus of structured specifications in and
across arbitrary pi-institutions."
"Simulink/Stateflow charts are widely used in industry for the specification
of control systems, which are often safety-critical. This suggests a need for a
formal treatment of such models. In previous work, we have proposed a technique
for automatic generation of formal models of Stateflow blocks to support
refinement-based reasoning. In this article, we present a refinement strategy
that supports the verification of automatically generated sequential C
implementations of Stateflow charts. In particular, we discuss how this
strategy can be specialised to take advantage of architectural features in
order to allow a higher level of automation."
"This article defines a complement of a function and conditions for existence
of such a complement function and presents few algorithms to construct a
complement."
"We develop local reasoning techniques for message passing concurrent programs
based on ideas from separation logics and resource usage analysis. We extend
processes with permission- resources and define a reduction semantics for this
extended language. This provides a foundation for interpreting separation
formulas for message-passing concurrency. We also define a sound proof system
permitting us to infer satisfaction compositionally using local,
separation-based reasoning."
"This paper talk about the influence of Connection and Dispersion on
Computational Complexity. And talk about the HornCNF's connection and CNF's
dispersion, and show the difference between CNFSAT and HornSAT. First, I talk
the relation between MUC decision problem and classifying the truth value
assignment. Second, I define the two inner products (""inner product"" and ""inner
harmony"") and talk about the influence of orthogonal and correlation to MUC.
And we can not reduce MUC to Orthogonalization MUC by using HornMUC in
polynomial size because HornMUC have high orthogonal of inner harmony and MUC
do not. So DP is not P, and NP is not P."
"We design a library for binary field arithmetic and we supply a core API
which is completely developed in DLAL, extended with a fix point formula. Since
DLAL is a restriction of linear logic where only functional programs with
polynomial evaluation cost can be typed, we obtain the core of a functional
programming setting for binary field arithmetic with built-in polynomial
complexity."
"With the help of the Internet, social networks have grown rapidly. This has
increased security requirements. We present a formalization of social networks
as composite behavioral objects, defined using the Observational Transition
System (OTS) approach. Our definition is then translated to the OTS/CafeOBJ
algebraic specification methodology. This translation allows the formal
verification of safety properties for social networks via the Proof Score
method. Finally, using this methodology we formally verify some security
properties."
"This volume contains the pre-proceedings of ICE'11, the 4th Interaction and
Concurrency Experience workshop, which was held in Reykjavik, Iceland on the
9th of June 2011 as a satellite event of DisCoTec'11.
  The topic of ICE'11 was Reliable and Contract-based Interaction. Reliable
interactions are, e.g., those enjoying suitable logical, behavioural, or
security properties, or adhering to certain QoS standards. Contract-based
interactions are, e.g., those where the interacting entities are committed to
give certain guarantees whenever certain assumptions are met by their operating
environment.
  The ICE procedure for paper selection allows for PC members to interact,
anonymously, with authors. During the review phase, each submitted paper is
published on a Wiki and associated with a discussion forum whose access is
restricted to the authors and to all the PC members not declaring a conflict of
interests. The PC members post comments and questions that the authors reply
to. Each paper was reviewed by four PC members, and altogether 8 papers (out of
12) were accepted for publication.
  We were proud to host three invited talks by Rocco De Nicola (joint with
PACO), Simon Gay and Prakash Panangaden, whose abstracts are included in this
volume together with the regular papers."
"Seeking a general framework for reasoning about and comparing programming
languages, we derive a new view of Milner's CCS. We construct a category E of
plays, and a subcategory V of views. We argue that presheaves on V adequately
represent innocent strategies, in the sense of game semantics. We then equip
innocent strategies with a simple notion of interaction. This results in an
interpretation of CCS.
  Based on this, we propose a notion of interactive equivalence for innocent
strategies, which is close in spirit to Beffara's interpretation of testing
equivalences in concurrency theory. In this framework we prove that the
analogues of fair and must testing equivalences coincide, while they differ in
the standard setting."
"PCF is a sequential simply typed lambda calculus language. There is a unique
order-extensional fully abstract cpo model of PCF, built up from equivalence
classes of terms. In 1979, G\'erard Berry defined the stable order in this
model and proved that the extensional and the stable order together form a
bicpo. He made the following two conjectures: 1) ""Extensional and stable order
form not only a bicpo, but a bidomain."" We refute this conjecture by showing
that the stable order is not bounded complete, already for finitary PCF of
second-order types. 2) ""The stable order of the model has the syntactic order
as its image: If a is less than b in the stable order of the model, for finite
a and b, then there are normal form terms A and B with the semantics a, resp.
b, such that A is less than B in the syntactic order."" We give counter-examples
to this conjecture, again in finitary PCF of second-order types, and also
refute an improved conjecture: There seems to be no simple syntactic
characterization of the stable order. But we show that Berry's conjecture is
true for unary PCF. For the preliminaries, we explain the basic fully abstract
semantics of PCF in the general setting of (not-necessarily complete) partial
order models (f-models.) And we restrict the syntax to ""game terms"", with a
graphical representation."
"Digital Right Management (DRM) Systems have been created to meet the need for
digital content protection and distribution. In this paper we present some of
the directions of our ongoing research to apply algebraic specification
techniques on mobile DRM systems."
"The coordination modelling language Paradigm addresses collaboration between
components in terms of dynamic constraints. Within a Paradigm model, component
dynamics are consistently specified at a detailed and a global level of
abstraction. To enable automated verification of Paradigm models, a translation
of Paradigm into process algebra has been defined in previous work. In this
paper we investigate, guided by a client-server example, reduction of Paradigm
models based on a notion of global inertness. Representation of Paradigm models
as process algebraic specifications helps to establish a property-preserving
equivalence relation between the original and the reduced Paradigm model.
Experiments indicate that in this way larger Paradigm models can be analyzed."
"We summarize the main results proved in recent work on the parameterized
verification of safety properties for ad hoc network protocols. We consider a
model in which the communication topology of a network is represented as a
graph. Nodes represent states of individual processes. Adjacent nodes represent
single-hop neighbors. Processes are finite state automata that communicate via
selective broadcast messages. Reception of a broadcast is restricted to
single-hop neighbors. For this model we consider a decision problem that can be
expressed as the verification of the existence of an initial topology in which
the execution of the protocol can lead to a configuration with at least one
node in a certain state. The decision problem is parametric both on the size
and on the form of the communication topology of the initial configurations. We
draw a complete picture of the decidability and complexity boundaries of this
problem according to various assumptions on the possible topologies."
"We investigate several technical and conceptual questions.
  Our main subject is the investigation of independence as a ternary relation
in the context of non-monotonic logic. In the context of probability, this
investigation was started by W.Spohn et al., and then followed by J.Pearl. We
look at products of function sets, and thus continue our own investigation of
independence in non-monotonic logic. We show that a finite characterization of
this relation in our context is impossible, and indicate how to construct all
valid rules."
"In this paper, we introduce an extension of the GSOS rule format with
predicates such as termination, convergence and divergence. For this format we
generalize the technique proposed by Aceto, Bloom and Vaandrager for the
automatic generation of ground-complete axiomatizations of bisimilarity over
GSOS systems. Our procedure is implemented in a tool that receives SOS
specifications as input and derives the corresponding axiomatizations
automatically. This paves the way to checking strong bisimilarity over process
terms by means of theorem-proving techniques."
"One of the proposed solutions for improving the scalability of semantics of
programming languages is Component-Based Semantics, introduced by Peter D.
Mosses. It is expected that this framework can also be used effectively for
modular meta theoretic reasoning. This paper presents a formalization of
Component-Based Semantics in the theorem prover Coq. It is based on Modular
SOS, a variant of SOS, and makes essential use of dependent types, while
profiting from type classes. This formalization constitutes a contribution
towards modular meta theoretic formalizations in theorem provers. As a small
example, a modular proof of determinism of a mini-language is developed."
"Many programming languages and tools, ranging from grep to the Java String
library, contain regular expression matchers. Rather than first translating a
regular expression into a deterministic finite automaton, such implementations
typically match the regular expression on the fly. Thus they can be seen as
virtual machines interpreting the regular expression much as if it were a
program with some non-deterministic constructs such as the Kleene star. We
formalize this implementation technique for regular expression matching using
operational semantics. Specifically, we derive a series of abstract machines,
moving from the abstract definition of matching to increasingly realistic
machines. First a continuation is added to the operational semantics to
describe what remains to be matched after the current expression. Next, we
represent the expression as a data structure using pointers, which enables
redundant searches to be eliminated via testing for pointer equality. From
there, we arrive both at Thompson's lockstep construction and a machine that
performs some operations in parallel, suitable for implementation on a large
number of cores, such as a GPU. We formalize the parallel machine using process
algebra and report some preliminary experiments with an implementation on a
graphics processor using CUDA."
"We continue with the task of obtaining a unifying view of process semantics
by considering in this case the logical characterization of the semantics. We
start by considering the classic linear time-branching time spectrum developed
by R.J. van Glabbeek. He provided a logical characterization of most of the
semantics in his spectrum but, without following a unique pattern. In this
paper, we present a uniform logical characterization of all the semantics in
the enlarged spectrum. The common structure of the formulas that constitute all
the corresponding logics gives us a much clearer picture of the spectrum,
clarifying the relations between the different semantics, and allows us to
develop generic proofs of some general properties of the semantics."
"Rewriting is a common approach to logic optimization based on local
transformations. Most commercially available logic synthesis tools include a
rewriting engine that may be used multiple times on the same netlist during
optimization. This paper presents an And-Inverter graph based rewriting
algorithm using 5-input cuts. The best circuits are pre-computed for a subset
of NPN classes of 5-variable functions. Cut enumeration and Boolean matching
are used to identify replacement candidates. The presented approach is expected
to complement existing rewriting approaches which are usually based on 4-input
cuts. The experimental results show that, by adding the new rewriting algorithm
to ABC synthesis tool, we can further reduce the area of heavily optimized
large circuits by 5.57% on average."
"In this paper, for a given sequentially Yoneda-complete T_1 quasi-metric
space (X,d), the domain theoretic models of the hyperspace K_0(X) of nonempty
compact subsets of (X,d) are studied. To this end, the $\omega$-Plotkin domain
of the space of formal balls BX, denoted by CBX is considered. This domain is
given as the chain completion of the set of all finite subsets of BX with
respect to the Egli-Milner relation. Further, a map $\phi:K_0(X)\rightarrow
CBX$ is established and proved that it is an embedding whenever K_0(X) is
equipped with the Vietoris topology and respectively CBX with the Scott
topology. Moreover, if any compact subset of (X,d) is d^{-1}-precompact, \phi
is an embedding with respect to the topology of Hausdorff quasi-metric H_d on
K_0(X). Therefore, it is concluded that (CBX,\sqsubseteq,\phi) is an
$\omega$-computational model for the hyperspace K_0(X) endowed with the
Vietoris and respectively the Hausdorff topology. Next, an algebraic
sequentially Yoneda-complete quasi-metric D on CBX$ is introduced in such a way
that the specialization order $\sqsubseteq_D$ is equivalent to the usual
partial order of CBX and, furthermore, $\phi:({\cal
K}_0(X),H_d)\rightarrow({\bf C}{\bf B}X,D)$ is an isometry. This shows that
(CBX,\sqsubseteq,\phi,D) is a quantitative $\omega$-computational model for
(K_(X),H_d)."
"We propose a new library to model and verify hardware circuits in the Coq
proof assistant. This library allows one to easily build circuits by following
the usual pen-and-paper diagrams. We define a deep-embedding: we use a
(dependently typed) data-type that models the architecture of circuits, and a
meaning function. We propose tactics that ease the reasoning about the behavior
of the circuits, and we demonstrate that our approach is practicable by proving
the correctness of various circuits: a text-book divide and conquer adder of
parametric size, some higher-order combinators of circuits, and some sequential
circuits: a buffer, and a register."
"We present a formalization of modern SAT solvers and their properties in a
form of abstract state transition systems. SAT solving procedures are described
as transition relations over states that represent the values of the solver's
global variables. Several different SAT solvers are formalized, including both
the classical DPLL procedure and its state-of-the-art successors. The
formalization is made within the Isabelle/HOL system and the total correctness
(soundness, termination, completeness) is shown for each presented system (with
respect to a simple notion of satisfiability that can be manually checked). The
systems are defined in a general way and cover procedures used in a wide range
of modern SAT solvers. Our formalization builds up on the previous work on
state transition systems for SAT, but it gives machine-verifiable proofs,
somewhat more general specifications, and weaker assumptions that ensure the
key correctness properties. The presented proofs of formal correctness of the
transition systems can be used as a key building block in proving correctness
of SAT solvers by using other verification approaches."
"Covariant-contravariant simulation is a combination of standard (covariant)
simulation, its contravariant counterpart and bisimulation. We have previously
studied its logical characterization by means of the covariant-contravariant
modal logic. Moreover, we have investigated the relationships between this
model and that of modal transition systems, where two kinds of transitions (the
so-called may and must transitions) were combined in order to obtain a simple
framework to express a notion of refinement over state-transition models. In a
classic paper, Boudol and Larsen established a precise connection between the
graphical approach, by means of modal transition systems, and the logical
approach, based on Hennessy-Milner logic without negation, to system
specification. They obtained a (graphical) representation theorem proving that
a formula can be represented by a term if, and only if, it is consistent and
prime. We show in this paper that the formulae from the covariant-contravariant
modal logic that admit a ""graphical"" representation by means of processes,
modulo the covariant-contravariant simulation preorder, are also the consistent
and prime ones. In order to obtain the desired graphical representation result,
we first restrict ourselves to the case of covariant-contravariant systems
without bivariant actions. Bivariant actions can be incorporated later by means
of an encoding that splits each bivariant action into its covariant and its
contravariant parts."
"Linearization is the procedure of rewriting a process term into a linear
form, which consist only of basic operators of the process language. This
procedure is interesting both from a theoretical and a practical point of view.
In particular, a linearization algorithm is needed for the Compositional
Interchange Format (CIF), an automaton based modeling language.
  The problem of devising efficient linearization algorithms is not trivial,
and has been already addressed in literature. However, the linearization
algorithms obtained are the result of an inventive process, and the proof of
correctness comes as an afterthought. Furthermore, the semantic specification
of the language does not play an important role on the design of the algorithm.
  In this work we present a method for obtaining an efficient linearization
algorithm, through a step-wise refinement of the SOS rules of CIF. As a result,
we show how the semantic specification of the language can guide the
implementation of such a procedure, yielding a simple proof of correctness."
"We study the relation between process calculi that differ in their either
synchronous or asynchronous interaction mechanism. Concretely, we are
interested in the conditions under which synchronous interaction can be
implemented using just asynchronous interactions in the pi-calculus. We assume
a number of minimal conditions referring to the work of Gorla: a ""good""
encoding must be compositional and preserve and reflect computations,
deadlocks, divergence, and success. Under these conditions, we show that it is
not possible to encode synchronous interactions without introducing additional
causal dependencies in the translation."
"We introduce event identifier logic (EIL) which extends Hennessy-Milner logic
by the addition of (1) reverse as well as forward modalities, and (2)
identifiers to keep track of events. We show that this logic corresponds to
hereditary history-preserving (HH) bisimulation equivalence within a particular
true-concurrency model, namely stable configuration structures. We furthermore
show how natural sublogics of EIL correspond to coarser equivalences. In
particular we provide logical characterisations of weak history-preserving (WH)
and history-preserving (H) bisimulation. Logics corresponding to HH and H
bisimulation have been given previously, but not to WH bisimulation (when
autoconcurrency is allowed), as far as we are aware. We also present
characteristic formulas which characterise individual structures with respect
to history-preserving equivalences."
"There are various interesting semantics' (extensions) designed for
argumentation frameworks. They enable to assign a meaning, e.g., to odd-length
cycles. Our main motivation is to transfer semantics' proposed by Baroni,
Giacomin and Guida for argumetation frameworks with odd-length cycles to logic
programs with odd-length cycles through default negation. The developed
construction is even stronger. For a given logic program an argumentation
framework is defined. The construction enables to transfer each semantics of
the resulting argumentation framework to a semantics of the given logic
program. Weak points of the construction are discussed and some future
continuations of this approach are outlined."
"In this paper we explore a unifying approach --- that of hypotheses
assumption --- as a means to provide a semantics for all Normal Logic Programs
(NLPs), the Minimal Hypotheses (MH) semantics. This semantics takes a positive
hypotheses assumption approach as a means to guarantee the desirable properties
of model existence, relevance and cumulativity, and of generalizing the Stable
Models in the process. To do so we first introduce the fundamental semantic
concept of minimality of assumed positive hypotheses, define the MH semantics,
and analyze the semantics' properties and applicability. Indeed, abductive
Logic Programming can be conceptually captured by a strategy centered on the
assumption of abducibles (or hypotheses). Likewise, the Argumentation
perspective of Logic Programs also lends itself to an arguments (or hypotheses)
assumption approach. Previous works on Abduction have depicted the atoms of
default negated literals in NLPs as abducibles, i.e., assumable hypotheses. We
take a complementary and more general view than these works to NLP semantics by
employing positive hypotheses instead."
"In this paper we present the core of LoCo, a logic-based high-level
representation language for expressing configuration problems. LoCo shall allow
to model these problems in an intuitive and declarative way, the dynamic
aspects of configuration notwithstanding. Our logic enforces that
configurations contain only finitely many components and reasoning can be
reduced to the task of model construction."
"We are aiming at a semantics of logic programs with preferences defined on
rules, which always selects a preferred answer set, if there is a non-empty set
of (standard) answer sets of the given program. It is shown in a seminal paper
by Brewka and Eiter that the goal mentioned above is incompatible with their
second principle and it is not satisfied in their semantics of prioritized
logic programs. Similarly, also according to other established semantics, based
on a prescriptive approach, there are programs with standard answer sets, but
without preferred answer sets. According to the standard prescriptive approach
no rule can be fired before a more preferred rule, unless the more preferred
rule is blocked. This is a rather imperative approach, in its spirit.
  In our approach, rules can be blocked by more preferred rules, but the rules
which are not blocked are handled in a more declarative style, their execution
does not depend on the given preference relation on the rules. An argumentation
framework (different from the Dung's framework) is proposed in this paper.
Argu- mentation structures are derived from the rules of a given program. An
attack relation on argumentation structures is defined, which is derived from
attacks of more preferred rules against the less preferred rules. Preferred
answer sets correspond to complete argumentation structures, which are not
blocked by other complete argumentation structures."
"The work we describe here is a part of a research program of developing
foundations of declarative solving of search problems. We consider the model
expansion task as the task representing the essence of search problems where we
are given an instance of a problem and are searching for a solution satisfying
certain properties. Such tasks are common in artificial intelligence, formal
verification, computational biology. Recently, the model expansion framework
was extended to deal with multiple modules. In the current paper, inspired by
practical combined solvers, we introduce an algorithm to solve model expansion
tasks for modular systems. We show that our algorithm closely corresponds to
what is done in practice in different areas such as Satisfiability Modulo
Theories (SMT), Integer Linear Programming (ILP), Answer Set Programming (ASP)."
"Given sets $\Phi_1=\{\phi_{11},...,\phi_{1u(1)}\},
...,\Phi_{z}=\{\phi_{z1},...,\phi_{zu(z)}\}$ of boolean formulas, a formula
$\omega$ follows from the conjunction $\bigwedge\Phi_i= \bigwedge \phi_{ij}$
iff $\neg \omega\wedge \bigwedge_{i=1}^z \Phi_i$ is unsatisfiable.
  Now assume that, given integers $0\leq e_i < u(i)$, we must check if $\neg
\omega\wedge \bigwedge_{i=1}^z \Phi'_i$ remains unsatisfiable, where
$\Phi'_i\subseteq \Phi_i$ is obtained by deleting $\,\,e_{i}$ arbitrarily
chosen formulas of $\Phi_i$, for each $i=1,...,z.$
  Intuitively, does $\omega$ {\it stably} follow, after removing $e_i$ random
formulas from each $\Phi_i$?
  We construct a quadratic reduction of this problem to the consequence problem
in infinite-valued \luk\ logic \L$_\infty$. In this way we obtain a
self-contained proof that the \L$_\infty$-consequence problem is coNP-complete."
"This paper presents a new application of logic programming to a real-life
problem in hydraulic engineering. The work is developed as a collaboration of
computer scientists and hydraulic engineers, and applies Constraint Logic
Programming to solve a hard combinatorial problem. This application deals with
one aspect of the design of a water distribution network, i.e., the valve
isolation system design.
  We take the formulation of the problem by Giustolisi and Savic (2008) and
show how, thanks to constraint propagation, we can get better solutions than
the best solution known in the literature for the Apulian distribution network.
  We believe that the area of the so-called hydroinformatics can benefit from
the techniques developed in Constraint Logic Programming and possibly from
other areas of logic programming, such as Answer Set Programming."
"This volume consists of the proceedings of the 5th Workshop on Formal
Languages and Analysis of Contract-Oriented Software (FLACOS'11). The FLACOS
Workshops serve as annual meeting places to bring together researchers and
practitioners working on language-based solutions to contract-oriented software
development. High-level models of contracts are needed as a tool to negotiate
contracts and provide services conforming to them. This Workshop provides
language-based solutions to the above issues through formalization of
contracts, design of appropriate abstraction mechanisms, and formal analysis of
contract languages and software. The program of this edition consists of 5
regular papers and 3 invited presentations.
  Detailed information about the FLACOS 2011 Workshop can be found at
http://flacos2011.lcc.uma.es/. The 5th edition of the FLACOS Workshop was
organized by the University of M\'alaga. It took place in M\'alaga, Spain,
during September 22-23, 2011."
"Providing adequate tools to tackle the problem of inconsistent compliance
rules is a critical research topic. This problem is of paramount importance to
achieve automatic support for early declarative design and to support evolution
of rules in contract-based or service-based systems. In this paper we
investigate the problem of extracting temporal unsatisfiable cores in order to
detect the inconsistent part of a specification. We extend conflict-driven
SAT-solver to provide a new conflict-driven depth-first-search solver for
temporal logic. We use this solver to compute LTL unsatisfiable cores without
re-exploring the history of the solver."
"Separation logic is a Hoare-style logic for reasoning about programs with
heap-allocated mutable data structures. As a step toward extending separation
logic to high-level languages with ML-style general (higher-order) storage, we
investigate the compatibility of nested Hoare triples with several variations
of higher-order frame rules. The interaction of nested triples and frame rules
can be subtle, and the inclusion of certain frame rules is in fact unsound. A
particular combination of rules can be shown consistent by means of a Kripke
model where worlds live in a recursively defined ultrametric space. The
resulting logic allows us to elegantly prove programs involving stored code. In
particular, using recursively defined assertions, it leads to natural
specifications and proofs of invariants required for dealing with recursion
through the store."
"We consider a framework in which a group of agents communicates by means of
emails, with the possibility of replies, forwards and blind carbon copies
(BCC). We study the epistemic consequences of such email exchanges by
introducing an appropriate epistemic language and semantics. This allows us to
find out what agents learn from the emails they receive and to determine when a
group of agents acquires common knowledge of the fact that an email was sent.
We also show that in our framework from the epistemic point of view the BCC
feature of emails cannot be simulated using messages without BCC recipients."
"The Fischer-Lynch-Paterson theorem (FLP) says that it is impossible for
processes in an asynchronous distributed system to achieve consensus on a
binary value when a single process can fail; it is a widely cited theoretical
result about network computing. All proofs that I know depend essentially on
classical (nonconstructive) logic, although they use the hypothetical
construction of a nonterminating execution as a main lemma.
  FLP is also a guide for protocol designers, and in that role there is a
connection to an important property of consensus procedures, namely that they
should not block, i.e. reach a global state in which no process can decide.
  A deterministic fault-tolerant consensus protocol is effectively nonblocking
if from any reachable global state we can find an execution path that decides.
In this article we effectively construct a nonterminating execution of any such
protocol. That is, given any effectively nonblocking protocol P and a natural
number n, we show how to compute the n-th step of an infinitely indecisive
computation of P. From this fully constructive result, the classical FLP
follows as a corollary as well as a stronger classical result, called here
Strong FLP. Moreover, the construction focuses attention on the important role
of nonblocking in protocol design.
  An interesting consequence of the constructive proof is that we can, in
principle, build an undefeatable attacker for a consensus protocol that is
provably correct, indeed because it is provably correct. We can do this in
practice on certain kinds of networks."
"The interpretation of propositional dynamic logic (PDL) through Kripke models
requires the relations constituting the interpreting Kripke model to closely
observe the syntax of the modal operators. This poses a significant challenge
for an interpretation of PDL through stochastic Kripke models, because the
programs' operations do not always have a natural counterpart in the set of
stochastic relations. We use rewrite rules for building up an interpretation of
PDL. It is shown that each program corresponds to an essentially unique
irreducible tree, which in turn is assigned a predicate lifting, serving as the
program's interpretation. The paper establishes and studies this
interpretation. It discusses the expressivity of probabilistic models for PDL
and relates properties like logical and behavioral equivalence or bisimilarity
to the corresponding properties of a Kripke model for a closely related
non-dynamic logic of the Hennessy-Milner type."
"The confluence of untyped \lambda-calculus with unconditional rewriting is
now well un- derstood. In this paper, we investigate the confluence of
\lambda-calculus with conditional rewriting and provide general results in two
directions. First, when conditional rules are algebraic. This extends results
of M\""uller and Dougherty for unconditional rewriting. Two cases are
considered, whether \beta-reduction is allowed or not in the evaluation of
conditions. Moreover, Dougherty's result is improved from the assumption of
strongly normalizing \beta-reduction to weakly normalizing \beta-reduction. We
also provide examples showing that outside these conditions, modularity of
confluence is difficult to achieve. Second, we go beyond the algebraic
framework and get new confluence results using a restricted notion of
orthogonality that takes advantage of the conditional part of rewrite rules."
"Seeking a general framework for reasoning about and comparing programming
languages, we derive a new view of Milner's CCS. We construct a category E of
'plays', and a subcategory V of 'views'. We argue that presheaves on V
adequately represent 'innocent' strategies, in the sense of game semantics. We
equip innocent strategies with a simple notion of interaction. We then prove
decomposition results for innocent strategies, and, restricting to presheaves
of finite ordinals, prove that innocent strategies are a final coalgebra for a
polynomial functor derived from the game. This leads to a translation of CCS
with recursive equations. Finally, we propose a notion of 'interactive
equivalence' for innocent strategies, which is close in spirit to Beffara's
interpretation of testing equivalences in concurrency theory. In this
framework, we consider analogues of fair testing and must testing. We show that
must testing is strictly finer in our model than in CCS, since it avoids what
we call 'spatial unfairness'. Still, it differs from fair testing, and we show
that it coincides with a relaxed form of fair testing."
"The static dependency pair method is a method for proving the termination of
higher-order rewrite systems a la Nipkow. It combines the dependency pair
method introduced for first-order rewrite systems with the notion of strong
computability introduced for typed lambda-calculi. Argument filterings and
usable rules are two important methods of the dependency pair framework used by
current state-of-the-art first-order automated termination provers. In this
paper, we extend the class of higher-order systems on which the static
dependency pair method can be applied. Then, we extend argument filterings and
usable rules to higher-order rewriting, hence providing the basis for a
powerful automated termination prover for higher-order rewrite systems."
"This paper defines intersection and union type assignment for the calculus X,
a substitution free language that enjoys the Curry-Howard correspondence with
respect to Gentzen's sequent calculus for classical logic. We show that this
notion is closed for subject-expansion, and show that it needs to be restricted
to satisfy subject-reduction as well, making it unsuitable to define a
semantics."
"We apply the principles of the intersection type discipline to the study of
class-based object oriented programs and; our work follows from a similar
approach (in the context of Abadi and Cardelli's Varsigma-object calculus)
taken by van Bakel and de'Liguoro. We define an extension of Featherweight
Java, FJc and present a predicate system which we show to be sound and
expressive. We also show that our system provides a semantic underpinning for
the object oriented paradigm by generalising the concept of approximant from
the Lambda Calculus and demonstrating an approximation result: all expressions
to which we can assign a predicate have an approximant that satisfies the same
predicate. Crucial to this result is the notion of predicate language, which
associates a family of predicates with a class."
"We study the Pi-calculus, enriched with pairing and non-blocking input, and
define a notion of type assignment that uses the type constructor ""arrow"". We
encode the circuits of the calculus X into this variant of Pi, and show that
all reduction (cut-elimination) and assignable types are preserved. Since X
enjoys the Curry-Howard isomorphism for Gentzen's calculus LK, this implies
that all proofs in LK have a representation in Pi."
"We consider CCS with value passing and elaborate a notion of noninterference
for the process calculi, which matches closely that of the programming
language. The idea is to view channels as information carriers rather than as
""events"", so that emitting a secret on output channel can be considered safe,
while inputting a secret may lead to some kind of leakage. This is in contrast
with the standard notion of noninterference for the process calculi where any
causal dependency of low-level action from any high-level action is forbidden."
"Higher-order rewrite systems (HRSs) and simply-typed term rewriting systems
(STRSs) are computational models of functional programs. We recently proposed
an extremely powerful method, the static dependency pair method, which is based
on the notion of strong computability, in order to prove termination in STRSs.
In this paper, we extend the method to HRSs. Since HRSs include
\lambda-abstraction but STRSs do not, we restructure the static dependency pair
method to allow \lambda-abstraction, and show that the static dependency pair
method also works well on HRSs without new restrictions."
"Abstraction is one of the most important strategies for dealing with the
state space explosion problem in model checking. In the abstract model, the
state space is largely reduced, however, a counterexample found in such a model
may not be a real counterexample in the concrete model. Accordingly, the
abstract model needs to be further refined. How to check whether or not a
reported counterexample is spurious is a key problem in the
abstraction-refinement loop. In this paper, a formal definition for spurious
path is given. Based on it, efficient algorithms for detecting spurious
counterexamples are proposed."
"The verification of multithreaded software is still a challenge. This comes
mainly from the fact that the number of thread interleavings grows
exponentially in the number of threads. The idea that thread interleavings can
be studied with a matrix calculus is a novel approach in this research area.
Our sparse matrix representations of the program are manipulated using a lazy
implementation of Kronecker algebra. One goal is the generation of a data
structure called Concurrent Program Graph (CPG) which describes all possible
interleavings and incorporates synchronization while preserving completeness.
We prove that CPGs in general can be represented by sparse adjacency matrices.
Thus the number of entries in the matrices is linear in their number of lines.
Hence efficient algorithms can be applied to CPGs. In addition, due to
synchronization only very small parts of the resulting matrix are actually
needed, whereas the rest is unreachable in terms of automata. Thanks to the
lazy implementation of the matrix operations the unreachable parts are never
calculated. This speeds up processing significantly and shows that this
approach is very promising. Various applications including data flow analysis
can be performed on CPGs. Furthermore, the structure of the matrices can be
used to prove properties of the underlying program for an arbitrary number of
threads. For example, deadlock freedom is proved for a large class of programs."
"Focusing, introduced by Jean-Marc Andreoli in the context of classical linear
logic, defines a normal form for sequent calculus derivations that cuts down on
the number of possible derivations by eagerly applying invertible rules and
grouping sequences of non-invertible rules. A focused sequent calculus is
defined relative to some non-focused sequent calculus; focalization is the
property that every non-focused derivation can be transformed into a focused
derivation.
  In this paper, we present a focused sequent calculus for propositional
intuitionistic logic and prove the focalization property relative to a standard
presentation of propositional intuitionistic logic. Compared to existing
approaches, the proof is quite concise, depending only on the internal
soundness and completeness of the focused logic. In turn, both of these
properties can be established (and mechanically verified) by structural
induction in the style of Pfenning's structural cut elimination without the
need for any tedious and repetitious invertibility lemmas. The proof of cut
admissibility for the focused system, which establishes internal soundness, is
not particularly novel. The proof of identity expansion, which establishes
internal completeness, is a major contribution of this work."
"It is shown that the finite satisfiability problem for two-variable logic
over structures with one total preorder relation, its induced successor
relation, one linear order relation and some further unary relations is
EXPSPACE-complete. Actually, EXPSPACE-completeness already holds for structures
that do not include the induced successor relation. As a special case, the
EXPSPACE upper bound applies to two-variable logic over structures with two
linear orders. A further consequence is that satisfiability of two-variable
logic over data words with a linear order on positions and a linear order and
successor relation on the data is decidable in EXPSPACE. As a complementing
result, it is shown that over structures with two total preorder relations as
well as over structures with one total preorder and two linear order relations,
the finite satisfiability problem for two-variable logic is undecidable."
"We establish completeness for intuitionistic first-order logic, iFOL, showing
that a formula is provable if and only if its embedding into minimal logic,
mFOL, is uniformly valid under the Brouwer Heyting Kolmogorov (BHK) semantics,
the intended semantics of iFOL and mFOL. Our proof is intuitionistic and
provides an effective procedure Prf that converts uniform minimal evidence into
a formal first-order proof. We have implemented Prf. Uniform validity is
defined using the intersection operator as a universal quantifier over the
domain of discourse and atomic predicates. Formulas of iFOL that are uniformly
valid are also intuitionistically valid, but not conversely. Our strongest
result requires the Fan Theorem; it can also be proved classically by showing
that Prf terminates using Konig's Theorem.
  The fundamental idea behind our completeness theorem is that a single
evidence term evd witnesses the uniform validity of a minimal logic formula F.
Finding even one uniform realizer guarantees intuitionistic validity because
Prf(F, evd) builds a first-order proof of F, establishing its intuitionistic
validity and providing a purely logical normalized realizer.
  We establish completeness for iFOL as follows. Friedman showed that iFOL can
be embedded in minimal logic (mFOL) by his A-transformation, mapping formula F
to FA. If F is uniformly valid, then so is FA, and by our completeness theorem,
we can find a proof of FA in minimal logic. Then we intuitionistically prove F
from FFalse, i.e. by taking False for A and for \bot of mFOL. Our result
resolves an open question posed by Beth in 1947."
"Open Answer Set Programming (OASP) is an undecidable framework for
integrating ontologies and rules. Although several decidable fragments of OASP
have been identified, few reasoning procedures exist. In this article, we
provide a sound, complete, and terminating algorithm for satisfiability
checking w.r.t. Forest Logic Programs (FoLPs), a fragment of OASP where rules
have a tree shape and allow for inequality atoms and constants. The algorithm
establishes a decidability result for FoLPs. Although believed to be decidable,
so far only the decidability for two small subsets of FoLPs, local FoLPs and
acyclic FoLPs, has been shown. We further introduce f-hybrid knowledge bases, a
hybrid framework where \SHOQ{} knowledge bases and forest logic programs
co-exist, and we show that reasoning with such knowledge bases can be reduced
to reasoning with forest logic programs only. We note that f-hybrid knowledge
bases do not require the usual (weakly) DL-safety of the rule component,
providing thus a genuine alternative approach to current integration approaches
of ontologies and rules."
"We propose a logic for true concurrency whose formulae predicate about events
in computations and their causal dependencies. The induced logical equivalence
is hereditary history preserving bisimilarity, and fragments of the logic can
be identified which correspond to other true concurrent behavioural
equivalences in the literature: step, pomset and history preserving
bisimilarity. Standard Hennessy-Milner logic, and thus (interleaving)
bisimilarity, is also recovered as a fragment. We also propose an extension of
the logic with fixpoint operators, thus allowing to describe causal and
concurrency properties of infinite computations. We believe that this work
contributes to a rational presentation of the true concurrent spectrum and to a
deeper understanding of the relations between the involved behavioural
equivalences."
"Virtualization promises significant benefits in security, efficiency,
dependability, and cost. Achieving these benefits depends upon the reliability
of the underlying virtual machine monitors (hypervisors). This paper describes
an ongoing project to develop and verify MinVisor, a simple but functional
Type-I x86 hypervisor, proving protection properties at the assembly level
using ACL2. Originally based on an existing research hypervisor, MinVisor
provides protection of its own memory from a malicious guest. Our long-term
goal is to fully verify MinVisor, providing a vehicle to investigate the
modeling and verification of hypervisors at the implementation level, and also
a basis for further systems research. Functional segments of the MinVisor C
code base are translated into Y86 assembly, and verified with respect to the
Y86 model. The inductive assertions (also known as ""compositional cutpoints"")
methodology is used to prove the correctness of the code. The proof of the code
that sets up the nested page tables is described. We compare this project to
related efforts in systems code verification and outline some useful steps
forward."
"Interactive theorem proving requires a lot of human guidance. Proving a
property involves (1) figuring out why it holds, then (2) coaxing the theorem
prover into believing it. Both steps can take a long time. We explain how to
use GL, a framework for proving finite ACL2 theorems with BDD- or SAT-based
reasoning. This approach makes it unnecessary to deeply understand why a
property is true, and automates the process of admitting it as a theorem. We
use GL at Centaur Technology to verify execution units for x86 integer, MMX,
SSE, and floating-point arithmetic."
"Deadlock detection is a challenging issue in the analysis and design of
on-chip networks. We have designed an algorithm to detect deadlocks
automatically in on-chip networks with wormhole switching. The algorithm has
been specified and proven correct in ACL2. To enable a top-down proof
methodology, some parts of the algorithm have been left unimplemented. For
these parts, the ACL2 specification contains constrained functions introduced
with defun-sk. We used single-threaded objects to represent the data structures
used by the algorithm. In this paper, we present details on the proof of
correctness of the algorithm. The process of formal verification was crucial to
get the algorithm flawless. Our ultimate objective is to have an efficient
executable, and formally proven correct implementation of the algorithm running
in ACL2."
"A new framework for presenting and analyzing the functionality of a modern
DLL-based SAT solver is proposed. Our approach exploits the inherent relation
between backtracking and resolution. We show how to derive the algorithm of a
modern SAT solver from DLL step-by-step. We analyze the inference power of
Boolean Constraint Propagation, Non-Chronological Backtracking and 1UIP-based
Conflict-Directed Backjumping. Our work can serve as an introduction to a
modern SAT solver functionality and as a basis for future work on the inference
power of a modern SAT solver and on practical SAT solver design."
"An algorithm to compute the set of prime implicates of a quantifier-free
clausal formula X in first order logic had been presented in earlier work. As
the knowledge base X is dynamic, new clauses are added to the old knowledge
base. In this paper an incremental algorithm is presented to compute the prime
implicates of X and a clause C from $\pi(X)\cup C$. The correctness of the
algorithm is also proved."
"The mathematical framework of Stone duality is used to synthesize a number of
hitherto separate developments in Theoretical Computer Science: - Domain
Theory, the mathematical theory of computation introduced by Scott as a
foundation for denotational semantics. - The theory of concurrency and systems
behaviour developed by Milner, Hennessy et al. based on operational semantics.
- Logics of programs.
  Stone duality provides a junction between semantics (spaces of points =
denotations of computational processes) and logics (lattices of properties of
processes). Moreover, the underlying logic is geometric, which can be
computationally interpreted as the logic of observable properties---i.e.
properties which can be determined to hold of a process on the basis of a
finite amount of information about its execution.
  These ideas lead to the following programme:
  1. A metalanguage is introduced, comprising
  - types = universes of discourse for various computational situations.
  - terms = programs = syntactic intensions for models or points.
  2. A standard denotational interpretation of the metalanguage is given,
assigning domains to types and domain elements to terms.
  3. The metalanguage is also given a {\em logical} interpretation, in which
types are interpreted as propositional theories and terms are interpreted via a
program logic, which axiomatizes the properties they satisfy.
  4. The two interpretations are related by showing that they are Stone duals
of each other. Hence, semantics and logic are guaranteed to be in harmony with
each other, and in fact each determines the other up to isomorphism.
  This opens the way to a whole range of applications. Given a denotational
description of a computational situation in our meta-language, we can turn the
handle to obtain a logic for that situation."
"We investigate quantitative properties of BCI and BCK logics. The first part
of the paper compares the number of formulas provable in BCI versus BCK logics.
We consider formulas built on implication and a fixed set of $k$ variables. We
investigate the proportion between the number of such formulas of a given
length $n$ provable in BCI logic against the number of formulas of length $n$
provable in richer BCK logic. We examine an asymptotic behavior of this
fraction when length $n$ of formulas tends to infinity. This limit gives a
probability measure that randomly chosen BCK formula is also provable in BCI.
We prove that this probability tends to zero as the number of variables tends
to infinity. The second part of the paper is devoted to the number of lambda
terms representing proofs of BCI and BCK logics. We build a proportion between
number of such proofs of the same length $n$ and we investigate asymptotic
behavior of this proportion when length of proofs tends to infinity. We
demonstrate that with probability 0 a randomly chosen BCK proof is also a proof
of a BCI formula."
"We introduce a library which provides an abstract data type of environments,
as a functor parameterized by a module defining variables, and a function which
builds environments for such variables with any Type of type. Usual operations
over environments are defined, along with an extensive set of basic and more
advanced properties. Moreover, we give an implementation using lists satisfying
all the required properties."
"Relying on the formulae-as-types paradigm for classical logic, we define a
program logic for an imperative language with higher-order procedural variables
and non-local jumps. Then, we show how to derive a sound program logic for this
programming language. As a by-product, we obtain a non-dependent type system
which is more permissive than what is usually found in statically typed
imperative languages. As a generic example, we encode imperative versions of
delimited continuations operators shift and reset."
"We formally specified a program logic for higher-order procedural variables
and non-local jumps with Ott and Twelf. Moreover, the dependent type systems
and the translation are both executable specifications thanks to Twelf's logic
programming engine. In particular, relying on Filinski's encoding of
shift/reset using callcc/throw and a global meta-continuation (simulated in
state passing style), we have mechanically checked the correctness of a few
examples (all source files are available on request)."
"Boolean function bi-decomposition is ubiquitous in logic synthesis. It
entails the decomposition of a Boolean function using two-input simple logic
gates. Existing solutions for bi-decomposition are often based on BDDs and,
more recently, on Boolean Satisfiability. In addition, the partition of the
input set of variables is either assumed, or heuristic solutions are considered
for finding good partitions. In contrast to earlier work, this paper proposes
the use of Quantified Boolean Formulas (QBF) for computing bi- decompositions.
These bi-decompositions are optimal in terms of the achieved disjointness and
balancedness of the input set of variables. Experimental results, obtained on
representative benchmarks, demonstrate clear improvements in the quality of
computed decompositions, but also the practical feasibility of QBF-based
bi-decomposition."
"We derive a Hoare-Floyd logic for non-local jumps and mutable higher-order
procedural variables from a formul{\ae}-as-types notion of control for
classical logic. The main contribution of this work is the design of an
imperative dependent type system for non-local jumps which corresponds to
classical logic but where the famous consequence rule is still derivable."
"We estimate the maximal length of interactions between strategies in HO/N
game semantics, in the spirit of the work by Schwichtenberg and Beckmann for
the length of reduction in simply typed lambdacalculus. Because of the
operational content of game semantics, the bounds presented here also apply to
head linear reduction on lambda-terms and to the execution of programs by
abstract machines (PAM/KAM), including in presence of computational effects
such as non-determinism or ground type references. The proof proceeds by
extracting from the games model a combinatorial rewriting rule on trees of
natural numbers, which can then be analyzed independently of game semantics or
lambda-calculus."
"The Algebraic Dichotomy Conjecture states that the Constraint Satisfaction
Problem over a fixed template is solvable in polynomial time if the algebra of
polymorphisms associated to the template lies in a Taylor variety, and is
NP-complete otherwise. This paper provides two new characterizations of
finitely generated Taylor varieties. The first characterization is using
absorbing subalgebras and the second one cyclic terms. These new conditions
allow us to reprove the conjecture of Bang-Jensen and Hell (proved by the
authors) and the characterization of locally finite Taylor varieties using weak
near-unanimity terms (proved by McKenzie and Mar\'oti) in an elementary and
self-contained way."
"A new characterization of provably recursive functions of first-order
arithmetic is described. Its main feature is using only terms consisting of 0,
the successor S and variables in the quantifier rules, namely, universal
elimination and existential introduction."
"This technical report contains the proofs to the lemmata and theorems of
[PN12] as well as some additional material. As main contributions [PN12]
presents an encoding of mixed choice in the context of the pi-calculus and a
criterion to measure whether the degree of distribution in process networks is
preserved."
"This paper defines a sound and complete semantic criterion, based on
reducibility candidates, for strong normalization of theories expressed in
minimal deduction modulo \`a la Curry. The use of Curry-style proof-terms
allows to build this criterion on the classic notion of pre-Heyting algebras
and makes that criterion concern all theories expressed in minimal deduction
modulo. Compared to using Church-style proof-terms, this method provides both a
simpler definition of the criterion and a simpler proof of its completeness."
"The Parameterised Model Checking Problem asks whether an implementation
Impl(t) satisfies a specification Spec(t) for all instantiations of parameter
t. In general, t can determine numerous entities: the number of processes used
in a network, the type of data, the capacities of buffers, etc. The main theme
of this paper is automation of uniform verification of a subclass of PMCP with
the parameter of the first kind, i.e. the number of processes in the network.
We use CSP as our formalism. We present a type reduction theory, which, for a
given verification problem, establishes a function \phi that maps all
(sufficiently large) instantiations T of the parameter to some fixed type T^
and allows us to deduce that if Spec(T^) is refined by \phi(Impl(T)), then
(subject to certain assumptions) Spec(T) is refined by Impl(T). The theory can
be used in practice by combining it with a suitable abstraction method that
produces a t-independent process Abstr that is refined by {\phi}(Impl(T)) for
all sufficiently large T. Then, by testing (with a model checker) if the
abstract model Abstr refines Spec(T^), we can deduce a positive answer to the
original uniform verification problem. The type reduction theory relies on
symbolic representation of process behaviour. We develop a symbolic operational
semantics for CSP processes that satisfy certain normality requirements, and we
provide a set of translation rules that allow us to concretise symbolic
transition graphs. Based on this, we prove results that allow us to infer
behaviours of a process instantiated with uncollapsed types from known
behaviours of the same process instantiated with a reduced type. One of the
main advantages of our symbolic operational semantics and the type reduction
theory is their generality, which makes them applicable in a wide range of
settings."
"We consider two characterisations of the may and must testing preorders for a
probabilistic extension of the finite pi-calculus: one based on notions of
probabilistic weak simulations, and the other on a probabilistic extension of a
fragment of Milner-Parrow-Walker modal logic for the pi-calculus. We base our
notions of simulations on the similar concepts used in previous work for
probabilistic CSP. However, unlike the case with CSP (or other
non-value-passing calculi), there are several possible definitions of
simulation for the probabilistic pi-calculus, which arise from different ways
of scoping the name quantification. We show that in order to capture the
testing preorders, one needs to use the ""earliest"" simulation relation (in
analogy to the notion of early (bi)simulation in the non-probabilistic case).
The key ideas in both characterisations are the notion of a ""characteristic
formula"" of a probabilistic process, and the notion of a ""characteristic test""
for a formula. As in an earlier work on testing equivalence for the pi-calculus
by Boreale and De Nicola, we extend the language of the $\pi$-calculus with a
mismatch operator, without which the formulation of a characteristic test will
not be possible."
"Girard's Light linear logic (LLL) characterized polynomial time in the
proof-as-program paradigm with a bound on cut elimination. This logic relied on
a stratification principle and a ""one-door"" principle which were generalized
later respectively in the systems L^4 and L^3a. Each system was brought with
its own complex proof of Ptime soundness.
  In this paper we propose a broad sufficient criterion for Ptime soundness for
linear logic subsystems, based on the study of paths inside the proof-nets,
which factorizes proofs of soundness of existing systems and may be used for
future systems. As an additional gain, our bound stands for any reduction
strategy whereas most bounds in the literature only stand for a particular
strategy."
"We consider infinite sequences of symbols, also known as streams, and the
decidability question for equality of streams defined in a restricted format.
This restricted format consists of prefixing a symbol at the head of a stream,
of the stream function `zip', and recursion variables. Here `zip' interleaves
the elements of two streams in alternating order, starting with the first
stream. For example, the Thue-Morse sequence is obtained by the
`zip-specification' {M = 0 : X, X = 1 : zip(X,Y), Y = 0 : zip(Y,X)}. Our
analysis of such systems employs both term rewriting and coalgebraic
techniques. We establish decidability for these zip-specifications, employing
bisimilarity of observation graphs based on a suitably chosen cobasis. The
importance of zip-specifications resides in their intimate connection with
automatic sequences. We establish a new and simple characterization of
automatic sequences. Thus we obtain for the binary zip that a stream is
2-automatic iff its observation graph using the cobasis (hd,even,odd) is
finite. The generalization to zip-k specifications and their relation to
k-automaticity is straightforward. In fact, zip-specifications can be perceived
as a term rewriting syntax for automatic sequences. Our study of
zip-specifications is placed in an even wider perspective by employing the
observation graphs in a dynamic logic setting, leading to an alternative
characterization of automatic sequences. We further obtain a natural extension
of the class of automatic sequences, obtained by `zip-mix' specifications that
use zips of different arities in one specification. We also show that
equivalence is undecidable for a simple extension of the zip-mix format with
projections like even and odd. However, it remains open whether zip-mix
specifications have a decidable equivalence problem."
"We propose a synthesis of the two proof styles of interactive theorem
proving: the procedural style (where proofs are scripts of commands, like in
Coq) and the declarative style (where proofs are texts in a controlled natural
language, like in Isabelle/Isar). Our approach combines the advantages of the
declarative style - the possibility to write formal proofs like normal
mathematical text - and the procedural style - strong automation and help with
shaping the proofs, including determining the statements of intermediate steps.
Our approach is new, and differs significantly from the ways in which the
procedural and declarative proof styles have been combined before in the
Isabelle, Ssreflect and Matita systems. Our approach is generic and can be
implemented on top of any procedural interactive theorem prover, regardless of
its architecture and logical foundations. To show the viability of our proposed
approach, we fully implemented it as a proof interface called miz3, on top of
the HOL Light interactive theorem prover. The declarative language that this
interface uses is a slight variant of the language of the Mizar system, and can
be used for any interactive theorem prover regardless of its logical
foundations. The miz3 interface allows easy access to the full set of tactics
and formal libraries of HOL Light, and as such has ""industrial strength"". Our
approach gives a way to automatically convert any procedural proof to a
declarative counterpart, where the converted proof is similar in size to the
original. As all declarative systems have essentially the same proof language,
this gives a straightforward way to port proofs between interactive theorem
provers."
"This paper describes a formalization of discrete real closed fields in the
Coq proof assistant. This abstract structure captures for instance the theory
of real algebraic numbers, a decidable subset of real numbers with good
algorithmic properties. The theory of real algebraic numbers and more generally
of semi-algebraic varieties is at the core of a number of effective methods in
real analysis, including decision procedures for non linear arithmetic or
optimization methods for real valued functions. After defining an abstract
structure of discrete real closed field and the elementary theory of real roots
of polynomials, we describe the formalization of an algebraic proof of
quantifier elimination based on pseudo-remainder sequences following the
standard computer algebra literature on the topic. This formalization covers a
large part of the theory which underlies the efficient algorithms implemented
in practice in computer algebra. The success of this work paves the way for
formal certification of these efficient methods."
"Introduced by Dal Lago and Hofmann, quantitative realizability is a technique
used to define models for logics based on Multiplicative Linear Logic. A
particularity is that functions are interpreted as bounded time computable
functions. It has been used to give new and uniform proofs of soundness of
several type systems with respect to certain time complexity classes. We
propose a reformulation of their ideas in the setting of Krivine's classical
realizability. The framework obtained generalizes Dal Lago and Hofmann's
realizability, and reveals deep connections between quantitative realizability
and a linear variant of Cohen's forcing."
"One central issue in the formal design and analysis of reactive systems is
the notion of refinement that asks whether all behaviors of the implementation
is allowed by the specification. The local interpretation of behavior leads to
the notion of simulation. Alternating transition systems (ATSs) provide a
general model for composite reactive systems, and the simulation relation for
ATSs is known as alternating simulation. The simulation relation for fair
transition systems is called fair simulation. In this work our main
contributions are as follows: (1) We present an improved algorithm for fair
simulation with B\""uchi fairness constraints; our algorithm requires $O(n^3
\cdot m)$ time as compared to the previous known $O(n^6)$-time algorithm, where
$n$ is the number of states and $m$ is the number of transitions. (2) We
present a game based algorithm for alternating simulation that requires
$O(m^2)$-time as compared to the previous known $O((n \cdot m)^2)$-time
algorithm, where $n$ is the number of states and $m$ is the size of transition
relation. (3) We present an iterative algorithm for alternating simulation that
matches the time complexity of the game based algorithm, but is more space
efficient than the game based algorithm."
"Game semantics is a trace-like denotational semantics for programming
languages where the notion of legal observable behaviour of a term is defined
combinatorially, by means of rules of a game between the term (the ""Proponent"")
and its context (the ""Opponent""). In general, the richer the computational
features a language has, the less constrained the rules of the semantic game.
In this paper we consider the consequences of taking this relaxation of rules
to the limit, by granting the Opponent omnipotence, that is, permission to play
any move without combinatorial restrictions. However, we impose an epistemic
restriction by not granting Opponent omniscience, so that Proponent can have
undisclosed secret moves. We introduce a basic C-like programming language and
we define such a semantic model for it. We argue that the resulting semantics
is an appealingly simple combination of operational and game semantics and we
show how certain traces explain system-level attacks, i.e. plausible attacks
that are realizable outside of the programming language itself. We also show
how allowing Proponent to have secrets ensures that some desirable equivalences
in the programming language are preserved."
"A tree automatic structure is a structure whose domain can be encoded by a
regular tree language such that each relation is recognisable by a finite
automaton processing tuples of trees synchronously. Words can be regarded as
specific simple trees and a structure is word automatic if it is encodable
using only these trees. The question naturally arises whether a given tree
automatic structure is already word automatic. We prove that this problem is
decidable for tree automatic scattered linear orderings. Moreover, we show that
in case of a positive answer a word automatic presentation is computable from
the tree automatic presentation."
"We consider the problem of existential quantifier elimination for Boolean
formulas in Conjunctive Normal Form (CNF). We present a new method for solving
this problem called Derivation of Dependency-Sequents (DDS). A
Dependency-sequent (D-sequent) is used to record that a set of quantified
variables is redundant under a partial assignment. We introduce a
resolution-like operation called join that produces a new D-sequent from two
existing D-sequents. We also show that DDS is compositional, i.e. if our input
formula is a conjunction of independent formulas, DDS automatically recognizes
and exploits this information. We introduce an algorithm based on DDS and
present experimental results demonstrating its potential."
"Association Rules are a basic concept of data mining. They are, however, not
understood as logical objects which can be used for reasoning. The purpose of
this paper is to investigate a model based semantic for implications with
certain constraints on their support and confidence in relational data, which
then resemble association rules, and to present a possibility to decide
entailment for them."
"We present a modification of the superposition calculus that is meant to
generate explanations why a set of clauses is satisfiable. This process is
related to abductive reasoning, and the explanations generated are clauses
constructed over so-called abductive constants. We prove the correctness and
completeness of the calculus in the presence of redundancy elimination rules,
and develop a sufficient condition guaranteeing its termination; this
sufficient condition is then used to prove that all possible explanations can
be generated infinite time for several classes of clause sets, including many
of interest to the SMT community. We propose a procedure that generates a set
of explanations that should be useful to a human user and conclude by
suggesting several extensions to this novel approach."
"The strong, intermediate, and weak Turing impossibility properties are
introduced. Some facts concerning Turing impossibility for stack machine
programming are trivially adapted from previous work. Several intriguing
questions are raised about the Turing impossibility properties concerning
different method interfaces for stack machine programming."
"This paper introduces an extension of Answer Set Programming called
Preference Set Constraint Programming which is a convenient and general
formalism to reason with preferences. PSC programming extends Set Constraint
Programming introduced by Marek and Remmel (Marek and Remmel 2004) by
introducing two types of preference set constraint atoms, measure preference
set constraint atoms and pre-ordered preference set constraint atoms, which are
extensions of set constraint atoms. We show that the question of whether a PSC
program has a preferred stable model is CoNP-complete. We give examples of the
uses of the preference set constraint atoms and show that Answer Set
Optimization (Brewka, Niemel\""a, and Truszczynski 2003) and General Preference
(Son and Pontelli 2006) can be expressed using preference set constraint atoms."
"Inspired by a recent graphical formalism for lambda-calculus based on linear
logic technology, we introduce an untyped structural lambda-calculus, called
lambda j, which combines actions at a distance with exponential rules
decomposing the substitution by means of weakening, contraction and
derelicition. First, we prove some fundamental properties of lambda j such as
confluence and preservation of beta-strong normalisation. Second, we add a
strong bisimulation to lambda j by means of an equational theory which captures
in particular Regnier's sigma-equivalence. We then complete this bisimulation
with two more equations for (de)composition of substitutions and we prove that
the resulting calculus still preserves beta-strong normalization. Finally, we
discuss some consequences of our results."
"We study the LTL model-checking in possibilistic Kripke structure using
possibility measure. First, the notion of possibilistic Kripke structure and
the related possibility measure are introduced, then model-checking of
reachability and repeated reachability linear-time properties in finite
possibilistic Kripke structure are studied. Standard safety property and
-regular property in possibilistic Kripke structure are introduced, the
verification of regular safety property and -regular property using finite
automata are thoroughly studied. It has been shown that the verification of
regular safety property and -regular property in finite possibilistic Kripke
structure can be transformed into the verification of reachability property and
repeated reachability property in the product possibilistic Kripke structure
introduced in this paper. Several examples are given to illustrate the methods
presented in the paper."
"We present in this paper a new procedure to saturate a set of clauses with
respect to a well-founded ordering on ground atoms such that A < B implies
Var(A) {\subseteq} Var(B) for every atoms A and B. This condition is satisfied
by any atom ordering compatible with a lexicographic, recursive, or multiset
path ordering on terms. Our saturation procedure is based on a priori ordered
resolution and its main novelty is the on-the-fly construction of a finite
complexity atom ordering. In contrast with the usual redundancy, we give a new
redundancy notion and we prove that during the saturation a non-redundant
inference by a priori ordered resolution is also an inference by a posteriori
ordered resolution. We also prove that if a set S of clauses is saturated with
respect to an atom ordering as described above then the problem of whether a
clause C is entailed from S is decidable."
"One of Courcelle's celebrated results states that if C is a class of graphs
of bounded tree-width, then model-checking for monadic second order logic
(MSO_2) is fixed-parameter tractable (fpt) on C by linear time parameterized
algorithms, where the parameter is the tree-width plus the size of the formula.
An immediate question is whether this is best possible or whether the result
can be extended to classes of unbounded tree-width. In this paper we show that
in terms of tree-width, the theorem cannot be extended much further. More
specifically, we show that if C is a class of graphs which is closed under
colourings and satisfies certain constructibility conditions and is such that
the tree-width of C is not bounded by \log^{84} n then MSO_2-model checking is
not fpt unless SAT can be solved in sub-exponential time. If the tree-width of
C is not poly-logarithmically bounded, then MSO_2-model checking is not fpt
unless all problems in the polynomial-time hierarchy can be solved in
sub-exponential time."
"In the last few years appeared pedagogical propositional natural deduction
systems. In these systems, one must satisfy the pedagogical constraint: the
user must give an example of any introduced notion. First we expose the reasons
of such a constraint and properties of these ""pedagogical"" calculi: the absence
of negation at logical side, and the ""usefulness"" feature of terms at
computational side (through the Curry-Howard correspondence). Then we construct
a simple pedagogical restriction of the calculus of constructions (CC) called
CCr. We establish logical limitations of this system, and compare its
computational expressiveness to Godel system T. Finally, guided by the logical
limitations of CCr, we propose a formal and general definition of what a
pedagogical calculus of constructions should be."
"We show that (1) the Minimal False QCNF search-problem (MF-search) and the
Minimal Unsatisfiable LTL formula search problem (MU-search) are FPSPACE
complete because of the very expressive power of QBF/LTL, (2) we extend the
PSPACE-hardness of the MF decision problem to the MU decision problem. As a
consequence, we deduce a positive answer to the open question of PSPACE
hardness of the inherent Vacuity Checking problem. We even show that the
Inherent Non Vacuous formula search problem is also FPSPACE-complete."
"The use of interpolants in verification is gaining more and more importance.
Since theories used in applications are usually obtained as (disjoint)
combinations of simpler theories, it is important to modularly re-use
interpolation algorithms for the component theories. We show that a sufficient
and necessary condition to do this for quantifier-free interpolation is that
the component theories have the 'strong (sub-)amalgamation' property. Then, we
provide an equivalent syntactic characterization, identify a sufficient
condition, and design a combined quantifier-free interpolation algorithm
capable of handling both convex and non-convex theories, that subsumes and
extends most existing work on combined interpolation."
"We introduce tree-width for first order formulae \phi, fotw(\phi). We show
that computing fotw is fixed-parameter tractable with parameter fotw. Moreover,
we show that on classes of formulae of bounded fotw, model checking is fixed
parameter tractable, with parameter the length of the formula. This is done by
translating a formula \phi\ with fotw(\phi)<k into a formula of the k-variable
fragment L^k of first order logic. For fixed k, the question whether a given
first order formula is equivalent to an L^k formula is undecidable. In
contrast, the classes of first order formulae with bounded fotw are fragments
of first order logic for which the equivalence is decidable.
  Our notion of tree-width generalises tree-width of conjunctive queries to
arbitrary formulae of first order logic by taking into account the quantifier
interaction in a formula. Moreover, it is more powerful than the notion of
elimination-width of quantified constraint formulae, defined by Chen and Dalmau
(CSL 2005): for quantified constraint formulae, both bounded elimination-width
and bounded fotw allow for model checking in polynomial time. We prove that
fotw of a quantified constraint formula \phi\ is bounded by the
elimination-width of \phi, and we exhibit a class of quantified constraint
formulae with bounded fotw, that has unbounded elimination-width. A similar
comparison holds for strict tree-width of non-recursive stratified datalog as
defined by Flum, Frick, and Grohe (JACM 49, 2002).
  Finally, we show that fotw has a characterization in terms of a cops and
robbers game without monotonicity cost."
"This paper is a survey of two kinds of ""compressed"" proof schemes, the
\emph{matrix method} and \emph{proof nets}, as applied to a variety of logics
ranging along the substructural hierarchy from classical all the way down to
the nonassociative Lambek system. A novel treatment of proof nets for the
latter is provided. Descriptions of proof nets and matrices are given in a
uniform notation based on sequents, so that the properties of the schemes for
the various logics can be easily compared."
"We give a method to prove confluence of term rewriting systems that contain
non-terminating rewrite rules such as commutativity and associativity. Usually,
confluence of term rewriting systems containing such rules is proved by
treating them as equational term rewriting systems and considering E-critical
pairs and/or termination modulo E. In contrast, our method is based solely on
usual critical pairs and it also (partially) works even if the system is not
terminating modulo E. We first present confluence criteria for term rewriting
systems whose rewrite rules can be partitioned into a terminating part and a
possibly non-terminating part. We then give a reduction-preserving completion
procedure so that the applicability of the criteria is enhanced. In contrast to
the well-known Knuth-Bendix completion procedure which preserves the
equivalence relation of the system, our completion procedure preserves the
reduction relation of the system, by which confluence of the original system is
inferred from that of the completed system."
"The termination method of weakly monotonic algebras, which has been defined
for higher-order rewriting in the HRS formalism, offers a lot of power, but has
seen little use in recent years. We adapt and extend this method to the
alternative formalism of algebraic functional systems, where the simply-typed
lambda-calculus is combined with algebraic reduction. Using this theory, we
define higher-order polynomial interpretations, and show how the implementation
challenges of this technique can be tackled. A full implementation is provided
in the termination tool WANDA."
"Axiomatic set theory is almost universally accepted as the basic theory which
provides the foundations of mathematics, and in which the whole of present day
mathematics can be developed. As such, it is the most natural framework for
Mathematical Knowledge Management. However, in order to be used for this task
it is necessary to overcome serious gaps that exist between the ""official""
formulations of set theory (as given e.g. by formal set theory ZF) and actual
mathematical practice.
  In this work we present a new unified framework for formalizations of
axiomatic set theories of different strength, from rudimentary set theory to
full ZF. It allows the use of set terms, but provides a static check of their
validity."
"We introduce a graphical refutation calculus for relational inclusions: it
reduces establishing a relational inclusion to establishing that a graph
constructed from it has empty extension. This sound and complete calculus is
conceptually simpler and easier to use than the usual ones."
"This work presents a formalization of the theorem of existence of most
general unifiers in first-order signatures in the higher-order proof assistant
PVS. The distinguishing feature of this formalization is that it remains close
to the textbook proofs that are based on proving the correctness of the
well-known Robinson's first-order unification algorithm. The formalization was
applied inside a PVS development for term rewriting systems that provides a
complete formalization of the Knuth-Bendix Critical Pair theorem, among other
relevant theorems of the theory of rewriting. In addition, the formalization
methodology has been proved of practical use in order to verify the correctness
of unification algorithms in the style of the original Robinson's unification
algorithm."
"In the last years, the adoption of active systems has increased in many
fields of computer science, such as databases, sensor networks, and software
engineering. These systems are able to automatically react to events, by
collecting information from outside and internally generating new events.
However, the collection of data is often hampered by uncertainty and vagueness
that can arise from the imprecision of the monitoring infrastructure,
unreliable data sources, and networks. The decision making mechanism used to
produce a reaction is also imprecise, and cannot be evaluated in a crisp way.
It depends on the evaluation of vague temporal constraints, which are expressed
on the collected data by humans. Despite fuzzy logic has been mainly conceived
as a mathematical abstraction to express vagueness, no attempt has been made to
fuzzify the temporal modalities. Existing fuzzy languages do not allow us to
represent temporal properties, such as ""almost always"" and ""soon"". Indeed, the
semantics of existing fuzzy temporal operators is based on the idea of
replacing classical connectives or propositions with their fuzzy counterparts.
To overcome these limitations, we propose a temporal framework, FTL (Fuzzy-time
Temporal Logic), to express vagueness on time. This framework formally defines
a set of fuzzy temporal modalities, which can be customized by choosing a
specific semantics for the connectives. The semantics of the language is sound,
and the introduced modalities respect a set of expected mutual relations. We
also prove that under the assumption that all events are crisp, FTL reduces to
LTL. Finally, for some of the possible fuzzy interpretations of the
connectives, we identify adequate sets of temporal operators, from which it is
possible to derive all the others."
"We develop and prove sound a concurrent separation logic for Pthreads-style
barriers. Although Pthreads barriers are widely used in systems, and separation
logic is widely used for verification, there has not been any effort to combine
the two. Unlike locks and critical sections, Pthreads barriers enable
simultaneous resource redistribution between multiple threads and are
inherently stateful, leading to significant complications in the design of the
logic and its soundness proof. We show how our logic can be applied to a
specific example program in a modular way. Our proofs are machine-checked in
Coq. We showcase a program verification toolset that automatically applies the
logic rules and discharges the associated proof obligations."
"We introduce a new symbolic representation based on an original
generalization of counter abstraction. Unlike classical counter abstraction
(used in the analysis of parameterized systems with unordered or unstructured
topologies) the new representation is tailored for proving properties of
linearly ordered parameterized systems, i.e., systems with arbitrary many
finite processes placed in an array. The relative positions in the array
capture the relative priorities of the processes. Configurations of such
systems are finite words of arbitrary lengths. The processes communicate using
global transitions constrained by their relative priorities. Intuitively, an
element of the symbolic representation has a base and a set of counters. It
denotes configurations that respect the constraints imposed by the counters and
that have the base as a sub word. We use the new representation in a uniform
and automatic Counter Example Guided Refinement scheme. We introduce a
relaxation operator that allows a well quasi ordering argument for the
termination of each iteration of the refinement loop. We explain how to refine
the relaxation to systematically prune out false positives. We implemented a
tool to illustrate the approach on a number of parameterized systems."
"Calculi with control operators have been studied as extensions of simple type
theory. Real programming languages contain datatypes, so to really understand
control operators, one should also include these in the calculus. As a first
step in that direction, we introduce lambda-mu-T, a combination of Parigot's
lambda-mu-calculus and G\""odel's T, to extend a calculus with control operators
with a datatype of natural numbers with a primitive recursor.
  We consider the problem of confluence on raw terms, and that of strong
normalization for the well-typed terms. Observing some problems with extending
the proofs of Baba at al. and Parigot's original confluence proof, we provide
new, and improved, proofs of confluence (by complete developments) and strong
normalization (by reducibility and a postponement argument) for our system.
  We conclude with some remarks about extensions, choices, and prospects for an
improved presentation."
"The Rogers semilattice of effective programming systems (epses) is the
collection of all effective numberings of the partial computable functions
ordered such that \theta\ is less than or equal to \psi\ whenever
\theta-programs can be algorithmically translated into \psi-programs. Herein,
it is shown that an eps \psi\ is minimal in this ordering if and only if, for
each translation function t into \psi, there exists a computably enumerable
equivalence relation (ceer) R such that (i) R is a subrelation of \psi's
program equivalence relation, and (ii) R equates each \psi-program to some
program in the range of t. It is also shown that there exists a minimal eps for
which no single such R does the work for all such t. In fact, there exists a
minimal eps \psi\ such that, for each ceer R, either R contradicts \psi's
program equivalence relation, or there exists a translation function t into
\psi\ such that the range of t fails to intersect infinitely many of R's
equivalence classes."
"In this paper we study strong and weak bisimulation equivalences for
continuous-time Markov decision processes (CTMDPs) and the logical
characterizations of these relations with respect to the continuous-time
stochastic logic (CSL). For strong bisimulation, it is well known that it is
strictly finer than CSL equivalence. In this paper we propose strong and weak
bisimulations for CTMDPs and show that for a subclass of CTMDPs, strong and
weak bisimulations are both sound and complete with respect to the equivalences
induced by CSL and the sub-logic of CSL without next operator respectively. We
then consider a standard extension of CSL, and show that it and its sub-logic
without X can be fully characterized by strong and weak bisimulations
respectively over arbitrary CTMDPs."
"We show that the model-checking problem is decidable for a fragment of the
epistemic \mu-calculus. The fragment allows free variables within the scope of
epistemic modalities in a restricted form that avoids constructing formulas
embodying any form of common knowledge. Our calculus subsumes known decidable
fragments of epistemic CTL/LTL. Its modal variant can express winning
strategies in two-player games with one player having imperfect information and
non-observable objectives, and, with a suitable encoding, decidable instances
of the model-checking problem for ATL with imperfect information and perfect
recall can be encoded as instances of the model-checking problem for this
epistemic \mu-calculus."
"The use of interpolants in model checking is becoming an enabling technology
to allow fast and robust verification of hardware and software. The application
of encodings based on the theory of arrays, however, is limited by the
impossibility of deriving quantifier- free interpolants in general. In this
paper, we show that it is possible to obtain quantifier-free interpolants for a
Skolemized version of the extensional theory of arrays. We prove this in two
ways: (1) non-constructively, by using the model theoretic notion of
amalgamation, which is known to be equivalent to admit quantifier-free
interpolation for universal theories; and (2) constructively, by designing an
interpolating procedure, based on solving equations between array updates.
(Interestingly, rewriting techniques are used in the key steps of the solver
and its proof of correctness.) To the best of our knowledge, this is the first
successful attempt of computing quantifier- free interpolants for a variant of
the theory of arrays with extensionality."
"A grammar logic refers to an extension to the multi-modal logic K in which
the modal axioms are generated from a formal grammar. We consider a proof
theory, in nested sequent calculus, of grammar logics with converse, i.e.,
every modal operator [a] comes with a converse. Extending previous works on
nested sequent systems for tense logics, we show all grammar logics (with or
without converse) can be formalised in nested sequent calculi, where the axioms
are internalised in the calculi as structural rules. Syntactic cut-elimination
for these calculi is proved using a procedure similar to that for display
logics. If the grammar is context-free, then one can get rid of all structural
rules, in favor of deep inference and additional propagation rules. We give a
novel semi-decision procedure for context-free grammar logics, using nested
sequent calculus with deep inference, and show that, in the case where the
given context-free grammar is regular, this procedure terminates. Unlike all
other existing decision procedures for regular grammar logics in the
literature, our procedure does not assume that a finite state automaton
encoding the axioms is given."
"We study the finitary satisfiability problem for first order logic with two
variables and two binary relations, corresponding to the induced successor
relations of two finite linear orders. We show that the problem is decidable in
NEXPTIME."
"We present a logic for the specification of static analysis problems that
goes beyond the logics traditionally used. Its most prominent feature is the
direct support for both inductive computations of behaviors as well as
co-inductive specifications of properties. Two main theoretical contributions
are a Moore Family result and a parametrized worst case time complexity result.
We show that the logic and the associated solver can be used for rapid
prototyping and illustrate a wide variety of applications within Static
Analysis, Constraint Satisfaction Problems and Model Checking. In all cases the
complexity result specializes to the worst case time complexity of the
classical methods."
"Proving programs terminating is a fundamental computer science challenge.
Recent research has produced powerful tools that can check a wide range of
programs for termination. The analog for probabilistic programs, namely
termination with probability one (""almost-sure termination""), is an equally
important property for randomized algorithms and probabilistic protocols. We
suggest a novel algorithm for proving almost-sure termination of probabilistic
programs. Our algorithm exploits the power of state-of-the-art model checkers
and termination provers for nonprobabilistic programs: it calls such tools
within a refinement loop and thereby iteratively constructs a ""terminating
pattern"", which is a set of terminating runs with probability one. We report on
various case studies illustrating the effectiveness of our algorithm. As a
further application, our algorithm can improve lower bounds on reachability
probabilities."
"We provide separations between the parameterized versions of Res(1)
(Resolution) and Res(2). Using a different set of parameterized contradictions,
we also separate the parameterized versions of Res*(1) (tree-Resolution) and
Res*(2)."
"A logic is presented for reasoning on iterated sequences of formulae over
some given base language. The considered sequences, or ""schemata"", are defined
inductively, on some algebraic structure (for instance the natural numbers, the
lists, the trees etc.). A proof procedure is proposed to relate the
satisfiability problem for schemata to that of finite disjunctions of base
formulae. It is shown that this procedure is sound, complete and terminating,
hence the basic computational properties of the base language can be carried
over to schemata."
"We positively answer the question A.1.6 in J. Klop's ""Ustica Notes"": ""Is
there a recursive normalizing one-step reduction strategy for micro
$\lambda$-calculus?"" Micro $\lambda$-calculus refers to an implementation of
the $\lambda$-calculus due to Revesz, implementing $\beta$-reduction by means
of ""micro steps"" recursively distributing a $\beta$-redex $(\lambda x.M)\ N$
over its body $M$."
"In a previous paper, we presented several extensions of ACP with conditional
expressions, including one with a retrospection operator on conditions to allow
for looking back on conditions under which preceding actions have been
performed. In this paper, we add a constant for a process that is only capable
of terminating successfully to those extensions of ACP, which can be very
useful in applications. It happens that in all cases the addition of this
constant is unproblematic."
"We present a formulation of the problem of probabilistic model checking as
one of query evaluation over probabilistic logic programs. To the best of our
knowledge, our formulation is the first of its kind, and it covers a rich class
of probabilistic models and probabilistic temporal logics. The inference
algorithms of existing probabilistic logic-programming systems are well defined
only for queries with a finite number of explanations. This restriction
prohibits the encoding of probabilistic model checkers, where explanations
correspond to executions of the system being model checked. To overcome this
restriction, we propose a more general inference algorithm that uses finite
generative structures (similar to automata) to represent families of
explanations. The inference algorithm computes the probability of a possibly
infinite set of explanations directly from the finite generative structure. We
have implemented our inference algorithm in XSB Prolog, and use this
implementation to encode probabilistic model checkers for a variety of temporal
logics, including PCTL and GPL (which subsumes PCTL*). Our experiment results
show that, despite the highly declarative nature of their encodings, the model
checkers constructed in this manner are competitive with their native
implementations."
"The Distributed Ontology Language (DOL) is currently being standardized
within the OntoIOp (Ontology Integration and Interoperability) activity of
ISO/TC 37/SC 3. It aims at providing a unified framework for (1) ontologies
formalized in heterogeneous logics, (2) modular ontologies, (3) links between
ontologies, and (4) annotation of ontologies.
  This paper focuses on an application of DOL's meta-theoretical features in
mathematical formalization: validating relationships between ontological
formalizations of mathematical concepts in COLORE (Common Logic Repository),
which provide the foundation for formalizing real-world notions such as spatial
and temporal relations."
"In this paper, we extend the sequent calculus LKF into a calculus LK(T),
allowing calls to a decision procedure. We prove cut-elimination of LK(T)."
"In this paper we relate different formulations of the DPLL(T) procedure. The
first formulation is based on a system of rewrite rules, which we denote
DPLL(T). The second formulation is an inference system of, which we denote
LKDPLL(T). The third formulation is the application of a standard proof-search
mechanism in a sequent calculus LKp(T) introduced here. We formalise an
encoding from DPLL(T) to LKDPLL(T) that was, to our knowledge, never explicitly
given and, in the case where DPLL(T) is extended with backjumping and Lemma
learning, never even implicitly given. We also formalise an encoding from
LKDPLL(T) to LKp(T), building on Ivan Gazeau's previous work: we extend his
work in that we handle the ""-modulo-Theory"" aspect of SAT-modulo-theory, by
extending the sequent calculus to allow calls to a theory solver (seen as a
blackbox). We also extend his work in that we handle advanced features of DPLL
such as backjumping and Lemma learning, etc. Finally, we re fine the approach
by starting to formalise quantitative aspects of the simulations: the
complexity is preserved (number of steps to build complete proofs). Other
aspects remain to be formalised (non-determinism of the search / width of
search space)."
"Modeling time related aspects is important in many applications of
verification methods. For precise results, it is necessary to interpret time as
a dense domain, e.g. using timed automata as a formalism, even though the
system's resulting infinite state space is challenging for verification
methods. Furthermore, fully symbolic treatment of both timing related and
non-timing related elements of the state space seems to offer an attractive
approach to model checking timed systems with a large amount of
non-determinism. This paper presents an SMT-based timed system extension to the
IC3 algorithm, a SAT-based novel, highly efficient, complete verification
method for untimed systems. Handling of the infinite state spaces of timed
system in the extended IC3 algorithm is based on suitably adapting the
well-known region abstraction for timed systems. Additionally, $k$-induction,
another symbolic verification method for discrete time systems, is extended in
a similar fashion to support timed systems. Both new methods are evaluated and
experimentally compared to a booleanization-based verification approach that
uses the original discrete time IC3 algorithm."
"We consider first-order logic with monoidal quantifiers over words. We show
that all languages with a neutral letter, definable using the addition
numerical predicate are also definable with the order predicate as the only
numerical predicate. Let S be a subset of monoids.
  Let LS be the logic closed under quantification over the monoids in S and N
be the class of neutral letter languages. Then we show that: LS[<,+] cap N =
LS[<] Our result can be interpreted as the Crane Beach conjecture to hold for
the logic LS[<,+]. As a corollary of our result we get the result of Roy and
Straubing that FO+MOD[<,+] collapses to FO+MOD[<].
  For cyclic groups, we answer an open question of Roy and Straubing, proving
that MOD[<,+] collapses to MOD[<]. Our result also shows that multiplication is
necessary for Barrington's theorem to hold.
  All these results can be viewed as separation results for very uniform
circuit classes. For example we separate FO[<,+]-uniform CC0 from
FO[<,+]-uniform ACC0."
"Inductive and coinductive specifications are widely used in formalizing
computational systems. Such specifications have a natural rendition in logics
that support fixed-point definitions. Another useful formalization device is
that of recursive specifications. These specifications are not directly
complemented by fixed-point reasoning techniques and, correspondingly, do not
have to satisfy strong monotonicity restrictions. We show how to incorporate a
rewriting capability into logics of fixed-point definitions towards
additionally supporting recursive specifications. In particular, we describe a
natural deduction calculus that adds a form of ""closed-world"" equality - a key
ingredient to supporting fixed-point definitions - to deduction modulo, a
framework for extending a logic with a rewriting layer operating on formulas.
We show that our calculus enjoys strong normalizability when the rewrite system
satisfies general properties and we demonstrate its usefulness in specifying
and reasoning about syntax-based descriptions. The integration of closed-world
equality into deduction modulo leads us to reconfigure the elimination
principle for this form of equality in a way that, for the first time, resolves
issues regarding the stability of finite proofs under reduction."
"We announce a tool for mapping derivations of the E theorem prover to Mizar
proofs. Our mapping complements earlier work that generates problems for
automated theorem provers from Mizar inference checking problems. We describe
the tool, explain the mapping, and show how we solved some of the difficulties
that arise in mapping proofs between different logical formalisms, even when
they are based on the same notion of logical consequence, as Mizar and E are
(namely, first-order classical logic with identity)."
"Given any collection F of computable functions over the reals, we show that
there exists an algorithm that, given any L_F-sentence \varphi containing only
bounded quantifiers, and any positive rational number \delta, decides either
""\varphi is true"", or ""a \delta-strengthening of \varphi is false"". Under mild
assumptions, for a C-computable signature F, the \delta-decision problem for
bounded \Sigma_k-sentences in L_F resides in (\Sigma_k^P)^C. The results stand
in sharp contrast to the well-known undecidability results, and serve as a
theoretical basis for the use of numerical methods in decision procedures for
nonlinear first-order theories over the reals."
"Nominal Isabelle is a definitional extension of the Isabelle/HOL theorem
prover. It provides a proving infrastructure for reasoning about programming
language calculi involving named bound variables (as opposed to de-Bruijn
indices). In this paper we present an extension of Nominal Isabelle for dealing
with general bindings, that means term constructors where multiple variables
are bound at once. Such general bindings are ubiquitous in programming language
research and only very poorly supported with single binders, such as
lambda-abstractions. Our extension includes new definitions of
alpha-equivalence and establishes automatically the reasoning infrastructure
for alpha-equated terms. We also prove strong induction principles that have
the usual variable convention already built in."
"A simple linear loop is a simple while loop with linear assignments and
linear loop guards. If a simple linear loop has only two program variables, we
give a complete algorithm for computing the set of all the inputs on which the
loop does not terminate. For the case of more program variables, we show that
the non-termination set cannot be described by Tarski formulae in general"
"Many systems include components interacting with each other that evolve with
possibly very different speeds. To deal with this situation many formal models
adopt the abstraction of ""zero-time transitions"", which do not consume time.
These however have several drawbacks in terms of naturalness and logic
consistency, as a system is modeled to be in different states at the same time.
We propose a novel approach that exploits concepts from non-standard analysis
to introduce a notion of micro- and macro-steps in an extension of the TRIO
metric temporal logic, called X-TRIO. We use X-TRIO to provide a formal
semantics and an automated verification technique to Stateflow-like notations
used in the design of flexible manufacturing systems."
"Left-sequential logics provide a means for reasoning about (closed)
propositional terms with atomic propositions that may have side effects and
that are evaluated sequentially from left to right. Such propositional terms
are commonly used in programming languages to direct the flow of a program. In
this thesis we explore two such left-sequential logics. First we discuss Fully
Evaluated Left-Sequential Logic, which employs a full evaluation strategy,
i.e., to evaluate a term every one of its atomic propositions is evaluated
causing its possible side effects to occur. We then turn to Short-Circuit
(Left-Sequential) Logic as presented in [BP10b], where the evaluation may be
'short-circuited', thus preventing some, if not all, of the atomic propositions
in a term being evaluated. We propose evaluation trees as a natural semantics
for both logics and provide axiomatizations for the least identifying variant
of each. From this, we define a logic with connectives that prescribe a full
evaluation strategy as well as connectives that prescribe a short-circuit
evaluation strategy."
"Logic can be made useful for programming and for databases independently of
logic programming. To be useful in this way, logic has to provide a mechanism
for the definition of new functions and new relations on the basis of those
given in the interpretation of a logical theory. We provide this mechanism by
creating a compositional semantics on top of the classical semantics. In this
approach verification of computational results relies on a correspondence
between logic interpretations and a class definition in languages like Java or
C++. The advantage of this approach is the combination of an expressive medium
for the programmer with, in the case of C++, optimal use of computer resources."
"This paper contributes to the general understanding of the geometrical model
of concurrency that was named higher dimensional automata (HDAs) by Pratt. In
particular we investigate modal logics for such models and their expressive
power in terms of the bisimulation that can be captured. The geometric model of
concurrency is interesting from two main reasons: its generality and
expressiveness, and the natural way in which autoconcurrency and action
refinement are captured. Logics for this model, though, are not well
investigated, where a simple, yet adequate, modal logic over HDAs was only
recently introduced. As this modal logic, with two existential modalities,
during and after, captures only split bisimulation, which is rather low in the
spectrum of van Glabbeek and Vaandrager, the immediate question was what small
extension of this logic could capture the more fine-grained hereditary history
preserving bisimulation (hh)? In response, the work in this paper provides
several insights. One is the fact that the geometrical aspect of HDAs makes it
possible to use for capturing the hh-bisimulation, a standard modal logic that
does not employ event variables, opposed to the two logics (over less
expressive models) that we compare with. The logic that we investigate here
uses standard past modalities and extends the previously introduced logic
(called HDML) that had only forward, action-labelled, modalities. Besides, we
try to understand better the above issues by introducing a related model that
we call ST-configuration structures, which extend the configuration structures
of van Glabbeek and Plotkin. We relate this model to HDAs, and redefine and
prove the earlier results in the light of this new model. These offer a
different view on why the past modalities and geometrical concurrency capture
the hereditary history preserving bisimulation. Additional correlating insights
are also gained."
"The advance of web services technologies promises to have far-reaching
effects on the Internet and enterprise networks allowing for greater
accessibility of data. The security challenges presented by the web services
approach are formidable. In particular, access control solutions should be
revised to address new challenges, such as the need of using certificates for
the identification of users and their attributes, human intervention in the
creation or selection of the certificates, and (chains of) certificates for
trust management. With all these features, it is not surprising that analyzing
policies to guarantee that a sensitive resource can be accessed only by
authorized users becomes very difficult. In this paper, we present an automated
technique to analyze scenario-based specifications of access control policies
in open and distributed systems. We illustrate our ideas on a case study
arising in the e-government area."
"We present BEE, a compiler which enables to encode finite domain constraint
problems to CNF. Using BEE both eases the encoding process for the user and
also performs transformations to simplify constraints and optimize their
encoding to CNF. These optimizations are based primarily on equi-propagation
and on partial evaluation, and also on the idea that a given constraint may
have various possible CNF encodings. Often, the better encoding choice is made
after constraint simplification. BEE is written in Prolog and integrates
directly with a SAT solver through a suitable Prolog interface. We demonstrate
that constraint simplification is often highly beneficial when solving hard
finite domain constraint problems. A BEE implementation is available with this
paper."
"The stochastic Boolean satisfiability (SSAT) problem has been introduced by
Papadimitriou in 1985 when adding a probabilistic model of uncertainty to
propositional satisfiability through randomized quantification. SSAT has many
applications, among them probabilistic bounded model checking (PBMC) of
symbolically represented Markov decision processes. This article identifies a
notion of Craig interpolant for the SSAT framework and develops an algorithm
for computing such interpolants based on a resolution calculus for SSAT. As a
potential application area of this novel concept of Craig interpolation, we
address the symbolic analysis of probabilistic systems. We first investigate
the use of interpolation in probabilistic state reachability analysis, turning
the falsification procedure employing PBMC into a verification technique for
probabilistic safety properties. We furthermore propose an interpolation-based
approach to probabilistic region stability, being able to verify that the
probability of stabilizing within some region is sufficiently large."
"In this paper we present two terminating tableau calculi for propositional
Dummett logic obeying the subformula property. The ideas of our calculi rely on
the linearly ordered Kripke semantics of Dummett logic. The first calculus
works on two semantical levels: the present and the next possible world. The
second calculus employs the usual object language of tableau systems and
exploits a property of the construction of the completeness theorem to
introduce a check which is an alternative to loop check mechanisms."
"We give an algebraic characterization of the syntax and semantics of a class
of simply-typed languages, such as the language PCF: we characterize
simply-typed binding syntax equipped with reduction rules via a universal
property, namely as the initial object of some category. For this purpose, we
employ techniques developed in two previous works: in [2], we model syntactic
translations between languages over different sets of types as initial
morphisms in a category of models. In [1], we characterize untyped syntax with
reduction rules as initial object in a category of models. In the present work,
we show that those techniques are modular enough to be combined: we thus
characterize simply-typed syntax with reduction rules as initial object in a
category. The universal property yields an operator which allows to specify
translations - that are semantically faithful by construction - between
languages over possibly different sets of types.
  We specify a language by a 2-signature, that is, a signature on two levels:
the syntactic level specifies the types and terms of the language, and
associates a type to each term. The semantic level specifies, through
inequations, reduction rules on the terms of the language. To any given
2-signature we associate a category of models. We prove that this category has
an initial object, which integrates the types and terms freely generated by the
2-signature, and the reduction relation on those terms generated by the given
inequations. We call this object the (programming) language generated by the
2-signature.
  [1] Ahrens, B.: Modules over relative monads for syntax and semantics (2011),
arXiv:1107.5252, to be published in Math. Struct. in Comp. Science
  [2] Ahrens, B.: Extended Initiality for Typed Abstract Syntax. Logical
Methods in Computer Science 8(2), 1-35 (2012)"
"In this thesis we give an algebraic characterization of the syntax and
semantics of simply-typed languages. More precisely, we characterize
simply-typed binding syntax equipped with reduction rules via a universal
property, namely as the initial object of some category. We specify a language
by a 2-signature ({\Sigma}, A), that is, a signature on two levels: the
syntactic level {\Sigma} specifies the sorts and terms of the language, and
associates a sort to each term. The semantic level A specifies, through
inequations, reduction rules on the terms of the language. To any given
2-signature ({\Sigma}, A) we associate a category of ""models"" of ({\Sigma}, A).
We prove that this category has an initial object, which integrates the terms
freely generated by {\Sigma} and the reduction relation - on those terms -
generated by A. We call this object the programming language generated by
({\Sigma}, A).
  Initiality provides an iteration principle which allows to specify
translations on the syntax, possibly to a language over different sorts.
Furthermore, translations specified via the iteration principle are by
construction type-safe and faithful with respect to reduction.
  To illustrate our results, we consider two examples extensively: firstly, we
specify a double negation translation from classical to intuitionistic
propositional logic via the category-theoretic iteration principle. Secondly,
we specify a translation from PCF to the untyped lambda calculus which is
faithful with respect to reduction in the source and target languages.
  In a second part, we formalize some of our initiality theorems in the proof
assistant Coq. The implementation yields a machinery which, when given a
2-signature, returns an implementation of its associated abstract syntax
together with certified substitution operation, iteration operator and a
reduction relation generated by the specified reduction rules."
"When their reading heads are allowed to move completely asynchronously,
finite-state automata with multiple tapes achieve a significant expressive
power, but also lose useful closure properties---closure under intersection, in
particular. This paper investigates to what extent it is still feasible to use
multi-tape automata as recognizer of polyadic predicates on words. On the
negative side, determining whether the intersection of asynchronous multi-tape
automata is expressible is not even semidecidable. On the positive side, we
present an algorithm that computes under-approximations of the intersection;
and discuss simple conditions under which it can construct complete
intersections. A prototype implementation and a few non-trivial examples
demonstrate the algorithm in practice."
"Unravelings are transformations from a conditional term rewriting system
(CTRS, for short) over an original signature into an unconditional term
rewriting systems (TRS, for short) over an extended signature. They are not
sound w.r.t. reduction for every CTRS, while they are complete w.r.t.
reduction. Here, soundness w.r.t. reduction means that every reduction sequence
of the corresponding unraveled TRS, of which the initial and end terms are over
the original signature, can be simulated by the reduction of the original CTRS.
In this paper, we show that an optimized variant of Ohlebusch's unraveling for
a deterministic CTRS is sound w.r.t. reduction if the corresponding unraveled
TRS is left-linear or both right-linear and non-erasing. We also show that
soundness of the variant implies that of Ohlebusch's unraveling. Finally, we
show that soundness of Ohlebusch's unraveling is the weakest in soundness of
the other unravelings and a transformation, proposed by Serbanuta and Rosu, for
(normal) deterministic CTRSs, i.e., soundness of them respectively implies that
of Ohlebusch's unraveling."
"Multi-objective probabilistic model checking provides a way to verify
several, possibly conflicting, quantitative properties of a stochastic system.
It has useful applications in controller synthesis and compositional
probabilistic verification. However, existing methods are based on linear
programming, which limits the scale of systems that can be analysed and makes
verification of time-bounded properties very difficult. We present a novel
approach that addresses both of these shortcomings, based on the generation of
successive approximations of the Pareto curve for a multi-objective model
checking problem. We illustrate dramatic improvements in efficiency on a large
set of benchmarks and show how the ability to visualise Pareto curves
significantly enhances the quality of results obtained from current
probabilistic verification tools."
"We study the notion of stratification, as used in subsystems of linear logic
with low complexity bounds on the cut-elimination procedure (the so-called
light logics), from an abstract point of view, introducing a logical system in
which stratification is handled by a separate modality. This modality, which is
a generalization of the paragraph modality of Girard's light linear logic,
arises from a general categorical construction applicable to all models of
linear logic. We thus learn that stratification may be formulated independently
of exponential modalities; when it is forced to be connected to exponential
modalities, it yields interesting complexity properties. In particular, from
our analysis stem three alternative reformulations of Baillot and Mazza's
linear logic by levels: one geometric, one interactive, and one semantic."
"We present a locale that abstracts over the necessary ingredients for
constructing a minimal bad sequence, as required in classical proofs of
Higman's lemma and Kruskal's tree theorem."
"We give a beginner-oriented introduction to Isabelle/jEdit, providing
motivation for using it as well as pointing at some differences to the
traditional Proof General interface and current limitations."
"This paper presents a tableau approach for deciding expressive description
logics with full role negation and role identity. We consider the description
logic ALBOid, which is the extension of ALC with the Boolean role operators,
inverse of roles, the identity role, and includes full support for individuals
and singleton concepts. ALBOid is expressively equivalent to the two-variable
fragment of first-order logic with equality and subsumes Boolean modal logic.
In this paper we define a sound and complete tableau calculus for the ALBOid
that provides a basis for decision procedures for this logic and all its
sublogics. An important novelty of our approach is the use of a generic
unrestricted blocking mechanism. Being based on a conceptually simple rule,
unrestricted blocking performs case distinctions over whether two individuals
are equal or not and equality reasoning to find finite models. The blocking
mechanism ties the proof of termination of tableau derivations to the finite
model property of ALBOid."
"Since the first termination competition in 2004 it is of great interest,
whether a proof that has been automatically generated by a termination tool, is
indeed correct. The increasing number of termination proving techniques as well
as the increasing complexity of generated proofs (e.g., combinations of several
techniques, exhaustive labelings, tree automata, etc.), make certifying (i.e.,
checking the correctness of) such proofs more and more tedious for humans.
Hence the interest in automated certification of termination proofs. This led
to the general approach of using proof assistants (like Coq and Isabelle) for
certification. We present the latest developments for IsaFoR/CeTA (version
1.03) which is the certifier CeTA, based on the Isabelle/HOL formalization of
rewriting IsaFoR."
"There are termination proofs that are produced by termination tools for which
certifiers are not powerful enough. However, a similar situation also occurs in
the other direction. We have formalized termination techniques in a more
general setting as they have been introduced. Hence, we can certify proofs
using techniques that no termination tool supports so far. In this paper we
shortly present two of these formalizations: Polynomial orders with negative
constants and Arctic termination."
"In this paper we generalize the DP framework to a relative DP framework,
where a so called split is possible."
"When we want to answer/certify whether a given equation is entailed by an
equational system we face the following problems: (1) It is hard to find a
conversion (but easy to certify a given one). (2) Under the assumption that
Knuth-Bendix completion is successful, it is easy to decide the existence of a
conversion but hard to certify this decision. In this paper we introduce
recording completion, which overcomes both problems."
"We report on our formalization of matrix-interpretation in Isabelle/HOL.
Matrices are required to certify termination proofs and we wish to utilize them
for complexity proofs, too. For the latter aim, only basic methods have already
been integrated, and we discuss some upcoming problems which arise when
formalizing more complicated results on matrix-interpretations, which are based
on Cayley-Hamilton's theorem or joint-spectral radius theory."
"An orthogonal approach to the fuzzification of both multisets and hybrid sets
is presented. In particular, we introduce L-multi-fuzzy and L-fuzzy hybrid
sets, which are general enough and in spirit with the basic concepts of fuzzy
set theory. In addition, we study the properties of these structures. Also, the
usefulness of these structures is examined in the framework of mechanical
multiset processing. More specifically, we introduce a variant of fuzzy P
systems and, since simple fuzzy membrane systems have been introduced
elsewhere, we simply extend previously stated results and ideas."
"We study two notions of expressiveness, which have appeared in abstraction
theory for model checking, and find them incomparable in general. In
particular, we show that according to the most widely used notion, the class of
Kripke Modal Transition Systems is strictly less expressive than the class of
Generalised Kripke Modal Transition Systems (a generalised variant of Kripke
Modal Transition Systems equipped with hypertransitions). Furthermore, we
investigate the ability of an abstraction framework to prove a formula with a
finite abstract model, a property known as completeness. We address the issue
of completeness from a general perspective: the way it depends on certain
abstraction parameters, as well as its relationship with expressiveness."
"This paper proposes a definition of what it means for one system description
language to encode another one, thereby enabling an ordering of system
description languages with respect to expressive power. I compare the proposed
definition with other definitions of encoding and expressiveness found in the
literature, and illustrate it on a case study: comparing the expressive power
of CCS and CSP."
"We present a concurrent operational Petri net semantics for the
join-calculus, a process calculus for specifying concurrent and distributed
systems. There often is a gap between system specifications and the actual
implementations caused by synchrony assumptions on the specification side and
asynchronously interacting components in implementations. The join-calculus is
promising to reduce this gap by providing an abstract specification language
which is asynchronously distributable. Classical process semantics establish an
implicit order of actually independent actions, by means of an interleaving. So
does the semantics of the join-calculus. To capture such independent actions,
step-based semantics, e.g., as defined on Petri nets, are employed. Our Petri
net semantics for the join-calculus induces step-behavior in a natural way. We
prove our semantics behaviorally equivalent to the original join-calculus
semantics by means of a bisimulation. We discuss how join specific assumptions
influence an existing notion of distributability based on Petri nets."
"The logic of reason-based preference advanced in Osherson and Weinstein
(2012) is extended to quantifiers. Basic properties of the new system are
discussed."
"We present the topos S of trees as a model of guarded recursion. We study the
internal dependently-typed higher-order logic of S and show that S models two
modal operators, on predicates and types, which serve as guards in recursive
definitions of terms, predicates, and types. In particular, we show how to
solve recursive type equations involving dependent types. We propose that the
internal logic of S provides the right setting for the synthetic construction
of abstract versions of step-indexed models of programming languages and
program logics. As an example, we show how to construct a model of a
programming language with higher-order store and recursive types entirely
inside the internal logic of S. Moreover, we give an axiomatic categorical
treatment of models of synthetic guarded domain theory and prove that, for any
complete Heyting algebra A with a well-founded basis, the topos of sheaves over
A forms a model of synthetic guarded domain theory, generalizing the results
for S."
"Category Theory is a well-known powerful mathematical modeling language with
a wide area of applications in mathematics and computer science, including
especially the semantical foundations of topics in software science and
development. Categorical methods are already well established for the
semantical foundation of type theory (cartesian closed categories), data type
specification frameworks (institutions) and graph transformation (adhesive high
level replacement categories).
  It is the intention of the ACCAT Workshops on Applied and Computational
Category Theory to bring together leading researchers in these areas with those
in software science and development in order to transfer categorical concepts
and theories in both directions. The workshops aims to represent a forum for
researchers and practitioners who are interested in an exchange of ideas,
notions, and techniques for different applications of category theory.
  The seventh ACCAT workshop on Applied and Computational Category Theory 2012
was held in Tallinn, Estonia on the 1st of April 2012 as a satellite event of
ETAPS 2012. This issue contains the full version of one of the invited talks as
well as the submitted papers, which cover a wide range of applications of
category theory, from model-driven engineering over transition systems in
stochastic processes to transformations in M-adhesive categories."
"In ubiquitous computing devices, users tend to store some valuable
information in their device. Even though the device can be borrowed by the
other user temporarily, it is not safe for any user to borrow or lend the
device as it may cause private data of the user to be public. To safeguard the
user data and also to preserve user privacy we propose and model the technique
of ownership authentication transfer. The user who is willing to sell the
device has to transfer the ownership of the device under sale. Once the device
is sold and the ownership has been transferred, the old owner will not be able
to use that device at any cost. Either of the users will not be able to use the
device if the process of ownership has not been carried out properly. This also
takes care of the scenario when the device has been stolen or lost, avoiding
the impersonation attack. The aim of this paper is to model basic process of
proposed ownership authentication transfer protocol and check its safety
properties by representing it using CSP and model checking approach. For model
checking we have used a symbolic model checker tool called NuSMV. The safety
properties of ownership transfer protocol has been modeled in terms of CTL
specification and it is observed that the system satisfies all the protocol
constraint and is safe to be deployed."
"We describe a simple, conceptual forward analysis procedure for
infinity-complete WSTS S. This computes the so-called clover of a state. When S
is the completion of a WSTS X, the clover in S is a finite description of the
downward closure of the reachability set. We show that such completions are
infinity-complete exactly when X is an omega-2-WSTS, a new robust class of
WSTS. We show that our procedure terminates in more cases than the generalized
Karp-Miller procedure on extensions of Petri nets and on lossy channel systems.
We characterize the WSTS where our procedure terminates as those that are
clover-flattable. Finally, we apply this to well-structured counter systems."
"Metric Temporal Logic (MTL) is a generalisation of Linear Temporal Logic in
which the Until and Since modalities are annotated with intervals that express
metric constraints. A seminal result of Hirshfeld and Rabinovich shows that
over the reals, first-order logic with binary order relation < and unary
function +1 is strictly more expressive than MTL with integer constants. Indeed
they prove that no temporal logic whose modalities are definable by formulas of
bounded quantifier depth can be expressively complete for FO(<,+1). In this
paper we show the surprising result that if we allow unary functions +q, (q
rational), in first-order logic and correspondingly allow rational constants in
MTL, then the two logics have the same expressive power. This gives the first
generalisation of Kamp's theorem on the expressive completeness of LTL for
FO(<) to the quantitative setting. The proof of this result involves a
generalisation of Gabbay's notion of separation."
"Alternating timed automata on infinite words are considered. The main result
is a characterization of acceptance conditions for which the emptiness problem
for these automata is decidable. This result implies new decidability results
for fragments of timed temporal logics. It is also shown that, unlike for MITL,
the characterisation remains the same even if no punctual constraints are
allowed."
"For many application-level distributed protocols and parallel algorithms, the
set of participants, the number of messages or the interaction structure are
only known at run-time. This paper proposes a dependent type theory for
multiparty sessions which can statically guarantee type-safe, deadlock-free
multiparty interactions among processes whose specifications are parameterised
by indices. We use the primitive recursion operator from G\""odel's System T to
express a wide range of communication patterns while keeping type checking
decidable. To type individual distributed processes, a parameterised global
type is projected onto a generic generator which represents a class of all
possible end-point types. We prove the termination of the type-checking
algorithm in the full system with both multiparty session types and recursive
types. We illustrate our type theory through non-trivial programming and
verification examples taken from parallel algorithms and Web services usecases."
"In 1965 Dag Prawitz presented an extension of Gentzen-type systems of Natural
Deduction to modal concepts of S4. Maria da Paz Medeiros showed in 2006 that
the proof of normalisation for classical S4 does not hold and proposed a new
proof of normalisation for a logically equivalent system, the system NS4.
However two problems in the proof of the critical lemma used by Medeiros in her
proof were pointed out by Yuuki Andou in 2009. This paper presents a proof of
the critical lemma, resulting in a proof of normalisation for NS4."
"In this paper, we study thetime-bounded reachability problem for rectangular
hybrid automata with non-negative rates (RHA+). This problem was recently shown
to be decidable [Brihaye et al, ICALP11] (even though the unbounded
reachability problem for even very simple classes of hybrid automata is
well-known to be undecidable). However, [Brihaye et al, ICALP11] does not
provide a precise characterisation of the complexity of the time-bounded
reachability problem. The contribution of the present paper is threefold.
First, we provide a new NExpTime algorithm to solve the timed-bounded
reachability problem on RHA+. This algorithm improves on the one of [Brihaye et
al, ICALP11] by at least one exponential. Second, we show that this new
algorithm is optimal, by establishing a matching lower bound: time-bounded
reachability for RHA+ is therefore NExpTime-complete. Third, we extend these
results in a practical direction, by showing that we can effectively compute
fixpoints that characterise the sets of states that are reachable (resp.
co-reachable) within T time units from a given starting state."
"The probabilistic modal {\mu}-calculus is a fixed-point logic designed for
expressing properties of probabilistic labeled transition systems (PLTS's). Two
equivalent semantics have been studied for this logic, both assigning to each
state a value in the interval [0,1] representing the probability that the
property expressed by the formula holds at the state. One semantics is
denotational and the other is a game semantics, specified in terms of
two-player stochastic parity games. A shortcoming of the probabilistic modal
{\mu}-calculus is the lack of expressiveness required to encode other important
temporal logics for PLTS's such as Probabilistic Computation Tree Logic (PCTL).
To address this limitation we extend the logic with a new pair of operators:
independent product and coproduct. The resulting logic, called probabilistic
modal {\mu}-calculus with independent product, can encode many properties of
interest and subsumes the qualitative fragment of PCTL. The main contribution
of this paper is the definition of an appropriate game semantics for this
extended probabilistic {\mu}-calculus. This relies on the definition of a new
class of games which generalize standard two-player stochastic (parity) games
by allowing a play to be split into concurrent subplays, each continuing their
evolution independently. Our main technical result is the equivalence of the
two semantics. The proof is carried out in ZFC set theory extended with
Martin's Axiom at an uncountable cardinal."
"Answer set programming is one of the most praised frameworks for declarative
programming in general and non-monotonic reasoning in particular. There has
been many efforts to extend stable model semantics so that answer set programs
can use a more extensive syntax. To such endeavor, the community of
non-monotonic reasoning has introduced extensions such as equilibrium models
and FLP semantics. However, both of these extensions suffer from two problems:
intended models according to such extensions (1) are not guaranteed to be
minimal, and (2) more importantly, may have self-justifications (i.e., the
justification for pertinence of an atom in an intended model may be its own
pertinence). Both of these properties directly violate the spirit of stable
model semantics. Therefore, we believe that we need a new extension of stable
model semantics that guarantees both minimality and being strongly grounded.
  This paper introduces one such extension using two different approaches:
firstly, by extending the goal-reachability interpretation of logic programs to
the full propositional language and, secondly, using derivability in
intuitionistic propositional logic. We show that both these approaches give the
same semantics, that we call the supported semantics. Moreover, using the first
approach, we also extend well-founded semantics to full propositional language.
Furthermore, we discuss how our supported models relate to other existing
semantics for non-monotonic reasoning including the equilibrium models. Last,
but not the least, we discuss the complexity of reasoning about supported
models and show that all interesting reasoning tasks (such as brave/cautious
reasoning) are PSPACE-complete. Therefore, supported model semantics is a much
more expressive semantics then the existing semantics such as equlibrium models
(that have reasoning procedures in $\Delta^P_3$)."
"Modular verification is a technique used to face the state explosion problem
often encountered in the verification of properties of complex systems such as
concurrent interactive systems. The modular approach is based on the
observation that properties of interest often concern a rather small portion of
the system. As a consequence, reduced models can be constructed which
approximate the overall system behaviour thus allowing more efficient
verification.
  Biochemical pathways can be seen as complex concurrent interactive systems.
Consequently, verification of their properties is often computationally very
expensive and could take advantage of the modular approach.
  In this paper we report preliminary results on the development of a modular
verification framework for biochemical pathways. We view biochemical pathways
as concurrent systems of reactions competing for molecular resources. A modular
verification technique could be based on reduced models containing only
reactions involving molecular resources of interest.
  For a proper description of the system behaviour we argue that it is
essential to consider a suitable notion of fairness, which is a
well-established notion in concurrency theory but novel in the field of pathway
modelling. We propose a modelling approach that includes fairness and we
identify the assumptions under which verification of properties can be done in
a modular way.
  We prove the correctness of the approach and demonstrate it on the model of
the EGF receptor-induced MAP kinase cascade by Schoeberl et al."
"The resource calculus is an extension of the lambda-calculus allowing to
model resource consumption. It is intrinsically non-deterministic and has two
general notions of reduction - one parallel, preserving all the possible
results as a formal sum, and one non-deterministic, performing an exclusive
choice at every step. We prove that the non-deterministic reduction enjoys a
notion of standardization, which is the natural extension with respect to the
similar one in classical lambda-calculus. The full parallel reduction only
enjoys a weaker notion of standardization instead. The result allows an
operational characterization of may-solvability, which has been introduced and
already characterized (from the syntactical and logical points of view) by
Pagani and Ronchi Della Rocca."
"The biologically inspired framework of port-graphs has been successfully used
to specify complex systems. It is the basis of the PORGY modelling tool. To
facilitate the specification of proof normalisation procedures via graph
rewriting, in this paper we add higher-order features to the original
port-graph syntax, along with a generalised notion of graph morphism. We
provide a matching algorithm which enables to implement higher-order port-graph
rewriting in PORGY, thus one can visually study the dynamics of the systems
modelled. We illustrate the expressive power of higher-order port-graphs with
examples taken from proof-net reduction systems."
"Session types capture precise protocol structure in concurrent programming,
but do not specify properties of the exchanged values beyond their basic type.
Refinement types are a form of dependent types that can address this
limitation, combining types with logical formulae that may refer to program
values and can constrain types using arbitrary predicates. We present a pi
calculus with assume and assert operations, typed using a session discipline
that incorporates refinement formulae written in a fragment of Multiplicative
Linear Logic. Our original combination of session and refinement types,
together with the well established benefits of linearity, allows very
fine-grained specifications of communication protocols in which refinement
formulae are treated as logical resources rather than persistent truths."
"We show that the proof-theoretic notion of logical preorder coincides with
the process-theoretic notion of contextual preorder for a CCS-like calculus
obtained from the formula-as-process interpretation of a fragment of linear
logic. The argument makes use of other standard notions in process algebra,
namely a labeled transition system and a coinductively defined simulation
relation. This result establishes a connection between an approach to reason
about process specifications and a method to reason about logic specifications."
"We extend the textual calculus for interaction nets by generic rules and
propose constraints to preserve uniform confluence. Furthermore, we discuss the
implementation of generic rules in the language inets, which is based on the
lightweight interaction nets calculus."
"Weighted model counting (WMC) is a well-known inference task on knowledge
bases, used for probabilistic inference in graphical models. We introduce
algebraic model counting (AMC), a generalization of WMC to a semiring
structure. We show that AMC generalizes many well-known tasks in a variety of
domains such as probabilistic inference, soft constraints and network and
database analysis. Furthermore, we investigate AMC from a knowledge compilation
perspective and show that all AMC tasks can be evaluated using sd-DNNF
circuits. We identify further characteristics of AMC instances that allow for
the use of even more succinct circuits."
"This paper presents a range of quantitative extensions for the temporal logic
CTL. We enhance temporal modalities with the ability to constrain the number of
states satisfying certain sub-formulas along paths. By selecting the
combinations of Boolean and arithmetic operations allowed in constraints, one
obtains several distinct logics generalizing CTL. We provide a thorough
analysis of their expressiveness and succinctness, and of the complexity of
their model-checking and satisfiability problems (ranging from P-complete to
undecidable). Finally, we present two alternative logics with similar features
and provide a comparative study of the properties of both variants."
"Continuous Markovian Logic (CML) is a multimodal logic that expresses
quantitative and qualitative properties of continuous-time labelled Markov
processes with arbitrary (analytic) state-spaces, henceforth called continuous
Markov processes (CMPs). The modalities of CML evaluate the rates of the
exponentially distributed random variables that characterize the duration of
the labeled transitions of a CMP. In this paper we present weak and strong
complete axiomatizations for CML and prove a series of metaproperties,
including the finite model property and the construction of canonical models.
CML characterizes stochastic bisimilarity and it supports the definition of a
quantified extension of the satisfiability relation that measures the
""compatibility"" between a model and a property. In this context, the
metaproperties allows us to prove two robustness theorems for the logic stating
that one can perturb formulas and maintain ""approximate satisfaction""."
"Parse trees are fundamental syntactic structures in both computational
linguistics and compilers construction. We argue in this paper that, in both
fields, there are good incentives for model-checking sets of parse trees for
some word according to a context-free grammar. We put forward the adequacy of
propositional dynamic logic (PDL) on trees in these applications, and study as
a sanity check the complexity of the corresponding model-checking problem:
although complete for exponential time in the general case, we find natural
restrictions on grammars for our applications and establish complexities
ranging from nondeterministic polynomial time to polynomial space in the
relevant cases."
"Post Embedding Problems are a family of decision problems based on the
interaction of a rational relation with the subword embedding ordering, and are
used in the literature to prove non multiply-recursive complexity lower bounds.
We refine the construction of Chambart and Schnoebelen (LICS 2008) and prove
parametric lower bounds depending on the size of the alphabet."
"In systems verification we are often concerned with multiple, inter-dependent
properties that a program must satisfy. To prove that a program satisfies a
given property, the correctness of intermediate states of the program must be
characterized. However, this intermediate reasoning is not always phrased such
that it can be easily re-used in the proofs of subsequent properties. We
introduce a function annotation logic that extends Hoare logic in two important
ways: (1) when proving that a function satisfies a Hoare triple, intermediate
reasoning is automatically stored as function annotations, and (2) these
function annotations can be exploited in future Hoare logic proofs. This
reduces duplication of reasoning between the proofs of different properties,
whilst serving as a drop-in replacement for traditional Hoare logic to avoid
the costly process of proof refactoring. We explain how this was implemented in
Isabelle/HOL and applied to an experimental branch of the seL4 microkernel to
significantly reduce the size and complexity of existing proofs."
"We present a refinement of the Calculus of Inductive Constructions in which
one can easily define a notion of relational parametricity. It provides a new
way to automate proofs in an interactive theorem prover like Coq."
"Full Intuitionistic Linear Logic (FILL) is multiplicative intuitionistic
linear logic extended with par. Its proof theory has been notoriously difficult
to get right, and existing sequent calculi all involve inference rules with
complex annotations to guarantee soundness and cut-elimination. We give a
simple and annotation-free display calculus for FILL which satisfies Belnap's
generic cut-elimination theorem. To do so, our display calculus actually
handles an extension of FILL, called Bi-Intuitionistic Linear Logic (BiILL),
with an `exclusion' connective defined via an adjunction with par. We refine
our display calculus for BiILL into a cut-free nested sequent calculus with
deep inference in which the explicit structural rules of the display calculus
become admissible. A separation property guarantees that proofs of FILL
formulae in the deep inference calculus contain no trace of exclusion. Each
such rule is sound for the semantics of FILL, thus our deep inference calculus
and display calculus are conservative over FILL. The deep inference calculus
also enjoys the subformula property and terminating backward proof search,
which gives the NP-completeness of BiILL and FILL."
"We develop the mathematical theory of epistemic updates with the tools of
duality theory. We focus on the Logic of Epistemic Actions and Knowledge (EAK),
introduced by Baltag-Moss- Solecki, without the common knowledge operator. We
dually characterize the product update construction of EAK as a certain
construction transforming the complex algebras associated with the given model
into the complex algebra associated with the updated model. This dual
characterization naturally generalizes to much wider classes of algebras, which
include, but are not limited to, arbitrary BAOs and arbitrary modal expansions
of Heyting algebras (HAOs). As an application of this dual characterization, we
axiomatize the intuitionistic analogue of the logic of epistemic knowledge and
actions, which we refer to as IEAK, prove soundness and completeness of IEAK
w.r.t. both algebraic and relational models, and illustrate how IEAK encodes
the reasoning of agents in a concrete epistemic scenario."
"Particle-style token machines are a way to interpret proofs and programs,
when the latter are defined according to the principles of linear logic. In
this paper, we show that token machines also make sense when the programs at
hand are those of a simple linear quantum $\lambda$-calculus. This, however,
requires generalizing the concept of a token machine to one in which more than
one particle can possibly travel around the term at the same time. This is
intimately related to entanglement and allows to give a simple operational
semantics to the calculus coherently with the principles of quantum
computation."
"Ludics is a reconstruction of logic with interaction as a primitive notion,
in the sense that the primary logical concepts are no more formulas and proofs
but cut-elimination interpreted as an interaction between objects called
designs. When the interaction between two designs goes well, such two designs
are said to be orthogonal. A behaviour is a set of designs closed under
bi-orthogonality. Logical formulas are then denoted by behaviours. Finally
proofs are interpreted as designs satisfying particular properties. In that
way, designs are more general than proofs and we may notice in particular that
they are not typed objects. Incarnation is introduced by Girard in Ludics as a
characterization of ""useful"" designs in a behaviour. The incarnation of a
design is defined as its subdesign that is the smallest one in the behaviour
ordered by inclusion. It is useful in particular because being ""incarnated"" is
one of the conditions for a design to denote a proof of a formula. The
computation of incarnation is important also as it gives a minimal denotation
for a formula, and more generally for a behaviour. We give here a constructive
way to capture the incarnation of the behaviour of a set of designs, without
computing the behaviour itself. The method we follow uses an alternative
definition of designs: rather than defining them as sets of chronicles, we
consider them as sets of paths, a concept very close to that of play in game
semantics that allows an easier handling of the interaction: the unfolding of
interaction is a path common to two interacting designs."
"Game semantics extends the Curry-Howard isomorphism to a three-way
correspondence: proofs, programs, strategies. But the universe of strategies
goes beyond intuitionistic logics and lambda calculus, to capture stateful
programs. In this paper we describe a logical counterpart to this extension, in
which proofs denote such strategies. The system is expressive: it contains all
of the connectives of Intuitionistic Linear Logic, and first-order
quantification. Use of Laird's sequoid operator allows proofs with imperative
behaviour to be expressed. Thus, we can embed first-order Intuitionistic Linear
Logic into this system, Polarized Linear Logic, and an imperative total
programming language.
  The proof system has a tight connection with a simple game model, where games
are forests of plays. Formulas are modelled as games, and proofs as
history-sensitive winning strategies. We provide a strong full completeness
result with respect to this model: each finitary strategy is the denotation of
a unique analytic (cut-free) proof. Infinite strategies correspond to analytic
proofs that are infinitely deep. Thus, we can normalise proofs, via the
semantics."
"Verification methods based on SAT, SMT, and Theorem Proving often rely on
proofs of unsatisfiability as a powerful tool to extract information in order
to reduce the overall effort. For example a proof may be traversed to identify
a minimal reason that led to unsatisfiability, for computing abstractions, or
for deriving Craig interpolants. In this paper we focus on two important
aspects that concern efficient handling of proofs of unsatisfiability:
compression and manipulation. First of all, since the proof size can be very
large in general (exponential in the size of the input problem), it is indeed
beneficial to adopt techniques to compress it for further processing. Secondly,
proofs can be manipulated as a flexible preprocessing step in preparation for
interpolant computation. Both these techniques are implemented in a framework
that makes use of local rewriting rules to transform the proofs. We show that a
careful use of the rules, combined with existing algorithms, can result in an
effective simplification of the original proofs. We have evaluated several
heuristics on a wide range of unsatisfiable problems deriving from SAT and SMT
test cases."
"Hierarchical proof trees (hiproofs for short) add structure to ordinary proof
trees, by allowing portions of trees to be hierarchically nested. The
additional structure can be used to abstract away from details, or to label
particular portions to explain their purpose. In this paper we present two
complementary methods for capturing hiproofs in HOL Light, along with a tool to
produce web-based visualisations. The first method uses tactic recording, by
modifying tactics to record their arguments and construct a hierarchical tree;
this allows a tactic proof script to be modified. The second method uses proof
recording, which extends the HOL Light kernel to record hierachical proof trees
alongside theorems. This method is less invasive, but requires care to manage
the size of the recorded objects. We have implemented both methods, resulting
in two systems: Tactician and HipCam."
"This paper extends the fibrational approach to induction and coinduction
pioneered by Hermida and Jacobs, and developed by the current authors, in two
key directions. First, we present a dual to the sound induction rule for
inductive types that we developed previously. That is, we present a sound
coinduction rule for any data type arising as the carrier of the final
coalgebra of a functor, thus relaxing Hermida and Jacobs' restriction to
polynomial functors. To achieve this we introduce the notion of a quotient
category with equality (QCE) that i) abstracts the standard notion of a
fibration of relations constructed from a given fibration; and ii) plays a role
in the theory of coinduction dual to that played by a comprehension category
with unit (CCU) in the theory of induction. Secondly, we show that inductive
and coinductive indexed types also admit sound induction and coinduction rules.
Indexed data types often arise as carriers of initial algebras and final
coalgebras of functors on slice categories, so we give sufficient conditions
under which we can construct, from a CCU (QCE) U:E \rightarrow B, a fibration
with base B/I that models indexing by I and is also a CCU (resp., QCE). We
finish the paper by considering the more general case of sound induction and
coinduction rules for indexed data types when the indexing is itself given by a
fibration."
"We propose for the Effective Topos an alternative construction: a
realisability framework composed of two levels of abstraction. This
construction simplifies the proof that the Effective Topos is a topos (equipped
with natural numbers), which is the main issue that this paper addresses. In
this our work can be compared to Frey's monadic tripos-to-topos construction.
However, no topos theory or even category theory is here required for the
construction of the framework itself, which provides a semantics for
higher-order type theories, supporting extensional equalities and the axiom of
unique choice."
"We consider the model checking problem for Gap-order Constraint Systems (GCS)
w.r.t. the branching-time temporal logic CTL, and in particular its fragments
EG and EF.
  GCS are nondeterministic infinitely branching processes described by
evolutions of integer-valued variables, subject to Presburger constraints of
the form $x-y\ge k$, where $x$ and $y$ are variables or constants and
$k\in\mathbb{N}$ is a non-negative constant.
  We show that EG model checking is undecidable for GCS, while EF is decidable.
In particular, this implies the decidability of strong and weak bisimulation
equivalence between GCS and finite-state systems."
"There have been several recent suggestions for tableau systems for deciding
satisfiability in the practically important branching time temporal logic known
as CTL*. In this paper we present a streamlined and more traditional tableau
approach built upon the author's earlier theoretical work.
  Soundness and completeness results are proved. A prototype implementation
demonstrates the significantly improved performance of the new approach on a
range of test formulas. We also see that it compares favourably to state of the
art, game and automata based decision procedures."
"In this paper we present a satisfiability-preserving reduction from MITL
interpreted over finitely-variable continuous behaviors to Constraint LTL over
clocks, a variant of CLTL that is decidable, and for which an SMT-based bounded
satisfiability checker is available. The result is a new complete and effective
decision procedure for MITL. Although decision procedures for MITL already
exist, the automata-based techniques they employ appear to be very difficult to
realize in practice, and, to the best of our knowledge, no implementation
currently exists for them. A prototype tool for MITL based on the encoding
presented here has, instead, been implemented and is publicly available."
"The problem of model-checking hybrid systems is a long-time challenge in the
scientific community. Most of the existing approaches and tools are either
limited on the properties that they can verify, or restricted to simplified
classes of systems. To overcome those limitations, a temporal logic called
HyLTL has been recently proposed. The model checking problem for this logic has
been solved by translating the formula into an equivalent hybrid automaton,
that can be analized using existing tools. The original construction employs a
declarative procedure that generates exponentially many states upfront, and can
be very inefficient when complex formulas are involved. In this paper we solve
a technical issue in the construction that was not considered in previous
works, and propose a new algorithm to translate HyLTL into hybrid automata,
that exploits optimized techniques coming from the discrete LTL community to
build smaller automata."
"We introduce basic notions and results about relation liftings on categories
enriched in a commutative quantale. We derive two necessary and sufficient
conditions for a 2-functor T to admit a functorial relation lifting: one is the
existence of a distributive law of T over the ""powerset monad"" on categories,
one is the preservation by T of ""exactness"" of certain squares. Both
characterisations are generalisations of the ""classical"" results known for set
functors: the first characterisation generalises the existence of a
distributive law over the genuine powerset monad, the second generalises
preservation of weak pullbacks. The results presented in this paper enable us
to compute predicate liftings of endofunctors of, for example, generalised
(ultra)metric spaces. We illustrate this by studying the coalgebraic cover
modality in this setting."
"We propose an improvement of the famous IC3 algorithm for model checking
safety properties of finite state systems. We collect models computed by the
SAT-solver during the clause propagation phase of the algorithm and use them as
witnesses for why the respective clauses could not be pushed forward. It only
makes sense to recheck a particular clause for pushing when its witnessing
model falsifies a newly added clause. Since this trigger test is both
computationally cheap and sufficiently precise, we can afford to keep clauses
pushed as far as possible at all times. Experiments indicate that this strategy
considerably improves IC3's performance."
"Separation logics are a family of extensions of Hoare logic for reasoning
about programs that mutate memory. These logics are ""abstract"" because they are
independent of any particular concrete memory model. Their assertion languages,
called propositional abstract separation logics, extend the logic of (Boolean)
Bunched Implications (BBI) in various ways.
  We develop a modular proof theory for various propositional abstract
separation logics using cut-free labelled sequent calculi. We first extend the
cut-fee labelled sequent calculus for BBI of Hou et al to handle Calcagno et
al's original logic of separation algebras by adding sound rules for
partial-determinism and cancellativity, while preserving cut-elimination. We
prove the completeness of our calculus via a sound intermediate calculus that
enables us to construct counter-models from the failure to find a proof. We
then capture other propositional abstract separation logics by adding sound
rules for indivisible unit and disjointness, while maintaining completeness. We
present a theorem prover based on our labelled calculus for these propositional
abstract separation logics."
"In program synthesis, we transform a specification into a program that is
guaranteed to satisfy the specification. In synthesis of reactive systems, the
environment in which the program operates may behave nondeterministically,
e.g., by generating different sequences of inputs in different runs of the
system. To satisfy the specification, the program needs to act so that the
specification holds in every computation generated by its interaction with the
environment. Often, the program cannot observe all attributes of its
environment. In this case, we should transform a specification into a program
whose behavior depends only on the observable history of the computation. This
is called synthesis with incomplete information. In such a setting, it is
desirable to have a knowledge-based specification, which can refer to the
uncertainty the program has about the environment's behavior. In this work we
solve the problem of synthesis with incomplete information with respect to
specifications in the logic of knowledge and time. We show that the problem has
the same worst-case complexity as synthesis with complete information."
"Linearisability has become the standard correctness criterion for concurrent
data structures, ensuring that every history of invocations and responses of
concurrent operations has a matching sequential history. Existing proofs of
linearisability require one to identify so-called linearisation points within
the operations under consideration, which are atomic statements whose execution
causes the effect of an operation to be felt. However, identification of
linearisation points is a non-trivial task, requiring a high degree of
expertise. For sophisticated algorithms such as Heller et al's lazy set, it
even is possible for an operation to be linearised by the concurrent execution
of a statement outside the operation being verified. This paper proposes an
alternative method for verifying linearisability that does not require
identification of linearisation points. Instead, using an interval-based logic,
we show that every behaviour of each concrete operation over any interval is a
possible behaviour of a corresponding abstraction that executes with
coarse-grained atomicity. This approach is applied to Heller et al's lazy set
to show that verification of linearisability is possible without having to
consider linearisation points within the program code."
"The concurrent logical framework CLF is an extension of the logical framework
LF designed to specify concurrent and distributed languages. While it can be
used to define a variety of formalisms, reasoning about such languages within
CLF has proved elusive. In this paper, we propose an extension of LF that
allows us to express properties of CLF specifications. We illustrate the
approach with a proof of safety for a small language with a parallel semantics."
"Event Structures (ESs) are mainly concerned with the representation of causal
relationships between events, usually accompanied by other event relations
capturing conflicts and disabling. Among the most prominent variants of ESs are
Prime ESs, Bundle ESs, Stable ESs, and Dual ESs, which differ in their
causality models and event relations. Yet, some application domains require
further kinds of relations between events. Here, we add the possibility to
express priority relationships among events.
  We exemplify our approach on Prime, Bundle, Extended Bundle, and Dual ESs.
Technically, we enhance these variants in the same way. For each variant, we
then study the interference between priority and the other event relations.
From this, we extract the redundant priority pairs-notably differing for the
types of ESs-that enable us to provide a comparison between the extensions. We
also exhibit that priority considerably complicates the definition of partial
orders in ESs."
"Probabilistic transition system specifications using the rule format
ntmuft-ntmuxt provide structural operational semantics for Segala-type systems
and guarantee that probabilistic bisimilarity is a congruence. Probabilistic
bisimilarity is for many applications too sensitive to the exact probabilities
of transitions. Approximate bisimulation provides a robust semantics that is
stable with respect to implementation and measurement errors of probabilistic
behavior. We provide a general method to quantify how much a process combinator
expands the approximate bisimulation distance. As a direct application we
derive an appropriate rule format that guarantees compositionality with respect
to approximate bisimilarity. Moreover, we describe how specification formats
for non-standard compositionality requirements may be derived."
"Modeling a sequence of design steps, or a sequence of parameter settings,
yields a sequence of dynamical systems. In many cases, such a sequence is
intended to approximate a certain limit case. However, formally defining that
limit turns out to be subject to ambiguity. Depending on the interpretation of
the sequence, i.e. depending on how the behaviors of the systems in the
sequence are related, it may vary what the limit should be. Topologies, and in
particular metrics, define limits uniquely, if they exist. Thus they select one
interpretation implicitly and leave no room for other interpretations. In this
paper, we define limits using category theory, and use the mentioned relations
between system behaviors explicitly. This resolves the problem of ambiguity in
a more controlled way. We introduce a category of prefix orders on executions
and partial history preserving maps between them to describe both discrete and
continuous branching time dynamics. We prove that in this category all
projective limits exist, and illustrate how ambiguity in the definition of
limits is resolved using an example. Moreover, we show how various problems
with known topological approaches are now resolved, and how the construction of
projective limits enables us to approximate continuous time dynamics as a
sequence of discrete time systems."
"Adding interaction to logic programming is an essential task. Expressive
logics such as linear logic provide a theoretical basis for such a mechanism.
Unfortunately, none of the existing linear logic languages can model
interactions with the user. This is because they uses provability as the sole
basis for computation. We propose to use the game semantics instead of
provability as the basis for computation to allow for more active participation
from the user. We illustrate our idea via muprolog, an extension of Prolog with
choice-disjunctive clauses."
"The paper introduces a propositional linguistic logic that serves as the
basis for automated uncertain reasoning with linguistic information. First, we
build a linguistic logic system with truth value domain based on a linear
symmetrical hedge algebra. Then, we consider G\""{o}del's t-norm and t-conorm to
define the logical connectives for our logic. Next, we present a resolution
inference rule, in which two clauses having contradictory linguistic truth
values can be resolved. We also give the concept of reliability in order to
capture the approximative nature of the resolution inference rule. Finally, we
propose a resolution procedure with the maximal reliability."
"We provide a characterisation of strongly normalising terms of the
lambda-mu-calculus by means of a type system that uses intersection and product
types. The presence of the latter and a restricted use of the type omega enable
us to represent the particular notion of continuation used in the literature
for the definition of semantics for the lambda-mu-calculus. This makes it
possible to lift the well-known characterisation property for
strongly-normalising lambda-terms - that uses intersection types - to the
lambda-mu-calculus. From this result an alternative proof of strong
normalisation for terms typeable in Parigot's propositional logical system
follows, by means of an interpretation of that system into ours."
"Non-idempotent intersection types are used in order to give a bound of the
length of the normalization beta-reduction sequence of a lambda term: namely,
the bound is expressed as a function of the size of the term."
"This paper investigates type isomorphism in a lambda-calculus with
intersection and union types. It is known that in lambda-calculus, the
isomorphism between two types is realised by a pair of terms inverse one each
other. Notably, invertible terms are linear terms of a particular shape, called
finite hereditary permutators. Typing properties of finite hereditary
permutators are then studied in a relevant type inference system with
intersection and union types for linear terms. In particular, an isomorphism
preserving reduction between types is defined. Type reduction is confluent and
terminating, and induces a notion of normal form of types. The properties of
normal types are a crucial step toward the complete characterisation of type
isomorphism. The main results of this paper are, on one hand, the fact that two
types with the same normal form are isomorphic, on the other hand, the
characterisation of the isomorphism between types in normal form, modulo
isomorphism of arrow types."
"Description Logics (DLs) are a family of languages used for the
representation and reasoning on the knowledge of an application domain, in a
structured and formal manner. In order to achieve this objective, several
provers, such as RACER and FaCT++, have been implemented, but these provers
themselves have not been yet certified. In order to ensure the soundness of
derivations in these DLs, it is necessary to formally verify the deductions
applied by these reasoners. Formal methods offer powerful tools for the
specification and verification of proof procedures, among them there are
methods for proving properties such as soundness, completeness and termination
of a proof procedure. In this paper, we present the definition of a proof
procedure for the Description Logic ALC, based on a semantic tableau method. We
ensure validity of our prover by proving its soundness, completeness and
termination properties using Isabelle proof assistant. The proof proceeds in
two phases, first by establishing these properties on an abstract level, and
then by instantiating them for an implementation based on lists."
"Modeling and analysis of soft errors in electronic circuits has traditionally
been done using computer simulations. Computer simulations cannot guarantee
correctness of analysis because they utilize approximate real number
representations and pseudo random numbers in the analysis and thus are not well
suited for analyzing safety-critical applications. In this paper, we present a
higher-order logic theorem proving based method for modeling and analysis of
soft errors in electronic circuits. Our developed infrastructure includes
formalized continuous random variable pairs, their Cumulative Distribution
Function (CDF) properties and independent standard uniform and Gaussian random
variables. We illustrate the usefulness of our approach by modeling and
analyzing soft errors in commonly used dynamic random access memory sense
amplifier circuits."
"Largely adopted by proof assistants, the conventional induction methods based
on explicit induction schemas are non-reductive and local, at schema level. On
the other hand, the implicit induction methods used by automated theorem
provers allow for lazy and mutual induction reasoning. In this paper, we
present a new tactic for the Coq proof assistant able to perform automatically
implicit induction reasoning. By using an automatic black-box approach,
conjectures intended to be manually proved by the certifying proof environment
that integrates Coq are proved instead by the Spike implicit induction theorem
prover. The resulting proofs are translated afterwards into certified Coq
scripts."
"Additional material for the original paper ""Automated Verification of
Interactive Rule-Based Configuration Systems""."
"We develop a polynomial translation from finite control pi-calculus processes
to safe low-level Petri nets. To our knowledge, this is the first such
translation. It is natural in that there is a close correspondence between the
control flows, enjoys a bisimulation result, and is suitable for practical
model checking."
"The free algebra adjunction, between the category of algebras of a monad and
the underlying category, induces a comonad on the category of algebras. The
coalgebras of this comonad are the topic of study in this paper (following
earlier work). It is illustrated how such coalgebras-on-algebras can be
understood as bases, decomposing each element x into primitives elements from
which x can be reconstructed via the operations of the algebra. This holds in
particular for the free vector space monad, but also for other monads, like
powerset or distribution. For instance, continuous dcpos or stably continuous
frames, where each element is the join of the elements way below it, can be
described as such coalgebras. Further, it is shown how these
coalgebras-on-algebras give rise to a comonoid structure for copy and delete,
and thus to diagonalisation of endomaps like in linear algebra."
"We consider the problem of finding pre-fixed points of interactive realizers
over arbitrary knowledge spaces, obtaining a relative recursive procedure.
Knowledge spaces and interactive realizers are an abstract setting to represent
learning processes, that can interpret non-constructive proofs. Atomic pieces
of information of a knowledge space are stratified into levels, and evaluated
into truth values depending on knowledge states. Realizers are then used to
define operators that extend a given state by adding and possibly removing
atoms: in a learning process states of knowledge change nonmonotonically.
Existence of a pre-fixed point of a realizer is equivalent to the termination
of the learning process with some state of knowledge which is free of patent
contradictions and such that there is nothing to add. In this paper we
generalize our previous results in the case of level 2 knowledge spaces and
deterministic operators to the case of omega-level knowledge spaces and of
non-deterministic operators."
"We consider state-based systems modelled as coalgebras whose type
incorporates branching, and show that by suitably adapting the definition of
coalgebraic bisimulation, one obtains a general and uniform account of the
linear-time behaviour of a state in such a coalgebra. By moving away from a
boolean universe of truth values, our approach can measure the extent to which
a state in a system with branching is able to exhibit a particular linear-time
behaviour. This instantiates to measuring the probability of a specific
behaviour occurring in a probabilistic system, or measuring the minimal cost of
exhibiting a specific behaviour in the case of weighted computations."
"We propose to study proof search from a coinductive point of view. In this
paper, we consider intuitionistic logic and a focused system based on
Herbelin's LJT for the implicational fragment. We introduce a variant of lambda
calculus with potentially infinitely deep terms and a means of expressing
alternatives for the description of the ""solution spaces"" (called B\""ohm
forests), which are a representation of all (not necessarily well-founded but
still locally well-formed) proofs of a given formula (more generally: of a
given sequent).
  As main result we obtain, for each given formula, the reduction of a
coinductive definition of the solution space to a effective coinductive
description in a finitary term calculus with a formal greatest fixed-point
operator. This reduction works in a quite direct manner for the case of Horn
formulas. For the general case, the naive extension would not even be true. We
need to study ""co-contraction"" of contexts (contraction bottom-up) for dealing
with the varying contexts needed beyond the Horn fragment, and we point out the
appropriate finitary calculus, where fixed-point variables are typed with
sequents. Co-contraction enters the interpretation of the formal greatest fixed
points - curiously in the semantic interpretation of fixed-point variables and
not of the fixed-point operator."
"We introduce a new class of abstract structures, which we call generalized
ultrametric semilattices, and in which the meet operation of the semilattice
coexists with a generalized distance function in a tightly coordinated way. We
prove a constructive fixed-point theorem for strictly contracting functions on
directed-complete generalized ultrametric semilattices, and introduce a
corresponding induction principle. We cite examples of application in the
semantics of logic programming and timed computation, where, until now, the
only tool available has been the non-constructive fixed-point theorem of
Priess-Crampe and Ribenboim for strictly contracting functions on spherically
complete generalized ultrametric semilattices."
"Motivated by the recent interest in models of guarded (co-)recursion we study
its equational properties. We formulate axioms for guarded fixpoint operators
generalizing the axioms of iteration theories of Bloom and Esik. Models of
these axioms include both standard (e.g., cpo-based) models of iteration
theories and models of guarded recursion such as complete metric spaces or the
topos of trees studied by Birkedal et al. We show that the standard result on
the satisfaction of all Conway axioms by a unique dagger operation generalizes
to the guarded setting. We also introduce the notion of guarded trace operator
on a category, and we prove that guarded trace and guarded fixpoint operators
are in one-to-one correspondence. Our results are intended as first steps
leading to the description of classifying theories for guarded recursion and
hence completeness results involving our axioms of guarded fixpoint operators
in future work."
"We study the strong normalization of a new Curry-Howard correspondence for HA
+ EM1, constructive Heyting Arithmetic with the excluded middle on
Sigma01-formulas. The proof-term language of HA + EM1 consists in the lambda
calculus plus an operator ||_a which represents, from the viewpoint of
programming, an exception operator with a delimited scope, and from the
viewpoint of logic, a restricted version of the excluded middle. We give a
strong normalization proof for the system based on a technique of
""non-deterministic immersion""."
"We study fragments of first-order logic and of least fixed point logic that
allow only unary negation: negation of formulas with at most one free variable.
These logics generalize many interesting known formalisms, including modal
logic and the $\mu$-calculus, as well as conjunctive queries and monadic
Datalog. We show that satisfiability and finite satisfiability are decidable
for both fragments, and we pinpoint the complexity of satisfiability, finite
satisfiability, and model checking. We also show that the unary negation
fragment of first-order logic is model-theoretically very well behaved. In
particular, it enjoys Craig Interpolation and the Projective Beth Property."
"We study a composition operation on monads, equivalently presented as large
equational theories. Specifically, we discuss the existence of tensors, which
are combinations of theories that impose mutual commutation of the operations
from the component theories. As such, they extend the sum of two theories,
which is just their unrestrained combination. Tensors of theories arise in
several contexts; in particular, in the semantics of programming languages, the
monad transformer for global state is given by a tensor. We present two main
results: we show that the tensor of two monads need not in general exist by
presenting two counterexamples, one of them involving finite powerset (i.e. the
theory of join semilattices); this solves a somewhat long-standing open
problem, and contrasts with recent results that had ruled out previously
expected counterexamples. On the other hand, we show that tensors with bounded
powerset monads do exist from countable powerset upwards."
"This paper is concerned with the complexity analysis of constructor term
rewrite systems and its ramification in implicit computational complexity. We
introduce a path order with multiset status, the polynomial path order POP*,
that is applicable in two related, but distinct contexts. On the one hand POP*
induces polynomial innermost runtime complexity and hence may serve as a
syntactic, and fully automatable, method to analyse the innermost runtime
complexity of term rewrite systems. On the other hand POP* provides an
order-theoretic characterisation of the polytime computable functions: the
polytime computable functions are exactly the functions computable by an
orthogonal constructor TRS compatible with POP*."
"We consider partially observable Markov decision processes (POMDPs) with
{\omega}-regular conditions specified as parity objectives. The class of
{\omega}-regular languages extends regular languages to infinite strings and
provides a robust specification language to express all properties used in
verification, and parity objectives are canonical forms to express
{\omega}-regular conditions. The qualitative analysis problem given a POMDP and
a parity objective asks whether there is a strategy to ensure that the
objective is satis- fied with probability 1 (resp. positive probability). While
the qualitative analysis problems are known to be undecidable even for very
special cases of parity objectives, we establish decidability (with optimal
complexity) of the qualitative analysis problems for POMDPs with all parity
objectives under finite- memory strategies. We establish optimal (exponential)
memory bounds and EXPTIME-completeness of the qualitative analysis problems
under finite-memory strategies for POMDPs with parity objectives."
"We formalise a general concept of distributed systems as sequential
components interacting asynchronously. We define a corresponding class of Petri
nets, called LSGA nets, and precisely characterise those system specifications
which can be implemented as LSGA nets up to branching ST-bisimilarity with
explicit divergence."
"This paper proposes a new logic RoCTL* to model robustness in concurrent
systems. RoCTL* extends CTL* with the addition of Obligatory and Robustly
operators, which quantify over failure-free paths and paths with one more
failure respectively. We present a number of examples of problems to which
RoCTL* can be applied. The core result of this paper is to show that RoCTL* is
expressively equivalent to CTL* but is non-elementarily more succinct. We
present a translation from RoCTL* into CTL* that preserves truth but may result
in non-elementary growth in the length of the translated formula as each nested
Robustly operator may result in an extra exponential blowup. However, we show
that this translation is optimal in the sense that any equivalence preserving
translation will require an extra exponential growth per nested Robustly. We
also compare RoCTL* to Quantified CTL* (QCTL*) and hybrid logics."
"We propose a process calculus to model high level wireless systems, where the
topology of a network is described by a digraph. The calculus enjoys features
which are proper of wireless networks, namely broadcast communication and
probabilistic behaviour. We first focus on the problem of composing wireless
networks, then we present a compositional theory based on a probabilistic
generalisation of the well known may-testing and must-testing pre- orders.
Also, we define an extensional semantics for our calculus, which will be used
to define both simulation and deadlock simulation preorders for wireless
networks. We prove that our simulation preorder is sound with respect to the
may-testing preorder; similarly, the deadlock simulation pre- order is sound
with respect to the must-testing preorder, for a large class of networks. We
also provide a counterexample showing that completeness of the simulation
preorder, with respect to the may testing one, does not hold. We conclude the
paper with an application of our theory to probabilistic routing protocols."
"Fixpoints are an important ingredient in semantics, abstract interpretation
and program logics. Their addition to a logic can add considerable expressive
power. One general issue is how to define proof systems for such logics. Here
we examine proof systems for modal logic with fixpoints. We present a tableau
proof system for checking validity of formulas which uses names to keep track
of unfoldings of fixpoint variables as devised by Jungteerapanich."
"The aim of this paper is to provide a general overview of the product
operators introduced in the literature as a tool to enhance the analysis
accuracy in the Abstract Interpretation framework. In particular we focus on
the Cartesian and reduced products, as well as on the reduced cardinal power,
an under-used technique whose features deserve to be stressed for their
potential impact in practical applications."
"Fully automated verification of concurrent programs is a difficult problem,
primarily because of state explosion: the exponential growth of a program state
space with the number of its concurrently active components. It is natural to
apply a divide and conquer strategy to ameliorate state explosion, by analyzing
only a single component at a time. We show that this strategy leads to the
notion of a ""split"" invariant, an assertion which is globally inductive, while
being structured as the conjunction of a number of local, per-component
invariants. This formulation is closely connected to the classical Owicki-Gries
method and to Rely-Guarantee reasoning. We show how the division of an
invariant into a number of pieces with limited scope makes it possible to apply
new, localized forms of symmetry and abstraction to drastically simplify its
computation. Split invariance also has interesting connections to parametric
verification. A quantified invariant for a parametric system is a split
invariant for every instance. We show how it is possible, in some cases, to
invert this connection, and to automatically generalize from a split invariant
for a small instance of a system to a quantified invariant which holds for the
entire family of instances."
"We show that alternating Turing machines, with a novel and natural definition
of acceptance, accept precisely the inductive (Pi-1-1) languages. Total
alternating machines, that either accept or reject each input, accept precisely
the hyper-elementary (Delta-1-1) languages. Moreover, bounding the permissible
number of alternations yields a characterization of the levels of the
arithmetical hierarchy. Notably, these results use simple finite computing
devices, with finitary and discrete operational semantics, and neither the
results nor their proofs make any use of transfinite ordinals. Our
characterizations elucidate the analogy between the polynomial-time hierarchy
and the arithmetical hierarchy, as well as between their respective limits,
namely polynomial-space and Pi-1-1."
"This paper explores the semantics of a combinatory fragment of reFLect, the
lambda-calculus underlying a functional language used by Intel Corporation for
hardware design and verification. ReFLect is similar to ML, but has a primitive
data type whose elements are the abstract syntax trees of reFLect expressions
themselves. Following the LCF paradigm, this is intended to serve as the object
language of a higher-order logic theorem prover for specification and reasoning
- but one in which object- and meta-languages are unified. The aim is to
intermix program evaluation and logical deduction through reflection
mechanisms. We identify some difficulties with the semantics of reFLect as
currently defined, and propose a minimal modification of the type system that
avoids these problems."
"We propose an extension of pure type systems with an algebraic presentation
of inductive and co-inductive type families with proper indices. This type
theory supports coercions toward from smaller sorts to bigger sorts via
explicit type construction, as well as impredicative sorts. Type families in
impredicative sorts are constructed with a bracketing operation. The necessary
restrictions of pattern-matching from impredicative sorts to types are confined
to the bracketing construct. This type theory gives an alternative presentation
to the calculus of inductive constructions on which the Coq proof assistant is
an implementation."
"Partial model checking was proposed by Andersen in 1995 to verify a temporal
logic formula compositionally on a composition of processes. It consists in
incrementally incorporating into the formula the behavioural information taken
from one process - an operation called quotienting - to obtain a new formula
that can be verified on a smaller composition from which the incorporated
process has been removed. Simplifications of the formula must be applied at
each step, so as to maintain the formula at a tractable size. In this paper, we
revisit partial model checking. First, we extend quotienting to the network of
labelled transition systems model, which subsumes most parallel composition
operators, including m-among-n synchronisation and parallel composition using
synchronisation interfaces, available in the ELOTOS standard. Second, we
reformulate quotienting in terms of a simple synchronous product between a
graph representation of the formula (called formula graph) and a process, thus
enabling quotienting to be implemented efficiently and easily, by reusing
existing tools dedicated to graph compositions. Third, we propose
simplifications of the formula as a combination of bisimulations and reductions
using Boolean equation systems applied directly to the formula graph, thus
enabling formula simplifications also to be implemented efficiently. Finally,
we describe an implementation in the CADP (Construction and Analysis of
Distributed Processes) toolbox and present some experimental results in which
partial model checking uses hundreds of times less memory than on-the-fly model
checking."
"We investigate unification problems related to the Cipher Block Chaining
(CBC) mode of encryption. We first model chaining in terms of a simple,
convergent, rewrite system over a signature with two disjoint sorts: list and
element. By interpreting a particular symbol of this signature suitably, the
rewrite system can model several practical situations of interest. An inference
procedure is presented for deciding the unification problem modulo this rewrite
system. The procedure is modular in the following sense: any given problem is
handled by a system of `list-inferences', and the set of equations thus derived
between the element-terms of the problem is then handed over to any
(`black-box') procedure which is complete for solving these element-equations.
An example of application of this unification procedure is given, as attack
detection on a Needham-Schroeder like protocol, employing the CBC encryption
mode based on the associative-commutative (AC) operator XOR. The 2-sorted
convergent rewrite system is then extended into one that fully captures a block
chaining encryption-decryption mode at an abstract level, using no AC-symbols;
and unification modulo this extended system is also shown to be decidable."
"In this paper we investigate further the tableaux system for a deontic action
logic we presented in previous work. This tableaux system uses atoms (of a
given boolean algebra of action terms) as labels of formulae, this allows us to
embrace parallel execution of actions and action complement, two action
operators that may present difficulties in their treatment. One of the
restrictions of this logic is that it uses vocabularies with a finite number of
actions. In this article we prove that this restriction does not affect the
coherence of the deduction system; in other words, we prove that the system is
complete with respect to language extension. We also study the computational
complexity of this extended deductive framework and we prove that the
complexity of this system is in PSPACE, which is an improvement with respect to
related systems."
"We prove an extensionality theorem for the ""type-in-type"" dependent type
theory with Sigma-types. We suggest that the extensional equality type be
identified with the logical equivalence relation on the free term model of type
theory."
"We give a type system in which the universe of types is closed by reflection
into it of the logical relation defined externally by induction on the
structure of types. This contribution is placed in the context of the search
for a natural, syntactic construction of the extensional equality type (Tait
[1995], Altenkirch [1999], Coquand [2011], Licata and Harper [2012], Martin-Lof
[2013]). The system is presented as an extension of lambda-*, the terminal pure
type system in which the universe of all types is a type. The universe
inconsistency is then removed by the usual method of stratification into
levels. We give a set-theoretic model for the stratified system. We conjecture
that Strong Normalization holds as well."
"We define memory-efficient certificates for $\mu$-calculus model checking
problems based on the well-known correspondence of the $\mu$-calculus model
checking with winning certain parity games. Winning strategies can
independently checked, in low polynomial time, by observing that there is no
reachable strongly connected component in the graph of the parity game whose
largest priority is odd. Winning strategies are computed by fixpoint iteration
following the naive semantics of $\mu$-calculus. We instrument the usual
fixpoint iteration of $\mu$-calculus model checking so that it produces
evidence in the form of a winning strategy; these winning strategies can be
computed in polynomial time in $|S|$ and in space $O(|S|^2 |{\phi}|^2)$, where
$|S|$ is the size of the state space and $|\phi|$ the length of the formula
$\phi$\@. The main technical contribution here is the notion and algebra of
partial winning strategies. On the technical level our work can be seen as a
new, simpler, and immediate constructive proof of the correspondence between
$\mu$-calculus and parity games."
"Introduced in 2006 by Japaridze, cirquent calculus is a refinement of sequent
calculus. The advent of cirquent calculus arose from the need for a deductive
system with a more explicit ability to reason about resources. Unlike the more
traditional proof-theoretic approaches that manipulate tree-like objects
(formulas, sequents, etc.), cirquent calculus is based on circuit-style
structures called cirquents, in which different ""peer"" (sibling, cousin, etc.)
substructures may share components. It is this resource sharing mechanism to
which cirquent calculus owes its novelty (and its virtues). From its inception,
cirquent calculus has been paired with an abstract resource semantics. This
semantics allows for reasoning about the interaction between a resource
provider and a resource user, where resources are understood in the their most
general and intuitive sense. Interpreting resources in a more restricted
computational sense has made cirquent calculus instrumental in axiomatizing
various fundamental fragments of Computability Logic, a formal theory of
(interactive) computability. The so-called ""classical"" rules of cirquent
calculus, in the absence of the particularly troublesome contraction rule,
produce a sound and complete system CL5 for Computability Logic. In this paper,
we investigate the computational complexity of CL5, showing it is
$\Sigma_2^p$-complete. We also show that CL5 without the duplication rule has
polynomial size proofs and is NP-complete."
"Ramsey Theorem [6] for pairs is intuitionistically but not classically
provable: it is equivalent to a subclassical principle [2]. In this note we
show that Ramsey may be restated in an intuitionistically provable form, which
is informative (or at least without negations), and classically equivalent to
the original. With respect to previous works of the same kind, we do not use no
counterexample as in [1], [5], nor we add a new principle to the intuitionism
as in [4]. We claim that this intuitionistic version of Ramsey could be use to
replace Ramsey Theorem in the convergence proof of programs included in [3].
  [1] Gianluigi Bellin. Ramsey interpreted: a parametric version of Ramsey
Theorem. In AMS, editor, Logic and Computation: Proceedings of a Symposium held
at Carnegie Mellon University, volume 106.
  [2] Stefano Berardi, Silvia Steila, Ramsey Theorem for pairs as a classical
principle in Intuitionistic Arithmetic, Submitted to the proceedings of Types
2013 in Toulouse.
  [3] Byron Cook, Abigail See, Florian Zuleger, Ramsey vs. Lexicographic
Termination Proving, LNCS 7795, 2013, Springer Berlin Heidelberg.
  [4] Thierry Coquand. A direct proof of Ramsey Theorem.
  [5] Paulo Oliva and Thomas Powell. A Constructive Interpretation of Ramsey
Theorem via the Product of Selection Functions. CoRR, arXiv:1204.5631, 2012.
  [6] F. P. Ramsey. On a problem in formal logic. Proc. London Math. Soc.,
1930."
"A groupoid semantics is presented for systems with both logical and thermal
degrees of freedom. We apply this to a syntactic model for encryption, and
obtain an algebraic characterization of the heat produced by the encryption
function, as predicted by Landauer's principle. Our model has a linear
representation theory that reveals an underlying quantum semantics, giving for
the first time a functorial classical model for quantum teleportation and other
quantum phenomena."
"We study the synthesis problem for distributed architectures with a
parametric number of finite-state components. Parameterized specifications
arise naturally in a synthesis setting, but thus far it was unclear how to
detect realizability and how to perform synthesis in a parameterized setting.
Using a classical result from verification, we show that for a class of
specifications in indexed LTL\X, parameterized synthesis in token ring networks
is equivalent to distributed synthesis in a network consisting of a few copies
of a single process. Adapting a well-known result from distributed synthesis,
we show that the latter problem is undecidable. We describe a semi-decision
procedure for the parameterized synthesis problem in token rings, based on
bounded synthesis. We extend the approach to parameterized synthesis in
token-passing networks with arbitrary topologies, and show applicability on a
simple case study. Finally, we sketch a general framework for parameterized
synthesis based on cutoffs and other parameterized verification techniques."
"Probabilistic applicative bisimulation is a recently introduced coinductive
methodology for program equivalence in a probabilistic, higher-order, setting.
In this paper, the technique is applied to a typed, call-by-value,
lambda-calculus. Surprisingly, the obtained relation coincides with context
equivalence, contrary to what happens when call-by-name evaluation is
considered. Even more surprisingly, full-abstraction only holds in a symmetric
setting."
"Basic Parallel Processes (BPPs) are a well-known subclass of Petri Nets. They
are the simplest common model of concurrent programs that allows unbounded
spawning of processes. In the probabilistic version of BPPs, every process
generates other processes according to a probability distribution. We study the
decidability and complexity of fundamental qualitative problems over
probabilistic BPPs -- in particular reachability with probability 1 of
different classes of target sets (e.g. upward-closed sets). Our results concern
both the Markov-chain model, where processes are scheduled randomly, and the
MDP model, where processes are picked by a scheduler."
"We describe a method for inverting Gentzen's cut-elimination in classical
first-order logic. Our algorithm is based on first computign a compressed
representation of the terms present in the cut-free proof and then cut-formulas
that realize such a compression. Finally, a proof using these cut-formulas is
constructed. This method allows an exponential compression of proof length. It
can be applied to the output of automated theorem provers, which typically
produce analytic proofs. An implementation is available on the web and
described in this paper."
"We prove that the bisimulation-invariant fragment of weak monadic
second-order logic (WMSO) is equivalent to the fragment of the modal
$\mu$-calculus where the application of the least fixpoint operator $\mu
p.\varphi$ is restricted to formulas $\varphi$ that are continuous in $p$. Our
proof is automata-theoretic in nature; in particular, we introduce a class of
automata characterizing the expressive power of WMSO over tree models of
arbitrary branching degree. The transition map of these automata is defined in
terms of a logic $\mathrm{FOE}_1^\infty$ that is the extension of first-order
logic with a generalized quantifier $\exists^\infty$, where $\exists^\infty x.
\phi$ means that there are infinitely many objects satisfying $\phi$. An
important part of our work consists of a model-theoretic analysis of
$\mathrm{FOE}_1^\infty$."
"We investigate the size of first-order rewritings of conjunctive queries over
OWL 2 QL ontologies of depth 1 and 2 by means of hypergraph programs computing
Boolean functions. Both positive and negative results are obtained. Conjunctive
queries over ontologies of depth 1 have polynomial-size nonrecursive datalog
rewritings; tree-shaped queries have polynomial positive existential
rewritings; however, in the worst case, positive existential rewritings can
only be of superpolynomial size. Positive existential and nonrecursive datalog
rewritings of queries over ontologies of depth 2 suffer an exponential blowup
in the worst case, while first-order rewritings are superpolynomial unless
$\text{NP} \subseteq \text{P}/\text{poly}$. We also analyse rewritings of
tree-shaped queries over arbitrary ontologies and observe that the query
entailment problem for such queries is fixed-parameter tractable."
"Two new logics for verification of hyperproperties are proposed.
Hyperproperties characterize security policies, such as noninterference, as a
property of sets of computation paths. Standard temporal logics such as LTL,
CTL, and CTL* can refer only to a single path at a time, hence cannot express
many hyperproperties of interest. The logics proposed here, HyperLTL and
HyperCTL*, add explicit and simultaneous quantification over multiple paths to
LTL and to CTL*. This kind of quantification enables expression of
hyperproperties. A model checking algorithm for the proposed logics is given.
For a fragment of HyperLTL, a prototype model checker has been implemented."
"This paper presents a construction which transforms categorical models of
additive-free propositional linear logic, closely based on de Paiva's
dialectica categories and Oliva's functional interpretations of classical
linear logic. The construction is defined using dependent type theory, which
proves to be a useful tool for reasoning about dialectica categories.
Abstractly, we have a closure operator on the class of models: it preserves
soundness and completeness and has a monad-like structure. When applied to
categories of games we obtain `games with bidding', which are hybrids of
dialectica and game models, and we prove completeness theorems for two specific
such models."
"In order to deal with the systematic verification with uncertain infromation
in possibility theory, Li and Li \cite{li12} introduced model checking of
linear-time properties in which the uncertainty is modeled by possibility
measures. Xue, Lei and Li \cite{Xue09} defined computation tree logic (CTL)
based on possibility measures, which is called possibilistic CTL (PoCTL). This
paper is a continuation of the above work. First, we study the expressiveness
of PoCTL. Unlike probabilistic CTL, it is shown that PoCTL (in particular,
qualitative PoCTL) is more powerful than CTL with respect to their
expressiveness. The equivalent expressions of basic CTL formulae using
qualitative PoCTL formulae are presented in detail. Some PoCTL formulae that
can not be expressed by any CTL formulae are presented. In particular, some
qualitative properties of repeated reachability and persistence are expressed
using PoCTL formulae. Next, adapting CTL model-checking algorithm, a method to
solve the PoCTL model-checking problem and its time complexity are discussed in
detail. Finally, an example is given to illustrate the PoCTL model-checking
method."
"Axioms are presented which encapsulate the properties satisfied by categories
of games which form the basis of results on full abstraction for PCF and other
programming languages, and on full completeness for various logics and type
theories.
  Axioms are presented on models of PCF from which full abstraction can be
proved. These axioms have been distilled from recent results on definability
and full abstraction of game semantics for a number of programming languages.
Full completeness for pure simply-typed $\lambda$-calculus is also axiomatized."
"Process algebra has been successful in many ways; but we don't yet see the
lineaments of a fundamental theory. Some fleeting glimpses are sought from
Petri Nets, physics and geometry."
"We use traced monoidal categories to give a precise general version of
""geometry of interaction"". We give a number of examples of both
""particle-style"" and ""wave-style"" instances of this construction. We relate
these ideas to semantics of computation."
"It is shown that for any fixed $i>0$, the $\Sigma_{i+1}$-fragment of
Presburger arithmetic, i.e., its restriction to $i+1$ quantifier alternations
beginning with an existential quantifier, is complete for
$\mathsf{\Sigma}^{\mathsf{EXP}}_{i}$, the $i$-th level of the weak EXP
hierarchy, an analogue to the polynomial-time hierarchy residing between
$\mathsf{NEXP}$ and $\mathsf{EXPSPACE}$. This result completes the
computational complexity landscape for Presburger arithmetic, a line of
research which dates back to the seminal work by Fischer & Rabin in 1974.
Moreover, we apply some of the techniques developed in the proof of the lower
bound in order to establish bounds on sets of naturals definable in the
$\Sigma_1$-fragment of Presburger arithmetic: given a $\Sigma_1$-formula
$\Phi(x)$, it is shown that the set of non-negative solutions is an ultimately
periodic set whose period is at most doubly-exponential and that this bound is
tight."
"We introduce a semantic approach to the study of logics for access control
and dependency analysis, based on Game Semantics. We use a variant of AJM games
with explicit justification (but without pointers). Based on this, we give a
simple and intuitive model of the information flow constraints underlying
access control. This is used to give strikingly simple proofs of
\emph{non-interference theorems} in robust, semantic versions."
"We present a new method for the constraint-based synthesis of termination
arguments for linear loop programs based on linear ranking templates. Linear
ranking templates are parametrized, well-founded relations such that an
assignment to the parameters gives rise to a ranking function. This approach
generalizes existing methods and enables us to use templates for many different
ranking functions with affine-linear components. We discuss templates for
multiphase, piecewise, and lexicographic ranking functions. Because these
ranking templates require both strict and non-strict inequalities, we use
Motzkin's Transposition Theorem instead of Farkas Lemma to transform the
generated $\exists\forall$-constraint into an $\exists$-constraint."
"The general setting of this work is the constraint-based synthesis of
termination arguments. We consider a restricted class of programs called lasso
programs. The termination argument for a lasso program is a pair of a ranking
function and an invariant. We present the---to the best of our
knowledge---first method to synthesize termination arguments for lasso programs
that uses linear arithmetic. We prove a completeness theorem. The completeness
theorem establishes that, even though we use only linear (as opposed to
non-linear) constraint solving, we are able to compute termination arguments in
several interesting cases. The key to our method lies in a constraint
transformation that replaces a disjunction by a sum."
"The scope of this work is the constraint-based synthesis of termination
arguments for the restricted class of programs called linear lasso programs. A
termination argument consists of a ranking function as well as a set of
supporting invariants.
  We extend existing methods in several ways. First, we use Motzkin's
Transposition Theorem instead of Farkas' Lemma. This allows us to consider
linear lasso programs that can additionally contain strict inequalities.
Existing methods are restricted to non-strict inequalities and equalities.
  Second, we consider several kinds of ranking functions: affine-linear,
piecewise and lexicographic ranking functions. Moreover, we present a novel
kind of ranking function called multiphase ranking function which proceeds
through a fixed number of phases such that for each phase, there is an
affine-linear ranking function. As an abstraction to the synthesis of specific
ranking functions, we introduce the notion ranking function template. This
enables us to handle all ranking functions in a unified way.
  Our method relies on non-linear algebraic constraint solving as a subroutine
which is known to scale poorly to large problems. As a mitigation we formalize
an assessment of the difficulty of our constraints and present an argument why
they are of an easier kind than general non-linear constraints.
  We prove our method to be complete: if there is a termination argument of the
form specified by the given ranking function template with a fixed number of
affine-linear supporting invariants, then our method will find a termination
argument.
  To our knowledge, the approach we propose is the most powerful technique of
synthesis-based discovery of termination arguments for linear lasso programs
and encompasses and enhances several methods having been proposed thus far."
"We investigate the complexity of modal satisfiability for certain
combinations of modal logics. In particular we examine four examples of
multimodal logics with dependencies and demonstrate that even if we restrict
our inputs to diamond-free formulas (in negation normal form), these logics
still have a high complexity. This result illustrates that having D as one or
more of the combined logics, as well as the interdependencies among logics can
be important sources of complexity even in the absence of diamonds and even
when at the same time in our formulas we allow only one propositional variable.
We then further investigate and characterize the complexity of the
diamond-free, 1-variable fragments of multimodal logics in a general setting."
"With the technology of the time, Kowalski's seminal 1974 paper {\em Predicate
Logic as a Programming Language} was a breakthrough for the use of logic in
computer science. It introduced two fundamental ideas: on the declarative side,
the use of the Horn clause logic fragment of classical logic, which was soon
extended with negation as failure, on the procedural side the procedural
interpretation which made it possible to write algorithms in the formalism.
  Since then, strong progress was made both on the declarative understanding of
the logic programming formalism and in automated reasoning technologies,
particularly in SAT solving, Constraint Programming and Answer Set Programming.
This has paved the way for the development of an extension of logic programming
that embodies a more pure view of logic as a modelling language and its role
for problem solving.
  In this paper, we present the \idp language and system. The language is
essentially classical logic extended with one of logic programmings most
important contributions to knowledge representation: the representation of
complex definitions as rule sets under well-founded semantics. The system is a
knowledge base system: a system in which complex declarative information is
stored in a knowledge base which can be used to solve different computational
problems by applying multiple forms of inference. In this view, theories are
declarative modellings, bags of information, descriptions of possible states of
affairs. They are neither procedures nor descriptions of computational
problems. As such, the \idp language and system preserve the fundamental idea
of a declarative reading of logic programs, while they break with the
fundamental idea of the procedural interpretation of logic programs."
"While reasoning in a logic extending a complete Boolean basis is coNP-hard,
restricting to conjunctive fragments of modal languages sometimes allows for
tractable reasoning even in the presence of greatest fixpoints. One such
example is the EL family of description logics; here, efficient reasoning is
based on satisfaction checking in suitable small models that characterize
formulas in terms of simulations. It is well-known, though, that not every
conjunctive modal language has a tractable reasoning problem. Natural questions
are then how common such tractable fragments are and how to identify them. In
this work we provide sufficient conditions for tractability in a general way by
considering unlabeled tableau rules for a given modal logic. We work in the
framework of coalgebraic logic as a unifying semantic setting. Apart from
recovering known results for description logics such as EL and FL0, we obtain
new ones for conjunctive fragments of relational and non-relational modal
logics with greatest fixpoints. Most notably we find tractable fragments of
game logic and the alternating-time mu-calculus."
"We study the complexity of reachability problems on branching extensions of
vector addition systems, which allows us to derive new non-elementary
complexity bounds for fragments and variants of propositional linear logic. We
show that provability in the multiplicative exponential fragment is Tower-hard
already in the affine case---and hence non-elementary. We match this lower
bound for the full propositional affine linear logic, proving its
Tower-completeness. We also show that provability in propositional contractive
linear logic is Ackermann-complete."
"This paper presents a formal characterisation of safety and liveness
properties \`a la Alpern and Schneider for fully probabilistic systems. As for
the classical setting, it is established that any (probabilistic tree) property
is equivalent to a conjunction of a safety and liveness property. A simple
algorithm is provided to obtain such property decomposition for flat
probabilistic CTL (PCTL). A safe fragment of PCTL is identified that provides a
sound and complete characterisation of safety properties. For liveness
properties, we provide two PCTL fragments, a sound and a complete one. We show
that safety properties only have finite counterexamples, whereas liveness
properties have none. We compare our characterisation for qualitative
properties with the one for branching time properties by Manolios and Trefler,
and present sound and complete PCTL fragments for characterising the notions of
strong safety and absolute liveness coined by Sistla."
"We investigate some finitely-valued generalizations of propositional dynamic
logic with tests. We start by introducing the (n+1)-valued Kripke models and a
corresponding language based on a modal extension of {\L}ukasiewicz many-valued
logic. We illustrate the definitions by providing a framework for an analysis
of the R\'enyi - Ulam searching game with errors.
  Our main result is the axiomatization of the theory of the (n+1)-valued
Kripke models. This result is obtained through filtration of the canonical
model of the smallest (n+1)-valued propositional dynamic logic."
"Staton has shown that there is an equivalence between the category of
presheaves on (the opposite of) finite sets and partial bijections and the
category of nominal restriction sets: see [2, Exercise 9.7]. The aim here is to
see that this extends to an equivalence between the category of cubical sets
introduced in [1] and a category of nominal sets equipped with a
""01-substitution"" operation. It seems to me that presenting the topos in
question equivalently as 01-substitution sets rather than cubical sets will
make it easier (and more elegant) to carry out the constructions and
calculations needed to build the intended univalent model of intentional
constructive type theory."
"In 1979 Richard Statman proved, using proof-theory, that the purely
implicational fragment of Intuitionistic Logic (M-imply) is PSPACE-complete. He
showed a polynomially bounded translation from full Intuitionistic
Propositional Logic into its implicational fragment. By the PSPACE-completeness
of S4, proved by Ladner, and the Goedel translation from S4 into Intuitionistic
Logic, the PSPACE- completeness of M-imply is drawn. The sub-formula principle
for a deductive system for a logic L states that whenever F1,...,Fk proves A,
there is a proof in which each formula occurrence is either a sub-formula of A
or of some of Fi. In this work we extend Statman result and show that any
propositional (possibly modal) structural logic satisfying a particular
formulation of the sub-formula principle is in PSPACE. If the logic includes
the minimal purely implicational logic then it is PSPACE-complete. As a
consequence, EXPTIME-complete propositional logics, such as PDL and the
common-knowledge epistemic logic with at least 2 agents satisfy this particular
sub-formula principle, if and only if, PSPACE=EXPTIME. We also show how our
technique can be used to prove that any finitely many-valued logic has the set
of its tautologies in PSPACE."
"This article surveys some of the recent work in verification of temporal
epistemic logic via symbolic model checking, focusing on OBDD-based and
SAT-based approaches for epistemic logics built on discrete and real-time
branching time temporal logics."
"A number of algorithms for computing the simulation preorder are available.
Let Sigma denote the state space, -> the transition relation and Psim the
partition of Sigma induced by simulation equivalence. The algorithms by
Henzinger, Henzinger, Kopke and by Bloom and Paige run in O(|Sigma||->|)-time
and, as far as time-complexity is concerned, they are the best available
algorithms. However, these algorithms have the drawback of a space complexity
that is more than quadratic in the size of the state space. The algorithm by
Gentilini, Piazza, Policriti--subsequently corrected by van Glabbeek and
Ploeger--appears to provide the best compromise between time and space
complexity. Gentilini et al.'s algorithm runs in O(|Psim|^2|->|)-time while the
space complexity is in O(|Psim|^2 + |Sigma|log|Psim|). We present here a new
efficient simulation algorithm that is obtained as a modification of Henzinger
et al.'s algorithm and whose correctness is based on some techniques used in
applications of abstract interpretation to model checking. Our algorithm runs
in O(|Psim||->|)-time and O(|Psim||Sigma|log|Sigma|)-space. Thus, this
algorithm improves the best known time bound while retaining an acceptable
space complexity that is in general less than quadratic in the size of the
state space. An experimental evaluation showed good comparative results with
respect to Henzinger, Henzinger and Kopke's algorithm."
"Weak affine light typing (WALT) assigns light affine linear formulae as types
to a subset of lambda-terms of System F. WALT is poly-time sound: if a
lambda-term M has type in WALT, M can be evaluated with a polynomial cost in
the dimension of the derivation that gives it a type. The evaluation proceeds
under any strategy of a rewriting relation which is a mix of both call-by-name
and call-by-value beta-reductions. WALT weakens, namely generalizes, the notion
of ""stratification of deductions"", common to some Light Systems -- those
logical systems, derived from Linear logic, to characterize the set of
Polynomial functions -- . A weaker stratification allows to define a
compositional embedding of Safe recursion on notation (SRN) into WALT. It turns
out that the expressivity of WALT is strictly stronger than the one of the
known Light Systems. The embedding passes through the representation of a
subsystem of SRN. It is obtained by restricting the composition scheme of SRN
to one that can only use its safe variables linearly. On one side, this
suggests that SRN, in fact, can be redefined in terms of more primitive
constructs. On the other, the embedding of SRN into WALT enjoys the two
following remarkable aspects. Every datatype, required by the embedding, is
represented from scratch, showing the strong structural proof-theoretical roots
of WALT. Moreover, the embedding highlights a stratification structure of the
normal and safe arguments, normally hidden inside the world of SRN-normal/safe
variables: the less an argument is ""polyomially impredicative"", the deeper, in
a formal, proof-theoretical sense, it is represented inside WALT. Finally,
since WALT is SRN-complete it is also polynomial-time complete since SRN is."
"LF is a dependent type theory in which many other formal systems can be
conveniently embedded. However, correct use of LF relies on nontrivial
metatheoretic developments such as proofs of correctness of decision procedures
for LF's judgments. Although detailed informal proofs of these properties have
been published, they have not been formally verified in a theorem prover. We
have formalized these properties within Isabelle/HOL using the Nominal Datatype
Package, closely following a recent article by Harper and Pfenning. In the
process, we identified and resolved a gap in one of the proofs and a small
number of minor lacunae in others. We also formally derive a version of the
type checking algorithm from which Isabelle/HOL can generate executable code.
Besides its intrinsic interest, our formalization provides a foundation for
studying the adequacy of LF encodings, the correctness of Twelf-style
metatheoretic reasoning, and the metatheory of extensions to LF."
"We describe a type system for a synchronous pi-calculus formalising the
notion of affine usage in signal-based communication. In particular, we
identify a limited number of usages that preserve affinity and that can be
composed. As a main application of the resulting system, we show that typable
programs are deterministic."
"We present the definition of the logical framework TF, the Type Framework. TF
is a lambda-free logical framework; it does not include lambda-abstraction or
product kinds. We give formal proofs of several results in the metatheory of
TF, and show how it can be conservatively embedded in the logical framework LF:
its judgements can be seen as the judgements of LF that are in beta-normal,
eta-long normal form. We show how several properties, such as adequacy theorems
for object theories and the injectivity of constants, can be proven more easily
in TF, and then `lifted' to LF."
"This paper presents simple, syntactic strong normalization proofs for the
simply-typed lambda-calculus and the polymorphic lambda-calculus (system F)
with the full set of logical connectives, and all the permutative reductions.
The normalization proofs use translations of terms and types to systems, for
which strong normalization property is known."
"Tree automata with one memory have been introduced in 2001. They generalize
both pushdown (word) automata and the tree automata with constraints of
equality between brothers of Bogaert and Tison. Though it has a decidable
emptiness problem, the main weakness of this model is its lack of good closure
properties.
  We propose a generalization of the visibly pushdown automata of Alur and
Madhusudan to a family of tree recognizers which carry along their (bottom-up)
computation an auxiliary unbounded memory with a tree structure (instead of a
symbol stack). In other words, these recognizers, called Visibly Tree Automata
with Memory (VTAM) define a subclass of tree automata with one memory enjoying
Boolean closure properties. We show in particular that they can be determinized
and the problems like emptiness, membership, inclusion and universality are
decidable for VTAM. Moreover, we propose several extensions of VTAM whose
transitions may be constrained by different kinds of tests between memories and
also constraints a la Bogaert and Tison comparing brother subtrees in the tree
in input. We show that some of these classes of constrained VTAM keep the good
closure and decidability properties, and we demonstrate their expressiveness
with relevant examples of tree languages."
"A web service is modeled here as a finite state machine. A composition
problem for web services is to decide if a given web service can be constructed
from a given set of web services; where the construction is understood as a
simulation of the specification by a fully asynchronous product of the given
services. We show an EXPTIME-lower bound for this problem, thus matching the
known upper bound. Our result also applies to richer models of web services,
such as the Roman model."
"This is a set of lecture notes that developed out of courses on the lambda
calculus that I taught at the University of Ottawa in 2001 and at Dalhousie
University in 2007 and 2013. Topics covered in these notes include the untyped
lambda calculus, the Church-Rosser theorem, combinatory algebras, the
simply-typed lambda calculus, the Curry-Howard isomorphism, weak and strong
normalization, polymorphism, type inference, denotational semantics, complete
partial orders, and the language PCF."
"We investigate here a new version of the Calculus of Inductive Constructions
(CIC) on which the proof assistant Coq is based: the Calculus of Congruent
Inductive Constructions, which truly extends CIC by building in arbitrary
first-order decision procedures: deduction is still in charge of the CIC
kernel, while computation is outsourced to dedicated first-order decision
procedures that can be taken from the shelves provided they deliver a proof
certificate. The soundness of the whole system becomes an incremental property
following from the soundness of the certificate checkers and that of the
kernel. A detailed example shows that the resulting style of proofs becomes
closer to that of the working mathematician."
"This article introduces a fully automated verification technique that permits
to analyze real-time systems described using a continuous notion of time and a
mixture of operational (i.e., automata-based) and descriptive (i.e.,
logic-based) formalisms. The technique relies on the reduction, under
reasonable assumptions, of the continuous-time verification problem to its
discrete-time counterpart. This reconciles in a viable and effective way the
dense/discrete and operational/descriptive dichotomies that are often
encountered in practice when it comes to specifying and analyzing complex
critical systems. The article investigates the applicability of the technique
through a significant example centered on a communication protocol. More
precisely, concurrent runs of the protocol are formalized by parallel instances
of a Timed Automaton, while the synchronization rules between these instances
are specified through Metric Temporal Logic formulas, thus creating a
multi-paradigm model. Verification tests run on this model using a bounded
validity checker implementing the technique show consistent results and
interesting performances."
"We introduce an algebra of data linkages. Data linkages are intended for
modelling the states of computations in which dynamic data structures are
involved. We present a simple model of computation in which states of
computations are modelled as data linkages and state changes take place by
means of certain actions. We describe the state changes and replies that result
from performing those actions by means of a term rewriting system with rule
priorities. The model in question is an upgrade of molecular dynamics. The
upgrading is mainly concerned with the features to deal with values and the
features to reclaim garbage."
"Separation logic is a recent extension of Hoare logic for reasoning about
programs with references to shared mutable data structures. In this paper, we
provide a new interpretation of the logic for a programming language with
higher types. Our interpretation is based on Reynolds's relational
parametricity, and it provides a formal connection between separation logic and
data abstraction."
"We argue that the language of Zermelo Fraenkel set theory with definitions
and partial functions provides the most promising bedrock semantics for
communicating and sharing mathematical knowledge. We then describe a syntactic
sugaring of that language that provides a way of writing remarkably readable
assertions without straying far from the set-theoretic semantics. We illustrate
with some examples of formalized textbook definitions from elementary set
theory and point-set topology. We also present statistics concerning the
complexity of these definitions, under various complexity measures."
"We consider games played on graphs with the winning conditions for the
players specified as weak-parity conditions. In weak-parity conditions the
winner of a play is decided by looking into the set of states appearing in the
play, rather than the set of states appearing infinitely often in the play. A
naive analysis of the classical algorithm for weak-parity games yields a
quadratic time algorithm. We present a linear time algorithm for solving
weak-parity games."
"In this paper it is shown that no public announcement scheme that can be
modeled in Dynamic Epistemic Logic (DEL) can solve the Russian Cards Problem
(RCP) in one announcement. Since DEL is a general model for any public
announcement scheme we conclude that there exist no single announcement
solution to the RCP. The proof demonstrates the utility of DEL in proving lower
bounds for communication protocols. It is also shown that a general version of
RCP has no two announcement solution when the adversary has sufficiently large
number of cards."
"We present a construction of a certain infinite complete partial order (CPO)
that differs from the standard construction used in Scott's denotational
semantics. In addition, we construct several other infinite CPO's. For some of
those, we apply the usual Fixed Point Theorem (FPT) to yield a fixed point for
every continuous function $\mu:2\to 2$ (where 2 denotes the set $\{0,1\}$),
while for the other CPO's we cannot invoke that theorem to yield such fixed
points. Every element of each of these CPO's is a binary string in the
monotypic form and we show that invalidation of the applicability of the FPT to
the CPO that Scott's constructed yields the concept of replication."
"We add commutativity to axioms defining mnesors and substitute a bitrop for
the lattice. We show that it can be applied to relational database querying:
set union, intersection and selection are redifined only from the mnesor
addition and the granular multiplication. Union-compatibility is not required."
"We specify the operational semantics and bisimulation relations for the
finite pi-calculus within a logic that contains the nabla quantifier for
encoding generic judgments and definitions for encoding fixed points. Since we
restrict to the finite case, the ability of the logic to unfold fixed points
allows this logic to be complete for both the inductive nature of operational
semantics and the coinductive nature of bisimulation. The nabla quantifier
helps with the delicate issues surrounding the scope of variables within
pi-calculus expressions and their executions (proofs). We illustrate several
merits of the logical specifications permitted by this logic: they are natural
and declarative; they contain no side-conditions concerning names of variables
while maintaining a completely formal treatment of such variables; differences
between late and open bisimulation relations arise from familar logic
distinctions; the interplay between the three quantifiers (for all, exists, and
nabla) and their scopes can explain the differences between early and late
bisimulation and between various modal operators based on bound input and
output actions; and proof search involving the application of inference rules,
unification, and backtracking can provide complete proof systems for one-step
transitions, bisimulation, and satisfaction in modal logic. We also illustrate
how one can encode the pi-calculus with replications, in an extended logic with
induction and co-induction."
"As systems become ever more complex, verification becomes more main stream.
Event-B and Alloy are two formal specification languages based on fairly
different methodologies. While Event-B uses theorem provers to prove that
invariants hold for a given specification, Alloy uses a SAT-based model finder.
In some settings, Event-B invariants may not be proved automatically, and so
the often difficult step of interactive proof is required. One solution for
this problem is to validate invariants with model checking. This work studies
the encoding of Event-B machines and contexts to Alloy in order to perform
temporal model checking with Alloy's SAT-based engine."
"We investigate the applicability of divisible residuated lattices (DRLs) as a
general evaluation framework for soft constraint satisfaction problems (soft
CSPs). DRLs are in fact natural candidates for this role, since they form the
algebraic semantics of a large family of substructural and fuzzy logics.
  We present the following results. (i) We show that DRLs subsume important
valuation structures for soft constraints, such as commutative idempotent
semirings and fair valuation structures, in the sense that the last two are
members of certain subvarieties of DRLs (namely, Heyting algebras and
BL-algebras respectively). (ii) In the spirit of previous work of J. Larrosa
and T. Schiex [2004], and S. Bistarelli and F. Gadducci [2006] we describe a
polynomial-time algorithm that enforces k-hyperarc consistency on soft CSPs
evaluated over DRLs. Observed that, in general, DRLs are neither idempotent nor
totally ordered, this algorithm amounts to a generalization of the available
algorithms that enforce k-hyperarc consistency."
"The model checking problem for open systems has been intensively studied in
the literature, for both finite-state (module checking) and infinite-state
(pushdown module checking) systems, with respect to Ctl and Ctl*. In this
paper, we further investigate this problem with respect to the \mu-calculus
enriched with nominals and graded modalities (hybrid graded Mu-calculus), in
both the finite-state and infinite-state settings. Using an automata-theoretic
approach, we show that hybrid graded \mu-calculus module checking is solvable
in exponential time, while hybrid graded \mu-calculus pushdown module checking
is solvable in double-exponential time. These results are also tight since they
match the known lower bounds for Ctl. We also investigate the module checking
problem with respect to the hybrid graded \mu-calculus enriched with inverse
programs (Fully enriched \mu-calculus): by showing a reduction from the domino
problem, we show its undecidability. We conclude with a short overview of the
model checking problem for the Fully enriched Mu-calculus and the fragments
obtained by dropping at least one of the additional constructs."
"Propositional canonical Gentzen-type systems, introduced in 2001 by Avron and
Lev, are systems which in addition to the standard axioms and structural rules
have only logical rules in which exactly one occurrence of a connective is
introduced and no other connective is mentioned. A constructive coherence
criterion for the non-triviality of such systems was defined and it was shown
that a system of this kind admits cut-elimination iff it is coherent. The
semantics of such systems is provided using two-valued non-deterministic
matrices (2Nmatrices). In 2005 Zamansky and Avron extended these results to
systems with unary quantifiers of a very restricted form. In this paper we
substantially extend the characterization of canonical systems to (n,k)-ary
quantifiers, which bind k distinct variables and connect n formulas, and show
that the coherence criterion remains constructive for such systems. Then we
focus on the case of k&#8712;{0,1} and for a canonical calculus G show that it
is coherent precisely when it has a strongly characteristic 2Nmatrix, which in
turn is equivalent to admitting strong cut-elimination."
"We give a simple proof that the straightforward generalisation of
clique-width to arbitrary structures can be unbounded on structures of bounded
tree-width. This can be corrected by allowing fusion of elements."
"We propose a notion of convergence-sensitive bisimulation that is built just
over the notions of (internal) reduction and of (static) context. In the
framework of timed CCS, we characterise this notion of `contextual'
bisimulation via the usual labelled transition system. We also remark that it
provides a suitable semantic framework for a fully abstract embedding of
untimed processes into timed ones. Finally, we show that the notion can be
refined to include sensitivity to divergence."
"Church's Higher Order Logic is a basis for influential proof assistants --
HOL and PVS. Church's logic has a simple set-theoretic semantics, making it
trustworthy and extensible. We factor HOL into a constructive core plus axioms
of excluded middle and choice. We similarly factor standard set theory, ZFC,
into a constructive core, IZF, and axioms of excluded middle and choice. Then
we provide the standard set-theoretic semantics in such a way that the
constructive core of HOL is mapped into IZF. We use the disjunction, numerical
existence and term existence properties of IZF to provide a program extraction
capability from proofs in the constructive core.
  We can implement the disjunction and numerical existence properties in two
different ways: one using Rathjen's realizability for IZF and the other using a
new direct weak normalization result for IZF by Moczydlowski. The latter can
also be used for the term existence property."
"We define the syntax and reduction relation of a recursively typed lambda
calculus with a parallel case-function (a parallel conditional). The reduction
is shown to be confluent. We interpret the recursive types as information
systems in a restricted form, which we call prime systems. A denotational
semantics is defined with this interpretation. We define the syntactical normal
form approximations of a term and prove the Approximation Theorem: The
semantics of a term equals the limit of the semantics of its approximations.
The proof uses inclusive predicates (logical relations). The semantics is
adequate with respect to the observation of Boolean values. It is also fully
abstract in the presence of the parallel case-function."
"In this paper, we first briefly survey automated termination proof methods
for higher-order calculi. We then concentrate on the higher-order recursive
path ordering, for which we provide an improved definition, the Computability
Path Ordering. This new definition appears indeed to capture the essence of
computability arguments \`a la Tait and Girard, therefore explaining the name
of the improved ordering."
"Deficiency in expressive power of the first-order logic has led to developing
its numerous extensions by fixed point operators, such as Least Fixed-Point
(LFP), inflationary fixed-point (IFP), partial fixed-point (PFP), etc. These
logics have been extensively studied in finite model theory, database theory,
descriptive complexity. In this paper we introduce unifying framework, the
logic with iteration operator, in which iteration steps may be accessed by
temporal logic formulae. We show that proposed logic FO+TAI subsumes all
mentioned fixed point extensions as well as many other fixed point logics as
natural fragments. On the other hand we show that over finite structures FO+TAI
is no more expressive than FO+PFP. Further we show that adding the same
machinery to the logic of monotone inductions (FO+LFP) does not increase its
expressive power either."
"After examining the {\bf P} versus {\bf NP} problem against the Kleene-Rosser
paradox of the $\lambda$-calculus [94], it was found that it represents a
counter-example to NP-completeness. We prove that it contradicts the proof of
Cook's theorem. A logical formalization of the liar's paradox leads to the same
result. This formalization of the liar's paradox into a computable form is a
2-valued instance of a fuzzy logic programming paradox discovered in the system
of [90]. Three proofs that show that {\bf SAT} is (NOT) NP-complete are
presented. The counter-example classes to NP-completeness are also
counter-examples to Fagin's theorem [36] and the Immermann-Vardi theorem
[89,110], the fundamental results of descriptive complexity. All these results
show that {\bf ZF$\not$C} is inconsistent."
"Compact sets in constructive mathematics capture our intuition of what
computable subsets of the plane (or any other complete metric space) ought to
be. A good representation of compact sets provides an efficient means of
creating and displaying images with a computer. In this paper, I build upon
existing work about complete metric spaces to define compact sets as the
completion of the space of finite sets under the Hausdorff metric. This
definition allowed me to quickly develop a computer verified theory of compact
sets. I applied this theory to compute provably correct plots of uniformly
continuous functions."
"We study shedding in the setting of data linkage dynamics, a simple model of
computation that bears on the use of dynamic data structures in programming.
Shedding is complementary to garbage collection. With shedding, each time a
link to a data object is updated by a program, it is determined whether or not
the link will possibly be used once again by the program, and if not the link
is automatically removed. Thus, everything is made garbage as soon as it can be
viewed as garbage. By that, the effectiveness of garbage collection becomes
maximal."
"This paper examines the complexity of hybrid logics over transitive frames,
transitive trees, and linear frames. We show that satisfiability over
transitive frames for the hybrid language extended with the downarrow operator
is NEXPTIME-complete. This is in contrast to undecidability of satisfiability
over arbitrary frames for this language (Areces, Blackburn, Marx 1999). It is
also shown that adding the @ operator or the past modality leads to
undecidability over transitive frames. This is again in contrast to the case of
transitive trees and linear frames, where we show these languages to be
nonelementarily decidable. Moreover, we establish 2EXPTIME and EXPTIME upper
bounds for satisfiability over transitive frames and transitive trees,
respectively, for the hybrid Until/Since language. An EXPTIME lower bound is
shown to hold for the modal Until language over both frame classes."
"This paper introduces a new machine architecture for evaluating lambda
expressions using the normal-order reduction, which guarantees that every
lambda expression will be evaluated if the expression has its normal form and
the system has enough memory. The architecture considered here operates using
heap memory only. Lambda expressions are represented as graphs, and all
algorithms used in the processing unit of this machine are non-recursive."
"In this paper we prove that any lambda-term that is strongly normalising for
beta-reduction is also strongly normalising for beta,assoc-reduction. assoc is
a call-by-value rule that has been used in works by Moggi, Joachimsky, Espirito
Santo and others. The result has often been justified with incomplete or
incorrect proofs. Here we give one in full details."
"We consider two-player games played over finite state spaces for an infinite
number of rounds. At each state, the players simultaneously choose moves; the
moves determine a successor state. It is often advantageous for players to
choose probability distributions over moves, rather than single moves. Given a
goal, for example, reach a target state, the question of winning is thus a
probabilistic one: what is the maximal probability of winning from a given
state?
  On these game structures, two fundamental notions are those of equivalences
and metrics. Given a set of winning conditions, two states are equivalent if
the players can win the same games with the same probability from both states.
Metrics provide a bound on the difference in the probabilities of winning
across states, capturing a quantitative notion of state similarity.
  We introduce equivalences and metrics for two-player game structures, and we
show that they characterize the difference in probability of winning games
whose goals are expressed in the quantitative mu-calculus. The quantitative
mu-calculus can express a large set of goals, including reachability, safety,
and omega-regular properties. Thus, we claim that our relations and metrics
provide the canonical extensions to games, of the classical notion of
bisimulation for transition systems. We develop our results both for
equivalences and metrics, which generalize bisimulation, and for asymmetrical
versions, which generalize simulation."
"This work presents three increasingly expressive Dynamic Logics in which the
programs are CCS processes (sCCS-PDL, CCS-PDL and XCCS-PDL). Their goal is to
reason about properties of concurrent programs and systems described using CCS.
In order to accomplish that, CCS's operators and constructions are added to a
basic modal logic in order to create dynamic logics that are suitable for the
description and verification of properties of communicating, concurrent and
non-deterministic programs and systems, in a similar way as PDL is used for the
sequential case. We provide complete axiomatizations for the three logics.
Unlike Peleg's Concurrent PDL with Channels, our logics have a simple Kripke
semantics, complete axiomatizations and the finite model property."
"This paper describes a resolution based Description Logic reasoning system
called DLog. DLog transforms Description Logic axioms into a Prolog program and
uses the standard Prolog execution for efficiently answering instance retrieval
queries. From the Description Logic point of view, DLog is an ABox reasoning
engine for the full SHIQ language. The DLog approach makes it possible to store
the individuals in a database instead of memory, which results in better
scalability and helps using description logic ontologies directly on top of
existing information sources.
  To appear in Theory and Practice of Logic Programming (TPLP)."
"The paper introduces fuzzy linguistic logic programming, which is a
combination of fuzzy logic programming, introduced by P. Vojtas, and hedge
algebras in order to facilitate the representation and reasoning on human
knowledge expressed in natural languages. In fuzzy linguistic logic
programming, truth values are linguistic ones, e.g., VeryTrue,
VeryProbablyTrue, and LittleFalse, taken from a hedge algebra of a linguistic
truth variable, and linguistic hedges (modifiers) can be used as unary
connectives in formulae. This is motivated by the fact that humans reason
mostly in terms of linguistic terms rather than in terms of numbers, and
linguistic hedges are often used in natural languages to express different
levels of emphasis. The paper presents: (i) the language of fuzzy linguistic
logic programming; (ii) a declarative semantics in terms of Herbrand
interpretations and models; (iii) a procedural semantics which directly
manipulates linguistic terms to compute a lower bound to the truth value of a
query, and proves its soundness; (iv) a fixpoint semantics of logic programs,
and based on it, proves the completeness of the procedural semantics; (v)
several applications of fuzzy linguistic logic programming; and (vi) an idea of
implementing a system to execute fuzzy linguistic logic programs."
"Stuttering bisimulation is a well-known behavioral equivalence that preserves
CTL-X, namely CTL without the next-time operator X. Correspondingly, the
stuttering simulation preorder induces a coarser behavioral equivalence that
preserves the existential fragment ECTL-{X,G}, namely ECTL without the
next-time X and globally G operators. While stuttering bisimulation equivalence
can be computed by the well-known Groote and Vaandrager's [1990] algorithm, to
the best of our knowledge, no algorithm for computing the stuttering simulation
preorder and equivalence is available. This paper presents such an algorithm
for finite state systems."
"Type and effect systems are a tool to analyse statically the behaviour of
programs with effects. We present a proof based on the so called reducibility
candidates that a suitable stratification of the type and effect system entails
the termination of the typable programs. The proof technique covers a simply
typed, multi-threaded, call-by-value lambda-calculus, equipped with a variety
of scheduling (preemptive, cooperative) and interaction mechanisms (references,
channels, signals)."
"In this paper we investigate fair computations in the pi-calculus. Following
Costa and Stirling's approach for CCS-like languages, we consider a method to
label process actions in order to filter out unfair computations. We contrast
the existing fair-testing notion with those that naturally arise by imposing
weak and strong fairness. This comparison provides insight about the
expressiveness of the various `fair' testing semantics and about their
discriminating power."
"We present QBAL, an extension of Girard, Scedrov and Scott's bounded linear
logic. The main novelty of the system is the possibility of quantifying over
resource variables. This generalization makes bounded linear logic considerably
more flexible, while preserving soundness and completeness for polynomial time.
In particular, we provide compositional embeddings of Leivant's RRW and
Hofmann's LFPL into QBAL."
"We show that each level of the quantifier alternation hierarchy within
FO^2[<] -- the 2-variable fragment of the first order logic of order on words
-- is a variety of languages. We then use the notion of condensed rankers, a
refinement of the rankers defined by Weis and Immerman, to produce a decidable
hierarchy of varieties which is interwoven with the quantifier alternation
hierarchy -- and conjecturally equal to it. It follows that the latter
hierarchy is decidable within one unit: given a formula alpha in FO^2[<], one
can effectively compute an integer m such that alpha is equivalent to a formula
with at most m+1 alternating blocks of quantifiers, but not to a formula with
only m-1 blocks. This is a much more precise result than what is known about
the quantifier alternation hierarchy within FO[<], where no decidability result
is known beyond the very first levels."
"Inconsistency robustness is ""information system performance in the face of
continually pervasive inconsistencies."" A fundamental principle of
Inconsistency Robustness is to make contradictions explicit so that arguments
for and against propositions can be formalized. This paper explores the role of
Inconsistency Robustness in the history and theory of Logic Programs.
  Robert Kowalski put forward a bold thesis: ""Looking back on our early
discoveries, I value most the discovery that computation could be subsumed by
deduction."" However, mathematical logic cannot always infer computational steps
because computational systems make use of arbitration for determining which
message is processed next by a recipient that is sent multiple messages
concurrently. Since reception orders are in general indeterminate, they cannot
be inferred from prior information by mathematical logic alone. Therefore
mathematical logic cannot in general implement computation.
  Over the course of history, the term ""Functional Program"" has grown more
precise and technical as the field has matured. ""Logic Program"" should be on a
similar trajectory. Accordingly, ""Logic Program"" should have a general precise
characterization. In the fall of 1972, different characterizations of Logic
Programs that have continued to this day:
  * A Logic Program uses Horn-Clause syntax for forward and backward chaining
  * Each computational step (according to Actor Model) of a Logic Program is
deductively inferred (e.g. in Direct Logic).
  The above examples are illustrative of how issues of inconsistency robustness
have repeatedly arisen in Logic Programs."
"Tiwari proved that termination of linear programs (loops with linear loop
conditions and updates) over the reals is decidable through Jordan forms and
eigenvectors computation. Braverman proved that it is also decidable over the
integers. In this paper, we consider the termination of loops with polynomial
loop conditions and linear updates over the reals and integers. First, we prove
that the termination of such loops over the integers is undecidable. Second,
with an assumption, we provide an complete algorithm to decide the termination
of a class of such programs over the reals. Our method is similar to that of
Tiwari in spirit but uses different techniques. Finally, we conjecture that the
termination of linear programs with polynomial loop conditions over the reals
is undecidable in general by %constructing a loop and reducing the problem to
another decision problem related to number theory and ergodic theory, which we
guess undecidable."
"We consider a temporal logic EF+F^-1 for unranked, unordered finite trees.
The logic has two operators: EF\phi, which says ""in some proper descendant \phi
holds"", and F^-1\phi, which says ""in some proper ancestor \phi holds"". We
present an algorithm for deciding if a regular language of unranked finite
trees can be expressed in EF+F^-1. The algorithm uses a characterization
expressed in terms of forest algebras."
"In this paper we briefly summarize the contents of Manzonetto's PhD thesis
which concerns denotational semantics and equational/order theories of the pure
untyped lambda-calculus. The main research achievements include: (i) a general
construction of lambda-models from reflexive objects in (possibly
non-well-pointed) categories; (ii) a Stone-style representation theorem for
combinatory algebras; (iii) a proof that no effective lambda-model can have
lambda-beta or lambda-beta-eta as its equational theory (this can be seen as a
partial answer to an open problem introduced by Honsell-Ronchi Della Rocca in
1984)."
"Weighted automata are nondeterministic automata with numerical weights on
transitions. They can define quantitative languages $L$ that assign to each
word $w$ a real number $L(w)$. In the case of infinite words, the value of a
run is naturally computed as the maximum, limsup, liminf, limit average, or
discounted sum of the transition weights. We study expressiveness and closure
questions about these quantitative languages.
  We first show that the set of words with value greater than a threshold can
be non-$\omega$-regular for deterministic limit-average and discounted-sum
automata, while this set is always $\omega$-regular when the threshold is
isolated (i.e., some neighborhood around the threshold contains no word). In
the latter case, we prove that the $\omega$-regular language is robust against
small perturbations of the transition weights.
  We next consider automata with transition weights 0 or 1 and show that they
are as expressive as general weighted automata in the limit-average case, but
not in the discounted-sum case.
  Third, for quantitative languages $L_1$ and $L_2$, we consider the operations
$\max(L_1,L_2)$, $\min(L_1,L_2)$, and $1-L_1$, which generalize the boolean
operations on languages, as well as the sum $L_1 + L_2$. We establish the
closure properties of all classes of quantitative languages with respect to
these four operations."
"Lindstr\""om theorems characterize logics in terms of model-theoretic
conditions such as Compactness and the L\""owenheim-Skolem property. Most
existing characterizations of this kind concern extensions of first-order
logic. But on the other hand, many logics relevant to computer science are
fragments or extensions of fragments of first-order logic, e.g., k-variable
logics and various modal logics. Finding Lindstr\""om theorems for these
languages can be challenging, as most known techniques rely on coding arguments
that seem to require the full expressive power of first-order logic. In this
paper, we provide Lindstr\""om theorems for several fragments of first-order
logic, including the k-variable fragments for k>2, Tarski's relation algebra,
graded modal logic, and the binary guarded fragment. We use two different proof
techniques. One is a modification of the original Lindstr\""om proof. The other
involves the modal concepts of bisimulation, tree unraveling, and finite depth.
Our results also imply semantic preservation theorems."
"This paper is a sequel to arXiv:0902.2355 and continues the study of quantum
logic via dagger kernel categories. It develops the relation between these
categories and both orthomodular lattices and Foulis semigroups. The relation
between the latter two notions has been uncovered in the 1960s. The current
categorical perspective gives a broader context and reconstructs this
relationship between orthomodular lattices and Foulis semigroups as special
instance."
"In computer science, various logical languages are defined to analyze
properties of systems. One way to pinpoint the essential differences between
those logics is to compare their expressivity in terms of distinguishing power
and expressive power. In this paper, we study those two concepts by regarding
the latter notion as the former lifted to classes of models. We show some
general results on lifting an invariance relation on models to one on classes
of models, such that when the former corresponds to the distinguishing power of
a logic, the latter corresponds to its expressive power, given certain
compactness requirements. In particular, we introduce the notion of class
bisimulation to capture the expressive power of modal logics. We demonstrate
the application of our results by revisiting modal definability with our new
insights."
"A strong confluence result for Q*, a quantum lambda-calculus with
measurements, is proved. More precisely, confluence is shown to hold both for
finite and infinite computations. The technique used in the confluence proof is
syntactical but innovative. This makes Q* different from similar quantum lambda
calculi, which are either measurement-free or provided with a reduction
strategy."
"A combination of program algebra with the theory of meadows is designed
leading to a theory of computation in algebraic structures which use in
addition to a zero test and copying instructions the instruction set $\{x
\Leftarrow 0, x \Leftarrow 1, x\Leftarrow -x, x\Leftarrow x^{-1}, x\Leftarrow
x+y, x\Leftarrow x\cdot y\}$. It is proven that total functions on cancellation
meadows can be computed by straight-line programs using at most 5 auxiliary
variables. A similar result is obtained for signed meadows."
"This paper introduces the notion of fuzzy process as a formalism for the idea
of fuzzy contact between a device and its environment. The notions of absolute
correctness and relative correctness are defined. In order to work with
concurrency it has been built an approach to manipulate the interactive
processes as a single process and the resulted behavior has been observed."
"The paper starts from the observation on the complexity of the manipulation
of fuzzy processes that increases very rapidly with the extents of the
processes representation. Therefore, a productive approach is to divide the
problem into smaller parts, treated separately and then the results combined.
Some algebraic results obtained by the authors are presented."
"We classify the complexity of the satisfiability problem for extensions of
CTL and UB. The extensions we consider are Boolean combinations of path
formulas, fairness properties, past modalities, and forgettable past. Our main
result shows that satisfiability for CTL with all these extensions is still in
2-EXPTIME, which strongly contrasts with the nonelementary complexity of CTL*
with forgettable past. We give a complete classification of combinations of
these extensions, yielding a dichotomy between extensions with
2-EXPTIME-complete and those with EXPTIME-complete complexity. In particular,
we show that satisfiability for the extension of UB with forgettable past is
complete for 2-EXPTIME, contradicting a claim for a stronger logic in the
literature. The upper bounds are established with the help of a new kind of
pebble automata."
"The paper studies the expressivity, relative succinctness and complexity of
satisfiability for hybrid extensions of the branching-time logics CTL and CTL+
by variables. Previous complexity results show that only fragments with one
variable do have elementary complexity. It is shown that H1CTL+ and H1CTL, the
hybrid extensions with one variable of CTL+ and CTL, respectively, are
expressively equivalent but H1CTL+ is exponentially more succinct than H1CTL.
On the other hand, HCTL+, the hybrid extension of CTL with arbitrarily many
variables does not capture CTL*, as it even cannot express the simple CTL*
property EGFp. The satisfiability problem for H1CTL+ is complete for triply
exponential time, this remains true for quite weak fragments and quite strong
extensions of the logic."
"This short note contains random thoughts about a factorization theorem for
closure/interior operators on a powerset which is reminiscent to the notion of
resolution for a monad/comonad. The question originated from formal topology
but is interesting in itself. The result holds constructively (even if it
classically has several variations); but usually not predicatively (in the
sense that the interpolant will no be given by a set). For those not familiar
with predicativity issues, we look at a ``classical'' version where we bound
the size of the interpolant."
"We show some incompleteness results a la Chaitin using the busy beaver
functions. Then, with the help of ordinal logics, we show how to obtain a
theory in which the values of the busy beaver functions can be provably
established and use this to reveal a structure on the provability of the values
of these functions."
"This paper presents a non-interleaving denotational semantics for the
?-calculus. The basic idea is to define a notion of test where the outcome is
not only whether a given process passes a given test, but also in how many
different ways it can pass it. More abstractly, the set of possible outcomes
for tests forms a semiring, and the set of process interpretations appears as a
module over this semiring, in which basic syntactic constructs are affine
operators. This notion of test leads to a trace semantics in which traces are
partial orders, in the style of Mazurkiewicz traces, extended with readiness
information. Our construction has standard may- and must-testing as special
cases."
"We investigate the relationship between two independently developed
termination techniques. On the one hand, sized-types based termination (SBT)
uses types annotated with size expressions and Girard's reducibility
candidates, and applies on systems using constructor matching only. On the
other hand, semantic labelling transforms a rewrite system by annotating each
function symbol with the semantics of its arguments, and applies to any rewrite
system. First, we introduce a simplified version of SBT for the simply-typed
lambda-calculus. Then, we give new proofs of the correctness of SBT using
semantic labelling, both in the first and in the higher-order case. As a
consequence, we show that SBT can be extended to systems using matching on
defined symbols (e.g. associative functions)."
"To produce a program guaranteed to satisfy a given specification one can
synthesize it from a formal constructive proof that a computation satisfying
that specification exists. This process is particularly effective if the
specifications are written in a high-level language that makes it easy for
designers to specify their goals. We consider a high-level specification
language that results from adding knowledge to a fragment of Nuprl specifically
tailored for specifying distributed protocols, called event theory. We then
show how high-level knowledge-based programs can be synthesized from the
knowledge-based specifications using a proof development system such as Nuprl.
Methods of Halpern and Zuck then apply to convert these knowledge-based
protocols to ordinary protocols. These methods can be expressed as heuristic
transformation tactics in Nuprl."
"The problem of computing Craig Interpolants has recently received a lot of
interest. In this paper, we address the problem of efficient generation of
interpolants for some important fragments of first order logic, which are
amenable for effective decision procedures, called Satisfiability Modulo Theory
solvers.
  We make the following contributions.
  First, we provide interpolation procedures for several basic theories of
interest: the theories of linear arithmetic over the rationals, difference
logic over rationals and integers, and UTVPI over rationals and integers.
  Second, we define a novel approach to interpolate combinations of theories,
that applies to the Delayed Theory Combination approach.
  Efficiency is ensured by the fact that the proposed interpolation algorithms
extend state of the art algorithms for Satisfiability Modulo Theories. Our
experimental evaluation shows that the MathSAT SMT solver can produce
interpolants with minor overhead in search, and much more efficiently than
other competitor solvers."
"Metric LTL formulas rely on the next operator to encode time distances,
whereas qualitative LTL formulas use only the until operator. This paper shows
how to transform any metric LTL formula M into a qualitative formula Q, such
that Q is satisfiable if and only if M is satisfiable over words with
variability bounded with respect to the largest distances used in M (i.e.,
occurrences of next), but the size of Q is independent of such distances.
Besides the theoretical interest, this result can help simplify the
verification of systems with time-granularity heterogeneity, where large
distances are required to express the coarse-grain dynamics in terms of
fine-grain time units."
"Modern functional-logic programming languages like Toy or Curry feature
non-strict non-deterministic functions that behave under call-time choice
semantics. A standard formulation for this semantics is the CRWL logic, that
specifies a proof calculus for computing the set of possible results for each
expression. In this paper we present a formalization of that calculus in the
Isabelle/HOL proof assistant. We have proved some basic properties of CRWL:
closedness under c-substitutions, polarity and compositionality. We also
discuss some insights that have been gained, such as the fact that left
linearity of program rules is not needed for any of these results to hold."
"Recursive relational specifications are commonly used to describe the
computational structure of formal systems. Recent research in proof theory has
identified two features that facilitate direct, logic-based reasoning about
such descriptions: the interpretation of atomic judgments through recursive
definitions and an encoding of binding constructs via generic judgments.
However, logics encompassing these two features do not currently allow for the
definition of relations that embody dynamic aspects related to binding, a
capability needed in many reasoning tasks. We propose a new relation between
terms called nominal abstraction as a means for overcoming this deficiency. We
incorporate nominal abstraction into a rich logic also including definitions,
generic quantification, induction, and co-induction that we then prove to be
consistent. We present examples to show that this logic can provide elegant
treatments of binding contexts that appear in many proofs, such as those
establishing properties of typing calculi and of arbitrarily cascading
substitutions that play a role in reducibility arguments."
"Metric coinduction is a form of coinduction that can be used to establish
properties of objects constructed as a limit of finite approximations. One can
prove a coinduction step showing that some property is preserved by one step of
the approximation process, then automatically infer by the coinduction
principle that the property holds of the limit object. This can often be used
to avoid complicated analytic arguments involving limits and convergence,
replacing them with simpler algebraic arguments. This paper examines the
application of this principle in a variety of areas, including infinite
streams, Markov chains, Markov decision processes, and non-well-founded sets.
These results point to the usefulness of coinduction as a general proof
technique."
"We introduce an approach to detecting inconsistencies in large biological
networks by using Answer Set Programming (ASP). To this end, we build upon a
recently proposed notion of consistency between biochemical/genetic reactions
and high-throughput profiles of cell activity. We then present an approach
based on ASP to check the consistency of large-scale data sets. Moreover, we
extend this methodology to provide explanations for inconsistencies by
determining minimal representations of conflicts. In practice, this can be used
to identify unreliable data or to indicate missing reactions."
"We investigate the simulation problem in of dense-time system. A
specification simulates a model if the specification can match every transition
that the model can make at a time point. We also adapt the approach of Emerson
and Lei and allow for multiple strong and weak fairness assumptions in checking
the simulation relation. Furthermore, we allow for fairness assumptions
specified as either state-predicates or event-predicates. We focus on a
subclass of the problem with at most one fairness assumption for the
specification. We then present a simulation-checking algorithm for this
subclass. We propose simulation of a model by a specification against a common
environment. We present efficient techniques for such simulations to take the
common environment into consideration. Our experiment shows that such a
consideration can dramatically improve the efficiency of checking simulation.
We also report the performance of our algorithm in checking the liveness
properties with fairness assumptions."
"Dependently typed lambda calculi such as the Logical Framework (LF) are
capable of representing relationships between terms through types. By
exploiting the ""formulas-as-types"" notion, such calculi can also encode the
correspondence between formulas and their proofs in typing judgments. As such,
these calculi provide a natural yet powerful means for specifying varied formal
systems. Such specifications can be transformed into a more direct form that
uses predicate formulas over simply typed lambda-terms and that thereby
provides the basis for their animation using conventional logic programming
techniques. However, a naive use of this idea is fraught with inefficiencies
arising from the fact that dependently typed expressions typically contain much
redundant typing information. We investigate syntactic criteria for recognizing
and, hence, eliminating such redundancies. In particular, we identify a
property of bound variables in LF types called ""rigidity"" and formally show
that checking that instantiations of such variables adhere to typing
restrictions is unnecessary for the purpose of ensuring that the overall
expression is well-formed. We show how to exploit this property in a
translation based approach to executing specifications in the Twelf language.
Recognizing redundancy is also relevant to devising compact representations of
dependently typed expressions. We highlight this aspect of our work and discuss
its connection with other approaches proposed in this context."
"By considering probability distributions over the set of assignments the
expected truth values assignment to propositional variables are extended
through linear operators, and the expected truth values of the clauses at any
given conjunctive form are also extended through linear maps. The probabilistic
satisfiability problems are discussed in terms of the introduced linear
extensions. The case of multiple truth values is also discussed."
"Upgradeability problems are a critical issue in modern operating systems. The
problem consists in finding the ""best"" solution according to some criteria, to
install, remove or upgrade packages in a given installation. This is a
difficult problem: the complexity of the upgradeability problem is NP complete
and modern OS contain a huge number of packages (often more than 20 000
packages in a Linux distribution). Moreover, several optimisation criteria have
to be considered, e.g., stability, memory efficiency, network efficiency. In
this paper we investigate the capabilities of MILP solvers to handle this
problem. We show that MILP solvers are very efficient when the resolution is
based on a linear combination of the criteria. Experiments done on real
benchmarks show that the best MILP solvers outperform CP solvers and that they
are significantly better than Pseudo Boolean solvers."
"Managing the software complexity of package-based systems can be regarded as
one of the main challenges in software architectures. Upgrades are required on
a short time basis and systems are expected to be reliable and consistent after
that. For each package in the system, a set of dependencies and a set of
conflicts have to be taken into account. Although this problem is
computationally hard to solve, efficient tools are required. In the best
scenario, the solutions provided should also be optimal in order to better
fulfill users requirements and expectations. This paper describes two different
tools, both based on Boolean satisfiability (SAT), for solving Linux
upgradeability problems. The problem instances used in the evaluation of these
tools were mainly obtained from real environments, and are subject to two
different lexicographic optimization criteria. The developed tools can provide
optimal solutions for many of the instances, but a few challenges remain.
Moreover, it is our understanding that this problem has many similarities with
other configuration problems, and therefore the same techniques can be used in
other domains."
"We apply to the semantics of Arithmetic the idea of ``finite approximation''
used to provide computational interpretations of Herbrand's Theorem, and we
interpret classical proofs as constructive proofs (with constructive rules for
$\vee, \exists$) over a suitable structure $\StructureN$ for the language of
natural numbers and maps of G\""odel's system $\SystemT$. We introduce a new
Realizability semantics we call ``Interactive learning-based Realizability'',
for Heyting Arithmetic plus $\EM_1$ (Excluded middle axiom restricted to
$\Sigma^0_1$ formulas). Individuals of $\StructureN$ evolve with time, and
realizers may ``interact'' with them, by influencing their evolution. We build
our semantics over Avigad's fixed point result, but the same semantics may be
defined over different constructive interpretations of classical arithmetic
(Berardi and de' Liguoro use continuations). Our notion of realizability
extends intuitionistic realizability and differs from it only in the atomic
case: we interpret atomic realizers as ``learning agents''."
"We present trichotomy results characterizing the complexity of reasoning with
disjunctive logic programs. To this end, we introduce a certain definition
schema for classes of programs based on a set of allowed arities of rules. We
show that each such class of programs has a finite representation, and for each
of the classes definable in the schema we characterize the complexity of the
existence of an answer set problem. Next, we derive similar characterizations
of the complexity of skeptical and credulous reasoning with disjunctive logic
programs. Such results are of potential interest. On the one hand, they reveal
some reasons responsible for the hardness of computing answer sets. On the
other hand, they identify classes of problem instances, for which the problem
is ""easy"" (in P) or ""easier than in general"" (in NP). We obtain similar results
for the complexity of reasoning with disjunctive programs under the
supported-model semantics. To appear in Theory and Practice of Logic
Programming (TPLP)"
"This article presents the formal proof of correctness for a plane Delaunay
triangulation algorithm. It consists in repeating a sequence of edge flippings
from an initial triangulation until the Delaunay property is achieved. To
describe triangulations, we rely on a combinatorial hypermap specification
framework we have been developing for years. We embed hypermaps in the plane by
attaching coordinates to elements in a consistent way. We then describe what
are legal and illegal Delaunay edges and a flipping operation which we show
preserves hypermap, triangulation, and embedding invariants. To prove the
termination of the algorithm, we use a generic approach expressing that any
non-cyclic relation is well-founded when working on a finite set."
"Abstraction is one of the most important strategies for dealing with the
state space explosion problem in model checking. In the abstract model,
although the state space is largely reduced, however, a counterexample found in
such a model may not be a real counterexample. And the abstract model needs to
be further refined where an NP-hard state separation problem is often involved.
In this paper, a novel method is presented by adding extra variables to the
abstract model for the refinement. With this method, not only the NP-hard state
separation problem is avoided, but also a smaller refined abstract model is
obtained."
"This paper introduces the theory and practice of formal verification of
self-assembling systems. We interpret a well-studied abstraction of
nanomolecular self assembly, the Abstract Tile Assembly Model (aTAM), into
Computation Tree Logic (CTL), a temporal logic often used in model checking. We
then consider the class of ""rectilinear"" tile assembly systems. This class
includes most aTAM systems studied in the theoretical literature, and all
(algorithmic) DNA tile self-assembling systems that have been realized in
laboratories to date. We present a polynomial-time algorithm that, given a tile
assembly system T as input, either provides a counterexample to T's
rectilinearity or verifies whether T has a unique terminal assembly. Using
partial order reductions, the verification search space for this algorithm is
reduced from exponential size to O(n^2), where n x n is the size of the
assembly surface. That reduction is asymptotically the best possible. We report
on experimental results obtained by translating tile assembly simulator files
into a Petri net format manipulable by the SMART model checking engines devised
by Ciardo et al. The model checker runs in O(|T| x n^4) time, where |T| is the
number of tile types in tile assembly system T, and n x n is the surface size.
Atypical for a model checking problem -- in which the practical limit usually
is insufficient memory to store the state space -- the limit in this case was
the amount of memory required to represent the rules of the model. (Storage of
the state space and of the reachability graph were small by comparison.) We
discuss how to overcome this obstacle by means of a front end tailored to the
characteristics of self-assembly."
"In this paper, we present a systematic way of deriving (1) languages of
(generalised) regular expressions, and (2) sound and complete axiomatizations
thereof, for a wide variety of systems. This generalizes both the results of
Kleene (on regular languages and deterministic finite automata) and Milner (on
regular behaviours and finite labelled transition systems), and includes many
other systems such as Mealy and Moore machines."
"We introduce fixpoint definitions, a rule-based reformulation of fixpoint
constructs. The logic FO(FD), an extension of classical logic with fixpoint
definitions, is defined. We illustrate the relation between FO(FD) and FO(ID),
which is developed as an integration of two knowledge representation paradigms.
The satisfiability problem for FO(FD) is investigated by first reducing FO(FD)
to difference logic and then using solvers for difference logic. These
reductions are evaluated in the computation of models for FO(FD) theories
representing fairness conditions and we provide potential applications of
FO(FD)."
"We observe that the various formulations of the operational semantics of
Constraint Handling Rules proposed over the years fall into a spectrum ranging
from the analytical to the pragmatic. While existing analytical formulations
facilitate program analysis and formal proofs of program properties, they
cannot be implemented as is. We propose a novel operational semantics, which
has a strong analytical foundation, while featuring a terminating execution
model. We prove its soundness and completeness with respect to existing
analytical formulations and we provide an implementation in the form of a
source-to-source transformation to CHR with rule priorities."
"Weighted automata are nondeterministic automata with numerical weights on
transitions. They can define quantitative languages~$L$ that assign to each
word~$w$ a real number~$L(w)$. In the case of infinite words, the value of a
run is naturally computed as the maximum, limsup, liminf, limit-average, or
discounted-sum of the transition weights. The value of a word $w$ is the
supremum of the values of the runs over $w$. We study expressiveness and
closure questions about these quantitative languages. We first show that the
set of words with value greater than a threshold can be non-$\omega$-regular
for deterministic limit-average and discounted-sum automata, while this set is
always $\omega$-regular when the threshold is isolated (i.e., some neighborhood
around the threshold contains no word). In the latter case, we prove that the
$\omega$-regular language is robust against small perturbations of the
transition weights. We next consider automata with transition weights Weighted
automata are nondeterministic automata with numerical weights ontransitions.
They can define quantitative languages~$L$ that assign to eachword~$w$ a real
number~$L(w)$. In the case of infinite words, the value of arun is naturally
computed as the maximum, limsup, liminf, limit-average, ordiscounted-sum of the
transition weights. The value of a word $w$ is thesupremum of the values of the
runs over $w$. We study expressiveness andclosure questions about these
quantitative languages. We first show that the set of words with value greater
than a threshold canbe non-$\omega$-regular for deterministic limit-average and
discounted-sumautomata, while this set is always $\omega$-regular when the
threshold isisolated (i.e., some neighborhood around the threshold contains no
word). Inthe latter case, we prove that the $\omega$-regular language is robust
againstsmall perturbations of the transition weights. We next consider automata
with transition weights $0$ or $1$ and show thatthey are as expressive as
general weighted automata in the limit-average case,but not in the
discounted-sum case. Third, for quantitative languages $L_1$ and~$L_2$, we
consider the operations$\max(L_1,L_2)$, $\min(L_1,L_2)$, and $1-L_1$, which
generalize the booleanoperations on languages, as well as the sum $L_1 + L_2$.
We establish theclosure properties of all classes of quantitative languages
with respect tothese four operations.$ or $ and show that they are as
expressive as general weighted automata in the limit-average case, but not in
the discounted-sum case. Third, for quantitative languages $L_1$ and~$L_2$, we
consider the operations $\max(L_1,L_2)$, $\min(L_1,L_2)$, and -L_1$, which
generalize the boolean operations on languages, as well as the sum $L_1 + L_2$.
We establish the closure properties of all classes of quantitative languages
with respect to these four operations."
"Querying over disjunctive ASP with functions is a highly undecidable task in
general. In this paper we focus on disjunctive logic programs with stratified
negation and functions under the stable model semantics (ASP^{fs}). We show
that query answering in this setting is decidable, if the query is finitely
recursive (ASP^{fs}_{fr}). Our proof yields also an effective method for query
evaluation. It is done by extending the magic set technique to ASP^{fs}_{fr}.
We show that the magic-set rewritten program is query equivalent to the
original one (under both brave and cautious reasoning). Moreover, we prove that
the rewritten program is also finitely ground, implying that it is decidable.
Importantly, finitely ground programs are evaluable using existing ASP solvers,
making the class of ASP^{fs}_{fr} queries usable in practice."
"We present a new approach to enhancing Answer Set Programming (ASP) with
Constraint Processing techniques which allows for solving interesting
Constraint Satisfaction Problems in ASP. We show how constraints on finite
domains can be decomposed into logic programs such that unit-propagation
achieves arc, bound or range consistency. Experiments with our encodings
demonstrate their computational impact."
"A well-known result by Palamidessi tells us that {\pi}mix (the {\pi}-calculus
with mixed choice) is more expressive than {\pi}sep (its subset with only
separate choice). The proof of this result argues with their different
expressive power concerning leader election in symmetric networks. Later on,
Gorla of- fered an arguably simpler proof that, instead of leader election in
symmetric networks, employed the reducibility of ""incestual"" processes (mixed
choices that include both enabled senders and receivers for the same channel)
when running two copies in parallel. In both proofs, the role of breaking (ini-
tial) symmetries is more or less apparent. In this paper, we shed more light on
this role by re-proving the above result-based on a proper formalization of
what it means to break symmetries-without referring to another layer of the
distinguishing problem domain of leader election.
  Both Palamidessi and Gorla rephrased their results by stating that there is
no uniform and reason- able encoding from {\pi}mix into {\pi}sep . We indicate
how the respective proofs can be adapted and exhibit the consequences of
varying notions of uniformity and reasonableness. In each case, the ability to
break initial symmetries turns out to be essential."
"Terms are a concise representation of tree structures. Since they can be
naturally defined by an inductive type, they offer data structures in
functional programming and mechanised reasoning with useful principles such as
structural induction and structural recursion. However, for graphs or
""tree-like"" structures - trees involving cycles and sharing - it remains
unclear what kind of inductive structures exists and how we can faithfully
assign a term representation of them. In this paper we propose a simple term
syntax for cyclic sharing structures that admits structural induction and
recursion principles. We show that the obtained syntax is directly usable in
the functional language Haskell and the proof assistant Agda, as well as
ordinary data structures such as lists and trees. To achieve this goal, we use
a categorical approach to initial algebra semantics in a presheaf category.
That approach follows the line of Fiore, Plotkin and Turi's models of abstract
syntax with variable binding."
"We study the decidability of termination for two CHR dialects which,
similarly to the Datalog like languages, are defined by using a signature which
does not allow function symbols (of arity >0). Both languages allow the use of
the = built-in in the body of rules, thus are built on a host language that
supports unification. However each imposes one further restriction. The first
CHR dialect allows only range-restricted rules, that is, it does not allow the
use of variables in the body or in the guard of a rule if they do not appear in
the head. We show that the existence of an infinite computation is decidable
for this dialect. The second dialect instead limits the number of atoms in the
head of rules to one. We prove that in this case, the existence of a
terminating computation is decidable. These results show that both dialects are
strictly less expressive than Turing Machines. It is worth noting that the
language (without function symbols) without these restrictions is as expressive
as Turing Machines."
"We examine a bidirectional propositional dynamic logic (PDL) for finite and
infinite message sequence charts (MSCs) extending LTL and TLC-. By this kind of
multi-modal logic we can express properties both in the entire future and in
the past of an event. Path expressions strengthen the classical until operator
of temporal logic. For every formula defining an MSC language, we construct a
communicating finite-state machine (CFM) accepting the same language. The CFM
obtained has size exponential in the size of the formula. This synthesis
problem is solved in full generality, i.e., also for MSCs with unbounded
channels. The model checking problem for CFMs and HMSCs turns out to be in
PSPACE for existentially bounded MSCs. Finally, we show that, for PDL with
intersection, the semantics of a formula cannot be captured by a CFM anymore."
"Plagiarism detection is a growing need among educational institutions and
solutions for different purposes exist. An important field in this direction is
detecting cases of source-code plagiarism. In this paper, we present the tool
Kato for supporting the detection of this kind of plagiarism in the area of
answer-set programming (ASP). Currently, the tool is implemented for DLV
programs but it is designed to handle other logic-programming dialects as well.
We review the basic features of Kato, introduce its theoretical underpinnings,
and discuss an application of Kato for plagiarism detection in the context of
courses on logic programming at the Vienna University of Technology."
"In this paper we work on (bi)simulation semantics of processes that exhibit
both nondeterministic and probabilistic behaviour. We propose a probabilistic
extension of the modal mu-calculus and show how to derive characteristic
formulae for various simulation-like preorders over finite-state processes
without divergence. In addition, we show that even without the fixpoint
operators this probabilistic mu-calculus can be used to characterise these
behavioural relations in the sense that two states are equivalent if and only
if they satisfy the same set of formulae."
"We consider CSP from the point of view of the algebraic theory of effects,
which classifies operations as effect constructors or effect deconstructors; it
also provides a link with functional programming, being a refinement of Moggi's
seminal monadic point of view. There is a natural algebraic theory of the
constructors whose free algebra functor is Moggi's monad; we illustrate this by
characterising free and initial algebras in terms of two versions of the stable
failures model of CSP, one more general than the other. Deconstructors are
dealt with as homomorphisms to (possibly non-free) algebras.
  One can view CSP's action and choice operators as constructors and the rest,
such as concealment and concurrency, as deconstructors. Carrying this programme
out results in taking deterministic external choice as constructor rather than
general external choice. However, binary deconstructors, such as the CSP
concurrency operator, provide unresolved difficulties. We conclude by
presenting a combination of CSP with Moggi's computational {\lambda}-calculus,
in which the operators, including concurrency, are polymorphic. While the paper
mainly concerns CSP, it ought to be possible to carry over similar ideas to
other process calculi."
"This paper characterises the coarsest refinement preorders on labelled
transition systems that are precongruences for renaming and partially
synchronous interleaving operators, and respect all safety, liveness, and
conditional liveness properties, respectively."
"Interactive theorem provers based on dependent type theory have the
flexibility to support both constructive and classical reasoning. Constructive
reasoning is supported natively by dependent type theory and classical
reasoning is typically supported by adding additional non-constructive axioms.
However, there is another perspective that views constructive logic as an
extension of classical logic. This paper will illustrate how classical
reasoning can be supported in a practical manner inside dependent type theory
without additional axioms. We will see several examples of how classical
results can be applied to constructive mathematics. Finally, we will see how to
extend this perspective from logic to mathematics by representing classical
function spaces using a weak value monad."
"In the context of Answer Set Programming, this paper investigates
symmetry-breaking to eliminate symmetric parts of the search space and,
thereby, simplify the solution process. We propose a reduction of disjunctive
logic programs to a coloured digraph such that permutational symmetries can be
constructed from graph automorphisms. Symmetries are then broken by introducing
symmetry-breaking constraints. For this purpose, we formulate a preprocessor
that integrates a graph automorphism system. Experiments demonstrate its
computational impact."
"This paper presents an efficient, combined formulation of two widely used
abstraction methods for bit-level verification: counterexample-based
abstraction (CBA) and proof-based abstraction (PBA). Unlike previous work, this
new method is formulated as a single, incremental SAT-problem, interleaving CBA
and PBA to develop the abstraction in a bottom-up fashion. It is argued that
the new method is simpler conceptually and implementation-wise than previous
approaches. As an added bonus, proof-logging is not required for the PBA part,
which allows for a wider set of SAT-solvers to be used."
"One of the basic sanity properties of a behavioural semantics is that it
constitutes a congruence with respect to standard process operators. This issue
has been traditionally addressed by the development of rule formats for
transition system specifications that define process algebras. In this paper we
suggest a novel, orthogonal approach. Namely, we focus on a number of process
operators, and for each of them attempt to find the widest possible class of
congruences. To this end, we impose restrictions on sublanguages of
Hennessy-Milner logic, so that a semantics whose modal characterization
satisfies a given criterion is guaranteed to be a congruence with respect to
the operator in question. We investigate action prefix, alternative
composition, two restriction operators, and parallel composition."
"Inspired by decomposition problems in rule-based formalisms in Computational
Systems Biology and recent work on compositionality in graph transformation,
this paper proposes to use arbitrary colimits to ""deconstruct"" models of
reactions in which states are represented as objects of adhesive categories.
The fundamental problem is the decomposition of complex reactions of large
states into simpler reactions of smaller states.
  The paper defines the local decomposition problem for transformations. To
solve this problem means to ""reconstruct"" a given transformation as the colimit
of ""smaller"" ones where the shape of the colimit and the decomposition of the
source object of the transformation are fixed in advance. The first result is
the soundness of colimit decomposition for arbitrary double pushout
transformations in any category, which roughly means that several ""local""
transformations can be combined into a single ""global"" one. Moreover, a
solution for a certain class of local decomposition problems is given, which
generalizes and clarifies recent work on compositionality in graph
transformation."
"We show that global constraints on finite domains like all-different can be
reformulated into answer set programs on which we achieve arc, bound or range
consistency. These reformulations offer a number of other advantages beyond
providing the power of global propagators to answer set programming. For
example, they provide other constraints with access to the state of the
propagator by sharing variables. Such sharing can be used to improve
propagation between constraints. Experiments with these encodings demonstrate
their promise."
"In the context of answer set programming, this work investigates symmetry
detection and symmetry breaking to eliminate symmetric parts of the search
space and, thereby, simplify the solution process. We contribute a reduction of
symmetry detection to a graph automorphism problem which allows to extract
symmetries of a logic program from the symmetries of the constructed coloured
graph. We also propose an encoding of symmetry-breaking constraints in terms of
permutation cycles and use only generators in this process which implicitly
represent symmetries and always with exponential compression. These ideas are
formulated as preprocessing and implemented in a completely automated flow that
first detects symmetries from a given answer set program, adds
symmetry-breaking constraints, and can be applied to any existing answer set
solver. We demonstrate computational impact on benchmarks versus direct
application of the solver.
  Furthermore, we explore symmetry breaking for answer set programming in two
domains: first, constraint answer set programming as a novel approach to
represent and solve constraint satisfaction problems, and second, distributed
nonmonotonic multi-context systems. In particular, we formulate a
translation-based approach to constraint answer set solving which allows for
the application of our symmetry detection and symmetry breaking methods. To
compare their performance with a-priori symmetry breaking techniques, we also
contribute a decomposition of the global value precedence constraint that
enforces domain consistency on the original constraint via the unit-propagation
of an answer set solver. We evaluate both options in an empirical analysis. In
the context of distributed nonmonotonic multi-context system, we develop an
algorithm for distributed symmetry detection and also carry over
symmetry-breaking constraints for distributed answer set programming."
"A collaboration network is a graph formed by communication channels between
parties. Parties communicate over these channels to establish secrets,
simultaneously enforcing interdependencies between the secrets. The paper
studies properties of these interdependencies that are induced by the topology
of the network. In previous work, the authors developed a complete logical
system for one such property, independence, also known in the information flow
literature as nondeducibility. This work describes a complete and decidable
logical system for the functional dependence relation between sets of secrets
over a collaboration network. The system extends Armstrong's system of axioms
for functional dependency in databases."
"In this paper we investigate to which extent a very simple and natural
""reachability as deducibility"" approach, originated in the research in formal
methods in security, is applicable to the automated verification of large
classes of infinite state and parameterized systems. The approach is based on
modeling the reachability between (parameterized) states as deducibility
between suitable encodings of states by formulas of first-order predicate
logic. The verification of a safety property is reduced to a pure logical
problem of finding a countermodel for a first-order formula. The later task is
delegated then to the generic automated finite model building procedures. In
this paper we first establish the relative completeness of the finite
countermodel finding method (FCM) for a class of parameterized linear arrays of
finite automata. The method is shown to be at least as powerful as known
methods based on monotonic abstraction and symbolic backward reachability.
Further, we extend the relative completeness of the approach and show that it
can solve all safety verification problems which can be solved by the
traditional regular model checking."
"We consider two-player games played in real time on game structures with
clocks where the objectives of players are described using parity conditions.
The games are \emph{concurrent} in that at each turn, both players
independently propose a time delay and an action, and the action with the
shorter delay is chosen. To prevent a player from winning by blocking time, we
restrict each player to play strategies that ensure that the player cannot be
responsible for causing a zeno run. First, we present an efficient reduction of
these games to \emph{turn-based} (i.e., not concurrent) \emph{finite-state}
(i.e., untimed) parity games. Our reduction improves the best known complexity
for solving timed parity games. Moreover, the rich class of algorithms for
classical parity games can now be applied to timed parity games. The states of
the resulting game are based on clock regions of the original game, and the
state space of the finite game is linear in the size of the region graph.
  Second, we consider two restricted classes of strategies for the player that
represents the controller in a real-time synthesis problem, namely,
\emph{limit-robust} and \emph{bounded-robust} winning strategies. Using a
limit-robust winning strategy, the controller cannot choose an exact
real-valued time delay but must allow for some nonzero jitter in each of its
actions. If there is a given lower bound on the jitter, then the strategy is
bounded-robust winning. We show that exact strategies are more powerful than
limit-robust strategies, which are more powerful than bounded-robust winning
strategies for any bound. For both kinds of robust strategies, we present
efficient reductions to standard timed automaton games. These reductions
provide algorithms for the synthesis of robust real-time controllers."
"Craig interpolation has emerged as an effective means of generating candidate
program invariants. We present interpolation procedures for the theories of
Presburger arithmetic combined with (i) uninterpreted predicates (QPA+UP), (ii)
uninterpreted functions (QPA+UF) and (iii) extensional arrays (QPA+AR). We
prove that none of these combinations can be effectively interpolated without
the use of quantifiers, even if the input formulae are quantifier-free. We go
on to identify fragments of QPA+UP and QPA+UF with restricted forms of guarded
quantification that are closed under interpolation. Formulae in these fragments
can easily be mapped to quantifier-free expressions with integer division. For
QPA+AR, we formulate a sound interpolation procedure that potentially produces
interpolants with unrestricted quantifiers."
"We study the underlying mathematical properties of various partial order
models of concurrency based on transition systems, Petri nets, and event
structures, and show that the concurrent behaviour of these systems can be
captured in a uniform way by two simple and general dualities of local
behaviour. Such dualities are used to define new mu-calculi and logic games for
the analysis of concurrent systems with partial order semantics. Some results
of this work are: the definition of a number of mu-calculi which, in some
classes of systems, induce the same identifications as some of the best known
bisimulation equivalences for concurrency; and the definition of (infinite)
higher-order logic games for bisimulation and model-checking, where the players
of the games are given (local) monadic second-order power on the sets of
elements they are allowed to play. More specifically, we show that our games
are sound and complete, and therefore, determined; moreover, they are decidable
in the finite case and underpin novel decision procedures for bisimulation and
model-checking. Since these mu-calculi and logic games generalise well-known
fixpoint logics and game-theoretic decision procedures for concurrent systems
with interleaving semantics, the results herein give some of the groundwork for
the design of a logic-based, game-theoretic framework for studying, in a
uniform way, several concurrent systems regardless of whether they have an
interleaving or a partial order semantics."
"Propositional Projection Temporal Logic (PPTL) is a useful formalism for
reasoning about period of time in hardware and software systems and can handle
both sequential and parallel compositions. In this paper, based on discrete
time Markov chains, we investigate the probabilistic model checking approach
for PPTL towards verifying arbitrary linear-time properties. We first define a
normal form graph, denoted by NFG_inf, to capture the infinite paths of PPTL
formulas. Then we present an algorithm to generate the NFG_inf. Since
discrete-time Markov chains are the deterministic probabilistic models, we
further give an algorithm to determinize and minimize the nondeterministic
NFG_inf following the Safra's construction."
"I show that, if a term is $SN$ for $\beta$, it remains $SN$ when some
permutation rules are added."
"Goedel's completeness theorem is concerned with provability, while Girard's
theorem in ludics (as well as full completeness theorems in game semantics) are
concerned with proofs. Our purpose is to look for a connection between these
two disciplines. Following a previous work [3], we consider an extension of the
original ludics with contraction and universal nondeterminism, which play dual
roles, in order to capture a polarized fragment of linear logic and thus a
constructive variant of classical propositional logic. We then prove a
completeness theorem for proofs in this extended setting: for any behaviour
(formula) A and any design (proof attempt) P, either P is a proof of A or there
is a model M of the orthogonal of A which defeats P. Compared with proofs of
full completeness in game semantics, ours exhibits a striking similarity with
proofs of Goedel's completeness, in that it explicitly constructs a
countermodel essentially using Koenig's lemma, proceeds by induction on
formulas, and implies an analogue of Loewenheim-Skolem theorem."
"In this paper we provide an abstract model theory for the untyped
differential lambda-calculus and the resource calculus. In particular we
propose a general definition of model of these calculi, namely the notion of
linear reflexive object in a Cartesian closed differential category. Examples
of models based on relations are provided."
"TLAPS, the TLA+ proof system, is a platform for the development and
mechanical verification of TLA+ proofs written in a declarative style requiring
little background beyond elementary mathematics. The language supports
hierarchical and non-linear proof construction and verification, and it is
independent of any verification tool or strategy. A Proof Manager uses backend
verifiers such as theorem provers, proof assistants, SMT solvers, and decision
procedures to check TLA+ proofs. This paper documents the first public release
of TLAPS, distributed with a BSD-like license. It handles almost all the
non-temporal part of TLA+ as well as the temporal reasoning needed to prove
standard safety properties, in particular invariance and step simulation, but
not liveness properties."
"In this paper, we show that theory of processes can be reduced to the theory
of spatial logic. Firstly, we propose a spatial logic SL for higher order
pi-calculus, and give an inference system of SL. The soundness and
incompleteness of SL are proved. Furthermore, we show that the structure
congruence relation and one-step transition relation can be described as the
logical relation of SL formulae. We also extend bisimulations for processes to
that for SL formulae. Then we extend all definitions and results of SL to a
weak semantics version of SL, called WL. At last, we add mu-operator to SL.
This new logic is named muSL. We show that WL is a sublogic of muSL and
replication operator can be expressed in muSL."
"This paper undertakes a re-examination of Sir William Hamilton's doctrine of
the quantification of the predicate. Hamilton's doctrine comprises two theses.
First, the predicates of traditional syllogistic sentence-forms contain
implicit existential quantifiers, so that, for example, ""All p are q"" is to be
understood as ""All p are some q"". Second, these implicit quantifiers can be
meaningfully dualized to yield novel sentence-forms, such as, for example, ""All
p are all q"". Hamilton attempted to provide a deductive system for his
language, along the lines of the classical syllogisms. We show, using
techniques unavailable to Hamilton, that such a system does exist, though with
qualifications that distinguish it from its classical counterpart."
"We extend the theory of labeled Markov processes with internal
nondeterminism, a fundamental concept for the further development of a process
theory with abstraction on nondeterministic continuous probabilistic systems.
We define nondeterministic labeled Markov processes (NLMP) and provide three
definition of bisimulations: a bisimulation following a traditional
characterization, a state based bisimulation tailored to our ""measurable""
non-determinism, and an event based bisimulation. We show the relation between
them, including that the largest state bisimulation is also an event
bisimulation. We also introduce a variation of the Hennessy-Milner logic that
characterizes event bisimulation and that is sound w.r.t. the other
bisimulations for arbitrary NLMP. This logic, however, is infinitary as it
contains a denumerable $\lor$. We then introduce a finitary sublogic that
characterize all bisimulations for image finite NLMP whose underlying measure
space is also analytic. Hence, in this setting, all notions of bisimulation we
deal with turn out to be equal. Finally, we show that all notions of
bisimulations are different in the general case. The counterexamples that
separate them turn to be non-probabilistic NLMP."
"We develop a general criterion for cut elimination in sequent calculi for
propositional modal logics, which rests on absorption of cut, contraction,
weakening and inversion by the purely modal part of the rule system. Our
criterion applies also to a wide variety of logics outside the realm of normal
modal logic. We give extensive example instantiations of our framework to
various conditional logics. For these, we obtain fully internalised calculi
which are substantially simpler than those known in the literature, along with
leaner proofs of cut elimination and complexity. In one case, conditional logic
with modus ponens and conditional excluded middle, cut elimination and
complexity were explicitly stated as open in the literature."
"We consider the non-deterministic extension of the call-by-value lambda
calculus, which corresponds to the additive fragment of the linear-algebraic
lambda-calculus. We define a fine-grained type system, capturing the right
linearity present in such formalisms. After proving the subject reduction and
the strong normalisation properties, we propose a translation of this calculus
into the System F with pairs, which corresponds to a non linear fragment of
linear logic. The translation provides a deeper understanding of the linearity
in our setting."
"SBV is a deep inference system that extends the set of logical operators of
multiplicative linear logic with the non commutative operator Seq. We introduce
the logical system SBVr which extends SBV by adding a self-dual atom-renaming
operator to it. We prove that the cut elimination holds on SBVr. SBVr and its
cut free subsystem BVr are complete and sound with respect to linear lambda
calculus with explicit substitutions. Under any strategy, a sequence of
evaluation steps of any linear lambda-term M becomes a process of proof-search
in SBVr (BVr) once M is mapped into a formula of SBVr. Completeness and
soundness follow from simulating linear beta-reduction with explicit
substitutions as processes. The role of the new renaming operator of SBVr is to
rename channel-names on-demand. This simulates the substitution that occurs in
a beta-reduction. Despite SBVr is a minimal extension of SBV its proof-search
can compute all boolean functions, as linear lambda-calculus with explicit
substitutions can compute all boolean functions as well. So, proof search of
SBVr and BVr is at least ptime-complete."
"We prove several decidability and undecidability results for nu-PN, an
extension of P/T nets with pure name creation and name management. We give a
simple proof of undecidability of reachability, by reducing reachability in
nets with inhibitor arcs to it. Thus, the expressive power of nu-PN strictly
surpasses that of P/T nets. We prove that nu-PN are Well Structured Transition
Systems. In particular, we obtain decidability of coverability and termination,
so that the expressive power of Turing machines is not reached. Moreover, they
are strictly Well Structured, so that the boundedness problem is also
decidable. We consider two properties, width-boundedness and depth-boundedness,
that factorize boundedness. Width-boundedness has already been proven to be
decidable. We prove here undecidability of depth-boundedness. Finally, we
obtain Ackermann-hardness results for all our decidable decision problems."
"For many practical applications of ASP, for instance data integration or
planning, query answering is important, and therefore query optimization
techniques for ASP are of great interest. Magic Sets are one of these
techniques, originally defined for Datalog queries (ASP without disjunction and
negation). Dynamic Magic Sets (DMS) are an extension of this technique, which
has been proved to be sound and complete for query answering over ASP programs
with stratified negation.
  A distinguishing feature of DMS is that the optimization can be exploited
also during the nondeterministic phase of ASP engines. In particular, after
some assumptions have been made during the computation, parts of the program
may become irrelevant to a query under these assumptions. This allows for
dynamic pruning of the search space, which may result in exponential
performance gains.
  In this paper, the correctness of DMS is formally established and proved for
brave and cautious reasoning over the class of super-consistent ASP programs
(ASP^{sc} programs). ASP^{sc} programs guarantee consistency (i.e., have answer
sets) when an arbitrary set of facts is added to them. This result generalizes
the applicability of DMS, since the class of ASP^{sc} programs is richer than
ASP programs with stratified negation, and in particular includes all
odd-cycle-free programs. DMS has been implemented as an extension of DLV, and
the effectiveness of DMS for ASP^{sc} programs is empirically confirmed by
experimental results with this system."
"Canonical inference rules and canonical systems are defined in the framework
of non-strict single-conclusion sequent systems, in which the succeedents of
sequents can be empty. Important properties of this framework are investigated,
and a general non-deterministic Kripke-style semantics is provided. This
general semantics is then used to provide a constructive (and very natural),
sufficient and necessary coherence criterion for the validity of the strong
cut-elimination theorem in such a system. These results suggest new syntactic
and semantic characterizations of basic constructive connectives."
"Open Answer Set Programming (OASP) is an attractive framework for integrating
ontologies and rules. In general OASP is undecidable. In previous work we
provided a tableau-based algorithm for satisfiability checking w.r.t. forest
logic programs, a decidable fragment of OASP, which has the forest model
property. In this paper we introduce an optimized version of that algorithm
achieved by means of a knowledge compilation technique. So-called unit
completion structures, which are possible building blocks of a forest model, in
the form of trees of depth 1, are computed in an initial step of the algorithm.
Repeated computations are avoided by using these structures in a
pattern-matching style when constructing a model. Furthermore we identify and
discard redundant unit completion structures: a structure is redundant if there
is another structure which can always replace the original structure in a
forest model."
"This volume contains the proceedings of the 17th International Workshop on
Expressiveness in Concurrency (EXPRESS'10), which took place on 30th August
2010 in Paris, co-located with CONCUR'10. The EXPRESS workshop series aim at
bringing together researchers who are interested in the expressiveness and
comparison of formal models that broadly relate to concurrency. In particular,
this also includes emergent fields such as logic and interaction,
game-theoretic models, and service-oriented computing."
"We introduce the process calculus Multi-CCS, which extends conservatively CCS
with an operator of strong prefixing able to model atomic sequences of actions
as well as multiparty synchronization. Multi-CCS is equipped with a labeled
transition system semantics, which makes use of a minimal structural
congruence. Multi-CCS is also equipped with an unsafe P/T Petri net semantics
by means of a novel technique. This is the first rich process calculus,
including CCS as a subcalculus, which receives a semantics in terms of unsafe,
labeled P/T nets. The main result of the paper is that a class of Multi-CCS
processes, called finite-net processes, is able to represent all finite
(reduced) P/T nets."
"We consider models of CSP based on recording what events are available as
possible alternatives to the events that are actually performed. We present
many different varieties of such models. For each, we give a compositional
semantics, congruent to the operational semantics, and prove full abstraction
and no-junk results. We compare the expressiveness of the different models."
"Sound behavioral equations on open terms may become unsound after
conservative extensions of the underlying operational semantics. Providing
criteria under which such equations are preserved is extremely useful; in
particular, it can avoid the need to repeat proofs when extending the specified
language.
  This paper investigates preservation of sound equations for several notions
of bisimilarity on open terms: closed-instance (ci-)bisimilarity and
formal-hypothesis (fh-)bisimilarity, both due to Robert de Simone, and
hypothesis-preserving (hp-)bisimilarity, due to Arend Rensink. For both
fh-bisimilarity and hp-bisimilarity, we prove that arbitrary sound equations on
open terms are preserved by all disjoint extensions which do not add labels. We
also define slight variations of fh- and hp-bisimilarity such that all sound
equations are preserved by arbitrary disjoint extensions. Finally, we give two
sets of syntactic criteria (on equations, resp. operational extensions) and
prove each of them to be sufficient for preserving ci-bisimilarity."
"A well-known result by Palamidessi tells us that \pimix (the \pi-calculus
with mixed choice) is more expressive than \pisep (its subset with only
separate choice). The proof of this result argues with their different
expressive power concerning leader election in symmetric networks. Later on,
Gorla offered an arguably simpler proof that, instead of leader election in
symmetric networks, employed the reducibility of incestual processes (mixed
choices that include both enabled senders and receivers for the same channel)
when running two copies in parallel. In both proofs, the role of breaking
(initial) symmetries is more or less apparent. In this paper, we shed more
light on this role by re-proving the above result - based on a proper
formalization of what it means to break symmetries without referring to another
layer of the distinguishing problem domain of leader election. Both Palamidessi
and Gorla rephrased their results by stating that there is no uniform and
reasonable encoding from \pimix into \pisep. We indicate how the respective
proofs can be adapted and exhibit the consequences of varying notions of
uniformity and reasonableness. In each case, the ability to break initial
symmetries turns out to be essential."
"Process behaviour is often defined either in terms of the tests they satisfy,
or in terms of the logical properties they enjoy. Here we compare these two
approaches, using extensional testing in the style of DeNicola, Hennessy, and a
recursive version of the property logic HML. We first characterise subsets of
this property logic which can be captured by tests. Then we show that those
subsets of the property logic capture precisely the power of tests."
"It is important to enable reasoning about the meaning and possible effects of
updates to ensure that the updated system operates correctly. A formal,
mathematical model of dynamic update should be developed, in order to
understand by both users and implementors of update technology what design
choices can be considered. In this paper, we define a formal calculus
$update\pi$, a variant extension of higher-order $\pi$ calculus, to model
dynamic updates of component-based software, which is language and technology
independent. The calculus focuses on following main concepts: proper
granularity of update, timing of dynamic update, state transformation between
versions, update failure check and recovery. We describe a series of rule on
safe component updates to model some general processes of dynamic update and
discuss its reduction semantics coincides with a labelled transition system
semantics that illustrate the expressive power of these calculi."
"The linear-algebraic lambda-calculus and the algebraic lambda-calculus are
untyped lambda-calculi extended with arbitrary linear combinations of terms.
The former presents the axioms of linear algebra in the form of a rewrite
system, while the latter uses equalities. When given by rewrites, algebraic
lambda-calculi are not confluent unless further restrictions are added. We
provide a type system for the linear-algebraic lambda-calculus enforcing strong
normalisation, which gives back confluence. The type system allows an abstract
interpretation in System F."
"The introduction of first-class type classes in the Coq system calls for
re-examination of the basic interfaces used for mathematical formalization in
type theory. We present a new set of type classes for mathematics and take full
advantage of their unique features to make practical a particularly flexible
approach formerly thought infeasible. Thus, we address both traditional proof
engineering challenges as well as new ones resulting from our ambition to build
upon this development a library of constructive analysis in which abstraction
penalties inhibiting efficient computation are reduced to a minimum.
  The base of our development consists of type classes representing a standard
algebraic hierarchy, as well as portions of category theory and universal
algebra. On this foundation we build a set of mathematically sound abstract
interfaces for different kinds of numbers, succinctly expressed using
categorical language and universal algebra constructions. Strategic use of type
classes lets us support these high-level theory-friendly definitions while
still enabling efficient implementations unhindered by gratuitous indirection,
conversion or projection.
  Algebra thrives on the interplay between syntax and semantics. The
Prolog-like abilities of type class instance resolution allow us to
conveniently define a quote function, thus facilitating the use of reflective
techniques."
"In [1], systems of weakening of intuitionistic negation logic called Z_n and
CZ_n were developed in the spirit of da Costa's approach(c.f. [2]) by
preserving, differently from da Costa, its fundamental properties:
antitonicity, inversion and additivity for distributive lattices. However,
according to [3], those systems turned out to be not paraconsistent but
extensions of intuitionistic logic. Taking into account of this result, we
shall here make some observations on the modified systems of Z_n and CZ_n, that
are paraconsistent as well."
"This note corrects a discrepancy between the semantics and the algorithm of
the multiple until operator of CSL, like in Pr_{> 0.0025} (a until[1,2] b
until[3,4] c), of the article: Model-checking continuous-time Markov chains by
Aziz, Sanwal, Singhal and Brayton, TOCL 1(1), July 2000, pp. 162-170."
"We study the process theoretic notion of stuttering equivalence in the
setting of parity games. We demonstrate that stuttering equivalent vertices
have the same winner in the parity game. This means that solving a parity game
can be accelerated by minimising the game graph with respect to stuttering
equivalence. While, at the outset, it might not be clear that this strategy
should pay off, our experiments using typical verification problems illustrate
that stuttering equivalence speeds up solving parity games in many cases."
"We define a logical framework with singleton types and one universe of small
types. We give the semantics using a PER model; it is used for constructing a
normalisation-by-evaluation algorithm. We prove completeness and soundness of
the algorithm; and get as a corollary the injectivity of type constructors.
Then we give the definition of a correct and complete type-checking algorithm
for terms in normal form. We extend the results to proof-irrelevant
propositions."
"In [arXiv:1006.4939] the enumeration order reducibility is defined on natural
numbers. For a c.e. set A, [A] denoted the class of all subsets of natural
numbers which are co-order with A. In definition 5 we redefine co-ordering for
rational numbers. One of the main questions there, was: ""For a specific c.e.
set A, consider set of all enumerations of it which is generated by some Turing
machine {TM_A} what are the associated order types in [A]?"" Here, we propose
the same question for rational numbers, and we try to investigate the varieties
of c.e. sets on Q. The theories here are hold for R_c and we could repeat the
same theories in this domain, in a parallel way."
"The relationship between Term Graph Rewriting and Term Rewriting is well
understood: a single term graph reduction may correspond to several term
reductions, due to sharing. It is also known that if term graphs are allowed to
contain cycles, then one term graph reduction may correspond to infinitely many
term reductions. We stress that this fact can be interpreted in two ways.
According to the ""sequential interpretation"", a term graph reduction
corresponds to an infinite sequence of term reductions, as formalized by
Kennaway et.al. using strongly converging derivations over the complete metric
space of infinite terms. Instead according to the ""parallel interpretation"" a
term graph reduction corresponds to the parallel reduction of an infinite set
of redexes in a rational term. We formalize the latter notion by exploiting the
complete partial order of infinite and possibly partial terms, and we stress
that this interpretation allows to explain the result of reducing circular
redexes in several approaches to term graph rewriting."
"We offer a simple graphical representation for proofs of intuitionistic
logic, which is inspired by proof nets and interaction nets (two formalisms
originating in linear logic). This graphical calculus of proofs inherits good
features from each, but is not constrained by them. By the Curry-Howard
isomorphism, the representation applies equally to the lambda calculus,
offering an alternative diagrammatic representation of functional computations."
"First-order applicative term rewriting systems provide a natural framework
for modeling higher-order aspects. In earlier work we introduced an uncurrying
transformation which is termination preserving and reflecting. In this paper we
investigate how this transformation behaves for innermost termination and
(innermost) derivational complexity. We prove that it reflects innermost
termination and innermost derivational complexity and that it preserves and
reflects polynomial derivational complexity. For the preservation of innermost
termination and innermost derivational complexity we give counterexamples.
Hence uncurrying may be used as a preprocessing transformation for innermost
termination proofs and establishing polynomial upper and lower bounds on the
derivational complexity. Additionally it may be used to establish upper bounds
on the innermost derivational complexity while it neither is sound for proving
innermost non-termination nor for obtaining lower bounds on the innermost
derivational complexity."
"We discuss the problem of experimentally evaluating linear-time temporal
logic (LTL) synthesis tools for reactive systems. We first survey previous such
work for the currently publicly available synthesis tools, and then draw
conclusions by deriving useful schemes for future such evaluations.
  In particular, we explain why previous tools have incompatible scopes and
semantics and provide a framework that reduces the impact of this problem for
future experimental comparisons of such tools. Furthermore, we discuss which
difficulties the complex workflows that begin to appear in modern synthesis
tools induce on experimental evaluations and give answers to the question how
convincing such evaluations can still be performed in such a setting."
"We show how to automatically construct a system that satisfies a given
logical specification and has an optimal average behavior with respect to a
specification with ratio costs.
  When synthesizing a system from a logical specification, it is often the case
that several different systems satisfy the specification. In this case, it is
usually not easy for the user to state formally which system she prefers. Prior
work proposed to rank the correct systems by adding a quantitative aspect to
the specification. A desired preference relation can be expressed with (i) a
quantitative language, which is a function assigning a value to every possible
behavior of a system, and (ii) an environment model defining the desired
optimization criteria of the system, e.g., worst-case or average-case optimal.
  In this paper, we show how to synthesize a system that is optimal for (i) a
quantitative language given by an automaton with a ratio cost function, and
(ii) an environment model given by a labeled Markov decision process. The
objective of the system is to minimize the expected (ratio) costs. The solution
is based on a reduction to Markov Decision Processes with ratio cost functions
which do not require that the costs in the denominator are strictly positive.
We find an optimal strategy for these using a fractional linear program."
"The idea of automatic synthesis of reactive programs starting from temporal
logic (LTL) specifications is quite old, but was commonly thought to be
infeasible due to the known double exponential complexity of the problem.
However, new ideas have recently renewed the interest in LTL synthesis: One
major new contribution in this area is the recent work of Piterman et al. who
showed how polynomial time synthesis can be achieved for a large class of LTL
specifications that is expressive enough to cover many practical examples.
These LTL specifications are equivalent to omega-automata having a so-called
GR(1) acceptance condition. This approach has been used to automatically
synthesize implementations of real-world applications. To this end, manually
written deterministic omega-automata having GR(1) conditions were used instead
of the original LTL specifications. However, manually generating deterministic
monitors is, of course, a hard and error-prone task. In this paper, we
therefore present algorithms to automatically translate specifications of a
remarkable large fragment of LTL to deterministic monitors having a GR(1)
acceptance condition so that the synthesis algorithms can start with more
readable LTL specifications."
"We present a system of relational syllogistic, based on classical
propositional logic, having primitives of the following form:
  Some A are R-related to some B;
  Some A are R-related to all B;
  All A are R-related to some B;
  All A are R-related to all B.
  Such primitives formalize sentences from natural language like `All students
read some textbooks'. Here A and B denote arbitrary sets (of objects), and R
denotes an arbitrary binary relation between objects. The language of the logic
contains only variables denoting sets, determining the class of set terms, and
variables denoting binary relations between objects, determining the class of
relational terms. Both classes of terms are closed under the standard Boolean
operations. The set of relational terms is also closed under taking the
converse of a relation. The results of the paper are the completeness theorem
with respect to the intended semantics and the computational complexity of the
satisfiability problem."
"The aim of illocutionary logic is to explain how context can affect the
meaning of certain special kinds of performative utterances. Recall that
performative utterances are understood as follows: a speaker performs the
illocutionary act (e.g. act of assertion, of conjecture, of promise) with the
illocutionary force (resp. assertion, conjecture, promise) named by an
appropriate performative verb in the way of representing himself as performing
that act. In the paper I proposed many-valued interpretation of illocutionary
forces understood as modal operators. As a result, I built up a non-Archimedean
valued logic for formalizing illocutionary acts. A formal many-valued approach
to illocutionary logic was offered for the first time."
"Timed temporal logics exhibit a bewildering diversity of operators and the
resulting decidability and expressiveness properties also vary considerably. We
study the expressive power of timed logics TPTL[U,S] and MTL[U,S] as well as of
their several fragments. Extending the LTL EF games of Etessami and Wilke, we
define MTL Ehrenfeucht-Fraisse games on a pair of timed words. Using the
associated EF theorem, we show that, expressively, the timed logics
BoundedMTL[U,S], MTL[F,P] and MITL[U,S] (respectively incorporating the
restrictions of boundedness, unary modalities and non-punctuality), are all
pairwise incomparable. As our first main result, we show that MTL[U,S] is
strictly contained within the freeze logic TPTL[U,S] for both weakly and
strictly monotonic timed words, thereby extending the result of Bouyer et al
and completing the proof of the original conjecture of Alur and Henziger from
1990. We also relate the expressiveness of a recently proposed deterministic
freeze logic TTL[X,Y] (with NP-complete satisfiability) to MTL. As our second
main result, we show by an explicit reduction that TTL[X,Y] lies strictly
within the unary, non-punctual logic MITL[F,P]. This shows that deterministic
freezing with punctuality is expressible in the non-punctual MITL[F,P]."
"The probabilistic (or quantitative) modal mu-calculus is a fixed-point logic
de- signed for expressing properties of probabilistic labeled transition
systems (PLTS). Two semantics have been studied for this logic, both assigning
to every process state a value in the interval [0,1] representing the
probability that the property expressed by the formula holds at the state. One
semantics is denotational and the other is a game semantics, specified in terms
of two-player stochastic games. The two semantics have been proved to coincide
on all finite PLTS's, but the equivalence of the two semantics on arbitrary
models has been open in literature. In this paper we prove that the equivalence
indeed holds for arbitrary infinite models, and thus our result strengthens the
fruitful connection between denotational and game semantics. Our proof adapts
the unraveling or unfolding method, a general proof technique for proving
result of parity games by induction on their complexity."
"We define a general framework of partition games for formulating two-player
pebble games over finite structures. We show that one particular such game,
which we call the invertible-map game, yields a family of polynomial-time
approximations of graph isomorphism that is strictly stronger than the
well-known Weisfeiler-Lehman method. The general framework we introduce
includes as special cases the pebble games for finite-variable logics with and
without counting. It also includes a matrix-equivalence game, introduced here,
which characterises equivalence in the finite-variable fragments of matrix-rank
logic. We show that the equivalence defined by the invertible-map game is a
refinement of the equivalence defined by each of these games for
finite-variable logics."
"This paper introduces a novel technique to decide the satisfiability of
formulae written in the language of Linear Temporal Logic with Both future and
past operators and atomic formulae belonging to constraint system D (CLTLB(D)
for short). The technique is based on the concept of bounded satisfiability,
and hinges on an encoding of CLTLB(D) formulae into QF-EUD, the theory of
quantifier-free equality and uninterpreted functions combined with D. Similarly
to standard LTL, where bounded model-checking and SAT-solvers can be used as an
alternative to automata-theoretic approaches to model-checking, our approach
allows users to solve the satisfiability problem for CLTLB(D) formulae through
SMT-solving techniques, rather than by checking the emptiness of the language
of a suitable automaton A_{\phi}. The technique is effective, and it has been
implemented in our Zot formal verification tool."
"We consider the property of unique parallel decomposition modulo branching
and weak bisimilarity. First, we show that infinite behaviours may fail to have
parallel decompositions at all. Then, we prove that totally normed behaviours
always have parallel decompositions, but that these are not necessarily unique.
Finally, we establish that weakly bounded behaviours have unique parallel
decompositions. We derive the latter result from a general theorem about unique
decompositions in partial commutative monoids."
"We extend the higher-order termination method of dynamic dependency pairs to
Algebraic Functional Systems (AFSs). In this setting, simply typed lambda-terms
with algebraic reduction and separate {\beta}-steps are considered. For
left-linear AFSs, the method is shown to be complete. For so-called local AFSs
we define a variation of usable rules and an extension of argument filterings.
All these techniques have been implemented in the higher-order termination tool
WANDA."
"We prove ""untyping"" theorems: in some typed theories (semirings, Kleene
algebras, residuated lattices, involutive residuated lattices), typed equations
can be derived from the underlying untyped equations. As a consequence, the
corresponding untyped decision procedures can be extended for free to the typed
settings. Some of these theorems are obtained via a detour through fragments of
cyclic linear logic, and give rise to a substantial optimisation of standard
proof search algorithms."
"We present a logic for the reasoning about necessity and justifications which
is independent from relational semantics. We choose the concept of
justification -- coming from a class of ""Justification Logics"" (Artemov 2008,
Fitting 2009) -- as the primitive notion on which the concept of necessity is
based. Our axiomatization extends Suszko's non-Fregean logic SCI (Brown, Suszko
1972) by basic axioms from Justification Logic, axioms for quantification over
propositions and over justifications, and some further principles. The core
axiom is: $\varphi$ is necessarily true iff there is a justification for
$\varphi$. That is, necessity is first-order definable by means of
justifications. Instead of defining purely algebraic models in the style of
(Brown, Suszko 1972) we extend the semantics investigated in (Lewitzka 2012) by
some algebraic structure for dealing with justifications and prove soundness
and completeness of our deductive system. Moreover, we are able to restore the
modal logic principle of Necessitation if we add the axiom schema
$\square\varphi \to \square\square\varphi$ and a rule of Axiom Necessitation to
our system. As a main result, we show that the modal logics S4 and S5 can be
captured by our semantics if we impose the corresponding modal logic principles
as additional semantic constraints. This will follow from proof-theoretic
considerations and from our completeness theorems. For the system S4 we present
also a purely model-theoretic proof."
"We characterize the languages in the individual levels of the quantifier
alternation hierarchy of first-order logic with two variables by identities.
This implies decidability of the individual levels. More generally we show that
the two-sided semidirect product of a decidable variety with the variety J is
decidable."
"Quantified constraints over the reals appear in numerous contexts. Usually
existential quantification occurs when some parameter can be chosen by the user
of a system, and univeral quantification when the exact value of a parameter is
either unknown, or when it occurs in infinitely many, similar versions. The
following is a list of application areas and publications that contain
applications for solving quantified constraints over the reals. The list is
certainly not complete, but grows as the author encounters new items.
Contributions are very welcome!"
"We give the first ExpTime (complexity-optimal) tableau decision procedure for
checking satisfiability of a knowledge base in the description logic SHIQ when
numbers are coded in unary. Our procedure is based on global state caching and
integer linear feasibility checking."
"Reachability and LTL model-checking problems for flat counter systems are
known to be decidable but whereas the reachability problem can be shown in NP,
the best known complexity upper bound for the latter problem is made of a tower
of several exponentials. Herein, we show that the problem is only NP-complete
even if LTL admits past-time operators and arithmetical constraints on
counters. Actually, the NP upper bound is shown by adequately combining a new
stuttering theorem for Past LTL and the property of small integer solutions for
quantifier-free Presburger formulae. Other complexity results are proved, for
instance for restricted classes of flat counter systems."
"This paper presents equational-based logics for proving first order
properties of programming languages involving effects. We propose two dual
inference system patterns that can be instanciated with monads or comonads in
order to be used for proving properties of different effects. The first pattern
provides inference rules which can be interpreted in the Kleisli category of a
monad and the coKleisli category of the associated comonad. In a dual way, the
second pattern provides inference rules which can be interpreted in the
coKleisli category of a comonad and the Kleisli category of the associated
monad. The logics combine a 3-tier effect system for terms consisting of pure
terms and two other kinds of effects called 'constructors/observers' and
'modifiers', and a 2-tier system for 'up-to-effects' and 'strong' equations.
Each pattern provides generic rules for dealing with any monad (respectively
comonad), and it can be extended with specific rules for each effect. The paper
presents two use cases: a language with exceptions (using the standard monadic
semantics), and a language with state (using the less standard comonadic
semantics). Finally, we prove that the obtained inference system for states is
Hilbert-Post complete."
"Several notions of bisimulation relations for probabilistic non-deterministic
transition systems have been considered in the literature. We consider a novel
testing-based behavioral equivalence called upper-expectation bisimilarity and
develop its theory using standard results from linear algebra and functional
analysis. We show that, for a wide class of systems, our new notion coincides
with Segala's convex bisimilarity. We develop logical characterizations in
terms of expressive probabilistic modal mu-calculi and a novel real-valued
modal logic. We prove that upper-expectation bisimilarity is a congruence for
the wide family of process algebras specified following the probabilistic GSOS
rule format."
"The quantified constraint satisfaction problem $\mathrm{QCSP}(\mathcal{A})$
is the problem to decide whether a positive Horn sentence, involving nothing
more than the two quantifiers and conjunction, is true on some fixed structure
$\mathcal{A}$. We study two containment problems related to the QCSP. Firstly,
we give a combinatorial condition on finite structures $\mathcal{A}$ and
$\mathcal{B}$ that is necessary and sufficient to render
$\mathrm{QCSP}(\mathcal{A}) \subseteq \mathrm{QCSP}(\mathcal{B})$. We prove
that $\mathrm{QCSP}(\mathcal{A}) \subseteq \mathrm{QCSP}(\mathcal{B})$, that is
all sentences of positive Horn logic true on $\mathcal{A}$ are true on
$\mathcal{B}$, iff there is a surjective homomorphism from
$\mathcal{A}^{|A|^{|B|}}$ to $\mathcal{B}$. This can be seen as improving an
old result of Keisler that shows the former equivalent to there being a
surjective homomorphism from $\mathcal{A}^\omega$ to $\mathcal{B}$. We note
that this condition is already necessary to guarantee containment of the
$\Pi_2$ restriction of the QCSP, that is $\Pi_2$-$\mathrm{CSP}(\mathcal{A})
\subseteq \Pi_2$-$\mathrm{CSP}(\mathcal{B})$. The exponent's bound of
${|A|^{|B|}}$ places the decision procedure for the model containment problem
in non-deterministic double-exponential time complexity. We further show the
exponent's bound $|A|^{|B|}$ to be close to tight by giving a sequence of
structures $\mathcal{A}$ together with a fixed $\mathcal{B}$, $|B|=2$, such
that there is a surjective homomorphism from $\mathcal{A}^r$ to $\mathcal{B}$
only when $r \geq |A|$. Secondly, we prove that the entailment problem for
positive Horn fragment of first-order logic is decidable. That is, given two
sentences $\varphi$ and $\psi$ of positive Horn, we give an algorithm that
determines whether $\varphi \rightarrow \psi$ is true in all structures
(models). Our result is in some sense tight, since we show that the entailment
problem for positive first-order logic (i.e. positive Horn plus disjunction) is
undecidable. In the final part of the paper we ponder a notion of Q-core that
is some canonical representative among the class of templates that engender the
same QCSP. Although the Q-core is not as well-behaved as its better known
cousin the core, we demonstrate that it is still a useful notion in the realm
of QCSP complexity classifications."
"Termination property of functions is an important issue in computability
theory. In this paper, we show that repeated iterations of a function can
induce an order amongst the elements of its domain set. Hasse diagram of the
poset, thus obtained, is shown to look like a forest of trees, with a possible
base set and a generator set (defined in the paper). Isomorphic forests may
arise for different functions and equivalences classes are, thus, formed. Based
on this analysis, a study of the class of deterministically terminating
functions is presented, in which the existence of a Self-Ranking Program, which
can prove its own termination, and a Universal Terminating Function, from which
every other terminating function can be derived, is conjectured."
"We present a typing system with non-idempotent intersection types, typing a
term syntax covering three different calculi: the pure {\lambda}-calculus, the
calculus with explicit substitutions {\lambda}S, and the calculus with explicit
substitutions, contractions and weakenings {\lambda}lxr. In each of the three
calculi, a term is typable if and only if it is strongly normalising, as it is
the case in (many) systems with idempotent intersections. Non-idempotency
brings extra information into typing trees, such as simple bounds on the
longest reduction sequence reducing a term to its normal form. Strong
normalisation follows, without requiring reducibility techniques. Using this,
we revisit models of the {\lambda}-calculus based on filters of intersection
types, and extend them to {\lambda}S and {\lambda}lxr. Non-idempotency
simplifies a methodology, based on such filter models, that produces modular
proofs of strong normalisation for well-known typing systems (e.g. System F).
We also present a filter model by means of orthogonality techniques, i.e. as an
instance of an abstract notion of orthogonality model formalised in this paper
and inspired by classical realisability. Compared to other instances based on
terms (one of which rephrases a now standard proof of strong normalisation for
the {\lambda}-calculus), the instance based on filters is shown to be better at
proving strong normalisation results for {\lambda}S and {\lambda}lxr. Finally,
the bounds on the longest reduction sequence, read off our typing trees, are
refined into an exact measure, read off a specific typing tree (called
principal); in each of the three calculi, a specific reduction sequence of such
length is identified. In the case of the {\lambda}-calculus, this complexity
result is, for longest reduction sequences, the counterpart of de Carvalho's
result for linear head-reduction sequences."
"Programming languages with countable nondeterministic choice are
computationally interesting since countable nondeterminism arises when modeling
fairness for concurrent systems. Because countable choice introduces
non-continuous behaviour, it is well-known that developing semantic models for
programming languages with countable nondeterminism is challenging. We present
a step-indexed logical relations model of a higher-order functional programming
language with countable nondeterminism and demonstrate how it can be used to
reason about contextually defined may- and must-equivalence. In earlier
step-indexed models, the indices have been drawn from {\omega}. Here the
step-indexed relations for must-equivalence are indexed over an ordinal greater
than {\omega}."
"We give a new true-concurrent model for probabilistic concurrent Kleene
algebra. The model is based on probabilistic event structures, which combines
ideas from Katoen's work on probabilistic concurrency and Varacca's
probabilistic prime event structures. The event structures are compared with a
true-concurrent version of Segala's probabilistic simulation. Finally, the
algebraic properties of the model are summarised to the extent that they can be
used to derive techniques such as probabilistic rely/guarantee inference rules."
"Exception handling is provided by most modern programming languages. It
allows to deal with anomalous or exceptional events which require special
processing. In computer algebra, exception handling is an efficient way to
implement the dynamic evaluation paradigm: for instance, in linear algebra,
dynamic evaluation can be used for applying programs which have been written
for matrices with coefficients in a field to matrices with coefficients in a
ring. Thus, a proof system for computer algebra should include a treatement of
exceptions, which must rely on a careful description of a semantics of
exceptions. The categorical notion of monad can be used for formalizing the
raising of exceptions: this has been proposed by Moggi and implemented in
Haskell. In this paper, we provide a proof system for exceptions which involves
both raising and handling, by extending Moggi's approach. Moreover, the core
part of this proof system is dual to a proof system for side effects in
imperative languages, which relies on the categorical notion of comonad. Both
proof systems are implemented in the Coq proof assistant."
"QBFs (quantified boolean formulas), which are a superset of propositional
formulas, provide a canonical representation for PSPACE problems. To overcome
the inherent complexity of QBF, significant effort has been invested in
developing QBF solvers as well as the underlying proof systems. At the same
time, formula preprocessing is crucial for the application of QBF solvers. This
paper focuses on a missing link in currently-available technology: How to
obtain a certificate (e.g. proof) for a formula that had been preprocessed
before it was given to a solver? The paper targets a suite of commonly-used
preprocessing techniques and shows how to reconstruct certificates for them. On
the negative side, the paper discusses certain limitations of the
currently-used proof systems in the light of preprocessing. The presented
techniques were implemented and evaluated in the state-of-the-art QBF
preprocessor bloqqer."