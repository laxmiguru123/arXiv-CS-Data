summary
"Chip-level CMP modeling is investigated to obtain the post-CMP film profile
thickness across a die from its design layout file and a few film deposition
and CMP parameters. The work covers both HDP and conformal CVD film. The
experimental CMP results agree well with the modeled results. Different
algorithms for filling of dummy structure are compared. A smart algorithm for
dummy filling is presented, which achieves maximal pattern-density uniformity
and CMP planarity."
"Taxes have major costs beyond the collected revenue: deadweight from
distorted incentives, compliance and enforcement costs, etc. A simple market
mechanism, the Equity Tax, avoids these problems for the trickiest cases:
corporate, dividend, and capital gains taxes.
  It exploits the ability of the share prices to reflect the expected true
annual return (as perceived by investors, not as defined by law) and works only
for publicly held corporations. Since going or staying public cannot be forced,
and for some constitutional reasons too, the conversion to equity tax must be a
voluntary contract. Repeated reconversions would be costly (all capital gains
are realized) and thus rare. The converts and their shareholders pay no income,
dividend, or capital gain taxes. Instead, they give the IRS, say, 2% of stock
per year to auction promptly. Debts are the lender's assets: its status, not
the debtor's, determines their equity-tax or income-tax treatment.
  The system looks too simple to be right. However, it does have no loopholes
(thus lowering the revenue-neutral tax rate), no compliance costs, requires
little regulation, and leaves all business decisions tax neutral. The total
capital the equity taxed sector absorbs is the only thing the tax could
possibly distort. The rates should match so as to minimize this distortion. The
equity tax enlarges the pre-tax profit since this is what the taxpayers
maximize, not a different after-tax net. The wealth shelter is paid for by
efficiency, not by lost tax."
"This paper develops three polynomial-time pricing techniques for European
Asian options with provably small errors, where the stock prices follow
binomial trees or trees of higher-degree. The first technique is the first
known Monte Carlo algorithm with analytical error bounds suitable for pricing
single-stock options with meaningful confidence and speed. The second technique
is a general recursive bucketing-based scheme that can use the
Aingworth-Motwani-Oldham aggregation algorithm, Monte-Carlo simulation and
possibly others as the base-case subroutine. This scheme enables robust
trade-offs between accuracy and time over subtrees of different sizes. For
long-term options or high frequency price averaging, it can price single-stock
options with smaller errors in less time than the base-case algorithms
themselves. The third technique combines Fast Fourier Transform with
bucketing-based schemes for pricing basket options. This technique takes
polynomial time in the number of days and the number of stocks, and does not
add any errors to those already incurred in the companion bucketing scheme.
This technique assumes that the price of each underlying stock moves
independently."
"The real monetary economy is grounded upon monetary flow equilibration or the
activity of actualizing monetary flow continuity at each economic agent except
for the central bank. Every update of monetary flow continuity at each agent
constantly causes monetary flow equilibration at the neighborhood agents. Every
monetary flow equilibration as the activity of shooting the mark identified as
monetary flow continuity turns out to be off the mark, and constantly generate
the similar activities in sequence. Monetary flow equilibration ceaselessly
reverberating in the economy performs two functions. One is to seek an
organization on its own, and the other is to perturb the ongoing organization.
Monetary flow equilibration as the agency of seeking and perturbing its
organization also serves as a means of predicting its behavior. The likely
organizational behavior could be the one that remains most robust against
monetary flow equilibration as an agency of applying perturbations."
"Any economic agent constituting the monetary economy maintains the activity
of monetary flow equilibration for fulfilling the condition of monetary flow
continuity in the record, except at the central bank. At the same time,
monetary flow equilibration at one economic agent constantly induces at other
agents in the economy further flow disequilibrium to be eliminated
subsequently. We propose the rate of monetary flow disequilibration as a figure
measuring the progressive movement of the economy. The rate of disequilibration
was read out of both the Japanese and the United States monetary economy
recorded over the last fifty years."
"This paper describes the parallel implementation of the TRANSIMS traffic
micro-simulation. The parallelization method is domain decomposition, which
means that each CPU of the parallel computer is responsible for a different
geographical area of the simulated region. We describe how information between
domains is exchanged, and how the transportation network graph is partitioned.
An adaptive scheme is used to optimize load balancing. We then demonstrate how
computing speeds of our parallel micro-simulations can be systematically
predicted once the scenario and the computer architecture are known. This makes
it possible, for example, to decide if a certain study is feasible with a
certain computing budget, and how to invest that budget. The main ingredients
of the prediction are knowledge about the parallel implementation of the
micro-simulation, knowledge about the characteristics of the partitioning of
the transportation network graph, and knowledge about the interaction of these
quantities with the computer system. In particular, we investigate the
differences between switched and non-switched topologies, and the effects of 10
Mbit, 100 Mbit, and Gbit Ethernet. keywords: Traffic simulation, parallel
computing, transportation planning, TRANSIMS"
"The optimal planning trajectory is analyzed on the basis of the growth model
with effectiveness. The saving per capital value has to be rather high
initially with smooth decrement in the future years."
"In a seminal paper in 1973, Black and Scholes argued how expected
distributions of stock prices can be used to price options. Their model assumed
a directed random motion for the returns and consequently a lognormal
distribution of asset prices after a finite time. We point out two problems
with their formulation. First, we show that the option valuation is not
uniquely determined; in particular, stratergies based on the delta-hedge and
CAMP (Capital Asset Pricing Model) are shown to provide different valuations of
an option. Second, asset returns are known not to be Gaussian distributed.
Empirically, distributions of returns are seen to be much better approximated
by an exponential distribution. This exponential distribution of asset prices
can be used to develop a new pricing model for options that is shown to provide
valuations that agree very well with those used by traders. We show how the
Fokker-Planck formulation of fluctuations (i.e., the dynamics of the
distribution) can be modified to provide an exponential distribution for
returns. We also show how a singular volatility can be used to go smoothly from
exponential to Gaussian returns and thereby illustrate why exponential returns
cannot be reached perturbatively starting from Gaussian ones, and explain how
the theory of 'stochastic volatility' can be obtained from our model by making
a bad approximation. Finally, we show how to calculate put and call prices for
a stretched exponential density."
"New services based on the best-effort paradigm could complement the current
deterministic services of an electronic financial exchange. Four crucial
aspects of such systems would benefit from a hybrid stance: proper use of
processing resources, bandwidth management, fault tolerance, and exception
handling. We argue that a more refined view on Quality-of-Service control for
exchange systems, in which the principal ambition of upholding a fair and
orderly marketplace is left uncompromised, would benefit all interested
parties."
"On markets with receding prices, artificial noise traders may consider
alternatives to buy-and-hold. By simulating variations of the Parrondo
strategy, using real data from the Swedish stock market, we produce first
indications of a buy-low-sell-random Parrondo variation outperforming
buy-and-hold. Subject to our assumptions, buy-low-sell-random also outperforms
the traditional value and trend investor strategies. We measure the success of
the Parrondo variations not only through their performance compared to other
kinds of strategies, but also relative to varying levels of perfect
information, received through messages within a multi-agent system of
artificial traders."
"Some roaming users need services to manipulate autonomous processes. Trading
agents running on agent trade servers are used as a case in point. We present a
solution that provides the agent owners with means to upkeeping their desktop
environment, and maintaining their agent trade server processes, via a
briefcase service."
"Currently statistical and artificial neural network methods dominate in
financial data mining. Alternative relational (symbolic) data mining methods
have shown their effectiveness in robotics, drug design and other applications.
Traditionally symbolic methods prevail in the areas with significant
non-numeric (symbolic) knowledge, such as relative location in robot
navigation. At first glance, stock market forecast looks as a pure numeric area
irrelevant to symbolic methods. One of our major goals is to show that
financial time series can benefit significantly from relational data mining
based on symbolic methods. The paper overviews relational data mining
methodology and develops this techniques for financial data mining."
"This paper presents a statistical framework for assessing wireless systems
performance using hierarchical data mining techniques. We consider WCDMA
(wideband code division multiple access) systems with two-branch STTD (space
time transmit diversity) and 1/2 rate convolutional coding (forward error
correction codes). Monte Carlo simulation estimates the bit error probability
(BEP) of the system across a wide range of signal-to-noise ratios (SNRs). A
performance database of simulation runs is collected over a targeted space of
system configurations. This database is then mined to obtain regions of the
configuration space that exhibit acceptable average performance. The shape of
the mined regions illustrates the joint influence of configuration parameters
on system performance. The role of data mining in this application is to
provide explainable and statistically valid design conclusions. The research
issue is to define statistically meaningful aggregation of data in a manner
that permits efficient and effective data mining algorithms. We achieve a good
compromise between these goals and help establish the applicability of data
mining for characterizing wireless systems performance."
"In mathematical modeling of the non-squared frequency-dependent diffusions,
also known as the anomalous diffusions, it is desirable to have a positive real
Fourier transform for the time derivative of arbitrary fractional or odd
integer order. The Fourier transform of the fractional time derivative in the
Riemann-Liouville and Caputo senses, however, involves a complex power function
of the fractional order. In this study, a positive time derivative of
fractional or odd integer order is introduced to respect the positivity in
modeling the anomalous diffusions."
"We show that, for the purpose of pricing Swaptions, the Swap rate and the
corresponding Forward rates can be considered lognormal under a single
martingale measure. Swaptions can then be priced as options on a basket of
lognormal assets and an approximation formula is derived for such options. This
formula is centered around a Black-Scholes price with an appropriate
volatility, plus a correction term that can be interpreted as the expected
tracking error. The calibration problem can then be solved very efficiently
using semidefinite programming."
"When interest rate dynamics are described by the Libor Market Model as in
BGM97, we show how some essential risk-management results can be obtained from
the dual of the calibration program. In particular, if the objetive is to
maximize another swaption's price, we show that the optimal dual variables
describe a hedging portfolio in the sense of \cite{Avel96}. In the general
case, the local sensitivity of the covariance matrix to all market movement
scenarios can be directly computed from the optimal dual solution. We also show
how semidefinite programming can be used to manage the Gamma exposure of a
portfolio."
"Standard quantitative models of the stock market predict a log-normal
distribution for stock returns (Bachelier 1900, Osborne 1959), but it is
recognised (Fama 1965) that empirical data, in comparison with a Gaussian,
exhibit leptokurtosis (it has more probability mass in its tails and centre)
and fat tails (probabilities of extreme events are underestimated). Different
attempts to explain this departure from normality have coexisted. In
particular, since one of the strong assumptions of the Gaussian model concerns
the volatility, considered finite and constant, the new models were built on a
non finite (Mandelbrot 1963) or non constant (Cox, Ingersoll and Ross 1985)
volatility. We investigate in this thesis a very recent model (Dragulescu et
al. 2002) based on a Brownian motion process for the returns, and a stochastic
mean-reverting process for the volatility. In this model, the forward
Kolmogorov equation that governs the time evolution of returns is solved
analytically. We test this new theory against different stock indexes (Dow
Jones Industrial Average, Standard and Poor s and Footsie), over different
periods (from 20 to 105 years). Our aim is to compare this model with the
classical Gaussian and with a simple Neural Network, used as a benchmark. We
perform the usual statistical tests on the kurtosis and tails of the expected
distributions, paying particular attention to the outliers. As claimed by the
authors, the new model outperforms the Gaussian for any time lag, but is
artificially too complex for medium and low frequencies, where the Gaussian is
preferable. Moreover this model is still rejected for high frequencies, at a
0.05 level of significance, due to the kurtosis, incorrectly handled."
"As in the car industry for quite some time, dynamic simulation of complete
vehicles is being practiced more and more in the development of off-road
machinery. However, specific questions arise due not only to company structure
and size, but especially to the type of product. Tightly coupled, non-linear
subsystems of different domains make prediction and optimisation of the
complete system's dynamic behaviour a challenge. Furthermore, the demand for
versatile machines leads to sometimes contradictory target requirements and can
turn the design process into a hunt for the least painful compromise. This can
be avoided by profound system knowledge, assisted by simulation-driven product
development. This paper gives an overview of joint research into this issue by
Volvo Wheel Loaders and Linkoping University on that matter, lists the results
of a related literature review and introduces the term ""operateability"". Rather
than giving detailed answers, the problem space for ongoing and future research
is examined and possible solutions are sketched."
"An analytical formula for the probability distribution of stock-market
returns, derived from the Heston model assuming a mean-reverting stochastic
volatility, was recently proposed by Dragulescu and Yakovenko in Quantitative
Finance 2002. While replicating their results, we found two significant
weaknesses in their method to pre-process the data, which cast a shadow over
the effective goodness-of-fit of the model. We propose a new method, more truly
capturing the market, and perform a Kolmogorov-Smirnov test and a Chi Square
test on the resulting probability distribution. The results raise some
significant questions for large time lags -- 40 to 250 days -- where the
smoothness of the data does not require such a complex model; nevertheless, we
also provide some statistical evidence in favour of the Heston model for small
time lags -- 1 and 5 days -- compared with the traditional Gaussian model
assuming constant volatility."
"This paper gives an overview of a reconstruction algorithm for muon events in
ATLAS experiment at CERN. After a short introduction on ATLAS Muon
Spectrometer, we will describe the procedure performed by the algorithms
Muonbox and Muonboy (last version) in order to achieve correctly the
reconstruction task. These algorithms have been developed in Fortran language
and are working in the official C++ framework Athena, as well as in stand alone
mode. A description of the interaction between Muonboy and Athena will be
given, together with the reconstruction performances (efficiency and momentum
resolution) obtained with MonteCarlo data."
"Business concepts are studied using a metamodel-based approach, using UML
2.0. The Notation Independent Business concepts metamodel is introduced. The
approach offers a mapping between different business modeling notations which
could be used for bridging BM tools and boosting the MDA approach."
"We present a C++ implementation of a fifth order semi-implicit Runge-Kutta
algorithm for solving Ordinary Differential Equations. This algorithm can be
used for studying many different problems and in particular it can be applied
for computing the evolution of any system whose Hamiltonian is known. We
consider in particular the problem of calculating the neutrino oscillation
probabilities in presence of matter interactions. The time performance and the
accuracy of this implementation is competitive with respect to the other
analytical and numerical techniques used in literature. The algorithm design
and the salient features of the code are presented and discussed and some
explicit examples of code application are given."
"An experimental server for stock trading autonomous agents is presented and
made available, together with an agent shell for swift development. The server,
written in Java, was implemented as proof-of-concept for an agent trade server
for a real financial exchange."
"We examine the problem of approximating, in the Frobenius-norm sense, a
positive, semidefinite symmetric matrix by a rank-one matrix, with an upper
bound on the cardinality of its eigenvector. The problem arises in the
decomposition of a covariance matrix into sparse factors, and has wide
applications ranging from biology to finance. We use a modification of the
classical variational representation of the largest eigenvalue of a symmetric
matrix, where cardinality is constrained, and derive a semidefinite programming
based relaxation for our problem. We also discuss Nesterov's smooth
minimization technique applied to the SDP arising in the direct sparse PCA
method."
"We compare static arbitrage price bounds on basket calls, i.e. bounds that
only involve buy-and-hold trading strategies, with the price range obtained
within a multi-variate generalization of the Black-Scholes model. While there
is no gap between these two sets of prices in the univariate case, we observe
here that contrary to our intuition about model risk for at-the-money calls,
there is a somewhat large gap between model prices and static arbitrage prices,
hence a similarly large set of prices on which a multivariate Black-Scholes
model cannot be calibrated but where no conclusion can be drawn on the presence
or not of a static arbitrage opportunity."
"The paper describes a new CNC control unit for machining centres with
learning ability and automatic intelligent generating of NC programs on the
bases of a neural network, which is built-in into a CNC unit as special device.
The device performs intelligent and completely automatically the NC part
programs only on the bases of 2D, 2,5D or 3D computer model of prismatic part.
Intervention of the operator is not needed. The neural network for milling,
drilling, reaming, threading and operations alike has learned to generate NC
programs in the learning module, which is a part of intelligent CAD/CAM system."
"The features of designing of reconstruction of the acting plant by its design
department are considered: the results of work are drawings corresponding with
the national standards; large number of the small projects for different acting
objects; variety of the types of the drawings in one project; large paper
archive. The models and methods of developing of the complex CAD system with
friend uniform environment of designing, with setting a profile of operations,
with usage of the general parts of the project, with a series of
problem-oriented subsystems are described on an example of a CAD system
TechnoCAD GlassX"
"The parametric model of build constructions and features of design operations
are described for making drawings, which are the common component of the
different parts of the projects of renovation of enterprises. The key moment of
the deep design automation is the using of so-called units in the drawings,
which are joining a visible graphic part and invisible parameters. The model
has passed check during designing of several hundreds of drawings"
"The characteristic of design works on firm at its renovation and of the
common directions of their informatization is given. The implantation of a CAD
is selected as the key direction, and the requirements to a complex CAD-system
are stated. The methods of such a CAD-system development are featured, and the
connectedness of this development with the process of integration of
information space of design department of the firm is characterized. The
experience of development and implantation of a complex CAD of renovation of
firms TechnoCAD GlassX lies in a basis of this reviewing"
"In this paper the distribution of charged particles is constructed under the
approximation of ambipolar diffusion. The results of mathematical modelling in
two-dimensional case taking into account the velocities of the system are
presented."
"In dynamic simulation of complete wheel loaders, one interesting aspect,
specific for the working task, is the momentary power distribution between
drive train and hydraulics, which is balanced by the operator.
  This paper presents the initial results to a simulation model of a human
operator. Rather than letting the operator model follow a predefined path with
control inputs at given points, it follows a collection of general rules that
together describe the machine's working cycle in a generic way. The advantage
of this is that the working task description and the operator model itself are
independent of the machine's technical parameters. Complete sub-system
characteristics can thus be changed without compromising the relevance and
validity of the simulation. Ultimately, this can be used to assess a machine's
total performance, fuel efficiency and operability already in the concept phase
of the product development process."
"In this article we study the behavior of a group of economic agents in the
context of cooperative game theory, interacting according to rules based on the
Potts Model with suitable modifications. Each agent can be thought of as
belonging to a chain, where agents can only interact with their nearest
neighbors (periodic boundary conditions are imposed). Each agent can invest an
amount &#963;_{i}=0,...,q-1. Using the transfer matrix method we study
analytically, among other things, the behavior of the investment as a function
of a control parameter (denoted &#946;) for the cases q=2 and 3. For q>3
numerical evaluation of eigenvalues and high precision numerical derivatives
are used in order to assess this information."
"Prediction and optimisation of a wheel loader's dynamic behaviour is a
challenge due to tightly coupled, non-linear subsystems of different technical
domains. Furthermore, a simulation regarding performance, efficiency, and
operability cannot be limited to the machine itself, but has to include
operator, environment, and work task. This paper presents some results of our
approach to an event-driven simulation model of a human operator. Describing
the task and the operator model independently of the machine's technical
parameters, gives the possibility to change whole sub-system characteristics
without compromising the relevance and validity of the simulation."
"This contribution is devoted to the comparison of various resampling
approaches that have been proposed in the literature on particle filtering. It
is first shown using simple arguments that the so-called residual and
stratified methods do yield an improvement over the basic multinomial
resampling approach. A simple counter-example showing that this property does
not hold true for systematic resampling is given. Finally, some results on the
large-sample behavior of the simple bootstrap filter algorithm are given. In
particular, a central limit theorem is established for the case where
resampling is performed using the residual approach."
"ReacProc is a program written in C/C++ programming language which can be used
(1) to check out of reactions describing particles interactions against
conservation laws and (2) to reduce input reaction into some canonical form. A
table with particles properties is available within ReacProc package."
"Using finite element software developed for metal cutting by Third Wave
Systems we investigate the forces involved in chatter, a self-sustained
oscillation of the cutting tool. The phenomena is decomposed into a vibrating
tool cutting a flat surface work piece, and motionless tool cutting a work
piece with a wavy surface. While cutting the wavy surface, the shearplane was
seen to oscillate in advance of the oscillation of the depth of cut, as were
the cutting, thrust, and shear plane forces. The vibrating tool was used to
investigate process damping through the interaction of the relief face of the
tool and the workpiece. Crushing forces are isolated and compared to the
contact length between the tool and workpiece. We found that the wavelength
dependence of the forces depended on the relative size of the wavelength to the
length of the relief face of the tool. The results indicate that the damping
force from crushing will be proportional to the cutting speed for short tools,
and inversely proportional for long tools."
"The COmputational MODule Integrator (COMODI) is an initiative aiming at a
component based framework, component developer tool and component repository
for scientific computing. We identify the main ingredients to a solution that
would be sufficiently appealing to scientists and engineers to consider
alternatives to their deeply rooted programming traditions. The overall
structure of the complete solution is sketched with special emphasis on the
Component Developer Tool standing at the basis of COMODI."
An introduction to numerical statistics.
"We derive tractable necessary and sufficient conditions for the absence of
buy-and-hold arbitrage opportunities in a perfectly liquid, one period market.
We formulate the positivity of Arrow-Debreu prices as a generalized moment
problem to show that this no arbitrage condition is equivalent to the positive
semidefiniteness of matrices formed by the market price of tradeable securities
and their products. We apply this result to a market with multiple assets and
basket call options."
"In this paper, we study a family of stochastic volatility processes; this
family features a mean reversion term for the volatility and a double CEV-like
exponent that generalizes SABR and Heston's models. We derive approximated
closed form formulas for the digital prices, the local and implied
volatilities. Our formulas are efficient for small maturities.
  Our method is based on differential geometry, especially small time
diffusions on riemanian spaces. This geometrical point of view can be extended
to other processes, and is very accurate to produce variate smiles for small
maturities and small moneyness."
"This paper presents the foundational ideas for a new way of modeling social
aggregation. Traditional approaches have been using network theory, and the
theory of random networks. Under that paradigm, every social agent is
represented by a node, and every social interaction is represented by a segment
connecting two nodes. Early work in family interactions, as well as more recent
work in the study of terrorist organizations, shows that network modeling may
be insufficient to describe the complexity of human social structures.
Specifically, network theory does not seem to have enough flexibility to
represent higher order aggregations, where several agents interact as a group,
rather than as a collection of pairs. The model we present here uses a well
established mathematical theory, the theory of simplicial complexes, to address
this complex issue prevalent in interpersonal and intergroup communication. The
theory enables us to provide a richer graphical representation of social
interactions, and to determine quantitative mechanisms to describe the
robustness of a social structure. We also propose a methodology to create
random simplicial complexes, with the purpose of providing a new method to
simulate computationally the creation and disgregation of social structures.
Finally, we propose several measures which could be taken and observed in order
to describe and study an actual social aggregation occurring in interpersonal
and intergroup contexts."
"In 1965, Feynman wrote of using a lattice containing one dimension of space
and one dimension of time to derive aspects of quantum mechanics. Instead of
summing the behavior of all possible paths as he did, this paper will consider
the motion of single particles within this discrete Space-Time lattice,
sometimes called Feynman's Checkerboard. This empirical approach yielded
several predicted emergent properties for a discrete Space-Time lattice, one of
which is novel and testable."
"Mathematical modelling and defining useful recommendations for construction
and regimes of exploitation for hot water solar installation with thermal
stratification is the main purpose of this work. A special experimental solar
module for hot water was build and equipped with sufficient measure apparatus.
The main concept of investigation is to optimise the stratified regime of
thermal accumulation and constructive parameters of heat exchange equipment
(heat serpentine in tank). Accumulation and heat exchange processes were
investigated by theoretical end experimental means. Special mathematical model
was composed to simulate the energy transfer in stratified tank. Computer
program was developed to solve mathematical equations for thermal accumulation
and energy exchange. Extensive numerical and experimental tests were carried
out. A good correspondence between theoretical and experimental data was
arrived. Keywords: Mathematical modelling, accumulation"
"Application of finite element method and heat conductivity transfer model for
calculation of temperature distribution in receiver for dish-Stirling
concentrating solar system is described. The method yields discretized
equations that are entirely local to the elements and provides complete
geometric flexibility. A computer program solving the finite element method
problem is created and great number of numerical experiments is carried out.
Illustrative numerical results are given for an array of triangular elements in
receiver for dish-Stirling system."
"The paper presents the methodology of modelling tooth flanks of cylindrical
gears in the Cad environment. The modelling consists in a computer simulation
of gear generation. A model of tooth flanks is an envelope curve of a family of
envelopes that originate from the rolling motion of a solid tool model in
relation to a solid model of the cylindrical gear. The surface stereometry and
topography of the tooth flanks, hobbed and chiselled by Fellows method, are
compared to their numerical models. Metrological measurements of the real gears
were carried out using a coordinated measuring machine and a two - and a
three-dimensional profilometer. A computer simulation of the gear generation
was performed in the Mechanical Desktop environment."
"We first formulate the problem of optimally scheduling air traffic low with
sector capacity constraints as a mixed integer linear program. We then use
semidefinite relaxation techniques to form a convex relaxation of that problem.
Finally, we present a randomization algorithm to further improve the quality of
the solution. Because of the specific structure of the air traffic flow
problem, the relaxation has a single semidefinite constraint of size dn where d
is the maximum delay and n the number of flights."
"The threats to security of the information originating owing to involuntary
operations of the users of a CAD, and methods of its protection implemented in
a complex CAD system of renovation of firms are considered: rollback, autosave,
automatic backup copying and electronic subscript. The specificity of a complex
CAD is reflected in necessity of rollback and autosave both of the draw and the
parametric representations of its parts, which are the information models of
the problem-oriented extensions of the CAD"
"Progressing methods of drawings creating automation is discussed on the basis
of so-called modules containing parametric representation of a part of the
drawing and the geometrical elements. The stages of evolution of modular
technology of automation of engineering are describing alternatives of applying
of moduluss for simple association of elements of the drawing without
parametric representation with an opportunity of its commenting, for graphic
symbols creating in the schemas of automation and drawings of pipelines, for
storage of the specific properties of elements, for development of the
specialized parts of the project: the axonometric schemas, profiles of outboard
pipe networks etc."
"The main ideas, data structures, structure and realization of operations with
them in environment of development of the programs of parametric creating of
the drawings are considered for the needs of the automated design engineering
system of renovation of the enterprises. The example of such program and
example of application of this environment for creating the drawing of the base
for equipment in CAD-system TechnoCAD GlassX are presented"
"The experience of automation of the specifications making of the projects of
renovation of the industrial enterprises is described, being based on the
special modules in the drawing containing the visible image and additional
parameters, and electronic catalogues"
"According to the requirements of the Russian standards, the automation
schemes are necessary practically in each project of renovation of industrial
buildings and facilities, in which any technological processes are realized.
The model representations of the automation schemes in CAD-system TechnoCAD
GlassX are described. The models follow a principle ""to exclude a repeated
input operations"""
"A framework for simulation of dynamics of mechanical aggregates has been
developed. This framework enables us to build model of aggregate from models of
its parts. Framework is a part of universal framework for science and
engineering."
"It is well known that the Black-Scholes-Merton model suffers from several
deficiencies. Jump-diffusion and Levy models have been widely used to partially
alleviate some of the biases inherent in this classical model. Unfortunately,
the resulting pricing problem requires solving a more difficult partial-integro
differential equation (PIDE) and although several approaches for solving the
PIDE have been suggested in the literature, none are entirely satisfactory. All
treat the integral and diffusive terms asymmetrically and are difficult to
extend to higher dimensions. We present a new, efficient algorithm, based on
transform methods, which symmetrically treats the diffusive and integrals
terms, is applicable to a wide class of path-dependent options (such as
Bermudan, barrier, and shout options) and options on multiple assets, and
naturally extends to regime-switching Levy models. We present a concise study
of the precision and convergence properties of our algorithm for several
classes of options and Levy models and demonstrate that the algorithm is
second-order in space and first-order in time for path-dependent options."
"In this paper we outline several points of view on the interplay between
discrete and continuous wavelet transforms; stressing both pure and applied
aspects of both. We outline some new links between the two transform
technologies based on the theory of representations of generators and
relations. By this we mean a finite system of generators which are represented
by operators in Hilbert space. We further outline how these representations
yield sub-band filter banks for signal and image processing algorithms."
"This paper concerns the use of neural networks for predicting the residual
life of machines and components. In addition, the advantage of using
condition-monitoring data to enhance the predictive capability of these neural
networks was also investigated. A number of neural network variations were
trained and tested with the data of two different reliability-related datasets.
The first dataset represents the renewal case where the failed unit is repaired
and restored to a good-as-new condition. Data was collected in the laboratory
by subjecting a series of similar test pieces to fatigue loading with a
hydraulic actuator. The average prediction error of the various neural networks
being compared varied from 431 to 841 seconds on this dataset, where test
pieces had a characteristic life of 8,971 seconds. The second dataset was
collected from a group of pumps used to circulate a water and magnetite
solution within a plant. The data therefore originated from a repaired system
affected by reliability degradation. When optimized, the multi-layer perceptron
neural networks trained with the Levenberg-Marquardt algorithm and the general
regression neural network produced a sum-of-squares error within 11.1% of each
other. The potential for using neural networks for residual life prediction and
the advantage of incorporating condition-based data into the model were proven
for both examples."
"This paper compares two neural network input selection schemes, the Principal
Component Analysis (PCA) and the Automatic Relevance Determination (ARD) based
on Mac-Kay's evidence framework. The PCA takes all the input data and projects
it onto a lower dimension space, thereby reduc-ing the dimension of the input
space. This input reduction method often results with parameters that have
significant influence on the dynamics of the data being diluted by those that
do not influence the dynamics of the data. The ARD selects the most relevant
input parameters and discards those that do not contribute significantly to the
dynamics of the data being modelled. The ARD sometimes results with important
input parameters being discarded thereby compromising the dynamics of the data.
The PCA and ARD methods are implemented together with a Multi-Layer-Perceptron
(MLP) network for fault identification in structures and the performance of the
two methods is as-sessed. It is observed that ARD and PCA give similar
accu-racy levels when used as input-selection schemes. There-fore, the choice
of input-selection scheme is dependent on the nature of the data being
processed."
"This paper proposes the response surface method for finite element model
updating. The response surface method is implemented by approximating the
finite element model surface response equation by a multi-layer perceptron. The
updated parameters of the finite element model were calculated using genetic
algorithm by optimizing the surface response equation. The proposed method was
compared to the existing methods that use simulated annealing or genetic
algorithm together with a full finite element model for finite element model
updating. The proposed method was tested on an unsymmetri-cal H-shaped
structure. It was observed that the proposed method gave the updated natural
frequen-cies and mode shapes that were of the same order of accuracy as those
given by simulated annealing and genetic algorithm. Furthermore, it was
observed that the response surface method achieved these results at a
computational speed that was more than 2.5 times as fast as the genetic
algorithm and a full finite element model and 24 times faster than the
simulated annealing."
"Condition monitoring techniques are described in this chapter. Two aspects of
condition monitoring process are considered: (1) feature extraction; and (2)
condition classification. Feature extraction methods described and implemented
are fractals, Kurtosis and Mel-frequency Cepstral Coefficients. Classification
methods described and implemented are support vector machines (SVM), hidden
Markov models (HMM), Gaussian mixture models (GMM) and extension neural
networks (ENN). The effectiveness of these features were tested using SVM, HMM,
GMM and ENN on condition monitoring of bearings and are found to give good
results."
"The paper makes a thermal predictive analysis of the electric power system
security for a day ahead. This predictive analysis is set as a thermal
computation of the expected security. This computation is obtained by
cointegrating the daily electric power systen load and the weather, by finding
the daily electric power system thermodynamics and by introducing tests for
this thermodynamics. The predictive analysis made shows the electricity
consumers' wisdom."
"We approximate the price of the American put for jump diffusions by a
sequence of functions, which are computed iteratively. This sequence converges
to the price function uniformly and exponentially fast. Each element of the
approximating sequence solves an optimal stopping problem for geometric
Brownian motion, and can be numerically computed using the classical finite
difference methods. We prove the convergence of this numerical scheme and
present examples to illustrate its performance."
"The method of location and spectral estimation of weak signals on a noise
background is being considered. The method is based on the optimized on order
and noise dispersion autoregressive model of a sought signal. A new approach of
model order determination is being offered. Available estimation of the noise
dispersion is close to the real one. The optimized model allows to define
function of empirical data spectral and dynamic features changes. The analysis
of the signal as dynamic invariant in respect of the linear shift
transformation yields the function of model consistency. Use of these both
functions enables to detect short-time and nonstationary wave packets at signal
to noise ratio as from -20 dB and above."
"In this note, we develop stock option price approximations for a model which
takes both the risk o default and the stochastic volatility into account. We
also let the intensity of defaults be influenced by the volatility. We show
that it might be possible to infer the risk neutral default intensity from the
stock option prices. Our option price approximation has a rich implied
volatility surface structure and fits the data implied volatility well. Our
calibration exercise shows that an effective hazard rate from bonds issued by a
company can be used to explain the implied volatility skew of the implied
volatility of the option prices issued by the same company."
"We construct a sequence of functions that uniformly converge (on compact
sets) to the price of Asian option, which is written on a stock whose dynamics
follows a jump diffusion, exponentially fast. Each of the element in this
sequence solves a parabolic partial differen- tial equation (not an
integro-differential equation). As a result we obtain a fast numerical
approximation scheme whose accuracy versus speed characteristics can be
controlled. We analyze the performance of our numerical algorithm on several
examples."
"Given multivariate time series, we study the problem of forming portfolios
with maximum mean reversion while constraining the number of assets in these
portfolios. We show that it can be formulated as a sparse canonical correlation
analysis and study various algorithms to solve the corresponding sparse
generalized eigenvalue problems. After discussing penalized parameter
estimation procedures, we study the sparsity versus predictability tradeoff and
the impact of predictability in various markets."
"During 1993-94 Venezuela experienced a severe banking crisis which ended up
with 18 commercial banks intervened by the government. Here we develop an early
warning system for detecting credit related bankruptcy through discriminant
functions developed on financial and macroeconomic data predating the crisis. A
robustness test performed on these functions shows high precision in error
estimation. The model calibrated on pre-crisis data could detect abnormal
financial tension in the late Banco Capital many months before it was
intervened and liquidated."
"We consider the problem of Adverse Selection and optimal derivative design
within a Principal-Agent framework. The principal's income is exposed to
non-hedgeable risk factors arising, for instance, from weather or climate
phenomena. She evaluates her risk using a coherent and law invariant risk
measure and tries minimize her exposure by selling derivative securities on her
income to individual agents. The agents have mean-variance preferences with
heterogeneous risk aversion coefficients. An agent's degree of risk aversion is
private information and hidden to the principal who only knows the overall
distribution. We show that the principal's risk minimization problem has a
solution and illustrate the effects of risk transfer on her income by means of
two specific examples. Our model extends earlier work of Barrieu and El Karoui
(2005) and Carlier, Ekeland and Touzi (2007)."
"Making a product conform to the functional requirements indicated by the
customer suppose to be able to manage the manufacturing process chosen to
realise the parts. A simulation step is generally performed to verify that the
expected generated deviations fit with these requirements. It is then necessary
to assess the actual deviations of the process in progress. This is usually
done by the verification of the conformity of the workpiece to manufacturing
tolerances at the end of each set-up. It is thus necessary to determine these
manufacturing tolerances. This step is called ""manufacturing tolerance
synthesis"". In this paper, a numerical method is proposed to perform 3D
manufacturing tolerances synthesis. This method uses the result of the
numerical analysis of tolerances to determine influent mall displacement of
surfaces. These displacements are described by small displacements torsors. An
algorithm is then proposed to determine suitable ISO manufacturing tolerances."
"This paper covers a massive acceleration of Monte-Carlo based pricing method
for financial products and financial derivatives. The method is applicable in
risk management settings, where a financial product has to be priced under a
number of potential future scenarios. Instead of starting a separate nested
Monte Carlo simulation for each scenario under consideration, the new method
covers the utilization of very few representative nested simulations and
estimating the product prices at each scenario by a smoothing method based on
the state-space. This smoothing technique can be e.g. non-parametric regression
or kernel smoothing."
"This paper presents an optimization technique for the multi-pass face milling
process. Genetic algorithm (GA) is used to obtain the optimum cutting
parameters by minimizing the unit production cost for a given amount of
material removal. Cutting speed, feed and depth of cut for the finish and rough
passes are the cutting parameters. An equal depth of cut for roughing passes
has been considered. A lookup table containing the feasible combinations of
depth of cut in finish and rough passes is generated so as to reduce the number
of variables by one. The resulting mixed integer nonlinear optimization problem
is solved in a single step using GA. The entire technique is demonstrated in a
case study. Post optimality analysis of the example problem is done to develop
a strategy for optimizing without running GA again for different values of
total depth of cut."
"We consider the basic features of complex dynamical and control systems.
Special attention is paid to the problems of synthesis of dynamical models of
complex systems, construction of efficient control models, and to the
development of simulation techniques. We propose an approach to the synthesis
of dynamic models of complex systems that integrates expert knowledge with the
process of modeling. A set-theoretic model of complex system is defined and
briefly analyzed. A mathematical model of complex dynamical system with
control, based on aggregate description, is also proposed. The structure of the
model is described, and architecture of computer simulation system is
presented, requirements to and components of computer simulation systems are
analyzed."
"Coupled human balancing tasks are investigated based on both pseudo-neural
controllers characterized by time-delayed feedback with random gain and natural
human balancing tasks. It is shown numerically that, compared to single
balancing tasks, balancing tasks coupled by mechanical structures exhibit
enhanced stability against balancing errors in terms of both amplitude and
velocity and also improve the tracking ability of the controllers. We then
perform an experiment in which numerical pseudo-neural controllers are replaced
with natural human balancing tasks carried out using computer mice. The results
reveal that the coupling structure generates asymmetric tracking abilities in
subjects whose tracking abilities are nearly symmetric in their single
balancing tasks."
"In this study, we investigate the appeared complexity of two-phase flow
(air-water) in a heterogeneous soil where the supposed porous media is
non-deformable media which is under the time-dependent gas pressure. After
obtaining of governing equations and considering the capillary
pressure-saturation and permeability functions, the evolution of the models
unknown parameters were obtained. In this way, using COMSOL (FEMLAB) and fluid
flow-script Module, the role of heterogeneity in intrinsic permeability was
analysed. Also, the evolution of relative permeability of wetting and
non-wetting fluid, capillary pressure and other parameters were elicited."
"The data describing a process of echo-image formation in bottlenose dolphin
sonar perception were accumulated in our experimental explorations. These data
were formalized mathematically and used in the computational model, comparative
testing of which in echo-discrimination tasks revealed no less capabilities
then those of bottlenose dolphins."
"Bioinformatics is a new discipline that addresses the need to manage and
interpret the data that in the past decade was massively generated by genomic
research. This discipline represents the convergence of genomics, biotechnology
and information technology, and encompasses analysis and interpretation of
data, modeling of biological phenomena, and development of algorithms and
statistics. This article presents an introduction to bioinformatics"
"The operations drip and mate considered in (mem)brane computing resemble the
operations cut and recombination well known from DNA computing. We here
consider sets of vesicles with multisets of objects on their outside membrane
interacting by drip and mate in two different setups: in test tube systems, the
vesicles may pass from one tube to another one provided they fulfill specific
constraints; in tissue-like P systems, the vesicles are immediately passed to
specified cells after having undergone a drip or mate operation. In both
variants, computational completeness can be obtained, yet with different
constraints for the drip and mate operations."
"In this paper we present an algorithm for optical phase evaluation based on
the wavelet transform technique. The main advantage of this method is that it
requires only one fringe pattern. This algorithm is based on the use of a
second {\pi}/2 phase shifted fringe pattern where it is calculated via the
Hilbert transform. To test its validity, the algorithm was used to demodulate a
simulated fringe pattern giving the phase distribution with a good accuracy."
"Perfusion analysis computes blood flow parameters (blood volume, blood flow,
mean transit time) from the observed flow of contrast agent, passing through
the patient's vascular system. Perfusion deconvolution has been widely accepted
as the principal numerical tool for perfusion analysis, and is used routinely
in clinical applications. This extensive use of perfusion in clinical
decision-making makes numerical stability and robustness of perfusion
computations vital for accurate diagnostics and patient safety. The main goal
of this paper is to propose a novel approach for validating numerical
properties of perfusion algorithms. The approach is based on Perfusion
Linearity Property (PLP), which we find in perfusion deconvolution, as well as
in many other perfusion techniques. PLP allows one to study perfusion values as
weighted averages of the original imaging data. This, in turn, uncovers hidden
problems with the existing deconvolution techniques, and may be used to suggest
more reliable computational approaches and methodology."
"This paper presents a set of robust PI tuning formulae for a first order plus
dead time process using particle swarm optimization. Also, tuning formulae for
an integrating process with dead time, which is a special case of a first order
plus dead time process, is given. The design problem considers three essential
requirements of control problems, namely load disturbance rejection, setpoint
regulation and robustness of closed-loop system against model uncertainties.
The primary design goal is to optimize load disturbance rejection. Robustness
is guaranteed by requiring that the maximum sensitivity is less than or equal
to a specified value. In the first step, PI controller parameters are
determined such that the IAE criterion to a load disturbance step is minimized
and the robustness constraint on maximum sensitivity is satisfied. Using a
structure with two degrees of freedom which introduces an extra parameter, the
setpoint weight, good setpoint regulation is achieved in the second step. The
main advantage of the proposed method is its simplicity. Once the equivalent
first order plus dead time model is determined, the PI parameters are
explicitly given by a set of tuning formulae. In order to show the performance
and effectiveness of the proposed tuning formulae, they are applied to three
simulation examples."
"Optimal selection of interdependent IT Projects for implementation in multi
periods has been challenging in the framework of real option valuation. This
paper presents a mathematical optimization model for multi-stage portfolio of
IT projects. The model optimizes the value of the portfolio within a given
budgetary and sequencing constraints for each period. These sequencing
constraints are due to time wise interdependencies among projects. A
Metaheuristic approach is well suited for solving this kind of a problem
definition and in this paper a genetic algorithm model has been proposed for
the solution. This optimization model and solution approach can help IT
managers taking optimal funding decision for projects prioritization in
multiple sequential periods. The model also gives flexibility to the managers
to generate alternative portfolio by changing the maximum and minimum number of
projects to be implemented in each sequential period."
"Appropriate ranking algorithms and incentive mechanisms are essential to the
creation of high-quality information by users of a social network. However,
evaluating such mechanisms in a quantifiable way is a difficult problem.
Studies of live social networks of limited utility, due to the subjective
nature of ranking and the lack of experimental control. Simulation provides a
valuable alternative: insofar as the simulation resembles the live social
network, fielding a new algorithm within a simulated network can predict the
effect it will have on the live network. In this paper, we propose a simulation
model based on the actor-conceptinstance model of semantic social networks,
then we evaluate the model against a number of common ranking algorithms.We
observe their effects on information creation in such a network, and we extend
our results to the evaluation of generic ranking algorithms and incentive
mechanisms."
"This paper shows that not every scan cell contributes equally to the power
consumption during scan based test. The transitions at some scan cells cause
more toggles at the internal signal lines of a circuit than the transitions at
other scan cells. Hence the transitions at these scan cells have a larger
impact on the power consumption during test application. These scan cells are
called power sensitive scan cells.A verilog based approach is proposed to
identify a set of power sensitive scan cells. Additional hardware is added to
freeze the outputs of power sensitive scan cells during scan shifting in order
to reduce the shift power consumption.when multiple scan chain is incorporated
along with freezing the power sensitive scan cell,over all power during testing
can be reduced to a larger extend."
"There are proposed models of contracts, technological equipment and gas
networks and methods of their optimization. The flow in network undergoes
restrictions of contracts and equipment to be operated. The values of sources
and sinks are provided by contracts. The contract models represent (sub-)
networks. The simplest contracts represent either nodes or edges. Equipment is
modeled by edges. More sophisticated equipment is represented by sub-networks.
Examples of such equipment are multi-poles and compressor stations with many
entries and exits. The edges can be of different types corresponding to
equipment and contracts. On such edges, there are given systems of equation and
inequalities simulating the contracts and equipment. On this base, the methods
proposed that allow: calculation and control of contract values for booking on
future days and for accounting of sales and purchases; simulation and
optimization of design and of operation of gas networks. These models and
methods are implemented in software systems ACCORD and Graphicord as well as in
the distributed control system used by Wingas, Germany. As numerical example,
the industrial computations are presented."
"A projection-based immersed boundary method is dominated by sparse linear
algebra routines. Using the open-source Cusp library, we observe a speedup
(with respect to a single CPU core) which reflects the constraints of a
bandwidth-dominated problem on the GPU. Nevertheless, GPUs offer the capacity
to solve large problems on commodity hardware. This work includes validation
and a convergence study of the GPU-accelerated IBM, and various optimizations."
"The prediction of thermo-mechanical behaviour of heterogeneous materials such
as heat and moisture transport is strongly influenced by the uncertainty in
parameters. Such materials occur e.g. in historic buildings, and the durability
assessment of these therefore needs a reliable and probabilistic simulation of
transport processes, which is related to the suitable identification of
material parameters. In order to include expert knowledge as well as
experimental results, one can employ an updating procedure such as Bayesian
inference. The classical probabilistic setting of the identification process in
Bayes's form requires the solution of a stochastic forward problem via
computationally expensive sampling techniques, which makes the method almost
impractical. In this paper novel stochastic computational techniques such as
the stochastic Galerkin method are applied in order to accelerate the updating
procedure. The idea is to replace the computationally expensive forward
simulation via the conventional finite element (FE) method by the evaluation of
a polynomial chaos expansion (PCE). Such an approximation of the FE model for
the forward simulation perfectly suits the Bayesian updating. The presented
uncertainty updating techniques are applied to the numerical model of coupled
heat and moisture transport in heterogeneous materials with spatially varying
coefficients defined by random fields."
"Recent papers in the field of Finite Element Model (FEM) updating have
highlighted the benefits of Bayesian techniques. The Bayesian approaches are
designed to deal with the uncertainties associated with complex systems, which
is the main problem in the development and updating of FEMs. This paper
highlights the complexities and challenges of implementing any Bayesian method
when the analysis involves a complicated structural dynamic model. In such
systems an analytical Bayesian formulation might not be available in an
analytic form; therefore this leads to the use of numerical methods, i.e.
sampling methods. The main challenge then is to determine an efficient sampling
of the model parameter space. In this paper, three sampling techniques, the
Metropolis-Hastings (MH) algorithm, Slice Sampling and the Hybrid Monte Carlo
(HMC) technique, are tested by updating a structural beam model. The efficiency
and limitations of each technique is investigated when the FEM updating problem
is implemented using the Bayesian Approach. Both MH and HMC techniques are
found to perform better than the Slice sampling when Young's modulus is chosen
as the updating parameter. The HMC method gives better results than MH and
Slice sampling techniques, when the area moment of inertias and section areas
are updated."
"The problem of path generation for the spherical 4R mechanism is solved using
the Differential Evolution algorithm (DE). Formulas for the spherical geodesics
are employed in order to obtain the parametric equation for the generated
trajectory. Direct optimization of the objective function gives the solution to
the path generation task without prescribed timing. Therefore, there is no need
to separate this task into two stages to make the optimization. Moreover, the
order defect problem can be solved without difficulty by means of manipulations
of the individuals in the DE algorithm. Two examples of optimum synthesis
showing the simplicity and effectiveness of this approach are included."
"This study examines the effectiveness of HMA overlay design strategies for
the purpose of controlling the development of reflective cracking. A parametric
study was conducted using a 3D Finite Element (FE) model of a rigid pavement
section including Linear Viscoelastic (LVE) material properties for the Hot Mix
Asphalt (HMA) overlay and non-uniform tire-pavement contact stresses. Several
asphalt mixtures were tested in the surface, intermediate, and leveling course
of the HMA overlay. Results obtained show that no benefits can be anticipated
by using either Polymer-Modified (PM) or Dense-Graded (DG) mixtures instead of
Standard Binder (SB) mixtures in the surface or intermediate course. For the
leveling course, the use of a PM asphalt binder was found beneficial in terms
of mitigating reflective cracking. As compared to the SB mix, the use of PM
asphalt mixture in the leveling course reduced the level of longitudinal
tensile stress at the bottom of the HMA overlay above the PCC joint by
approximately 30%."
"In this paper the problem of finding the sparsest (i.e., minimum cardinality)
critical $k$-tuple including one arbitrarily specified measurement is
considered. The solution to this problem can be used to identify weak points in
the measurement set, or aid the placement of new meters. The critical $k$-tuple
problem is a combinatorial generalization of the critical measurement
calculation problem. Using topological network observability results, this
paper proposes an efficient and accurate approximate solution procedure for the
considered problem based on solving a minimum-cut (Min-Cut) problem and
enumerating all its optimal solutions. It is also shown that the sparsest
critical $k$-tuple problem can be formulated as a mixed integer linear
programming (MILP) problem. This MILP problem can be solved exactly using
available solvers such as CPLEX and Gurobi. A detailed numerical study is
presented to evaluate the efficiency and the accuracy of the proposed Min-Cut
and MILP calculations."
"This contribution presents a simple Finite Element model aimed at efficient
simulation of layered glass units. The adopted approach is based on considering
independent kinematics of each layer, tied together via Lagrange multipliers.
Validation and verification of the resulting model against independent data
demonstrate its accuracy, showing its potential for generalization towards more
complex problems."
"This paper presents a methodology and software tools for parametric design of
complex architectural objects, called digital or algorithmic forms. In order to
provide a flexible tool, the proposed design philosophy involves two open
source utilities Donkey and MIDAS written in Grasshopper algorithm editor and
C++, respectively, that are to be linked with a scripting-based architectural
modellers Rhinoceros, IntelliCAD and the open source Finite Element solver
OOFEM. The emphasis is put on the mechanical response in order to provide
architects with a consistent learning framework and an insight into structural
behaviour of designed objects. As demonstrated on three case studies, the
proposed modular solution is capable of handling objects of considerable
structural complexity, thereby accelerating the process of finding procedural
design parameters from orders of weeks to days or hours."
"We describe a theoretically-motivated algorithm for extrapolation of antenna
radiation patterns from a small number of measurements. This algorithm exploits
constraints on the antenna's underlying design to avoid ambiguities, but is
sufficiently general to address many different antenna types. A theoretical
basis for the robustness of this algorithm is developed, and its performance is
verified in simulation using a number of popular antenna designs."
"The study focuses on the applicability of system identification to identify
building and system dynamics for climate control design. The main problem
regarding the simulation of the dynamic response of a building using building
simulation software is that (1) the simulation of a large complex building is
time consuming, and (2) simulation results often lack information regarding
fast dynamic behaviour (in the order of seconds), since most software uses a
discrete time step, usually fixed to one hour. The first objective is to study
the applicability of system identification to reduce computing time for the
simulation of large complex buildings. The second objective is to research the
applicability of system identification to identify building dynamics based on
discrete time data (one hour) for climate control design. The study illustrates
that system identification is applicable for the identification of building
dynamics with a frequency that is smaller as the maximum sample frequency as
used for identification. The research shows that system identification offers
good perspectives for the modelling of heat, air and moisture processes in a
building. The main advantages of system identification models compared to the
modelling of building dynamics using building simulation software are, that (1)
the computing time is reduced significantly, and (2) system identification
models run in a MATLAB environment, in which many building simulation tools
have been developed"
"We present a novel angular fingerprinting algorithm for detecting changes in
the direction of rotation of a target with a monostatic, stationary sonar
platform. Unlike other approaches, we assume that the target's centroid is
stationary, and exploit doppler multipath signals to resolve the otherwise
unavoidable ambiguities that arise. Since the algorithm is based on an
underlying differential topological theory, it is highly robust to distortions
in the collected data. We demonstrate performance of this algorithm
experimentally, by exhibiting a pulsed doppler sonar collection system that
runs on a smartphone. The performance of this system is sufficiently good to
both detect changes in target rotation direction using angular fingerprints,
and also to form high-resolution inverse synthetic aperature images of the
target."
"Manual for Christhin 0.1.36 Christhin (Chromatography Riser Thin) is software
developed for the quantitative analysis of data obtained from thin-layer
chromatographic techniques (TLC). Once installed on your computer, the program
is very easy to use, and provides data quickly and accurately. This manual
describes the program, and reading should be enough to use it properly."
"This user manual has been written to describe the open source code WM to be
distributed associated with a research article submitted to the information
technology journal 45001-ITJ-ANSI, entitled: ""Maintenance and Reengineering of
software: Creating a Visual C++ Graphical User Interface to Perform Specific
Tasks Related to Soil Structure Interaction in Poroelastic Soil""."
"The indoor climate conditions of monumental buildings are very important for
the conservation of these objects. Simplified models with physical meaning are
desired that are capable of simulating temperature and relative humidity. In
this paper we research state-space models as methodology for the inverse
modeling of climate responses of unheated monumental buildings. It is concluded
that this approach is very promising for obtaining physical models and
parameters of indoor climate responses. Furthermore state space models can be
simulated very efficiently: the simulation duration time of a 100 year hourly
based period take less than a second on an ordinary computer."
"Vibration measurements have been used to reliably diagnose performance
problems in machinery and related mechanical products. A vibration data
collector can be used effectively to measure and analyze the machinery
vibration content in gearboxes, engines, turbines, fans, compressors, pumps and
bearings. Ideally, a machine will have little or no vibration, indicating that
the rotating components are appropriately balanced, aligned, and well
maintained. Quick analysis and assessment of the vibration content can lead to
fault diagnosis and prognosis of a machine's ability to continue running. The
aim of this research used vibration measurements to pinpoint mechanical defects
such as (unbalance, misalignment, resonance, and part loosening), consequently
diagnosis all necessary process for engineers and technicians who desire to
understand the vibration that exists in structures and machines.
  Keywords- vibration data collectors; analysis software; rotating components."
"Regenerated (FRF curves), synthesis of (FRF) curves there are two main
requirement in the form of response model, The first being that of regenerating
""Theoretical"" curve for the frequency response function actually measured and
analysis and the second being that of synthesising the other functions which
were not measured,(FRF) that isolates the inherent dynamic properties of a
mechanical structure. Experimental modal parameters (frequency, damping, and
mode shape) are also obtained from a set of (FRF) measurements. The (FRF)
describes the input-output relationship between two points on a structure as a
function of frequency. Therefore, an (FRF) is actually defined between a single
input DOF (point & direction), and a single output (DOF), although the FRF was
previously defined as a ratio of the Fourier transforms of an output and input
signal. In this paper we detection FRF curve using Nyquist plot under
gyroscopic effect in revolving structure using computer smart office software.
  Keywords - FRF curve; modal test; Nyquist plot; software engineering;
gyroscopic effect; smart office."
"In this paper we present recent results on parametric analysis of biological
models. The underlying method is based on the algorithms for computing
trajectory sets of hybrid systems with polynomial dynamics. The method is then
applied to two case studies of biological systems: one is a cardiac cell model
for studying the conditions for cardiac abnormalities, and the second is a
model of insect nest-site choice."
"The analysis and interpretation of relationships between biological molecules
is done with the help of networks. Networks are used ubiquitously throughout
biology to represent the relationships between genes and gene products. Network
models have facilitated a shift from the study of evolutionary conservation
between individual gene and gene products towards the study of conservation at
the level of pathways and complexes. Recent work has revealed much about
chemical reactions inside hundreds of organisms as well as universal
characteristics of metabolic networks, which shed light on the evolution of the
networks. However, characteristics of individual metabolites have been
neglected in this network. The current paper provides an overview of
bioinformatics software used in visualization of biological networks using
proteomic data, their main functions and limitations of the software."
"In this study, we consider the use of seismic sensors for footstep
localization in indoor environments. A popular strategy of localization is to
use the measured differences in arrival times of source signals at multiple
pairs of receivers. In the literature, most algorithms that are based on time
differences of arrival (TDOA) assume that the propagation velocity is a
constant as a function of the source position, which is valid for air
propagation or even for narrow band signals. However a bounded medium such as a
concrete slab (encountered in indoor environement) is usually dispersive and
damped. In this study, we demonstrate that under such conditions, the concrete
slab can be assimilated to a thin plate; considering a Kelvin-Voigt damping
model, we introduce the notion of {\em perceived propagation velocity}, which
decreases when the source-sensor distance increases. This peculiar behaviour
precludes any possibility to rely on existing localization methods in indoor
environment. Therefore, a new localization algorithm that is adapted to a
damped and dispersive medium is proposed, using only on the sign of the
measured TDOA (SO-TDOA). A simulation and some experimental results are
included, to define the performance of this SO-TDOA algorithm."
"Protein sequence classification involves feature selection for accurate
classification. Popular protein sequence classification techniques involve
extraction of specific features from the sequences. Researchers apply some
well-known classification techniques like neural networks, Genetic algorithm,
Fuzzy ARTMAP,Rough Set Classifier etc for accurate classification. This paper
presents a review is with three different classification models such as neural
network model, fuzzy ARTMAP model and Rough set classifier model. This is
followed by a new technique for classifying protein sequences. The proposed
model is typically implemented with an own designed tool and tries to reduce
the computational overheads encountered by earlier approaches and increase the
accuracy of classification"
"Modern progress in artificial intelligence permits to realize algorithms of
adaptation for critical events (in addition to ERP). A production emergence, an
appearance of new competitive goods, a major change in financial state of
partners, a radical change in exchange rate, a change in custom and tax
legislation, a political and energy crisis, an ecocatastrophe can lead up to a
decrease of profit or bankruptcy of enterprise. Therefore it is necessary to
assess a probability of threat and to take preventive actions. If a critical
event took place, one must estimate restoration expenses and possible
consequences as well as to prepare appropriate propositions. This is provided
using modern methods of diagnostics, prediction, and decision making as well as
an inference engine and semantic analysis. Mathematical methods in use are
called in algorithms of adaptation automatically. Because the enterprise is a
complex system, to overcome complexity of control it is necessary to apply
semantic representations. Such representations are formed from descriptions of
events, facts, persons, organizations, goods, operations, scripts on a natural
language. Semantic representations permit as well to formulate actual problems
and to find ways to resolve these problems."
"It is widely accepted that the immune system undergoes age-related changes
correlating with increased disease in the elderly. T cell subsets have been
implicated. The aim of this work is firstly to implement and validate a
simulation of T regulatory cell (Treg) dynamics throughout the lifetime, based
on a model by Baltcheva. We show that our initial simulation produces an
inversion between precursor and mature Treys at around 20 years of age, though
the output differs significantly from the original laboratory dataset.
Secondly, this report discusses development of the model to incorporate new
data from a cross-sectional study of healthy blood donors addressing balance
between Treys and Th17 cells with novel markers for Treg. The potential for
simulation to add insight into immune aging is discussed."
"When planning to change operations at ports there are two key stake holders
with very different interests involved in the decision making processes. Port
operators are attentive to their standards, a smooth service flow and economic
viability while border agencies are concerned about national security. The time
taken for security checks often interferes with the compliance to service
standards that port operators would like to achieve. Decision support tools as
for example Cost-Benefit Analysis or Multi Criteria Analysis are useful helpers
to better understand the impact of changes to a system. They allow
investigating future scenarios and helping to find solutions that are
acceptable for all parties involved in port operations. In this paper we
evaluate two different modelling methods, namely scenario analysis and discrete
event simulation. These are useful for driving the decision support tools (i.e.
they provide the inputs the decision support tools require). Our aims are, on
the one hand, to guide the reader through the modelling processes and, on the
other hand, to demonstrate what kind of decision support information one can
obtain from the different modelling methods presented."
"Simulation is a well established what-if scenario analysis tool in
Operational Research (OR). While traditionally Discrete Event Simulation (DES)
and System Dynamics Simulation (SDS) are the predominant simulation techniques
in OR, a new simulation technique, namely Agent-Based Simulation (ABS), has
emerged and is gaining more attention. In our research we focus on discrete
simulation methods (i.e. DES and ABS). The contribution made by this paper is
the comparison of DES and combined DES/ABS for modelling human reactive and
different level of detail of human proactive behaviour in service systems. The
results of our experiments show that the level of proactiveness considered in
the model has a big impact on the simulation output. However, there is not a
big difference between the results from the DES and the combined DES/ABS
simulation models. Therefore, for service systems of the type we investigated
we would suggest to use DES as the preferred analysis tool."
"Adverse drug reaction (ADR) is widely concerned for public health issue. ADRs
are one of most common causes to withdraw some drugs from market. Prescription
event monitoring (PEM) is an important approach to detect the adverse drug
reactions. The main problem to deal with this method is how to automatically
extract the medical events or side effects from high-throughput medical data,
which are collected from day to day clinical practice. In this study we propose
an original approach to detect the ADRs using feature matrix and feature
selection. The experiments are carried out on the drug Pravastatin. Major side
effects for the drug are detected. The detected ADRs are based on computerized
method, further investigation is needed."
"Some common systems modelling and simulation approaches for immune problems
are Monte Carlo simulations, system dynamics, discrete-event simulation and
agent-based simulation. These methods, however, are still not widely adopted in
immunology research. In addition, to our knowledge, there is few research on
the processes for the development of simulation models for the immune system.
Hence, for this work, we have two contributions to knowledge. The first one is
to show the importance of systems simulation to help immunological research and
to draw the attention of simulation developers to this research field. The
second contribution is the introduction of a quick guide containing the main
steps for modelling and simulation in immunology, together with challenges that
occur during the model development. Further, this paper introduces an example
of a simulation problem, where we test our guidelines."
"Modelling and simulating the traffic of heavily used but secure environments
such as seaports and airports is of increasing importance. This paper discusses
issues and problems that may arise when extending an existing microsimulation
strategy. We also discuss how extensions of these simulations can aid planners
with optimal physical and operational feedback. Conclusions are drawn about how
microsimulations can be moved forward as a robust planning tool for the 21st
century."
"We introduce a robust numerical technique to verify the causality of sampled
scattering parameters given on a finite bandwidth. The method is based on a
filtered Fourier transform and includes a rigorous estimation of the errors
caused by missing out-of-band samples. Compared to existing techniques, the
method is simpler to implement and provides a useful insight on the time-domain
characteristics of the detected violation. Through an applicative example, we
shows its usefulness to improve the accuracy and reliability of macromodeling
techniques used to convert sampled scattering parameters into models for
transient analysis."
"We present an efficient computational framework to quantify the impact of
individual observations in four dimensional variational data assimilation. The
proposed methodology uses first and second order adjoint sensitivity analysis,
together with matrix-free algorithms to obtain low-rank approximations of ob-
servation impact matrix. We illustrate the application of this methodology to
important applications such as data pruning and the identification of faulty
sensors for a two dimensional shallow water test system."
"Load curve data in power systems refers to users' electrical energy
consumption data periodically collected with meters. It has become one of the
most important assets for modern power systems. Many operational decisions are
made based on the information discovered in the data. Load curve data, however,
usually suffers from corruptions caused by various factors, such as data
transmission errors or malfunctioning meters. To solve the problem, tremendous
research efforts have been made on load curve data cleansing. Most existing
approaches apply outlier detection methods from the supply side (i.e.,
electricity service providers), which may only have aggregated load data. In
this paper, we propose to seek aid from the demand side (i.e., electricity
service users). With the help of readily available knowledge on consumers'
appliances, we present a new appliance-driven approach to load curve data
cleansing. This approach utilizes data generation rules and a Sequential Local
Optimization Algorithm (SLOA) to solve the Corrupted Data Identification
Problem (CDIP). We evaluate the performance of SLOA with real-world trace data
and synthetic data. The results indicate that, comparing to existing load data
cleansing methods, such as B-spline smoothing, our approach has an overall
better performance and can effectively identify consecutive corrupted data.
Experimental results also demonstrate that our method is robust in various
tests. Our method provides a highly feasible and reliable solution to an
emerging industry application."
"This paper describes COMSOL simulations of the stress and crack development
in the area where a masonry wall supports a floor. In these simulations one of
the main material properties of calcium silicate, its E-value, was assigned
randomly to the finite elements of the modeled specimen. Calcium silicate is a
frequently used building material with a relatively brittle fracture
characteristic. Its initial E-value varies, as well as tensile strength and
post peak behavior. Therefore, in the simulation, initial E-values were
randomly assigned to the elements of the model and a step function used for
describing the descending branch. The method also allows for variation in
strength to be taken into account in future research. The performed non-linear
simulation results are compared with experimental findings. They show the
stress distribution and cracking behavior in point loaded masonry when varying
material properties are used."
"The area of topology optimization of continuum structures of which is allowed
to change in order to improve the performance is now dominated by methods that
employ the material distribution concept. The typical methods of the topology
optimization based on the structural optimization of two phase composites are
the so-called variable density ones, like the SIMP (Solid Isotropic Material
with Penalization) and the BESO (Bi-directional Evolutional Structure
Optimization). The topology optimization problem refers to the saddle-point
variation one as well as the so-called Stokes flow problem of the compressive
fluid. The checkerboard patterns often appear in the results computed by the
SIMP and the BESO in which the Q1-P0 element is used for FEM (Finite Element
Method), since these patterns are more favourable than uniform density regions.
Computational experiments of SIMP and BESO have shown that filtering of
sensitivity information of the optimization problem is a highly efficient way
that the checkerboard patterns disappeared and to ensure mesh-independency. SIn
this paper, we discuss the theoretical basis for the filtering method of the
SIMP and the BESO and as a result, the filtering method can be understood by
the theorem of partition of unity and the convolution operator of low-pass
filter."
"A mathematical model allowing coupled hygro-thermo-mechanical analysis of
spalling in concrete walls at high temperatures by means of the moving boundary
problem is presented. A simplified mechanical approach to account for effects
of thermal stresses and pore pressure build-up on spalling is incorporated into
the model. The numerical algorithm based on finite element discretization in
space and the semi-implicit method for discretization in time is presented. The
validity of the developed model is carefully examined by a comparison between
experimental tests performed by Kalifa et al. (2000) and Mindeguia (2009) on
concrete prismatic specimens under unidirectional heating of temperature of 600
${\deg}$C and ISO 834 fire curve and the results obtained from the numerical
model."
"This article addresses the formulation and validation of a simple PC based
software application developed for simulating commercially available solar
panels. The important feature of this application is its capability to produce
speedy results in the form of solar panel output characteristics at given
environmental conditions by using minimal input data. Besides, it is able to
deliver critical information about the maximum power point of the panel at a
given environmental condition in quick succession. The application is based on
a standard equation which governs solar panels and works by means of estimating
unknown parameters in the equation to fit a given solar panel. The process of
parameter estimation is described in detail with the aid of equations and data
of a commercial solar panel. A validation of obtained results for commercial
solar panels is also presented by comparing the panel manufacturers' results
with the results generated by the application. In addition, implications of the
obtained results are discussed along with possible improvements to the
developed software application."
"EEG monitoring has an important milestone provide valuable information of
those candidates who suffer from epilepsy.In this paper human normal and
epileptic Electroencephalogram signals are analyzed with popular and efficient
signal processing techniques like Fourier and Wavelet transform. The delta,
theta, alpha, beta and gamma sub bands of EEG are obtained and studied for
detection of seizure and epilepsy. The extracted feature is then applied to ANN
for classification of the EEG signals."
"Numerical modeling using computers is known to present several advantages
compared to experimental testing. The high cost and the amount of time required
to prepare and to perform a test were among the main problems on the table when
the first tools for modeling structures in fire were developed. The discipline
structures-in-fire modeling is still currently the subject of important
research efforts around the word, those research efforts led to develop many
software. In this paper, our task is oriented to the study of fire behavior and
the impact of the span reinforced concrete walls with different sections
belonging to a residential building braced by a system composed of porticoes
and sails. Regarding the design and mechanical loading (compression forces and
moments) exerted on the walls in question, we are based on the results of a
study conducted at cold. We use on this subject the software Safir witch obeys
to the Eurocode laws, to realize this study. It was found that loading,
heating, and sizing play a capital role in the state of failed walls. Our
results justify well the use of reinforced concrete walls, acting as a
firewall. Their role is to limit the spread of fire from one structure to
another structure nearby, since we get fire resistance reaching more than 10
hours depending on the loading considered."
"The paper presents a slope stability analysis for a heterogeneous earthen
levee in Boston, UK, which is prone to occasional slope failures under tidal
loads. Dynamic behavior of the levee under tidal fluctuations was simulated
using a finite element model of variably saturated linear elastic perfectly
plastic soil. Hydraulic conductivities of the soil strata have been calibrated
according to piezometers readings, in order to obtain correct range of
hydraulic loads in tidal mode. Finite element simulation was complemented with
series of limit equilibrium analyses. Stability analyses have shown that slope
failure occurs with the development of a circular slip surface located in the
soft clay layer. Both models (FEM and LEM) confirm that the least stable
hydraulic condition is the combination of the minimum river levels at low tide
with the maximal saturation of soil layers. FEM results indicate that in winter
time the levee is almost at its limit state, at the margin of safety (strength
reduction factor values are 1.03 and 1.04 for the low-tide and high-tide
phases, respectively); these results agree with real-life observations. The
stability analyses have been implemented as real-time components integrated
into the UrbanFlood early warning system for flood protection."
"In the automotive industry, predicting noise during design cycle is a
necessary step. Well-known methods exist to answer this issue in low frequency
domain. Among these, Finite Element Methods, adapted to closed domains, are
quite easy to implement whereas Boundary Element Methods are more adapted to
infinite domains, but may induce singularity problems. In this article, the
described method, the SDM, allows to use both methods in their best application
domain. A new method is also presented to solve the SDM exterior problem.
Instead of using Boundary Element Methods, an original use of Finite Elements
is made. Efficiency of this new version of the Substructure Deletion Method is
discussed."
"Many studies simulates the machining process by using a single degree of
freedom spring-mass sytem to model the tool stiffness, or the workpiece
stiffness, or the unit tool-workpiece stiffness in modelings 2D. Others impose
the tool action, or use more or less complex modelings of the efforts applied
by the tool taking account the tool geometry. Thus, all these models remain
two-dimensional or sometimes partially three-dimensional. This paper aims at
developing an experimental method allowing to determine accurately the real
three-dimensional behaviour of a machining system (machine tool, cutting tool,
tool-holder and associated system of force metrology six-component
dynamometer). In the work-space model of machining, a new experimental
procedure is implemented to determine the machining system elastic behaviour.
An experimental study of machining system is presented. We propose a machining
system static characterization. A decomposition in two distinct blocks of the
system ""Workpiece-Tool-Machine"" is realized. The block Tool and the block
Workpiece are studied and characterized separately by matrix stiffness and
displacement (three translations and three rotations). The Castigliano's theory
allows us to calculate the total stiffness matrix and the total displacement
matrix. A stiffness center point and a plan of tool tip static displacement are
presented in agreement with the turning machining dynamic model and especially
during the self induced vibration. These results are necessary to have a good
three-dimensional machining system dynamic characterization."
"We discuss a multi-objective/goal programming model for the allocation of
inventory of graphical advertisements. The model considers two types of
campaigns: guaranteed delivery (GD), which are sold months in advance, and
non-guaranteed delivery (NGD), which are sold using real-time auctions. We
investigate various advertiser and publisher objectives such as (a) revenue
from the sale of impressions, clicks and conversions, (b) future revenue from
the sale of NGD inventory, and (c) ""fairness"" of allocation. While the first
two objectives are monetary, the third is not. This combination of demand types
and objectives leads to potentially many variations of our model, which we
delineate and evaluate. Our experimental results, which are based on
optimization runs using real data sets, demonstrate the effectiveness and
flexibility of the proposed model."
"The non-invasive pulse-oximeter is a crucial parameter in continuous
monitoring systems. It plays a vital role from admission of the patient to
surgeries with general anaesthesia. The paper proposes the application of
wavelet transform to delineate the raw plethysmograph signals obtained from
basic portable and mobile-powered electronic hardware. The paper primarily
focuses on finding peaks and baseline from noisy infrared and red waveforms
which are responsible for calculating heart-rate and oxygen saturation
percentages."
"E-commerce in today's conditions has the highest dependence on network
infrastructure of banking. However, when the possibility of communicating with
the Banking network is not provided, business activities will suffer. This
paper proposes a new approach of digital wallet based on mobile devices without
the need to exchange physical money or communicate with banking network. A
digital wallet is a software component that allows a user to make an electronic
payment in cash (such as a credit card or a digital coin), and hides the
low-level details of executing the payment protocol that is used to make the
payment. The main features of proposed architecture are secure awareness, fault
tolerance, and infrastructure-less protocol."
"We give a stochastic extension of the Brane Calculus, along the lines of
recent work by Cardelli and Mardare. In this presentation, the semantics of a
Brane process is a measure of the stochastic distribution of possible
derivations. To this end, we first introduce a labelled transition system for
Brane Calculus, proving its adequacy w.r.t. the usual reduction semantics.
Then, brane systems are presented as Markov processes over the measurable space
generated by terms up-to syntactic congruence, and where the measures are
indexed by the actions of this new LTS. Finally, we provide a SOS presentation
of this stochastic semantics, which is compositional and syntax-driven."
"The induction of a signaling pathway is characterized by transient complex
formation and mutual posttranslational modification of proteins. To faithfully
capture this combinatorial process in a mathematical model is an important
challenge in systems biology. Exploiting the limited context on which most
binding and modification events are conditioned, attempts have been made to
reduce the combinatorial complexity by quotienting the reachable set of
molecular species, into species aggregates while preserving the deterministic
semantics of the thermodynamic limit. Recently we proposed a quotienting that
also preserves the stochastic semantics and that is complete in the sense that
the semantics of individual species can be recovered from the aggregate
semantics. In this paper we prove that this quotienting yields a sufficient
condition for weak lumpability and that it gives rise to a backward Markov
bisimulation between the original and aggregated transition system. We
illustrate the framework on a case study of the EGF/insulin receptor crosstalk."
"In this paper, we aim at modelling and analyzing the regulation processes in
multi-cellular biological systems, in particular tissues.
  The modelling framework is based on interconnected logical regulatory
networks a la Rene Thomas equipped with information about their spatial
relationships. The semantics of such models is expressed through colored Petri
nets to implement regulation rules, combined with topological collections to
implement the spatial information.
  Some constraints are put on the the representation of spatial information in
order to preserve the possibility of an enumerative and exhaustive state space
exploration.
  This paper presents the modelling framework, its semantics, as well as a
prototype implementation that allowed preliminary experimentation on some
applications."
"Optimum implementation of non-conventional wells allows us to increase
considerably hydrocarbon recovery. By considering the high drilling cost and
the potential improvement in well productivity, well placement decision is an
important issue in field development. Considering complex reservoir geology and
high reservoir heterogeneities, stochastic optimization methods are the most
suitable approaches for optimum well placement. This paper proposes an
optimization methodology to determine optimal well location and trajectory
based upon the Covariance Matrix Adaptation - Evolution Strategy (CMA-ES) which
is a variant of Evolution Strategies recognized as one of the most powerful
derivative-free optimizers for continuous optimization. To improve the
optimization procedure, two new techniques are investigated: (1). Adaptive
penalization with rejection is developed to handle well placement constraints.
(2). A meta-model, based on locally weighted regression, is incorporated into
CMA-ES using an approximate ranking procedure. Therefore, we can reduce the
number of reservoir simulations, which are computationally expensive. Several
examples are presented. Our new approach is compared with a Genetic Algorithm
incorporating the Genocop III technique. It is shown that our approach
outperforms the genetic algorithm: it leads in general to both a higher NPV and
a significant reduction of the number of reservoir simulations."
"The optimal dimensional synthesis for planar mechanisms using differential
evolution (DE) is demonstrated. Four examples are included: in the first case,
the synthesis of a mechanism for hybrid-tasks, considering path generation,
function generation, and motion generation, is carried out. The second and
third cases pertain to path generation, with and without prescribed timing.
Finally, the synthesis of an Ackerman mechanism is reported. Order defect
problem is solved by manipulating individuals instead of penalizing or
discretizing the search space for the parameters. A technique that consists in
applying a transformation in order to satisfy the Grashof and crank conditions
to generate an initial elitist population is introduced. As a result, the
evolutionary algorithm increases its efficiency."
"We investigate the use of Tikhonov regularization with the minimum support
stabilizer for underdetermined 2-D inversion of gravity data. This stabilizer
produces models with non-smooth properties which is useful for identifying
geologic structures with sharp boundaries. A very important aspect of using
Tikhonov regularization is the choice of the regularization parameter that
controls the trade off between the data fidelity and the stabilizing
functional. The L-curve and generalized cross validation techniques, which only
require the relative sizes of the uncertainties in the observations are
considered. Both criteria are applied in an iterative process for which at each
iteration a value for regularization parameter is estimated. Suitable values
for the regularization parameter are successfully determined in both cases for
synthetic but practically relevant examples. Whenever the geologic situation
permits, it is easier and more efficient to model the subsurface with a 2-D
algorithm, rather than to apply a full 3-D approach. Then, because the problem
is not large it is appropriate to use the generalized singular value
decomposition for solving the problem efficiently. The method is applied on a
profile of gravity data acquired over the Safo mining camp in Maku-Iran, which
is well known for manganese ores. The presented results demonstrate success in
reconstructing the geometry and density distribution of the subsurface source."
"Gene Regulatory Network (GRN) plays an important role in knowing insight of
cellular life cycle. It gives information about at which different
environmental conditions genes of particular interest get over expressed or
under expressed. Modelling of GRN is nothing but finding interactive
relationships between genes. Interaction can be positive or negative. For
inference of GRN, time series data provided by Microarray technology is used.
Key factors to be considered while constructing GRN are scalability,
robustness, reliability and maximum detection of true positive interactions
between genes. This paper gives detailed technical review of existing methods
applied for building of GRN along with scope for future work."
"We evaluate susceptibility to lava flows on Mt. Etna based on specially
designed die-toss experiments using probabilities for type, time and place of
activation from the volcano's 400-year recorded history and current studies on
its known fractures and fissures. The types of activations were forcast using a
table of probabilities for events, typed by duration and volume of ejecta.
Lengths of time were represented by the number of activations to expect within
a given time-frame, calculated assuming Poisson-distributed inter-arrival times
for activations. Locations of future activations were forecast with a
probability distribution function for activation probabilities. Most likely
scenarios for risk and resulting topography were generated for Etna's next
activation (average 7.76 years), the next 25, 50 and 100 years. Forecasts for
areas most likely affected are in good agreement with previous risk studies
made. Forecasts for risks of lava invasions, as well as future topographies
might be a first. Threats to lifelines are also discussed."
"In this paper Fourier Transform and Wavelet Transform are applied in case of
recent 300 years of sunspot numbers to explain the solar cycles. Here basically
parallel study of Fourier and Wavelet analysis are done and we have observed
that the better result can be obtained from Wavelet analysis during sunspot
number analysis. We are able to show various minima and maxima in the recent
ages of solar cycles with this tool. The exact periodicity and other possible
periodicities in the cyclic phenomenon of sunspot activity are determined."
"Return on Investment (ROI) is one of the most popular performance measurement
and evaluation metrics. ROI analysis (when applied correctly) is a powerful
tool in comparing solutions and making informed decisions on the acquisitions
of information systems. The ROI sensitivity to error is a natural thought, and
common sense suggests that ROI evaluations cannot be absolutely accurate.
However, literature review revealed that in most publications and analyst firms
reports, this issue is just overlooked. On the one hand, the results of the ROI
calculations are implied to be produced with a mathematical rigor, possibility
of errors is not mentioned and amount of errors is not estimated. On the
contrary, another approach claims ROI evaluations to be absolutely inaccurate
because, in view of their authors, future benefits (especially, intangible)
cannot be estimated within any reasonable boundaries. The purpose of this study
is to provide a systematic research of the accuracy of the ROI evaluations in
the context of the information systems implementations. The main contribution
of the study is that this is the first systematic effort to evaluate ROI
accuracy. Analytical expressions have been derived for estimating errors of the
ROI evaluations. Results of the Monte Carlo simulation will help practitioners
in making informed decisions based on explicitly stated factors influencing the
ROI uncertainties. The results of this research are intended for researchers in
information systems, technology solutions and business management, and also for
information specialists, project managers, program managers, technology
directors, and information systems evaluators. Most results are applicable to
ROI evaluations in a wider subject area."
"As high-throughput sequencers become standard equipment outside of sequencing
centers, there is an increasing need for efficient methods for pre-processing
and primary analysis. While a vast literature proposes methods for HTS data
analysis, we argue that significant improvements can still be gained by
exploiting expensive pre-processing steps which can be amortized with savings
from later stages. We propose a method to accelerate and improve read mapping
based on an initial clustering of possibly billions of high-throughput
sequencing reads, yielding clusters of high stringency and a high degree of
overlap. This clustering improves on the state-of-the-art in running time for
small datasets and, for the first time, makes clustering high-coverage human
libraries feasible. Given the efficiently computed clusters, only one
representative read from each cluster needs to be mapped using a traditional
readmapper such as BWA, instead of individually mapping all reads. On human
reads, all processing steps, including clustering and mapping, only require
11%-59% of the time for individually mapping all reads, achieving speed-ups for
all readmappers, while minimally affecting mapping quality. This accelerates a
highly sensitive readmapper such as Stampy to be competitive with a fast
readmapper such as BWA on unclustered reads."
"In this paper we consider a generalization of the Markowitz's Mean-Variance
model under linear transaction costs and cardinality constraints. The
cardinality constraints are used to limit the number of assets in the optimal
portfolio. The generalized model is formulated as a mixed integer quadratic
programming (MIP) problem. The purpose of this paper is to investigate a
continuous approach based on difference of convex functions (DC) programming
for solving the MIP model. The preliminary comparative results of the proposed
approach versus CPLEX are presented."
"In matter of Portfolio selection, we consider a generalization of the
Markowitz Mean-Variance model which includes buy-in threshold constraints.
These constraints limit the amount of capital to be invested in each asset and
prevent very small investments in any asset. The new model can be converted
into a NP-hard mixed integer quadratic programming problem. The purpose of this
paper is to investigate a continuous approach based on DC programming and DCA
for solving this new model. DCA is a local continuous approach to solve a wide
variety of nonconvex programs for which it provided quite often a global
solution and proved to be more robust and efficient than standard methods.
Preliminary comparative results of DCA and a classical Branch-and-Bound
algorithm will be presented. These results show that DCA is an efficient and
promising approach for the considered portfolio selection problem."
"We investigate a new application of Difference of Convex functions
programming and DCA in solving the constrained two-dimensional non-guillotine
cutting problem. This problem consists of cutting a number of rectangular
pieces from a large rectangular object. The cuts are done under some
constraints and the objective is to maximize the total value of the pieces cut.
We reformulate this problem as a DC program and solve it by DCA. The
performance of the approach is compared with the standard solver CPLEX."
"This work aims at introducing model methodology and numerical studies related
to a Lagrangian stochastic approach applied to the computation of the wind
circulation around mills. We adapt the Lagrangian stochastic downscaling method
that we have introduced in [3] and [4] to the atmospheric boundary layer and we
introduce here a Lagrangian version of the actuator disc methods to take
account of the mills. We present our numerical method and numerical experiments
in the case of non rotating and rotating actuator disc models. We also present
some features of our numerical method, in particular the computation of the
probability distribution of the wind in the wake zone, as a byproduct of the
fluid particle model and the associated PDF method."
"This paper deals with combination of two modern engineering methods in order
to optimise the shape of a representative casting product. The product being
analysed is a sling, which is used to attach pulling rope in timber
transportation. The first step was 3D modelling and static stress/strain
analysis using CAD/CAE software NX4. The slinger shape optimization was
performed using Traction method, by means of software Optishape-TS. To define
constraints for shape optimization, FEA software FEMAP was used. The mould
pattern with optimized 3D shape was then prepared using Fused Deposition
Modelling (FDM) Rapid prototyping method. The sling mass decreased by 20%,
while signifficantly better stress distribution was achieved, with maximum
stress 3.5 times less than initial value. The future researches should use 3D
scanning technology in order to provide more accurate 3D model of initial part.
Results of this research can be used by toolmakers in order to engage FEA/RP
technology to design and manufacture lighter products with acceptable stress
distribution."
"The use of simultaneous sources in geophysical inverse problems has
revolutionized the ability to deal with large scale data sets that are obtained
from multiple source experiments. However, the technique breaks when the data
has non-uniform standard deviation or when some data are missing. In this paper
we develop, study, and compare a number of techniques that enable to utilize
advantages of the simultaneous source framework for these cases. We show that
the inverse problem can still be solved efficiently by using these new
techniques. We demonstrate our new approaches on the Direct Current Resistivity
inverse problem."
"Bioinformatics encompass storing, analyzing and interpreting the biological
data. Most of the challenges for Machine Learning methods like Cellular
Automata is to furnish the functional information with the corresponding
biological sequences. In eukaryotes DNA is divided into introns and exons. The
introns will be removed to make the coding region by a process called splicing.
By indentifying a splice site we can easily specify the DNA sequence category
(Donor/Accepter/Neither).Splicing sites play an important role in understanding
the genes. A class of CA which can handle fuzzy logic is employed with modified
clonal algorithm is proposed to identify the splicing site. This classifier is
tested with Irvine Primate Splice Junction Database. It is compared with
NNspIICE, GENIO, HSPL and SPIICE VIEW. The reported accuracy and efficiency of
prediction is quite promising."
"The reproduction and replication of reported scientific results is a hot
topic within the academic community. The retraction of numerous studies from a
wide range of disciplines, from climate science to bioscience, has drawn the
focus of many commentators, but there exists a wider socio-cultural problem
that pervades the scientific community. Sharing code, data and models often
requires extra effort; this is currently seen as a significant overhead that
may not be worth the time investment.
  Automated systems, which allow easy reproduction of results, offer the
potential to incentivise a culture change and drive the adoption of new
techniques to improve the efficiency of scientific exploration. In this paper,
we discuss the value of improved access and sharing of the two key types of
results arising from work done in the computational sciences: models and
algorithms. We propose the development of an integrated cloud-based system
underpinning computational science, linking together software and data
repositories, toolchains, workflows and outputs, providing a seamless automated
infrastructure for the verification and validation of scientific models and in
particular, performance benchmarks."
"Adverse drug reaction (ADR) is widely concerned for public health issue. In
this study we propose an original approach to detect the ADRs using feature
matrix and feature selection. The experiments are carried out on the drug
Aspirin. Major side effects for the drug are detected and better performance is
achieved compared to other computerized methods. The detected ADRs are based on
the computerized method, further investigation is needed."
"How a household varies their regular usage of electricity is useful
information for organisations to allow accurate targeting of behaviour
modification initiatives with the aim of improving the overall efficiency of
the electricity network. The variability of regular activities in a household
is one possible indication of that household's willingness to accept incentives
to change their behaviour.
  An approach is presented for identifying a way of representing the
variability of a household's behaviour and developing an efficient way of
clustering the households, using these measures of variability, into a few,
usable groupings.
  To evaluate the effectiveness of the variability measures, a number of
cluster validity indexes are explored with regard to how the indexes vary with
the number of clusters, the number of attributes, and the quality of the
attributes. The Cluster Dispersion Indicator (CDI) and the Davies-Boulden
Indicator (DBI) are selected for future work developing various indicators of
household behaviour variability.
  The approach is tested using data from 180 UK households monitored for over a
year at a sampling interval of 5 minutes. Data is taken from the evening peak
electricity usage period of 4pm to 8pm."
"Adverse drug reactions (ADR) are widely concerning for public health issue.
In this study we propose an original approach to detect ADRs using a feature
matrix and feature selection. The experiments are carried out on the drug
Simvastatin. Major side effects for the drug are detected and better
performance is achieved compared to other computerized methods. Because
currently the detected ADRs are based solely on computerized methods, further
expert investigation is needed."
"Process variations are a major concern in today's chip design since they can
significantly degrade chip performance. To predict such degradation, existing
circuit and MEMS simulators rely on Monte Carlo algorithms, which are typically
too slow. Therefore, novel fast stochastic simulators are highly desired. This
paper first reviews our recently developed stochastic testing simulator that
can achieve speedup factors of hundreds to thousands over Monte Carlo. Then, we
develop a fast hierarchical stochastic spectral simulator to simulate a complex
circuit or system consisting of several blocks. We further present a fast
simulation approach based on anchored ANOVA (analysis of variance) for some
design problems with many process variations. This approach can reduce the
simulation cost and can identify which variation sources have strong impacts on
the circuit's performance. The simulation results of some circuit and MEMS
examples are reported to show the effectiveness of our simulator"
"This brief paper proposes an uncertainty quantification method for the
periodic steady-state (PSS) analysis with both Gaussian and non-Gaussian
variations. Our stochastic testing formulation for the PSS problem provides
superior efficiency over both Monte Carlo methods and existing spectral
methods. The numerical implementation of a stochastic shooting Newton solver is
presented for both forced and autonomous circuits. Simulation results on some
analog/RF circuits are reported to show the effectiveness of our proposed
algorithms."
"Uncertainties have become a major concern in integrated circuit design. In
order to avoid the huge number of repeated simulations in conventional Monte
Carlo flows, this paper presents an intrusive spectral simulator for
statistical circuit analysis. Our simulator employs the recently developed
generalized polynomial chaos expansion to perform uncertainty quantification of
nonlinear transistor circuits with both Gaussian and non-Gaussian random
parameters. We modify the nonintrusive stochastic collocation (SC) method and
develop an intrusive variant called stochastic testing (ST) method to
accelerate the numerical simulation. Compared with the stochastic Galerkin (SG)
method, the resulting coupled deterministic equations from our proposed ST
method can be solved in a decoupled manner at each time point. At the same
time, ST uses fewer samples and allows more flexible time step size controls
than directly using a nonintrusive SC solver. These two properties make ST more
efficient than SG and than existing SC methods, and more suitable for
time-domain circuit simulation. Simulation results of several digital, analog
and RF circuits are reported. Since our algorithm is based on generic
mathematical models, the proposed ST algorithm can be applied to many other
engineering problems."
"Demands on the disaster response capacity of the European Union are likely to
increase, as the impacts of disasters continue to grow both in size and
frequency. This has resulted in intensive research on issues concerning
spatially-explicit information and modelling and their multiple sources of
uncertainty. Geospatial support is one of the forms of assistance frequently
required by emergency response centres along with hazard forecast and event
management assessment. Robust modelling of natural hazards requires dynamic
simulations under an array of multiple inputs from different sources.
Uncertainty is associated with meteorological forecast and calibration of the
model parameters. Software uncertainty also derives from the data
transformation models (D-TM) needed for predicting hazard behaviour and its
consequences. On the other hand, social contributions have recently been
recognized as valuable in raw-data collection and mapping efforts traditionally
dominated by professional organizations. Here an architecture overview is
proposed for adaptive and robust modelling of natural hazards, following the
Semantic Array Programming paradigm to also include the distributed array of
social contributors called Citizen Sensor in a semantically-enhanced strategy
for D-TM modelling. The modelling architecture proposes a multicriteria
approach for assessing the array of potential impacts with qualitative rapid
assessment methods based on a Partial Open Loop Feedback Control (POLFC) schema
and complementing more traditional and accurate a-posteriori assessment. We
discuss the computational aspect of environmental risk modelling using
array-based parallel paradigms on High Performance Computing (HPC) platforms,
in order for the implications of urgency to be introduced into the systems
(Urgent-HPC)."
"We propose a model which can be jointly calibrated to the corporate bond term
structure and equity option volatility surface of the same company. Our purpose
is to obtain explicit bond and equity option pricing formulas that can be
calibrated to find a risk neutral model that matches a set of observed market
prices. This risk neutral model can then be used to price more exotic, illiquid
or over-the-counter derivatives. We observe that the model implied credit
default swap (CDS) spread matches the market CDS spread and that our model
produces a very desirable CDS spread term structure. This is observation is
worth noticing since without calibrating any parameter to the CDS spread data,
it is matched by the CDS spread that our model generates using the available
information from the equity options and corporate bond markets. We also observe
that our model matches the equity option implied volatility surface well since
we properly account for the default risk premium in the implied volatility
surface. We demonstrate the importance of accounting for the default risk and
stochastic interest rate in equity option pricing by comparing our results to
Fouque, Papanicolaou, Sircar and Solna (2003), which only accounts for
stochastic volatility."
"We consider the basic features of complex dynamic and control systems,
including systems having hierarchical structure. Special attention is paid to
the problems of design and synthesis of complex systems and control models, and
to the development of simulation techniques and systems. A model of complex
system is proposed and briefly analyzed."
"The present paper is based on studying, analyzing and implementing the expert
systems in the financial and accounting domain of the companies, describing the
use method of the informational systems that can be used in the multi-national
companies, public interest institutions, and medium and small dimension
economical entities, in order to optimize the managerial decisions and render
efficient the financial-accounting functionality. The purpose of this paper is
aimed to identifying the economical exigencies of the entities, based on the
already used accounting instruments and the management software that could
consent the control of the economical processes and patrimonial assets."
"Heterogeneous object design is an active research area in recent years. The
conventional CAD modeling approaches only provide geometry and topology of the
object, but do not contain any information with regard to the materials of the
object and so can not be used for the fabrication of heterogeneous objects (HO)
through rapid prototyping. Current research focuses on computer-aided design
issues in heterogeneous object design. A new CAD modeling approach is proposed
to integrate the material information into geometric regions thus model the
material distributions in the heterogeneous object. The gradient references are
used to represent the complex geometry heterogeneous objects which have
simultaneous geometry intricacies and accurate material distributions. The
gradient references helps in flexible manipulability and control to
heterogeneous objects, which guarantees the local control over gradient regions
of developed heterogeneous objects. A systematic approach on data flow,
processing, computer visualization, and slicing of heterogeneous objects for
rapid prototyping is also presented."
"This research report summarizes the progress of work carried out jointly by
the IRCCyN and the \'Ecole Polytechnique de Montr\'eal about the resolution of
the inverse problem for the seismic imaging of transmission overhead line
structure foundations. Several methods aimed at mapping the underground medium
are considered. More particularly, we focus on methods based on a bilinear
formulation of the forward problem on one hand (CSI, modified gradient, etc.)
and on methods based on a ""primal"" formulation on the other hand. The
performances of these methods are compared using synthetic data. This work was
partially funded by RTE (R\'eseau de Transport d'\'Electricit\'e), which has
initiated the project, and was carried out in collaboration with EDF R&D
(\'Electricit\'e de France - Recherche et D\'eveloppement)."
"This paper proposes to extend the discrete Verhulst power equilibrium
approach, previously suggested in [1], to the power-rate optimal allocation
problem. Multirate users associated to different types of traffic are
aggregated to distinct user' classes, with the assurance of minimum rate
allocation per user and QoS. Herein, Verhulst power allocation algorithm was
adapted to the single-input-single-output DS/CDMA jointly power-rate control
problem. The analysis was carried out taking into account the convergence time,
quality of solution, in terms of the normalized squared error (NSE), when
compared with the analytical solution based on interference matrix inverse, and
computational complexity. Numerical results demonstrate the validity of the
proposed resource allocation methodology."
"Domain decomposition methods are widely used to solve sparse linear systems
from scientific problems, but they are not suited to solve sparse linear
systems extracted from integrated circuits. The reason is that the sparse
linear system of integrated circuits may be non-diagonal-dominant, and domain
decomposition method might be unconvergent for these non-diagonal-dominant
matrices. In this paper, we propose a mini-step strategy to do the circuit
transient analysis. Different from the traditional large-step approach, this
strategy is able to generate diagonal-dominant sparse linear systems. As a
result, preconditioned domain decomposition methods can be used to simulate the
large integrated circuits on the supercomputers and clouds."
"Radiotherapy represents an important phase of treatment for a large number of
cancer patients. It is essential that resources used to deliver this treatment
are employed effectively. This paper presents a new integer linear programming
model for real-world radiotherapy treatment scheduling and analyses the
effectiveness of using this model on a daily basis in a hospital. Experiments
are conducted varying the days on which schedules can be created. Results
obtained using real-world data from the Nottingham University Hospitals NHS
Trust, UK, are presented and show how the proposed model can be used with
different policies in order to achieve good quality schedules."
"The geological surveying presently uses methods and tools for the computer
modeling of 3D-structures of the geographical subsurface and geotechnical
characterization as well as the application of geoinformation systems for
management and analysis of spatial data, and their cartographic presentation.
The objectives of this paper are to present a 3D geological surface model of
Latur district in Maharashtra state of India. This study is undertaken through
the several processes which are discussed in this paper to generate and
visualize the automated 3D geological surface model of a projected area."
"As air traffic grows significantly, aircraft accidents increase. Many
aviation accidents could be prevented if the precise aircraft positions and
weather conditions on the aircraft's route were known. Existing studies propose
determining the precise aircraft positions via a VHF channel with an air-to-air
radio relay system that is based on mobile ad-hoc networks. However, due to the
long propagation delay, the existing TDMA MAC schemes underutilize the
networks. The existing TDMA MAC sends data and receives ACK in one time slot,
which requires two guard times in one time slot. Since aeronautical
communications spans a significant distance, the guard time occupies a
significantly large portion of the slot. To solve this problem, we propose a
piggybacking mechanism ACK. Our proposed MAC has one guard time in one time
slot, which enables the transmission of more data. Using this additional data,
we can send weather conditions that pertain to the aircraft's current position.
Our analysis shows that this proposed MAC performs better than the existing
MAC, since it offers better throughput and network utilization. In addition,
our weather condition notification model achieves a much lower transmission
delay than a HF (high frequency) voice communication."
"The aim of this research review is to propose the logic and search mechanism
for the development of an artificially intelligent automaton (AIA) that can
find affected cells in a 3-dimensional biological system. Research on the
possible application of such automatons to detect and control cancer cells in
the human body are greatly focused MRI and PET scans finds the affected regions
at the tissue level even as we can find the affected regions at the cellular
level using the framework. The AIA may be designed to ensure optimum
utilization as they record and might control the presence of affected cells in
a human body. The proposed models and techniques can be generalized and used in
any application where cells are injured or affected by some disease or
accident. The best method to import AIA into the body without surgery or
injection is to insert small pill like automata, carrying material viz drugs or
leukocytes that is needed to correct the infection. In this process, the AIA
can be compared to nano pills to deliver or support therapy. NanoHive
simulation software was used to validate the framework of this paper. The
existing nanomedicine models such as obstacle avoidance algorithm based models
(Hla K H S et al 2008) and the framework in this model were tested in different
simulation based experiments. The existing models such as obstacle avoidance
based models failed in complex environmental conditions (such as changing
environmental conditions, presence of semi-solid particles, etc) while the
model in this paper executed its framework successfully.Come systems biology,
this field of automatons deserves a bigger leap of understanding especially
when pharmacogenomics is at its peak. The results also indicate the importance
of artificial intelligence and other computational capabilities in the proposed
model for the successful detection of affected cells."
"Discovery of transcription factor binding sites is a much explored and still
exploring area of research in functional genomics. Many computational tools
have been developed for finding motifs and each of them has their own
advantages as well as disadvantages. Most of these algorithms need prior
knowledge about the data to construct background models. However there is not a
single technique that can be considered as best for finding regulatory motifs.
This paper proposes an artificial immune system based algorithm for finding the
transcription factor binding sites or motifs and two new weighted scores for
motif evaluation. The algorithm is enumerative, but sufficient pruning of the
pattern search space has been incorporated using immune system concepts. The
performance of AISMOTIF has been evaluated by comparing it with eight state of
art composite motif discovery algorithms and found that AISMOTIF predicts known
motifs as well as new motifs from the benchmark dataset without any prior
knowledge about the data."
"Through this research, embedded synthetic fracture networks in rock masses
are studied. To analysis the fluid flow complexity in fracture networks with
respect to the variation of connectivity patterns, two different approaches are
employed, namely, the Lattice Boltzmann method and graph theory. The Lattice
Boltzmann method is used to show the sensitivity of the permeability and fluid
velocity distribution to synthetic fracture networks' connectivity patterns.
Furthermore, the fracture networks are mapped into the graphs, and the
characteristics of these graphs are compared to the main spatial fracture
networks. Among different characteristics of networks, we distinguish the
modularity of networks and sub-graphs distributions. We map the flow regimes
into the proper regions of the network's modularity space. Also, for each type
of fluid regime, corresponding motifs shapes are scaled. Implemented power law
distributions of fracture length in spatial fracture networks yielded the same
node's degree distribution in transformed networks. Two general spatial
networks are considered: random networks and networks with ""hubness"" properties
mimicking a spatial damage zone (both with power law distribution of fracture
length). In the first case, the fractures are embedded in uniformly distributed
fracture sets; the second case covers spatial fracture zones. We prove
numerically that the abnormal change (transition) in permeability is controlled
by the hub growth rate. Also, comparing LBM results with the characteristic
mean length of transformed networks' links shows a reverse relationship between
the aforementioned parameters. In addition, the abnormalities in advection
through nodes are presented."
"Research in bioinformatics is a complex phenomenon as it overlaps two
knowledge domains, namely, biological and computer sciences. This paper has
tried to introduce an efficient data mining approach for classifying proteins
into some useful groups by representing them in hierarchy tree structure. There
are several techniques used to classify proteins but most of them had few
drawbacks on their grouping. Among them the most efficient grouping technique
is used by PSIMAP. Even though PSIMAP (Protein Structural Interactome Map)
technique was successful to incorporate most of the protein but it fails to
classify the scale free property proteins. Our technique overcomes this
drawback and successfully maps all the protein in different groups, including
the scale free property proteins failed to group by PSIMAP. Our approach
selects the six major attributes of protein: a) Structure comparison b)
Sequence Comparison c) Connectivity d) Cluster Index e) Interactivity f)
Taxonomic to group the protein from the databank by generating a hierarchal
tree structure. The proposed approach calculates the degree (probability) of
similarity of each protein newly entered in the system against of existing
proteins in the system by using probability theorem on each six properties of
proteins."
"A new technology (in addition to ERP) is proposed to provide an increase of
profit and normal cash flow. This technology involves the next functions:
forming of intellectual interface on a natural language to communicate with a
control system; joint planning of production and sales to get the maximal
profit; an adaptation of control system to internal and external events. The
use of the natural language permits to overcome a barrier between the control
system and upper managers. To solve posed actual problems of management the
selection of information from a database and call to mathematical methods are
executed automatically. Optimal planning provides the maximal use of available
resources and opportunities of market. Adaptive control implements the
efficient reaction to critical events that lead up to a decrease of profit and
increase of accounts receivable."
"This paper proposes an exchange rate forecasting method by using the grey
relative combination approach of chaos wavelet SVM-Markov model. The problem of
short-term forecast of exchange rate by using the comprehensive method of the
phase space reconstitution and SVM method has been researched. We have
suggested a wavelet-SVR-Markov forecasting model to predict the finance time
series and demonstrated that can more improve the forecasting performance by
the rational combination of the forecast results through various combinational
tests. Our test result has been showed that the two-stage combination model is
more excellent than the normal combination model. Also we have comprehensively
estimated the combination forecast methods according to the forecasting
performance indicators.The estimated result have been shown that the
combination forecast methods on the basic of the degree of grey relation and
the optimal grey relation combination have fine forecast performance."
"We propose a hybrid forecast based on extended discrete grey Markov and
variable dimension Kalman model and show that our hybrid model can improve much
more the performance of forecast than traditional grey Markov and Kalman
models. Our simulation results are given to demonstrate that our hybrid
forecast method combined with degree of grey incidence are better than grey
Markov and ARFIMA model or Kalman methods."
"The variance component tests used in genomewide association studies of
thousands of individuals become computationally exhaustive when multiple traits
are analysed in the context of omics studies. We introduce two high-throughput
algorithms -- CLAK-CHOL and CLAK-EIG -- for single and multiple phenotype
genome-wide association studies (GWAS). The algorithms, generated with the help
of an expert system, reduce the computational complexity to the point that
thousands of traits can be analyzed for association with millions of
polymorphisms in a course of days on a standard workstation. By taking
advantage of problem specific knowledge, CLAK-CHOL and CLAK-EIG significantly
outperform the current state-of-the-art tools in both single and multiple trait
analysis."
"We propose a hybrid forecast model based on discrete grey-fuzzy Markov and
grey neural network model and show that our hybrid model can improve much more
the performance of forecast than traditional grey-Markov model and neural
network models. Our simulation results are shown that our hybrid forecast
method with the combinational weight based on optimal grey relation degree
method is better than the hybrid model with combinational weight based
minimization of error-squared criterion."
"In the well placement problem, as well as in other field development
optimization problems, geological uncertainty is a key source of risk affecting
the viability of field development projects. Well placement problems under
geological uncertainty are formulated as optimization problems in which the
objective function is evaluated using a reservoir simulator on a number of
possible geological realizations. In this paper, we present a new approach to
handle geological uncertainty for the well placement problem with a reduced
number of reservoir simulations. The proposed approach uses already simulated
well configurations in the neighborhood of each well configuration for the
objective function evaluation. We use thus only one single reservoir simulation
performed on a randomly chosen realization together with the neighborhood to
estimate the objective function instead of using multiple simulations on
multiple realizations. This approach is combined with the stochastic optimizer
CMA-ES. The proposed approach is shown on the benchmark reservoir case PUNQ-S3
to be able to capture the geological uncertainty using a smaller number of
reservoir simulations. This approach is compared to the reference approach
using all the possible realizations for each well configuration, and shown to
be able to reduce significantly the number of reservoir simulations (around
80%)."
"Currently there are many clinical trials using paper case report forms as the
primary data collection tool. Cloud Computing platforms provide big potential
for increasing efficiency through a web-based data collection interface,
especially for large-scale multi-center trials. Traditionally, clinical and
biological data for multi-center trials are stored in one dedicated,
centralized database system running at a data coordinating center (DCC). This
paper presents C-PASS-PC, a cloud-driven prototype of multi-center proactive
surveillance system for prostate cancer. The prototype is developed in PHP,
JQuery and CSS with an Oracle backend in a local Web server and database server
and deployed on Google App Engine (GAE) and Google Cloud SQL-MySQL. The
deploying process is fast and easy to follow. The C-PASS-PC prototype can be
accessed through an SSL-enabled web browser. Our approach proves the concept
that cloud computing platforms such as GAE is a suitable and flexible solution
in the near future for multi-center clinical trials."
"Emerging interest of trading companies and hedge funds in mining social web
has created new avenues for intelligent systems that make use of public opinion
in driving investment decisions. It is well accepted that at high frequency
trading, investors are tracking memes rising up in microblogging forums to
count for the public behavior as an important feature while making short term
investment decisions. We investigate the complex relationship between tweet
board literature (like bullishness, volume, agreement etc) with the financial
market instruments (like volatility, trading volume and stock prices). We have
analyzed Twitter sentiments for more than 4 million tweets between June 2010
and July 2011 for DJIA, NASDAQ-100 and 11 other big cap technological stocks.
Our results show high correlation (upto 0.88 for returns) between stock prices
and twitter sentiments. Further, using Granger's Causality Analysis, we have
validated that the movement of stock prices and indices are greatly affected in
the short term by Twitter discussions. Finally, we have implemented Expert
Model Mining System (EMMS) to demonstrate that our forecasted returns give a
high value of R-square (0.952) with low Maximum Absolute Percentage Error
(MaxAPE) of 1.76% for Dow Jones Industrial Average (DJIA). We introduce a novel
way to make use of market monitoring elements derived from public mood to
retain a portfolio within limited risk state (highly improved hedging bets)
during typical market conditions."
"A simplified model of indoor short wave radiation couplings adapted to
multi-zone simulations is proposed, thanks to a simplifying hypothesis and to
the introduction of an indoor short wave exchange matrix. The specific
properties of this matrix appear useful to quantify the thermal radiation
exchanges between the zones separated by windows or large openings. Integrated
in CODYRUN software, this module is detailed and compared to experimental
measurements carried out on a real scale tropical building."
"The calculation of airflows is of great importance for detailed building
thermal simulation computer codes, these airflows most frequently constituting
an important thermal coupling between the building and the outside on one hand,
and the different thermal zones on the other. The driving effects of air
movement, which are the wind and the thermal buoyancy, are briefly outlined and
we look closely at their coupling in the case of buildings, by exploring the
difficulties associated with large openings. Some numerical problems tied to
the resolving of the non-linear system established are also covered. Part of a
detailled simulation software (CODYRUN), the numerical implementation of this
airflow model is explained, insisting on data organization and processing
allowing the calculation of the airflows. Comparisons are then made between the
model results and in one hand analytical expressions and in another and
experimental measurements in case of a collective dwelling."
"Electric load profiles of tropical islands in developed countries are
characterised by morning, midday and evening peaks arising from all year round
high power demand in the commercial and residential sectors, due mostly to air
conditioning appliances and bad thermal conception of the building. The work
presented in this paper has led to the conception of a global quality standards
obtained through optimized bioclimatic urban planning and architectural design,
the use of passive cooling architectural components, natural ventilation and
energy efficient systems such as solar water heaters. We evaluated, with the
aid of an airflow and thermal building simulation software (CODYRUN), the
impact of each technical solution on thermal comfort within the building. These
technical solutions have been implemented in 280 new pilot dwelling projects
through the year 1996."
"As part of our efforts to complete the software CODYRUN validation, we chose
as test building a block of flats constructed in Reunion Island, which has a
humid tropical climate. The sensitivity analysis allowed us to study the
effects of both diffuse and direct solar radiation on our model of this
building. With regard to the choice and location of sensors, this stage of the
study also led us to measure the solar radiation falling on the windows. The
comparison of measured and predicted radiation clearly showed that our
predictions over-estimated the incoming solar radiation, and we were able to
trace the problem to the algorithm which calculates diffuse solar radiation. By
calculating view factors between the windows and the associated shading
devices, changes to the original program allowed us to improve the predictions,
and so this article shows the importance of sensitivity analysis in this area
of research."
"Thermal buildings simulation softwares need meteorological files in thermal
comfort, energetic evaluation studies. Few tools can make significant
meteorological data available such as generated typical year, representative
days, or artificial meteorological database. This paper deals about the
presentation of a new software, RUNEOLE, used to provide weather data in
buildings applications with a method adapted to all kind of climates. RUNEOLE
associates three modules of description, modelling and generation of weather
data. The statistical description of an existing meteorological database makes
typical representative days available and leads to the creation of model
libraries. The generation module leads to the generation of non existing
sequences. This software tends to be usable for the searchers and designers, by
means of interactivity, facilitated use and easy communication. The conceptual
basis of this tool will be exposed and we'll propose two examples of
applications in building physics for tropical humid climates."
"Developing a group of machine cells and their corresponding part families to
minimize the inter-cell and intra-cell material flow is the basic objective of
the designing of a cellular manufacturing system (CMS). Afterwards achieving a
competent cell layout is essential in order to minimize the total inter-cell
part travels, which is principally noteworthy. There are plentiful articles of
CMS literature which considered cell formation problems; however cell layout
topic has rarely been addressed. Therefore this research is intended to focus
on an adapted mathematical model of the layout design problem considering
material handling cost and closeness ratings of manufacturing cells. Owing to
the combinatorial class of the said problem, an efficient NP-hard technique
based on Simulated Annealing metaheuristic is proposed henceforth. Some test
problems are solved using the proposed technique. Computational results show
that the proposed metaheuristic approach is extremely effective and efficient
in terms of solution quality and computational complexity."
"Many models exist in the scientific literature for determining indoor
daylighting values. They are classified in three categories: numerical,
simplified and empirical models. Nevertheless, each of these categories of
models are not convenient for every application. Indeed, the numerical model
requires high calculation time; conditions of use of the simplified models are
limited, and experimental models need not only important financial resources
but also a perfect control of experimental devices (e.g. scale model), as well
as climatic characteristics of the location (e.g. in situ experiment). In this
article, a new model based on a combination of multiple simplified models is
established. The objective is to improve this category of model. The
originality of our paper relies on the coupling of several simplified models of
indoor daylighting calculations. The accuracy of the simulation code,
introduced into CODYRUN software to simulate correctly indoor illuminance, is
then verified. Besides, the software consists of a numerical building
simulation code, developed in the Physics and Mathematical Engineering
Laboratory for Energy and Environment (P.I.M.E.N.T) at the University of
Reunion. Initially dedicated to the thermal, airflow and hydrous phenomena in
the buildings, the software has been completed for the calculation of indoor
daylighting. New models and algorithms - which rely on a semi-detailed approach
- will be presented in this paper. In order to validate the accuracy of the
integrated models, many test cases have been considered as analytical,
inter-software comparisons and experimental comparisons. In order to prove the
accuracy of the new model - which can properly simulate the illuminance - a
confrontation between the results obtained from the software (developed in this
research paper) and the major made at a given place is described in details. A
new statistical indicator to appreciate the margins of errors - named RSD
(Reliability of Software Degrees) - is also be defined."
"Simulation codes of thermal behaviour could significantly improve housing
construction design. Among the existing software, CODYRUN and TRNSYS are
calculations codes of different conceptions. CODYRUN is exclusively dedicated
to housing thermal behaviour, whereas TRNSYS is more generally used on any
thermal system. The purpose of this article is to compare these two instruments
in two different conditions . We will first modelize a mono-zone test cell, and
analyse the results by means of signal treatment methods. Then, we will
modelize a real case of multi-zone housing, representative of housing in wet
tropical climates. We could so evaluate influences of meteorological and
building description data on model errors."
"The designer's preoccupation to reduce the energy needs and get a better
thermal quality of ambiances helped in the development of several packages
simulating the dynamic behaviour of buildings. This paper shows the adaptation
of a method of thermal analysis, the nodal analysis, linked to the case of
building's thermal behaviour. We take successively an interest in the case of
conduction into a wall, in the coupling with superficial exchanges and finally
in the constitution of thermal state models of the building. Big variations
existing from one building to another, it's necessary to build the thermal
model from the building description. This article shows the chosen method in
the case of our thermal simulation program for buildings, CODYRUN"
"The aim of this paper is to briefly recall heat transfer modes and explain
their integration within a software dedicated to building simulation (CODYRUN).
Detailed elements of the validation of this software are presented and two
applications are finally discussed. One concerns the modeling of a flat plate
air collector and the second focuses on the modeling of Trombe solar walls. In
each case, detailed modeling of heat transfer allows precise understanding of
thermal and energetic behavior of the studied structures. Recent decades have
seen a proliferation of tools for building thermal simulation. These
applications cover a wide spectrum from very simplified steady state models to
dynamic simulation ones, including computational fluid dynamics modules
(Clarke, 2001). These tools are widely available in design offices and
engineering firms. They are often used for the design of HVAC systems and still
subject to detailed research, particularly with respect to the integration of
new fields (specific insulation materials, lighting, pollutants transport,
etc.). Available from:
http://www.intechopen.com/books/evaporation-condensation-and-heat-transfer/heat-transfer-in-buildings-application-to-solar-air-collector-and-trombe-wall-design"
"The designers pre-occupation to reduce energy consumption and to achieve
better thermal ambience levels, has favoured the setting up of numerous
building thermal dynamic simulation programs. The progress in the modelling of
phenomenas and its transfer into the professional field has resulted in various
numerical approaches ranging from softwares dedicated to architects for design
use to tools for laboratory use by the expert thermal researcher. This analysis
shows that each approach tends to fulfil the specific needs of a certain kind
of manipulator only, in the building conception process. Our objective is
notably different as it is a tool which can be used from the very initial stage
of a construction project, to the energy audit for the existing building. In
each of these cases, the objective results, the precision advocated and the
time delay of the results are different parameters which call for a multiple
model approach of the building system"
"Validation of building energy simulation programs is of major interest to
both users and modellers. To achieve such a task, it is essential to apply a
methodology based on a priori test and empirical validation. A priori test
consists in verifying that models embedded in a program and their
implementation are correct. this should be achieved before carrying out
experiments. The aim of this report is to present results from the application
of the BESTEST procedure to our code. We will emphasise the way it allows to
find bugs in our program and also how it permits to qualify models of heat
transfer by conduction"
"Statistical traffic data analysis is a hot topic in traffic management and
control. In this field, current research progresses focus on analyzing traffic
flows of individual links or local regions in a transportation network. Less
attention are paid to the global view of traffic states over the entire
network, which is important for modeling large-scale traffic scenes. Our aim is
precisely to propose a new methodology for extracting spatio-temporal traffic
patterns, ultimately for modeling large-scale traffic dynamics, and long-term
traffic forecasting. We attack this issue by utilizing Locality-Preserving
Non-negative Matrix Factorization (LPNMF) to derive low-dimensional
representation of network-level traffic states. Clustering is performed on the
compact LPNMF projections to unveil typical spatial patterns and temporal
dynamics of network-level traffic states. We have tested the proposed method on
simulated traffic data generated for a large-scale road network, and reported
experimental results validate the ability of our approach for extracting
meaningful large-scale space-time traffic patterns. Furthermore, the derived
clustering results provide an intuitive understanding of spatial-temporal
characteristics of traffic flows in the large-scale network, and a basis for
potential long-term forecasting."
"The machine-part cell formation problem consists of creating machine cells
and their corresponding part families with the objective of minimizing the
inter-cell and intra-cell movement while maximizing the machine utilization.
This article demonstrates a hybrid clustering approach for the cell formation
problem in cellular manufacturing that conjoins Sorenson s similarity
coefficient based method to form the production cells. Computational results
are shown over the test datasets obtained from the past literature. The hybrid
technique is shown to outperform the other methods proposed in literature and
including powerful soft computing approaches such as genetic algorithms,
genetic programming by exceeding the solution quality on the test problems."
"In detailled buiding simulation models, airflow modelling and solving are
still open and crucial problems, specially in the case of open buildings as
encountered in tropical climates. As a consequence, wind speed conditioning
indoor thermal comfort or energy needs in case of air conditionning are uneasy
to predict. A first part of the problem is the lack of reliable and usable
large opening elementary modelling and another one concerns the numerical
solving of airflow network. This non linear pressure system is solved by
numerous methods mainly based on Newton Raphson (NR) method. This paper is
adressing this part of the difficulty, in our software CODYRUN. After model
checks, we propose to use Picard method (known also as fixed point) to
initialise zone pressures. A linear system (extracted from the non linear set
of equations) is solved around 10 times at each time step and NR uses this
result for initial values. Known to be uniformly but slowly convergent, this
method appears to be really powerful for the building pressure system. The
comparison of the methods in terms of number of iterations is illustrated using
a real test case experiment."
"This article presents the CODYRUN software developped by University of La
R\'eunion. It is a multizone thermal software, with detailled airflow and
humidity transfer calculations. One of its specific aspects is that it
constitutes a research tool, a design tool used by the lab and professionnals
and also a teaching tool. After a presentation of the multiple model aspect,
some details of the tree modules associated to physical phenomenons are given.
Elements of validation are exposed in next paraghaph, and then a few details of
the front end."
"The first purpose of our work has been to allow -as far as heat transfer
modes, airflow calculation and meteorological data reconstitution are
concerned- the integration of diverse interchangeable physical models in a
single software tool for professional use, CODYRUN. The designer's objectives,
precision requested and calculation time consideration, lead us to design a
structure accepting selective use of models, taking into account multizone
description and airflow patterns. With a building case study in Reunion Island,
we first analyse the sensibility of the thermal model to diffuse radiation
reconstitution on tilted surfaces. Then, a realistic balance between precision
required and calculation time leads us to select detailed models for the zone
of main interest, but to choose simplified models for the other zones."
"Considering the natural ventilation, the thermal behavior of buildings can be
described by a linear time varying model. In this paper, we describe an
implementation of model reduction of linear time varying systems. We show the
consequences of the model reduction on computing time and accuracy. Finally, we
compare experimental measures and simulation results using the initial model or
the reduced model. The reduced model shows negligible difference in accuracy,
and the computing time shortens."
"This paper deals about the presentation of a new software RUNEOLE used to
provide weather data in buildings physics. RUNEOLE associates three modules
leading to the description, the modelling and the generation of weather data.
The first module is dedicated to the description of each climatic variable
included in the database. Graphic representation is possible (with histograms
for example). Mathematical tools used to compare statistical distributions,
determine daily characteristic evolutions, find typical days, and the
correlations between the different climatic variables have been elaborated in
the second module. Artificial weather datafiles adapted to different simulation
codes are available at the issue of the third module. This tool can then be
used in HVAC system evaluation, or in the study of thermal comfort. The studied
buildings can then be tested under different thermal, aeraulic, and radiative
solicitations, leading to a best understanding of their behaviour for example
in humid climates."
"The purpose of our research deals with the description of a methodology for
the definition of specific weather sequences and their influence on the energy
needs of HVAC system. We'll apply the method on the tropical Reunion Island.
The methodological approach based on a detailed analysis of weather sequences
leads to a classification of climatic situations that can be applied to the
site. These sequences have been used to simulate buildings and air handling
systems thanks to a thermal simulation code, CODYRUN. Results bring to the
light how necessary it is to have coherent meteorological data for this kind of
simulation."
"The beginning of this work is the achievement of a design tool, which is a
multiple model software called "" CODYRUN "", suitable for professionnals and
usable by researchers. The original aspect of this software is that the
designer has at his disposal a wide panel of choices between different heat
transfer models More precisely, it consists in a multizone software integrating
both natural ventilation and moisture tranfers . This software is developed on
PC micro computer and gets advantage of the Microsoft WINDOWS front-end. Most
of time, HVAC systems and specially domestic air conditioners, are taken into
account in a very simplified way, or in a elaborated one. On one side,they are
just supposed to supply the demand of cooling loads with an ideal control loop
(no delay between the sollicitations and the time response of the system), The
available outputs are initially the hourly cooling and heating consumptions
without integrating the real caracteristics of the HVAC system This paper is
also following the same multiple model approach than for the building modelling
by defining different modelling levels for the air conditionning systems, from
a very simplified one to a detailled one. An experimental validation is
achieved in order to compare the sensitivity of each defined model and to point
out the interaction between the thermal behaviour of the envelop and the
electrical system consumption. For validation purposes, we will describe the
data acquisition system. and the used real size test cell located in the
University of Reunion island, Indian Ocean."
"This paper proposes a methodology to calculate both the first and second
derivatives of a vector function of one variable in a single computation step.
The method is based on the nested application of the dual number approach for
first order derivatives.
  It has been implemented in Fortran language, a module which contains the dual
version of elementary functions as well as more complex functions, which are
common in the field of rotational kinematics. Since we have three quantities of
interest, namely the function itself and its first and second derivative, our
basic numerical entity has three elements. Then, for a given vector function
$f:\mathbb{R}\to \mathbb{R}^m$, its dual version will have the form
$\tilde{f}:\mathbb{R}^3\to \mathbb{R}^{3m}$.
  As a study case, the proposed methodology is used to calculate the velocity
and acceleration of a point moving on the coupler-point curve generated by a
spherical four-bar mechanism."
"The metal-V belt drive includes a large number of parts which interact
between them to transmit power from the input to the output pulleys. A
compression belt composed of a great number of struts is maintained by a
tension flat belt. Power is them shared into the two belts that moves generally
in opposite directions. Due to the particular geometry of the elements and to
the great number of parts, a numerical approach achieves the global equilibrium
of the mechanism from the elementary part equilibrium. Sliding arc on each
pulley can be thus defined both for the compression and tension belts. Finally,
power sharing can be calculated as differential motion between the belts, is
defined. The first part of the paper will present the different steps of the
quasi-static mechanical analysis and their numerical implementations. Load
distributions, speed profiles and sliding angle values will be discussed. The
second part of the paper will deal to a systematic use of the computer
software. Speed ratio, transmitted torque, strut geometry and friction
coefficients effect will be analysed with the output parameter variations.
Finally, the effect pulley deformable flanges will be discussed."
"We present a decision support system for flood early warning and disaster
management. It includes the models for data-driven meteorological predictions,
for simulation of atmospheric pressure, wind, long sea waves and seiches; a
module for optimization of flood barrier gates operation; models for stability
assessment of levees and embankments, for simulation of city inundation
dynamics and citizens evacuation scenarios. The novelty of this paper is a
coupled distributed simulation of surface and subsurface flows that can predict
inundation of low-lying inland zones far from the submerged waterfront areas,
as observed in St. Petersburg city during the floods. All the models are
wrapped as software services in the CLAVIRE platform for urgent computing,
which provides workflow management and resource orchestration."
"Standard enthalpies of formation are used for assessing the efficiency and
safety of chemical processes in the chemical industry. However, the number of
compounds for which the enthalpies of formation are available is many orders of
magnitude smaller than the number of known compounds. Thermochemical data
prediction methods are therefore clearly needed. Several commercial and free
chemical databases are currently available, the NIST WebBook being the most
used free source. To overcome this problem a cheminformatics system was
designed and built with two main objectives in mind: collecting and retrieving
critically evaluated thermochemical values, and estimating new data. In its
present version, by using cheminformatics techniques, ThermInfo allows the
retrieval of the value of a thermochemical property, such as a gas-phase
standard enthalpy of formation, by inputting, for example, the molecular
structure or the name of a compound. The same inputs can also be used to
estimate data (presently restricted to non-polycyclic hydrocarbons) by using
the Extended Laidler Bond Additivity (ELBA) method. The information system is
publicly available at http://www.therminfo.com or
http://therminfo.lasige.di.fc.ul.pt. ThermInfo's strength lies in the data
quality, availability (free access), search capabilities, and, in particular,
prediction ability, based on a user-friendly interface that accepts inputs in
several formats."
"Due to the current environmental situation, energy saving has become the
leading drive in modern research. Although the residential houses in tropical
climate do not use air conditioning to maintain thermal comfort in order to
avoid use of electricity. As the thermal comfort is maintained by adequate
envelope composition and natural ventilation, this paper shows that it is
possible to determine the thickness of envelope layers for which the best
thermal comfort is obtained. The building is modeled in EnergyPlus software and
HookeJeves optimization methodology. The investigated house is a typical
residential house one-storey high with five thermal zones located at Reunion
Island, France. Three optimizations are performed such as the optimization of
the thickness of the concrete block layer, of the wood layer, and that of the
thermal insulation layer. The results show optimal thickness of thermal
envelope layers that yield the maximum TC according to Fanger predicted mean
vote."
"Low temperature heating panel systems offer distinctive advantages in terms
of thermal comfort and energy consumption, allowing work with low exergy
sources. The purpose of this paper is to compare floor, wall, ceiling, and
floor-ceiling panel heating systems in terms of energy, exergy and CO2
emissions. Simulation results for each of the analyzed panel system are given
by its energy (the consumption of gas for heating, electricity for pumps and
primary energy) and exergy consumption, the price of heating, and its carbon
dioxide emission. Then, the values of the air temperatures of rooms are
investigated and that of the surrounding walls and floors. It is found that the
floor-ceiling heating system has the lowest energy, exergy, CO2 emissions,
operating costs, and uses boiler of the lowest power. The worst system by all
these parameters is the classical ceiling heating"
"Cancer is one of the most feared diseases in the world it has increased
disturbingly and breast cancer occurs in one out of eight women, the prediction
of malignancies plays essential roles not only in revealing human genome, but
also in discovering effective prevention and treatment of cancers. Generally
cancer disease driven by somatic mutations in an individual DNA sequence, or
genome that accumulates during the lifetime of person. This paper is proposed a
novel method can predict the disease by mutations despite The presence in gene
sequence is not necessary it are malignant, so will be compare the protein of
patient with the gene's protein of disease if there is difference between these
two proteins then can say there is malignant mutations. This method will use
bioinformatics techniques like FASTA, CLUSTALW, etc which shows whether
malignant mutations or not, then training the backpropagation algorithm using
all expected malignant mutations for a certain genes (e.g. BRCA1 and BRCA2) of
disease, and using it to test whether patient is holder the disease or not.
Implementing this novel method as the first way to predict the disease based on
mutations in the sequence of the gene that causes the disease shows two
decisions are achieved successfully, the first diagnose whether the patient has
mutations of cancer or not using bioinformatics techniques the second
classifying these mutations are related to breast cancer (e.g. BRCA1 and BRCA2)
using backpropagation with mean square rate 0.0000001. Keywords-Gene sequence;
Protein; Deoxyribonucleic Acid DNA; Malignant mutation; Bioinformatics;
Back-propagation algorithm."
"This paper considers the Cram\`er-Rao lower Bound (CRB) for the source
localization problem in the near field. More specifically, we use the exact
expression of the delay parameter for the CRB derivation and show how this
exact CRB can be significantly different from the one given in the literature
and based on an approximate time delay expression (usually considered in the
Fresnel region). This CRB derivation is then generalized by considering the
exact expression of the received power profile (i.e., variable gain case)
which, to our best knowledge, has been ignored in the literature. Finally, we
exploit the CRB expression to introduce the new concept of Near Field
Localization (NFL) region for a target localization performance associated to
the application at hand. We illustrate the usefulness of the proposed CRB
derivation and its developments as well as the NFL region concept through
numerical simulations in different scenarios."
"We present an efficient numerical technique for calculating the series
impedance matrix of systems with round conductors. The method is based on a
surface admittance operator in combination with the method of moments and it
accurately predicts both skin and proximity effects. Application to a
three-phase armored cable with wire screens demonstrates a speed-up by a factor
of about 100 compared to a finite elements computation. The inclusion of
proximity effect in combination with the high efficiency makes the new method
very attractive for cable modeling within EMTP-type simulation tools.
Currently, these tools can only take skin effect into account."
"Laminated glass structures are formed by stiff layers of glass connected with
a compliant plastic interlayer. Due to their slenderness and heterogeneity,
they exhibit a complex mechanical response that is difficult to capture by
single-layer models even in the elastic range. The purpose of this paper is to
introduce an efficient and reliable finite element approach to the simulation
of the immediate response of laminated glass beams. It proceeds from a refined
plate theory due to Mau (1973), as we treat each layer independently and
enforce the compatibility by the Lagrange multipliers. At the layer level, we
adopt the finite-strain shear deformable formulation of Reissner (1972) and the
numerical framework by Ibrahimbegovi\'{c} and Frey (1993). The resulting system
is solved by the Newton method with consistent linearization. By comparing the
model predictions against available experimental data, analytical methods and
two-dimensional finite element simulations, we demonstrate that the proposed
formulation is reliable and provides accuracy comparable to the detailed
two-dimensional finite element analyzes. As such, it offers a convenient basis
to incorporate more refined constitutive description of the interlayer."
"Temperature stabilization of oil and gas wells is used to ensure stability
and prevent deformation of a subgrade estuary zone. In this work, we consider
the numerical simulation of thermal stabilization using vertical seasonal
freezing columns.
  A mathematical model of such problems is described by a time-dependent
temperature equation with phase transitions from water to ice. The resulting
equation is a standard nonlinear parabolic equation.
  Numerical implementation is based on the finite element method using the
package Fenics. After standard purely implicit approximation in time and simple
linearization, we obtain a system of linear algebraic equations. Because the
size of freezing columns are substantially less than the size of the modeled
area, we obtain mesh refinement near columns. Due to this, we get a large
system of equations which are solved using high performance computing systems."
"The Single Cut or Join (SCJ) operation on genomes, generalizing chromosome
evolution by fusions and fissions, is the computationally simplest known model
of genome rearrangement. While most genome rearrangement problems are already
hard when comparing three genomes, it is possible to compute in polynomial time
a most parsimonious SCJ scenario for an arbitrary number of genomes related by
a binary phylogenetic tree.
  Here we consider the problems of sampling and counting the most parsimonious
SCJ scenarios. We show that both the sampling and counting problems are easy
for two genomes, and we relate SCJ scenarios to alternating permutations.
However, for an arbitrary number of genomes related by a binary phylogenetic
tree, the counting and sampling problems become hard. We prove that if a Fully
Polynomial Randomized Approximation Scheme or a Fully Polynomial Almost Uniform
Sampler exist for the most parsimonious SCJ scenario, then RP = NP.
  The proof has a wider scope than genome rearrangements: the same result holds
for parsimonious evolutionary scenarios on any set of discrete characters."
"Constitutive models for concrete based on the microplane concept have
repeatedly proven their ability to well-reproduce its non-linear response on
material as well as structural scales. The major obstacle to a routine
application of this class of models is, however, the calibration of
microplane-related constants from macroscopic data. The goal of this paper is
two-fold: (i) to introduce the basic ingredients of a robust inverse procedure
for the determination of dominant parameters of the M4 model proposed by Bazant
and co-workers based on cascade Artificial Neural Networks trained by
Evolutionary Algorithm and (ii) to validate the proposed methodology against a
representative set of experimental data. The obtained results demonstrate that
the soft computing-based method is capable of delivering the searched response
with an accuracy comparable to the values obtained by expert users."
"We suggest modified bi-level approach for finding the best stacking sequence
of laminated composite structures subject to mechanical, blending and
manufacturing constraints. We propose to use both the number of plies laid up
at predefined angles and lamination parameters as independent variables at
outer (global) stage of bi-level scheme aimed to satisfy buckling, strain and
percentage constraints. Our formulation allows precise definition of the
feasible region of lamination parameters and greatly facilitates the solution
of inner level problem of finding the optimal stacking sequence."
"Choice of load signature or feature space is one of the most fundamental
design choices for non-intrusive load monitoring or energy disaggregation
problem. Electrical power quantities, harmonic load characteristics, canonical
transient and steady-state waveforms are some of the typical choices of load
signature or load signature basis for current research addressing appliance
classification and prediction. This paper expands and evaluates appliance load
signatures based on V-I trajectory - the mutual locus of instantaneous voltage
and current waveforms - for precision and robustness of prediction in
classification algorithms used to disaggregate residential overall energy use
and predict constituent appliance profiles. We also demonstrate the use of
variants of differential evolution as a novel strategy for selection of optimal
load models in context of energy disaggregation. A publicly available benchmark
dataset REDD is employed for evaluation purposes. Our experimental evaluations
indicate that these load signatures, in conjunction with a number of popular
classification algorithms, offer better or generally comparable overall
precision of prediction, robustness and reliability against dynamic, noisy and
highly similar load signatures with reference to electrical power quantities
and harmonic content. Herein, wave-shape features are found to be an effective
new basis of classification and prediction for semi-automated energy
disaggregation and monitoring."
"json2run is a tool to automate the running, storage and analysis of
experiments. The main advantage of json2run is that it allows to describe a set
of experiments concisely as a JSON-formatted parameter tree. It also supports
parallel execution of experiments, automatic parameter tuning through the
F-Race framework and storage and analysis of experiments with MongoDB and R."
"This paper deals with new proposals for the design of passive solutions
adapted to the climate of the highlands of Madagascar. While the strongest
population density is located in the central highlands, the problem of thermal
comfort in buildings occurs mainly during winter time. Currently, people use
raw wood to warm the poorly designed houses. This leads to a large scale
deforestation of the areas and causes erosion and environmental problems. The
methodology used consisted of the identification of a typical building and of a
typical meteorological year. Simulations were carried out using a thermal and
airflow software (CODYRUN) to improve each building component (roof, walls,
windows, and soil) in such a way as to estimate the influence of some technical
solutions on each component in terms of thermal comfort. The proposed solutions
also took into account the use of local materials and the standard of living of
the country."
"Service industries, such as ports, are attentive to their standards, a smooth
service flow and economic viability. Cost benefit analysis has proven itself as
a useful tool to support this type of decision making; it has been used by
businesses and governmental agencies for many years. In this book chapter we
demonstrate different modelling methods that are used for estimating input
factors required for conducting cost benefit analysis based on a single case
study. These methods are: scenario analysis, decision trees, Monte-Carlo
simulation modelling and discrete event simulation modelling. Our aims are, on
the one hand, to guide the analyst through the modelling processes and, on the
other hand, to demonstrate what additional decision support information can be
obtained from applying each of these modelling methods."
"Variance reduction techniques have been shown by others in the past to be a
useful tool to reduce variance in Simulation studies. However, their
application and success in the past has been mainly domain specific, with
relatively little guidelines as to their general applicability, in particular
for novices in this area. To facilitate their use, this study aims to
investigate the robustness of individual techniques across a set of scenarios
from different domains. Experimental results show that Control Variates is the
only technique which achieves a reduction in variance across all domains.
Furthermore, applied individually, Antithetic Variates and Control Variates
perform particularly well in the Cross-docking scenarios, which was previously
unknown."
"The fourth international workshop on Computational Models for Cell Processes
(CompMod 2013) took place on June 11, 2013 at the {\AA}bo Akademi University,
Turku, Finland, in conjunction with iFM 2013. The first edition of the workshop
(2008) took place in Turku, Finland, in conjunction with Formal Methods 2008,
the second edition (2009) took place in Eindhoven, the Netherlands, as well in
conjunction with Formal Methods 2009, and the third one took place in Aachen,
Germany, in conjunction with CONCUR 2013. This volume contains the final
versions of all contributions accepted for presentation at the workshop.
  The goal of the CompMod workshop series is to bring together researchers in
Computer Science and Mathematics (both discrete and continuous), interested in
the opportunities and the challenges of Systems Biology. The Program Committee
of CompMod 2013 selected 3 papers for presentation at the workshop. In
addition, we had two invited talks and five informal presentations.
  The scientific program of the workshop spans an interesting mix of approaches
to systems and even synthetic biology, encompassing several different modeling
approaches, ranging from quantitative to qualitative techniques, from
continuous to discrete mathematics, and from deterministic to stochastic
methods. We thank our invited speakers Daniela Besozzi (Universita degli Studi
di Milano, Milano, Italy) and Juho Rousu (Aalto University, Finland) for
accepting our invitation and for presenting some of their recent results at
CompMod 2013.
  The technical contributions address the mathematical modeling of the PDGF
signalling pathway, the canonical labelling of site graphs, rule-based modeling
of polymerization reactions, rule-based modeling as a platform for the analysis
of synthetic self-assembled nano-systems, robustness analysis of stochastic
systems, an algebraic approach to gene assembly in ciliates, and large-scale
text mining of biomedical literature."
"This paper summarizes the results in Integral Biomathics obtained to this
moment and provides an outlook for future research in the field."
"Wide-band cable models for the prediction of electromagnetic transients in
power systems require the accurate calculation of the cable series impedance as
function of frequency. A surface current approach was recently proposed for
systems of round solid conductors, with inclusion of skin and proximity
effects. In this paper we extend the approach to include tubular conductors,
allowing to model realistic cables with tubular sheaths, armors and pipes. We
also include the effect of a lossy ground. A noteworthy feature of the proposed
technique is the accurate prediction of proximity effects, which can be of
major importance in three-phase, pipe type, and closely-packed single-core
cables. The new approach is highly efficient compared to finite elements. In
the case of a cross-bonded cable system featuring three phase conductors and
three screens, the proposed technique computes the required 120 frequency
samples in only six seconds of CPU time."
"Three different variations of PSO algorithms, i.e. Canonical, Gaussian
Bare-bone and L\'evy Bare-bone PSO, are tested to optimize the ultimate oil
recovery of a large heavy oil reservoir. The performance of these algorithms
was compared in terms of convergence behaviour and the final optimization
results. It is found that, in general, all three types of PSO methods are able
to improve the objective function. The best objective function is found by
using the Canonical PSO, while the other two methods give similar results. The
Gaussian Bare-bone PSO may picks positions that are far away from the optimal
solution. The L\'evy Bare-bone PSO has similar convergence behaviour as the
Canonical PSO. For the specific optimization problem investigated in this
study, it is found that the temperature of the injection steam, CO2 composition
in the injection gas, and the gas injection rates have bigger impact on the
objective function, while steam injection rate and the liquid production rate
have less impact on the objective function."
"We introduce a simulation model of the port of Calais with a focus on the
operation of immigration controls. Our aim is to compare the cost and benefits
of different screening policies. Methodologically, we are trying to understand
the limits of discrete event simulation of rare events. When will they become
'too rare' for simulation to give meaningful results?"
"The main purpose of this work is to obtain a mathematical model consistent
with the thermal behavior of concentrating solar cookers, such as
Jorhejpataranskua. We also want to simulate different conditions respect to the
parameters involved of several materials for its construction and efficiency.
The model is expressed in terms of a coupled nonlinear system of differential
equations which are solved using Mathematica 8. The results obtained by our
model are compared with measurements of solar cooker in field testing
operation. We obtained good results in agreement with experimental data.
Moreover, the simulation results are used by calculating cooking power and
standardized cooking power of solar cooker for different parameters."
"This paper develops a computational framework for optimizing the parameters
of data assimilation systems in order to improve their performance. The
approach formulates a continuous meta-optimization problem for parameters; the
meta-optimization is constrained by the original data assimilation problem. The
numerical solution process employs adjoint models and iterative solvers. The
proposed framework is applied to optimize observation values, data weighting
coefficients, and the location of sensors for a test problem. The ability to
optimize a distributed measurement network is crucial for cutting down
operating costs and detecting malfunctions."
"The appropriate weather prediction is a challenging task and it can be
feasible with proper wind speed fluctuation analysis. In this current paper
daubechies-4 wavelet is used to analyze the winter wind speed fluctuations due
to lesser agitated wind data samples of winter. In summer abrupt changes in
wind speed occurs which creates difficulty for wavelets to keep proper track of
wind speed fluctuations. So, in that case the concept of the S-transform is
introduced."
"Performances of building energy innovations are most of the time dependent on
the external climate conditions. This means a high performance of a specific
innovation in a certain part of Europe, does not imply the same performances in
other regions. The mapping of simulated building performances at the EU scale
could prevent the waste of potential good ideas by identifying the best region
for a specific innovation. This paper presents a methodology for obtaining maps
of performances of building innovations that are virtually spread over whole
Europe. It is concluded that these maps are useful for finding regions at the
EU where innovations have the highest expected performances."
"In this study we propose a novel method to successfully detect the ADRs using
feature matrix and feature selection. A feature matrix, which characterizes the
medical events before patients take drugs or after patients take drugs, is
created from THIN database. The feature selection method of Student's t-test is
used to detect the significant features from thousands of medical events. The
significant ADRs, which are corresponding to significant features, are
detected. Experiments are performed on the drug Pioglitazone. Compared to other
computerized methods, our proposed method achieves good performance."
"Adverse drug reactions (ADRs) are big concern for public health. ADRs are one
of most common causes to withdraw some drugs from markets. Now two major
methods for detecting ADRs are spontaneous reporting system (SRS), and
prescription event monitoring (PEM). The World Health Organization (WHO)
defines a signal in pharmacovigilance as ""any reported information on a
possible causal relationship between an adverse event and a drug, the
relationship being unknown or incompletely documented previously"". For
spontaneous reporting systems, many machine learning methods are used to detect
ADRs, such as Bayesian confidence propagation neural network (BCPNN), decision
support methods, genetic algorithms, knowledge based approaches, etc. One
limitation is the reporting mechanism to submit ADR reports, which has serious
underreporting and is not able to accurately quantify the corresponding risk.
Another limitation is hard to detect ADRs with small number of occurrences of
each drug-event association in the database. In this paper we propose feature
selection approach to detect ADRs from The Health Improvement Network (THIN)
database. First a feature matrix, which represents the medical events for the
patients before and after taking drugs, is created by linking patients'
prescriptions and corresponding medical events together. Then significant
features are selected based on feature selection methods, comparing the feature
matrix before patients take drugs with one after patients take drugs. Finally
the significant ADRs can be detected from thousands of medical events based on
corresponding features. Experiments are carried out on the drug Atorvastatin.
Good performance is achieved."
"Option contracts can be valued by using the Black-Scholes equation, a partial
differential equation with initial conditions. An exact solution for European
style options is known. The computation time and the error need to be minimized
simultaneously. In this paper, the authors have solved the Black-Scholes
equation by employing a reasonably accurate implicit method. Options with known
analytic solutions have been evaluated. Furthermore, an overall second order
accurate space and time discretization is proposed in this paper Keywords:
Computational finance, implicit methods, finite differences, call/put options."
"We present a series of optimal (in the sense of least-squares) curve fits for
the stiffened gas equation of state for single-phase liquid water. At high
pressures and (subcritical) temperatures, the parameters produced by these
curve fits are found to have very small relative errors: less than $1\%$ in the
pressure model, and less than $2\%$ in the temperature model. At low pressures
and temperatures, especially near the liquid-vapor transition line, the error
in the curve fits increases rapidly. The smallest pressure value for which
curve fits are reported in the present work is 25 MPa, high enough to ensure
that the fluid remains a single-phase liquid up to the maximum subcritical
temperature of approximately 647K."
"A nodal Discontinuous Galerkin (DG) method is derived for the analysis of
time-domain (TD) scattering from doubly periodic PEC/dielectric structures
under oblique interrogation. Field transformations are employed to elaborate a
formalism that is free from any issues with causality that are common when
applying spatial periodic boundary conditions simultaneously with incident
fields at arbitrary angles of incidence. An upwind numerical flux is derived
for the transformed variables, which retains the same form as it does in the
original Maxwell problem for domains without explicitly imposed periodicity.
This, in conjunction with the amenability of the DG framework to non-conformal
meshes, provides a natural means of accurately solving the first order TD
Maxwell equations for a number of periodic systems of engineering interest.
Results are presented that substantiate the accuracy and utility of our method."
"The spurious pressure jump at a contact discontinuity, in SPH simulations of
the compressible Euler equations is investigated. From the spatiotemporal
behaviour of the error, the SPH pressure jump is likened to entropy errors
observed for artificial viscosity based finite difference/volume schemes. The
error is observed to be generated at start-up and dissipation is the only
recourse to mitigate it's effect. We show that similar errors are generated for
the Lagrangian plus remap version of the Piecewise Parabolic Method (PPM)
finite volume code (PPMLR). Through a comparison with the direct Eulerian
version of the PPM code (PPMDE), we argue that a lack of diffusion across the
material wave (contact discontinuity) is responsible for the error in PPMLR. We
verify this hypothesis by constructing a more dissipative version of the remap
code using a piecewise constant reconstruction. As an application to SPH, we
propose a hybrid GSPH scheme that adds the requisite dissipation by utilizing a
more dissipative Riemann solver for the energy equation. The proposed
modification to the GSPH scheme, and it's improved treatment of the anomaly is
verified for flows with strong shocks in one and two dimensions. The result
that dissipation must act across the density and energy equations provides a
consistent explanation for many of the hitherto proposed ""cures"" or ""fixes"" for
the problem."
"Many theoretical works and tools on epidemiological field reflect the
emphasis on decision-making Tools by both public health and the scientific
community, which continues to increase. Indeed, in the epidemiological field,
modeling tools are proving a very important way in helping to make decision.
However, the variety, the large volume of data and the nature of epidemics lead
us to seek solutions to alleviate the heavy burden imposed on both experts and
developers. In this paper, we present a new approach: the passage of an
epidemic model realized in Bio-PEPA to a narrative language using the basics of
SBML language. Our goal is to allow on one hand, epidemiologists to verify and
validate the model, and the other hand, developers to optimize the model in
order to achieve a better model of decision making. We also present some
preliminary results and some suggestions to improve the simulated model."
"This survey presents a literature review on friction stir welding (FSW)
modeling with a special focus on the heat generation due to the contact
conditions between the FSW tool and the workpiece. The physical process is
described and the main process parameters that are relevant to its modeling are
highlighted. The contact conditions (sliding/sticking) are presented as well as
an analytical model that allows estimating the associated heat generation. The
modeling of the FSW process requires the knowledge of the heat loss mechanisms,
which are discussed mainly considering the more commonly adopted formulations.
Different approaches that have been used to investigate the material flow are
presented and their advantages/drawbacks are discussed. A reliable FSW process
modeling depends on the fine tuning of some process and material parameters.
Usually, these parameters are achieved with base on experimental data. The
numerical modeling of the FSW process can help to achieve such parameters with
less effort and with economic advantages."
"In the measurement process, there are many parameters affecting the
measurement results: the influence of the probe system, material stiffness of
measured workpiece, the calibration of the probe with a reference sphere, the
thermal effects. We want to obtain the limits of a measurement methodology to
be able to validate a result. The study is applied to a simple part. We observe
the dispersion of the position of different drilled holes (XYZ values in a
coordinate system) when we change the quality of the part and the method of
calculation. We use the Design of Experiment (Taguchi method) to realize our
study. We study the influence of the part quality on a measurement results. We
consider two parameters to define the part quality (flatness and
perpendicularity). We will also study the influence of different methods of
calculation to determine the coordinate system. We can use two options in
Metrolog XG software (tangent plane with or without orientation constraint).
The originality of this paper is that we present a method for the design of
experiment that uses CATIA (CAD system) to generate the measured parts. In this
way we can realize a design of experiment with a largest number of experimental
results. This is a positive point for a statistical analysis. We are also free
to define the parts we want to study without manufacturing difficulties."
"In the present paper we have reported a wavelet based time-frequency
multiresolution analysis of an ECG signal. The ECG (electrocardiogram), which
records hearts electrical activity, is able to provide with useful information
about the type of Cardiac disorders suffered by the patient depending upon the
deviations from normal ECG signal pattern. We have plotted the coefficients of
continuous wavelet transform using Morlet wavelet. We used different ECG signal
available at MIT-BIH database and performed a comparative study. We
demonstrated that the coefficient at a particular scale represents the presence
of QRS signal very efficiently irrespective of the type or intensity of noise,
presence of unusually high amplitude of peaks other than QRS peaks and Base
line drift errors. We believe that the current studies can enlighten the path
towards development of very lucid and time efficient algorithms for identifying
and representing the QRS complexes that can be done with normal computers and
processors."
"Electrospray tandem mass spectrometry (ESI-MS/MS) is commonly used in high
throughput metabolomics. One of the key obstacles to the effective use of this
technology is the difficulty in interpreting measured spectra to accurately and
efficiently identify metabolites. Traditional methods for automated metabolite
identification compare the target MS or MS/MS spectrum to the spectra in a
reference database, ranking candidates based on the closeness of the match.
However the limited coverage of available databases has led to an interest in
computational methods for predicting reference MS/MS spectra from chemical
structures.
  This work proposes a probabilistic generative model for the MS/MS
fragmentation process, which we call Competitive Fragmentation Modeling (CFM),
and a machine learning approach for learning parameters for this model from
MS/MS data. We show that CFM can be used in both a MS/MS spectrum prediction
task (ie, predicting the mass spectrum from a chemical structure), and in a
putative metabolite identification task (ranking possible structures for a
target MS/MS spectrum).
  In the MS/MS spectrum prediction task, CFM shows significantly improved
performance when compared to a full enumeration of all peaks corresponding to
substructures of the molecule. In the metabolite identification task, CFM
obtains substantially better rankings for the correct candidate than existing
methods (MetFrag and FingerID) on tripeptide and metabolite data, when querying
PubChem or KEGG for candidate structures of similar mass."
"Antitubercular activity of 5-nitrofuran-2-yl Derivatives series were
subjected to Quantitative Structure Activity Relationship (QSAR) Analysis with
an effort to derive and understand a correlation between the biological
activity as response variable and different molecular descriptors as
independent variables. QSAR models are built using 40 molecular descriptor
dataset. Different statistical regression expressions were got using Partial
Least Squares (PLS),Multiple Linear Regression (MLR) and Principal Component
Regression (PCR) techniques. The among these technique, Partial Least Square
Regression (PLS) technique has shown very promising result as compared to MLR
technique A QSAR model was build by a training set of 30 molecules with
correlation coefficient ($r^2$) of 0.8484, significant cross validated
correlation coefficient ($q^2$) is 0.0939, F test is 48.5187, ($r^2$) for
external test set (pred$_r^2$) is -0.5604, coefficient of correlation of
predicted data set (pred$_r^2se$) is 0.7252 and degree of freedom is 26 by
Partial Least Squares Regression technique."
"Missing data imputation is an important research topic in data mining.
Large-scale Molecular descriptor data may contains missing values (MVs).
However, some methods for downstream analyses, including some prediction tools,
require a complete descriptor data matrix. We propose and evaluate an iterative
imputation method MiFoImpute based on a random forest. By averaging over many
unpruned regression trees, random forest intrinsically constitutes a multiple
imputation scheme. Using the NRMSE and NMAE estimates of random forest, we are
able to estimate the imputation error. Evaluation is performed on two molecular
descriptor datasets generated from a diverse selection of pharmaceutical fields
with artificially introduced missing values ranging from 10% to 30%. The
experimental result demonstrates that missing values has a great impact on the
effectiveness of imputation techniques and our method MiFoImpute is more robust
to missing value than the other ten imputation methods used as benchmark.
Additionally, MiFoImpute exhibits attractive computational efficiency and can
cope with high-dimensional data."
"From the past decade outlier detection has been in use. Detection of outliers
is an emerging topic and is having robust applications in medical sciences and
pharmaceutical sciences. Outlier detection is used to detect anomalous
behaviour of data. Typical problems in Bioinformatics can be addressed by
outlier detection. A computationally fast method for detecting outliers is
shown, that is particularly effective in high dimensions. PrCmpOut algorithm
make use of simple properties of principal components to detect outliers in the
transformed space, leading to significant computational advantages for high
dimensional data. This procedure requires considerably less computational time
than existing methods for outlier detection. The properties of this estimator
(Outlier error rate (FN), Non-Outlier error rate(FP) and computational costs)
are analyzed and compared with those of other robust estimators described in
the literature through simulation studies. Numerical evidence based Oxazolines
and Oxazoles molecular descriptor dataset shows that the proposed method
performs well in a variety of situations of practical interest. It is thus a
valuable companion to the existing outlier detection methods."
"In this contribution a practical approach to determine and store position
dependent parameters is presented. These parameters can be obtained, among
others, using experimental results or expert knowledge and are stored in
'Information Maps'. Each Information Map can be interpreted as a kind of static
grid map and the framework allows to link different maps hierarchically. The
Information Maps can be local or global, with static and dynamic information in
it. One application of Information Maps is the representation of position
dependent characteristics of a sensor. Thus, for instance, it is feasible to
store arbitrary attributes of a sensor's preprocessing in an Information Map
and utilize them by simply taking the map value at the current position. This
procedure is much more efficient than using the attributes of the sensor
itself. Some examples where and how Information Maps can be used are presented
in this publication. The Information Map is meant to be a simple and practical
approach to the problem of position dependent parameterization in all kind of
algorithms when the analytical description is not possible or can not be
implemented efficiently."
"Among the various features of amino acids, the hydrophobic property has most
visible impact on stability of a sequence folding. This is mentioned in many
protein folding related work, in this paper we more elaborately discuss the
computational impact of the well defined hydrophobic aspect in determining
stability, approach with the help of a developed free energy computing
algorithm covering various aspects preprocessing of an amino acid sequence,
generating the folding and calculating free energy. Later discussing its use in
protein structure related research work."
"The article discusses building models based on the reconstructed attractors
of the time series. Discusses the use of the properties of dynamical chaos,
namely to identify the strange attractors structure models. Here is used the
group properties of differential equations, which consist in the symmetry of
particular solutions. Examples of modeling engineering systems are given."
"Macroscopically heterogeneous materials, characterised mostly by comparable
heterogeneity lengthscale and structural sizes, can no longer be modelled by
deterministic approach instead. It is convenient to introduce stochastic
approach with uncertain material parameters quantified as random fields and/or
random variables. The present contribution is devoted to propagation of these
uncertainties in mechanical modelling of inelastic behaviour. In such case the
Monte Carlo method is the traditional approach for solving the proposed
problem. Nevertheless, convergence rate is relatively slow, thus new methods
(e.g. stochastic Galerkin method, stochastic collocation approach, etc.) have
been recently developed to offer fast convergence for sufficiently smooth
solution in the probability space. Our goal is to accelerate the uncertainty
propagation using a polynomial chaos expansion based on stochastic collocation
method. The whole concept is demonstrated on a simple numerical example of
uniaxial test at a material point where interesting phenomena can be clearly
understood."
"Recent advances in Agricultural Engineering include image processing,
robotics and geographic information systems (GIS). Some tasks are still
accomplished manually, like drawing plantation lines that optimize
productivity. Herewith we present an algorithm to find the optimal plantation
lines in linear time. The algorithm is based upon classical results of Geometry
which enabled a source code with only 573 lines. We have implemented it in
Matlab for sugar cane, and it can be easily adapted to other crops like coffee,
maize and soy."
"This research proposes a computation approach to address the evaluation of
end product machining accuracy in elliptical surfaced helical pipe bending
using 6dof parallel manipulator as a pipe bender. The target end product is
wearable metal muscle supporters used in build-to-order welfare product
manufacturing. This paper proposes a product testing model that mainly corrects
the surface direction estimation errors of existing least squares ellipse
fittings, followed by arc length and central angle evaluations. This
post-machining modelling requires combination of reverse rotations and
translations to a specific location before accuracy evaluation takes place,
i.e. the reverse comparing to pre-machining product modelling. This specific
location not only allows us to compute surface direction but also the amount of
excessive surface twisting as a rotation angle about a specified axis, i.e.
quantification of surface torsion. At first we experimented three ellipse
fitting methods such as, two least-squares fitting methods with Bookstein
constraint and Trace constraint, and one non- linear least squares method using
Gauss-Newton algorithm. From fitting results, we found that using Trace
constraint is more reliable and designed a correction filter for surface
torsion observation. Finally we apply 2D total least squares line fitting
method with a rectification filter for surface direction detection."
"This research proposes an effective vertical clustering strategy of 3D data
in an elliptical helical shape based on 2D geometry. The clustering object is
an elliptical cross-sectioned metal pipe which is been bended in to an
elliptical helical shape which is used in wearable muscle support designing for
welfare industry. The aim of this proposed method is to maximize the vertical
clustering (vertical partitioning) ability of surface data in order to run the
product evaluation process addressed in research [2]. The experiment results
prove that the proposed method outperforms the existing threshold no of
clusters that preserves the vertical shape than applying the conventional 3D
data. This research also proposes a new product testing strategy that provides
the flexibility in computer aided testing by not restricting the sequence
depending measurements which apply weight on measuring process. The clustering
algorithms used for the experiments in this research are self-organizing map
(SOM) and K-medoids."
"This paper validates 3D simulation results of electron beam melting (EBM)
processes comparing experimental and numerical data. The physical setup is
presented which is discretized by a three dimensional (3D) thermal lattice
Boltzmann method (LBM). An experimental process window is used for the
validation depending on the line energy injected into the metal powder bed and
the scan velocity of the electron beam. In the process window the EBM products
are classified into the categories, porous, good and swelling, depending on the
quality of the surface. The same parameter sets are used to generate a
numerical process window. A comparison of numerical and experimental process
windows shows a good agreement. This validates the EBM model and justifies
simulations for future improvements of EBM processes. In particular numerical
simulations can be used to explain future process window scenarios and find the
best parameter set for a good surface quality and dense products."
"Option contracts can be valued by using the Black-Scholes equation, a partial
differential equation with initial conditions. An exact solution for European
style options is known. The computation time and the error need to be minimized
simultaneously. In this paper, the authors have solved the Black-Scholes
equation by employing a reasonably accurate implicit method. Options with known
analytic solutions have been evaluated. Furthermore, an overall second order
accurate space and time discretization has been accomplished in this paper."
"Surrogate models for computational simulations are input-output
approximations that allow computationally intensive analyses, such as
uncertainty propagation and inference, to be performed efficiently. When a
simulation output does not depend smoothly on its inputs, the error and
convergence rate of many approximation methods deteriorate substantially. This
paper details a method for efficiently localizing discontinuities in the input
parameter domain, so that the model output can be approximated as a piecewise
smooth function. The approach comprises an initialization phase, which uses
polynomial annihilation to assign function values to different regions and thus
seed an automated labeling procedure, followed by a refinement phase that
adaptively updates a kernel support vector machine representation of the
separating surface via active learning. The overall approach avoids structured
grids and exploits any available simplicity in the geometry of the separating
surface, thus reducing the number of model evaluations required to localize the
discontinuity. The method is illustrated on examples of up to eleven
dimensions, including algebraic models and ODE/PDE systems, and demonstrates
improved scaling and efficiency over other discontinuity localization
approaches."
"Homogenization of a simultaneous heat and moisture flow in a masonry wall is
presented in this paper. The principle objective is to examine an impact of the
assumed imperfect hydraulic contact on the resulting homogenized properties.
Such a contact is characterized by a certain mismatching resistance allowing us
to represent a discontinuous evolution of temperature and moisture fields
across the interface, which is in general attributed to discontinuous capillary
pressures caused by different pore size distributions of the adjacent porous
materials. In achieving this, two particular laboratory experiments were
performed to provide distributions of temperature and relative humidity in a
sample of the masonry wall, which in turn served to extract the corresponding
jumps and subsequently to obtain the required interface transition parameters
by matching numerical predictions and experimental results. The results suggest
a low importance of accounting for imperfect hydraulic contact for the
derivation of macroscopic homogenized properties. On the other hand, they
strongly support the need for a fully coupled multi-scale analysis due to
significant dependence of the homogenized properties on actual moisture
gradients and corresponding values of both macroscopic temperature and relative
humidity."
"The aim of presented paper is modeling of degradation processes in historical
mortars exposed to moisture impact during freezing. Internal damage caused by
ice crystallization in pores is one of the most important factors limiting the
service life of historical structures. Coupling the transport processes with
the mechanical part will allow us to address the impact of moisture on the
durability, strength and stiffness of mortars. This should be accomplished with
the help of a complex thermo-hygro-mechanical model representing one of the
prime objectives of this work. The proposed formulation is based on the
extension of the classical poroelasticity models with the damage mechanics. An
example of two-dimensional moisture transport in the environment with
temperature below freezing point is presented to support the theoretical
derivations."
"This monograph presents a geometric modeling method nonlinear dynamical
systems from experimental data . basis method is a qualitative approach to the
analysis of linear models and construction of the symmetry groups of attractors
of dynamical systems with controls . A theoretical study including the central
theorem manifold defining conditions of existence of the class in question
models in the local area , taking into account the group properties ,
estimation algorithms invariant characteristics , methods of constructing
models and identifiable description of the results obtained using the method
for simulation -driven engineering processes . included two application is the
development of the proposed approach : identification of groups symmetries on
the phase portraits of dynamical systems and the method of constructing neural
network predictive models"
"Pattern matching is commonly required in many application areas and
bioinformatics is a major area of interest that requires both exact and
approximate pattern matching. Much work has been done in this area, yet there
is still a significant space for improvement in efficiency, flexibility, and
throughput. This paper presents a hardware software co-design of Aho-Corasick
algorithm in Nios II soft-processor and a study on its scalability for a
pattern matching application. A software only approach is used to compare the
throughput and the scalability of the hardware software co-design approach.
According to the results we obtained, we conclude that the hardware software
co-design implementation shows a maximum of 10 times speed up for pattern size
of 1200 peptides compared to the software only implementation. The results also
show that the hardware software co-design approach scales well for increasing
data size compared to the software only approach."
"Protein inference plays a vital role in the proteomics study. Two major
approaches could be used to handle the problem of protein inference; top-down
and bottom-up. This paper presents a framework for protein inference, which
uses hardware accelerated protein inference framework for handling the most
important step in a bottom-up approach, viz. peptide identification during the
assembling process. In our framework, identified peptides and their
probabilities are used to predict the most suitable reference protein cluster
for a given input amino acid sequence with the probability of identified
peptides. The framework is developed on an FPGA where hardware software
co-design techniques are used to accelerate the computationally intensive parts
of the protein inference process. In the paper we have measured, compared and
reported the time taken for the protein inference process in our framework
against a pure software implementation."
"The prediction of uncertain and predictive nonlinear systems is an important
and challenging problem. Fuzzy logic models are often a good choice to describe
such systems however in many cases these become complex soon. commonlly, too
less effort is put into descriptor selection and in the creation of suitable
local rules. Moreover, in common no model reduction is applied, while this may
analyze the model by removing redundant data. This paper suggests a combined
method that deal with these issues in order to create compact Takagi Sugeno
(TS) models that can be effectively used to represent complex predictive
systems. A new fuzzy clustering method is come up with for the identification
of compact TS-fuzzy models. The best relevant consequent variables of the TS
model are choosen by an orthogonal least squares technique based on the
obtained clusters.For the selection of the relevant antecedent (scheduling)
variables a new method has been developed based on Fisher's interclass
separability basis. This complete approach is demonstrated by means of the
Oxazolines and Oxazoles derivatives as antituberculosis agent for nonlinear
regression benchmark. The results are compared with results obtained by
neuro-fuzzy i.e. ANFIS algorithm and advanced fuzzyy clustering techniques i.e
FMID toolbox ."
"This paper investigates in hatching process strategies for additive
manufacturing using an electron beam by numerical simulations. The underlying
physical model and the corresponding three dimensional thermal free surface
lattice Boltzmann method of the simulation software are briefly presented. The
simulation software has already been validated on the basis of experiments up
to 1.2 kW beam power by hatching a cuboid with a basic process strategy,
whereby the results are classified into `porous', `good' and `uneven',
depending on their relative density and top surface smoothness. In this paper
we study the limitations of this basic process strategy in terms of higher beam
powers and scan velocities to exploit the future potential of high power
electron beam guns up to 10 kW. Subsequently, we introduce modified process
strategies, which circumvent these restrictions, to build the part as fast as
possible under the restriction of a fully dense part with a smooth top surface.
These process strategies are suitable to reduce the build time and costs,
maximize the beam power usage and therefore use the potential of high power
electron beam guns."
"The paper presents a new study method of mechanic vibrations with the help of
the data acquisition systems. The study of vibrations with the help of data
acquisition systems allows the solving of some engineering problems connected
to the measurement of some parameters which are difficult to measure having in
view the improvement of the technical performances of the industrial equipment
or devices"
"The availability of accurate and broadband models for underground and
submarine cable systems is of paramount importance for the correct prediction
of electromagnetic transients in power grids. Recently, we proposed the MoM-SO
method for extracting the series impedance of power cables while accounting for
skin and proximity effect in the conductors. In this paper, we extend the
method to include ground return effects and to handle cables placed inside a
tunnel. Numerical tests show that the proposed method is more accurate than
widely-used analytic formulas, and is much faster than existing proximity-aware
approaches like finite elements. For a three-phase cable system in a tunnel,
the proposed method requires only 0.3 seconds of CPU time per frequency point,
against the 8.3 minutes taken by finite elements, for a speed up beyond 1000 X."
"Advances in life sciences over the last few decades have lead to the
generation of a huge amount of biological data. Computing research has become a
vital part in driving biological discovery where analysis and categorization of
biological data are involved. String matching algorithms can be applied for
protein/gene sequence matching and with the phenomenal increase in the size of
string databases to be analyzed, software implementations of these algorithms
seems to have hit a hard limit and hardware acceleration is increasingly being
sought. Several hardware platforms such as Field Programmable Gate Arrays
(FPGA), Graphics Processing Units (GPU) and Chip Multi Processors (CMP) are
being explored as hardware platforms. In this paper, we give a comprehensive
overview of the literature on hardware acceleration of string matching
algorithms, we take an FPGA hardware exploration and expedite the design time
by a design automation technique. Further, our design automation is also
optimized for better hardware utilization through optimizing the number of
peptides that can be represented in an FPGA tile. The results indicate
significant improvements in design time and hardware utilization which are
reported in this paper."
"At this article will be created a software written in visual basic for
efficiency and penetration calculation in a fibrous filter medium for given
values of particles diameter that are retained in the filter. Initially, will
become report of mathematical models of air filtration in fibrous filters media
and then will develop the code and the graphical interface of application, that
are the base for software creation in the visual basic platform."
"This paper aims at providing a survey on the problems that can be easily
addressed by clonalbased cellular automata in bioinformatics. Researchers try
to address the problems in bioinformatics independent of each problem. None of
the researchers has tried to relate the major problems in bioinformatics and
find a solution using common frame work. We tried to find various problems in
bioinformatics which can be addressed easily by clonal based cellular automata.
Extensive literature survey is conducted. We have considered some papers in
various journals and conferences for conduct of our research. This paper
provides intuition towards relating various problems in bioinformatics
logically and tries to attain a common frame work with respect to clonal based
cellular automata classifier for addressing the same."
"Flexible piezoelectric devices made of polymeric materials are widely used
for micro- and nano-electro-mechanical systems. In particular, numerous recent
applications concern energy harvesting. Due to the importance of computational
modeling to understand the influence that microscale geometry and constitutive
variables exert on the macroscopic behavior, a numerical approach is developed
here for multiscale and multiphysics modeling of thin piezoelectric sheets made
of aligned arrays of polymeric nanofibers, manufactured by electrospinning. At
the microscale, the representative volume element consists in piezoelectric
polymeric nanofibers, assumed to feature a piezoelastic behavior and subjected
to electromechanical contact constraints. The latter are incorporated into the
virtual work equations by formulating suitable electric, mechanical and
coupling potentials and the constraints are enforced by using the penalty
method. From the solution of the micro-scale boundary value problem, a suitable
scale transition procedure leads to identifying the performance of a
macroscopic thin piezoelectric shell element."
"Recent results in telecardiology show that compressed sensing (CS) is a
promising tool to lower energy consumption in wireless body area networks for
electrocardiogram (ECG) monitoring. However, the performance of current
CS-based algorithms, in terms of compression rate and reconstruction quality of
the ECG, still falls short of the performance attained by state-of-the-art
wavelet based algorithms. In this paper, we propose to exploit the structure of
the wavelet representation of the ECG signal to boost the performance of
CS-based methods for compression and reconstruction of ECG signals. More
precisely, we incorporate prior information about the wavelet dependencies
across scales into the reconstruction algorithms and exploit the high fraction
of common support of the wavelet coefficients of consecutive ECG segments.
Experimental results utilizing the MIT-BIH Arrhythmia Database show that
significant performance gains, in terms of compression rate and reconstruction
quality, can be obtained by the proposed algorithms compared to current
CS-based methods."
"A revised Incremental Conductance (IncCond) maximum power point tracking
(MPPT) algorithm for PV generation systems is proposed in this paper. The
commonly adopted traditional IncCond method uses a constant step size for
voltage adjustment and is difficult to achieve both a good tracking performance
and quick elimination of the oscillations, especially under the dramatic
changes of the environment conditions. For the revised algorithm, the
incremental voltage change step size is adaptively adjusted based on the slope
of the power-voltage (P-V) curve. An accelerating factor and a decelerating
factor are further applied to adjust the voltage step change considering
whether the sign of the P-V curve slope remains the same or not in a subsequent
tracking step. In addition, the upper bound of the maximum voltage step change
is also updated considering the information of sign changes. The revised MPPT
algorithm can quickly track the maximum power points (MPPs) and remove the
oscillation of the actual operation points around the real MPPs. The
effectiveness of the revised algorithm is demonstrated using a simulation."
"Determination of boiling range of xylene mixed in PX device is currently a
crucial topic in the practical applications because of the recent disputes of
PX project in China. In our study, instead of determining the boiling range of
xylene mixed by traditional approach in laboratory or industry, we successfully
established two Artificial Neural Networks (ANNs) models to determine the
initial boiling point and final boiling point respectively. Results show that
the Multilayer Feedforward Neural Networks (MLFN) model with 7 nodes (MLFN-7)
is the best model to determine the initial boiling point of xylene mixed, with
the RMS error 0.18; while the MLFN model with 4 nodes (MLFN-4) is the best
model to determine the final boiling point of xylene mixed, with the RMS error
0.75. The training and testing processes both indicate that the models we
developed are robust and precise. Our research can effectively avoid the damage
of the PX device to human body and environment."
"Multibody dynamics simulations have become widely used tools for vehicle
systems analysis and design. As this approach evolves, it becomes able to
provide additional information for various types of analyses. One very
important direction is the optimization of multibody systems. Sensitivity
analysis of multibody system dynamics is essential for design optimization.
Dynamic sensitivities, when needed, are often calculated by means of finite
differences. However, depending of the number of parameters involved, this
procedure can be computationally expensive. Moreover, in many cases the results
suffer from low accuracy when real perturbations are used. This paper develops
the adjoint sensitivity analysis of multibody systems in the context of penalty
formulations. The resulting sensitivities are applied to perform dynamical
optimization of a full vehicle system."
"Wood increment is critical information in forestry management. Previous
studies used mathematics models to describe complex growing pattern of forest
stand, in order to determine the dynamic status of growing forest stand in
multiple conditions. In our research, we aimed at studying non-linear
relationships to establish precise and robust Artificial Neural Networks (ANN)
models to predict the precise values of tree height and forest stock volume
based on data of Chinese fir. Results show that Multilayer Feedforward Neural
Networks with 4 nodes (MLFN-4) can predict the tree height with the lowest RMS
error (1.77); Multilayer Feedforward Neural Networks with 7 nodes (MLFN-7) can
predict the forest stock volume with the lowest RMS error (4.95). The training
and testing process have proved that our models are precise and robust."
"Abrasion resistance of solution polymerized styrene-butadiene rubber (SSBR)
based composites is a typical and crucial property in practical applications.
Previous studies show that the abrasion resistance can be calculated by the
multiple linear regression model. In our study, considering this relationship
can also be described into the non-linear conditions, a Multilayer Feed-forward
Neural Networks model with 3 nodes (MLFN-3) was successfully established to
describe the relationship between the abrasion resistance and other properties,
using 23 groups of data, with the RMS error 0.07. Our studies have proved that
Artificial Neural Networks (ANN) model can be used to predict the SSBR-based
composites, which is an accurate and robust process."
"Fast-oopsi was developed by Joshua Vogelstein in 2009, which is now widely
used to extract neuron spike activities from calcium fluorescence signals.
Here, we propose detailed implementation of the fast-oopsi algorithm in python
programming language. Some corrections are also made to the original fast-oopsi
paper."
"Considering lateral influence from adjacent lane, an improved car-following
model is developed in this paper. Then linear and non-linear stability analyses
are carried out. The modified Korteweg-de Vries (MKdV) equation is derived with
the kink-antikink soliton solution. Numerical simulations are implemented and
the result shows good consistency with theoretical study."
"System dynamics (SD) is an effective approach for helping reveal the temporal
behavior of complex systems. Although there have been recent developments in
expanding SD to include systems' spatial dependencies, most applications have
been restricted to the simulation of diffusion processes; this is especially
true for models on structural change (e.g. LULC modeling). To address this
shortcoming, a Python program is proposed to tightly couple SD software to a
Geographic Information System (GIS). The approach provides the required
capacities for handling bidirectional and synchronized interactions of
operations between SD and GIS. In order to illustrate the concept and the
techniques proposed for simulating structural changes, a fictitious environment
called Daisyworld has been recreated in a spatial system dynamics (SSD)
environment. The comparison of spatial and non-spatial simulations emphasizes
the importance of considering spatio-temporal feedbacks. Finally, practical
applications of structural change models in agriculture and disaster management
are proposed."
"Predicting protein secondary structure using lattice model is one of the most
studied computational problem in bioinformatics. Here secondary structure or
three dimensional structure of protein is predicted from its amino acid
sequence. Secondary structure refers to local sub-structures of protein. Mostly
founded secondary structures are alpha helix and beta sheets. Since, it is a
problem of great potential complexity many simplified energy model have been
proposed in literature on basis of interaction of amino acid residue in
protein. Here we use well researched Hydrophobic-Polar (HP) energy model. In
this paper, we proposed hexagonal prism lattice with diagonal that can overcome
the problems of other lattice structure, e.g., parity problem. We give two
approximation algorithm for protein folding on this lattice. Our first
algorithm leads us to similar structure of helix structure which is commonly
found in protein structure. This motivated us to find next algorithm which
improves the algorithm ratio of 9/7."
"Wind Energy is a useful resource for Renewable energy purpose. Wind speed
plays a vital role for wind energy calculation of certain location. So, it is
very much necessary to know the wind speed data characteristics. In this paper
fourier and wavelet transform are applied to study the wind speed data. We have
compared wind speed of winter with summer by taking their speed into account
using various discrete wavelets namely Haar and Daubechies-4 (Db-4). Also the
periodicity of wind speed is checked using Continuous Wavelet Transform (MCWT)
like Morlet. Thereafter a comparative study is done for detecting the
periodicity of both summer and winter. Then wavelet coherence is checked
between these two data for extracting the phase coherency information."
"Motivation: Intimately tied to assembly quality is the complexity of the de
Bruijn graph built by the assembler. Thus, there have been many paradigms
developed to decrease the complexity of the de Bruijn graph. One obvious
combinatorial paradigm for this is to allow the value of $k$ to vary; having a
larger value of $k$ where the graph is more complex and a smaller value of $k$
where the graph would likely contain fewer spurious edges and vertices. One
open problem that affects the practicality of this method is how to predict the
value of $k$ prior to building the de Bruijn graph. We show that optimal values
of $k$ can be predicted prior to assembly by using the information contained in
a phylogenetically-close genome and therefore, help make the use of multiple
values of $k$ practical for genome assembly.
  Results: We present HyDA-Vista, which is a genome assembler that uses
homology information to choose a value of $k$ for each read prior to the de
Bruijn graph construction. The chosen $k$ is optimal if there are no sequencing
errors and the coverage is sufficient. Fundamental to our method is the
construction of the {\em maximal sequence landscape}, which is a data structure
that stores for each position in the input string, the largest repeated
substring containing that position. In particular, we show the maximal sequence
landscape can be constructed in $O(n + n \log n)$-time and $O(n)$-space.
HyDA-Vista first constructs the maximal sequence landscape for a homologous
genome. The reads are then aligned to this reference genome, and values of $k$
are assigned to each read using the maximal sequence landscape and the
alignments. Eventually, all the reads are assembled by an iterative de Bruijn
graph construction method. Our results and comparison to other assemblers
demonstrate that HyDA-Vista achieves the best assembly of {\em E. coli} before
repeat resolution or scaffolding."
"EnKF-C provides a light-weight generic framework for off-line data
assimilation into large-scale layered geophysical models with the ensemble
Kalman filter (EnKF). It is coded in C for GNU/Linux platform and can work
either in EnKF or ensemble optimal interpolation (EnOI) mode."
"Lifting operators play an important role in starting a kinetic Boltzmann
model from given macroscopic information. The macroscopic variables need to be
mapped to the distribution functions, mesoscopic variables of the Boltzmann
model. A well-known numerical method for the initialization of Boltzmann models
is the Constrained Runs algorithm. This algorithm is used in literature for the
initialization of lattice Boltzmann models, special discretizations of the
Boltzmann equation. It is based on the attraction of the dynamics toward the
slow manifold and uses lattice Boltzmann steps to converge to the desired
dynamics on the slow manifold. We focus on applying the Constrained Runs
algorithm to map density, average flow velocity, and temperature, the
macroscopic variables, to distribution functions. Furthermore, we do not
consider only lattice Boltzmann models. We want to perform the algorithm for
different discretizations of the Boltzmann equation and consider a standard
finite volume discretization."
"We extend the applicability of the numerical Chapman-Enskog expansion as a
lifting operator for lattice Boltzmann models to map density and momentum to
distribution functions. In earlier work [Vanderhoydonc et al. Multiscale Model.
Simul. 10(3): 766-791, 2012] such an expansion was constructed in the context
of lifting only the zeroth order velocity moment, namely the density. A lifting
operator is necessary to convert information from the macroscopic to the
mesoscopic scale. This operator is used for the initialization of lattice
Boltzmann models. Given only density and momentum, the goal is to initialize
the distribution functions of lattice Boltzmann models. For this
initialization, the numerical Chapman-Enskog expansion is used in this paper."
"A multi-physics formulation for Data Driven Prognosis (DDP) is developed.
Unlike traditional predictive strategies that require controlled off-line
measurements or training for determination of constitutive parameters to derive
the transitional statistics, the proposed DDP algorithm relies solely on in
situ measurements. It utilizes a deterministic mechanics framework, but the
stochastic nature of the solution arises naturally from the underlying
assumptions regarding the order of the conservation potential as well as the
number of dimensions involved. The proposed DDP scheme is capable of predicting
onset of instabilities. Since the need for off-line testing (or training) is
obviated, it can be easily implemented for systems where such a priori testing
is difficult or even impossible to conduct. The prognosis capability is
demonstrated here via a balloon burst experiment where the instability is
predicted utilizing only on-line visual observations. The DDP scheme never
failed to predict the incipient failure, and no false positives were issued.
The DDP algorithm is applicable to others types of datasets. Time horizons of
DDP predictions can be adjusted by using memory over different time windows.
Thus, a big dataset can be parsed in time to make a range of predictions over
varying time horizons."
"Laminated glass units exhibit complex response as a result of different
mechanical behavior and properties of glass and polymer foil. We aim to develop
a finite element model for elastic laminated glass plates based on the refined
plate theory by Mau. For a geometrically nonlinear description of the behavior
of units, each layer behaves according to the Reissner-Mindlin kinematics,
complemented with membrane effects and the von K\'{a}rm\'{a}n assumptions.
Nodal Lagrange multipliers enforce the compatibility of independent layers in
this approach. We have derived the discretized model by the energy-minimization
arguments, assuming that the unknown fields are approximated by bi-linear
functions at the element level, and solved the resulting system by the Newton
method with consistent linearization. We have demonstrated through verification
and validation examples that the proposed formulation is reliable and
accurately reproduces the behavior of laminated glass units. This study
represents a first step to the development of a comprehensive, mechanics-based
model for laminated glass systems that is suitable for implementation in common
engineering finite element solvers."
"Individuals who suffer anterior cruciate ligament (ACL) injury are at higher
risk of developing knee osteoarthritis (OA) and almost 50% display symptoms 10
to 20 years post injury. Anterior cruciate ligament reconstruction (ACLR) often
does not protect against knee OA development. Accordingly, a multiscale
formulation for Data Driven Prognosis (DDP) of post ACLR is developed. Unlike
traditional predictive strategies that require controlled off-line measurements
or training for determination of constitutive parameters to derive the
transitional statistics, the proposed DDP algorithm relies solely on in situ
measurements. The proposed DDP scheme is capable of predicting onset of
instabilities. Since the need for off line testing (or training) is obviated,
it can be easily implemented for ACLR, where such controlled a priori testing
is almost impossible to conduct. The DDP algorithm facilitates hierarchical
handling of the large data set, and can assess the state of recovery in post
ACLR conditions based on data collected from stair ascent and descent exercises
of subjects. The DDP algorithm identifies inefficient knee varus motion and
knee rotation as primary difficulties experienced by some of the post ACLR
population. In such cases, levels of energy dissipation rate at the knee, and
its fluctuation may be used as measures for assessing progress after ACL
reconstruction."
"The cubic spline interpolation method, the Runge--Kutta method, and the
Newton-Raphson method are extended to dual versions (developed in the context
of dual numbers). This extension allows the calculation of the derivatives of
complicated compositions of functions which are not necessarily defined by a
closed form expression. The code for the algorithms has been written in Fortran
and some examples are presented. Among them, we use the dual Newton--Raphson
method to obtain the derivatives of the output angle in the RRRCR spatial
mechanism; we use the dual normal cubic spline interpolation algorithm to
obtain the thermal diffusivity using photothermal techniques; and we use the
dual Runge--Kutta method to obtain the derivatives of functions depending on
the solution of the Duffing equation."
"In this paper, we present an effectively numerical approach based on
isogeometric analysis (IGA) and higher-order shear deformation theory (HSDT)
for geometrically nonlinear analysis of laminated composite plates. The HSDT
allows us to approximate displacement field that ensures by itself the
realistic shear strain energy part without shear correction factors. IGA
utilizing basis functions namely B-splines or non-uniform rational B-splines
(NURBS) enables to satisfy easily the stringent continuity requirement of the
HSDT model without any additional variables. The nonlinearity of the plates is
formed in the total Lagrange approach based on the von-Karman strain
assumptions. Numerous numerical validations for the isotropic, orthotropic,
cross-ply and angle-ply laminated plates are provided to demonstrate the
effectiveness of the proposed method."
"This paper applies topology optimisation to the design of structures with
periodic microstructural details without length scale separation, i.e.
considering the complete macroscopic structure and its response, while
resolving all microstructural details, as compared to the often used
homogenisation approach. The approach takes boundary conditions into account
and ensures connected and macroscopically optimised microstructures regardless
of the difference in micro- and macroscopic length scales. This results in
microstructures tailored for specific applications rather than specific
properties.
  Dealing with the complete macroscopic structure and its response is
computationally challenging as very fine discretisations are needed in order to
resolve all microstructural details. Therefore, this article shows the benefits
of applying a contrast-independent spectral preconditioner based on the
multiscale finite element method (MsFEM) to large structures with
fully-resolved microstructural details.
  The density-based topology optimisation approach combined with a Heaviside
projection filter and a stochastic robust formulation is used on various
problems, with both periodic and layered microstructures. The presented
approach is shown to allow for the topology optimisation of very large problems
in \textsc{Matlab}, specifically a problem with 26 million displacement degrees
of freedom in 26 hours using a single computational thread."
"Most biological flyers undergo orderly deformation in flight, and the
deformations of wings lead to complex fluid-structure interactions. In this
paper, an aerodynamic-structural coupling method of flapping wing is developed
based on ANSYS to simulate the flapping of flexible wing. Fluent module and
Transient Structural module are connected through the System Coupling module to
make a two-way fluid-structure Coupling computational framework. Comparing with
the rigid wing of a cicada, the coupling results of the flexible wing shows
that the flexible deformation can increase the aerodynamic performances of
flapping flight."
"A new topology optimization method called the Proportional Topology
Optimization (PTO) is presented. As a non-gradient method, PTO is simple to
understand, easy to implement, and is also efficient and accurate at the same
time. It is implemented into two MATLAB programs to solve the stress
constrained and minimum compliance problems. Descriptions of the algorithm and
computer programs are provided in detail. The method is applied to solve three
numerical examples for both types of problems. The method shows comparable
efficiency and accuracy with an existing gradient optimality criteria method.
Also, the PTO stress constrained algorithm and minimum compliance algorithm are
compared by feeding output from one algorithm to the other in an alternative
manner, where the former yields lower maximum stress and volume fraction but
higher compliance compared to the latter. Advantages and disadvantages of the
proposed method and future works are discussed. The computer programs are
self-contained and publicly shared in the website www.ptomethod.org."
"When fluid flow in a pipeline is suddenly halted, a pressure surge or wave is
created within the pipeline. This phenomenon, called water hammer, can cause
major damage to pipelines, including pipeline ruptures. In this paper, we model
the problem of mitigating water hammer during valve closure by an optimal
boundary control problem involving a nonlinear hyperbolic PDE system that
describes the fluid flow along the pipeline. The control variable in this
system represents the valve boundary actuation implemented at the pipeline
terminus. To solve the boundary control problem, we first use {the method of
lines} to obtain a finite-dimensional ODE model based on the original PDE
system. Then, for the boundary control design, we apply the control
parameterization method to obtain an approximate optimal parameter selection
problem that can be solved using nonlinear optimization techniques such as
Sequential Quadratic Programming (SQP). We conclude the paper with simulation
results demonstrating the capability of optimal boundary control to
significantly reduce flow fluctuation."
"Deformation modeling of cardiac muscle is an important issue in the field of
cardiac analysis. Many approaches have been developed to better estimate the
cardiac muscle deformation, and to obtain a practical model to be used in
diagnostic procedures. But there are some conditions, like in case of
myocardial infarction, in which the regular modeling approaches are not useful.
In this article, using a point-wise approach, we try to estimate the
deformation under some abnormal conditions of cardiac muscle. First, the
endocardial and epicardial contour points are ordered with respect to the
center of gravity of the endocardial contour and displacement vectors of
boundary points are extracted. Then to solve the governing equations of the
deformation, which is an elliptic equation, we apply boundary conditions in
accordance with the computed displacement vectors and then the Finite Element
method (FEM) will be used to solve the governing equations. Using the obtained
displacement field of the cardiac muscle, strain map is extracted to show the
mechanical behavior of the cardiac muscle. Several tests are conducted using
phantom and real cardiac data in order to show the validity of the proposed
method."
"STEP-NC (ISO 14649) is becoming a promising standard to replace or supplement
the conventional G-code programs based on ISO 6983 due to its feature based
machine independent characteristics and its centric role to enable efficient
CAD/CAM/CNC interoperability. The re-use of G-code programs is important for
both manufacturing and capitalization of machining knowledge, nevertheless the
conversion is a tedious task when carried out manually and machining knowledge
is almost hidden in the low level G-code. Mapping G-code into STEP-NC should
benefit from more expressiveness of the manufacturing feature-based
characteristics of this new standard. The work presented here proposes an
overall method for G-code to STEP-NC conversion. First, G-code is converted
into canonical machining functions, this can make the method more applicable
and make subsequent processes easier to implement; then these functions are
parsed to generate the neutral format of STEP-NC Part21 toolpath file, this
turns G-code into object instances, and can facilitate company's usage of
legacy programs; and finally, also optionally, machining features are extracted
to generate Part21 CC2 (conformance class) file. The proposed extraction method
employs geometric information of cutting area inferred from toolpaths and
machining strategies, in addition to cutting tools' data and workpiece's
dimension data. This comprehensive use of available data makes the extraction
more accurate and reliable. The conversion method is holistic, and can be
extended to process a wide range of G-code programs (e.g. turning or mill-turn
codes) with as few user interventions as possible."
"The paper describes an electronic multiprocessor system that assures
functionality of a miniature UAV capable of 3D flying. The apparatus consists
of six independently controlled brushless DC motors, each having a propeller
attached to it. Since the brushless motor requires complex algorithms in order
to achieve maximum torque, efficiency and response time a DSP must be used. All
the motors are then controlled by a main microprocessor which is capable of
reading sensors (Inertial Measurement Unit (IMU)-orientation and GPS),
receiving input commands (remote controller or trajectory plan) and sending
independent commands to each of the six motors. The apparatus contains a total
of eight microcontrollers: the main unit, the IMU mathematical processor and
one microcontroller for each of the six brushless DC motors. Applications for
such an apparatus could include not only military, but also search-and-rescue,
geodetics, aerial photography and aerial assistance."
"We use a collection of Python programs for numerical simulation of liver
perfusion. We have an application for semi-automatic generation of a finite
element mesh of the human liver from computed tomography scans and for
reconstruction of the liver vascular structure. When the real vascular trees
can not be obtained from the CT data we generate artificial trees using the
constructive optimization method. The generated FE mesh and vascular trees are
imported into SfePy (Simple Finite Elements in Python) and numerical
simulations are performed in order to get the pressure distribution and
perfusion flows in the liver tissue. In the post-processing steps we calculate
transport of a contrast fluid through the liver parenchyma."
"In the mechanisms area, minimization of the magnitude of the acceleration of
the center of mass (ACoM) implies shaking force balancing. For a mechanism
operating in cycles, the case when the ACoM is zero implies that the
gravitational potential energy (GPE) is constant. This article shows an
efficient and effective optimum synthesis method for minimum acceleration of
the center of mass of a spherical 4R mechanism by using dual functions and the
counterweights balancing method. Once the dual function for ACoM has been
written, one can minimize the shaking forces from a kinematic point of view. We
present the synthesis of a spherical 4R mechanism for the case of a path
generation task. The synthesis process involves the optimization of two
objective functions, this multiobjective problem is solved by using the
weighted sum method implemented in the evolutionary algorithm known as
Differential Evolution."
"The problem of inferring proteins from complex peptide cocktails (digestion
products of biological samples) in shotgun proteomic workflow sets extreme
demands on computational resources in respect of the required very high
processing throughputs, rapid processing rates and reliability of results. This
is exacerbated by the fact that, in general, a given protein cannot be defined
by a fixed sequence of amino acids due to the existence of splice variants and
isoforms of that protein. Therefore, the problem of protein inference could be
considered as one of identifying sequences of amino acids with some limited
tolerance. In the current paper a model-based hardware acceleration of a
structured and practical inference approach is developed and validated on a
mass spectrometry experiment of realistic size. We have achieved 10 times
maximum speed-up in the co-designed workflow compared to a similar
software-only workflow run on the processor used for co-design."
"We study the design of pricing schemes for a group of consumers with smart
meters (e.g., in a Greenfield area) who are connected through a gateway to a
traditional electricity greed with a progressive tariff. Because the
progressive tariff cannot take into account the time aspect of electricity
demands, we apply it to consumers in both an individual and a group basis over
a shorter time period, which can flatten the overall demand over time and
thereby reduce peak load. This scenario for the coexistence of traditional and
smart girds and the pricing schemes under this scenario can enable smooth
migration to a future smart grid."
"With the reference of the several documents in the field of soil structure
interaction a document of present and past literature has been made with the
including a main focus on interaction of pile supported frames. This study
focuses on the complexity and excessive simplification of the model for
foundation system and structures, and should be carried forward for its
significance. The review is carried out including analytical, experimental and
numerical approaches considered in the past study. The perusal of literature
reveals that very few studies investigated on asymmetrical buildings supported
on pile foundations. In this paper, an attempt is made to understand research
carried out in pile soil structure interaction and research gap along with the
scope of research has been identified to carry out the present research work."
"Numerical methods are extremely useful in gaining insights into the behaviour
of reinforced soil retaining walls. However, traditional numerical approaches
such as limit equilibrium or finite element methods are unable to simulate
large deformation and post-failure behaviour of soils and retaining wall blocks
in the reinforced soil retaining walls system. To overcome this limitation, a
novel numerical approach is developed aiming to predict accurately the large
deformation and post-failure behaviour of soil and segmental wall blocks.
Herein, soil is modelled using an elasto-plastic constitutive model, while
segmental wall blocks are assumed rigid with full degrees of freedom. A soft
contact model is proposed to simulate the interaction between soil-block and
block-block. A two dimensional experiment of reinforced soil retaining walls
collapse was conducted to verify the numerical results. It is shown that the
proposed method can simulate satisfactory post-failure behaviour of segmental
wall blocks in reinforced soil retaining wall systems. The comparison showed
that the proposed method can provide satisfactory agreement with experiments."
"In recent advances in solving the problem of transmission network expansion
planning, the use of robust optimization techniques has been put forward, as an
alternative to stochastic mathematical programming methods, to make the problem
tractable in realistic systems. Different sources of uncertainty have been
considered, mainly related to the capacity and availability of generation
facilities and demand, and making use of adaptive robust optimization models.
The mathematical formulations for these models give rise to three-level
mixed-integer optimization problems, which are solved using different
strategies. Although it is true that these robust methods are more efficient
than their stochastic counterparts, it is also correct that solution times for
mixed-integer linear programming problems increase exponentially with respect
to the size of the problem. Because of this, practitioners and system operators
need to use computationally efficient methods when solving this type of
problem. In this paper the issue of improving computational performance by
taking different features from existing algorithms is addressed. In particular,
we replace the lower-level problem with a dual one, and solve the resulting
bi-level problem using a primal cutting plane algorithm within a decomposition
scheme. By using this alternative and simple approach, the computing time for
solving transmission expansion planning problems has been reduced drastically.
Numerical results in an illustrative example, the IEEE-24 and IEEE 118-bus test
systems demonstrate that the algorithm is superior in terms of computational
performance with respect to existing methods."
"Rainfall induced landslides and soil erosion are part of a complex system of
multiple interacting processes, and both are capable of significantly affecting
sediment budgets. These sediment mass movements also have the potential to
significantly impact on a broad network of ecosystems health, functionality and
the services they provide. To support the integrated assessment of these
processes it is necessary to develop reliable modelling architectures. This
paper proposes a semi-quantitative integrated methodology for a robust
assessment of soil erosion rates in data poor regions affected by landslide
activity. It combines heuristic, empirical and probabilistic approaches. This
proposed methodology is based on the geospatial semantic array programming
paradigm and has been implemented on a catchment scale methodology using
Geographic Information Systems (GIS) spatial analysis tools and GNU Octave. The
integrated data-transformation model relies on a modular architecture, where
the information flow among modules is constrained by semantic checks. In order
to improve computational reproducibility, the geospatial data transformations
implemented in ESRI ArcGis are made available in the free software GRASS GIS.
The proposed modelling architecture is flexible enough for future
transdisciplinary scenario analysis to be more easily designed. In particular,
the architecture might contribute as a novel component to simplify future
integrated analyses of the potential impact of wildfires or vegetation types
and distributions, on sediment transport from water induced landslides and
erosion."
"This paper shows the complementary roles of mathematical and engineering
points of view when dealing with truss analysis problems involving systems of
linear equations and inequalities. After the compatibility condition and the
mathematical structure of the general solution of a system of linear equations
is discussed, the truss analysis problem is used to illustrate its mathematical
and engineering multiple aspects, including an analysis of the compatibility
conditions and a physical interpretation of the general solution, and the
generators of the resulting affine space. Next, the compatibility and the
mathematical structure of the general solution of linear systems of
inequalities are analyzed and the truss analysis problem revisited adding some
inequality constraints, and discussing how they affect the resulting general
solution and many other aspects of it. Finally, some conclusions are drawn."
"A finite-difference Micromagnetic simulation code written in MATLAB is
presented with Graphics Processing Unit (GPU) acceleration. The high
performance of Graphics Processing Unit (GPU) is demonstrated compared to a
typical Central Processing Unit (CPU) based code. The speed-up of GPU to CPU is
shown to be greater than 30 for problems with larger sizes on a mid-end GPU in
single precision. The code is less than 200 lines and suitable for new
algorithm developing."
"We present an application of SPH to saturated soilproblems. Herein, the
standard SPH formulation was improved to model saturated soil. It is shown that
the proposed formulation could yield several advantages such as: it takes into
account the pore-water pressure in an accurate manner, it automatically
satisfies the dynamics boundary conditions between submerged soil and water,
and it reduced the computational cost. Discussions on the use of the standard
and the new SPH formulations are also given through some numerical tests.
Furthermore, some techniques to obtained correct SPH solution are also proposed
and discussed. To the end, this paper suggests that the proposed SPH
formulation should be considered as the basic formulation for further
developments of SPH for soil-water couple problems"
"A new embedded model for curved thin panels is developed in the Transmission
Line Modeling (TLM) method. In this model, curved panels are first linearized
and then embedded between adjacent 2D TLM nodes allowing for arbitrary
positioning between adjacent node centers. The embedded model eliminates the
necessity for fine discretization thus reducing the run time and memory
requirements for the calculation. The accuracy and convergence of the model are
verified by comparing the resonant frequencies of an elliptical cylinder formed
using carbon fiber composite (CFC) materials with those of the equivalent metal
cylinder. Furthermore, the model is used to analyze the shielding performance
of CFC airfoil NACA2415."
"Scattering of neutron collision inside a reactor depends upon geometry of the
reactor, diffusion coefficient and absorption coefficient etc. In general these
parameters are not crisp and hence we may get uncertain neutron diffusion
equation. In this paper we have investigated the above problem for a bare
triangular homogeneous reactor. Here the uncertain governing differential
equation is modelled by a modified fuzzy finite element method using newly
proposed interval arithmetic. Obtained eigenvalues by the proposed method are
studied in detail. Further the eigenvalues are compared with the classical
finite element method in special cases and various uncertain results have been
discussed."
"Diabetes has affected over 246 million people worldwide with a majority of
them being women. According to the WHO report, by 2025 this number is expected
to rise to over 380 million. The disease has been named the fifth deadliest
disease in the United States with no imminent cure in sight. With the rise of
information technology and its continued advent into the medical and healthcare
sector, the cases of diabetes as well as their symptoms are well documented.
This paper aims at finding solutions to diagnose the disease by analyzing the
patterns found in the data through classification analysis by employing
Decision Tree and Na\""ive Bayes algorithms. The research hopes to propose a
quicker and more efficient technique of diagnosing the disease, leading to
timely treatment of the patients."
"Inferring causality using longitudinal observational databases is challenging
due to the passive way the data are collected. The majority of associations
found within longitudinal observational data are often non-causal and occur due
to confounding.
  The focus of this paper is to investigate incorporating information from
additional databases to complement the longitudinal observational database
analysis. We investigate the detection of prescription drug side effects as
this is an example of a causal relationship. In previous work a framework was
proposed for detecting side effects only using longitudinal data. In this paper
we combine a measure of association derived from mining a spontaneous reporting
system database to previously proposed analysis that extracts domain expertise
features for causal analysis of a UK general practice longitudinal database.
  The results show that there is a significant improvement to the performance
of detecting prescription drug side effects when the longitudinal observation
data analysis is complemented by incorporating additional drug safety sources
into the framework. The area under the receiver operating characteristic curve
(AUC) for correctly classifying a side effect when other data were considered
was 0.967, whereas without it the AUC was 0.923 However, the results of this
paper may be biased by the evaluation and future work should overcome this by
developing an unbiased reference set."
"The paper is devoted to the numerical solution of elastoplastic constitutive
initial value problems. An improved form of the implicit return-mapping scheme
for nonsmooth yield surfaces is proposed that systematically builds on a
subdifferential formulation of the flow rule. The main advantage of this
approach is that the treatment of singular points, such as apices or edges at
which the flow direction is multivalued involves only a uniquely defined set of
non-linear equations, similarly to smooth yield surfaces. This paper (PART I)
is focused on isotropic models containing: $a)$ yield surfaces with one or two
apices (singular points) laying on the hydrostatic axis; $b)$ plastic
pseudo-potentials that are independent of the Lode angle; $c)$ nonlinear
isotropic hardening (optionally). It is shown that for some models the improved
integration scheme also enables to a priori decide about a type of the return
and investigate existence, uniqueness and semismoothness of discretized
constitutive operators in implicit form. Further, the semismooth Newton method
is introduced to solve incremental boundary-value problems. The paper also
contains numerical examples related to slope stability with available Matlab
implementation."
"An accurate knowledge of the per-unit length impedance of power cables is
necessary to correctly predict electromagnetic transients in power systems. In
particular, skin, proximity, and ground return effects must be properly
estimated. In many applications, the medium that surrounds the cable is not
uniform and can consist of multiple layers of different conductivity, such as
dry and wet soil, water, or air. We introduce a multilayer ground model for the
recently-proposed MoM-SO method, suitable to accurately predict ground return
effects in such scenarios. The proposed technique precisely accounts for skin,
proximity, ground and tunnel effects, and is applicable to a variety of cable
configurations, including underground and submarine cables. Numerical results
show that the proposed method is more accurate than analytic formulas typically
employed for transient analyses, and delivers an accuracy comparable to the
finite element method (FEM). With respect to FEM, however, MoM-SO is over 1000
times faster, and can calculate the impedance of a submarine cable inside a
three-layer medium in 0.10~s per frequency point."
"This paper describes the application of the adaptive whitening filter and the
wavelet transform used to detect the abrupt changes in the signals recorded
during disturbances in the electrical power network in South Africa. Main focus
has been to estimate exactly the time-instants of the changes in the signal
model parameters during the pre-fault condition and following events like
initiation of fault, circuit-breaker opening, auto-reclosure of the
circuit-breakers. The key idea is to decompose the fault signals, de-noised
using the adaptive whitening filter, into effective detailed and smoothed
version using the multiresolution signal decomposition technique based on
discrete wavelet transform. Then we apply the threshold method on the
decomposed signals to estimate the change time-instants, segmenting the fault
signals into the event-specific sections for further signal processing and
analysis. This paper presents application on the recorded signals in the power
transmission network of South Africa."
"In the dynamic analysis of structural engineering systems, it is common
practice to introduce damping models to reproduce experimentally observed
features. These models, for instance Rayleigh damping, account for the damping
sources in the system altogether and often lack physical basis. We report on an
alternative path for reproducing damping coming from material nonlinear
response through the consideration of the heterogeneous character of material
mechanical properties. The parameterization of that heterogeneity is performed
through a stochastic model. It is shown that such a variability creates the
patterns in the concrete cyclic response that are classically regarded as
source of damping."
"This paper deals with uncertain parabolic fluid flow problem where the
uncertainty occurs due to the initial conditions and parameters involved in the
system. Uncertain values are considered as fuzzy and these are handled through
a recently developed method. Here the concepts of fuzzy numbers are combined
with Finite Difference Method (FDM) and then Fuzzy Finite Difference Method
(FFDM) has been proposed. The proposed FFDM has been used to solve the fluid
flow problem bounded by two parallel plates. Finally sensitivity of the fuzzy
parameters has also been analysed."
"Reproducibility of computationally-derived scientific discoveries should be a
certainty. As the product of several person-years' worth of effort, results --
whether disseminated through academic journals, conferences or exploited
through commercial ventures -- should at some level be expected to be
repeatable by other researchers. While this stance may appear to be obvious and
trivial, a variety of factors often stand in the way of making it commonplace.
Whilst there has been detailed cross-disciplinary discussions of the various
social, cultural and ideological drivers and (potential) solutions, one factor
which has had less focus is the concept of reproducibility as a technical
challenge. Specifically, that the definition of an unambiguous and measurable
standard of reproducibility would offer a significant benefit to the wider
computational science community.
  In this paper, we propose a high-level technical specification for a service
for reproducibility, presenting cyberinfrastructure and associated workflow for
a service which would enable such a specification to be verified and validated.
In addition to addressing a pressing need for the scientific community, we
further speculate on the potential contribution to the wider software
development community of services which automate de novo compilation and
testing of code from source. We illustrate our proposed specification and
workflow by using the BioModelAnalyzer tool as a running example."
"Large scale circuit simulation, such as power delivery network analysis, has
become increasingly challenge in the VLSI design verification flow. Power
delivery network can be simulated by both SPICE-type circuit-based model and
eletromagnetics-based model when full-wave accuracy is desired. In the early
time of the time domain finite element simulation for integrated circuit, the
modes having the highest eigenvalues supported by the numerical system will be
excited. Because of the band limited source, after the early time, the modes
having a resonance frequency well beyond the input frequency band will die
down, and all physically important high-order modes and DC mode will show up
and become dominant. Among these modes, the DC mode is the last one to show up.
Although the convergence criterion is not applied on the DC mode, the existence
of DC mode in the field solution will deteriorate the convergence rate of the
first several high order modes. Therefore, this paper first analyzed the
mathematic characteristics of the DC mode and proposed a rigorous and fast
solution to extract the DC mode from the numerical system in order to speed up
the convergence rate. Experimental results demonstrated the robustness and
superior performance of this method."
"In order to project electromagnetic fields between different meshes with
respect to the conservation of energetic values, Galerkin projection
formulations based on the energetic norm are developed in this communication.
The proposed formulations are applied to an academic example."
"Bio-medical ontologies can contain a large number of concepts. Often many of
these concepts are very similar to each other, and similar or identical to
concepts found in other bio-medical databases. This presents both a challenge
and opportunity: maintaining many similar concepts is tedious and fastidious
work, which could be substantially reduced if the data could be derived from
pre-existing knowledge sources. In this paper, we describe how we have achieved
this for an ontology of the mitochondria using our novel ontology development
environment, the Tawny-OWL library."
"In the last few decades or so, we witness a paradigm shift in our nature
studies - from a data-processing based computational approach to an
information-processing based cognitive approach. The process is restricted and
often misguided by the lack of a clear understanding about what information is
and how it should be treated in research applications (in general) and in
biological studies (in particular). The paper intend to provide some remedies
for this bizarre situation."
"There is multiple databases contain datasets of TP53 gene and its tumor
protein P53 which believed to be involved in over 50% of human cancers cases,
these databases are rich as datasets covered all mutations caused diseases
(cancers), but they haven't efficient mining method can classify and diagnosis
mutations patient's then predict the cancer of that patient. This paper
proposed a novel mining of cancer via mutations because there is no mining
method before offers friendly, effective and flexible predict or diagnosis of
cancers via using whole common database of TP53 gene (tumor protein P53) as
dataset and selecting a minimum number of fields in training and testing quick
propagation algorithm which supporting this miming method. Simulating quick
propagation network for the train dataset shows results the Correlation
(0.9999), R-squared (0.9998) and mean of Absolute Relative Error (0.0029),
while the training for the ALL datasets (train, test and validation dataset)
have results the Correlation (0.9993), R-squared (0.9987) and mean of Absolute
Relative Error (0.0057)."
"The fraction nonconforming is a key quality measure used in statistical
quality control design in clinical laboratory medicine. The confidence bounds
of normal populations of measurements for the fraction nonconforming each of
the lower and upper quality specification limits when both the random and the
systematic error are unknown can be calculated using the noncentral
t-distribution, as it is described in detail and illustrated with examples."
"GROMACS is a widely used package for biomolecular simulation, and over the
last two decades it has evolved from small-scale efficiency to advanced
heterogeneous acceleration and multi-level parallelism targeting some of the
largest supercomputers in the world. Here, we describe some of the ways we have
been able to realize this through the use of parallelization on all levels,
combined with a constant focus on absolute performance. Release 4.6 of GROMACS
uses SIMD acceleration on a wide range of architectures, GPU offloading
acceleration, and both OpenMP and MPI parallelism within and between nodes,
respectively. The recent work on acceleration made it necessary to revisit the
fundamental algorithms of molecular simulation, including the concept of
neighborsearching, and we discuss the present and future challenges we see for
exascale simulation - in particular a very fine-grained task parallelism. We
also discuss the software management, code peer review and continuous
integration testing required for a project of this complexity."
"This paper deals also with a problem of gas absorption accompanied by an
instantaneous, irreversible reaction in the liquid layer. The well-known
methods for calculating such processes are based usually on the certain
amendments to solutions, which are obtained disregarding the chemical reaction.
Unlike the known work (1. D. Baetens, R. Van Keer, L.H. Hosten. Gas-liquid
reaction: absorption accompanied by an instantaneous, irreversible reaction//
Moving Boundaries IV, Southampton, Boston, 1997) the approach we used takes
into account the influence of reaction resulting product on the arising and
velocity of a moving reaction plane. The known results in the theory of
chemical apparatuses scaling are devoted to apparatuses with non-regular
packings mainly. However how the phases distribution over the regular packings
of chemical columns effects the heat and mass efficiency is studied lesser.
This paper deals with the methods of simulation the scaling effects applying to
chemical towers with regular packings of various types. The models for
describing the influence of initial liquid and gas distribution in chemical
columns with regular packing on the mass transfer efficiency have been
submitted. The sufficiently simple methods for evaluating the influence of
large-scale factor on the efficiency of mass transfer have been obtained. These
methods are suitable for use in engineering calculation techniques."
"Hydrodynamic modelling is an important tool for the development of tidal
stream energy projects. Many hydrodynamic models incorporate the effect of
tidal turbines through an enhanced bottom drag. In this paper we show that
although for coarse grid resolutions (kilometre scale) the resulting force
exerted on the flow agrees well with the theoretical value, the force starts
decreasing with decreasing grid sizes when these become smaller than the length
scale of the wake recovery. This is because the assumption that the upstream
velocity can be approximated by the local model velocity, is no longer valid.
Using linear momentum actuator disc theory however, we derive a relationship
between these two velocities and formulate a correction to the enhanced bottom
drag formulation that consistently applies a force that remains closed to the
theoretical value, for all grid sizes down to the turbine scale. In addition, a
better understanding of the relation between the model, upstream, and actual
turbine velocity, as predicted by actuator disc theory, leads to an improved
estimate of the usefully extractable energy. We show how the corrections can be
applied (demonstrated here for the models MIKE 21 and Fluidity) by a simple
modification of the drag coefficient."
"Due to the development of intelligent demand-side management with automatic
control, distributed populations of large residential loads, such as air
conditioners (ACs) and electrical water heaters (EWHs), have the opportunities
to provide effective demand-side ancillary services for load serving entities
(LSEs) to reduce the emissions and network operating costs. Most present
approaches are restricted to 1) the scenarios involving with efficiently
scheduling the large number of appliances in real time, 2) the issues about
evaluating the contributions of individual residents towards participating
demand response (DR) program, and fairly distributing the rewards, and 3) the
concerns on performing cost-effective demand reduction request (DRR) for LSEs
with minimal rewards costs while not affecting their living comfortableness.
Therefore, this paper presents an optimal framework for residential load
aggregators (RLAs) which helps solve the problems mentioned above. Under this
framework, RLAs are able to realize the DRR for LSEs to generate optimal
control strategies over residential appliances quickly and efficiently. To
residents, the framework is designed with probabilistic model of
comfortableness, which minimizes the impact of DR program to their daily life.
To LSEs, the framework helps minimize the total reward costs of performing
DRRs. Moreover, the framework fairly and strategically distributes the
financial rewards to residents, which may stimulate the potential capability of
loads optimized and controlled by RLAs in demand side management. The proposed
framework has been validated on several numerical case studies."
"Expansion of natural gas networks is a critical process involving substantial
capital expenditures with complex decision-support requirements. Given the
non-convex nature of gas transmission constraints, global optimality and
infeasibility guarantees can only be offered by global optimisation approaches.
Unfortunately, state-of-the-art global optimisation solvers are unable to scale
up to real-world size instances. In this study, we present a convex
mixed-integer second-order cone relaxation for the gas expansion planning
problem under steady-state conditions. The underlying model offers tight lower
bounds with high computational efficiency. In addition, the optimal solution of
the relaxation can often be used to derive high-quality solutions to the
original problem, leading to provably tight optimality gaps and, in some cases,
global optimal soluutions. The convex relaxation is based on a few key ideas,
including the introduction of flux direction variables, exact McCormick
relaxations, on/off constraints, and integer cuts. Numerical experiments are
conducted on the traditional Belgian gas network, as well as other real larger
networks. The results demonstrate both the accuracy and computational speed of
the relaxation and its ability to produce high-quality solutions."
"We present the result of our microsimulation study on the effects of six
traffic schemes $T=\{t_0, t_1, \dots, t_5\}$ on the mean total delay time
($\Delta$) and mean speed ($\Sigma$) of vehicles at the non-signalized Bicutan
Roundabout (BR) in Upper Bicutan, Taguig City, Metro Manila, with $t_0$ as the
current traffic scheme being enforced and $t_{i>0}$ as the proposed ones. We
present first that our simulation approach, a hybridized multi-agent system
(MAS) with the car-following and lane-changing models (CLM), can mimic the
current observed traffic scenario $C$ at a statistical significance of
$\alpha=0.05$. That is, the respective absolute differences of the $\Delta$ and
$\Sigma$ between $C$ and $t_0$ are not statistically different from zero. Next,
using our MAS-CLM, we simulated all proposed $t_{i>0}$ and compared their
respective $\Delta$ and $\Sigma$. We found out using DMRT that the best traffic
scheme is $t_3$ (i.e., when we converted the bi-directional 4-lane PNR-PNCC
road into a bi-directional 1-lane PNR-to-PNCC and 3-lane PNCC-to-PNR routes
during rush hours). Then, we experimented on converting BR into a signalized
junction and re-implemented all $t_3$ with controlled stops of $S=\{15s,
45s\}$. We found out that $t_3$ with a 15-s stop has the best performance.
Finally, we simulated the effect of increased in vehicular volume $V$ due to
traffic improvement and we found out that $t_3$ with 15-s stop still
outperforms the others for all increased in $V=\{10\%, 50\%, 100\%\}$."
"Using microsimulations of vehicular dynamics, we studied the effects of
several proposed infrastructure developments to the mean travel delay
time~$\Delta$ and mean speed~$\Sigma$ of vehicles passing a busy three-road
fork, particularly in the non-signalized roundabout junction of Lower Bicutan,
Taguig City, Metro Manila. We designed and implemented multi-agent-based
microsimulation models to mimic the autonomous driving behavior of
heterogeneous individuals and measured the effect of various proposed
infrastructure developments on~$\Delta$ and~$\Sigma$. Our aim is to find out
the best infrastructure development from among three choices being considered
by the local government for the purpose of solving the traffic problems in the
area. We created simulation models of the current vehicular traffic situation
in the area using the mean travel times~$\tau$ of statistically sampled
vehicles to show that our model can simulate the real-world at a significance
level of $\alpha=0.05$. Based on these models, we then simulated the effect of
the proposed infrastructure developments on~$\Delta$ and~$\Sigma$ and used
these metrics as our basis of comparison. We found out that the proposed
widening of one fork from two lanes to three lanes has the most improved
metrics at the same $\alpha=0.05$ compared to the metrics we observed in the
current situation. Under this infrastructure development, the~$\Delta$
increases linearly ($R^2=0.98$) at the rate of 1.03~$s$, while the~$\Sigma$
decreases linearly ($R^2>0.99$) at the rate of 0.14~$km/h$ per percent increase
in the total vehicle volume~$\mathcal{V}$."
"To the knowledge of the author, this is the first time it has been shown that
interest rates that are extremely high by modern standards are necessary within
a zero-sum monetary system. Extreme interest rates persisted for long periods
of time in many places. Prior to the invention of banking, most money was
hard-money in the form of some type of coin. Here a model is presented that
examines the interest rate required to succeed as an investor in a zero-sum
hard-money system. Even when the playing field is significantly tilted toward
the investor, interest rates need to be much higher than expected. In a
completely fair zero-sum system, an investor cannot break even without charging
100% interest. Even with a 5% advantage, an investor will not break even at 15%
interest. From this it is concluded that what we consider usurious rates today
are, within a hard-money system, driven by necessity.
  Cryptocurrency is a novel form of hard-currency. The inability to virtualize
the money creates a system close to zero-sum. Therefore, within the bounds of a
cryptocurrency system that limits money creation, interest rates must rise to
levels that the modern world considers usury. It is impossible, therefore, that
a cryptocurrency that is not expandable could take over a modern economy and
replace modern fiat currency."
"This article deals with the fusion of flaw detections from multi-sensor
nondestructive materials testing. Because each testing method makes use of
different physical effects for defect localization, a multi-method approach is
promising to effectively distinguish the many false alarms from actual material
defects. To this end, we propose a new fusion technique for scattered two- or
three-dimensional location data. Using a density-based approach, the proposed
method is able to explicitly address the localization uncertainties such as
registration errors. We provide guidelines on how to set all key parameters and
demonstrate the technique's robustness. Finally, we apply our fusion approach
to experimental data and demonstrate its ability to find small defects by
substantially reducing false alarms under conditions where no single-sensor
method is adequate."
"Model order reduction (MOR) techniques play a crucial role in the
computer-aided design of modern integrated circuits, where they are used to
reduce the size of parasitic networks. Unfortunately, the efficient reduction
of passive networks with many ports is still an open problem. Existing
techniques do not scale well with the number of ports, and lead to dense
reduced models that burden subsequent simulations. In this paper, we propose
TurboMOR, a novel MOR technique for the efficient reduction of passive RC
networks. TurboMOR is based on moment-matching, achieved through efficient
congruence transformations based on Householder reflections. A novel feature of
TurboMOR is the block-diagonal structure of the reduced models, that makes them
more efficient than the dense models produced by existing techniques. Moreover,
the model structure allows for an insightful interpretation of the reduction
process in terms of system theory. Numerical results show that TurboMOR scales
more favourably than existing techniques in terms of reduction time, simulation
time and memory consumption."
"Pore-scale modeling and simulation of reactive flow in porous media has a
range of diverse applications, and poses a number of research challenges. It is
known that the morphology of a porous medium has significant influence on the
local flow rate, which can have a substantial impact on the rate of chemical
reactions. While there are a large number of papers and software tools
dedicated to simulating either fluid flow in 3D computerized tomography (CT)
images or reactive flow using pore-network models, little attention to date has
been focused on the pore-scale simulation of sorptive transport in 3D CT
images, which is the specific focus of this paper. Here we first present an
algorithm for the simulation of such reactive flows directly on images, which
is implemented in a sophisticated software package. We then use this software
to present numerical results in two resolved geometries, illustrating the
importance of pore-scale simulation and the flexibility of our software
package."
"In this work, we investigate the interaction of free and porous media flow by
large scale lattice Boltzmann simulations. We study the transport phenomena at
the porous interface on multiple scales, i.e., we consider both,
computationally generated pore-scale geometries and homogenized models at a
macroscopic scale. The pore-scale results are compared to those obtained by
using different transmission models. Two-domain approaches with sharp interface
conditions, e.g., of Beavers--Joseph--Saffman type, as well as a single-domain
approach with a porosity depending viscosity are taken into account. For the
pore-scale simulations, we use a highly scalable communication-reducing scheme
with a robust second order boundary handling. We comment on computational
aspects of the pore-scale simulation and on how to generate pore-scale
geometries. The two-domain approaches depend sensitively on the choice of the
exact position of the interface, whereas a well-designed single-domain approach
can significantly better recover the averaged pore-scale results."
"Computational experiments using spatial stochastic simulations have led to
important new biological insights, but they require specialized tools, a
complex software stack, as well as large and scalable compute and data analysis
resources due to the large computational cost associated with Monte Carlo
computational workflows. The complexity of setting up and managing a
large-scale distributed computation environment to support productive and
reproducible modeling can be prohibitive for practitioners in systems biology.
This results in a barrier to the adoption of spatial stochastic simulation
tools, effectively limiting the type of biological questions addressed by
quantitative modeling. In this paper, we present PyURDME, a new, user-friendly
spatial modeling and simulation package, and MOLNs, a cloud computing appliance
for distributed simulation of stochastic reaction-diffusion models. MOLNs is
based on IPython and provides an interactive programming platform for
development of sharable and reproducible distributed parallel computational
experiments."
"Computational molecular modeling and visualization has seen significant
progress in recent years with sev- eral molecular modeling and visualization
software systems in use today. Nevertheless the molecular biology community
lacks techniques and tools for the rigorous analysis, quantification and
visualization of the associated errors in molecular structure and its
associated properties. This paper attempts at filling this vacuum with the
introduction of a systematic statistical framework where each source of
structural uncertainty is modeled as a ran- dom variable (RV) with a known
distribution, and properties of the molecules are defined as dependent RVs. The
framework consists of a theoretical basis, and an empirical implementation
where the uncertainty quantification (UQ) analysis is achieved by using
Chernoff-like bounds. The framework enables additionally the propagation of
input structural data uncertainties, which in the molecular protein world are
described as B-factors, saved with almost all X-ray models deposited in the
Protein Data Bank (PDB). Our statistical framework is also able and has been
applied to quantify and visualize the uncertainties in molecular properties,
namely solvation interfaces and solvation free energy estimates. For each of
these quantities of interest (QOI) of the molecular models we provide several
novel and intuitive visualizations of the input, intermediate, and final
propagated uncertainties. These methods should enable the end user achieve a
more quantitative and visual evaluation of various molecular PDB models for
structural and property correctness, or the lack thereof."
"In this paper, a framework is developed for power transformer (Generator Step
up Unit) insulation life evaluation (PTILE) study on power system Network.
Parameters used for studies include real time sample data obtained from power
transformer field studies in the South-South Niger Delta region of Nigeria. It
is used for performing simulations over varying number of years. Simulation
reports shows a polynomial running time complexity and validates the stochastic
Hot Spot theory indicating that the transformers in such region should be
replaced sooner due to higher hot spots and transformer loading in such regions"
"The paper is devoted to a constitutive solution, limit load analysis and
Newton-like methods in elastoplastic problems containing the Mohr-Coulomb yield
criterion. Within the constitutive problem, we introduce a self-contained
derivation of the implicit return-mapping solution scheme using a recent
subdifferential-based treatment. Unlike conventional techniques based on
Koiter's rules, the presented scheme a priori detects a position of the unknown
stress tensor on the yield surface even if the constitutive solution cannot be
found in closed form. This fact eliminates blind guesswork from the scheme,
enables to analyze properties of the constitutive operator, and simplifies
construction of the consistent tangent operator which is important for the
semismooth Newton method applied on the incremental boundary value
elastoplastic problem. The incremental problem in Mohr-Coulomb plasticity is
combined with the limit load analysis. Beside a conventional direct method of
the incremental limit analysis, a recent indirect one is introduced and its
advantages are described. The paper contains 2D and 3D numerical experiments on
slope stability with publicly available Matlab implementations."
"The economic dispatch of wind power units is quite different from that in
conventional thermal units, since the adopted model should take into
consideration the intermittency nature of wind speed as well. Therefore, this
paper uses a model that takes into account the aforementioned consideration in
addition to whether the utility owns wind turbines or not. The economic
dispatch is solved by using one of the modern optimization algorithms: the
particle swarm optimization algorithm. A 6-bus system is used and it includes
wind-powered generators besides to thermal generators. The thorough analysis of
the results is also provided."
"The paper has been withdrawn effective November 18, 2015.
  Hydraulic Pump sumps are designed to provide a swirl free flow to the pump.
The degree of swirl is measured in physical model tests using a swirl meter and
a quantity known as swirl angle is generally measured. The present paper
presents a novel method to compute the bulk swirl angle using the local
velocity field obtained from computational fluid dynamics data. The basis for
the present method is the conservation of angular momentum conservation. By
carrying out both numerical and experimental studies the novel swirl angle
calculation method is validated. Further the effect of vortex suppression
devices in reducing the swirl angle is also demonstrated."
"Mining biological data is an emergent area at the intersection between
bioinformatics and data mining (DM). The intelligent agent based model is a
popular approach in constructing Distributed Data Mining (DDM) systems to
address scalable mining over large scale distributed data. The nature of
associations between different amino acids in proteins has also been a subject
of great anxiety. There is a strong need to develop new models and exploit and
analyze the available distributed biological data sources. In this study, we
have designed and implemented a multi-agent system (MAS) called Agent enriched
Quantitative Association Rules Mining for Amino Acids in distributed Protein
Data Banks (AeQARM-AAPDB). Such globally strong association rules enhance
understanding of protein composition and are desirable for synthesis of
artificial proteins. A real protein data bank is used to validate the system."
"This paper presents a semi-analytical estimate of the response of a
grandstand occupied by an active crowd and by a passive crowd. Filtered
Gaussian white noise processes are used to approximate the loading terms
representing an active crowd. Lumped biodynamic models with a single degree of
freedom are included to reflect passive spectators occupying the structure. The
response is described in terms of the first two moments, employing the It\^o
formula and the state augmentation method for the stationary time domain
solution. The quality of the approximation is compared on the basis of three
examples of varying complexity using Monte Carlo simulation based on a
synthetic generator available in the literature. For comparative purposes,
there is also a brief review of frequency domain estimates."
"We present mathematical models, computational algorithms and software, which
can be used for prediction of results of prosthetic treatment. More interest
issue is biomechanics of the periodontal complex because any prosthesis is
accompanied by a risk of overloading the supporting elements. Such risk can be
avoided by the proper load distribution and prediction of stresses that occur
during the use of dentures. We developed the mathematical model of the
periodontal complex and its software implementation. This model is based on
linear elasticity theory and allows to calculate the stress and strain fields
in periodontal ligament and jawbone. The input parameters for the developed
model can be divided into two groups. The first group of parameters describes
the mechanical properties of periodontal ligament, teeth and jawbone (for
example, elasticity of periodontal ligament etc.). The second group
characterized the geometric properties of objects: the size of the teeth, their
spatial coordinates, the size of periodontal ligament etc. The mechanical
properties are the same for almost all, but the input of geometrical data is
complicated because of their individual characteristics. In this connection, we
develop algorithms and software for processing of images obtained by computed
tomography (CT) scanner and for constructing individual digital model of the
tooth-periodontal ligament-jawbone system of the patient. Integration of models
and algorithms described allows to carry out biomechanical analysis on
three-dimensional digital model and to select prosthesis design."
"An accurate modeling of skin effect inside conductors is of capital
importance to solve transmission line and scattering problems. This paper
presents a surface-based formulation to model skin effect in conductors of
arbitrary cross section, and compute the per-unit-length impedance of a
multiconductor transmission line. The proposed formulation is based on the
Dirichlet-Neumann operator that relates the longitudinal electric field to the
tangential magnetic field on the boundary of a conductor. We demonstrate how
the surface operator can be obtained through the contour integral method for
conductors of arbitrary shape. The proposed algorithm is simple to implement,
efficient, and can handle arbitrary cross-sections, which is a main advantage
over the existing approach based on eigenfunctions, which is available only for
canonical conductor's shapes. The versatility of the method is illustrated
through a diverse set of examples, which includes transmission lines with
trapezoidal, curved, and V-shaped conductors. Numerical results demonstrate the
accuracy, versatility, and efficiency of the proposed technique."
"There is a common need to search of molecular databases for compounds
resembling some shape, what suggests having similar biological activity while
searching for new drugs. The large size of the databases requires fast methods
for such initial screening, for example based on feature vectors constructed to
fulfill the requirement that similar molecules should correspond to close
vectors. Ultrafast Shape Recognition (USR) is a popular approach of this type.
It uses vectors of 12 real number as 3 first moments of distances from 4
emphasized points. These coordinates might contain unnecessary correlations and
does not allow to reconstruct the approximated shape. In contrast, spherical
harmonic (SH) decomposition uses orthogonal coordinates, suggesting their
independence and so lager informational content of the feature vector. There is
usually considered rotationally invariant SH descriptors, what means discarding
of some essential information.
  This article discusses framework for descriptors with normalized rotation,
for example by using principal component analysis (PCA-SH). As one of the most
interesting are ligands which have to slide into a protein, we will introduce
descriptors optimized for such flat elongated shapes. Bent deformed cylinder
(BDC) describes the molecule as a cylinder which was first bent, then deformed
such that its cross-sections became ellipses of evolving shape. Legendre
polynomials are used to describe the central axis of such bent cylinder.
Additional polynomials are used to define evolution of such elliptic
cross-section along the main axis. There will be also discussed bent
cylindrical harmonics (BCH), which uses cross-sections described by cylindrical
harmonics instead of ellipses. All these normalized rotation descriptors allow
to reconstruct (decode) the approximated representation of the shape, hence can
be also used for lossy compression purposes."
"The reliability of processes with moving elastic and isotropic material
containing initial cracks is considered in terms of fracture. The material is
modelled as a moving plate which is simply supported from two of its sides and
subjected to homogeneous tension acting in the travelling direction. For
tension, two models are studied: i) tension is constant with respect to time,
and ii) tension varies temporally according to an Ornstein-Uhlenbeck process.
Cracks of random length are assumed to occur in the material according to a
stochastic counting process. For a general counting process, a representation
of the nonfracture probability of the system is obtained that exploits
conditional Monte Carlo simulation. Explicit formulae are derived for special
cases. To study the reliability of the system with temporally varying tension,
a known explicit result for the first passage time of an Ornstein-Uhlenbeck
process to a constant boundary is utilized. Numerical examples are provided for
printing presses and paper material."
"Recent breakthroughs in Transmission Network Expansion Planning (TNEP) have
demonstrated that the use of robust optimization, as opposed to stochastic
programming methods, renders the expansion planning problem considering
uncertainties computationally tractable for real systems. However, there is
still a yet unresolved and challenging problem as regards the resolution of the
dynamic TNEP problem (DTNEP), which considers the year-by-year representation
of uncertainties and investment decisions in an integrated way. This problem
has been considered to be a highly complex and computationally intractable
problem, and most research related to this topic focuses on very small case
studies or used heuristic methods and has lead most studies about TNEP in the
technical literature to take a wide spectrum of simplifying assumptions. In
this paper an adaptive robust transmission network expansion planning
formulation is proposed for keeping the full dynamic complexity of the problem.
The method overcomes the problem size limitations and computational
intractability associated with dynamic TNEP for realistic cases. Numerical
results from an illustrative example and the IEEE 118-bus system are presented
and discussed, demonstrating the benefits of this dynamic TNEP approach with
respect to classical methods."
"Network alignment has extensive applications in comparative interactomics.
Traditional approaches aim to simultaneously maximize the number of conserved
edges and the underlying similarity of aligned entities. We propose a novel
formulation of the network alignment problem that extends topological
similarity to higher-order structures and provides a new objective function
that maximizes the number of aligned substructures. This objective function
corresponds to an integer programming problem, which is NP-hard. Consequently,
we identify a closely related surrogate function whose maximization results in
a tensor eigenvector problem. Based on this formulation, we present an
algorithm called Triangular AlignMEnt (TAME), which attempts to maximize the
number of aligned triangles across networks. Using a case study on the
NAPAbench dataset, we show that triangular alignment is capable of producing
mappings with high node correctness. We further evaluate our method by aligning
yeast and human interactomes. Our results indicate that TAME outperforms the
state-of-art alignment methods in terms of conserved triangles. In addition, we
show that the number of conserved triangles is more significantly correlated,
compared to the conserved edge, with node correctness and co-expression of
edges. Our formulation and resulting algorithms can be easily extended to
arbitrary motifs."
"Variable and higher pulse repetition frequencies (PRFs) are increasingly
being used to meet the stricter requirements and complexities of current
airborne and spaceborne synthetic aperture radar (SAR) systems associated with
higher resolution and wider area products. POLYPHASE, the proposed resampling
scheme, downsamples and unifies variable PRFs within a single look complex
(SLC) SAR acquisition and across a repeat pass sequence of acquisitions down to
an effective lower PRF. A sparsity condition of the received SAR data ensures
that the uniformly resampled data approximates the spectral properties of a
decimated densely sampled version of the received SAR data. While experiments
conducted with both synthetically generated and real airborne SAR data show
that POLYPHASE retains comparable performance to the state-of-the-art BLUI
scheme in image quality, a polyphase filter-based implementation of POLYPHASE
offers significant computational savings for arbitrary (not necessarily
periodic) input PRF variations, thus allowing fully on-board, in-place, and
real-time implementation."
"In this paper, equilibrium and stability equations of functionally graded
material (FGM) plate under thermal environment are formulated based on
isogeometric analysis (IGA) in combination with higher-order shear deformation
theory (HSDT). The FGM plate is made by a mixture of two distinct components,
for which material properties not only vary continuously through thickness
according to a power-law distribution but also are assumed to be a function of
temperature. Temperature field is assumed to be constant in any plane and
uniform, linear and nonlinear through plate thickness, respectively. The
governing equation is in nonlinear form based on von Karman assumption and
thermal effect. A NURBS-based isogeometric finite element formulation is
utilized to naturally fulfil the rigorous C1-continuity required by the present
plate model. Influences of gradient indices, boundary conditions, temperature
distributions, material properties, length-to-thickness ratios on the behaviour
of FGM plate are discussed in details. Numerical results demonstrate excellent
performance of the present approach."
"Numerical simulation of anechoic chambers is a hot topic since it can provide
useful data about the performance of the EMC site. However, the mathematical
nature of the problem, the physical dimensions of the simulated sites and the
frequency ranges pose nontrivial challenges to the simulation. Computational
requirements in particular will quickly become unmanageable if adequate
techniques are not employed. In this work we describe a novel approach, based
on equivalent elements, that enables the simulation of large chambers with
modest computational resources. The method is then validated against real
measurement results."
"A major challenge of using AMI data in power system analysis is the large
size of the data sets. For rapid analysis that addresses historical behavior of
systems consisting of a few hundred feeders, all of the AMI load data can be
loaded into memory and used in a power flow analysis. However, if a system
contains thousands of feeders then the handling of the AMI data in the analysis
becomes more challenging. The work here seeks to demonstrate that the
information contained in large AMI data sets can be compressed into accurate
load models using wavelets. Two types of wavelet based load models are
considered, the multi-resolution wavelet load model for each individual
customer and the classified wavelet load model for customers that share similar
load patterns. The multi-resolution wavelet load model compresses the data, and
the classified wavelet load model further compresses the data. The method of
grouping customers into classes using the wavelet based classification
technique is illustrated."
"In this paper we describe the design, and implementation of the Open Science
Data Cloud, or OSDC. The goal of the OSDC is to provide petabyte-scale data
cloud infrastructure and related services for scientists working with large
quantities of data. Currently, the OSDC consists of more than 2000 cores and 2
PB of storage distributed across four data centers connected by 10G networks.
We discuss some of the lessons learned during the past three years of operation
and describe the software stacks used in the OSDC. We also describe some of the
research projects in biology, the earth sciences, and social sciences enabled
by the OSDC."
"An effective model of single and multilayered thin panels, including those
formed using carbon fiber composite (CFC) materials, is incorporated into the
Transmission Line Modeling (TLM) method. The thin panel model is a
one-dimensional (1D) one based on analytical expansions of cotangent and
cosecant functions that are used to describe the admittance matrix in the
frequency domain; these are then converted into the time domain by using
digital filter theory and an inverse Z transform. The model, which is extended
to allow for material anisotropy, is executed within 1D TLM codes. And, for the
first time, the two-dimensional (2D) thin surface model is embedded in
unstructured three-dimensional (3D) TLM codes. The approach is validated by
using it to study some canonical structures with analytic solutions, and
against results taken from the literature. It is then used to investigate
shielding effectiveness of carbon fiber composite materials in a practical
curved aerospace-related structure."
"In this paper, we propose a computational framework,which is based on a
domain decomposition technique, to employ both finite element method (which is
a popular continuum modeling approach) and lattice Boltzmann method (which is a
popular pore-scale modeling approach) in the same computational domain. To
bridge the gap across the disparate length and time-scales, we first propose a
new method to enforce continuum-scale boundary conditions (i.e., Dirichlet and
Neumann boundary conditions) onto the numerical solution from the lattice
Boltzmann method. This method are based on maximization of entropy and preserve
the non-negativity of discrete distributions under the lattice Boltzmann
method. The proposed computational framework allows different grid sizes,
orders of interpolation, and time-steps in different subdomains. This allows
for different desired resolutions in the numerical solution in different
subdomains. Through numerical experiments, the effect of grid and time-step
refinement, disparity of time-steps in different subdomains, domain
partitioning, and the number of iteration steps on the accuracy and rate of
convergence of the proposed methodology are studied. Finally, to showcase the
performance of this framework in porous media applications, we use it to
simulate the dissolution of calcium carbonate in a porous structure."
"The article considers the discrete analogue of the method of quickest descent
for an inverse Acoustics problem in case of a smooth source. The authors
derived the gradient of functional in differential and discrete cases,
described the algorithm of solving a problem, and compared gradients of
functional in continuous and discrete cases. In the article the improved
estimates of the rates of convergence of gradient-based methods are obtained,
which are very important for practice because they provide with the possibility
to make input data errors consistent with the iteration number. There is a
practical application of the proposed new method of deriving the gradient of
functional for an Acoustics discrete problem, for it provides with calculations
that are more accurate. The theoretical importance of the method is the
developed technique of deriving estimates and gradients of functional at a
discrete level."
"The FTTH (Fiber To The Home) market currently needs new network maintenance
technologies that can, economically and effectively, cope with massive fiber
plants. However, operating these networks requires adequate means for an
effective monitoring cost. Especially for troubleshooting faults that are
associated with the possibility of remote identification of fiber breaks, which
may exist in the network. Optical Time Domain Reflectometry (OTDR) techniques
are widely used in point-to-point optical network topologies. Nevertheless, it
has major limitations in tree-structured PONs (Passive Optical Networks), where
all different branches backscatter the light in just one conventional OTDR
trace with combined signals arriving on the OLT (Optical Line Terminal) side.
Furthermore, passive power splitters used in FTTH networks input large
attenuation, impoverishing the reflected signal. This makes the identification
of the very branch affected by the problem practically impossible, when
considering conventional analyses. The use of constraint-based techniques have
been applied in a large amount of applications for Engineering Design, where
the duties imposed for graphics and equations constraints result in valued
features to CAD/CAE software capabilities. Currently, it provides a faster
decision making capacity for engineers. This work applies the constraint based
approach along with a Differential Evolutionary Algorithm to separate the
superimposed OTDR signals, after the splitters of a FTTH Passive Optical
Networks. This research introduces a new set of algorithms performing a
coupling to an Optical Network (ON) CAD Design with its correspondent OTDR
measurement signal, considering its geographical distribution branches of
different lengths after the splitter. Results of this work are presented in a
FTTN (Fiber To The Node) prototype arrangement, using a 1:8 passive power
splitter."
"The cylindrically converging shock wave was numerically simulated by solving
the Euler equations in cylindrical coordinates with TVD scheme and MUSCL
approach, using Roe's approximate Riemann solver and super-bee nonlinear
limiter. The present study used the in house code developed for this purpose.
The behavior of the solution in the vicinity of axis is investigated and the
results of the numerical solution are compared with the computed data given by
Payne, Lapidus, Abarbanel, and Goldberg, Sod, and Leutioff et al."
"Multiscale entropy (MSE) has been a prevalent algorithm to quantify the
complexity of fluctuations in the local mean value of biomedical time series.
Recent developments in the field have tried to improve the MSE by reducing its
variability in large scale factors. On the other hand, there has been recent
interest in using other statistical moments than the mean, i.e. variance, in
the coarse-graining step of the MSE. Building on these trends, here we
introduce the so-called refined composite multiscale fuzzy entropy based on the
standard deviation (RCMFE{\sigma}) to quantify the dynamical properties of
spread over multiple time scales. We demonstrate the dependency of the
RCMFE{\sigma}, in comparison with other multiscale approaches, on several
straightforward signal processing concepts using a set of synthetic signals. We
also investigate the complementarity of using the standard deviation instead of
the mean in the coarse-graining process using magnetoencephalograms in
Alzheimer disease and publicly available electroencephalograms recorded from
focal and non-focal areas in epilepsy. Our results indicate that RCMFE{\sigma}
offers complementary information to that revealed by classical coarse-graining
approaches and that it has superior performance to distinguish different types
of physiological activity."
"Connection of offshore wind farms to shore requires the use of submarine
cables. In the case of long HVAC connections, the capacitive charging currents
limit the transfer capability and lead to high losses. This paper shows that
the losses can be substantially reduced by continuously adjusting the cable
operating voltage according to the instantaneous wind farm power
production.Calculations for a 320 MW windfarm connected to shore via a 200 km
cable at 220 kV nominal voltage shows that an annual loss reduction of 9
percent is achievable by simply using a 15 percent tap changer voltage
regulation on the two transformers. Allowing a larger voltage regulation range
leads to further loss reduction (13 percent for 0.4-1.0 p.u. voltage range). If
the windfarm has a low utilization factor, the loss reduction potential is
demonstrated to be as high as 21 percent . The methodology can be applied
without introducing new technology that needs to be developed or qualified."
"Conventionally, many researchers have used both regression and black box
techniques to estimate the unconfined compressive strength (UCS) of different
rocks. The advantage of the regression approach is that it can be used to
render a functional relationship between the predictive rock indices and its
UCS. The advantage of the black box techniques is in rendering more accurate
predictions. Gene expression programming (GEP) is proposed, in this study, as a
robust mathematical alternative for predicting the UCS of carbonate rocks. The
two parameters of total porosity and P-wave speed were selected as predictive
indices. The proposed GEP model had the advantage of the both traditionally
used approaches by proposing a mathematical model, similar to a regression,
while keeping the prediction errors as low as the black box methods. The GEP
outperformed both artificial neural networks and support vector machines in
terms of yielding more accurate estimates of UCS. Both the porosity and the
P-wave velocity were sufficient predictive indices for estimating the UCS of
the carbonate rocks in this study. Nearly, 95% of the observed variation in the
UCS values was explained by these two parameters (i.e., R2 =95%)."
"In this paper, we focus on hidden period identification and the periodic
decomposition of signals. Based on recent results on the Ramanujan subspace, we
reveal the conjugate symmetry of the Ramanujan subspace with a set of complex
exponential basis functions and represent the subspace as the union of a series
of conjugate subspaces. With these conjugate subspaces, the signal periodic
model is introduced to characterize the periodic structure of a signal. To
achieve the decomposition of the proposed model, the conjugate subspace
matching pursuit (CSMP) algorithm is proposed based on two different greedy
strategies. The CSMP is performed iteratively in two stages. In the first
stage, the dominant hidden period is chosen with the periodicity strategy.
Then, the dominant conjugate subspace is chosen with the energy strategy in the
second stage. Compared with the current state-of-the-art methods for hidden
period identification, the main advantages provided by the CSMP are the
following: (i) the capability of identifying all the hidden periods in the
range from $1$ to the maximum hidden period $Q$ of a signal of any length,
without truncating the signal; (ii) the ability to identify the time-varying
hidden period with its shifted version; and (iii) the low computational cost,
without generating and using a large over-complete dictionary. Moreover, we
provide examples and applications to demonstrate the abilities of the proposed
two-stage CSMP algorithm, which include hidden period identification, signal
approximation, time-varying period detection, and pitch detection of speech."
"This paper presents our work on designing a platform for large-scale
reservoir simulations. Detailed components, such as grid and linear solver, and
data structures are introduced, which can serve as a guide to parallel
reservoir simulations and other parallel applications. The main objective of
platform is to support implementation of various parallel reservoir simulators
on distributed-memory parallel systems, where MPI (Message Passing Interface)
is employed for communications among computation nodes. It provides structured
grid due to its simplicity and cell-centered data is applied for each cell. The
platform has a distributed matrix and vector module and a map module. The
matrix and vector module is the base of our parallel linear systems. The map
connects grid and linear system modules, which defines various mappings between
grid and linear systems. Commonly-used Krylov subspace linear solvers are
implemented, including the restarted GMRES method and the BiCGSTAB method. It
also has an interface to a parallel algebraic multigrid solver, BoomerAMG from
HYPRE. Parallel general-purpose preconditioners and special preconditioners for
reservoir simulations are also developed. Various data structures are designed,
such as grid, cell, data, linear solver and preconditioner, and some key
default parameters are presented in this paper. The numerical experiments show
that our platform has excellent scalability and it can simulate giant reservoir
models with hundreds of millions of grid cells using thousands of CPU cores."
"In this paper we study the effective thermal behaviour of 3D representative
volume elements (RVEs) of two-phased composite materials constituted by a
matrix with cylindrical and spherical inclusions distributed randomly, with
periodic boundaries. Variations around the shape of inclusions have been taken
into account, by corrugating shapes, excavating and/or by removing pieces of
inclusions. The effective behaviour is computed with the help of homogenization
process based on an accelerated FFT-scheme giving the thermal conductivity
tensor. Several morphological parameters are also taken into account for
instance the number and the volume fraction of each type of inclusions,... in
order to analyse the behaviour of the composite for a large number of
geometries. We compare the results obtained for RVEs with and without
variations, and then with the mechanical results of such composite studied in
our previous paper."
"Prevailing computational tools available to and used by architecture and
engineering professionals purport to gather and present thorough and accurate
perspectives of the environmental impacts associated with their contributions
to the built environment. The presented research of building modeling and
analysis software used by the Architecture, Engineering, Construction, and
Operations (AECO) industry reveals that many of the most heavily relied-upon
industry tools are isolated in functionality, utilize incomplete models and
data, and are disruptive to normative design and building optimization
workflows. This paper describes the current models and tools, their primary
functions and limitations, and presents our concurrent research to develop more
advanced models to assess lifetime building energy consumption alongside
operating energy use. A series of case studies describes the current
state-of-the-art in tools and building energy analysis followed by the research
models and novel design and analysis Tool that the Green Scale Research Group
has developed in response. A fundamental goal of this effort is to increase the
use and efficacy of building impact studies conducted by architects, engineers,
and building owners and operators during the building design process."
"We propose an adaptive polygonal finite element formulation for collapse
plastic analysis of solids. The article contributes into four crucial points:
1) Wachspress shape functions at vertex and bubble nodes handled at a
primal-mesh level; 2) plastic strain rates and dissipation performed over a
dual-mesh level; 3) a new adaptive primal-mesh strategy driven by the L^2
-norm-based indicator of strain rates; and 4) a spatial decomposition structure
obtained from a so-called polytree mesh scheme. We investigate both purely
cohesive and cohesive-frictional materials. We prove numerically that the
present method performs well for volumetric locking problem. In addition, the
optimization formulation of limit analysis is written by the form of
second-order cone programming (SOCP) in order to exploit the high efficiency of
interior-point solvers. The present method retains a low number of optimization
variables. This convenient approach allows us to design and solve the
large-scale optimization problems effectively. Numerical validations show the
excellent performance of the proposed method."
"Global Sensitivity Analysis (GSA) methods are useful tools to rank input
parameters uncertainties regarding their impact on result variability. In
practice, such type of approach is still at an exploratory level for studies
relying on 2D Shallow Water Equations (SWE) codes as GSA requires specific
tools and deals with important computational capacity. The aim of this paper is
to provide both a protocol and a tool to carry out a GSA for 2D hydraulic
modelling applications. A coupled tool between Prom{\'e}th{\'e}e (a parametric
computation environment) and FullSWOF 2D (a code relying on 2D SWE) has been
set up: Prom{\'e}th{\'e}e-FullSWOF 2D (P-FS). The main steps of our protocol
are: i) to identify the 2D hydraulic code input parameters of interest and to
assign them a probability density function, ii) to propagate uncertainties
within the model, and iii) to rank the effects of each input parameter on the
output of interest. For our study case, simulations of a river flood event were
run with uncertainties introduced through three parameters using P-FS tool.
Tests were performed on regular computational mesh, spatially discretizing an
urban area, using up to 17.9 million of computational points. P-FS tool has
been installed on a cluster for computation. Method and P-FS tool successfully
allow the computation of Sobol indices maps. Keywords Uncertainty, flood hazard
modelling, global sensitivity analysis, 2D shallow water equation, Sobol index.
Analyse globale de sensibilit{\'e} en mod{\'e}lisation hydrauliqu{\`e} a
surface libre 2D : application d'un protocole et d{\'e}veloppement d'outils
op{\'e}rationnels -- Les m{\'e}thodes d'analyse de sensibilit{\'e} permettent
de contr{\^o}ler la robustesse des r{\'e}sultats de mod{\'e}lisation ainsi que
d'identifier le degr{\'e} d'influence des param etres d' entr{\'e}e sur le
r{\'e}sultat en sortie d'un mod ele. Le processus complet constitue une analyse
globale de sensibilit{\'e} (GSA). Ce type d'approche pr{\'e}sente un grand
int{\'e}r{\^e}t pour analyser les incer-titudes de r{\'e}sultats de
mod{\'e}lisation , mais est toujours a un stade exploratoire dans les etudes
appliqu{\'e}es mettant en jeu des codes bas{\'e}s sur la r{\'e}solution
bidimensionnelle des equations de Saint-Venant. En effet, l' impl{\'e}mentation
d'une GSA est d{\'e}licate car elle"
"We discuss the problem of the optimal design of a micro-tubular fuel cell
applying an inverse homogenization technique. Fuel cells are extremely clean
and efficient electrochemical power generation devices, made up of a
cathode/electrolyte/anode structure, whose energetic potential has not being
fully exploited in propulsion systems in aeronautics due to their low power
densities. Nevertheless, thanks to the recent development of additive layer
manufacturing techniques (3D printing), complex structures usually impossible
to design with conventional manufacturing techniques can be constructed with a
low cost, allowing notably to build porous or foam-type structures for fuel
cells. We seek thus to come up with the micro-structure of an arrangement of
micro-tubular cathodes which maximizes the contact surface subject to a
pressure drop and a permeability constraint. The optimal periodic design
(fluid/solid) emerges from the application of a shape gradient algorithm
coupled to a level-set method for the geometrical description of the
corresponding cell problem."
"The objective of this work is the development of a novel finite element
formulation describing the contact interaction of slender beams in complex 3D
configurations involving arbitrary beam-to-beam orientations. It is shown in a
mathematically concise manner that standard beam contact models based on a
point-wise contact force fail to describe a considerable range of
configurations, which are, however, likely to occur in practical applications.
On the contrary, the formulation proposed here models beam-to-beam contact by
means of distributed line forces, a procedure that is shown to be applicable
for arbitrary geometrical configurations. The proposed formulation is based on
a Gauss-point-to-segment type contact discretization and a penalty
regularization of the contact constraint. By means of detailed theoretical and
numerical investigations, it is shown that this approach is more suitable for
beam contact than possible alternatives based on mortar type contact
discretizations or constraint enforcement by means of Lagrange multipliers. The
proposed formulation is enhanced by a consistently linearized integration
interval segmentation avoiding numerical integration across strong
discontinuities. In combination with a smoothed contact force law and the
employed C1-continuous beam elements, this procedure drastically reduces the
numerical integration error, an essential prerequisite for optimal spatial
convergence rates. The resulting line-to-line contact algorithm is supplemented
by contact contributions of the beam endpoints, which represent boundary minima
of the underlying minimal distance problem. Finally, a series of numerical test
cases is analyzed in order to investigate the accuracy and consistency of the
proposed formulation regarding integration error, spatial convergence behavior
and resulting contact force distributions."
"Analysis of static bending, free vibration and buckling behaviours of
functionally graded microplates is investigated in this study. The main idea is
to use the isogeometric analysis in associated with novel four-variable refined
plate theory and quasi-3D theory. More importantly, the modified couple stress
theory with only one material length scale parameter is employed to effectively
capture the size-dependent effects within the microplates. Meanwhile, the
quasi-3D theory which is constructed from a novel seventh-order shear
deformation refined plate theory with four unknowns is able to consider both
shear deformations and thickness stretching effect without requiring shear
correction factors. The NURBS-based isogeometric analysis is integrated to
exactly describe the geometry and approximately calculate the unknown fields
with higher-order derivative and continuity requirements. The convergence and
verification show the validity and efficiency of this proposed computational
approach in comparison with those existing in the literature. It is further
applied to study the static bending, free vibration and buckling responses of
rectangular and circular functionally graded microplates with various types of
boundary conditions. A number of investigations are also conducted to
illustrate the effects of the material length scale, material index, and
length-to-thickness ratios on the responses of the microplates."
"We present a new method for the estimation of two dimensional (2D) angles of
arrival (AOAs), namely, azimuth and incidence angles of multiple narrowband
signals of same frequency in the far field of antenna array."
"This paper investigates nonlinear bending and buckling behaviours of
composite plates characterized by a thickness variation. Layer interfaces are
described as functions of inplane coordinates. Top and bottom surfaces of the
plate are symmetric about the midplane and the plate could be considered as a
flat surface in analysis along with thickness parameters which vary over the
plate. The variable thickness at a certain position in the midplane is modeled
by a set of control points (or thickness-parameters) through NURBS (Non-Uniform
Rational B-Spline) basic functions. The knot parameter space which is referred
in modelling geometry and approximating displacement variables is employed for
approximating thickness, simultaneously. The use of quadratic NURBS functions
results in C^1 continuity of modeling variable thickness and analyzing
solutions. Thin to moderately thick laminates in bound of first-order shear
deformation theory (FSDT) are taken into account. Strain-displacement relations
in sense of von-Karman theory are employed for large deformation. Riks method
is used for geometrically nonlinear analysis. The weak form is approximated
numerically by the isogeometric analysis (IGA), which has been found to be a
robust, stable and realistic numerical tool. Numerical results confirm the
reliability and capacity of the propose method."
"Mathematical models of physical systems are subject to many uncertainties
such as measurement errors and uncertain initial and boundary conditions. After
accounting for these uncertainties, it is often revealed that discrepancies
between the model output and the observations remain; if so, the model is said
to be inadequate. In practice, the inadequate model may be the best that is
available or tractable, and so despite its inadequacy the model may be used to
make predictions of unobserved quantities. In this case, a representation of
the inadequacy is necessary, so the impact of the observed discrepancy can be
determined. We investigate this problem in the context of chemical kinetics and
propose a new technique to account for model inadequacy that is both
probabilistic and physically meaningful. A stochastic inadequacy operator
$\mathcal{S}$ is introduced which is embedded in the ODEs describing the
evolution of chemical species concentrations and which respects certain
physical constraints such as conservation laws. The parameters of $\mathcal{S}$
are governed by probability distributions, which in turn are characterized by a
set of hyperparameters. The model parameters and hyperparameters are calibrated
using high-dimensional hierarchical Bayesian inference. We apply the method to
a typical problem in chemical kinetics---the reaction mechanism of hydrogen
combustion."
"This work addresses the optimal control of multibody systems being actuated
with control forces in order to find a dynamically feasible minimum-energy
trajectory of the system. The optimal control problem and its constraints are
integrated in a discrete version of the equation of motion allowing the
minimization of system energy with respect to a discrete state and control
trajectory. The work is centred on a specific type of open-chain multibody
system, with strong local propensity, where the overall system kinematics is
described essentially by the torsion around the links that connect rigid
bodies. The coupling between the rigid body motion, and the optimal
conformation is described as an elastic band of replicas of the original system
with different conformations. The band forces are used to control system's
motion directly, reflecting the influence of the system energy field on its
conformation, using for that the Nudged-Elastic Band method. Here the equation
of motion of the multibody grid are solved by using the augmented Lagrangean
method. In this context, if a feasible minimum-energy trajectory of the
original system exists it is a stationary state of the extended system. This
approach is applied to the folding of a single chain protein."
"In this paper we use Clustering Method to understand whether stock market
volatility can be predicted at all, and if so, when it can be predicted. The
exercise has been performed for the Indian stock market on daily data for two
years. For our analysis we map number of clusters against number of variables.
We then test for efficiency of clustering. Our contention is that, given a
fixed number of variables, one of them being historic volatility of NIFTY
returns, if increase in the number of clusters improves clustering efficiency,
then volatility cannot be predicted. Volatility then becomes random as, for a
given time period, it gets classified in various clusters. On the other hand,
if efficiency falls with increase in the number of clusters, then volatility
can be predicted as there is some homogeneity in the data. If we fix the number
of clusters and then increase the number of variables, this should have some
impact on clustering efficiency. Indeed if we can hit upon, in a sense, an
optimum number of variables, then if the number of clusters is reasonably
small, we can use these variables to predict volatility. The variables that we
consider for our study are volatility of NIFTY returns, volatility of gold
returns, India VIX, CBOE VIX, volatility of crude oil returns, volatility of
DJIA returns, volatility of DAX returns, volatility of Hang Seng returns and
volatility of Nikkei returns. We use three clustering algorithms namely Kernel
K-Means, Self Organizing Maps and Mixture of Gaussian models and two internal
clustering validity measures, Silhouette Index and Dunn Index, to assess the
quality of generated clusters."
"An air pollution model is generally described by a system of PDEs on
unbounded domain. Transformation of the independent variable is used to convert
the problem for nonlinear air pollution on finite computational domain. We
investigate the new, degenerated parabolic problem in Sobolev spaces with
weights for well-posedness and positivity of the solution. Then we construct a
fitted finite volume difference scheme. Some results from computations are
presented."
"Technologies such as aerial photogrammetry allow production of 3D topographic
data including complex environments such as urban areas. Therefore, it is
possible to create High Resolution (HR) Digital Elevation Models (DEM)
incorporating thin above ground elements influencing overland flow paths. Even
though this category of big data has a high level of accuracy, there are still
errors in measurements and hypothesis under DEM elaboration. Moreover,
operators look for optimizing spatial discretization resolution in order to
improve flood models computation time. Errors in measurement, errors in DEM
generation, and operator choices for inclusion of this data within 2D hydraulic
model, might influence results of flood models simulations. These errors and
hypothesis may influence significantly flood modelling results variability. The
purpose of this study is to investigate uncertainties related to (i) the own
error of high resolution topographic data, and (ii) the modeller choices when
including topographic data in hydraulic codes. The aim is to perform a Global
Sensitivity Analysis (GSA) which goes through a Monte-Carlo uncertainty
propagation, to quantify impact of uncertainties, followed by a Sobol' indices
computation, to rank influence of identified parameters on result variability.
A process using a coupling of an environment for parametric computation
(Prom{\'e}th{\'e}e) and a code relying on 2D shallow water equations (FullSWOF
2D) has been developed (P-FS tool). The study has been performed over the lower
part of the Var river valley using the estimated hydrograph of 1994 flood
event. HR topographic data has been made available for the study area, which is
17.5 km 2 , by Nice municipality. Three uncertain parameters were studied: the
measurement error (var. E), the level of details of above-ground element
representation in DEM (buildings, sidewalks, etc.) (var. S), and the spatial
discretization resolution (grid cell size for regular mesh) (var. R). Parameter
var. E follows a probability density function, whereas parameters var. S and
var. R. are discrete operator choices. Combining these parameters, a database
of 2, 000 simulations has been produced using P-FS tool implemented on a high
performance computing structure. In our study case, the output of interest is
the maximal"
"Due to the diversity of fuel cycle simulator modeling assumptions, direct
comparison and benchmarking can be difficult. In 2012 the Organisation for
Economic Co-operation and Development completed a benchmark study that is
perhaps the most complete published comparison performed. Despite this, various
results from the simulators were often significantly different because of
inconsistencies in modeling decisions involving reprocessing strategies,
refueling behavior, reactor end-of-life handling, etc. This work identifies and
quantifies the effects of selected modeling choices that may sometimes be taken
for granted in the fuel cycle simulation domain. Four scenarios are compared
using combinations of fleet-based or individually modeled reactors with monthly
or quarterly (3-month) time steps. The scenarios approximate a transition from
the current U.S. once-through light water reactor fleet to a full sodium fast
reactor fuel cycle. The Cyclus fuel cycle simulator's plug-in capability along
with its market-like dynamic material routing allow it to be used as a level
playing field for comparing the scenarios. When under supply-constraint
pressure, the four cases exhibit noticeably different behavior. Fleet-based
modeling is more efficient in supply-constrained environments at the expense of
losing insight on issues such as realistically suboptimal fuel distribution and
challenges in reactor refueling cycle staggering. Finer-grained time steps
enable more efficient material use in supply-constrained environments resulting
in lower standing inventories of separated Pu. Large simulations with
fleet-based reactors run much more quickly than their individual reactor
counterparts. Gaining a better understanding of how these and other modeling
choices affect fuel cycle dynamics will enable making more deliberate decisions
with respect to trade-offs such as computational investment vs. realism."
"In this paper we present computational techniques to investigate the
solutions of two-component, nonlinear reaction-diffusion (RD) systems on
arbitrary surfaces. We build on standard techniques for linear and nonlinear
analysis of RD systems, and extend them to operate on large-scale meshes for
arbitrary surfaces. In particular, we use spectral techniques for a linear
stability analysis to characterize and directly compose patterns emerging from
homogeneities. We develop an implementation using surface finite element
methods and a numerical eigenanalysis of the Laplace-Beltrami operator on
surface meshes. In addition, we describe a technique to explore solutions of
the nonlinear RD equations using numerical continuation. Here, we present a
multiresolution approach that allows us to trace solution branches of the
nonlinear equations efficiently even for large-scale meshes. Finally, we
demonstrate the working of our framework for two RD systems with applications
in biological pattern formation: a Brusselator model that has been used to
model pattern development on growing plant tips, and a chemotactic model for
the formation of skin pigmentation patterns. While these models have been used
previously on simple geometries, our framework allows us to study the impact of
arbitrary geometries on emerging patterns."
"Tensor decomposition plays a key role in identifying common features across a
collection of matrices in many areas of science. A fundamental need in big data
research is to process data tabulated as large-scale matrices using
eigenvectors. A higher order generalized singular value decomposition technique
successfully captures the common features of the same organ from multiple
animals in genomic signal processing. A recent semidefinite programming
approach to solve an AC optimal power flow was accompanied by the problem
formulation in the Cartesian coordinate system. The collection of nodal
Kirchhoff laws introduces a 3D tensor with a common feature of individual
matrices to maintain local power balance. In this paper, the mathematical
process is established and the common feature is identified. The common feature
is a key element to a fully decentralized and therefore scalable algorithm to
solve AC optimal power flow."
"For many macromolecular systems the accurate sampling of the relevant regions
on the potential energy surface cannot be obtained by a single, long Molecular
Dynamics (MD) trajectory. New approaches are required to promote more efficient
sampling. We present the design and implementation of the Extensible Toolkit
for Advanced Sampling and analYsis (ExTASY) for building and executing advanced
sampling workflows on HPC systems. ExTASY provides Python based ""templated
scripts"" that interface to an interoperable and high-performance pilot-based
run time system, which abstracts the complexity of managing multiple
simulations. ExTASY supports the use of existing highly-optimised parallel MD
code and their coupling to analysis tools based upon collective coordinates
which do not require a priori knowledge of the system to bias. We describe two
workflows which both couple large ""ensembles"" of relatively short MD
simulations with analysis tools to automatically analyse the generated
trajectories and identify molecular conformational structures that will be used
on-the-fly as new starting points for further ""simulation-analysis"" iterations.
One of the workflows leverages the Locally Scaled Diffusion Maps technique; the
other makes use of Complementary Coordinates techniques to enhance sampling and
generate start-points for the next generation of MD simulations. We show that
the ExTASY tools have been deployed on a range of HPC systems including ARCHER
(Cray CX30), Blue Waters (Cray XE6/XK7), and Stampede (Linux cluster), and that
good strong scaling can be obtained up to 1000s of MD simulations, independent
of the size of each simulation. We discuss how ExTASY can be easily extended or
modified by end-users to build their own workflows, and ongoing work to improve
the usability and robustness of ExTASY."
"An overall objective of energy efficiency in the built environment is to
improve building and systems performances in terms of durability, comfort and
economics. In order to predict, improve and meet a certain set of performance
requirements related to the indoor climate of buildings and the associated
energy demand, building energy simulation (BES) tools are indispensable. Due to
the rapid development of FEM software and the Multiphysics approaches, it
should possible to build and simulate full 3D models of buildings regarding the
energy demand. The paper presents a methodology for performing building energy
simulation with Comsol. The method was applied to an international test box
experiment. The results showed an almost perfect agreement between the used BES
model and Comsol. These preliminary results confirm the great opportunities to
use FEM related software for building energy performance simulation."
"We discuss Bayesian inference (BI) for the probabilistic identification of
material parameters. This contribution aims to shed light on the use of BI for
the identification of elastoplastic material parameters. For this purpose a
single spring is considered, for which the stress-strain curves are
artificially created. Besides offering a didactic introduction to BI, this
paper proposes an approach to incorporate statistical errors both in the
measured stresses, and in the measured strains. It is assumed that the
uncertainty is only due to measurement errors and the material is homogeneous.
Furthermore, a number of possible misconceptions on BI are highlighted based on
the purely elastic case."
"In this work we study the dynamic behaviour of compound shells of revolution
partially filled with an ideal incompressible fluid based on boundary-value
problems. New analytical mathematical model with corresponding discrete scheme
for the elastic displacements and the dynamic liquid pressure is developed. The
discrete scheme is based on the method of discrete singularities. A code to
perform the numerical analysis is developed. Comprehensive benchmarking of the
obtained results against other methods is done and good agreement is observed.
The convergence of the proposed numerical method is demonstrated. One of the
advantages of this new model is that the initial 3D problem is analytically
reduced to a 1D integral equation. Moreover, it can handle the behaviour of the
pressure in the vicinity of the nodes explicitly and the computational
technique used has a quick convergence requiring a negligible amount of CPU
time."
"Estimation of the Saupe tensor is central to the determination of molecular
structures from residual dipolar couplings (RDC) or chemical shift
anisotropies. Assuming a given template structure, the singular value
decomposition (SVD) method proposed in Losonczi et al. 1999 has been used
traditionally to estimate the Saupe tensor. Despite its simplicity, whenever
the template structure has large structural noise, the eigenvalues of the
estimated tensor have a magnitude systematically smaller than their actual
values. This leads to systematic error when calculating the eigenvalue
dependent parameters, magnitude and rhombicity. We propose here a Monte Carlo
simulation method to remove such bias. We further demonstrate the effectiveness
of our method in the setting when the eigenvalue estimates from multiple
template protein fragments are available and their average is used as an
improved eigenvalue estimator. For both synthetic and experimental RDC datasets
of ubiquitin, when using template fragments corrupted by large noise, the
magnitude of our proposed bias-reduced estimator generally reaches at least 90%
of the actual value, whereas the magnitude of SVD estimator can be shrunk below
80% of the true value."
"This paper establishes a far-reaching connection between the
Finite-Difference Time-Domain method (FDTD) and the theory of dissipative
systems. The FDTD equations for a rectangular region are written as a dynamical
system having the magnetic and electric fields on the boundary as inputs and
outputs. Suitable expressions for the energy stored in the region and the
energy absorbed from the boundaries are introduced, and used to show that the
FDTD system is dissipative under a generalized Courant-Friedrichs-Lewy
condition. Based on the concept of dissipation, a powerful theoretical
framework to investigate the stability of FDTD methods is devised. The new
method makes FDTD stability proofs simpler, more intuitive, and modular.
Stability conditions can indeed be given on the individual components (e.g.
boundary conditions, meshes, embedded models) instead of the whole coupled
setup. As an example of application, we derive a new subgridding method with
material traverse, arbitrary grid refinement, and guaranteed stability. The
method is easy to implement and has a straightforward stability proof.
Numerical results confirm its stability, low reflections, and ability to handle
material traverse."
"In this study I briefly illustrate application of the Gaussian mixtures to
approximate empirical distributions of financial indices (DAX, Dow Jones,
Nikkei, RTSI, S&P 500). The resulting distributions illustrate very high
quality of approximation as evaluated by Kolmogorov-Smirnov test. This implies
further study of application of the Gaussian mixtures to approximate empirical
distributions of financial indices."
"The stochastic Landau-Lifshitz-Gilbert-Slonczewski (s-LLGS) equation is
widely used to study the temporal evolution of the macrospin subject to spin
torque and thermal noise. The numerical simulation of the s-LLGS equation
requires an appropriate choice of stochastic calculus and numerical integration
scheme. In this paper, we comprehensively evaluate the accuracy and complexity
of various numerical techniques to solve the s-LLGS equation. We focus on
implicit midpoint, Heun, and Euler-Heun methods that converge to the
Stratonovich solution of the s-LLGS equation. By performing numerical tests for
both strong (path-wise) and weak (statistical) convergence, we quantify the
accuracy of various numerical schemes used to solve the s-LLGS equation. We
demonstrate a new method intended to solve Stochastic Differential Equations
(SDEs) with small noise (RK4-Heun), and test its capability to handle the
s-LLGS equation. We also discuss the circuit implementation of nanomagnets for
large-scale SPICE-based simulations. We evaluate the efficacy of SPICE in
handling the stochastic dynamics of the multiplicative noise in the s-LLGS
equation. Numerical schemes such as Euler and Gear, typically used by
SPICE-based circuit simulators do not yield the expected outcome when solving
the Stratonovich s-LLGS equation. While the trapezoidal method in SPICE does
solve for the Stratonovich solution, its accuracy is limited by the minimum
time step of integration in SPICE. We implement the s-LLGS equation in both its
cartesian and spherical coordinates form in SPICE and compare the stability and
accuracy of the two implementations. The results in this paper will serve as
guidelines for researchers to understand the tradeoffs between accuracy and
complexity of various numerical methods and the choice of appropriate calculus
to solve the s-LLGS equation."
"Existing beam contact formulations can be categorized in point contact models
that consider a discrete contact force at the closest point of the beams, and
line contact models that assume distributed contact forces. In this work, it
will be shown that line contact formulations provide accurate and robust
mechanical models in the range of small contact angles, whereas the
computational efficiency considerably decreases with increasing contact angles.
On the other hand, point contact formulations serve as sufficiently accurate
and very efficient models in the regime of large contact angles, while they are
not applicable for small contact angles as a consequence of non-unique closest
point projections. In order to combine the advantages of these basic
formulations, a novel all-angle beam contact (ABC) formulation is developed
that applies a point contact formulation for large contact angles and a
recently developed line contact formulation for small contact angles, the two
being smoothly connected by means of a variationally consistent model
transition. Based on a stringent analysis, two different transition laws are
investigated, optimal algorithmic parameters are suggested and conservation of
energy and momentum is shown. All configuration-dependent quantities are
consistently linearized, thus allowing for their application within implicit
time integration schemes. Furthermore, a step size control of the nonlinear
solution scheme that allows for large displacement increments per time step and
an efficient two-stage contact search based on dynamically adapted search
segments are proposed. A series of numerical test cases is analyzed in order to
verify the accuracy and consistency of the proposed contact model transition
regarding contact force distributions and conservation properties, but also for
quantifying the efficiency gains as compared to standard beam contact
formulations."
"A modeling paradigm is developed to augment predictive models of turbulence
by effectively utilizing limited data generated from physical experiments. The
key components of our approach involve inverse modeling to infer the spatial
distribution of model discrepancies, and, machine learning to reconstruct
discrepancy information from a large number of inverse problems into corrective
model forms. We apply the methodology to turbulent flows over airfoils
involving flow separation. Model augmentations are developed for the Spalart
Allmaras (SA) model using adjoint-based full field inference on experimentally
measured lift coefficient data. When these model forms are reconstructed using
neural networks (NN) and embedded within a standard solver, we show that much
improved predictions in lift can be obtained for geometries and flow conditions
that were not used to train the model. The NN-augmented SA model also predicts
surface pressures extremely well. Portability of this approach is demonstrated
by confirming that predictive improvements are preserved when the augmentation
is embedded in a different commercial finite-element solver. The broader vision
is that by incorporating data that can reveal the form of the innate model
discrepancy, the applicability of data-driven turbulence models can be extended
to more general flows."
"In this article, we present a new unified finite element method (UFEM) for
simulation of general Fluid-Structure interaction (FSI) which has the same
generality and robustness as monolithic methods but is significantly more
computationally efficient and easier to implement. Our proposed approach has
similarities with classical immersed finite element methods (IFEMs), by
approximating a single velocity and pressure field in the entire domain (i.e.
occupied by fluid and solid) on a single mesh, but differs by treating the
corrections due to the solid deformation on the left-hand side of the modified
fluid flow equations (i.e. implicitly). The method is described in detail,
followed by the presentation of multiple computational examples in order to
validate it across a wide range of fluid and solid parameters and interactions."
"A new approach for generating stress-constrained topological designs in
continua is presented. The main novelty is in the use of elasto-plastic
modeling and in optimizing the design such that it will exhibit a
linear-elastic response. This is achieved by imposing a single global
constraint on the total sum of equivalent plastic strains, providing accurate
control over all local stress violations. The single constraint essentially
replaces a large number of local stress constraints or an approximate
aggregation of them--two common approaches in the literature. A classical
rate-independent plasticity model is utilized, for which analytical adjoint
sensitivity analysis is derived and verified. Several examples demonstrate the
capability of the computational procedure to generate designs that challenge
results from the literature, in terms of the obtained stiffness-strength-weight
trade-offs. A full elasto-plastic analysis of the optimized designs shows that
prior to the initial yielding, these designs can sustain significantly higher
loads than minimum compliance topological layouts, with only a minor compromise
on stiffness."
"The present work focuses on geometrically exact finite elements for highly
slender beams. It aims at the proposal of novel formulations of Kirchhoff-Love
type, a detailed review of existing formulations of Kirchhoff-Love and
Simo-Reissner type as well as a careful evaluation and comparison of the
proposed and existing formulations. Two different rotation interpolation
schemes with strong or weak Kirchhoff constraint enforcement, respectively, as
well as two different choices of nodal triad parametrizations in terms of
rotation or tangent vectors are proposed. The combination of these schemes
leads to four novel finite element variants, all of them based on a
C1-continuous Hermite interpolation of the beam centerline. Essential
requirements such as representability of general 3D, large-deformation, dynamic
problems involving slender beams with arbitrary initial curvatures and
anisotropic cross-section shapes or preservation of objectivity and
path-independence will be investigated analytically and verified numerically
for the different formulations. It will be shown that the geometrically exact
Kirchhoff-Love beam elements proposed in this work are the first ones of this
type that fulfill all the considered requirements. On the contrary,
Simo-Reissner type formulations fulfilling these requirements can be found in
the literature very well. However, it will be argued that the shear-free
Kirchhoff-Love formulations can provide considerable numerical advantages when
applied to highly slender beams. Concretely, several representative numerical
test cases confirm that the proposed Kirchhoff-Love formulations exhibit a
lower discretization error level as well as a considerably improved nonlinear
solver performance in the range of high beam slenderness ratios as compared to
two representative Simo-Reissner element formulations from the literature."
"We propose a new approach to linear ill-posed inverse problems. Our algorithm
alternates between enforcing two constraints: the measurements and the
statistical correlation structure in some transformed space. We use a
non-linear multiscale scattering transform which discards the phase and thus
exposes strong spectral correlations otherwise hidden beneath the phase
fluctuations. As a result, both constraints may be put into effect by linear
projections in their respective spaces. We apply the algorithm to
super-resolution and tomography and show that it outperforms ad hoc convex
regularizers and stably recovers the missing spectrum."
"We propose the use of an angel-daemon framework to perform an uncertainty
analysis of short-term macroeconomic models with exogenous components. An
uncertainty profile $\mathcal U$ is a short and macroscopic description of a
potentially perturbed situation. The angel-daemon framework uses $\mathcal U$
to define a strategic game where two agents, the angel and the daemon, act
selfishly having different goals. The Nash equilibria of those games provide
the stable strategies in perturbed situations, giving a natural estimation of
uncertainty.
  In this initial work we apply the framework in order to get an uncertainty
analysis of linear versions of the IS-LM and the IS-MP models. In those models,
by considering uncertainty profiles, we can capture different economical
situations. Some of them can be described in terms of macroeconomic policy
coordination. In other cases we just analyse the results of the system under
some possible perturbation level. Besides providing examples of application we
analyse the structure of the Nash equilibria in some particular cases of
interest."
"This work deals with the computation of industry-relevant bond wire failure
probabilities in microelectronic packages. Under operating conditions, a
package is subject to Joule heating that can lead to electrothermally induced
failures. Manufacturing tolerances result, e.g., in uncertain bond wire
geometries that often induce very small failure probabilities requiring a high
number of Monte Carlo (MC) samples to be computed. Therefore, a hybrid MC
sampling scheme that combines the use of an expensive computer model with a
cheap surrogate is used. The fraction of surrogate evaluations is maximized
using an iterative procedure, yielding accurate results at reduced cost.
Moreover, the scheme is non-intrusive, i.e., existing code can be reused. The
algorithm is used to compute the failure probability for an example package and
the computational savings are assessed by performing a surrogate efficiency
study."
"The computational efficiency of the Finite-Difference Time-Domain (FDTD)
method can be significantly reduced by the presence of complex objects with
fine features. Small geometrical details impose a fine mesh and a reduced time
step, significantly increasing computational cost. Model order reduction has
been proposed as a systematic way to generate compact models for complex
objects, that one can then instantiate into a main FDTD mesh. However, the
stability of FDTD with embedded reduced models remains an open problem. We
propose a systematic method to generate reduced models for FDTD domains, and
embed them into a main FDTD mesh with guaranteed stability. Models can be
created for arbitrary domains containing inhomogeneous and lossy materials. The
Courant-Friedrichs-Lewy (CFL) limit of the final scheme is provided by the
theory, and can be extended with a simple perturbation of the coefficients of
the reduced models. Numerical tests confirm the stability of the proposed
method, and its potential to accelerate multiscale FDTD simulations."
"Two-stage robust optimization has emerged as a relevant approach to deal with
uncertain demand and generation capacity in the transmission network expansion
planning problem. Unfortunately, available solution methodologies for the
resulting trilevel robust counterpart are unsuitable for large-scale problems.
In order to overcome this shortcoming, this paper presents an alternative
column-and-constraint generation algorithm wherein the max-min problem
associated with the second stage is solved by a novel coordinate descent
method. As a major salient feature, the proposed approach does not rely on the
transformation of the second-stage problem to a single-level equivalent. As a
consequence, bilinear terms involving dual variables or Lagrange multipliers do
not arise, thereby precluding the use of computationally expensive big-M-based
linearization schemes. Thus, not only is the computational effort reduced, but
also the typically overlooked non-trivial tuning of bounding parameters for
dual variables or Lagrange multipliers is avoided. The practical applicability
of the proposed methodology is confirmed by numerical testing on several
benchmarks including a case based on the Polish 2383-bus system, which is well
beyond the capability of the robust methods available in the literature."
"This paper presents a formulation for brittle fracture in 3D elastic solids
within the context of configurational mechanics. The local form of the first
law of thermodynamics provides a condition for equilibrium of the crack front.
The direction of the crack propagation is shown to be given by the direction of
the configurational forces on the crack front that maximise the local
dissipation. The evolving crack front is continuously resolved by the finite
element mesh, without the need for face splitting or the use of enrichment
techniques. A monolithic solution strategy is adopted, solving simultaneously
for both the material displacements (i.e. crack extension) and the spatial
displacements, is adopted. In order to trace the dissipative loading path, an
arc-length procedure is developed that controls the incremental crack area
growth. In order to maintain mesh quality, smoothing of the mesh is undertaken
as a continuous process, together with face flipping, node merging and edge
splitting where necessary. Hierarchical basis functions of arbitrary polynomial
order are adopted to increase the order of approximation without the need to
change the finite element mesh. Performance of the formulation is demonstrated
by means of three representative numerical simulations, demonstrating both
accuracy and robustness."
"We explore the performance and advantages/disadvantages of using
unconditionally stable explicit super time-stepping (STS) algorithms versus
implicit schemes with Krylov solvers for integrating parabolic operators in
thermodynamic MHD models of the solar corona. Specifically, we compare the
second-order Runge-Kutta Legendre (RKL2) STS method with the implicit backward
Euler scheme computed using the preconditioned conjugate gradient (PCG) solver
with both a point-Jacobi and a non-overlapping domain decomposition ILU0
preconditioner. The algorithms are used to integrate anisotropic Spitzer
thermal conduction and artificial kinematic viscosity at time-steps much larger
than classic explicit stability criteria allow. A key component of the
comparison is the use of an established MHD model (MAS) to compute a real-world
simulation on a large HPC cluster. Special attention is placed on the parallel
scaling of the algorithms. It is shown that, for a specific problem and model,
the RKL2 method is comparable or surpasses the implicit method with PCG solvers
in performance and scaling, but suffers from some accuracy limitations. These
limitations, and the applicability of RKL methods are briefly discussed."
"We present methodology, algorithms and software for evaluating flow and view
for architectural settlement layouts. For a given settlement layout consisting
of a number of buildings arbitrarily positioned on a piece of land, in the
present study an island situated on the west coast of Sweden, the methodology
allows for evaluation of flow patterns and for evaluating the view experienced
from the buildings. The computation of flow is based on a multimesh finite
element method, which allows each building to be embedded in a boundary-fitted
mesh which can be moved around freely in a fixed background mesh. The
computation of view is based on a novel and objective measure of the view which
can be efficiently computed by rasterization."
"A coupled hygro-thermo-mechanical computational model is proposed for fibre
reinforced polymers, formulated within the framework of Computational
Homogenisation (CH). At each macrostructure Gauss point, constitutive matrices
for thermal, moisture transport and mechanical responses are calculated from CH
of the underlying representative volume element (RVE). A degradation model,
developed from experimental data relating evolution of mechanical properties
over time for a given exposure temperature and moisture concentration is also
developed and incorporated in the proposed computational model. A unified
approach is used to impose the RVE boundary conditions, which allows convenient
switching between linear Dirichlet, uniform Neumann and periodic boundary
conditions. A plain weave textile composite RVE consisting of yarns embedded in
a matrix is considered in this case. Matrix and yarns are considered as
isotropic and transversely isotropic materials respectively. Furthermore, the
computational framework utilises hierarchic basis functions and designed to
take advantage of distributed memory high-performance computing."
"The Closest Point method, initially developed by Ruuth and Merriman, allows
for the numerical solution of surface partial differential equations without
the need for a parameterization of the surface itself. Surface quantities are
embedded into the surrounding domain by assigning each value at a given spatial
location to the corresponding value at the closest point on the surface. This
embedding allows for surface derivatives to be replaced by their Cartesian
counterparts (e.g. $\nabla_s = \nabla$). This equivalence is only valid on the
surface, and thus, interpolation is used to enforce what is known as the side
condition away from the surface. To improve upon the method, this work derives
an operator embedding that incorporates curvature information, making it valid
in a neighborhood of the surface. With this, direct enforcement of the side
condition is no longer needed. Comparisons in $\mathbb{R}^2$ and $\mathbb{R}^3$
show that the resulting Curvature-Augmented Closest Point method has better
accuracy and requires less memory, through increased matrix sparsity, than the
Closest Point method, while maintaining similar matrix condition numbers. To
demonstrate the utility of the method in a physical application, simulations of
inextensible, bi-lipid vesicles evolving toward equilibrium shapes are also
included."
"In this paper, electrothermal field phenomena in electronic components are
considered. This coupling is tackled by multiphysical field simulations using
the Finite Integration Technique (FIT). In particular, the design of bonding
wires with respect to thermal degradation is investigated. Instead of resolving
the wires by the computational grid, lumped element representations are
introduced as point-to-point connections in the spatially distributed model.
Fabrication tolerances lead to uncertainties of the wires' parameters and
influence the operation and reliability of the final product. Based on
geometric measurements, the resulting variability of the wire temperatures is
determined using the stochastic electrothermal field-circuit model."
"Starting from a 3D electrothermal field problem discretized by the Finite
Integration Technique, the equivalence to a circuit description is shown by
exploiting the analogy to the Modified Nodal Analysis approach. Using this
analogy, an algorithm for the automatic generation of a monolithic SPICE
netlist is presented. Joule losses from the electrical circuit are included as
heat sources in the thermal circuit. The thermal simulation yields nodal
temperatures that influence the electrical conductivity. Apart from the used
field discretization, this approach applies no further simplifications. An
example 3D chip package is used to validate the algorithm."
"A three-dimensional multi-scale computational homogenisation framework is
developed for the prediction of nonlinear micro/meso-mechanical response of the
fibre-reinforced polymer (FRP) composites. Two dominant damage mechanisms, i.e.
matrix elasto-plastic response and fibre-matrix decohesion are considered and
modelled using a non-associative pressure dependent paraboloidal yield
criterion and cohesive interface elements respectively. A linear-elastic
transversely isotropic material model is used to model yarns/fibres within the
representative volume element (RVE). A unified approach is used to impose the
RVE boundary conditions, which allows convenient switching between linear
displacement, uniform traction and periodic boundary conditions. The
computational model is implemented within the framework of the hierarchic
finite element, which permits the use of arbitrary orders of approximation.
Furthermore, the computational framework is designed to take advantage of
distributed memory high-performance computing. The accuracy and performance of
the computational framework are demonstrated with a variety of numerical
examples, including unidirectional FRP composite, a composite comprising a
multi-fibre and multi-layer RVE, with randomly generated fibres, and a single
layered plain weave textile composite. Results are validated against the
reference experimental/numerical results from the literature. The computational
framework is also used to study the effect of matrix and fibre-matrix
interfaces properties on the homogenised stress-strain responses."
"Electrical machines commonly consist of moving and stationary parts. The
field simulation of such devices can be very demanding if the underlying
numerical scheme is solely based on a domain discretization, such as in case of
the Finite Element Method (FEM). Here, a coupling scheme based on FEM together
with Boundary Element Methods (BEM) is presented that neither hinges on
re-meshing techniques nor deals with a special treatment of sliding interfaces.
While the numerics are certainly more involved the reward is obvious: The
modeling costs decrease and the application engineer is provided with an
easy-to-use, versatile, and accurate simulation tool."
"Saturated hydraulic conductivity Ksat is a fundamental characteristic in
modeling flow and contaminant transport in soils and sediments. Therefore, many
models have been developed to estimate Ksat from easily measureable parameters,
such as textural properties, bulk density, etc. However, Ksat is not only
affected by textural and structural characteristics, but also by scale e.g.,
internal diameter and height. Using the UNSODA database and the contrast
pattern aided regression (CPXR) method, we recently developed scale-dependent
pedotransfer functions to estimate Ksat from textural data, bulk density, and
sample dimensions. The main objectives of this study were evaluating the
proposed pedotransfer functions using a larger database, and comparing them
with seven other models. For this purpose, we selected more than nineteen
thousands soil samples from all around the United States. Results showed that
the scale-dependent pedotransfer functions estimated Ksat more accurately than
seven other models frequently used in the literature."
"This paper describes the use of a quadratic manifold for the model order
reduction of structural dynamics problems featuring geometric nonlinearities.
The manifold is tangent to a subspace spanned by the most relevant vibration
modes, and its curvature is provided by modal derivatives obtained by
sensitivity analysis of the eigenvalue problem, or its static approximation,
along the vibration modes. The construction of the quadratic manifold requires
minimal computational effort once the vibration modes are known. The reduced
order model is then obtained by Galerkin projection, where the
configuration-dependent tangent space of the manifold is used to project the
discretized equations of motion."
"In this paper, a generalization of a quadratic manifold approach for the
reduction of geometrically nonlinear structural dynamics problems is presented.
This generalization is constructed by a linearization of the static force with
respect to the generalized coordinates, resulting in a shift of the quadratic
behavior from the force to the manifold. In this framework, static derivatives
emerge as natural extensions to modal derivatives for displacement fields other
than the vibration modes, such as the Krylov subspace vectors. Here the dynamic
problem is projected onto the tangent space of the quadratic manifold, allowing
for a much less number of generalized coordinates compared to linear basis
methods. The potential of the quadratic manifold approach is investigated in a
numerical study, where several variations of the approach are compared on
different examples, indicating a clear pattern where the proposed approach is
applicable."
"Nowadays hydroelectric energy is one of the best energy sources: it is
cleaner, safer and more programmable than other sources. For this reason, its
manage could not be done in an approssimative way, but advance mathematical
models must be use. In this article we consider an overview of the problem: we
introduce the problem, then we show its simplest but quite exaustive
mathematical formulation and in the end we produce numerical results under the
ipothesis that all input are deterministic."
"This paper presents a new numerical procedure for evaluating the vibration
frequencies and mode shapes of masonry buildings in the presence of cracks. The
algorithm has been implemented within the NOSA-ITACA code, which models masonry
as a nonlinear elastic material with zero tensile strength. Some case studies
are reported, and the differences between linear and nonlinear behavior
highlighted."
"We introduce a symplectic dual quaternion variational integrator(DQVI) for
simulating single rigid body motion in all six degrees of freedom. Dual
quaternion is used to represent rigid body kinematics and one-step Lie group
variational integrator is used to conserve the geometric structure, energy and
momentum of the system during the simulation. The combination of these two
becomes the first Lie group variational integrator for rigid body simulation
without decoupling translations and rotations. Newton-Raphson method is used to
solve the recursive dynamic equation. This method is suitable for real-time
rigid body simulations with high precision under large time step. DQVI respects
the symplectic structure of the system with excellent long-term conservation of
geometry structure, momentum and energy. It also allows the reference point and
6-by-6 inertia matrix to be arbitrarily defined, which is very convenient for a
variety of engineering problems."
"Using the traditional surface integral methods, the computation of scattering
from a dielectric object requires two equivalent current densities on the
boundary of the dielectric. In this paper, we present an approach that requires
only a single current density. Our method is based on a surface admittance
operator and is applicable to dielectric bodies of arbitrary shape. The
formulation results in four times lower memory consumption and up to eight
times lower time to solve the linear system than the traditional PMCHWT
formulation. Numerical results demonstrate that the proposed technique is as
accurate as the PMCHWT formulation."
"Unlike nonlocal models, there is no need to introduce an internal length in
the constitutive law for lattice model at the mesoscopic scale. Actually, the
internal length is not explicitly introduced but rather governed by the
mesostructure characteristics themselves. The influence of the mesostructure on
the width of the fracture process zone which is assumed to be correlated to the
characteristic length of the homogenized quasi-brittle material is studied. The
influence of the ligament size (a structural parameter) is also investigated.
This analysis provides recommendations/warnings when extracting an internal
length required for nonlocal damage models from the material mesostructure"
"Transformer terminal equivalents obtained via admittance measurements are
suitable for simulating high-frequency transient interaction between the
transformer and the network. This paper augments the terminal equivalent
approach with a measurement-based voltage transfer function model which permits
calculation of voltages at internal points in the regulating winding. The
approach is demonstrated for a single-phase three-winding transformer in tap
position Nom+ with inclusion of three internal points in the regulating winding
that represent the mid-point and the two extreme ends. The terminal equivalent
modeling makes use of additional common-mode measurements to avoid error
magnifications to result from the ungrounded tertiary winding. The final model
is used in a time domain simulation where ground-fault initiation results in a
resonant voltage build-up in the winding. It is shown that that the peak value
of the resonant overvoltage can be higher than during the lightning impulse
test, with unfavorable network conditions. Additional measurements show that
the selected tap position affects the terminal behavior of the transformer,
changing the frequency and peak value of the lower resonance point in the
voltage transfer between windings."
"We employ Maxwell's equations formulated in Space-Time Algebra to perform
discretization of moving geometries directly in space-time. All the derivations
are carried out without any non-relativistic assumptions, thus the application
area of the scheme is not restricted to low velocities. The 4D mesh
construction is based on a 3D mesh stemming from a conventional 3D mesh
generator. The movement of the system is encoded in the 4D mesh geometry,
enabling an easy extension of well-known 3D approaches to the space-time
setting. As a research example, we study a manifestation of Sagnac's effect in
a rotating ring resonator. In case of constant rotation, the space-time
approach enhances the efficiency of the scheme, as the material matrices are
constant for every time step, without abandoning the relativistic framework."
"This work is related to PHG (Parallel Hierarchical Grid). PHG is a toolbox
for developing parallel adaptive finite element programs, which is under active
development at the State Key Laboratory of Scientific and Engineering
Computing. The main results of this work are as follows.
  1) For the tetrahedral meshes used in PHG, under reasonable assumptions, we
proved the existence of through-vertex Hamiltonian paths between arbitrary two
vertices, as well as the existence of through-vertex Hamiltonian cycles, and
designed an efficient algorithm with linear complexity for constructing
through-vertex Hamiltonian paths. The resulting algorithm has been implemented
in PHG, and is used for ordering elements in the coarsest mesh for the
refinement tree mesh partitioning algorithm.
  2) We designed encoding and decoding algorithms for high dimensional Hilbert
order. Hilbert order has good locality, and it has wide applications in various
fields in computer science, such as memory management, database, and dynamic
load balancing.
  3) We implemented refinement tree and space filling curve based mesh
partitioning algorithms in PHG, and designed the dynamic load balancing module
of PHG. The refinement tree based partitioning algorithm was originally
proposed by Mitchell, the one implemented in PHG was improved in several
aspects. The space filling curve based mesh partitioning function in PHG can
use either Hilbert or Morton space filling curve.
  4) We studied existing hp-adaptive strategies in the literature, and proposed
a new strategy. Numerical experiments show that our new strategy achieves
exponential convergence, and is superior, in both precision of the solutions
and computation time, to the strategy compared. This part of the work also
serves to validate the hp-adaptivity module of PHG."
"The direct geodesic problem on an oblate spheroid is described as an initial
value problem and is solved numerically in geodetic and Cartesian coordinates.
The geodesic equations are formulated by means of the theory of differential
geometry. The initial value problem under consideration is reduced to a system
of first-order ordinary differential equations, which is solved using a
numerical method. The solution provides the coordinates and the azimuths at any
point along the geodesic. The Clairaut constant is not assumed known but it is
computed, allowing to check the precision of the method. An extended data set
of geodesics is used, in order to evaluate the performance of the method in
each coordinate system. The results for the direct geodesic problem are
validated by comparison to Karney's method. We conclude that a complete,
stable, precise, accurate and fast solution of the problem in Cartesian
coordinates is accomplished."
"When comparing measurements to numerical simulations of moisture transfer
through porous materials a rush of the experimental moisture front is commonly
observed in several works shown in the literature, with transient models that
consider only the diffusion process. Thus, to overcome the discrepancies
between the experimental and the numerical models, this paper proposes to
include the moisture advection transfer in the governing equation. To solve the
advection-diffusion differential equation, it is first proposed two efficient
numerical schemes and their efficiencies are investigated for both linear and
nonlinear cases. The first scheme, Scharfetter-Gummel (SG), presents a
Courant-Friedrichs-Lewy (CFL) condition but is more accurate and faster than
the second scheme, the well-known Crank-Nicolson approach. Furthermore, the SG
scheme has the advantages of being well-balanced and asymptotically preserved.
Then, to conclude, results of the convective moisture transfer problem obtained
with the SG numerical scheme are compared to experimental data from the
literature. The inclusion of an advective term in the model may clearly lead to
better results than purely diffusive models."
"A review of the properties that bond the particles under Lennard Jones
Potential allow to states properties and conditions for building evolutive
algorithms using the CB lattice with other different lattices. The new lattice
is called CB lattice and it is based on small cubes. A set of propositions
states convergence and optimal conditions over the CB lattice for an
evolutionary algorithm. The evolutionary algorithm is a reload version of
previous genetic algorithms based in phenotypes. The novelty using CB lattice,
together with the other lattices, and ad-hoc cluster segmentation and
enumeration, is to allow the combination of genotype (DNA coding for cluster
using their particle's number) and phenotype (geometrical shapes using
particle's coordinates in 3D). A parallel version of an evolutionary algorithm
for determining the global optimality is depicted. The results presented are
from a standalone program for a personal computer of the evolutionary
algorithm, which can estimate all putative optimal Lennard Jones Clusters from
13 to 1612 particles. The novelty are the theoretical results for the
evolutionary algorithm's efficiency, the strategies with phenotype or genotype,
and the classification of the clusters based in an ad-hoc geometric algorithm
for segmenting a cluster into its nucleus and layers. Also, the standalone
program is not only capable to replicate the optimal Lennard Jones clusters in
The Cambridge Cluster Database (CCD), but to find new ones."
"Laminated glass elements, which consist of stiff elastic glass layers
connected with a compliant viscoelastic polymer foil, exhibit geometrically
non-linear and time/temperature-sensitive behavior. In computational modeling,
the viscoelastic effects are often neglected or a detailed continuum
formulation typically based on the volumetric-deviatoric elastic-viscoelastic
split is used for the interlayer. Four layerwise beam theories are introduced
in this paper, which differ in the non-linear beam formulation at the layer
level (von K\'{a}rm\'{a}n/Reissner) and in constitutive assumptions for the
interlayer (a viscoelastic solid with the time-independent bulk modulus/Poisson
ratio). We perform detailed verification and validation studies at different
temperatures and compare the accuracy of the selected formulation with
simplified elastic solutions used in practice. We show that all the four
formulations predict very similar responses. Therefore, our suggestion is to
use the most straightforward formulation that combines the von K\'{a}rm\'{a}n
model with the assumption of time-independent Poisson ratio. The simplified
elastic model mostly provides a response in satisfactory agreement with full
viscoelastic solutions. However, it can lead to unsafe or inaccurate
predictions for rapid changes of loading. These findings provide a suitable
basis for extensions towards laminated plates and glass layer fracture, owing
to the modular format of layerwise theories."
"Classical Density Functional Theory (DFT) is a statistical-mechanical
framework to analyze fluids, which accounts for nanoscale fluid inhomogeneities
and non-local intermolecular interactions. DFT can be applied to a wide range
of interfacial phenomena, as well as problems in adsorption, colloidal science
and phase transitions in fluids. Typical DFT equations are highly non-linear,
stiff and contain several convolution terms. We propose a novel, efficient
pseudo-spectral collocation scheme for computing the non-local terms in real
space with the help of a specialized Gauss quadrature. Due to the exponential
accuracy of the quadrature and a convenient choice of collocation points near
interfaces, we can use grids with a significantly lower number of nodes than
most other reported methods. We demonstrate the capabilities of our numerical
methodology by studying equilibrium and dynamic two-dimensional test cases with
single- and multispecies hard-sphere and hard-disc particles modelled with
fundamental measure theory, with and without van der Waals attractive forces,
in bounded and unbounded physical domains. We show that our results satisfy
statistical mechanical sum rules."
"This paper presents our work on developing parallel computational methods for
two-phase flow on modern parallel computers, where techniques for linear
solvers and nonlinear methods are studied and the standard and inexact Newton
methods are investigated. A multi-stage preconditioner for two-phase flow is
applied and advanced matrix processing strategies are studied. A local
reordering method is developed to speed the solution of linear systems.
Numerical experiments show that these computational methods are effective and
scalable, and are capable of computing large-scale reservoir simulation
problems using thousands of CPU cores on parallel computers. The nonlinear
techniques, preconditioner and matrix processing strategies can also be applied
to three-phase black oil, compositional and thermal models."
"In this paper a procedure for the static identification and reconstruction of
concentrated damage distribution in beam-like structures, implemented in a
dedicated software, is presented. The proposed damage identification strategy
relies on the solution of an optimisation problem, by means of a genetic
algorithm, which exploits the closed form solution based on the distribution
theory of multi-cracked beams subjected to static loads. Precisely, the
adoption of the latter closed-form solution allows a straightforward evolution
of an initial random population of chromosomes, representing different damage
distributions along the beam axis, towards the fittest and selected as the
sought solution. This method allows the identification of the position and
intensity of an arbitrary number of cracks and is limited only by the amount of
data experimentally measured. The proposed procedure, which has the great
advantage of being robust and very fast, has been implemented in the powerful
agent based software environment NetLogo, and is here presented and validated
with reference to several benchmark cases of single and multi-cracked beams
considering different load scenarios and boundary conditions. Sensitivity
analyses to assess the influence of instrumental errors are also included in
the study."
"In this paper, several rigorous numerical simulations were conducted to
examine the relevance of mean-field micromechanical models compared to the Fast
Fourier Transform full-field computation by considering spherical or
ellipsoidal inclusions. To be more general, the numerical study was extended to
a mixture of different kind of microstructures consisting of spheroidal shapes
within the same RVE. Although the Fast Fourier Transform full field calculation
is sensitive to high contrasts, calculation time, for a combination of complex
microstructures, remains reasonable compared with those obtained with
mean-field micromechanical models. Moreover, for low volume fractions of
inclusions, the results of the mean-field approximations and those of the Fast
Fourier Transform-based (FFTb) full-field computation are very close, whatever
the inclusions morphology is. For RVEs consisting of ellipsoidal or a mixture
of ellipsoidal and spherical inclusions, when the inclusions volume fraction
becomes higher, one observes that Lielens' model and the FFTb full-field
computation give similar estimates. The accuracy of the computational methods
depends on the shape of the inclusions' and their volume fraction."
"In this paper we propose an anisotropic extension of the isotropic
exponentiated Hencky en- ergy, based on logarithmic strain invariants. Unlike
other elastic formulations, the isotropic exponentiated Hencky elastic energy
has been derived solely on differential geometric grounds, involving the
geodesic distance of the deformation gradient F to the group of rotations. We
formally extend this approach towards anisotropy by defining additional
anisotropic logarith- mic strain invariants with the help of suitable
structural tensors and consider our findings for biomechanical applications."
"This paper presents a new adaptive multiscale homogenization scheme for the
simulation of damage and fracture in concrete structures. A two-scale
homogenization method, coupling meso-scale discrete particle models to macro-
scale finite element models, is formulated into an adaptive framework. A
continuum multiaxial failure criterion for concrete is calibrated on the basis
of fine-scale simulations, and it serves as the adaptive criterion in the
multiscale framework. Thus, in this approach, simulations start without
assigning any material Representative Volume Element (RVE) to the macro-scale
finite elements. The finite elements that meet the adaptive criterion and must
be entered into the multiscale homogenization framework are detected on the
fly. This leads to a substantial reduction of the computational cost especially
for loading conditions leading to damage localization in which only a small
portion of the FE mesh is enriched with the homogenized RVE. Several numerical
simulations are carried out to investigate the capability of the developed
adaptive homogenization method. In addition, a detailed study on the
computational cost is performed."
"This paper proposes an online steady-state voltage stability assessment
scheme to evaluate the proximity to voltage collapse at each bus of a load
area. Using a non-iterative holomorphic embedding method (HEM) with a proposed
physical germ solution, an accurate loading limit at each load bus can be
calculated based on online state estimation on the entire load area and a
measurement-based equivalent for the external system. The HEM employs a power
series to calculate an accurate Power-Voltage (P-V) curve at each load bus and
accordingly evaluates the voltage stability margin considering load variations
in the next period. An adaptive two-stage Pade approximants method is proposed
to improve the convergence of the power series for accurate determination of
the nose point on the P-V curve with moderate computational burden. The
proposed method is illustrated in detail on a 4-bus test system and then
demonstrated on a load area of the Northeast Power Coordinating Council (NPCC)
48-geneartor, 140-bus power system."
"Breathing signal monitoring can provide important clues for human's physical
health problems. Comparing to existing techniques that require wearable devices
and special equipment, a more desirable approach is to provide contact-free and
long-term breathing rate monitoring by exploiting wireless signals. In this
paper, we propose TensorBeat, a system to employ channel state information
(CSI) phase difference data to intelligently estimate breathing rates for
multiple persons with commodity WiFi devices. The main idea is to leverage the
tensor decomposition technique to handle the CSI phase difference data. The
proposed TensorBeat scheme first obtains CSI phase difference data between
pairs of antennas at the WiFi receiver to create CSI tensor data. Then
Canonical Polyadic (CP) decomposition is applied to obtain the desired
breathing signals. A stable signal matching algorithm is developed to find the
decomposed signal pairs, and a peak detection method is applied to estimate the
breathing rates for multiple persons. Our experimental study shows that
TensorBeat can achieve high accuracy under different environments for
multi-person breathing rate monitoring."
"Computations have helped elucidate the dynamics of Earth's mantle for several
decades already. The numerical methods that underlie these simulations have
greatly evolved within this time span, and today include dynamically changing
and adaptively refined meshes, sophisticated and efficient solvers, and
parallelization to large clusters of computers. At the same time, many of these
methods -- discussed in detail in a previous paper in this series -- were
developed and tested primarily using model problems that lack many of the
complexities that are common to the realistic models our community wants to
solve today.
  With several years of experience solving complex and realistic models, we
here revisit some of the algorithm designs of the earlier paper and discuss the
incorporation of more complex physics. In particular, we re-consider time
stepping and mesh refinement algorithms, evaluate approaches to incorporate
compressibility, and discuss dealing with strongly varying material
coefficients, latent heat, and how to track chemical compositions and
heterogeneities. Taken together and implemented in a high-performance,
massively parallel code, the techniques discussed in this paper then allow for
high resolution, 3d, compressible, global mantle convection simulations with
phase transitions, strongly temperature dependent viscosity and realistic
material properties based on mineral physics data."
"Transmission lines are vital components in power systems. Tripping of
transmission lines caused by over-temperature is a major threat to the security
of system operations, so it is necessary to efficiently simulate line
temperature under both normal operation conditions and foreseen fault
conditions. Existing methods based on thermal-steady-state analyses cannot
reflect transient temperature evolution, and thus cannot provide timing
information needed for taking remedial actions. Moreover, conventional
numerical method requires huge computational efforts and barricades system-wide
analysis. In this regard, this paper derives an approximate analytical solution
of transmission-line temperature evolution enabling efficient analysis on
multiple operation states. Considering the uncertainties in environmental
parameters, the region of over-temperature is constructed in the environmental
parameter space to realize the over-temperature risk assessment in both the
planning stage and real-time operations. A test on a typical conductor model
verifies the accuracy of the approximate analytical solution. Based on the
analytical solution and numerical weather prediction (NWP) data, an efficient
simulation method for temperature evolution of transmission systems under
multiple operation states is proposed. As demonstrated on an NPCC 140-bus
system, it achieves over 1000 times of efficiency enhancement, verifying its
potentials in online risk assessment and decision support."
"A finite element method simulation of a carbon fibre reinforced polymer block
is used to analyse the nonlinearities arising from a contacting delamination
gap inside the material. The ultrasonic signal is amplified and nonlinearities
are analysed by delayed Time Reversal -- Nonlinear Elastic Wave Spectroscopy
signal processing method. This signal processing method allows to focus the
wave energy onto the receiving transducer and to modify the focused wave shape,
allowing to use several different methods, including pulse inversion, for
detecting the nonlinear signature of the damage. It is found that the small
crack with contacting acoustic nonlinearity produces a noticeable nonlinear
signature when using pulse inversion signal processing, and even higher
signature with delayed time reversal, without requiring any baseline
information from an undamaged medium."
"The mathematical models used to represent physical phenomena are generally
known to be imperfect representations of reality. Model inadequacies arise for
numerous reasons, such as incomplete knowledge of the phenomena or
computational intractability of more accurate models. In such situations it is
impractical or impossible to improve the model, but necessity requires its use
to make predictions. With this in mind, it is important to represent the
uncertainty that a model's inadequacy causes in its predictions, as neglecting
to do so can cause overconfidence in its accuracy. A powerful approach to
addressing model inadequacy leverages the composite nature of physical models
by enriching a flawed embedded closure model with a stochastic error
representation. This work outlines steps in the development of a stochastic
operator as an inadequacy representation by establishing the framework for
inferring an infinite-dimensional operator and by introducing a novel method
for interrogating available high-fidelity models to learn about modeling error."
"The development of a system that would ease the diagnosis of heart diseases
would also fasten the work of the cardiologic department in hospitals and
facilitate the monitoring of patients with portable devices. This paper
presents a tool for ECG signal analysis which is designed in Matlab. The
Hermite transform domain is exploited for the analysis. The proposed transform
domain is very convenient for ECG signal analysis and classification. Parts of
the ECG signals, i.e. QRS complexes, show shape similarity with the Hermite
basis functions, which is one of the reasons for choosing this domain. Also,
the information about the signal can be represented using a small set of
coefficients in this domain, which makes data transmission and analysis faster.
The signal concentration in the Hermite domain and consequently, the number of
samples required for signal representation, can additionally be reduced by
performing the parametization of the Hermite transform. For the comparison
purpose, the Fourier transform domain is also implemented within the software,
in order to compare the signal concentration in two transform domains."
"Multiscale optimization is an attractive research field recently. For the
most of optimization tools, design parameters should be updated during a close
loop. Therefore, a simple Python code is programmed to obtain effective
properties of Representative Volume Element (RVE) under Periodic Boundary
Conditions (PBCs). It can compute the mechanical properties of a composite with
a periodic structure, in two or three dimensions. The computation method is
based on the Asymptotic Homogenization Theory (AHT). With simple modifications,
the basic Python code may be extended to the computation of the effective
properties of more complex microstructure. Moreover, the code provides a
convenient platform upon the optimization for the material and geometric
composite design. The user may experiment with various algorithms and tackle a
wide range of problems. To verify the effectiveness and reliability of the
code, a three-dimensional case is employed to illuminate the code. Finally
numerical results obtained by the code agree well with the available
theoretical and experimental results"
"This study presents a meshless-based local reanalysis (MLR) method. The
purpose of this study is to extend reanalysis methods to the Kriging
interpolation meshless method due to its high efficiency. In this study, two
reanalysis methods: combined approximations CA) and indirect factorization
updating (IFU) methods are utilized. Considering the computational cost of
meshless methods, the reanalysis method improves the efficiency of the full
meshless method significantly. Compared with finite element method (FEM)-based
reanalysis methods, the main superiority of meshless-based reanalysis method is
to break the limitation of mesh connection. The meshless-based reanalysis is
much easier to obtain the stiffness matrix even for solving the mesh distortion
problems. However, compared with the FEM-based reanalysis method, the critical
challenge is to use much more nodes in the influence domain due to high order
interpolation. Therefore, a local reanalysis method which only needs to
calculate the local stiffness matrix in the influence domain is suggested to
improve the efficiency further. Several typical numerical examples are tested
and the performance of the suggested method is verified."
"A numerical method for particle-laden fluids interacting with a deformable
solid domain and mobile rigid parts is proposed and implemented in a full
engineering system. The fluid domain is modeled with a lattice Boltzmann
representation, the particles and rigid parts are modeled with a discrete
element representation, and the deformable solid domain is modeled using a
Lagrangian mesh. The main issue of this work, since separately each of these
methods is a mature tool, is to develop coupling and model-reduction approaches
in order to efficiently simulate coupled problems of this nature, as occur in
various geological and engineering applications. The lattice Boltzmann method
incorporates a large-eddy simulation technique using the Smagorinsky turbulence
model. The discrete element method incorporates spherical and polyhedral
particles for stiff contact interactions. A neo-Hookean hyperelastic model is
used for the deformable solid. We provide a detailed description of how to
couple the three solvers within a unified algorithm. The technique we propose
for rubber modeling/coupling exploits a simplification that prevents having to
solve a finite-element problem each time step. We also develop a technique to
reduce the domain size of the full system by replacing certain zones with
quasi-analytic solutions, which act as effective boundary conditions for the
lattice Boltzmann method. The major ingredients of the routine are are
separately validated. To demonstrate the coupled method in full, we simulate
slurry flows in two kinds of piston-valve geometries. The dynamics of the valve
and slurry are studied and reported over a large range of input parameters."
"Cosimulation methods allow combination of simulation tools of physical
systems running in parallel to act as a single simulation environment for a big
system. As data is passed across subsystem boundaries instead of solving the
system as one single equation system, it is not ensured that systemwide
balances are fulfilled. If the exchanged data is a flow of a conserved
quantity, approximation errors can accumulate and make simulation results
inaccurate. The problem of approximation errors is typically addressed with
extrapolation of exchanged data. Nevertheless balance errors occur as
extrapolation is approximation. This problem can be handled with balance
correction methods which compensate these errors by adding corrections for the
balances to the signal in next coupling time step. This work aims at combining
extrapolation of exchanged data and balance correction in a way that the
exchanged signal not only remains smooth, meaning the existence of continuous
derivatives, but even in a way reducing the derivatives, in order to avoid
unphysical dynamics caused by the coupling. To this end, suitable switch and
hat functions are constructed and applied to the problem."
"We present a novel complex number formulation along with tight convex
relaxations for the aircraft conflict resolution problem. Our approach combines
both speed and heading control and provides global optimality guarantees
despite non-convexities in the feasible region. As a side result, we present a
new characterization of the conflict separation condition in the form of
disjunctive linear constraints. Our formulation features one binary variable
per pair of aircraft, is free of trigonometric functions, and captures the
non-convexity in a set of quadratic concave constraints. Using our approach, we
are able to close a number of open instances and reduce computational time by
up to two orders of magnitude on standard instances."
"With the increasing rate of power consumption, many new distribution systems
need to be constructed to accommodate connecting the new consumers to the power
grid. On the other hand, the increasing penetration of renewable distributed
generation (DG) resources into the distribution systems and the necessity of
optimally place them in the network can dramatically change the problem of
distribution system planning and design. In this paper, the problem of optimal
distribution system planning including conductor sizing, DG placement,
alongside with placement and sizing of shunt capacitors is studied. A new
Binary-Selective Particle Swarm Optimization (PSO) approach which is capable of
handling all types of continuous, binary and selective variables,
simultaneously, is proposed to solve the optimization problem of distribution
system planning. The objective of the problem is to minimize the system costs.
Load growth rate, cost of energy, cost of power, and inflation rate are all
taken into account. The efficacy of the proposed method is tested on a 26-bus
distribution system."
"This paper proposes a novel method to automatically enforce controls and
limits for Voltage Source Converter (VSC) based multi-terminal HVDC in the
Newton power flow iteration process. A general VSC MT-HVDC model with primary
PQ or PV control and secondary voltage control is formulated. Both the
dependent and independent variables are included in the propose formulation so
that the algebraic variables of the VSC MT-HVDC are adjusted simultaneously.
The proposed method also maintains the number of equations and the dimension of
the Jacobian matrix unchanged so that, when a limit is reached and a control is
released, the Jacobian needs no re-factorization. Simulations on the IEEE
14-bus and Polish 9241-bus systems are performed to demonstrate the
effectiveness of the method."
"In the present study, a general probabilistic design framework is developed
for cyclic fatigue life prediction of metallic hardware using methods that
address uncertainty in experimental data and computational model. The
methodology involves (i) utilization of fatigue tests data conducted on coupons
of Ti6Al4V material (ii) continuum damage mechanics based material constitutive
models to simulate cyclic fatigue behavior of material (iii) variance-based
global sensitivity analysis (iv) Bayesian framework for model calibration and
uncertainty quantification and (v) computational life prediction and
probabilistic design decision making under uncertainty. The outcomes of
computational analyses using the experimental data prove the feasibility of the
probabilistic design methods for model calibration in presence of incomplete
and noisy data. Moreover, using probabilistic design meth- ods result in
assessment of reliability of fatigue life predicted by computational models."
"The immersed boundary (IB) method is a mathematical and numerical framework
for problems of fluid-structure interaction, treating the particular case in
which an elastic structure is immersed in a viscous incompressible fluid. The
IB approach to such problems is to describe the elasticity of the immersed
structure in Lagrangian form, and to describe the momentum, viscosity, and
incompressibility of the coupled fluid-structure system in Eulerian form.
Interaction between Lagrangian and Eulerian variables is mediated by integral
equations with Dirac delta function kernels. The IB method provides a unified
formulation for fluid-structure interaction models involving both thin elastic
boundaries and also thick viscoelastic bodies. In this work, we describe the
application of an adaptive, staggered-grid version of the IB method to the
three-dimensional simulation of the fluid dynamics of the aortic heart valve.
Our model describes the thin leaflets of the aortic valve as immersed elastic
boundaries, and describes the wall of the aortic root as a thick, semi-rigid
elastic structure. A physiological left-ventricular pressure waveform is used
to drive flow through the model valve, and dynamic pressure loading conditions
are provided by a reduced (zero-dimensional) circulation model that has been
fit to clinical data. We use this model and method to simulate aortic valve
dynamics over multiple cardiac cycles. The model is shown to approach rapidly a
periodic steady state in which physiological cardiac output is obtained at
physiological pressures. These realistic flow rates are not specified in the
model, however. Instead, they emerge from the fluid-structure interaction
simulation."
"In this paper, we propose a novel method to estimate and characterize spatial
variations on dies or wafers. This new technique exploits recent developments
in matrix completion, enabling estimation of spatial variation across wafers or
dies with a small number of randomly picked sampling points while still
achieving fairly high accuracy. This new approach can be easily generalized,
including for estimation of mixed spatial and structure or device type
information."
"Although implicit methods require extra calculation, they have been largely
used for obtaining numerical approximations of time-dependent differential
conservation equations in the building science domain, thanks to their
stability conditions that enable the use of larger time steps. Nevertheless,
they require important sub-iterations when dealing with highly nonlinear
problems such as the combined heat and moisture transfer through porous
building elements or when the whole-building is simulated and there is
important coupling among the building elements themselves and among
neighbouring zones and HVAC systems. On the other hand, the classical explicit
Euler scheme is generally not used because its stability condition imposes very
fine time discretisation. Hence, this paper explores the use of an improved
explicit approach - the Dufort-Frankel scheme - to overcome the disadvantage of
the classical explicit one and to bring benefits that cannot be obtained by
implicit methods. The Dufort-Frankel approach is first compared to the
classical implicit and explicit Euler schemes to compute the solution of both
linear and nonlinear heat and moisture transfer through porous materials. Then,
the analysis of the Dufort-Frankel unconditionally stable explicit scheme is
extended to the coupled heat and moisture balances on the scale of a one- and a
two-zone building models. The Dufort-Frankel scheme has the benefits of being
unconditionally stable, second-order accurate in time O(dt^2) and to compute
explicitly the solution at each time step, avoiding costly sub-iterations. This
approach may reduce the computational cost by twenty as well as it may enable
perfect synchronism for whole-building simulation and co-simulation. In
addition, it can be easier parallelised on high-performance computer systems."
"Data missing is an common issue in seismic data, and many methods have been
proposed to solve it. In this paper, we present the low-tubal-rank tensor model
and a novel tensor completion algorithm to recover 3D seismic data. This is a
fast iterative algorithm, called Tubal-Alt-Min which completes our 3D seismic
data by exploiting the low-tubal-rank property expressed as the product of two
much smaller tensors. TubalAlt-Min alternates between estimating those two
tensor using least squares minimization. We evaluate its reconstruction
performance both on synthetic seismic data and land data survey. The
experimental results show that compared with the tensor nuclear norm
minimization algorithm, Tubal-Alt-Min improves the reconstruction error by
orders of magnitude."
"Skew bridges are common in highways and railway lines when non perpendicular
crossings are encountered. The structural effect of skewness is an additional
torsion on the bridge deck which may have a considerable effect, making its
analysis and design more complex. In this paper, an analytical model following
3D beam theory is firstly derived in order to evaluate the dynamic response of
skew bridges under moving loads. Following, a simplified 2D model is also
considered which includes only vertical beam bending. The natural frequencies,
eigenmodes and orthogonality relationships are determined from the boundary
conditions. The dynamic response is determined in time domain by using the
""exact'"" integration. Both models are validated through some numerical examples
by comparing with the results obtained by 3D FE models. A parametric study is
performed with the simplified model in order to identify parameters that
significantly influence the vertical dynamic response of the skew bridge under
traffic loads. The results show that the grade of skewness has an important
influence on the vertical displacement, but hardly on the vertical acceleration
of the bridge. The torsional stiffness really has effect on the vertical
displacement when the skew angle is large. The span length reduces the skewness
effect on the dynamic behavior of the skew bridge."
"Energy disaggregation or Non-Intrusive Load Monitoring (NILM) addresses the
issue of extracting device-level energy consumption information by monitoring
the aggregated signal at one single measurement point without installing meters
on each individual device. Energy disaggregation can be formulated as a source
separation problem where the aggregated signal is expressed as linear
combination of basis vectors in a matrix factorization framework. In this
paper, an approach based on Sum-to-k constrained Non-negative Matrix
Factorization (S2K-NMF) is proposed. By imposing the sum-to-k constraint and
the non-negative constraint, S2K-NMF is able to effectively extract
perceptually meaningful sources from complex mixtures. The strength of the
proposed algorithm is demonstrated through two sets of experiments: Energy
disaggregation in a residential smart home, and HVAC components energy
monitoring in an industrial building testbed maintained at the Oak Ridge
National Laboratory (ORNL). Extensive experimental results demonstrate the
superior performance of S2K-NMF as compared to state-of-the-art
decomposition-based disaggregation algorithms."
"This paper introduces a novel boundary integral approach of shape uncertainty
quantification for the Helmholtz scattering problem in the framework of the
so-called parametric method. The key idea is to form a low-dimensional spatial
embedding within the family of uncertain boundary deformations via the Coarea
formula. The embedding, essentially, encompasses any irregular behavior of the
boundary deformations and facilitates a low-dimensional integration rule
capturing the bulk variation of output functionals defined on the boundary. In
a second phase a matching parametric grid is imposed. For the ease of
presentation the theory is restricted to 2D star-shaped obstacles in
low-dimensional setting. We employ the null-field reconstruction technique
which is capable of handling large shape deformations. Higher spatial and
parametric dimensional cases are discussed, though, not extensively explored in
the current study."
"In the paper we discuss the main features of the software package for
numerical simulations of the surface water dynamics. We consider an
approximation of the shallow water equations together with the parallel
technologies for NVIDIA CUDA graphics processors. The numerical hydrodynamic
code is based on the combined Lagrangian-Euler method~(CSPH-TVD). We focused on
the features of the parallel implementation of Tesla line of graphics
processors: C2070, K20, K40, K80. By using hierarchical grid systems at
different spatial scales we increase the efficiency of the computing resources
usage and speed up our simulations of a various flooding problems."
"We present an automated methodology to optimize laminated composite
structures. Our approach, inspired by bi-level optimization scheme, avoids its
prime deficiencies and is based on developed computationally inexpensive
stacking sequences reconstruction algorithm, which identically satisfies
conventional set of blending rules. We combine it with proposed tight
approximation to feasible domain of composite integral parameters and
hierarchical zoning procedure to get highly efficient optimization methodology,
which we test in two example applications. In both cases it is shown that
blending rules compliant optimal solution remains within 10% gap from one with
no rules applied."
"Weather and environmental factors are verified to have played significant
roles in historical major cascading outages and blackouts. Therefore, in the
simulation and risk assessment of cascading outages in power systems, it is
necessary to consider the weather and environmental effects. This paper
proposes a method for the risk assessment of weather-related cascading outages.
Based on the analysis of historical outage records and temperature-dependent
physical outage mechanisms of transmission lines, an outage rate model
considering weather condition and conductor temperature is proposed, and the
analytical form of outage probability of lines are derived. With the
weather-dependent outage model, a two-stage risk assessment method based on
Markovian tree (MT) search is proposed, which consists of offline full
assessment, and online efficient update of risk assessment results and
continued MT search using updated NWP data. The test cases on NPCC 140-bus test
system model in winter and summer scenarios verify the advantages of the
proposed risk assessment method in both accuracy and efficiency."
"In this paper the effect of posibilistic or mixed background risk on the
level of optimal prevention is studied. In the framework of five purely
possibilistic or mixed models, necessary and sufficient conditions are found
such that the level of optimal saving decreases or increases as a result of the
actions of various types of background risk. This way our results complete
those obtained by Courbage and Rey for some prevention models with
probabilistic background risk."
"The multilevel Monte Carlo method is applied to an academic example in the
field of electromagnetism. The method exhibits a reduced variance by assigning
the samples to multiple models with a varying spatial resolution. For the given
example it is found that the main costs of the method are spent on the coarsest
level."
"This paper introduces a new and novel radar interferometry based on Doppler
synthetic aperture radar (Doppler-SAR) paradigm. Conventional SAR
interferometry relies on wideband transmitted waveforms to obtain high range
resolution. Topography of a surface is directly related to the range difference
between two antennas configured at different positions. Doppler-SAR is a novel
imaging modality that uses ultra-narrowband continuous waves (UNCW). It takes
advantage of high resolution Doppler information provided by UNCWs to form high
resolution SAR images.
  We introduced the theory of Doppler-SAR interferometry. We derived
interferometric phase model and develop the equations of height mapping. Unlike
conventional SAR interferometry, we show that the topography of a scene is
related to the difference in Doppler between two antennas configured at
different velocities. While the conventional SAR interferometry uses range,
Doppler and Doppler due to interferometric phase in height mapping, Doppler-SAR
interferometry uses Doppler, Doppler-rate and Doppler-rate due to
interferometric phase in height mapping. We demonstrate our theory in numerical
simulations.
  Doppler-SAR interferometry offers the advantages of long-range, robust,
environmentally friendly operations; low-power, low-cost, lightweight systems
suitable for low-payload platforms, such as micro-satellites; and passive
applications using sources of opportunity transmitting UNCW."
"Optimization of the refrigerant circuitry can improve a heat exchanger's
performance. Design engineers currently choose the refrigerant circuitry
according to their experience and heat exchanger simulations. However, the
design of an optimized refrigerant circuitry is difficult. The number of
refrigerant circuitry candidates is enormous. Therefore, exhaustive search
algorithms cannot be used and intelligent techniques must be developed to
explore the solution space efficiently. In this paper, we formulate refrigerant
circuitry design as a binary constrained optimization problem. We use
CoilDesigner, a simulation and design tool of air to refrigerant heat
exchangers, in order to simulate the performance of different refrigerant
circuitry designs. We treat CoilDesigner as a black-box system since the exact
relationship of the objective function with the decision variables is not
explicit. Derivative-free optimization (DFO) algorithms are suitable for
solving this black-box model since they do not require explicit functional
representations of the objective function and the constraints. The aim of this
paper is twofold. First, we compare four mixed-integer constrained DFO solvers
and one box-bounded DFO solver and evaluate their ability to solve a difficult
industrially relevant problem. Second, we demonstrate that the proposed
formulation is suitable for optimizing the circuitry configuration of heat
exchangers. We apply the DFO solvers to 17 heat exchanger design problems.
Results show that TOMLAB/glcDirect and TOMLAB/glcSolve can find optimal or
near-optimal refrigerant circuitry designs after a relatively small number of
circuit simulations."
"Spatial discretisation of geometrically complex computational domains often
entails unstructured meshes of general topology for Computational Fluid
Dynamics (CFD). Mesh skewness is then typically encountered causing severe
deterioration of the formal order of accuracy of the discretisation, or
boundedness of the solution, or both. Particularly methods inherently relying
on the accurate and bounded transport of sharp fields suffer from all types of
mesh-induced skewness errors, namely both non-orthogonality and
non-conjunctionality errors. This work is devoted to a boundedness-preserving
strategy to correct for skewness errors arising from discretisation of
advection and diffusion terms within the context of interfacial heat and mass
transfer based on the Volume-of-Fluid methodology. The implementation has been
accomplished using a second-order finite volume method with support for
unstructured meshes of general topology. We examine and advance suitable
corrections for the finite volume discretisation of a consistent single-field
model, where both accurate and bounded transport due to diffusion and advection
is crucial. In order to ensure consistency of both the volume fraction and the
species concentration transport, i.e. to avoid artificial heat or species
transfer, corrections are studied for both cases. The cross interfacial jump
and adjacent sharp gradients of species concentration render the correction for
skewness-induced diffusion and advection errors additionally demanding and has
not so far been addressed in the literature."
"Formulating a consistent theory for rigid-body dynamics with impacts is an
intricate problem. Twenty years ago Stewart published the first consistent
theory with purely inelastic impacts and an impulsive friction model analogous
to Coulomb friction. In this paper we demonstrate that the consistent impact
model can exhibit multiple solutions with a varying degree of dissipation even
in the single-contact case. Replacing the impulsive friction model based on
Coulomb friction by a model based on the maximum dissipation principle resolves
the non-uniqueness in the single-contact impact problem. The paper constructs
the alternative impact model and presents integral equations describing
rigid-body dynamics with a non-impulsive and non-compliant contact model and an
associated purely inelastic impact model maximizing dissipation. An analytic
solution is derived for the single-contact impact problem. The models are then
embedded into a time-stepping scheme. The macroscopic behaviour is compared to
Coulomb friction in a large-scale granular flow problem."
"In a previous paper (Johnsen et al., 2015) and presentation (Johnsen et al.,
2016), we developed and demonstrated a generic modelling framework for the
modelling of direct precipitation fouling from multi-component fluid mixtures
that become super-saturated at the wall. The modelling concept involves the
1-dimensional transport of the fluid species through the turbulent boundary
layer close to the wall. The governing equations include the Reynolds-averaged
(RANS) advection-diffusion equations for each fluid species, and the axial
momentum and energy equations for the fluid mixture. The driving force for the
diffusive transport is the local gradient in the species' chemical potential.
Adsorption mechanisms are not modelled per se, but the time-scale of adsorption
is reflected in the choice of Dirichlet boundary conditions for the depositing
species, at the fluid-solid interface.
  In this paper, the modelling framework is implemented as a user-defined
function (UDF) for the CFD software ANSYS Fluent, to act as a wall boundary
condition for mass-transfer to the wall. The subgrid, 1-dimensional formulation
of the model reduces the computational cost associated with resolving the fine
length-scales at which the boundary-layer mass transfer is determined, and
allows for efficient modelling of industry-scale heat exchangers suffering from
fouling.
  The current paper describes the modelling framework, and demonstrates and
validates its applicability in a simplified 2D heat exchanger geometry
(experimental and detailed CFD modelling data by P\""a\""akk\""onen et al. (2012,
2016)). By tuning the diffusivity, only, good agreement with the experimental
data and the detailed CFD model was obtained, in terms of area-averaged
deposition rates."
"This research concerns a type of configuration optimization problems
frequently encountered in engineering design and manufacturing, where the
envelope volume in space occupied by a number of components needs to be
minimized along with other objectives such as minimizing connective lines
between the components under various constraints. Since in practical
applications the objectives and constraints are usually complex, the
formulation of computationally tractable optimization becomes difficult.
Moreover, unlike conventional multi-objective optimization problems, such
configuration problems usually comes with a number of demanding constraints
that are hard to satisfy, which results in the critical challenge of balancing
solution feasibility with optimality. In this research, we first present the
mathematical formulation for a representative problem of configuration
optimization with multiple hard constraints, and then develop two versions of
an enhanced multi-objective simulated annealing approach, referred to as
MOSA/R, to solve this problem. To facilitate the optimization computationally,
in MOSA/R, a versatile re-seed scheme that allows biased search while avoiding
pre-mature convergence is designed. Our case study indicates that the new
algorithm yields significantly improved performance towards both constrained
benchmark tests and constrained configuration optimization problem. The
configuration optimization framework developed can benefit both existing
design/manufacturing practices and future additive manufacturing."
"In this paper we present a novel two-scale framework to optimize the
structure and the material distribution of an object given its functional
specifications. Our approach utilizes multi-material microstructures as
low-level building blocks of the object. We start by precomputing the material
property gamut -- the set of bulk material properties that can be achieved with
all material microstructures of a given size. We represent the boundary of this
material property gamut using a level set field. Next, we propose an efficient
and general topology optimization algorithm that simultaneously computes an
optimal object topology and spatially-varying material properties constrained
by the precomputed gamut. Finally, we map the optimal spatially-varying
material properties onto the microstructures with the corresponding properties
in order to generate a high-resolution printable structure. We demonstrate the
efficacy of our framework by designing, optimizing, and fabricating objects in
different material property spaces on the level of a trillion voxels, i.e
several orders of magnitude higher than what can be achieved with current
systems."
"In this paper, we investigate the damage detection of structures seen as an
optimization problem, using modal characterization to evaluate the dynamic
response of the structure given a damage model. We implemented the firefly
optimization algorithm with a simple numerical damage model to assess the
performance of the method and its advantages for structural health monitoring
(SHM). We show some implementation details and discuss the obtained results."
"Quasi-brittle materials such as concrete suffer from cracks during their life
cycle, requiring great cost for conventional maintenance or replacement. In the
last decades, self-healing materials are developed which are capable of filling
and healing the cracks and regaining part of the stiffness and strength
automatically after getting damaged, bringing the possibility of
maintenance-free materials and structures.
  In this paper, a time dependent softening-healing law for self-healing
quasi-brittle materials is presented by introducing limited material parameters
with clear physical background. Strong Discontinuity embedded Approach (SDA) is
adopted for evaluating the reliability of the model. In the numerical studies,
values of healing parameters are firstly obtained by back analysis of
experimental results of self-healing beams. Then numerical models regarding
concrete members and structures built with self-healing and non-healing
materials are simulated and compared for showing the capability of the
self-healing material."
"Modeling of dynamic processes in nuclear reactors is carried out, mainly, on
the basis of the multigroup diffusion approximation for the neutron flux. The
basic model includes a multidimensional set of coupled parabolic equations and
ordinary differential equations. Dynamic processes are modeled by a successive
change of the reactor states, which are characterized by given coefficients of
the equations. In the modal method, the approximate solution is represented as
an expansion on the first eigenfunctions of some spectral problem. The
numerical-analytical method is based on the use of dominant time-eigenvalues of
a multigroup diffusion model taking into account delayed neutrons. Numerical
simulations of the dynamic process were performed in the framework of the
two-group approximation for the VVER-1000 reactor test model. The last is
characterized by the fact that some eigenvalues are complex."
"This paper introduces capital flow to the single item stochastic lot sizing
problem. A retailer can leverage business overdraft to deal with unexpected
capital shortage, but needs to pay interest if its available balance goes below
zero. A stochastic dynamic programming model maximizing expected final capital
increment is formulated to solve the problem to optimality. We then investigate
the performance of four controlling policies: ($R, Q$), ($R, S$), ($s, S$) and
($s$, $\overline{Q}$, $S$); for these policies, we adopt simulation-genetic
algorithm to obtain approximate values of the controlling parameters. Finally,
a simulation-optimization heuristic is also employed to solve this problem.
Computational comparisons among these approaches show that policy $(s, S)$ and
policy $(s, \overline{Q}, S)$ provide performance close to that of optimal
solutions obtained by stochastic dynamic programming, while
simulation-optimization heuristic offers advantages in terms of computational
efficiency. Our numerical tests also show that capital availability as well as
business overdraft interest rate can substantially affect the retailer's
optimal lot sizing decisions."
"This paper presents a finite-strain hyperviscoplastic constitutive model
within a thermodynamically consistent framework for peat which was categorised
as a material with both rate-dependent and thermodynamic equilibrium hysteresis
based on the data reported in the literature. The model was implemented
numerically using implicit time integration and verified against analytical
solutions under simplified conditions. Experimental studies on the undrained
relaxation and loading-unloading-reloading behaviour of an undisturbed fibrous
peat were carried out to define the thermodynamic equilibrium state during
deviatoric loading as a prerequisite for further modelling, to fit particularly
those model parameters related to solid matrix properties, and to validate the
proposed model under undrained conditions. This validation performed by
comparison to experimental results showed that the hyperviscoplastic model
could simulate undrained triaxial compression tests carried out at five
different strain rates with loading/unloading relaxation steps."
"Summary This work presents variational concepts associated with reduced
Trefftz type approaches and discusses the interrelationship between various
concepts of the displacement, hybrid and Trefftz methods. The basic concept of
the displacement version of the reduced Trefftz method operates on the natural
boundary conditions enforced in an integral form whereas the stress version of
the reduced Trefftz type approach operates on the essential boundary conditions
enforced in an integral sense. The application of the method proposed in the
framework of the finite element method is briefly outlined. The methods used by
the reduced Trefftz type approach for enforcing conformity and interelement
continuity between neighboured elements are also discussed. Comparisons with
other known methods for the same purpose are performed. General strategy for
developing finite elements of general geometric form such as quadrilateral
elements with invariance properties is presented. The basic idea of this
strategy consists in using the natural coordinate system only for defining the
element geometry and performing the element integration in the biunit interval.
For defining the approximation functions a local coordinate system defined from
the directions of the covariant base vectors and the perpendicular
contravariant base vectors computed in the geometric centre of the element is
used. This strategy can also be used to implement other versions of finite
elements and other forms of finite elements. Different numerical calculations
and comparisons in the linear statics and kinetics are performed in order to
assess the convergence and the numerical performance of finite elements
developed by applying the reduced Trefftz type approach."
"We study the exploitation of polarimetric diversity in passive multistatic
radar for detecting moving targets. We first derive a data model that takes
into account polarization and anisotropy of targets inherent in multistatic
configurations. Unlike conventional isotropic models in which targets are
modeled as a collection of uniform spheres, we model targets as a collection of
dipole antennas with unknown directions. We consider a multistatic
configuration in which each receiver is equipped with a pair of orthogonally
polarized antennas, one directed to a scene of interest collecting target-path
signal and another one having a direct line-of-sight to a
transmitter-of-opportunity collecting direct-path signal. We formulate the
detection of moving target problem in a generalized likelihood ratio test
framework under the assumption that direct-path signal is available. We show
that the result can be reduced to the case in which the direct-path signal is
absent. We present a method for estimating the dipole moments of targets.
Extensive numerical simulations show the performance of both the detection and
the dipole estimation tasks with and without polarimetric diversity."
"Prediction of new drug-target interactions is extremely important as it can
lead the researchers to find new uses for old drugs and to realize the
therapeutic profiles or side effects thereof. However, experimental prediction
of drug-target interactions is expensive and time-consuming. As a result,
computational methods for prediction of new drug-target interactions have
gained much interest in recent times. We present iDTI-ESBoost, a prediction
model for identification of drug-target interactions using evolutionary and
structural features. Our proposed method uses a novel balancing technique and a
boosting technique for the binary classification problem of drug-target
interaction. On four benchmark datasets taken from a gold standard data,
iDTI-ESBoost outperforms the state-of-the-art methods in terms of area under
Receiver operating characteristic (auROC) curve. iDTI-ESBoost also outperforms
the latest and the best-performing method in the literature to-date in terms of
area under precision recall (auPR) curve. This is significant as auPR curves
are argued to be more appropriate as a metric for comparison for imbalanced
datasets, like the one studied in this research. In the sequel, our experiments
establish the effectiveness of the classifier, balancing methods and the novel
features incorporated in iDTI-ESBoost. iDTI-ESBoost is a novel prediction
method that has for the first time exploited the structural features along with
the evolutionary features to predict drug-protein interactions. We believe the
excellent performance of iDTI-ESBoost both in terms of auROC and auPR would
motivate the researchers and practitioners to use it to predict drug-target
interactions. To facilitate that, iDTI-ESBoost is readily available for use at:
http://farshidrayhan.pythonanywhere.com/iDTI-ESBoost/"
"We design, analyse and implement an arbitrary order scheme applicable to
generic meshes for a coupled elliptic-parabolic PDE system describing miscible
displacement in porous media. The discretisation is based on several
adaptations of the Hybrid-High-Order (HHO) method due to Di Pietro et al.
[Computational Methods in Applied Mathematics, 14(4), (2014)]. The equation
governing the pressure is discretised using an adaptation of the HHO method for
variable diffusion, while the discrete concentration equation is based on the
HHO method for advection-diffusion-reaction problems combined with numerically
stable flux reconstructions for the advective velocity that we have derived
using the results of Cockburn et al. [ESAIM: Mathematical Modelling and
Numerical Analysis, 50(3), (2016)]. We perform some rigorous analysis of the
method to demonstrate its $L^2$ stability under the irregular data often
presented by reservoir engineering problems and present several numerical tests
to demonstrate the quality of the results that are produced by the proposed
scheme."
"Computational modeling of the structural behavior of continuous fiber
composite materials often takes into account the periodicity of the underlying
micro-structure. A well established method dealing with the structural behavior
of periodic micro-structures is the so- called Asymptotic Expansion
Homogenization (AEH). By considering a periodic perturbation of the material
displacement, scale bridging functions, also referred to as elastic correctors,
can be derived in order to connect the strains at the level of the
macro-structure with micro- structural strains. For complicated inhomogeneous
micro-structures, the derivation of such functions is usually performed by the
numerical solution of a PDE problem - typically with the Finite Element Method.
Moreover, when dealing with uncertain micro-structural geometry and material
parameters, there is considerable uncertainty introduced in the actual stresses
experienced by the materials. Due to the high computational cost of computing
the elastic correctors, the choice of a pure Monte-Carlo approach for dealing
with the inevitable material and geometric uncertainties is clearly
computationally intractable. This problem is even more pronounced when the
effect of damage in the micro-scale is considered, where re-evaluation of the
micro-structural representative volume element is necessary for every occurring
damage. The novelty in this paper is that a non-intrusive surrogate modeling
approach is employed with the purpose of directly bridging the macro-scale
behavior of the structure with the material behavior in the micro-scale,
therefore reducing the number of costly evaluations of corrector functions,
allowing for future developments on the incorporation of fatigue or static
damage in the analysis of composite structural components."
"When mobiTopp was initially designed, more than 10 years ago, it has been the
first travel demand simulation model intended for an analysis period of one
week. However, the first version supported only an analysis period of one day.
This paper describes the lessons learned while extending the simulation period
from one day to one week. One important issue is ensuring realistic start times
of activities. Due to differences between the realized trip durations during
the simulation and the trip durations assumed when creating the activity
schedule, the realized activity schedule and the planned activity schedule may
deviate from each other at some point in time during simulation. A suitable
rescheduling strategy is needed to prevent this. Another issue is the different
behavior at weekends, when more joint activities take place than on weekdays,
resulting in an increased share of trips made using the mode car as passenger.
If a mode choice model that takes availability of ride-sharing opportunities
into account is used, it can be difficult to reproduce the correct modal split
without modeling explicitly these joint activities. When modeling travel demand
for a week, it is also important to account for infrequent long-distance trips.
While the share of these trips is low, the total number is not negligible. It
seems that these long-distance trips are not well covered by the destination
choice model used for the day-to-day trips, indicating the need for a
long-distance trip model of infrequent events."
"In this article we introduce the principles to detect leakage using a
mathematical model based on machine learning and domestic water consumption
monitoring in real time. The model uses data which is measured from a water
meter, analyzes the water consumption, and uses two criteria simultaneously:
deviation from the average consumption, and comparison of steady water
consumptions over a period of time. Simulation of the model on a regular
household consumer was implemented on Antileaks - device that we have built
that designed to transfer consumption information from an analogue water meter
to a digital form in real time."
"-Molecular simulations allow the study of properties and interactions of
molecular systems. This article presents an improved version of the Adaptive
Resolution Scheme that links two systems having atomistic (also called
fine-grained) and coarse-graine"
"Genomic sequence alignment is an important research topic in bioinformatics
and continues to attract significant efforts. As genomic data grow
exponentially, however, most of alignment methods face challenges due to their
huge computational costs. HMMER, a suite of bioinformatics tools, is widely
used for the analysis of homologous protein and nucleotide sequences with high
sensitivity, based on profile hidden Markov models (HMMs). Its latest version,
HMMER3, introdues a heuristic pipeline to accelerate the alignment process,
which is carried out on central processing units (CPUs) with the support of
streaming SIMD extensions (SSE) instructions. Few acceleration results have
since been reported based on HMMER3. In this paper, we propose a five-tiered
parallel framework, CUDAMPF++, to accelerate the most computationally intensive
stages of HMMER3's pipeline, multiple/single segment Viterbi (MSV/SSV), on a
single graphics processing unit (GPU). As an architecture-aware design, the
proposed framework aims to fully utilize hardware resources via exploiting
finer-grained parallelism (multi-sequence alignment) compared with its
predecessor (CUDAMPF). In addition, we propose a novel method that proactively
sacrifices L1 Cache Hit Ratio (CHR) to get improved performance and scalability
in return. A comprehensive evaluation shows that the proposed framework
outperfroms all existig work and exhibits good consistency in performance
regardless of the variation of query models or protein sequence datasets. For
MSV (SSV) kernels, the peak performance of the CUDAMPF++ is 283.9 (471.7) GCUPS
on a single K40 GPU, and impressive speedups ranging from 1.x (1.7x) to 168.3x
(160.7x) are achieved over the CPU-based implementation (16 cores, 32 threads)."
"The general scheme of two-level parallelization (TLP) for direct simulation
Monte Carlo of unsteady gas flows on shared memory multiprocessor computers has
been described. The high efficient algorithm of parallel independent runs is
used on the first level. The data parallelization is employed for the second
one. Two versions of TLP algorithm are elaborated with static and dynamic load
balancing. The method of dynamic processor reallocation is used for dynamic
load balancing. Two gasdynamic unsteady problems were used to study speedup and
efficiency of the algorithms. The conditions of efficient application field for
the algorithms have been determined."
"The structure of weighting coefficient matrices of Harmonic Differential
Quadrature (HDQ) is found to be either centrosymmetric or skew centrosymmetric
depending on the order of the corresponding derivatives. The properties of both
matrices are briefly discussed in this paper. It is noted that the
computational effort of the harmonic quadrature for some problems can be
further reduced up to 75 per cent by using the properties of the
above-mentioned matrices."
"By using the Hadamard matrix product concept, this paper introduces two
generalized matrix formulation forms of numerical analogue of nonlinear
differential operators. The SJT matrix-vector product approach is found to be a
simple, efficient and accurate technique in the calculation of the Jacobian
matrix of the nonlinear discretization by finite difference, finite volume,
collocation, dual reciprocity BEM or radial functions based numerical methods.
We also present and prove simple underlying relationship (theorem (3.1))
between general nonlinear analogue polynomials and their corresponding Jacobian
matrices, which forms the basis of this paper. By means of theorem 3.1,
stability analysis of numerical solutions of nonlinear initial value problems
can be easily handled based on the well-known results for linear problems.
Theorem 3.1 also leads naturally to the straightforward extension of various
linear iterative algorithms such as the SOR, Gauss-Seidel and Jacobi methods to
nonlinear algebraic equations. Since an exact alternative of the quasi-Newton
equation is established via theorem 3.1, we derive a modified BFGS quasi-Newton
method. A simple formula is also given to examine the deviation between the
approximate and exact Jacobian matrices. Furthermore, in order to avoid the
evaluation of the Jacobian matrix and its inverse, the pseudo-Jacobian matrix
is introduced with a general applicability of any nonlinear systems of
equations. It should be pointed out that a large class of real-world nonlinear
problems can be modeled or numerically discretized polynomial-only algebraic
system of equations. The results presented here are in general applicable for
all these problems. This paper can be considered as a starting point in the
research of nonlinear computation and analysis from an innovative viewpoint."
"This paper points out that the differential quadrature (DQ) and differential
cubature (DC) methods due to their global domain property are more efficient
for nonlinear problems than the traditional numerical techniques such as finite
element and finite difference methods. By introducing the Hadamard product of
matrices, we obtain an explicit matrix formulation for the DQ and DC solutions
of nonlinear differential and integro-differential equations. Due to its
simplicity and flexibility, the present Hadamard product approach makes the DQ
and DC methods much easier to be used. Many studies on the Hadamard product can
be fully exploited for the DQ and DC nonlinear computations. Furthermore, we
first present SJT product of matrix and vector to compute accurately and
efficiently the Frechet derivative matrix in the Newton-Raphson method for the
solution of the nonlinear formulations. We also propose a simple approach to
simplify the DQ or DC formulations for some nonlinear differential operators
and thus the computational efficiency of these methods is improved
significantly. We give the matrix multiplication formulas to compute
efficiently the weighting coefficient matrices of the DC method. The spherical
harmonics are suggested as the test functions in the DC method to handle the
nonlinear differential equations occurring in global and hemispheric weather
forecasting problems. Some examples are analyzed to demonstrate the simplicity
and efficiency of the presented techniques. It is emphasized that innovations
presented are applicable to the nonlinear computations of the other numerical
methods as well."
"A novel nonlinear formulation of the finite element and Galerkin methods is
presented here, which leads to the Hadamard product expression of the resultant
nonlinear algebraic analogue. The presented formulation attains the advantages
of weak formulation in the standard finite element and Galerkin schemes and
avoids the costly repeated numerical integration of the Jacobian matrix via the
recently developed SJT product approach. This also provides possibility of the
nonlinear decoupling computations."
"Software for the resolution of certain kind of problems, those that rate high
in the Stringent Performance Objectives adjustment factor (IFPUG scheme), can
be described using a combination of game theory and autonomous systems. From
this description it can be shown that some of those problems exhibit chaotic
behavior, an important fact in understanding the functioning of the related
software. As a relatively simple example, it is shown that chess exhibits
chaotic behavior in its configuration space. This implies that static
evaluators in chess programs have intrinsic limitations."
"The present author recently proposed and proved a relationship theorem
between nonlinear polynomial equations and the corresponding Jacobian matrix.
By using this theorem, this paper derives a Newton iterative formula without
requiring the evaluation of nonlinear function values in the solution of
nonlinear polynomial-only problems."
"The Hadamard and SJT product of matrices are two types of special matrix
product. The latter was first defined by Chen. In this study, they are applied
to the differential quadrature (DQ) solution of geometrically nonlinear bending
of isotropic and orthotropic rectangular plates. By using the Hadamard product,
the nonlinear formulations are greatly simplified, while the SJT product
approach minimizes the effort to evaluate the Jacobian derivative matrix in the
Newton-Raphson method for solving the resultant nonlinear formulations. In
addition, the coupled nonlinear formulations for the present problems can
easily be decoupled by means of the Hadamard and SJT product. Therefore, the
size of the simultaneous nonlinear algebraic equations is reduced by two-thirds
and the computing effort and storage requirements are alleviated greatly. Two
recent approaches applying the multiple boundary conditions are employed in the
present DQ nonlinear computations. The solution accuracies are improved
obviously in comparison to the previously given by Bert et al. The numerical
results and detailed solution procedures are provided to demonstrate the superb
efficiency, accuracy and simplicity of the new approaches in applying DQ method
for nonlinear computations."
"The principal innovative idea in this paper is to transform the original
complex nonlinear modeling problem into a combination of linear problem and
very simple nonlinear problems. The key step is the generalized linearization
of nonlinear terms. This paper only presents the introductory strategy of this
methodology. The practical numerical experiments will be provided subsequently."
"Representing scientific data sets efficiently on external storage usually
involves converting them to a byte string representation using specialized
reader/writer routines. The resulting storage files are frequently difficult to
interpret without these specialized routines as they do not contain information
about the logical structure of the data. Avoiding such problems usually
involves heavy-weight data format libraries or data base systems. We present a
simple C++ library that allows to create and access data files that store
structured data. The structure of the data is described by a data type that can
be built from elementary data types (integer and floating-point numbers, byte
strings) and composite data types (arrays, structures, unions). An abstract
data access class presents the data to the application. Different actual data
file structures can be implemented under this layer. This method is
particularly suited to applications that require complex data structures, e.g.
molecular dynamics simulations. Extensions such as late type binding and object
persistence are discussed."
"The problem of finding a center string that is `close' to every given string
arises and has many applications in computational biology and coding theory.
This problem has two versions: the Closest String problem and the Closest
Substring problem. Assume that we are given a set of strings ${\cal S}=\{s_1,
s_2, ..., s_n\}$ of strings, say, each of length $m$. The Closest String
problem asks for the smallest $d$ and a string $s$ of length $m$ which is
within Hamming distance $d$ to each $s_i\in {\cal S}$. This problem comes from
coding theory when we are looking for a code not too far away from a given set
of codes. The problem is NP-hard. Berman et al give a polynomial time algorithm
for constant $d$. For super-logarithmic $d$, Ben-Dor et al give an efficient
approximation algorithm using linear program relaxation technique. The best
polynomial time approximation has ratio 4/3 for all $d$ given by Lanctot et al
and Gasieniec et al. The Closest Substring problem looks for a string $t$ which
is within Hamming distance $d$ away from a substring of each $s_i$. This
problem only has a $2- \frac{2}{2|\Sigma|+1}$ approximation algorithm
previously Lanctot et al and is much more elusive than the Closest String
problem, but it has many applications in finding conserved regions, genetic
drug target identification, and genetic probes in molecular biology. Whether
there are efficient approximation algorithms for both problems are major open
questions in this area. We present two polynomial time approxmation algorithms
with approximation ratio $1+ \epsilon$ for any small $\epsilon$ to settle both
questions."
"This paper describes two approaches for fault detection: an immune-based
mechanism and a formal language algorithm. The first one is based on the
feature of immune systems in distinguish any foreign cell from the body own
cell. The formal language approach assumes the system as a linguistic source
capable of generating a certain language, characterised by a grammar. Each
algorithm has particular characteristics, which are analysed in the paper,
namely in what cases they can be used with advantage. To test their
practicality, both approaches were applied on the problem of fault detection in
an induction motor."
"This paper initiates a study into the century-old issue of market
predictability from the perspective of computational complexity. We develop a
simple agent-based model for a stock market where the agents are traders
equipped with simple trading strategies, and their trades together determine
the stock prices. Computer simulations show that a basic case of this model is
already capable of generating price graphs which are visually similar to the
recent price movements of high tech stocks. In the general model, we prove that
if there are a large number of traders but they employ a relatively small
number of strategies, then there is a polynomial-time algorithm for predicting
future price movements with high accuracy. On the other hand, if the number of
strategies is large, market prediction becomes complete in two new
computational complexity classes CPP and BCPP, which are between P^NP[O(log n)]
and PP. These computational completeness results open up a novel possibility
that the price graph of an actual stock could be sufficiently deterministic for
various prediction goals but appear random to all polynomial-time prediction
algorithms."
"Two general algorithms based on opportunity costs are given for approximating
a revenue-maximizing set of bids an auctioneer should accept, in a
combinatorial auction in which each bidder offers a price for some subset of
the available goods and the auctioneer can only accept non-intersecting bids.
Since this problem is difficult even to approximate in general, the algorithms
are most useful when the bids are restricted to be connected node subsets of an
underlying object graph that represents which objects are relevant to each
other. The approximation ratios of the algorithms depend on structural
properties of this graph and are small constants for many interesting families
of object graphs. The running times of the algorithms are linear in the size of
the bid graph, which describes the conflicts between bids. Extensions of the
algorithms allow for efficient processing of additional constraints, such as
budget constraints that associate bids with particular bidders and limit how
many bids from a particular bidder can be accepted."
"In this paper, we study the problem of designing proxies (or portfolios) for
various stock market indices based on historical data. We use four different
methods for computing market indices, all of which are formulas used in actual
stock market analysis. For each index, we consider three criteria for designing
the proxy: the proxy must either track the market index, outperform the market
index, or perform within a margin of error of the index while maintaining a low
volatility. In eleven of the twelve cases (all combinations of four indices
with three criteria except the problem of sacrificing return for less
volatility using the price-relative index) we show that the problem is NP-hard,
and hence most likely intractable."
"In the context of investment analysis, we formulate an abstract online
computing problem called a planning game and develop general tools for solving
such a game. We then use the tools to investigate a practical buy-and-hold
trading problem faced by long-term investors in stocks. We obtain the unique
optimal static online algorithm for the problem and determine its exact
competitive ratio. We also compare this algorithm with the popular dollar
averaging strategy using actual market data."
"This paper studies some basic problems in a multiple-object auction model
using methodologies from theoretical computer science. We are especially
concerned with situations where an adversary bidder knows the bidding
algorithms of all the other bidders. In the two-bidder case, we derive an
optimal randomized bidding algorithm, by which the disadvantaged bidder can
procure at least half of the auction objects despite the adversary's a priori
knowledge of his algorithm. In the general $k$-bidder case, if the number of
objects is a multiple of $k$, an optimal randomized bidding algorithm is found.
If the $k-1$ disadvantaged bidders employ that same algorithm, each of them can
obtain at least $1/k$ of the objects regardless of the bidding algorithm the
adversary uses. These two algorithms are based on closed-form solutions to
certain multivariate probability distributions. In situations where a
closed-form solution cannot be obtained, we study a restricted class of bidding
algorithms as an approximation to desired optimal algorithms."
"The tandem mass spectrometry fragments a large number of molecules of the
same peptide sequence into charged prefix and suffix subsequences, and then
measures mass/charge ratios of these ions. The de novo peptide sequencing
problem is to reconstruct the peptide sequence from a given tandem mass
spectral data of k ions. By implicitly transforming the spectral data into an
NC-spectrum graph G=(V,E) where |V|=2k+2, we can solve this problem in
O(|V|+|E|) time and O(|V|) space using dynamic programming. Our approach can be
further used to discover a modified amino acid in O(|V||E|) time and to analyze
data with other types of noise in O(|V||E|) time. Our algorithms have been
implemented and tested on actual experimental data."
"An evolutionary tree is a rooted tree where each internal vertex has at least
two children and where the leaves are labeled with distinct symbols
representing species. Evolutionary trees are useful for modeling the
evolutionary history of species. An agreement subtree of two evolutionary trees
is an evolutionary tree which is also a topological subtree of the two given
trees. We give an algorithm to determine the largest possible number of leaves
in any agreement subtree of two trees T_1 and T_2 with n leaves each. If the
maximum degree d of these trees is bounded by a constant, the time complexity
is O(n log^2(n)) and is within a log(n) factor of optimal. For general d, this
algorithm runs in O(n d^2 log(d) log^2(n)) time or alternatively in O(n d
sqrt(d) log^3(n)) time."
"We present an algorithm for computing a maximum agreement subtree of two
unrooted evolutionary trees. It takes O(n^{1.5} log n) time for trees with
unbounded degrees, matching the best known time complexity for the rooted case.
Our algorithm allows the input trees to be mixed trees, i.e., trees that may
contain directed and undirected edges at the same time. Our algorithm adopts a
recursive strategy exploiting a technique called label compression. The
backbone of this technique is an algorithm that computes the maximum weight
matchings over many subgraphs of a bipartite graph as fast as it takes to
compute a single matching."
"We consider the problem of selecting a portfolio of assets that provides the
investor a suitable balance of expected return and risk. With respect to the
seminal mean-variance model of Markowitz, we consider additional constraints on
the cardinality of the portfolio and on the quantity of individual shares. Such
constraints better capture the real-world trading system, but make the problem
more difficult to be solved with exact methods. We explore the use of local
search techniques, mainly tabu search, for the portfolio selection problem. We
compare and combine previous work on portfolio selection that makes use of the
local search approach and we propose new algorithms that combine different
neighborhood relations. In addition, we show how the use of randomization and
of a simple form of adaptiveness simplifies the setting of a large number of
critical parameters. Finally, we show how our techniques perform on public
benchmarks."
"This note carries three purposes involving our latest advances on the radial
basis function (RBF) approach. First, we will introduce a new scheme employing
the boundary knot method (BKM) to nonlinear convection-diffusion problem. It is
stressed that the new scheme directly results in a linear BKM formulation of
nonlinear problems by using response point-dependent RBFs, which can be solved
by any linear solver. Then we only need to solve a single nonlinear algebraic
equation for desirable unknown at some inner node of interest. The numerical
results demonstrate high accuracy and efficiency of this nonlinear BKM
strategy. Second, we extend the concepts of distance function, which include
time-space and variable transformation distance functions. Finally, we
demonstrate that if the nodes are symmetrically placed, the RBF coefficient
matrices have either centrosymmetric or skew centrosymmetric structures. The
factorization features of such matrices lead to a considerable reduction in the
RBF computing effort. A simple approach is also presented to reduce the
ill-conditioning of RBF interpolation matrices in general cases."
"In branching simulation, a novel approach to simulation presented in this
paper, a multiplicity of plausible scenarios are concurrently developed and
implemented. In conventional simulations of complex systems, there arise from
time to time uncertainties as to which of two or more alternatives are more
likely to be pursued by the system being simulated. Under these conditions the
simulationist makes a judicious choice of one of these alternatives and embeds
this choice in the simulation model. By contrast, in the branching approach,
two or more of such alternatives (or branches) are included in the model and
implemented for concurrent computer solution. The theoretical foundations for
branching simulation as a computational process are in the domains of
alternating Turing machines, molecular computing, and E-machines. Branching
simulations constitute the development of diagrams of scenarios representing
significant, alternative flows of events. Logical means for interpretation and
investigation of the branching simulation and prediction are provided by the
logical theories of possible worlds, which have been formalized by the
construction of logical varieties. Under certain conditions, the branching
approach can considerably enhance the efficiency of computer simulations and
provide more complete insights into the interpretation of predictions based on
simulations. As an example, the concepts developed in this paper have been
applied to a simulation task that plays an important role in radiology - the
noninvasive treatment of brain aneurysms."
"We describe a binding schema markup language (BSML) for describing data
interchange between scientific codes. Such a facility is an important
constituent of scientific problem solving environments (PSEs). BSML is designed
to integrate with a PSE or application composition system that views model
specification and execution as a problem of managing semistructured data. The
data interchange problem is addressed by three techniques for processing
semistructured data: validation, binding, and conversion. We present BSML and
describe its application to a PSE for wireless communications system design."
"A universalization of a parameterized investment strategy is an online
algorithm whose average daily performance approaches that of the strategy
operating with the optimal parameters determined offline in hindsight. We
present a general framework for universalizing investment strategies and
discuss conditions under which investment strategies are universalizable. We
present examples of common investment strategies that fit into our framework.
The examples include both trading strategies that decide positions in
individual stocks, and portfolio strategies that allocate wealth among multiple
stocks. This work extends Cover's universal portfolio work. We also discuss the
runtime efficiency of universalization algorithms. While a straightforward
implementation of our algorithms runs in time exponential in the number of
parameters, we show that the efficient universal portfolio computation
technique of Kalai and Vempala involving the sampling of log-concave functions
can be generalized to other classes of investment strategies."
"Data mining has traditionally focused on the task of drawing inferences from
large datasets. However, many scientific and engineering domains, such as fluid
dynamics and aircraft design, are characterized by scarce data, due to the
expense and complexity of associated experiments and simulations. In such
data-scarce domains, it is advantageous to focus the data collection effort on
only those regions deemed most important to support a particular data mining
objective. This paper describes a mechanism that interleaves bottom-up data
mining, to uncover multi-level structures in spatial data, with top-down
sampling, to clarify difficult decisions in the mining process. The mechanism
exploits relevant physical properties, such as continuity, correspondence, and
locality, in a unified framework. This leads to effective mining and sampling
decisions that are explainable in terms of domain knowledge and data
characteristics. This approach is demonstrated in two diverse applications --
mining pockets in spatial data, and qualitative determination of Jordan forms
of matrices."
"This report aims to present my research updates on distance function wavelets
(DFW) based on the fundamental solutions and the general solutions of the
Helmholtz, modified Helmholtz, and convection-diffusion equations, which
include the isotropic Helmholtz-Fourier (HF) transform and series, the
Helmholtz-Laplace (HL) transform, and the anisotropic convection-diffusion
wavelets and ridgelets. The latter is set to handle discontinuous and track
data problems. The edge effect of the HF series is addressed. Alternative
existence conditions for the DFW transforms are proposed and discussed. To
simplify and streamline the expression of the HF and HL transforms, a new
dimension-dependent function notation is introduced. The HF series is also used
to evaluate the analytical solutions of linear diffusion problems of arbitrary
dimensionality and geometry. The weakness of this report is lacking of rigorous
mathematical analysis due to the author's limited mathematical knowledge."
"Atkinson developed a strategy which splits solution of a PDE system into
homogeneous and particular solutions, where the former have to satisfy the
boundary and governing equation, while the latter only need to satisfy the
governing equation without concerning geometry. Since the particular solution
can be solved irrespective of boundary shape, we can use a readily available
fast Fourier or orthogonal polynomial technique O(NlogN) to evaluate it in a
regular box or sphere surrounding physical domain. The distinction of this
study is that we approximate homogeneous solution with nonsingular general
solution RBF as in the boundary knot method. The collocation method using
general solution RBF has very high accuracy and spectral convergent speed and
is a simple, truly meshfree approach for any complicated geometry. More
importantly, the use of nonsingular general solution avoids the controversial
artificial boundary in the method of fundamental solution due to the
singularity of fundamental solution."
"Report II is concerned with the extended results of distance function
wavelets (DFW). The fractional DFW transforms are first addressed relating to
the fractal geometry and fractional derivative, and then, the discrete
Helmholtz-Fourier transform is briefly presented. The Green second identity may
be an alternative devise in developing the theoretical framework of the DFW
transform and series. The kernel solutions of the Winkler plate equation and
the Burger's equation are used to create the DFW transforms and series. Most
interestingly, it is found that the translation invariant monomial solutions of
the high-order Laplace equations can be used to make very simple harmonic
polynomial DFW series. In most cases of this study, solid mathematical analysis
is missing and results are obtained intuitively in the conjecture status."
"In this study, we presented the high-order fundamental solutions and general
solutions of convection-diffusion equation. To demonstrate their efficacy, we
applied the high-order general solutions to the boundary particle method (BPM)
for the solution of some inhomogeneous convection-diffusion problems, where the
BPM is a new truly boundary-only meshfree collocation method based on multiple
reciprocity principle. For the sake of completeness, the BPM is also briefly
described here."
"Part III of the reports consists of various unconventional distance function
wavelets (DFW). The dimension and the order of partial differential equation
(PDE) are first used as a substitute of the scale parameter in the DFW
transforms and series, especially with the space and time-space potential
problems. It is noted that the recursive multiple reciprocity formulation is
the DFW series. The Green second identity is used to avoid the singularity of
the zero-order fundamental solution in creating the DFW series. The fundamental
solutions of various composite PDEs are found very flexible and efficient to
handle a borad range of problems. We also discuss the underlying connections
between the crucial concepts of dimension, scale and the order of PDE through
the analysis of dissipative acoustic wave propagation. The shape parameter of
the potential problems is also employed as the ""scale parameter"" to create the
non-orthogonal DFW. This paper also briefly discusses and conjectures the DFW
correspondences of a variety of coordinate variable transforms and series.
Practically important, the anisotropic and inhomogeneous DFW's are developed by
using the geodesic distance variable. The DFW and the related basis functions
are also used in making the kernel distance sigmoidal functions, which are
potentially useful in the artificial neural network and machine learning. As or
even worse than the preceding two reports, this study scarifies mathematical
rigor and in turn unfetter imagination. Most results are intuitively obtained
without rigorous analysis. Follow-up research is still under way. The paper is
intended to inspire more research into this promising area."
"The boundary knot method (BKM) is a recent boundary-type radial basis
function (RBF) collocation scheme for general PDEs. Like the method of
fundamental solution (MFS), the RBF is employed to approximate the
inhomogeneous terms via the dual reciprocity principle. Unlike the MFS, the
method uses a nonsingular general solution instead of a singular fundamental
solution to evaluate the homogeneous solution so as to circumvent the
controversial artificial boundary outside the physical domain. The BKM is
meshfree, superconvergent, integration free, very easy to learn and program.
The original BKM, however, loses symmetricity in the presense of mixed
boundary. In this study, by analogy with Hermite RBF interpolation, we
developed a symmetric BKM scheme. The accuracy and efficiency of the symmetric
BKM are also numerically validated in some 2D and 3D Helmholtz and diffusion
reaction problems under complicated geometries."
"This paper made some significant advances in the dual reciprocity and
boundary-only RBF techniques. The proposed boundary knot method (BKM) is
different from the standard boundary element method in a number of important
aspects. Namely, it is truly meshless, exponential convergence,
integration-free (of course, no singular integration), boundary-only for
general problems, and leads to symmetric matrix under certain conditions (able
to be extended to general cases after further modified). The BKM also avoids
the artificial boundary in the method of fundamental solution. An amazing
finding is that the BKM can formulate linear modeling equations for nonlinear
partial differential systems with linear boundary conditions. This merit makes
it circumvent all perplexing issues in the iteration solution of nonlinear
equations. On the other hand, by analogy with Green's second identity, this
paper also presents a general solution RBF (GSR) methodology to construct
efficient RBFs in the dual reciprocity and domain-type RBF collocation methods.
The GSR approach first establishes an explicit relationship between the BEM and
RBF itself on the ground of the weighted residual principle. This paper also
discusses the RBF convergence and stability problems within the framework of
integral equation theory."
"This paper aims to survey our recent work relating to the radial basis
function (RBF) from some new views of points. In the first part, we established
the RBF on numerical integration analysis based on an intrinsic relationship
between the Green's boundary integral representation and RBF. It is found that
the kernel function of integral equation is important to create efficient RBF.
The fundamental solution RBF (FS-RBF) was presented as a novel strategy
constructing operator-dependent RBF. We proposed a conjecture formula featuring
the dimension affect on error bound to show the independent-dimension merit of
the RBF techniques. We also discussed wavelet RBF, localized RBF schemes, and
the influence of node placement on the RBF solution accuracy. The
centrosymmetric matrix structure of the RBF interpolation matrix under
symmetric node placing is proved.
  The second part of this paper is concerned with the boundary knot method
(BKM), a new boundary-only, meshless, spectral convergent, integration-free RBF
collocation technique. The BKM was tested to the Helmholtz, Laplace, linear and
nonlinear convection-diffusion problems. In particular, we introduced the
response knot-dependent nonsingular general solution to calculate
varying-parameter and nonlinear steady convection-diffusion problems very
efficiently. By comparing with the multiple dual reciprocity method, we
discussed the completeness issue of the BKM.
  Finally, the nonsingular solutions for some known differential operators were
given in appendix. Also we expanded the RBF concepts by introducing time-space
RBF for transient problems."
"This paper has made some significant advances in the boundary-only and
domain-type RBF techniques. The proposed boundary knot method (BKM) is
different from the standard boundary element method in a number of important
aspects. Namely, it is truly meshless, exponential convergence,
integration-free (of course, no singular integration), boundary-only for
general problems, and leads to symmetric matrix under certain conditions (able
to be extended to general cases after further modified). The BKM also avoids
the artificial boundary in the method of fundamental solution. An amazing
finding is that the BKM can formulate linear modeling equations for nonlinear
partial differential systems with linear boundary conditions. This merit makes
it circumvent all perplexing issues in the iteration solution of nonlinear
equations. On the other hand, by analogy with Green's second identity, we also
presents a general solution RBF (GSR) methodology to construct efficient RBFs
in the domain-type RBF collocation method and dual reciprocity method. The GSR
approach first establishes an explicit relationship between the BEM and RBF
itself on the ground of the potential theory. This paper also discusses some
essential issues relating to the RBF computing, which include time-space RBFs,
direct and indirect RBF schemes, finite RBF method, and the application of
multipole and wavelet to the RBF solution of the PDEs."
"Very few studies involve how to construct the efficient RBFs by means of
problem features. Recently the present author presented general solution RBF
(GS-RBF) methodology to create operator-dependent RBFs successfully [1]. On the
other hand, the normal radial basis function (RBF) is defined via Euclidean
space distance function or the geodesic distance [2]. This purpose of this note
is to redefine distance function in conjunction with problem features, which
include problem-dependent and time-space distance function."
"This paper shows that the weighting coefficient matrices of the differential
quadrature method (DQM) are centrosymmetric or skew-centrosymmetric if the grid
spacings are symmetric irrespective of whether they are equal or unequal. A new
skew centrosymmetric matrix is also discussed. The application of the
properties of centrosymmetric and skew centrosymmetric matrix can reduce the
computational effort of the DQM for calculations of the inverse, determinant,
eigenvectors and eigenvalues by 75%. This computational advantage are also
demonstrated via several numerical examples."
"Civan and Sliepcevich [1, 2] suggested that special matrix solver should be
developed to further reduce the computing effort in applying the differential
quadrature (DQ) method for the Poisson and convection-diffusion equations.
Therefore, the purpose of the present communication is to introduce and apply
the Lyapunov formulation which can be solved much more efficiently than the
Gaussian elimination method. Civan and Sliepcevich [2] first presented DQ
approximate formulas in polynomial form for partial derivatives in
tow-dimensional variable domain. For simplifying formulation effort, Chen et
al. [3] proposed the compact matrix form of these DQ approximate formulas. In
this study, by using these matrix approximate formulas, the DQ formulations for
the Poisson and convection-diffusion equations can be expressed as the Lyapunov
algebraic matrix equation. The formulation effort is simplified, and a simple
and explicit matrix formulation is obtained. A variety of fast algorithms in
the solution of the Lyapunov equation [4-6] can be successfully applied in the
DQ analysis of these two-dimensional problems, and, thus, the computing effort
can be greatly reduced. Finally, we also point out that the present reduction
technique can be easily extended to the three-dimensional cases."
"This paper presents the first coupling application of the dual reciprocity
BEM (DRBEM) and dynamic programming filter to inverse elastodynamic problem.
The DRBEM is the only BEM method, which does not require domain discretization
for general linear and nonlinear dynamic problems. Since the size of numerical
discretization system has a great effect on the computing effort of recursive
or iterative calculations of inverse analysis, the intrinsic boundary-only
merit of the DRBEM causes a considerable computational saving. On the other
hand, the strengths of the dynamic programming filter lie in its mathematical
simplicity, easy to program and great flexibility in the type, number and
locations of measurements and unknown inputs. The combination of these two
techniques is therefore very attractive for the solution of practical inverse
problems. In this study, the spatial and temporal partial derivatives of the
governing equation are respectively discretized first by the DRBEM and the
precise integration method, and then, by using dynamic programming with
regularization, dynamic load is estimated based on noisy measurements of
velocity and displacement at very few locations. Numerical experiments involved
with the periodic and Heaviside impact load are conducted to demonstrate the
applicability, efficiency and simplicity of this strategy. The affect of noise
level, regularization parameter, and measurement types on the estimation is
also investigated."
"This paper is concerned with the two new boundary-type radial basis function
collocation schemes, boundary knot method (BKM) and boundary particle method
(BPM). The BKM is developed based on the dual reciprocity theorem, while the
BPM employs the multiple reciprocity technique. Unlike the method of
fundamental solution, the wto methods use the nonsingular general solutions
instead of singular fundamental solution to circumvent the controversial
artificial boundary outside physical domain. Compared with the boundary element
method, both the BKM and BPM are meshfree, superconvergent, meshfree,
integration free, symmetric, and mathematically simple collocation techniques
for general PDEs. In particular, the BPM does not require any inner nodes for
inhomogeneous problems. In this study, the accuracy and efficiency of the two
methods are numerically demonstrated to some 2D, 3D Helmholtz and
convection-diffusion problems under complicated geometries."
"Based on the radial basis function (RBF), non-singular general solution and
dual reciprocity method (DRM), this paper presents an inherently meshless,
integration-free, boundary-only RBF collocation techniques for numerical
solution of various partial differential equation systems. The basic ideas
behind this methodology are very mathematically simple. In this study, the RBFs
are employed to approximate the inhomogeneous terms via the DRM, while
non-singular general solution leads to a boundary-only RBF formulation for
homogenous solution. The present scheme is named as the boundary knot method
(BKM) to differentiate it from the other numerical techniques. In particular,
due to the use of nonsingular general solutions rather than singular
fundamental solutions, the BKM is different from the method of fundamental
solution in that the former does no require the artificial boundary and results
in the symmetric system equations under certain conditions. The efficiency and
utility of this new technique are validated through a number of typical
numerical examples. Completeness concern of the BKM due to the only use of
non-singular part of complete fundamental solution is also discussed."
"By far, the fractional derivative model is mainly related to the modelling of
complicated solid viscoelastic material. In this study, we try to build the
fractional derivative PDE model for broadband ultrasound propagation through
human tissues."
"The frequency-dependent attenuation of broadband acoustics is often
confronted in many different areas. However, the related time domain simulation
is rarely found in literature due to enormous technical difficulty. The
currently popular relaxation models with the presence of convolution operation
require some material parameters which are not readily available. In this
study, three reports are contributed to address broadband ultrasound
frequency-dependent absorptions using the readily available empirical
parameters. This report is the first in series concerned with developing a
direct time domain FEM formulation. The next two reports are about the
frequency decomposition model and the fractional derivative model."
"In this paper, we present a diagnosis method of diseases from clinical data.
The data are routine test such as urine test, hematology, chemistries etc.
Though those tests have been done for people who check in medical institutes,
how each item of the data interacts each other and which combination of them
cause a disease are neither understood nor studied well. Here we attack the
practically important problem by putting the data into mathematical setup and
applying support vector machine. Finally we present simulation results for
fatty liver, gastritis etc and discuss about their implications."
"Grid infrastructures and computing environments have progressed significantly
in the past few years. The vision of truly seamless Grid usage relies on
runtime systems support that is cognizant of the operational issues underlying
grid computations and, at the same time, is flexible enough to accommodate
diverse application scenarios. This paper addresses the twin aspects of Grid
infrastructure and application support through a novel combination of two
computational technologies: Weaves - a source-language independent parallel
runtime compositional framework that operates through reverse-analysis of
compiled object files, and runtime recommender systems that aid in dynamic
knowledge-based application composition. Domain-specific adaptivity is
exploited through a novel compositional system that supports runtime
recommendation of code modules and a sophisticated checkpointing and runtime
migration solution that can be transparently deployed over Grid
infrastructures. A core set of ""adaptivity schemas"" are provided as templates
for adaptive composition of large-scale scientific computations. Implementation
issues, motivating application contexts, and preliminary results are described."
"In this paper we model several simple biochemical operations on RNA molecules
that modify their secondary structure by means of a suitable variation of
Gro\ss e-Rhode's Algebra Transformation Systems."
"The paper reports the obtained results for the projection and realization of
a digitally system aiming to assist the equipment for a regulatory and
pre-regulatory tools and holding tools within the flexible fabrication systems
(FFS). Moreover, based on the present results, the same methodology can be
applied for assisting tools from the point of view of their integrity and to
wear compensation in the FFS framework."
"The boundary knot method (BKM) [1] is a meshless boundary-type radial basis
function (RBF) collocation scheme, where the nonsingular general solution is
used instead of fundamental solution to evaluate the homogeneous solution,
while the dual reciprocity method (DRM) is employed to approximation of
particular solution. Despite the fact that there are not nonsingular RBF
general solutions available for Laplace and biharmonic problems, this study
shows that the method can be successfully applied to these problems. The
high-order general and fundamental solutions of Burger and Winkler equations
are also first presented here."
"Practical data analysis involves many implicit or explicit assumptions about
the good behavior of the data, and excludes consideration of various
potentially pathological or limit cases. In this work, we present a new general
theory of data, and of data processing, to bypass some of these assumptions.
The new framework presented is focused on integration, and has direct
applicability to expectation, distance, correlation, and aggregation. In a case
study, we seek to reveal faint structure in financial data. Our new foundation
for data encoding and handling offers increased justification for our
conclusions."
"Generally, in the financial literature, the notion of quadratic VaR is
implicitly confused with the Delta-Gamma VaR, because more authors dealt with
portfolios that contains derivatives instruments.
  In this paper, we postpone to estimate the Value-at-Risk of a quadratic
portfolio of securities (i.e equities) without the Delta and Gamma greeks, when
the joint log-returns changes with multivariate elliptic distribution. We have
reduced the estimation of the quadratic VaR of such portfolio to a resolution
of one dimensional integral equation. To illustrate our method, we give special
attention to the mixture of normal and mixture of t-student distribution. For
given VaR, when joint Risk Factors changes with elliptic distribution, we show
how to estimate an Expected Shortfall ."
"We present an unusual algorithm involving classification trees where two
trees are grown in opposite directions so that they are matched at their
leaves. This approach finds application in a new data mining task we formulate,
called ""redescription mining"". A redescription is a shift-of-vocabulary, or a
different way of communicating information about a given subset of data; the
goal of redescription mining is to find subsets of data that afford multiple
descriptions. We highlight the importance of this problem in domains such as
bioinformatics, which exhibit an underlying richness and diversity of data
descriptors (e.g., genes can be studied in a variety of ways). Our approach
helps integrate multiple forms of characterizing datasets, situates the
knowledge gained from one dataset in the context of others, and harnesses
high-level abstractions for uncovering cryptic and subtle features of data.
Algorithm design decisions, implementation details, and experimental results
are presented."
"The concept of the ""modulus"" in the CAD system drawings is characterized,
being a base of developing of the problem-oriented extensions. The modulus
consists of visible geometric elements of the drawing and invisible parametric
representation of the modelling object. The technological advantages of
moduluss in a complex CAD system developing are described."
"The modular technology of creation of the problem-oriented extensions of a
CAD system is described, which was realised in a system TechnoCAD GlassX for
designing of reconstruction of the plants. The modularity of the technology is
expressed in storage of all parameters of the design in one element of the
drawing - modulus, with automatic generation of a geometrical part of the
modulus from these parameters. The common principles of the system organization
of extensions developing are described: separation of the part of the design to
automize in this extension, architecture of parameters in the form of the lists
of objects with their properties and links to another objects, separation of
common and special operations, stages of the developing, boundaries of
applicability of technology."
"The hierarchic block model of the tables in design documentation as a part of
a CAD system is described, intended for automatic specifications making of
elements of the drawings, with usage of the electronic catalogs. The model is
created for needs of a CAD system of reconstruction of the industrial plants,
where the result of designing are the drawings, which include the
specifications of different types. The adequate simulation of the specification
tables is ensured with technology of storing in the drawing of the visible
geometric elements and invisible parametric representation, sufficient for
generation of this elements."
"Applying the modular technology of developing of the problem-oriented
extensions of a CAD system to a problem of automation of creating of the
axonometric piping diagrams on an example of the program system TechnoCAD
GlassX is described. The proximity of composition of the schemas is detected
for special technological pipe lines, systems of a water line and water drain,
heating, heat supply, ventilating, air conditioning. The structured parametric
representation of the schemas, including properties of objects, their link,
common settings, settings by default and the special links of compatibility is
reviewed."
"Applying the modular technology of developing of the problem-oriented
extensions of a CAD system to a problem of automation of creating of the
axonometric piping diagrams on an example of the program system TechnoCAD
GlassX is described. The features of realization of common operations,
composition and realization of special operations of a designing of the schemas
of the special technological pipe lines, systems of a water line and water
drain, heating, heat supply, ventilating, air conditioning are reviewed."
"The problem of the automation of the designing of the axonometric piping
diagrams include, as the minimum, manipulations with the flat schemas of
three-dimensional wireframe objects (with dimension of 2,5). The specialized
model, methodical and mathematical approaches are required because of large
bulk of calculuss. Coordinate systems, data types, common principles of
realization of operation with data and composition of the basic operations are
described which are realised in the complex CAD system of the reconstruction of
the plants TechnoCAD GlassX."
"The paper proposes a new methodology for defining business process measures
and their computation. The approach is based on metamodeling according to MOF.
Especially, a metamodel providing precise definitions of typical process
measures for UML activity diagram-like notation is proposed, including precise
definitions how measures should be aggregated for composite process elements.
The proposed approach allows defining values in a natural way, and measurement
of data, which are of interest to business, without deep investigation into
specific technical solutions. This provides new possibilities for business
process measurement, decreasing the gap between technical solutions and asset
management methodologies."
"In this paper we present ideas and architectural principles upon which we are
basing the development of a distributed, open-source infrastructure that, in
turn, will support the expression of business models, the dynamic composition
of software services, and the optimisation of service chains through automatic
self-organising and evolutionary algorithms derived from biology. The target
users are small and medium-sized enterprises (SMEs). We call the collection of
the infrastructure, the software services, and the SMEs a Digital Business
Ecosystem (DBE)."
"The modular technology of development of the problem-oriented CAD expansions
is applied to a task of designing of profiles of outside networks of water
supply and water drain with realization in program system TechnoCAD GlassX. The
unity of structure of this profiles is revealed, the system model of the
drawings of profiles of networks is developed including the structured
parametric representation (properties of objects and their interdependence,
general settings and default settings) and operations with it, which
efficiently automate designing"
"The modular technology of development of the problem-oriented CAD expansions
is applied to a task of designing of protection of the buildings from the
lightning with realization in program system TechnoCAD GlassX. The system model
of the drawings of lightning protection is developed including the structured
parametric representation (properties of objects and their interdependence,
general settings and default settings) and operations with it, which
efficiently automate designing"
"The methods of support of the requirements of the Russian standards in a CAD
of industrial objects are explained, which were implemented in the CAD system
TechnoCAD GlassX with an own graphics core and own structures of data storage.
It is rotined, that the binding of storage structures and program code of a CAD
to the requirements of standards enable not only to fulfil these requirements
in project documentation, but also to increase a degree of compactness of
storage of drawings both on the disk and in the RAM"
"In this paper is proposed the method of the identification of complex dynamic
systems. Method can be used for the identification of linear and nonlinear
complex dynamic systems for the determined or stochastic signals at the inputs
and the outputs. It is proposed to use a basis of wavelets for obtaining the
impulse transient function (ITF) of system. ITF is considered in the form of
surface in the 3D space. Are given the results of experiments on the
identification of systems in the basis of wavelets."
"We address a problem of covariance selection, where we seek a trade-off
between a high likelihood against the number of non-zero elements in the
inverse covariance matrix. We solve a maximum likelihood problem with a penalty
term given by the sum of absolute values of the elements of the inverse
covariance matrix, and allow for imposing bounds on the condition number of the
solution. The problem is directly amenable to now standard interior-point
algorithms for convex optimization, but remains challenging due to its size. We
first give some results on the theoretical computational complexity of the
problem, by showing that a recent methodology for non-smooth convex
optimization due to Nesterov can be applied to this problem, to greatly improve
on the complexity estimate given by interior-point algorithms. We then examine
two practical algorithms aimed at solving large-scale, noisy (hence dense)
instances: one is based on a block-coordinate descent approach, where columns
and rows are updated sequentially, another applies a dual version of Nesterov's
method."
"In this article two implementations of a symmetric finite difference
algorithm for a first-order partial differential equation are discussed. The
considered partial differential equation discribes the time evolution of the
crack length distribution of microcracks in brittle materia."
"Computational scientists are facing a new era where the old ways of
developing and reusing code have to be left behind and a few daring steps are
to be made towards new horizons. The present work analyzes the needs that drive
this change, the factors that contribute to the inertia of the community and
slow the transition, the status and perspective of present attempts, the
principle, practical and technical problems that are to be addressed in the
short and long run."
"The paper proposes a more formalized definition of UML 2.0 Activity Diagram
semantics. A subset of activity diagram constructs relevant for business
process modeling is considered. The semantics definition is based on the
original token flow methodology, but a more constructive approach is used. The
Activity Diagram Virtual machine is defined by means of a metamodel, with
operations defined by a mix of pseudocode and OCL pre- and postconditions. A
formal procedure is described which builds the virtual machine for any activity
diagram. The relatively complicated original token movement rules in control
nodes and edges are combined into paths from an action to action. A new
approach is the use of different (push and pull) engines, which move tokens
along the paths. Pull engines are used for paths containing join nodes, where
the movement of several tokens must be coordinated. The proposed virtual
machine approach makes the activity semantics definition more transparent where
the token movement can be easily traced. However, the main benefit of the
approach is the possibility to use the defined virtual machine as a basis for
UML activity diagram based workflow or simulation engine."
"In this paper, an attempt is made to systematically discuss the development
of simulation systems for manufacturing system design. General requirements on
manufacturing simulators are formulated and a framework to address the
requirements is suggested. Problems of information representation as an
activity underlying simulation are considered. This is to form the necessary
mathematical foundation for manufacturing simulations. The theoretical findings
are explored through a pilot study. A conclusion about the suitability of the
suggested approach to the development of simulation systems for manufacturing
system design is made, and implications for future research are described."
"The research results described are concerned with: - developing a domain
modeling method and tools to provide the design and implementation of
decision-making support systems for computer integrated manufacturing; -
building a decision-making support system based on know-how and its software
environment. The research is funded by NEDO, Japan."
"The ever higher complexity of manufacturing systems, continually shortening
life cycles of products and their increasing variety, as well as the unstable
market situation of the recent years require introducing grater flexibility and
responsiveness to manufacturing processes. From this perspective, one of the
critical manufacturing tasks, which traditionally attract significant attention
in both academia and the industry, but which have no satisfactory universal
solution, is production scheduling. This paper proposes an approach based on
genetics-based machine learning (GBML) to treat the problem of flow shop
scheduling. By the approach, a set of scheduling rules is represented as an
individual of genetic algorithms, and the fitness of the individual is
estimated based on the makespan of the schedule generated by using the
rule-set. A concept of the interactive software environment consisting of a
simulator and a GBML simulation engine is introduced to support human
decision-making during scheduling. A pilot study is underway to evaluate the
performance of the GBML technique in comparison with other methods (such as
Johnson's algorithm and simulated annealing) while completing test examples."
"Although it has contributed to remarkable improvements in some specific
areas, attempts to develop a universal design theory are generally
characterized by failure. This paper sketches arguments for a new approach to
engineering design based on Semiotics - the science about signs. The approach
is to combine different design theories over all the product life cycle stages
into one coherent and traceable framework. Besides, it is to bring together the
designer's and user's understandings of the notion of 'good product'. Building
on the insight from natural sciences that complex systems always exhibit a
self-organizing meaning-influential hierarchical dynamics, objective laws
controlling product development are found through an examination of design as a
semiosis process. These laws are then applied to support evolutionary design of
products. An experiment validating some of the theoretical findings is
outlined, and concluding remarks are given."
"We construct fast algorithms for evaluating transforms associated with
families of functions which satisfy recurrence relations. These include
algorithms both for computing the coefficients in linear combinations of the
functions, given the values of these linear combinations at certain points,
and, vice versa, for evaluating such linear combinations at those points, given
the coefficients in the linear combinations; such procedures are also known as
analysis and synthesis of series of certain special functions. The algorithms
of the present paper are efficient in the sense that their computational costs
are proportional to n (ln n) (ln(1/epsilon))^3, where n is the amount of input
and output data, and epsilon is the precision of computations. Stated somewhat
more precisely, we find a positive real number C such that, for any positive
integer n > 10, the algorithms require at most C n (ln n) (ln(1/epsilon))^3
floating-point operations and words of memory to evaluate at n appropriately
chosen points any linear combination of n special functions, given the
coefficients in the linear combination, where epsilon is the precision of
computations."
"In this paper, we describe a general method for constructing the posterior
distribution of an option price. Our framework takes as inputs the prior
distributions of the parameters of the stochastic process followed by the
underlying, as well as the likelihood function implied by the observed price
history for the underlying. Our work extends that of Karolyi (1993) and
Darsinos and Satchell (2001), but with the crucial difference that the
likelihood function we use for inference is that which is directly implied by
the underlying, rather than imposed in an ad hoc manner via the introduction of
a function representing ""measurement error."" As such, an important problem
still relevant for our method is that of model risk, and we address this issue
by describing how to perform a Bayesian averaging of parameter inferences based
on the different models considered using our framework."
"We analyze numerical stability of a recursive computation scheme of present
value (PV) amd show that the absolute error increases exponentially for
positive discount rates. We show that reversing the direction of calculations
in the recurrence equation yields a robust PV computation routine."
"We present a quasi-analytic perturbation expansion for multivariate
N-dimensional Gaussian integrals. The perturbation expansion is an infinite
series of lower-dimensional integrals (one-dimensional in the simplest
approximation). This perturbative idea can also be applied to multivariate
Student-t integrals. We evaluate the perturbation expansion explicitly through
2nd order, and discuss the convergence, including enhancement using Pade
approximants. Brief comments on potential applications in finance are given,
including options, models for credit risk and derivatives, and correlation
sensitivities."
"A framework for virtual reality of engineering objects has been developed.
This framework may simulate different equipment related to virtual reality.
Framework supports 6D dynamics, ordinary differential equations, finite
formulas, vector and matrix operations. The framework also supports embedding
of external software."
"A new algorithm for calculating intermolecular pair forces in Molecular
Dynamics (MD) simulations on a distributed parallel computer is presented. The
Arbitrary Interacting Cells Algorithm (AICA) is designed to operate on
geometrical domains defined by an unstructured, arbitrary polyhedral mesh,
which has been spatially decomposed into irregular portions for
parallelisation. It is intended for nano scale fluid mechanics simulation by MD
in complex geometries, and to provide the MD component of a hybrid MD/continuum
simulation. AICA has been implemented in the open-source computational toolbox
OpenFOAM, and verified against a published MD code."
"The ``first passage-time'' (FPT) problem is an important problem with a wide
range of applications in mathematics, physics, biology and finance.
Mathematically, such a problem can be reduced to estimating the probability of
a (stochastic) process first to reach a critical level or threshold. While in
other areas of applications the FPT problem can often be solved analytically,
in finance we usually have to resort to the application of numerical
procedures, in particular when we deal with jump-diffusion stochastic processes
(JDP). In this paper, we develop a Monte-Carlo-based methodology for the
solution of the FPT problem in the context of a multivariate jump-diffusion
stochastic process. The developed methodology is tested by using different
parameters, the simulation results indicate that the developed methodology is
much more efficient than the conventional Monte Carlo method. It is an
efficient tool for further practical applications, such as the analysis of
default correlation and predicting barrier options in finance."
"Many problems in finance require the information on the first passage time
(FPT) of a stochastic process. Mathematically, such problems are often reduced
to the evaluation of the probability density of the time for such a process to
cross a certain level, a boundary, or to enter a certain region. While in other
areas of applications the FPT problem can often be solved analytically, in
finance we usually have to resort to the application of numerical procedures,
in particular when we deal with jump-diffusion stochastic processes (JDP). In
this paper, we propose a Monte-Carlo-based methodology for the solution of the
first passage time problem in the context of multivariate (and correlated)
jump-diffusion processes. The developed technique provide an efficient tool for
a number of applications, including credit risk and option pricing. We
demonstrate its applicability to the analysis of the default rates and default
correlations of several different, but correlated firms via a set of empirical
data."
"Evaluation of default correlation is an important task in credit risk
analysis. In many practical situations, it concerns the joint defaults of
several correlated firms, the task that is reducible to a first passage time
(FPT) problem. This task represents a great challenge for jump-diffusion
processes (JDP), where except for very basic cases, there are no analytical
solutions for such problems. In this contribution, we generalize our previous
fast Monte-Carlo method (non-correlated jump-diffusion cases) for multivariate
(and correlated) jump-diffusion processes. This generalization allows us, among
other things, to evaluate the default events of several correlated assets based
on a set of empirical data. The developed technique is an efficient tool for a
number of other applications, including credit risk and option pricing."
"The first passage time (FPT) problem is ubiquitous in many applications. In
finance, we often have to deal with stochastic processes with jump-diffusion,
so that the FTP problem is reducible to a stochastic differential equation with
jump-diffusion. While the application of the conventional Monte-Carlo procedure
is possible for the solution of the resulting model, it becomes computationally
inefficient which severely restricts its applicability in many practically
interesting cases. In this contribution, we focus on the development of
efficient Monte-Carlo-based computational procedures for solving the FPT
problem under the multivariate (and correlated) jump-diffusion processes. We
also discuss the implementation of the developed Monte-Carlo-based technique
for multivariate jump-diffusion processes driving by several compound Poisson
shocks. Finally, we demonstrate the application of the developed methodologies
for analyzing the default rates and default correlations of differently rated
firms via historical data."
"In this paper, the finite volume method is developed to analyze coupled
dynamic problems of nonlinear thermoelasticity. The major focus is given to the
description of martensitic phase transformations essential in the modelling of
shape memory alloys. Computational experiments are carried out to study the
thermo-mechanical wave interactions in a shape memory alloy rod, and a patch.
Both mechanically and thermally induced phase transformations, as well as
hysteresis effects, in a one-dimensional structure are successfully simulated
with the developed methodology. In the two-dimensional case, the main focus is
given to square-to-rectangular transformations and examples of martensitic
combinations under different mechanical loadings are provided."
"In this paper, phase combinations among martensitic variants in shape memory
alloys patches and bars are simulated by a hybrid optimization methodology. The
mathematical model is based on the Landau theory of phase transformations. Each
stable phase is associated with a local minimum of the free energy function,
and the phase combinations are simulated by minimizing the bulk energy. At low
temperature, the free energy function has double potential wells leading to
non-convexity of the optimization problem. The methodology proposed in the
present paper is based on an initial estimate of the global solution by a
genetic algorithm, followed by a refined quasi-Newton procedure to locally
refine the optimum. By combining the local and global search algorithms, the
phase combinations are successfully simulated. Numerical experiments are
presented for the phase combinations in a SMA patch under several typical
mechanical loadings."
"A numerical model is constructed for modelling macroscale damping effects
induced by the first order martensite phase transformations in a shape memory
alloy rod. The model is constructed on the basis of the modified
Landau-Ginzburg theory that couples nonlinear mechanical and thermal fields.
The free energy function for the model is constructed as a double well function
at low temperature, such that the external energy can be absorbed during the
phase transformation and converted into thermal form. The Chebyshev spectral
methods are employed together with backward differentiation for the numerical
analysis of the problem. Computational experiments performed for different
vibration energies demonstrate the importance of taking into account damping
effects induced by phase transformations."
"This paper aims to provide a practical example on the assessment and
propagation of input uncertainty for option pricing when using tree-based
methods. Input uncertainty is propagated into output uncertainty, reflecting
that option prices are as unknown as the inputs they are based on. Option
pricing formulas are tools whose validity is conditional not only on how close
the model represents reality, but also on the quality of the inputs they use,
and those inputs are usually not observable. We provide three alternative
frameworks to calibrate option pricing tree models, propagating parameter
uncertainty into the resulting option prices. We finally compare our methods
with classical calibration-based results assuming that there is no options
market established. These methods can be applied to pricing of instruments for
which there is not an options market, as well as a methodological tool to
account for parameter and model uncertainty in theoretical option pricing."
"A method based on Bayesian neural networks and genetic algorithm is proposed
to control the fermentation process. The relationship between input and output
variables is modelled using Bayesian neural network that is trained using
hybrid Monte Carlo method. A feedback loop based on genetic algorithm is used
to change input variables so that the output variables are as close to the
desired target as possible without the loss of confidence level on the
prediction that the neural network gives. The proposed procedure is found to
reduce the distance between the desired target and measured outputs
significantly."
"This paper investigates the use of evolutionary optimisation techniques to
register a template with a scene image. An error function is created to measure
the correspondence of the template to the image. The problem presented here is
to optimise the horizontal, vertical and scaling parameters that register the
template with the scene. The Genetic Algorithm, Simulated Annealing and
Particle Swarm Optimisations are compared to a Nelder-Mead Simplex optimisation
with starting points chosen in a pre-processing stage. The paper investigates
the precision and accuracy of each method and shows that all four methods
perform favourably for image registration. SA is the most precise, GA is the
most accurate. PSO is a good mix of both and the Simplex method returns local
minima the most. A pre-processing stage should be investigated for the
evolutionary methods in order to improve performance. Discrete versions of the
optimisation methods should be investigated to further improve computational
performance."
"Options have provided a field of much study because of the complexity
involved in pricing them. The Black-Scholes equations were developed to price
options but they are only valid for European styled options. There is added
complexity when trying to price American styled options and this is why the use
of neural networks has been proposed. Neural Networks are able to predict
outcomes based on past data. The inputs to the networks here are stock
volatility, strike price and time to maturity with the output of the network
being the call option price. There are two techniques for Bayesian neural
networks used. One is Automatic Relevance Determination (for Gaussian
Approximation) and one is a Hybrid Monte Carlo method, both used with
Multi-Layer Perceptrons."
"This paper proposes the use of particle swarm optimization method (PSO) for
finite element (FE) model updating. The PSO method is compared to the existing
methods that use simulated annealing (SA) or genetic algorithms (GA) for FE
model for model updating. The proposed method is tested on an unsymmetrical
H-shaped structure. It is observed that the proposed method gives updated
natural frequencies the most accurate and followed by those given by an updated
model that was obtained using the GA and a full FE model. It is also observed
that the proposed method gives updated mode shapes that are best correlated to
the measured ones, followed by those given by an updated model that was
obtained using the SA and a full FE model. Furthermore, it is observed that the
PSO achieves this accuracy at a computational speed that is faster than that by
the GA and a full FE model which is faster than the SA and a full FE model."
"We discuss a method for predicting financial movements and finding pockets of
predictability in the price-series, which is built around inferring the
heterogeneity of trading strategies in a multi-agent trader population. This
work explores extensions to our previous framework (arXiv:physics/0506134).
Here we allow for more intelligent agents possessing a richer strategy set, and
we no longer constrain the estimate for the heterogeneity of the agents to a
probability space. We also introduce a scheme which allows the incorporation of
models with a wide variety of agent types, and discuss a mechanism for the
removal of bias from relevant parameters."
"Commercial graphics processors (GPUs) have high compute capacity at very low
cost, which makes them attractive for general purpose scientific computing. In
this paper we show how graphics processors can be used for N-body simulations
to obtain improvements in performance over current generation CPUs. We have
developed a highly optimized algorithm for performing the O(N^2) force
calculations that constitute the major part of stellar and molecular dynamics
simulations. In some of the calculations, we achieve sustained performance of
nearly 100 GFlops on an ATI X1900XTX. The performance on GPUs is comparable to
specialized processors such as GRAPE-6A and MDGRAPE-3, but at a fraction of the
cost. Furthermore, the wide availability of GPUs has significant implications
for cluster computing and distributed computing efforts like Folding@Home."
"Within the context of the nascent e-Science infrastructure in Venezuela, we
describe several web-based scientific applications developed at the Centro
Nacional de Calculo Cientifico Universidad de Los Andes (CeCalCULA), Merida,
and at the Instituto Venezolano de Investigaciones Cientificas (IVIC), Caracas.
The different strategies that have been followed for implementing quantum
chemistry and atomic physics applications are presented. We also briefly
discuss a damage portal based on dynamic, nonlinear, finite elements of lumped
damage mechanics and a biomedical portal developed within the framework of the
\textit{E-Infrastructure shared between Europe and Latin America} (EELA)
initiative for searching common sequences and inferring their functions in
parasitic diseases such as leishmaniasis, chagas and malaria."
"The Recoil Growth algorithm, proposed in 1999 by Consta et al., is one of the
most efficient algorithm available in the literature to sample from a
multi-polymer system. Such problems are closely related to the generation of
self-avoiding paths. In this paper, we study a variant of the original Recoil
Growth algorithm, where we constrain the generation of a new polymer to take
place on a specific class of graphs. This makes it possible to make a fine
trade-off between computational cost and success rate. We moreover give a
simple proof for a lower bound on the irreducibility of this new algorithm,
which applies to the original algorithm as well."
"An adventure at engineering design and modeling is possible with a Virtual
Reality Environment (VRE) that uses multiple computer-generated media to let a
user experience situations that are temporally and spatially prohibiting. In
this paper, an approach to developing some advanced architecture and modeling
tools is presented to allow multiple frameworks work together while being
shielded from the application program. This architecture is being developed in
a framework of workbench interactive tools for next generation
nanoparticle-reinforced damping/dynamic systems. Through the use of system, an
engineer/programmer can respectively concentrate on tailoring an engineering
design concept of novel system and the application software design while using
existing databases/software outputs."
"Besides an indicator of the GDP, the Central Bank of Venezuela generates the
so called Monthly Economic Activity General Indicator. The a priori knowledge
of this indicator, which represents and sometimes even anticipates the
economy's fluctuations, could be helpful in developing public policies and in
investment decision making. The purpose of this study is forecasting the IGAEM
through non parametric methods, an approach that has proven effective in a wide
variety of problems in economics and finance."
"This paper intends to explain Venezuela's country spread behavior through the
Neural Networks analysis of a monthly economic activity general index of
economic indicators constructed by the Central Bank of Venezuela, a measure of
the shocks affecting country risk of emerging markets and the U.S. short term
interest rate. The use of non parametric methods allowed the finding of non
linear relationship between these inputs and the country risk. The networks
performance was evaluated using the method of excess predictability."
"A neural net model for forecasting the prices of Venezuelan crude oil is
proposed. The inputs of the neural net are selected by reference to a dynamic
system model of oil prices by Mashayekhi (1995, 2001) and its performance is
evaluated using two criteria: the Excess Profitability test by Anatoliev and
Gerko (2005) and the characteristics of the equity curve generated by a trading
strategy based on the neural net predictions.
  -----
  Se introduce aqui un modelo no parametrico para pronosticar los precios del
petroleo Venezolano cuyos insumos son seleccionados en base a un sistema
dinamico que explica los precios en terminos de dichos insumos. Se describe el
proceso de recoleccion y pre-procesamiento de datos y la corrida de la red y se
evaluan sus pronosticos a traves de un test estadistico de predictibilidad y de
las caracteristicas del Equity Curve inducido por la estrategia de compraventa
bursatil generada por dichos pronosticos."
"This paper focuses on super helical memory system's design, 'Engineering,
Architectural and Satellite Communications' as a theoretical approach of an
invention-model to 'store time-data'. The current release entails three
concepts: 1- an in-depth theoretical physics engineering of the chip including
its, 2- architectural concept based on VLSI methods, and 3- the time-data
versus data-time algorithm. The 'Parallel Time Varying & Data Super-helical
Access Memory' (PTVD-SHAM), possesses a waterfall effect in its architecture
dealing with the process of voltage output-switch into diverse logic and
quantum states described as 'Boolean logic & image-logic', respectively.
Quantum dot computational methods are explained by utilizing coiled carbon
nanotubes (CCNTs) and CNT field effect transistors (CNFETs) in the chip's
architecture. Quantum confinement, categorized quantum well substrate, and
B-field flux involvements are discussed in theory. Multi-access of coherent
sequences of 'qubit addressing' in any magnitude, gained as pre-defined, here
e.g., the 'big O notation' asymptotically confined into singularity while
possessing a magnitude of 'infinity' for the orientation of array displacement.
Gaussian curvature of k<0 versus k'>(k<0) is debated in aim of specifying the
2D electron gas characteristics, data storage system for defining short and
long time cycles for different CCNT diameters where space-time continuum is
folded by chance for the particle. Precise pre/post data timing for, e.g.,
seismic waves before earthquake mantle-reach event occurrence, including time
varying self-clocking devices in diverse geographic locations for radar systems
is illustrated in the Subsections of the paper. The theoretical fabrication
process, electromigration between chip's components is discussed as well."
"The elucidation upon fly's neuronal patterns as a link to computer graphics
and memory cards I/O's, is investigated for the phenomenon by propounding a
unified theory of Einstein's two known relativities. It is conclusive that
flies could contribute a certain amount of neuromatrices indicating an imagery
function of a visual-computational system into computer graphics and storage
systems. The visual system involves the time aspect, whereas flies possess
faster pulses compared to humans' visual ability due to the E-field state on an
active fly's eye surface. This behaviour can be tested on a dissected fly
specimen at its ommatidia. Electro-optical contacts and electrodes are wired
through the flesh forming organic emitter layer to stimulate light emission,
thereby to a computer circuit. The next step is applying a threshold voltage
with secondary voltages to the circuit denoting an array of essential
electrodes for bit switch. As a result, circuit's dormant pulses versus active
pulses at the specimen's area are recorded. The outcome matrix possesses a
construction of RGB and time radicals expressing the time problem in
consumption, allocating time into computational algorithms, enhancing the
technology far beyond. The obtained formulation generates consumed distance
cons(x), denoting circuital travel between data source/sink for pixel data and
bendable wavelengths. Once 'image logic' is in place, incorporating this point
of graphical acceleration permits one to enhance graphics and optimize
immensely central processing, data transmissions between memory and computer
visual system. The phenomenon can be mainly used in 360-deg. display/viewing,
3D scanning techniques, military and medicine, a robust and cheap substitution
for e.g. pre-motion pattern analysis, real-time rendering and LCDs."
"One source of disturbance in a pulsed T-ray signal is attributed to ambient
water vapor. Water molecules in the gas phase selectively absorb T-rays at
discrete frequencies corresponding to their molecular rotational transitions.
This results in prominent resonances spread over the T-ray spectrum, and in the
time domain the T-ray signal is observed as fluctuations after the main pulse.
These effects are generally undesired, since they may mask critical
spectroscopic data. So, ambient water vapor is commonly removed from the T-ray
path by using a closed chamber during the measurement. Yet, in some
applications a closed chamber is not applicable. This situation, therefore,
motivates the need for another method to reduce these unwanted artifacts. This
paper presents a study on a computational means to address the problem.
Initially, a complex frequency response of water vapor is modeled from a
spectroscopic catalog. Using a deconvolution technique, together with fine
tuning of the strength of each resonance, parts of the water-vapor response are
removed from a measured T-ray signal, with minimal signal distortion."
"This paper mainly discusses the American option's hedging strategies via
binomialmodel and the basic idea of pricing and hedging American option.
Although the essential scheme of hedging is almost the same as European option,
small differences may arise when simulating the process for American option
holder has more rights, spelling that the option can be exercised at anytime
before its maturity. Our method is dynamic-hedging method."
"A mathematical model is developed that captures the transport of liquid water
in hardened concrete, as well as the chemical reactions that occur between the
imbibed water and the residual calcium silicate compounds residing in the
porous concrete matrix. The main hypothesis in this model is that the reaction
product -- calcium silicate hydrate gel -- clogs the pores within the concrete
thereby hindering water transport. Numerical simulations are employed to
determine the sensitivity of the model solution to changes in various physical
parameters, and compare to experimental results available in the literature."
"Ontario (Canada) Health System stakeholders support the idea and necessity of
the integrated source of data that would include both clinical (e.g. diagnosis,
intervention, length of stay, case mix group) and financial (e.g. cost per
weighted case, cost per diem) characteristics of the Ontario healthcare system
activities at the patient-specific level. At present, the actual patient-level
case costs in the explicit form are not available in the financial databases
for all hospitals. The goal of this research effort is to develop financial
models that will assign each clinical case in the patient-specific data
warehouse a dollar value, representing the cost incurred by the Ontario health
care facility which treated the patient. Five mathematical models have been
developed and verified using real dataset. All models can be classified into
two groups based on their underlying method: 1. Models based on using relative
intensity weights of the cases, and 2. Models based on using cost per diem."
"Wireless Fidelity (WiFi) is the fastest growing wireless technology to date.
In addition to providing wire-free connectivity to the Internet WiFi technology
also enables mobile devices to connect directly to each other and form highly
dynamic wireless adhoc networks. Such distributed networks can be used to
perform cooperative communication tasks such ad data routing and information
dissemination in the absence of a fixed infrastructure. Furthermore, adhoc
grids composed of wirelessly networked portable devices are emerging as a new
paradigm in grid computing. In this paper we review computational and
algorithmic challenges of high-fidelity simulations of such WiFi-based wireless
communication and computing networks, including scalable topology maintenance,
mobility modelling, parallelisation and synchronisation. We explore
similarities and differences between the simulations of these networks and
simulations of interacting many-particle systems, such as molecular dynamics
(MD) simulations. We show how the cell linked-list algorithm which we have
adapted from our MD simulations can be used to greatly improve the
computational performance of wireless network simulators in the presence of
mobility, and illustrate with an example from our simulation studies of worm
attacks on mobile wireless adhoc networks."
"We present a method of discrete modeling and analysis of multilevel dynamics
of complex large-scale hierarchical dynamic systems subject to external dynamic
control mechanism. Architectural model of information system supporting
simulation and analysis of dynamic processes and development scenarios
(strategies) of complex large-scale hierarchical systems is also proposed."
"A new numerical algorithm for solving the symmetric eigenvalue problem is
presented. The technique deviates fundamentally from the traditional Krylov
subspace iteration based techniques (Arnoldi and Lanczos algorithms) or other
Davidson-Jacobi techniques, and takes its inspiration from the contour
integration and density matrix representation in quantum mechanics. It will be
shown that this new algorithm - named FEAST - exhibits high efficiency,
robustness, accuracy and scalability on parallel architectures. Examples from
electronic structure calculations of Carbon nanotubes (CNT) are presented, and
numerical performances and capabilities are discussed."
"In recent years, with the development of microarray technique, discovery of
useful knowledge from microarray data has become very important. Biclustering
is a very useful data mining technique for discovering genes which have similar
behavior. In microarray data, several objectives have to be optimized
simultaneously and often these objectives are in conflict with each other. A
Multi Objective model is capable of solving such problems. Our method proposes
a Hybrid algorithm which is based on the Multi Objective Particle Swarm
Optimization for discovering biclusters in gene expression data. In our method,
we will consider a low level of overlapping amongst the biclusters and try to
cover all elements of the gene expression matrix. Experimental results in the
bench mark database show a significant improvement in both overlap among
biclusters and coverage of elements in the gene expression matrix."
"In this study, we investigate the appeared complexity of two-phase flow
(air/water) in a heterogeneous soil where the supposed porous media is
non-deformable media which is under the timedependent gas pressure. After
obtaining of governing equations and considering the capillary
pressuresaturation and permeability functions, the evolution of the model
unknown parameters were obtained. In this way, using COMSOL (FEMLAB) and fluid
flow/script Module, the role of heterogeneity in intrinsic permeability was
analysed. Also, the evolution of relative permeability of wetting and
non-wetting fluid, capillary pressure and other parameters were elicited. In
the last part, a complex network approach to analysis of emerged patterns will
be employed."
"The fields of computing and biology have begun to cross paths in new ways. In
this paper a review of the current research in biological computing is
presented. Fundamental concepts are introduced and these foundational elements
are explored to discuss the possibilities of a new computing paradigm. We
assume the reader to possess a basic knowledge of Biology and Computer Science"
"Development of computer technology in chemistry, bring many application of
chemistry. Not only the application to visualize the structure of molecule but
also to molecular dynamics simulation. One of them is Gromacs. Gromacs is an
example of molecular dynamics application developed by Groningen University.
This application is a non-commercial and able to work in the operating system
Linux. The main ability of Gromacs is to perform molecular dynamics simulation
and minimization energy. In this paper, the author discusses about how to work
Gromacs in molecular dynamics simulation of some protein. In the molecular
dynamics simulation, Gromacs does not work alone. Gromacs interact with pymol
and Grace. Pymol is an application to visualize molecule structure and Grace is
an application in Linux to display graphs. Both applications will support
analysis of molecular dynamics simulation."
"We explore the application of graph coloring to biological networks,
specifically protein-protein interaction (PPI) networks. First, we find that
given similar conditions (i.e. number of nodes, number of links, degree
distribution and clustering), fewer colors are needed to color disassortative
(high degree nodes tend to connect to low degree nodes and vice versa) than
assortative networks. Fewer colors create fewer independent sets which in turn
imply higher concurrency potential for a network. Since PPI networks tend to be
disassortative, we suggest that in addition to functional specificity and
stability proposed previously by Maslov and Sneppen (Science 296, 2002), the
disassortative nature of PPI networks may promote the ability of cells to
perform multiple, crucial and functionally diverse tasks concurrently. Second,
since graph coloring is closely related to the presence of cliques in a graph,
the significance of node coloring information to the problem of identifying
protein complexes, i.e. dense subgraphs in a PPI network, is investigated. We
find that for PPI networks where 1% to 11% of nodes participate in at least one
identified protein complex, such as H. sapien (DIP20070219, DIP20081014 and
HPRD070609), DSATUR (a well-known complete graph coloring algorithm) node
coloring information can improve the quality (homogeneity and separation) of
initial candidate complexes. This finding may help to improve existing protein
complex detection methods, and/or suggest new methods."
"In this study, we investigate the complexity of two-phase flow (air/water) in
a heterogeneous soil sample by using complex network theory, where the supposed
porous media is non-deformable media, under the time-dependent gas pressure.
Based on the different similarity measurements (i.e., correlation, Euclidean
metrics) over the emerged patterns from the evolution of saturation of
non-wetting phase of a multi-heterogeneous soil sample, the emerged complex
networks are recognized. Understanding of the properties of complex networks
(such degree distribution, mean path length, clustering coefficient) can be
supposed as a way to analysis of variation of saturation profiles structures
(as the solution of finite element method on the coupled PDEs) where complexity
is coming from the changeable connection and links between assumed nodes. Also,
the path of evolution of the supposed system will be illustrated on the state
space of networks either in correlation and Euclidean measurements. The results
of analysis showed in a closed system the designed complex networks approach to
small world network where the mean path length and clustering coefficient are
low and high, respectively. As another result, the evolution of macro -states
of system (such mean velocity of air or pressure) can be scaled with
characteristics of structure complexity of saturation. In other part, we tried
to find a phase transition criterion based on the variation of non-wetting
phase velocity profiles over a network which had been constructed over
correlation distance."
"Signalling pathways are abstractions that help life scientists structure the
coordination of cellular activity. Cross-talk between pathways accounts for
many of the complex behaviours exhibited by signalling pathways and is often
critical in producing the correct signal-response relationship. Formal models
of signalling pathways and cross-talk in particular can aid understanding and
drive experimentation. We define an approach to modelling based on the concept
that a pathway is the (synchronising) parallel composition of instances of
generic modules (with internal and external labels). Pathways are then composed
by (synchronising) parallel composition and renaming; different types of
cross-talk result from different combinations of synchronisation and renaming.
We define a number of generic modules in PRISM and five types of cross-talk:
signal flow, substrate availability, receptor function, gene expression and
intracellular communication. We show that Continuous Stochastic Logic
properties can both detect and distinguish the types of cross-talk. The
approach is illustrated with small examples and an analysis of the cross-talk
between the TGF-b/BMP, WNT and MAPK pathways."
"Compositionality is a key feature of process algebras which is often cited as
one of their advantages as a modelling technique. It is certainly true that in
biochemical systems, as in many other systems, model construction is made
easier in a formalism which allows the problem to be tackled compositionally.
In this paper we consider the extent to which the compositional structure which
is inherent in process algebra models of biochemical systems can be exploited
during model solution. In essence this means using the compositional structure
to guide decomposed solution and analysis.
  Unfortunately the dynamic behaviour of biochemical systems exhibits strong
interdependencies between the components of the model making decomposed
solution a difficult task. Nevertheless we believe that if such decomposition
based on process algebras could be established it would demonstrate substantial
benefits for systems biology modelling. In this paper we present our
preliminary investigations based on a case study of the pheromone pathway in
yeast, modelling in the stochastic process algebra Bio-PEPA."
"Up to now, it is not possible to obtain analytical solutions for complex
molecular association processes (e.g. Molecule recognition in Signaling or
catalysis). Instead Brownian Dynamics (BD) simulations are commonly used to
estimate the rate of diffusional association, e.g. to be later used in
mesoscopic simulations. Meanwhile a portfolio of diffusional association (DA)
methods have been developed that exploit BD.
  However, DA methods do not clearly distinguish between modeling, simulation,
and experiment settings. This hampers to classify and compare the existing
methods with respect to, for instance model assumptions, simulation
approximations or specific optimization strategies for steering the computation
of trajectories.
  To address this deficiency we propose FADA (Flexible Architecture for
Diffusional Association) - an architecture that allows the flexible definition
of the experiment comprising a formal description of the model in SpacePi,
different simulators, as well as validation and analysis methods. Based on the
NAM (Northrup-Allison-McCammon) method, which forms the basis of many existing
DA methods, we illustrate the structure and functioning of FADA. A discussion
of future validation experiments illuminates how the FADA can be exploited in
order to estimate reaction rates and how validation techniques may be applied
to validate additional features of the model."
"Molecular interactions are wired in a fascinating way resulting in complex
behavior of biological systems. Theoretical modeling provides a useful
framework for understanding the dynamics and the function of such networks. The
complexity of the biological networks calls for conceptual tools that manage
the combinatorial explosion of the set of possible interactions. A suitable
conceptual tool to attack complexity is compositionality, already successfully
used in the process algebra field to model computer systems. We rely on the
BlenX programming language, originated by the beta-binders process calculus, to
specify and simulate high-level descriptions of biological circuits. The
Gillespie's stochastic framework of BlenX requires the decomposition of
phenomenological functions into basic elementary reactions. Systematic
unpacking of complex reaction mechanisms into BlenX templates is shown in this
study. The estimation/derivation of missing parameters and the challenges
emerging from compositional model building in stochastic process algebras are
discussed. A biological example on circadian clock is presented as a case study
of BlenX compositionality."
"We formally characterize a set of causality-based properties of metabolic
networks. This set of properties aims at making precise several notions on the
production of metabolites, which are familiar in the biologists' terminology.
  From a theoretical point of view, biochemical reactions are abstractly
represented as causal implications and the produced metabolites as causal
consequences of the implication representing the corresponding reaction. The
fact that a reactant is produced is represented by means of the chain of
reactions that have made it exist. Such representation abstracts away from
quantities, stoichiometric and thermodynamic parameters and constitutes the
basis for the characterization of our properties. Moreover, we propose an
effective method for verifying our properties based on an abstract model of
system dynamics. This consists of a new abstract semantics for the system seen
as a concurrent network and expressed using the Chemical Ground Form calculus.
  We illustrate an application of this framework to a portion of a real
metabolic pathway."
"This volume contains the papers presented at the 3rd Workshop ""From Biology
To Concurrency and back"", FBTC 2010, held in Paphos, Cyprus, on March 27, 2010,
as satellite event of the Joint European Conference on Theory and Practice of
Software, ETAPS 2010.
  The Workshop aimed at gathering together researchers with special interest at
the convergence of life and computer science, with particular focus on the
application of techniques and methods from concurrency. The papers contained in
this volume present works on modelling, analysis, and validation of biological
behaviours using concurrency-inspired methods and platforms, and bio-inspired
models and tools for describing distributed interactions."
"Predicting protein structure from amino acid sequence is one of the most
important unsolved problems of molecular biology and biophysics.Not only would
a successful prediction algorithm be a tremendous advance in the understanding
of the biochemical mechanisms of proteins, but, since such an algorithm could
conceivably be used to design proteins to carry out specific
functions.Prediction of the secondary structure of a protein (alpha-helix,
beta-sheet, coil) is an important step towards elucidating its three
dimensional structure as well as its function. In this research, we use
different Hidden Markov models for protein secondary structure prediction. In
this paper we have proposed an algorithm for predicting protein secondary
structure. We have used Hidden Markov model with sliding window for secondary
structure prediction.The secondary structure has three regular forms, for each
secondary structural element we are using one Hidden Markov Model."
"Given the return series for a set of instruments, a \emph{trading strategy}
is a switching function that transfers wealth from one instrument to another at
specified times. We present efficient algorithms for constructing (ex-post)
trading strategies that are optimal with respect to the total return, the
Sterling ratio and the Sharpe ratio. Such ex-post optimal strategies are useful
analysis tools. They can be used to analyze the ""profitability of a market"" in
terms of optimal trading; to develop benchmarks against which real trading can
be compared; and, within an inductive framework, the optimal trades can be used
to to teach learning systems (predictors) which are then used to identify
future trading opportunities."
"This paper presents implementation details and empirical results for a hybrid
message passing and shared memory paralleliziation of the adaptive integral
method (AIM). AIM is implemented on a (near) petaflop supercomputing cluster of
quad-core processors and its accuracy, complexity, and scalability are
investigated by solving benchmark scattering problems. The timing and speedup
results on up to 1024 processors show that the hybrid MPI/OpenMP
parallelization of AIM exhibits better strong scalability (fixed problem size
speedup) than pure MPI parallelization of it when multiple cores are used on
each processor."
"This study presents a new visualization tool for classification of satellite
imagery. Visualization of feature space allows exploration of patterns in the
image data and insight into the classification process and related uncertainty.
Visual Data Mining provides added value to image classifications as the user
can be involved in the classification process providing increased confidence in
and understanding of the results. In this study, we present a prototype
visualization tool for visual data mining (VDM) of satellite imagery. The
visualization tool is showcased in a classification study of highresolution
imageries of Latur district in Maharashtra state of India."
"This work aims to provide standard formulations for direct minimization
approaches on various types of static problems of continuum mechanics.
Particularly, form-finding problems of tension structures are discussed in the
first half and the large deformation problems of continuum bodies are discussed
in the last half. In the first half, as the standards of iterative direct
minimization strategies, two types of simple recursive methods are presented,
namely the two-term method and the three-term method. The dual estimate is also
introduced as a powerful means of involving equally constraint conditions into
minimization problems. As examples of direct minimization approaches on usual
engineering issues, some form finding problems of tension structures which can
be solved by the presented strategies are illustrated. Additionally, it is
pointed out that while the two-term method sometimes becomes useless, the
three-term method always provides remarkable rate of global convergence
efficiency. Then, to show the potential ability of the three-term method, in
the last part of this work, some principle of virtual works which usually
appear in the continuum mechanics are approximated and discretized in a common
manner, which are suitable to be solved by the three-term method. Finally, some
large deformation analyses of continuum bodies which can be solved by the
three-term method are presented."
"This volume contains the final versions of the papers presented at the 3rd
International Workshop on Computational Models for Cell Processes (CompMod
2011). The workshop took place on September 10, 2011 at the University of
Aachen, Germany, in conjunction with CONCUR 2011. The first edition of the
workshop (2008) took place in Turku, Finland, in conjunction with Formal
Methods 2008 and the second edition (2009) took place in Eindhoven, the
Netherlands, as well in conjunction with Formal Methods 2009. The goal of the
CompMod workshop series is to bring together researchers in Computer Science
(especially in Formal Methods) and Mathematics (both discrete and continuous),
interested in the opportunities and the challenges of Systems Biology."
"In this paper we propose a systematic approach to construct mathematical
models describing populations of cancer-cells at different stages of disease
development. The methodology we propose is based on stochastic Concurrent
Constraint Programming, a flexible stochastic modelling language. The
methodology is tested on (and partially motivated by) the study of prostate
cancer. In particular, we prove how our method is suitable to systematically
reconstruct different mathematical models of prostate cancer growth - together
with interactions with different kinds of hormone therapy - at different levels
of refinement."
"Semantic equivalences are used in process algebra to capture the notion of
similar behaviour, and this paper proposes a semi-quantitative equivalence for
a stochastic process algebra developed for biological modelling. We consider
abstracting away from fast reactions as suggested by the Quasi-Steady-State
Assumption. We define a fast-slow bisimilarity based on this idea. We also show
congruence under an appropriate condition for the cooperation operator of
Bio-PEPA. The condition requires that there is no synchronisation over fast
actions, and this distinguishes fast-slow bisimilarity from weak bisimilarity.
We also show congruence for an operator which extends the reactions available
for a species. We characterise models for which it is only necessary to
consider the matching of slow transitions and we illustrate the equivalence on
two models of competitive inhibition."
"In this paper we present a minimal object oriented core calculus for
modelling the biological notion of type that arises from biological ontologies
in formalisms based on term rewriting. This calculus implements encapsulation,
method invocation, subtyping and a simple formof overriding inheritance, and it
is applicable to models designed in the most popular term-rewriting formalisms.
The classes implemented in a formalism can be used in several models, like
programming libraries."
"A one-step, a two-step, an abridged, a skeletal and four detailed kinetic
schemes of hydrogen oxidation have been tested. A new skeletal kinetic scheme
of hydrogen oxidation has been developed. The CFD calculations were carried out
using ANSYS CFX software. Ignition delay times and speeds of flames were
derived from the computational results. The computational data obtained using
ANSYS CFX and CHEMKIN, and experimental data were compared. The precision,
reliability, and range of validity of the kinetic schemes in CFD simulations
were estimated. The impact of kinetic scheme on the results of computations was
discussed. The relationship between grid spacing, timestep, accuracy, and
computational cost were analyzed."
"We consider a class of inverse problems where it is possible to aggregate the
results of multiple experiments. This class includes problems where the forward
model is the solution operator to linear ODEs or PDEs. The tremendous size of
such problems motivates dimensionality reduction techniques based on randomly
mixing experiments. These techniques break down, however, when robust
data-fitting formulations are used, which are essential in cases of missing
data, unusually large errors, and systematic features in the data unexplained
by the forward model. We survey robust methods within a statistical framework,
and propose a semistochastic optimization approach that allows dimensionality
reduction. The efficacy of the methods are demonstrated for a large-scale
seismic inverse problem using the robust Student's t-distribution, where a
useful synthetic velocity model is recovered in the extreme scenario of 60%
data missing at random. The semistochastic approach achieves this recovery
using 20% of the effort required by a direct robust approach."
"At the heart of many scientific applications is the solution of algebraic
systems, such as linear systems of equations, eigenvalue problems, and
optimization problems, to name a few. TOPS, which stands for Towards Optimal
Petascale Simulations, is a SciDAC applied math center focused on the
development of solvers for tackling these algebraic systems, as well as the
deployment of such technologies in large-scale scientific applications of
interest to the U.S. Department of Energy. In this paper, we highlight some of
the solver technologies we have developed in optimization and matrix
computations. We also describe some accomplishments achieved using these
technologies in UNEDF, a SciDAC application project on nuclear physics."
"A fully coupled transient heat and moisture transport in a masonry structure
is examined in this paper. Supported by several successful applications in
civil engineering the nonlinear diffusion model proposed by K\""{u}nzel is
adopted in the present study. A strong material heterogeneity together with a
significant dependence of the model parameters on initial conditions as well as
the gradients of heat and moisture fields vindicates the use of a hierarchical
modeling strategy to solve the problem of this kind. Attention is limited to
the classical first order homogenization in a spatial domain developed here in
the framework of a two step (meso-macro) multi-scale computational scheme (FE^2
problem). Several illustrative examples are presented to investigate the
influence of transient flow at the level of constituents (meso-scale) on the
macroscopic response including the effect of macro-scale boundary conditions. A
two-dimensional section of Charles Bridge subjected to actual climatic
conditions is analyzed next to confirm the suitability of algorithmic format of
FE^2 scheme for the parallel computing."
"This paper presents a model for the simulation of liquid-gas-solid flows by
means of the lattice Boltzmann method. The approach is built upon previous
works for the simulation of liquid-solid particle suspensions on the one hand,
and on a liquid-gas free surface model on the other. We show how the two
approaches can be unified by a novel set of dynamic cell conversion rules. For
evaluation, we concentrate on the rotational stability of non-spherical rigid
bodies floating on a plane water surface - a classical hydrostatic problem
known from naval architecture. We show the consistency of our method in this
kind of flows and obtain convergence towards the ideal solution for the
measured heeling stability of a floating box."
"This short paper presents an abstract, tunable model of genomic structural
change within the cell lifecycle and explores its use with simulated evolution.
A well-known Boolean model of genetic regulatory networks is extended to
include changes in node connectivity based upon the current cell state, e.g.,
via transposable elements. The underlying behaviour of the resulting dynamical
networks is investigated before their evolvability is explored using a version
of the NK model of fitness landscapes. Structural dynamism is found to be
selected for in non-stationary environments and subsequently shown capable of
providing a mechanism for evolutionary innovation when such reorganizations are
inherited."
"The traditional methods of the biology, based on illustrative descriptions
and linear logic explanations, are discussed. This work aims to improve this
approach by introducing alternative tools to describe and represent complex
biological systems. Two models were developed, one mathematical and another
computational, both were made in order to study the biological process between
free radicals and antioxidants. Each model was used to study the same process
but in different scenarios. The mathematical model was used to study the
biological process in an epithelial cells culture; this model was validated
with the experimental data of Anne Hanneken's research group from the
Department of Molecular and Experimental Medicine, published by the journal
Investigative Ophthalmology and Visual Science in July 2006. The computational
model was used to study the same process in an individual. The model was made
using C++ programming language, supported by the network theory of aging."
"Sensitivity analysis plays an important role in the understanding of complex
models. It helps to identify influence of input parameters in relation to the
outputs. It can be also a tool to understand the behavior of the model and then
can help in its development stage. This study aims to analyze and illustrate
the potential usefulness of combining first and second-order sensitivity
analysis, applied to a building energy model (ESP-r). Through the example of a
collective building, a sensitivity analysis is performed using the method of
elementary effects (also known as Morris method), including an analysis of
interactions between the input parameters (second order analysis). Importance
of higher-order analysis to better support the results of first order analysis,
highlighted especially in such complex model. Several aspects are tackled to
implement efficiently the multi-order sensitivity analysis: interval size of
the variables, management of non-linearity, usefulness of various outputs."
"CSVM (CSV with Metadata) is a simple file format for tabular data. The
possible application domain is the same as typical spreadsheets files, but CSVM
is well suited for long term storage and the inter-conversion of RAW data. CSVM
embeds different levels for data, metadata and annotations in human readable
format and flat ASCII files. As a proof of concept, Perl and Python toolkits
were designed in order to handle CSVM data and objects in workflows. These
parsers can process CSVM files independently of data types, so it is possible
to use same data format and parser for a lot of scientific purposes. CSVM-1 is
the first version of CSVM specification, an extension of CSVM-1 for
implementing a translation system between CSVM files is presented in this
paper. The necessary data used to make the translation are also coded in
another CSVM file. This particular kind of CSVM is called a CSVM dictionary, it
is also readable by the current CSVM parser and it is fully supported by the
Python toolkit. This report presents a proposal for CSVM dictionaries, a
working example in chemistry, and some elements of Python toolkit usable to
handle these files."
"We present a new method for parameter identification of ODE system
descriptions based on data measurements. Our method works by splitting the
system into a number of subsystems and working on each of them separately,
thereby being easily parallelisable, and can also deal with noise in the
observations."
"Tumors constitute a wide family of diseases kinetically characterized by the
co-presence of multiple spatio-temporal scales. So, tumor cells ecologically
interplay with other kind of cells, e.g. endothelial cells or immune system
effectors, producing and exchanging various chemical signals. As such, tumor
growth is an ideal object of hybrid modeling where discrete stochastic
processes model agents at low concentrations, and mean-field equations model
chemical signals. In previous works we proposed a hybrid version of the
well-known Panetta-Kirschner mean-field model of tumor cells, effector cells
and Interleukin-2. Our hybrid model suggested -at variance of the inferences
from its original formulation- that immune surveillance, i.e. tumor elimination
by the immune system, may occur through a sort of side-effect of large
stochastic oscillations. However, that model did not account that, due to both
chemical transportation and cellular differentiation/division, the
tumor-induced recruitment of immune effectors is not instantaneous but,
instead, it exhibits a lag period. To capture this, we here integrate a
mean-field equation for Interleukins-2 with a bi-dimensional delayed stochastic
process describing such delayed interplay. An algorithm to realize trajectories
of the underlying stochastic process is obtained by coupling the Piecewise
Deterministic Markov process (for the hybrid part) with a Generalized
Semi-Markovian clock structure (to account for delays). We (i) relate tumor
mass growth with delays via simulations and via parametric sensitivity analysis
techniques, (ii) we quantitatively determine probabilistic eradication times,
and (iii) we prove, in the oscillatory regime, the existence of a heuristic
stochastic bifurcation resulting in delay-induced tumor eradication, which is
neither predicted by the mean-field nor by the hybrid non-delayed models."
"This paper presents novel extensions and applications of the UPPAAL-SMC model
checker. The extensions allow for statistical model checking of stochastic
hybrid systems. We show how our race-based stochastic semantics extends to
networks of hybrid systems, and indicate the integration technique applied for
implementing this semantics in the UPPAAL-SMC simulation engine. We report on
two applications of the resulting tool-set coming from systems biology and
energy aware buildings."
"One 'problem' with the 21st century world, particularly the economic and
business worlds, is the phenomenal and increasing number of interconnections
between economic agents (consumers, firms, banks, markets, national economies).
This implies that such agents are all interacting and consequently giving raise
to enormous degrees of non-linearity, a.k.a. complexity. Complexity often
brings with it unexpected phenomena, such as chaos and emerging behaviour, that
can become challenges for the survival of economic agents and systems.
Developing econophysics approaches are beginning to apply, to the 'economic
web', methods and models that have been used in physics and/or systems theory
to tackle non-linear domains. The paper gives an account of the research in
progress in this field and shows its implications for enteprise information
systems, anticipating the emergence of software that will allow to reflect the
complexity of the business world, as holistic risk management becomes a mandate
for financial institutions and business organizations."
"A wide range of engineering design problems have been solved by the
algorithms that simulates collective intelligence in swarms of birds or
insects. The Artificial Bee Colony or ABC is one of the recent additions to the
class of swarm intelligence based algorithms that mimics the foraging behavior
of honey bees. ABC consists of three groups of bees namely employed, onlooker
and scout bees. In ABC, the food locations represent the potential candidate
solution. In the present study an attempt is made to generate the population of
food sources (Colony Size) adaptively and the variant is named as A-ABC. A-ABC
is further enhanced to improve convergence speed and exploitation capability,
by employing the concept of elitism, which guides the bees towards the best
food source. This enhanced variant is called E-ABC. The proposed algorithms are
validated on a set of standard benchmark problems with varying dimensions taken
from literature and on five engineering design problems. The numerical results
are compared with the basic ABC and three recent variant of ABC. Numerically
and statistically simulated results illustrate that the proposed method is very
efficient and competitive."
"The prevention of dangerous chemical accidents is a primary problem of
industrial manufacturing. In the accidents of dangerous chemicals, the oil gas
explosion plays an important role. The essential task of the explosion
prevention is to estimate the better explosion limit of a given oil gas. In
this paper, Support Vector Machines (SVM) and Logistic Regression (LR) are used
to predict the explosion of oil gas. LR can get the explicit probability
formula of explosion, and the explosive range of the concentrations of oil gas
according to the concentration of oxygen. Meanwhile, SVM gives higher accuracy
of prediction. Furthermore, considering the practical requirements, the effects
of penalty parameter on the distribution of two types of errors are discussed."
"In the last four years, daily deals have emerged from nowhere to become a
multi-billion dollar industry world-wide. Daily deal sites such as Groupon and
Livingsocial offer products and services at deep discounts to consumers via
email and social networks. As the industry matures, there are many questions
regarding the impact of daily deals on the marketplace. Important questions in
this regard concern the reasons why businesses decide to offer daily deals and
their longer-term impact on businesses. In the present paper, we investigate
whether the unobserved factors that make marketers run daily deals are
correlated with the unobserved factors that influence the business, In
particular, we employ the framework of seemingly unrelated regression to model
the correlation between the errors in predicting whether a business uses a
daily deal and the errors in predicting the business' survival. Our analysis
consists of the survival of 985 small businesses that offered daily deals
between January and July 2011 in the city of Chicago. Our results indicate that
there is a statistically significant correlation between the unobserved factors
that influence the business' decision to offer a daily deal and the unobserved
factors that impact its survival. Furthermore, our results indicate that the
correlation coefficient is significant in certain business categories (e.g.
restaurants)."
"The high-throughput data generated by microarray experiments provides
complete set of genes being expressed in a given cell or in an organism under
particular conditions. The analysis of these enormous data has opened a new
dimension for the researchers. In this paper we describe a novel algorithm to
microarray data analysis focusing on the identification of genes that are
differentially expressed in particular internal or external conditions and
which could be potential drug targets. The algorithm uses the time-series gene
expression data as an input and recognizes genes which are expressed
differentially. This algorithm implements standard statistics-based gene
functional investigations, such as the log transformation, mean, log-sigmoid
function, coefficient of variations, etc. It does not use clustering analysis.
The proposed algorithm has been implemented in Perl. The time-series gene
expression data on yeast Saccharomyces cerevisiae from the Stanford Microarray
Database (SMD)consisting of 6154 genes have been taken for the validation of
the algorithm. The developed method extracted 48 genes out of total 6154 genes.
These genes are mostly responsible for the yeast's resistants at a high
temperature."
"In this paper, we deal with the problem of implementing an abstract machine
for a stochastic version of the Brane Calculus. Instead of defining an ad hoc
abstract machine, we consider the generic stochastic abstract machine
introduced by Lakin, Paulev\'e and Phillips. The nested structure of membranes
is flattened into a set of species where the hierarchical structure is
represented by means of names. In order to reduce the overhead introduced by
this encoding, we modify the machine by adding a copy-on-write optimization
strategy. We prove that this implementation is adequate with respect to the
stochastic structural operational semantics recently given for the Brane
Calculus. These techniques can be ported also to other stochastic calculi
dealing with nested structures."
"The paper describes concept and implementation details of integrating a
finite element module for dike stability analysis Virtual Dike into an early
warning system for flood protection. The module operates in real-time mode and
includes fluid and structural sub-models for simulation of porous flow through
the dike and for dike stability analysis. Real-time measurements obtained from
pore pressure sensors are fed into the simulation module, to be compared with
simulated pore pressure dynamics. Implementation of the module has been
performed for a real-world test case - an earthen levee protecting a sea-port
in Groningen, the Netherlands. Sensitivity analysis and calibration of
diffusivities have been performed for tidal fluctuations. An algorithm for
automatic diffusivities calibration for a heterogeneous dike is proposed and
studied. Analytical solutions describing tidal propagation in one-dimensional
saturated aquifer are employed in the algorithm to generate initial estimates
of diffusivities."
"We combine non-hydrostatic flow simulations of the free surface with a
discharge model based on elementary gate flow equations for decision support in
operation of hydraulic structure gates. A water level-based gate control used
in most of today's general practice does not take into account the fact that
gate operation scenarios producing similar total discharged volumes and similar
water levels may have different local flow characteristics. Accurate and timely
prediction of local flow conditions around hydraulic gates is important for
several aspects of structure management: ecology, scour, flow-induced gate
vibrations and waterway navigation. The modelling approach is described and
tested for a multi-gate sluice structure regulating discharge from a river to
the sea. The number of opened gates is varied and the discharge is stabilized
with automated control by varying gate openings. The free-surface model was
validated for discharge showing a correlation coefficient of 0.994 compared to
experimental data. Additionally, we show the analysis of CFD results for
evaluating bed stability and gate vibrations."
"In multi-domain proteins, the domains are connected by a flexible
unstructured region called as protein domain linker. The accurate demarcation
of these linkers holds a key to understanding of their biochemical and
evolutionary attributes. This knowledge helps in designing a suitable linker
for engineering stable multi-domain chimeric proteins. Here we propose a novel
method for the demarcation of the linker based on a three-dimensional protein
structure and a domain definition. The proposed method is based on biological
knowledge about structural flexibility of the linkers. We performed structural
analysis on a linker probable region (LPR) around domain boundary points of
known SCOP domains. The LPR was described using a set of overlapping peptide
fragments of fixed size. Each peptide fragment was then described by geometric
invariants (GIs) and subjected to clustering process where the fragments
corresponding to actual linker come up as outliers. We then discover the actual
linkers by finding the longest continuous stretch of outlier fragments from
LPRs. This method was evaluated on a benchmark dataset of 51 continuous
multi-domain proteins, where it achieves F1 score of 0.745 (0.83 precision and
0.66 recall). When the method was applied on 725 continuous multi-domain
proteins, it was able to identify novel linkers that were not reported
previously. This method can be used in combination with supervised / sequence
based linker prediction methods for accurate linker demarcation."
"Complexes of physically interacting proteins are one of the fundamental
functional units responsible for driving key biological mechanisms within the
cell. Their identification is therefore necessary not only to understand
complex formation but also the higher level organization of the cell. With the
advent of high-throughput techniques in molecular biology, significant amount
of physical interaction data has been cataloged from organisms such as yeast,
which has in turn fueled computational approaches to systematically mine
complexes from the network of physical interactions among proteins (PPI
network). In this survey, we review, classify and evaluate some of the key
computational methods developed till date for the identification of protein
complexes from PPI networks. We present two insightful taxonomies that reflect
how these methods have evolved over the years towards improving automated
complex prediction. We also discuss some open challenges facing accurate
reconstruction of complexes, the crucial ones being presence of high proportion
of errors and noise in current high-throughput datasets and some key aspects
overlooked by current complex detection methods. We hope this review will not
only help to condense the history of computational complex detection for easy
reference, but also provide valuable insights to drive further research in this
area."
"Data-mining techniques have frequently been developed for Spontaneous
reporting databases. These techniques aim to find adverse drug events
accurately and efficiently. Spontaneous reporting databases are prone to
missing information, under reporting and incorrect entries. This often results
in a detection lag or prevents the detection of some adverse drug events. These
limitations do not occur in electronic health-care databases. In this paper,
existing methods developed for spontaneous reporting databases are implemented
on both a spontaneous reporting database and a general practice electronic
health-care database and compared. The results suggests that the application of
existing methods to the general practice database may help find signals that
have gone undetected when using the spontaneous reporting system database. In
addition the general practice database provides far more supplementary
information, that if incorporated in analysis could provide a wealth of
information for identifying adverse events more accurately."
"This paper takes an approach to clustering domestic electricity load profiles
that has been successfully used with data from Portugal and applies it to UK
data. Clustering techniques are applied and it is found that the preferred
technique in the Portuguese work (a two stage process combining Self Organised
Maps and Kmeans) is not appropriate for the UK data. The work shows that up to
nine clusters of households can be identified with the differences in usage
profiles being visually striking. This demonstrates the appropriateness of
breaking the electricity usage patterns down to more detail than the two load
profiles currently published by the electricity industry. The paper details
initial results using data collected in Milton Keynes around 1990. Further work
is described and will concentrate on building accurate and meaningful clusters
of similar electricity users in order to better direct demand side management
initiatives to the most relevant target customers."
"Changes in the UK electricity market mean that domestic users will be
required to modify their usage behaviour in order that supplies can be
maintained. Clustering allows usage profiles collected at the household level
to be clustered into groups and assigned a stereotypical profile which can be
used to target marketing campaigns. Fuzzy C Means clustering extends this by
allowing each household to be a member of many groups and hence provides the
opportunity to make personalised offers to the household dependent on their
degree of membership of each group. In addition, feedback can be provided on
how user's changing behaviour is moving them towards more ""green"" or cost
effective stereotypical usage."
"In immune system simulation there are two competing simulation approaches:
System Dynamics Simulation (SDS) and Agent-Based Simulation (ABS). In the
literature there is little guidance on how to choose the best approach for a
specific immune problem. Our overall research aim is to develop a framework
that helps researchers with this choice. In this paper we investigate if it is
possible to easily convert simulation models between approaches. With no
explicit guidelines available from the literature we develop and test our own
set of guidelines for converting SDS models into ABS models in a non-spacial
scenario. We also define guidelines to convert ABS into SDS considering a
non-spatial and a spatial scenario. After running some experiments with the
developed models we found that in all cases there are significant differences
between the results produced by the different simulation methods."
"Adverse drug reaction (ADR) is widely concerned for public health issue. In
this study we propose an original approach to detect the ADRs using feature
matrix and feature selection. The experiments are carried out on the drug
Simvastatin. Major side effects for the drug are detected and better
performance is achieved compared to other computerized methods. The detected
ADRs are based on the computerized method, further investigation is needed."
"Classical deterministic simulations of epidemiological processes, such as
those based on System Dynamics, produce a single result based on a fixed set of
input parameters with no variance between simulations. Input parameters are
subsequently modified on these simulations using Monte-Carlo methods, to
understand how changes in the input parameters affect the spread of results for
the simulation. Agent Based simulations are able to produce different output
results on each run based on knowledge of the local interactions of the
underlying agents and without making any changes to the input parameters. In
this paper we compare the influence and effect of variation within these two
distinct simulation paradigms and show that the Agent Based simulation of the
epidemiological SIR (Susceptible, Infectious, and Recovered) model is more
effective at capturing the natural variation within SIR compared to an
equivalent model using System Dynamics with Monte-Carlo simulation. To
demonstrate this effect, the SIR model is implemented using both System
Dynamics (with Monte-Carlo simulation) and Agent Based Modelling based on
previously published empirical data."
"A physical model for the simulation ink/paper interaction at the mesoscopic
scale is developed. It is based on the modified Ising model, and is generalized
to consider the restriction of the finite-volume of ink and also its dynamic
seepage. This allows the model to obtain the ink distribution within the paper
volume. At the mesoscopic scale, the paper is modeled using a discretized fiber
structure. The ink distribution is obtained by solving its equivalent
optimization problem, which is solved using a modified genetic algorithm, along
with a new boundary condition and the quasi-linear technique. The model is able
to simulate the finite-volume distribution of ink."
"A new approach to the solution of Economic Dispatch using Particle Swarm
Optimization is presented. It is the progression of allocating production
amongst the dedicated units such that the restriction forced are fulfilled and
the power needs are reduced. More just, the soft computing method has received
supplementary concentration and was used in a quantity of successful and
sensible applications. Here, an attempt has been made to find out the minimum
cost by using Particle Swarm Optimization Algorithm using the data of three
generating units. In this work, data has been taken such as the loss
coefficients with the max-min power limit and cost function. PSO and Simulated
Annealing are functional to put out the least amount for dissimilar energy
requirements. When the outputs are compared with the conventional method, PSO
seems to give an improved result with enhanced convergence feature. All the
methods are executed in MATLAB environment. The effectiveness and feasibility
of the proposed method were demonstrated by three generating units case study.
Output gives hopeful results, signifying that the projected method of
calculation is competent of economically formative advanced eminence solutions
addressing economic dispatch problems."
"Microarrays are made it possible to simultaneously monitor the expression
profiles of thousands of genes under various experimental conditions.
Identification of co-expressed genes and coherent patterns is the central goal
in microarray or gene expression data analysis and is an important task in
bioinformatics research. Feature selection is a process to select features
which are more informative. It is one of the important steps in knowledge
discovery. The problem is that not all features are important. Some of the
features may be redundant, and others may be irrelevant and noisy. In this work
the unsupervised Gene selection method and Enhanced Center Initialization
Algorithm (ECIA) with K-Means algorithms have been applied for clustering of
Gene Expression Data. This proposed clustering algorithm overcomes the
drawbacks in terms of specifying the optimal number of clusters and
initialization of good cluster centroids. Gene Expression Data show that could
identify compact clusters with performs well in terms of the Silhouette
Coefficients cluster measure."
"Motivation: Since susceptibility to diseases increases with age, studying
aging gains importance. Analyses of gene expression or sequence data, which
have been indispensable for investigating aging, have been limited to studying
genes and their protein products in isolation, ignoring their connectivities.
However, proteins function by interacting with other proteins, and this is
exactly what biological networks (BNs) model. Thus, analyzing the proteins' BN
topologies could contribute to understanding of aging. Current methods for
analyzing systems-level BNs deal with their static representations, even though
cells are dynamic. For this reason, and because different data types can give
complementary biological insights, we integrate current static BNs with
aging-related gene expression data to construct dynamic, age-specific BNs.
Then, we apply sensitive measures of topology to the dynamic BNs to study
cellular changes with age.
  Results: While global BN topologies do not significantly change with age,
local topologies of a number of genes do. We predict such genes as
aging-related. We demonstrate credibility of our predictions by: 1) observing
significant overlap between our predicted aging-related genes and ""ground
truth"" aging-related genes; 2) showing that our aging-related predictions group
by functions and diseases that are different than functions and diseases of
genes that are not predicted as aging-related; 3) observing significant overlap
between functions and diseases that are enriched in our aging-related
predictions and those that are enriched in ""ground truth"" aging-related data;
4) providing evidence that diseases which are enriched in our aging-related
predictions are linked to human aging; and 5) validating all of our
high-scoring novel predictions via manual literature search."
"Microarray technology is a process that allows thousands of genes
simultaneously monitor to various experimental conditions. It is used to
identify the co-expressed genes in specific cells or tissues that are actively
used to make proteins, This method is used to analysis the gene expression, an
important task in bioinformatics research. Cluster analysis of gene expression
data has proved to be a useful tool for identifying co-expressed genes,
biologically relevant groupings of genes and samples. In this paper we analysed
K-Means with Automatic Generations of Merge Factor for ISODATA- AGMFI, to group
the microarray data sets on the basic of ISODATA. AGMFI is to generate initial
values for merge and Spilt factor, maximum merge times instead of selecting
efficient values as in ISODATA. The initial seeds for each cluster were
normally chosen either sequentially or randomly. The quality of the final
clusters was found to be influenced by these initial seeds. For the real life
problems, the suitable number of clusters cannot be predicted. To overcome the
above drawback the current research focused on developing the clustering
algorithms without giving the initial number of clusters."
"A prototype tool to assist architects during the early design stage of floor
plans has been developed, consisting of an Evolutionary Program for the Space
Allocation Problem (EPSAP), which generates sets of floor plan alternatives
according to the architect's preferences; and a Floor Plan Performance
Optimization Program (FPOP), which optimizes the selected solutions according
to thermal performance criteria. The design variables subject to optimization
are window position and size, overhangs, fins, wall positioning, and building
orientation. A procedure using a transformation operator with gradient descent,
such as behavior, coupled with a dynamic simulation engine was developed for
the thermal evaluation and optimization process. However, the need to evaluate
all possible alternatives regarding designing variables being used during the
optimization process leads to an intensive use of thermal simulation, which
dramatically increases the simulation time, rendering it unpractical. An
alternative approach is a smart optimization approach, which utilizes an
oriented and adaptive search technique to efficiently find the near optimum
solution. This paper presents the search methodology for the building
orientation of floor plan designs, and the corresponding efficiency and
effectiveness indicators. The calculations are based on 100 floor plan designs
generated by EPSAP. All floor plans have the same design program, location, and
weather data, changing only their geometry. Dynamic simulation of buildings was
effectively used together with the optimization procedure in this approach to
significantly improve the designs. The use of the orientation variable has been
included in the algorithm."
"Drug Names, Population Level Surveillance and the FDA's Adverse Event
Reporting System: An Exploratory Data Survey of Drug Name Incidence and
Prevalence, 2004-2012Q2 Purpose: To count and monitor the drug names reported
in the publicly available version of the Federal Adverse Event Reporting System
(FAERS) from 2004 to 2012Q2 in a maximized sensitivity relational model.
Methods: Data mining and data modeling was conducted and event based summary
statistics with plots were created from over nine continuous years of
continuous FAERS data. Results: This FAERS model contains 344,452 individual
drug names and 432,541,994 count references which occurred across 4,148,761
human subjects in the 34 quarter study period. Plots for the top 100 scoring
drug name references are reported by year and quarter; the top 100 drug names
contain 143,384,240 references or 33% of all drug name references over 34
quarters of continuous FAERS data. Conclusions: While FAERS contains many drugs
and adverse event reports, its data pertains to very few of them. Drug name
incidence lends timely and effective surveillance of large populations of
Averse Event Reports and does not require the cause of the AE, nor its validity
to be known to detect a mass poisoning."
"Hybrid modeling provides an effective solution to cope with multiple time
scales dynamics in systems biology. Among the applications of this method, one
of the most important is the cell cycle regulation. The machinery of the cell
cycle, leading to cell division and proliferation, combines slow growth,
spatio-temporal re-organisation of the cell, and rapid changes of regulatory
proteins concentrations induced by post-translational modifications. The
advancement through the cell cycle comprises a well defined sequence of stages,
separated by checkpoint transitions. The combination of continuous and discrete
changes justifies hybrid modelling approaches to cell cycle dynamics. We
present a piecewise-smooth version of a mammalian cell cycle model, obtained by
hybridization from a smooth biochemical model. The approximate hybridization
scheme, leading to simplified reaction rates and binary event location
functions, is based on learning from a training set of trajectories of the
smooth model. We discuss several learning strategies for the parameters of the
hybrid model."
"We present the Populus toolkit for exploring the dynamics of mass action
systems under different assumptions."
"The Computational Infrastructure for Geodynamics (CIG) is an NSF funded
project that develops, supports, and disseminates community-accessible software
for the geodynamics research community. CIG software supports a variety of
computational geodynamic research from mantle and core dynamics, to crustal and
earthquake dynamics, to magma migration and seismology. To support this type of
project a backend computational infrastructure is necessary.
  Part of this backend infrastructure is an automated build and testing system
to ensure codes and changes to them are compatible with multiple platforms and
that the changes do not significantly affect the scientific results. In this
paper we describe the build and test infrastructure for CIG based on the BaTLab
system, how it is organized, and how it assists in operations. We demonstrate
the use of this type of testing for a suite of geophysics codes, show why codes
may compile on one platform but not on another, and demonstrate how minor
changes may alter the computed results in unexpected ways that can influence
the scientific interpretation. Finally, we examine result comparison between
platforms and show how the compiler or operating system may affect results."
"DNA sequence analysis is fundamental to life science research. The rapid
development of next generation sequencing (NGS) technologies, and the richness
and diversity of applications it makes feasible, have created an enormous gulf
between the potential of this technology and the development of computational
methods to realize this potential. Bridging this gap holds possibilities for
broad impacts toward multiple grand challenges and offers unprecedented
opportunities for software innovation and research. We argue that NGS-enabled
applications need a critical mass of sustainable software to benefit from
emerging computing platforms' transformative potential. Accumulating the
necessary critical mass will require leaders in computational biology,
bioinformatics, computer science, and computer engineering work together to
identify core opportunity areas, critical software infrastructure, and software
sustainability challenges. Furthermore, due to the quickly changing nature of
both bioinformatics software and accelerator technology, we conclude that
creating sustainable accelerated bioinformatics software means constructing a
sustainable bridge between the two fields. In particular, sustained
collaboration between domain developers and technology experts is needed to
develop the accelerated kernels, libraries, frameworks and middleware that
could provide the needed flexible link from NGS bioinformatics applications to
emerging platforms."
"In this little vision paper we analyze the human immune system from a
computer science point of view with the aim of understanding the architecture
and features that allow robust, effective behavior to emerge from local sensing
and actions. We then recall the notion of fractionated cyber-physical systems,
and compare and contrast this to the immune system. We conclude with some
challenges."
"In this contribution, an image-guided therapy system supporting gynecologic
radiation therapy is introduced. The overall workflow of the presented system
starts with the arrival of the patient and ends with follow-up examinations by
imaging and a superimposed visualization of the modeled device from a PACS
system. Thereby, the system covers all treatments stages (pre-, intra- and
postoperative) and has been designed and constructed by a computer scientist
with feedback from an interdisciplinary team of physicians and engineers. This
integrated medical system enables dispatch of diagnostic images directly after
acquisition to a processing workstation that has an on-board 3D Computer Aided
Design model of a medical device. Thus, allowing precise identification of
catheter location in the 3D imaging model which later provides rapid feedback
to the clinician regarding device location. Moreover, the system enables the
ability to perform patient-specific pre-implant evaluation by assessing the
placement of interstitial needles prior to an intervention via virtual template
matching with a diagnostic scan."
"The Wivace 2013 Electronic Proceedings in Theoretical Computer Science
(EPTCS) contain some selected long and short articles accepted for the
presentation at Wivace 2013 - Italian Workshop on Artificial Life and
Evolutionary Computation, which was held at the University of Milan-Bicocca,
Milan, on the 1st and 2nd of July, 2013."
"Over the last years, analyses performed on a stochastic model of catalytic
reaction networks have provided some indications about the reasons why wet-lab
experiments hardly ever comply with the phase transition typically predicted by
theoretical models with regard to the emergence of collectively
self-replicating sets of molecule (also defined as autocatalytic sets, ACSs), a
phenomenon that is often observed in nature and that is supposed to have played
a major role in the emergence of the primitive forms of life. The model at
issue has allowed to reveal that the emerging ACSs are characterized by a
general dynamical fragility, which might explain the difficulty to observe them
in lab experiments. In this work, the main results of the various analyses are
reviewed, with particular regard to the factors able to affect the generic
properties of catalytic reactions network, for what concerns, not only the
probability of ACSs to be observed, but also the overall activity of the
system, in terms of production of new species, reactions and matter."
"The bottom-up construction of synthetic cells is one of the most intriguing
and interesting research arenas in synthetic biology. Synthetic cells are built
by encapsulating biomolecules inside lipid vesicles (liposomes), allowing the
synthesis of one or more functional proteins. Thanks to the in situ synthesized
proteins, synthetic cells become able to perform several biomolecular
functions, which can be exploited for a large variety of applications. This
paves the way to several advanced uses of synthetic cells in basic science and
biotechnology, thanks to their versatility, modularity, biocompatibility, and
programmability. In the previous WIVACE (2012) we presented the
state-of-the-art of semi-synthetic minimal cell (SSMC) technology and
introduced, for the first time, the idea of chemical communication between
synthetic cells and natural cells. The development of a proper synthetic
communication protocol should be seen as a tool for the nascent field of
bio/chemical-based Information and Communication Technologies (bio-chem-ICTs)
and ultimately aimed at building soft-wet-micro-robots. In this contribution
(WIVACE, 2013) we present a blueprint for realizing this project, and show some
preliminary experimental results. We firstly discuss how our research goal
(based on the natural capabilities of biological systems to manipulate chemical
signals) finds a proper place in the current scientific and technological
contexts. Then, we shortly comment on the experimental approaches from the
viewpoints of (i) synthetic cell construction, and (ii) bioengineering of
microorganisms, providing up-to-date results from our laboratory. Finally, we
shortly discuss how autopoiesis can be used as a theoretical framework for
defining synthetic minimal life, minimal cognition, and as bridge between
synthetic biology and artificial intelligence."
"Epigenetic Tracking (ET) is an Artificial Embryology system which allows for
the evolution and development of large complex structures built from artificial
cells. In terms of the number of cells, the complexity of the bodies generated
with ET is comparable with the complexity of biological organisms. We have
previously used ET to simulate the growth of multicellular bodies with
arbitrary 3-dimensional shapes which perform computation using the paradigm of
""metabolic computing"". In this paper we investigate the memory capacity of such
computational structures and analyse the trade-off between shape and
computation. We now plan to build on these foundations to create a
biologically-inspired model in which the encoding of the phenotype is efficient
(in terms of the compactness of the genome) and evolvable in tasks involving
non-trivial computation, robust to damage and capable of self-maintenance and
self-repair."
"Reactions forming a pathway can be rewritten by making explicit the different
molecular components involved in them. A molecular component represents a
biological entity (e.g. a protein) in all its states (free, bound, degraded,
etc.). In this paper we show the application of a component identification
algorithm to a number of real-world models to experimentally validate the
approach. Components identification allows subpathways to be computed to better
understand the pathway functioning."
"In this work we introduce some preliminary analyses on the role of a
semi-permeable membrane in the dynamics of a stochastic model of catalytic
reaction sets (CRSs) of molecules. The results of the simulations performed on
ensembles of randomly generated reaction schemes highlight remarkable
differences between this very simple protocell description model and the
classical case of the continuous stirred-tank reactor (CSTR). In particular, in
the CSTR case, distinct simulations with the same reaction scheme reach the
same dynamical equilibrium, whereas, in the protocell case, simulations with
identical reaction schemes can reach very different dynamical states, despite
starting from the same initial conditions."
"The Spatial Processes package enables an explicit definition of a spatial
environment on top of the normal dynamic modeling SBML capabilities. The
possibility of an explicit representation of spatial dynamics increases the
representation power of SBML. In this work we used those new SBML features to
define an extensive model of colonic crypts composed of the main cellular types
(from stem cells to fully differentiated cells), alongside their spatial
dynamics."
"An approach that combines Self-Organizing maps, hierarchical clustering and
network components is presented, aimed at comparing protein conformational
ensembles obtained from multiple Molecular Dynamic simulations. As a first
result the original ensembles can be summarized by using only the
representative conformations of the clusters obtained. In addition the network
components analysis allows to discover and interpret the dynamic behavior of
the conformations won by each neuron. The results showed the ability of this
approach to efficiently derive a functional interpretation of the protein
dynamics described by the original conformational ensemble, highlighting its
potential as a support for protein engineering."
"The study of biological systems witnessed a pervasive cross-fertilization
between experimental investigation and computational methods. This gave rise to
the development of new methodologies, able to tackle the complexity of
biological systems in a quantitative manner. Computer algorithms allow to
faithfully reproduce the dynamics of the corresponding biological system, and,
at the price of a large number of simulations, it is possible to extensively
investigate the system functioning across a wide spectrum of natural
conditions. To enable multiple analysis in parallel, using cheap, diffused and
highly efficient multi-core devices we developed GPU-powered simulation
algorithms for stochastic, deterministic and hybrid modeling approaches, so
that also users with no knowledge of GPUs hardware and programming can easily
access the computing power of graphics engines."
"An extensive rewiring of cell metabolism supports enhanced proliferation in
cancer cells. We propose a systems level approach to describe this phenomenon
based on Flux Balance Analysis (FBA). The approach does not explicit a cell
biomass formation reaction to be maximized, but takes into account an ensemble
of alternative flux distributions that match the cancer metabolic rewiring
(CMR) phenotype description. The underlying concept is that the analysis the
common/distinguishing properties of the ensemble can provide indications on how
CMR is achieved and sustained and thus on how it can be controlled."
"Protein Structure Predication from sequences of amino acid has gained a
remarkable attention in recent years. Even though there are some prediction
techniques addressing this problem, the approximate accuracy in predicting the
protein structure is closely 75%. An automated procedure was evolved with MACA
(Multiple Attractor Cellular Automata) for predicting the structure of the
protein. Most of the existing approaches are sequential which will classify the
input into four major classes and these are designed for similar sequences.
PSMACA is designed to identify ten classes from the sequences that share
twilight zone similarity and identity with the training sequences. This method
also predicts three states (helix, strand, and coil) for the structure. Our
comprehensive design considers 10 feature selection methods and 4 classifiers
to develop MACA (Multiple Attractor Cellular Automata) based classifiers that
are build for each of the ten classes. We have tested the proposed classifier
with twilight-zone and 1-high-similarity benchmark datasets with over three
dozens of modern competing predictors shows that PSMACA provides the best
overall accuracy that ranges between 77% and 88.7% depending on the dataset."
"Protein can be represented by amino acid interaction network. This network is
a graph whose vertices are the proteins amino acids and whose edges are the
interactions between them. This interaction network is the first step of
proteins three-dimensional structure prediction. In this paper we present a
multi-objective evolutionary algorithm for interaction prediction and ant
colony probabilistic optimization algorithm is used to confirm the interaction."
"miRNA and gene expression profiles have been proved useful for classifying
cancer samples. Efficient classifiers have been recently sought and developed.
A number of attempts to classify cancer samples using miRNA/gene expression
profiles are known in literature. However, the use of semi-supervised learning
models have been used recently in bioinformatics, to exploit the huge corpuses
of publicly available sets. Using both labeled and unlabeled sets to train
sample classifiers, have not been previously considered when gene and miRNA
expression sets are used. Moreover, there is a motivation to integrate both
miRNA and gene expression for a semi-supervised cancer classification as that
provides more information on the characteristics of cancer samples. In this
paper, two semi-supervised machine learning approaches, namely self-learning
and co-training, are adapted to enhance the quality of cancer sample
classification. These approaches exploit the huge public corpuses to enrich the
training data. In self-learning, miRNA and gene based classifiers are enhanced
independently. While in co-training, both miRNA and gene expression profiles
are used simultaneously to provide different views of cancer samples. To our
knowledge, it is the first attempt to apply these learning approaches to cancer
classification. The approaches were evaluated using breast cancer,
hepatocellular carcinoma (HCC) and lung cancer expression sets. Results show up
to 20% improvement in F1-measure over Random Forests and SVM classifiers.
Co-Training also outperforms Low Density Separation (LDS) approach by around
25% improvement in F1-measure in breast cancer."
"X-ray computed tomography at the nanometer scale (nano-CT) offers a wide
range of applications in scientific and industrial areas. Here we describe a
reliable, user-friendly and fast software package based on LabVIEW that may
allow to perform all procedures after the acquisition of raw projection images
in order to obtain the inner structure of the investigated sample. A suitable
image alignment process to address misalignment problems among image series due
to mechanical manufacturing errors, thermal expansion and other external
factors has been considered together with a novel fast parallel beam 3D
reconstruction procedure, developed ad hoc to perform the tomographic
reconstruction. Remarkably improved reconstruction results obtained at the
Beijing Synchrotron Radiation Facility after the image calibration confirmed
the fundamental role of this image alignment procedure that minimizes unwanted
blurs and additional streaking artifacts always present in reconstructed
slices. Moreover, this nano-CT image alignment and its associated 3D
reconstruction procedure fully based on LabVIEW routines, significantly reduce
the data post-processing cycle, thus making faster and easier the activity of
the users during experimental runs."
"Human body consists of lot of cells, each cell consist of DeOxaRibo Nucleic
Acid (DNA). Identifying the genes from the DNA sequences is a very difficult
task. But identifying the coding regions is more complex task compared to the
former. Identifying the protein which occupy little place in genes is a really
challenging issue. For understating the genes coding region analysis plays an
important role. Proteins are molecules with macro structure that are
responsible for a wide range of vital biochemical functions, which includes
acting as oxygen, cell signaling, antibody production, nutrient transport and
building up muscle fibers. Promoter region identification and protein structure
prediction has gained a remarkable attention in recent years. Even though there
are some identification techniques addressing this problem, the approximate
accuracy in identifying the promoter region is closely 68% to 72%. We have
developed a Cellular Automata based tool build with hybrid multiple attractor
cellular automata (HMACA) classifier for protein coding region, promoter region
identification and protein structure prediction which predicts the protein and
promoter regions with an accuracy of 76%. This tool also predicts the structure
of protein with an accuracy of 80%."
"Genes carry the instructions for making proteins that are found in a cell as
a specific sequence of nucleotides that are found in DNA molecules. But, the
regions of these genes that code for proteins may occupy only a small region of
the sequence. Identifying the coding regions play a vital role in understanding
these genes. In this paper we propose a unsupervised Fuzzy Multiple Attractor
Cellular Automata (FMCA) based pattern classifier to identify the coding region
of a DNA sequence. We propose a distinct K-Means algorithm for designing FMACA
classifier which is simple, efficient and produces more accurate classifier
than that has previously been obtained for a range of different sequence
lengths. Experimental results confirm the scalability of the proposed
Unsupervised FCA based classifier to handle large volume of datasets
irrespective of the number of classes, tuples and attributes. Good
classification accuracy has been established."
"This work proposes a model-reduction methodology that preserves Lagrangian
structure (equivalently Hamiltonian structure) and achieves computational
efficiency in the presence of high-order nonlinearities and arbitrary parameter
dependence. As such, the resulting reduced-order model retains key properties
such as energy conservation and symplectic time-evolution maps. We focus on
parameterized simple mechanical systems subjected to Rayleigh damping and
external forces, and consider an application to nonlinear structural dynamics.
To preserve structure, the method first approximates the system's `Lagrangian
ingredients'---the Riemannian metric, the potential-energy function, the
dissipation function, and the external force---and subsequently derives
reduced-order equations of motion by applying the (forced) Euler--Lagrange
equation with these quantities. From the algebraic perspective, key
contributions include two efficient techniques for approximating parameterized
reduced matrices while preserving symmetry and positive definiteness: matrix
gappy POD and reduced-basis sparsification (RBS). Results for a parameterized
truss-structure problem demonstrate the importance of preserving Lagrangian
structure and illustrate the proposed method's merits: it reduces computation
time while maintaining high accuracy and stability, in contrast to existing
nonlinear model-reduction techniques that do not preserve structure."
"This paper describes a novel numerical model aiming at solving
moving-boundary problems such as free-surface flows or fluid-structure
interaction. This model uses a moving-grid technique to solve the
Navier--Stokes equations expressed in the arbitrary Lagrangian--Eulerian
kinematics. The discretization in space is based on the spectral element
method. The coupling of the fluid equations and the moving-grid equations is
essentially done through the conditions on the moving boundaries. Two- and
three-dimensional simulations are presented: translation and rotation of a
cylinder in a fluid, and large-amplitude sloshing in a rectangular tank. The
accuracy and robustness of the present numerical model is studied and
discussed."
"In this paper, we present an adaptive investment strategy for environments
with periodic returns on investment. In our approach, we consider an investment
model where the agent decides at every time step the proportion of wealth to
invest in a risky asset, keeping the rest of the budget in a risk-free asset.
Every investment is evaluated in the market via a stylized return on investment
function (RoI), which is modeled by a stochastic process with unknown
periodicities and levels of noise. For comparison reasons, we present two
reference strategies which represent the case of agents with zero-knowledge and
complete-knowledge of the dynamics of the returns. We consider also an
investment strategy based on technical analysis to forecast the next return by
fitting a trend line to previous received returns. To account for the
performance of the different strategies, we perform some computer experiments
to calculate the average budget that can be obtained with them over a certain
number of time steps. To assure for fair comparisons, we first tune the
parameters of each strategy. Afterwards, we compare the performance of these
strategies for RoIs with different periodicities and levels of noise."
"Several simulation environments exist for the simulation of large-scale
evacuations of buildings, ships, or other enclosed spaces. These offer
sophisticated tools for the study of human behaviour, the recreation of
environmental factors such as fire or smoke, and the inclusion of architectural
or structural features, such as elevators, pillars and exits. Although such
simulation environments can provide insights into crowd behaviour, they lack
the ability to examine potentially dangerous forces building up within a crowd.
These are commonly referred to as crush conditions, and are a common cause of
death in emergency evacuations.
  In this paper, we describe a methodology for the prediction and mitigation of
crush conditions. The paper is organised as follows. We first establish the
need for such a model, defining the main factors that lead to crush conditions,
and describing several exemplar case studies. We then examine current methods
for studying crush, and describe their limitations. From this, we develop a
three-stage hybrid approach, using a combination of techniques. We conclude
with a brief discussion of the potential benefits of our approach."
"We study the application of Tuplix Calculus in modular financial budget
design. We formalize organizational structure using financial transfer
networks. We consider the notion of flux of money over a network, and a way to
enforce the matching of influx and outflux for parts of a network. We exploit
so-called signed attribute notation to make internal streams visible through
encapsulations. Finally, we propose a Tuplix Calculus construct for the
definition of data functions."
"In this research note we provide a variational basis for the optimal
artificial diffusion method, which has been a cornerstone in developing many
stabilized methods. The optimal artificial diffusion method produces exact
nodal solutions when applied to one-dimensional problems with constant
coefficients and forcing function. We first present a variational principle for
a multi-dimensional advective-diffusive system, and then derive a new stable
weak formulation. When applied to one-dimensional problems with constant
coefficients and forcing function, this resulting weak formulation will be
equivalent to the optimal artificial diffusion method. We present
representative numerical results to corroborate our theoretical findings."
"The top-down approach of engineering software integration is considered in
this parer. A set of advantages of this approach are presented, by examples.
All examples are supplied by open source code."
"We have developed a new programming framework, called Sieve, to support
parallel numerical PDE algorithms operating over distributed meshes. We have
also developed a reference implementation of Sieve in C++ as a library of
generic algorithms operating on distributed containers conforming to the Sieve
interface. Sieve makes instances of the incidence relation, or \emph{arrows},
the conceptual first-class objects represented in the containers. Further,
generic algorithms acting on this arrow container are systematically used to
provide natural geometric operations on the topology and also, through duality,
on the data. Finally, coverings and duality are used to encode not only
individual meshes, but all types of hierarchies underlying PDE data structures,
including multigrid and mesh partitions.
  In order to demonstrate the usefulness of the framework, we show how the mesh
partition data can be represented and manipulated using the same fundamental
mechanisms used to represent meshes. We present the complete description of an
algorithm to encode a mesh partition and then distribute a mesh, which is
independent of the mesh dimension, element shape, or embedding. Moreover, data
associated with the mesh can be similarly distributed with exactly the same
algorithm. The use of a high level of abstraction within the Sieve leads to
several benefits in terms of code reuse, simplicity, and extensibility. We
discuss these benefits and compare our approach to other existing mesh
libraries."
"The stochastic modelling of biological systems is an informative, and in some
cases, very adequate technique, which may however result in being more
expensive than other modelling approaches, such as differential equations. We
present StochKit-FF, a parallel version of StochKit, a reference toolkit for
stochastic simulations. StochKit-FF is based on the FastFlow programming
toolkit for multicores and exploits the novel concept of selective memory. We
experiment StochKit-FF on a model of HIV infection dynamics, with the aim of
extracting information from efficiently run experiments, here in terms of
average and variance and, on a longer term, of more structured data."
"Algorithms are developed for calculating dealiased linear convolution sums
without the expense of conventional zero-padding or phase-shift techniques. For
one-dimensional in-place convolutions, the memory requirements are identical
with the zero-padding technique, with the important distinction that the
additional work memory need not be contiguous with the input data. This
decoupling of data and work arrays dramatically reduces the memory and
computation time required to evaluate higher-dimensional in-place convolutions.
The technique also allows one to dealias the higher-order convolutions that
arise from Fourier transforming cubic and higher powers. Implicitly dealiased
convolutions can be built on top of state-of-the-art fast Fourier transform
libraries: vectorized multidimensional implementations for the complex and
centered Hermitian (pseudospectral) cases have been implemented in the
open-source software FFTW++."
"This volume contains the papers presented at the first International Workshop
on Applications of Membrane Computing, Concurrency and Agent-based Modelling in
Population Biology (AMCA-POP 2010) held in Jena, Germany on August 25th, 2010
as a satellite event of the 11th Conference on Membrane Computing (CMC11).
  The aim of the workshop is to investigate whether formal modelling and
analysis techniques could be applied with profit to systems of interest for
population biology and ecology. The considered modelling notations include
membrane systems, Petri nets, agent-based notations, process calculi,
automata-based notations, rewriting systems and cellular automata. Such
notations enable the application of analysis techniques such as simulation,
model checking, abstract interpretation and type systems to study systems of
interest in disciplines such as population biology, ecosystem science,
epidemiology, genetics, sustainability science, evolution and other disciplines
in which population dynamics and interactions with the environment are studied.
Papers contain results and experiences in the modelling and analysis of systems
of interest in these fields."
"Metapopulations are models of ecological systems, describing the interactions
and the behavior of populations that live in fragmented habitats. In this
paper, we present a model of metapopulations based on the multivolume
simulation algorithm tau-DPP, a stochastic class of membrane systems, that we
utilize to investigate the influence that different habitat topologies can have
on the local and global dynamics of metapopulations. In particular, we focus
our analysis on the migration rate of individuals among adjacent patches, and
on their capability of colonizing the empty patches in the habitat. We compare
the simulation results obtained for each habitat topology, and conclude the
paper with some proposals for other research issues concerning metapopulations."
"Multi-valued network models are an important qualitative modelling approach
used widely by the biological community. In this paper we consider developing
an abstraction theory for multi-valued network models that allows the state
space of a model to be reduced while preserving key properties of the model.
This is important as it aids the analysis and comparison of multi-valued
networks and in particular, helps address the well-known problem of state space
explosion associated with such analysis. We also consider developing techniques
for efficiently identifying abstractions and so provide a basis for the
automation of this task. We illustrate the theory and techniques developed by
investigating the identification of abstractions for two published MVN models
of the lysis-lysogeny switch in the bacteriophage lambda."
"In this paper, we survey five different computational modeling methods. For
comparison, we use the activation cycle of G-proteins that regulate cellular
signaling events downstream of G-protein-coupled receptors (GPCRs) as a driving
example. Starting from an existing Ordinary Differential Equations (ODEs)
model, we implement the G-protein cycle in the stochastic Pi-calculus using
SPiM, as Petri-nets using Cell Illustrator, in the Kappa Language using
Cellucidate, and in Bio-PEPA using the Bio-PEPA eclipse plug in. We also
provide a high-level notation to abstract away from communication primitives
that may be unfamiliar to the average biologist, and we show how to translate
high-level programs into stochastic Pi-calculus processes and chemical
reactions."
"Many biological phenomena are inherently multiscale, i.e. they are
characterized by interactions involving different spatial and temporal scales
simultaneously. Though several approaches have been proposed to provide
""multilayer"" models, only Complex Automata, derived from Cellular Automata,
naturally embed spatial information and realize multiscaling with
well-established inter-scale integration schemas. Spatial P systems, a variant
of P systems in which a more geometric concept of space has been added, have
several characteristics in common with Cellular Automata. We propose such a
formalism as a basis to rephrase the Complex Automata multiscaling approach
and, in this perspective, provide a 2-scale Spatial P system describing bone
remodelling. The proposed model not only results to be highly faithful and
expressive in a multiscale scenario, but also highlights the need of a deep and
formal expressiveness study involving Complex Automata, Spatial P systems and
other promising multiscale approaches, such as our shape-based one already
resulted to be highly faithful."
"Delays in biological systems may be used to model events for which the
underlying dynamics cannot be precisely observed, or to provide abstraction of
some behavior of the system resulting more compact models. In this paper we
enrich the stochastic process algebra Bio-PEPA, with the possibility of
assigning delays to actions, yielding a new non-Markovian process algebra:
Bio-PEPAd. This is a conservative extension meaning that the original syntax of
Bio-PEPA is retained and the delay specification which can now be associated
with actions may be added to existing Bio-PEPA models. The semantics of the
firing of the actions with delays is the delay-as-duration approach, earlier
presented in papers on the stochastic simulation of biological systems with
delays. These semantics of the algebra are given in the Starting-Terminating
style, meaning that the state and the completion of an action are observed as
two separate events, as required by delays. Furthermore we outline how to
perform stochastic simulation of Bio-PEPAd systems and how to automatically
translate a Bio-PEPAd system into a set of Delay Differential Equations, the
deterministic framework for modeling of biological systems with delays. We end
the paper with two example models of biological systems with delays to
illustrate the approach."
"This paper investigates the use of graph rewriting systems as a modelling
tool, and advocates the embedding of such systems in an interactive
environment. One important application domain is the modelling of biochemical
systems, where states are represented by port graphs and the dynamics is driven
by rules and strategies. A graph rewriting tool's capability to interactively
explore the features of the rewriting system provides useful insights into
possible behaviours of the model and its properties. We describe PORGY, a
visual and interactive tool we have developed to model complex systems using
port graphs and port graph rewrite rules guided by strategies, and to navigate
in the derivation history. We demonstrate via examples some functionalities
provided by PORGY."
"The broad application range of the predator-prey modelling enabled us to
apply it to represent the dynamics of the work-employment system. For the
adopted period, we conclude that this dynamics is chaotic in the beginning of
the time series and tends to less perturbed states, as time goes by, due to
public policies and hidden intrinsic system features. Basic Lotka-Volterra
approach was revised and adapted to the reality of the study. The final aim is
to provide managers with generalized theoretical elements that allow to a more
accurate understanding of the behavior of the work-employment system."
"This article highlights some of the basic concepts of bioinformatics and data
mining. The major research areas of bioinformatics are highlighted. The
application of data mining in the domain of bioinformatics is explained. It
also highlights some of the current challenges and opportunities of data mining
in bioinformatics."
"Finite element method (FEM) suffers from a serious mesh distortion problem
when used for high velocity impact analyses. The smooth particle hydrodynamics
(SPH) method is appropriate for this class of problems involving severe damages
but at considerable computational cost. It is beneficial if the latter is
adopted only in severely distorted regions and FEM further away. The coupled
smooth particle hydrodynamics - finite element method (SFM) has been adopted in
a commercial hydrocode LS-DYNA to study the perforation of Weldox 460E steel
and AA5083-H116 aluminum plates with varying thicknesses and various projectile
nose geometries including blunt, conical and ogival noses. Effects of the SPH
domain size and particle density are studied considering the friction effect
between the projectile and the target materials. The simulated residual
velocities and the ballistic limit velocities from the SFM agree well with the
published experimental data. The study shows that SFM is able to emulate the
same failure mechanisms of the steel and aluminum plates as observed in various
experimental investigations for initial impact velocity of 170 m/s and higher."
"Genetic Regulatory Networks (GRNs) plays a vital role in the understanding of
complex biological processes. Modeling GRNs is significantly important in order
to reveal fundamental cellular processes, examine gene functions and
understanding their complex relationships. Understanding the interactions
between genes gives rise to develop better method for drug discovery and
diagnosis of the disease since many diseases are characterized by abnormal
behaviour of the genes. In this paper we have reviewed various evolutionary
algorithms-based approach for modeling GRNs and discussed various opportunities
and challenges."
"Many natural patterns, such as the distributions of blood particles in a
blood sample, proteins on cell surfaces, biological populations in their
habitat, galaxies in the universe, the sequence of human genes, and the fitness
in evolutionary computing, have been found to follow power law. Taylor's power
law (Taylor 1961: Nature, 189:732-) is well recognized as one of the
fundamental models in population ecology. A fundamental property of biological
populations, which Taylor's power law reveals, is the near universal
heterogeneity of population abundance distribution in habitat. Obviously, the
heterogeneity also exists at the community level, where not only the
distributions of population abundances but also the proportions of the species
composition in the community are often heterogeneous. Nevertheless, existing
community diversity indexes such as Shannon index and Simpson index can only
measure ""local"" or ""static"" diversity in the sense that they are computed for
each habitat at a specific time point, and the indexes alone do not reflect the
diversity changes. In this note, I propose to extend the application scope of
Taylor's power law to the studies of human microbial communities, specifically,
the community heterogeneity at both population and community levels. I further
suggested that population dispersion models such as Taylor (1980: Nature, 286,
53-), which is known to generate population distribution patterns consistent
with the power law, should also be very useful for analyzing the distribution
patterns of human microbes within the human body. Overall, I hope that the
approach to human microbial community with the power law offers an example that
ecological theories can play an important role in the emerging medical ecology,
which aims at studying the ecology of human microbiome and its implications to
human diseases and health, as well as in personalized medicine."
"MicroRNAs (miRNAs) are a class of non-coding RNAs that regulate gene
expression. Identification of total number of miRNAs even in completely
sequenced organisms is still an open problem. However, researchers have been
using techniques that can predict limited number of miRNA in an organism. In
this paper, we have used homology based approach for comparative analysis of
miRNA of hexapoda group .We have used Apis mellifera, Bombyx mori, Anopholes
gambiae and Drosophila melanogaster miRNA datasets from miRBase repository. We
have done pair wise as well as multiple alignments for the available miRNAs in
the repository to identify and analyse conserved regions among related species.
Unfortunately, to the best of our knowledge, miRNA related literature does not
provide in depth analysis of hexapods. We have made an attempt to derive the
commonality among the miRNAs and to identify the conserved regions which are
still not available in miRNA repositories. The results are good approximation
with a small number of mismatches. However, they are encouraging and may
facilitate miRNA biogenesis for"
"Near linear scaling fragment based quantum chemical calculations are becoming
increasingly popular for treating large systems with high accuracy and is an
active field of research. However, it remains difficult to set up these
calculations without expert knowledge. To facilitate the use of such methods,
software tools need to be available to support these methods and help to set up
reasonable input files which will lower the barrier of entry for usage by
non-experts. Previous tools relies on specific annotations in structure files
for automatic and successful fragmentation such as residues in PDB files. We
present a general fragmentation methodology and accompanying tools called
FragIt to help setup these calculations. FragIt uses the SMARTS language to
locate chemically appropriate fragments in large structures and is applicable
to fragmentation of any molecular system given suitable SMARTS patterns. We
present SMARTS patterns of fragmentation for proteins, DNA and polysaccharides,
specifically for D-galactopyranose for use in cyclodextrins. FragIt is used to
prepare input files for the Fragment Molecular Orbital method in the GAMESS
program package, but can be extended to other computational methods easily."
"Tumor protein P53 is believed to be involved in over half of human cancers
cases, the prediction of malignancies plays essential roles not only in advance
detection for cancer, but also in discovering effective prevention and
treatment of cancer, till now there isn't approach be able in prediction the
mutated in tumor protein P53 which is caused high ratio of human cancers like
breast, Blood, skin, liver, lung, bladder etc. This research proposed a new
approach for prediction pre-cancer via detection malignant mutations in tumor
protein P53 using bioinformatics tools like FASTA, BLAST, CLUSTALW and TP53
databases worldwide. Implement and apply this new approach of prediction
pre-cancer through mutations at tumor protein P53 shows an effective result
when used more specific parameters/features to extract the prediction result
that means when the user increase the number of filters of the results which
obtained from the database gives more specific diagnosis and classify, addition
that the detecting pre-cancer via prediction mutated tumor protein P53 will
reduces a person's cancers in the future by avoiding exposure to toxins,
radiation or monitoring themselves at older ages by change their food,
environment, even the pace of living. Also that new approach of prediction
pre-cancer will help if there is any treatment can give for that person to
therapy the mutated tumor protein P53. Index Terms (Normal Homology TP53 gene,
Tumor Protein P53, Oncogene Labs, GC and AT content, FASTA, BLAST, ClustalW)"
"Solvency games, introduced by Berger et al., provide an abstract framework
for modelling decisions of a risk-averse investor, whose goal is to avoid ever
going broke. We study a new variant of this model, where, in addition to
stochastic environment and fixed increments and decrements to the investor's
wealth, we introduce interest, which is earned or paid on the current level of
savings or debt, respectively.
  We study problems related to the minimum initial wealth sufficient to avoid
bankruptcy (i.e. steady decrease of the wealth) with probability at least p. We
present an exponential time algorithm which approximates this minimum initial
wealth, and show that a polynomial time approximation is not possible unless P
= NP. For the qualitative case, i.e. p=1, we show that the problem whether a
given number is larger than or equal to the minimum initial wealth belongs to
both NP and coNP, and show that a polynomial time algorithm would yield a
polynomial time algorithm for mean-payoff games, existence of which is a
longstanding open problem. We also identify some classes of solvency MDPs for
which this problem is in P. In all above cases the algorithms also give
corresponding bankruptcy avoiding strategies."
"CA has grown as potential classifier for addressing major problems in
bioinformatics. Lot of bioinformatics problems like predicting the protein
coding region, finding the promoter region, predicting the structure of protein
and many other problems in bioinformatics can be addressed through Cellular
Automata. Even though there are some prediction techniques addressing these
problems, the approximate accuracy level is very less. An automated procedure
was proposed with MACA (Multiple Attractor Cellular Automata) which can address
all these problems. The genetic algorithm is also used to find rules with good
fitness values. Extensive experiments are conducted for reporting the accuracy
of the proposed tool. The average accuracy of MACA when tested with ENCODE,
BG570, HMR195, Fickett and Tongue, ASP67 datasets is 78%."
"The computational modeling of genetic regulatory networks is now common
place, either by fitting a system to experimental data or by exploring the
behaviour of abstract systems with the aim of identifying underlying
principles. This paper presents an approach to the latter, considering the
response to environmental changes of a well-known model placed upon tunable
fitness landscapes. The effects on genome size and gene connectivity are
explored."
"Research capacity is critical in understanding systemic risk and informing
new regulation. Banking regulation has not kept pace with all the complexities
of financial innovation. The academic literature on systemic risk is rapidly
expanding. The majority of papers analyse a single source or a consolidated
source of risk and its effect. A fraction of publications quantify systemic
risk measures or formulate penalties for systemically important financial
institutions that are of practical regulatory relevance. The challenges facing
systemic risk evaluation and regulation still persist, as the definition of
systemic risk is somewhat unsettled and that affects attempts to provide
solutions. Our understanding of systemic risk is evolving and the awareness of
data relevance is rising gradually; this challenge is reflected in the focus of
major international research initiatives. There is a consensus that the direct
and indirect costs of a systemic crisis are enormous as opposed to preventing
it, and that without regulation the externalities will not be prevented; but
there is no consensus yet on the extent and detail of regulation, and research
expectations are to facilitate the regulatory process. This report outlines an
integrated approach for systemic risk evaluation based on multiple types of
interbank exposures through innovative modelling approaches as tensorial
multilayer networks, suggests how to relate underlying economic data and how to
extend the network to cover financial market information. We reason about data
requirements and time scale effects, and outline a multi-model hypernetwork of
systemic risk knowledge as a scenario analysis and policy support tool. The
argument is that logical steps forward would incorporate the range of risk
sources and their interrelated effects as contributions towards an overall
systemic risk indicator, would perform an integral analysis of ..."
"Protein structure prediction based on Hydrophobic-Polar energy model
essentially becomes searching for a conformation having a compact hydrophobic
core at the center. The hydrophobic core minimizes the interaction energy
between the amino acids of the given protein. Local search algorithms can
quickly find very good conformations by moving repeatedly from the current
solution to its ""best"" neighbor. However, once such a compact hydrophobic core
is found, the search stagnates and spends enormous effort in quest of an
alternative core. In this paper, we attempt to restructure segments of a
conformation with such compact core. We select one large segment or a number of
small segments and apply exhaustive local search. We also apply a mix of
heuristics so that one heuristic can help escape local minima of another. We
evaluated our algorithm by using Face Centered Cubic (FCC) Lattice on a set of
standard benchmark proteins and obtain significantly better results than that
of the state-of-the-art methods."
"Motivation: The Stochastic Simulation Algorithm (SSA) has largely diffused in
the field of systems biology. This approach needs many realizations for
establishing statistical results on the system under study. It is very
computationnally demanding, and with the advent of large models this burden is
increasing. Hence parallel implementation of SSA are needed to address these
needs.
  At the very heart of the SSA is the selection of the next reaction to occur
at each time step, and to the best of our knowledge all implementations are
based on an inverse transformation method. However, this method involves a
random number of steps to select this next reaction and is poorly amenable to a
parallel implementation.
  Results: Here, we introduce a parallel acceptance-rejection algorithm to
select the K next reactions to occur. This algorithm uses a deterministic
number of steps, a property well suited to a parallel implementation. It is
simple and small, accurate and scalable. We propose a Graphics Processing Unit
(GPU) implementation and validate our algorithm with simulated propensity
distributions and the propensity distribution of a large model of yeast iron
metabolism. We show that our algorithm can handle thousands of selections of
next reaction to occur in parallel on the GPU, paving the way to massive SSA.
  Availability: We present our GPU-AR algorithm that focuses on the very heart
of the SSA. We do not embed our algorithm within a full implementation in order
to stay pedagogical and allows its rapid implementation in existing software.
We hope that it will enable stochastic modelers to implement our algorithm with
the benefits of their own optimizations."
"This paper aims at providing a survey on the problems that can be easily
addressed by cellular automata in bioinformatics. Some of the authors have
proposed algorithms for addressing some problems in bioinformatics but the
application of cellular automata in bioinformatics is a virgin field in
research. None of the researchers has tried to relate the major problems in
bioinformatics and find a common solution. Extensive literature surveys were
conducted. We have considered some papers in various journals and conferences
for conduct of our research. This paper provides intuition towards relating
various problems in bioinformatics logically and tries to attain a common frame
work for addressing the same."
"Protein Folding is concerned with the reasons and mechanism behind a
protein's tertiary structure. The thermodynamic hypothesis of Anfinsen
postulates an universal energy function (UEF) characterizing the tertiary
structure, defined consistently across proteins, in terms of their aminoacid
sequence.
  We consider the approach of examining multiple protein structure descriptors
in the PDB (Protein Data Bank), and infer individual preferences, biases
favoring particular classes of aminoacid interactions in each of them, later
aggregating these individual preferences into a global preference. This 2-step
process would ideally expose intrinsic biases on classes of aminoacid
interactions in the UEF itself. The intuition is that any intrinsic biases in
the UEF are expressed within each protein in a specific manner consistent with
its specific aminoacid sequence, size, and fold (consistently with Anfinsen's
thermodynamic hypothesis), making a 1-step, holistic aggregation less
desirable.
  Our intention is to illustrate how some impossibility results from voting
theory would apply in this setting, being possibly applicable to other protein
folding problems as well. We consider concepts and results from voting theory
and unveil methodological difficulties for the approach mentioned above. With
our observations, we intend to highlight how key theoretical barriers, already
exposed by economists, can be relevant for the development of new methods, new
algorithms, for problems related to protein folding."
"Bioinformatics incorporates information regarding biological data storage,
accessing mechanisms and presentation of characteristics within this data. Most
of the problems in bioinformatics and be addressed efficiently by computer
techniques. This paper aims at building a classifier based on Multiple
Attractor Cellular Automata (MACA) which uses fuzzy logic with version Z to
predict splicing site, protein coding and promoter region identification in
eukaryotes. It is strengthened with an artificial immune system technique
(AIS), Clonal algorithm for choosing rules of best fitness. The proposed
classifier can handle DNA sequences of lengths 54,108,162,252,354. This
classifier gives the exact boundaries of both protein and promoter regions with
an average accuracy of 90.6%. This classifier can predict the splicing site
with 97% accuracy. This classifier was tested with 1, 97,000 data components
which were taken from Fickett & Toung , EPDnew, and other sequences from a
renowned medical university."
"Making sense of the physical world has always been at the core of mapping. Up
until recently, this has always dependent on using the human eye. Using
airborne lasers, it has become possible to quickly ""see"" more of the world in
many more dimensions. The resulting enormous point clouds serve as data sources
for applications far beyond the original mapping purposes ranging from flooding
protection and forestry to threat mitigation. In order to process these large
quantities of data, novel methods are required. In this contribution, we
develop models to automatically classify ground cover and soil types. Using the
logic of machine learning, we critically review the advantages of supervised
and unsupervised methods. Focusing on decision trees, we improve accuracy by
including beam vector components and using a genetic algorithm. We find that
our approach delivers consistently high quality classifications, surpassing
classical methods."
"In the present work, a new computational framework for structural topology
optimization based on the concept of moving deformable components is proposed.
Compared with the traditional pixel or node point-based solution framework, the
proposed solution paradigm can incorporate more geometry and mechanical
information into topology optimization directly and therefore render the
solution process more flexible. It also has the great potential to reduce the
computational burden associated with topology optimization substantially. Some
representative examples are presented to illustrate the effectiveness of the
proposed approach."
"In animal behavioral biology, there are several cases in which an autonomous
observing/training system would be useful. 1) Observation of certain species
continuously, or for documenting specific events, which happen irregularly; 2)
Longterm intensive training of animals in preparation for behavioral
experiments; and 3) Training and testing of animals without human interference,
to eliminate potential cues and biases induced by humans. The primary goal of
this study is to build a system named CATOS (Computer Aided Training/Observing
System) that could be used in the above situations. As a proof of concept, the
system was built and tested in a pilot experiment, in which cats were trained
to press three buttons differently in response to three different sounds (human
speech) to receive food rewards. The system was built in use for about 6
months, successfully training two cats. One cat learned to press a particular
button, out of three buttons, to obtain the food reward with over 70 percent
correctness."
"High-Content Digital Microscopy enhances user comfort, data storage and
analysis throughput, paving the way to new researches and medical diagnostics.
A digital microscopy platform aims at capturing an image of a cover slip, at
storing information on a file server and a database, at visualising the image
and analysing its content. We will discuss how the Python ecosystem can provide
such software framework efficiently. Moreover this paper will give an
illustration of the data chunking approach to manage the huge amount of data."
"SfePy (Simple Finite Elements in Python) is a framework for solving various
kinds of problems (mechanics, physics, biology, ...) described by partial
differential equations in two or three space dimensions by the finite element
method. The paper illustrates its use in an interactive environment or as a
framework for building custom finite-element based solvers."
"Unsupervised word segmentation methods were applied to analyze the protein
sequence. Protein sequences, such as 'MTMDKSELVQKA...', were used as input to
these methods. Segmented 'protein word' sequences, such as 'MTM DKSE LVQKA',
were then obtained. We compare the 'protein words' produced by unsupervised
segmentation and the protein secondary structure segmentation. An interesting
finding is that the unsupervised word segmentation is more efficient than
secondary structure segmentation in expressing information. Our experiment also
suggests there may be some 'protein ruins' in current noncoding regions."
"The Synchrosqueezing transform is a time-frequency analysis method that can
decompose complex signals into time-varying oscillatory components. It is a
form of time-frequency reassignment that is both sparse and invertible,
allowing for the recovery of the signal. This article presents an overview of
the theory and stability properties of Synchrosqueezing, as well as
applications of the technique to topics in cardiology, climate science and
economics."
"The pharmaceutical industry is plagued by the problem of side effects that
can occur anytime a prescribed medication is ingested. There has been a recent
interest in using the vast quantities of medical data available in longitudinal
observational databases to identify causal relationships between drugs and
medical events. Unfortunately the majority of existing post marketing
surveillance algorithms measure how dependant or associated an event is on the
presence of a drug rather than measuring causality. In this paper we
investigate potential attributes that can be used in causal inference to
identify side effects based on the Bradford-Hill causality criteria. Potential
attributes are developed by considering five of the causality criteria and
feature selection is applied to identify the most suitable of these attributes
for detecting side effects. We found that attributes based on the specificity
criterion may improve side effect signalling algorithms but the experiment and
dosage criteria attributes investigated in this paper did not offer sufficient
additional information."
"Due to the recent evolution of sequencing techniques, the number of available
genomes is rising steadily, leading to the possibility to make large scale
genomic comparison between sets of close species. An interesting question to
answer is: what is the common functionality genes of a collection of species,
or conversely, to determine what is specific to a given species when compared
to other ones belonging in the same genus, family, etc. Investigating such
problem means to find both core and pan genomes of a collection of species,
\textit{i.e.}, genes in common to all the species vs. the set of all genes in
all species under consideration. However, obtaining trustworthy core and pan
genomes is not an easy task, leading to a large amount of computation, and
requiring a rigorous methodology. Surprisingly, as far as we know, this
methodology in finding core and pan genomes has not really been deeply
investigated. This research work tries to fill this gap by focusing only on
chloroplastic genomes, whose reasonable sizes allow a deep study. To achieve
this goal, a collection of 99 chloroplasts are considered in this article. Two
methodologies have been investigated, respectively based on sequence
similarities and genes names taken from annotation tools. The obtained results
will finally be evaluated in terms of biological relevance."
"Previous work, mostly published, developed two-shell recursive trading
systems. An inner-shell of Canonical Momenta Indicators (CMI) is adaptively fit
to incoming market data. A parameterized trading-rule outer-shell uses the
global optimization code Adaptive Simulated Annealing (ASA) to fit the trading
system to historical data. A simple fitting algorithm, usually not requiring
ASA, is used for the inner-shell fit. An additional risk-management
middle-shell has been added to create a three-shell recursive
optimization/sampling/fitting algorithm. Portfolio-level distributions of
copula-transformed multivariate distributions (with constituent markets
possessing different marginal distributions in returns space) are generated by
Monte Carlo samplings. ASA is used to importance-sample weightings of these
markets.
  The core code, Trading in Risk Dimensions (TRD), processes Training and
Testing trading systems on historical data, and consistently interacts with
RealTime trading platforms at minute resolutions, but this scale can be
modified. This approach transforms constituent probability distributions into a
common space where it makes sense to develop correlations to further develop
probability distributions and risk/uncertainty analyses of the full portfolio.
ASA is used for importance-sampling these distributions and for optimizing
system parameters."
"Classification of proteins based on their structure provides a valuable
resource for studying protein structure, function and evolutionary
relationships. With the rapidly increasing number of known protein structures,
manual and semi-automatic classification is becoming ever more difficult and
prohibitively slow. Therefore, there is a growing need for automated, accurate
and efficient classification methods to generate classification databases or
increase the speed and accuracy of semi-automatic techniques. Recognizing this
need, several automated classification methods have been developed. In this
survey, we overview recent developments in this area. We classify different
methods based on their characteristics and compare their methodology, accuracy
and efficiency. We then present a few open problems and explain future
directions."
"The second international workshop on Computational Models for Cell Processes
(ComProc 2009) took place on November 3, 2009 at the Eindhoven University of
Technology, in conjunction with Formal Methods 2009. The workshop was jointly
organized with the EC-MOAN project. This volume contains the final versions of
all contributions accepted for presentation at the workshop."
"Computational techniques have shown much promise in the field of Finance,
owing to their ability to extract sense out of dauntingly complex systems. This
paper reviews the most promising of these techniques, from traditional
computational intelligence methods to their machine learning siblings, with
particular view to their application in optimising the management of a
portfolio of financial instruments. The current state of the art is assessed,
and prospective further work is assessed and recommended"
"Lattice protein models, as the Hydrophobic-Polar (HP) model, are a common
abstraction to enable exhaustive studies on structure, function, or evolution
of proteins. A main issue is the high number of optimal structures, resulting
from the hydrophobicity-based energy function applied. We introduce an
equivalence relation on protein structures that correlates to the energy
function. We discuss the efficient enumeration of optimal representatives of
the corresponding equivalence classes and the application of the results."
"The simulation of a protein's folding process is often done via stochastic
local search, which requires a procedure to apply structural changes onto a
given conformation. Here, we introduce a constraint-based approach to enumerate
lattice protein structures according to k-local moves in arbitrary lattices.
Our declarative description is much more flexible for extensions than standard
operational formulations. It enables a generic calculation of k-local neighbors
in backbone-only and side chain models. We exemplify the procedure using a
simple hierarchical folding scheme."
"Feature selection techniques have been used as the workhorse in biomarker
discovery applications for a long time. Surprisingly, the stability of feature
selection with respect to sampling variations has long been under-considered.
It is only until recently that this issue has received more and more attention.
In this article, we review existing stable feature selection methods for
biomarker discovery using a generic hierarchal framework. We have two
objectives: (1) providing an overview on this new yet fast growing topic for a
convenient reference; (2) categorizing existing methods under an expandable
framework for future research and development."
"The Gene Ontology (GO) provides a knowledge base to effectively describe
proteins. However, measuring similarity between proteins based on GO remains a
challenge. In this paper, we propose a new similarity measure, information
coefficient similarity measure (SimIC), to effectively integrate both the
information content (IC) of GO terms and the structural information of GO
hierarchy to determine the similarity between proteins. Testing on yeast
proteins, our results show that SimIC efficiently addresses the shallow
annotation issue in GO, thus improves the correlations between GO similarities
of yeast proteins and their expression similarities as well as between GO
similarities of yeast proteins and their sequence similarities. Furthermore, we
demonstrate that the proposed SimIC is superior in predicting yeast protein
interactions. We predict 20484 yeast protein-protein interactions (PPIs)
between 2462 proteins based on the high SimIC values of biological process (BP)
and cellular component (CC). Examining the 214 MIPS complexes in our predicted
PPIs shows that all members of 159 MIPS complexes can be found in our PPI
predictions, which is more than those (120/214) found in PPIs predicted by
relative specificity similarity (RSS). Integrating IC and structural
information of GO hierarchy can improve the effectiveness of the semantic
similarity measure of GO terms. The new SimIC can effectively correct the
effect of shallow annotation, and then provide an effective way to measure
similarity between proteins based on Gene Ontology."
"The haplotype resolution from xor-genotype data has been recently formulated
as a new model for genetic studies. The xor-genotype data is a cheaply
obtainable type of data distinguishing heterozygous from homozygous sites
without identifying the homozygous alleles. In this paper we propose a
formulation based on a well-known model used in haplotype inference: pure
parsimony. We exhibit exact solutions of the problem by providing polynomial
time algorithms for some restricted cases and a fixed-parameter algorithm for
the general case. These results are based on some interesting combinatorial
properties of a graph representation of the solutions. Furthermore, we show
that the problem has a polynomial time k-approximation, where k is the maximum
number of xor-genotypes containing a given SNP. Finally, we propose a heuristic
and produce an experimental analysis showing that it scales to real-world large
instances taken from the HapMap project."
"In the present work an attempt has been made to study the dynamics of a
diatomic molecule N2 in water. The proposed model consists of Langevin
stochastic differential equation whose solution is obtained through Euler's
method. The proposed work has been concluded by studying the behavior of
statistical parameters like variance in position, variance in velocity and
covariance between position and velocity. This model incorporates the important
parameters like acceleration, intermolecular force, frictional force and random
force."
"In situ hybridisation gene expression information helps biologists identify
where a gene is expressed. However, the databases that republish the
experimental information are often both incomplete and inconsistent. This paper
examines a system, Argudas, designed to help tackle these issues. Argudas is an
evolution of an existing system, and so that system is reviewed as a means of
both explaining and justifying the behaviour of Argudas. Throughout the
discussion of Argudas a number of issues will be raised including the
appropriateness of argumentation in biology and the challenges faced when
integrating apparently similar online biological databases."
"A hybrid method for the incompressible Navier--Stokes equations is presented.
The method inherits the attractive stabilizing mechanism of upwinded
discontinuous Galerkin methods when momentum advection becomes significant,
equal-order interpolations can be used for the velocity and pressure fields,
and mass can be conserved locally. Using continuous Lagrange multiplier spaces
to enforce flux continuity across cell facets, the number of global degrees of
freedom is the same as for a continuous Galerkin method on the same mesh.
Different from our earlier investigations on the approach for the
Navier--Stokes equations, the pressure field in this work is discontinuous
across cell boundaries. It is shown that this leads to very good local mass
conservation and, for an appropriate choice of finite element spaces, momentum
conservation. Also, a new form of the momentum transport terms for the method
is constructed such that global energy stability is guaranteed, even in the
absence of a point-wise solenoidal velocity field. Mass conservation, momentum
conservation and global energy stability are proved for the time-continuous
case, and for a fully discrete scheme. The presented analysis results are
supported by a range of numerical simulations."
"In this work we present the development of a workflow based on Taverna which
is going to be implemented for calculations in Phylogeny by means of the
MrBayes tool. It has a friendly interface developed with the Gridsphere
framework. The user is able to define the parameters for doing the Bayesian
calculation, determine the model of evolution, check the accuracy of the
results in the intermediate stages as well as do a multiple alignment of the
sequences previously to the final result. To do this, no knowledge from his/her
side about the computational procedure is required."
"In the last years an increasing demand for Grid Infrastructures has resulted
in several international collaborations. This is the case of the EELA Project,
which has brought together collaborating groups of Latin America and Europe.
One year ago we presented this e-infrastructure used, among others, by the
Biomedical groups for the studies of oncological analysis, neglected diseases,
sequence alignments and computation phylogenetics. After this period, the
achieved advances are summarised in this paper."
"The Cell Broad Engine (BE) Processor has unique memory access architecture
besides its powerful computing engines. Many computing-intensive applications
have been ported to Cell/BE successfully. But memory-intensive applications are
rarely investigated except for several micro benchmarks. Since Cell/BE has
powerful software visible DMA engine, this paper studies on whether Cell/BE is
suit for applica- tions with large amount of random memory accesses. Two
benchmarks, GUPS and SSCA#2, are used. The latter is a rather complex one that
in representative of real world graph analysis applications. We find both
benchmarks have good performance on Cell/BE based IBM QS20/22. Com- pared with
2 conventional multi-processor systems with the same core/thread number, GUPS
is about 40-80% fast and SSCA#2 about 17-30% fast. The dynamic load balanc- ing
and software pipeline for optimizing SSCA#2 are intro- duced. Based on the
experiment, the potential of Cell/BE for random access is analyzed in detail as
well as its limita- tions of memory controller, atomic engine and TLB manage-
ment.Our research shows although more programming effort are needed, Cell/BE
has the potencial for irregular memory access applications."
"The problem for selection of appropriate assets investing composition
projects such as assets rationalization plays an important role in promotion of
business systems. We consider the assets investing composition plan problems
subject to grey multiobjective programming with the grey inequality
constraints. In this paper, we show in detail the entire process of the
application from modeling the case problem to generating its solution. To solve
the grey multi objective programming problem, we then develop and apply an
algorithm of grey multiple objective programming by weighting method and an
algorithm of grey multiple objective programming based on q -positioned
programming method. These algorithms all regard as of great importance
uncertainty (greyness) at grey multiobjective programming and simple and easy
the calculating process. The calculating examples of paper also show ability
and effectiveness of algorithms."
"In this work, we present a short review about the high level design
methodology (HLDM), that is based on the use of very high level (VHL)
programing language as main, and the use of the intermediate level (IL)
language only for the critical processing time. The languages used are Python
(VHL) and FORTRAN (IL). Moreover, this methodology, making use of the oriented
object programing (OOP), permits to produce a readable, portable and reusable
code. Also is presented the concept of computational framework, that naturally
appears from the OOP paradigm. As an example, we present the framework called
PYGRAWC (Python framework for Gravitational Waves from Cosmological origin).
Even more, we show that the use of HLDM with Python and FORTRAN produces a
powerful tool for solving astrophysical problems."
"In this paper we formulate a contract design problem where a primary license
holder wishes to profit from its excess spectrum capacity by selling it to
potential secondary users/buyers. It needs to determine how to optimally price
the excess spectrum so as to maximize its profit, knowing that this excess
capacity is stochastic in nature, does not come with exclusive access, and
cannot provide deterministic service guarantees to a buyer. At the same time,
buyers are of different {\em types}, characterized by different communication
needs, tolerance for the channel uncertainty, and so on, all of which a buyer's
private information. The license holder must then try to design different
contracts catered to different types of buyers in order to maximize its profit.
We address this problem by adopting as a reference a traditional spectrum
market where the buyer can purchase exclusive access with fixed/deterministic
guarantees. We fully characterize the optimal solution in the cases where there
is a single buyer type, and when multiple types of buyers share the same, known
channel condition as a result of the primary user activity. In the most general
case we construct an algorithm that generates a set of contracts in a
computationally efficient manner, and show that this set is optimal when the
buyer types satisfy a monotonicity condition."
"This volume contains the proceedings of the 7th International Workshop on
Developments in Computational Models (DCM 2011) which was held on Sunday July
3, 2011, in Zurich, Switzerland, as a satelite workshop of ICALP 2011.
  Recently several new models of computation have emerged, for instance for
bio-computing and quantum-computing, and in addition traditional models of
computation have been adapted to accommodate new demands or capabilities of
computer systems. The aim of DCM is to bring together researchers who are
currently developing new computational models or new features for traditional
computational models, in order to foster their interaction, to provide a forum
for presenting new ideas and work in progress, and to enable newcomers to learn
about current activities in this area."
"In this paper we present a variant of the Calculus of Looping Sequences (CLS
for short) with global and local rewrite rules. While global rules, as in CLS,
are applied anywhere in a given term, local rules can only be applied in the
compartment on which they are defined. Local rules are dynamic: they can be
added, moved and erased. We enrich the new calculus with a parallel semantics
where a reduction step is lead by any number of global and local rules that
could be performed in parallel. A type system is developed to enforce the
property that a compartment must contain only local rules with specific
features. As a running example we model some interactions happening in a cell
starting from its nucleus and moving towards its mitochondria."
"The development of sputtering facilities, at the moment, is mainly pursued
through experimental tests, or simply by expertise in the field, and relies
much less on numerical simulation of the process environment. This leads to
great efforts and empirically, roughly optimized solutions: in fact, the
simulation of these devices, at the state of art, is quite good in predicting
the behavior of single steps of the overall deposition process, but it seems
still ahead a full integration among the tools simulating the various phenomena
involved in a sputter. We summarize here the techniques and codes already
available for problems of interest in sputtering facilities, and we try to
outline the possible features of a comprehensive simulation framework. This
framework should be able to integrate the single paradigms, dealing with
aspects going from the plasma environment up to the distribution and properties
of the deposited film, not only on the surface of the substrate, but also on
the walls of the process chamber."
"Data compression plays an important role to deal with high volumes of DNA
sequences in the field of Bioinformatics. Again data compression techniques
directly affect the alignment of DNA sequences. So the time needed to
decompress a compressed sequence has to be given equal priorities as with
compression ratio. This article contains first introduction then a brief review
of different biological sequence compression after that my proposed work then
our two improved Biological sequence compression algorithms after that result
followed by conclusion and discussion, future scope and finally references.
These algorithms gain a very good compression factor with higher saving
percentage and less time for compression and decompression than the previous
Biological Sequence compression algorithms. Keywords: Hash map table, Tandem
repeats, compression factor, compression time, saving percentage, compression,
decompression process."
"Analysis of algorithms with complete knowledge of its inputs is sometimes not
up to our expectations. Many times we are surrounded with such scenarios where
inputs are generated without any prior knowledge. Online Algorithms have found
their applicability in broad areas of computer engineering. Among these, an
online financial algorithm is one of the most important areas where lots of
efforts have been used to produce an efficient algorithm. In this paper various
Online Algorithms have been reviewed for their efficiency and various
alternative measures have been explored for analysis purposes."
"A geometrically exact membrane formulation is presented that is based on
curvilinear coordinates and isogeometric finite elements, and is suitable for
both solid and liquid membranes. The curvilinear coordinate system is used to
describe both the theory and the finite element equations of the membrane. In
the latter case this avoids the use of local cartesian coordinates at the
element level. Consequently, no transformation of derivatives is required. The
formulation considers a split of the in-plane and out-of-plane membrane
contributions, which allows the construction of a stable formulation for liquid
membranes with constant surface tension. The proposed membrane formulation is
general, and accounts for dead and live loading, as well as enclosed volume,
area, and contact constraints. The new formulation is illustrated by several
challenging examples, considering linear and quadratic Lagrange elements, as
well as isogeometric elements based on quadratic NURBS and cubic T-splines. It
is seen that the isogeometric elements are much more accurate than standard
Lagrange elements. The gain is especially large for the liquid membrane
formulation since it depends explicitly on the surface curvature."
"Shotgun proteomics is a high-throughput technology used to identify unknown
proteins in a complex mixture. At the heart of this process is a prediction
task, the spectrum identification problem, in which each fragmentation spectrum
produced by a shotgun proteomics experiment must be mapped to the peptide
(protein subsequence) which generated the spectrum. We propose a new algorithm
for spectrum identification, based on dynamic Bayesian networks, which
significantly outperforms the de-facto standard tools for this task: SEQUEST
and Mascot."
"In this work, we presented the strategies and techniques that we have
developed for predicting the near-future churners and win-backs for a telecom
company. On a large-scale and real-world database containing customer profiles
and some transaction data from a telecom company, we first analyzed the data
schema, developed feature computation strategies and then extracted a large set
of relevant features that can be associated with the customer churning and
returning behaviors. Our features include both the original driver factors as
well as some derived features. We evaluated our features on the imbalance
corrected dataset, i.e. under-sampled dataset and compare a large number of
existing machine learning tools, especially decision tree-based classifiers,
for predicting the churners and win-backs. In general, we find RandomForest and
SimpleCart learning algorithms generally perform well and tend to provide us
with highly competitive prediction performance. Among the top-15 driver factors
that signal the churn behavior, we find that the service utilization, e.g. last
two months' download and upload volume, last three months' average upload and
download, and the payment related factors are the most indicative features for
predicting if churn will happen soon. Such features can collectively tell
discrepancies between the service plans, payments and the dynamically changing
utilization needs of the customers. Our proposed features and their
computational strategy exhibit reasonable precision performance to predict
churn behavior in near future."
"This paper discusses the use of the 3-dimensional panel method for dynamical
system simulation. Specifically, the advantages and disadvantages of model
exchange versus co-simulation of the aerodynamics and the dynamical system
model are discussed. Based on a trade-off analysis, a set of recommendations
for a panel method implementation and for a co-simulation environment is
proposed. These recommendations are implemented in a C++ library, offered
on-line under an open source license. This code is validated against XFLR5, and
its suitability for co-simulation is demonstrated with an example of a tethered
wing, i.e, a kite. The panel method implementation and the co-simulation
environment are shown to be able to solve this stiff problem in a stable
fashion."
"The issue of computing (co)homology generators of a cell complex is gaining a
pivotal role in various branches of science. While this issue can be rigorously
solved in polynomial time, it is still overly demanding for large scale
problems. Drawing inspiration from low-frequency electrodynamics, this paper
presents a physics inspired algorithm for first cohomology group computations
on three-dimensional complexes. The algorithm is general and exhibits orders of
magnitude speed up with respect to competing ones, allowing to handle problems
not addressable before. In particular, when generators are employed in the
physical modeling of magneto-quasistatic problems, this algorithm solves one of
the most long-lasting problems in low-frequency computational electromagnetics.
In this case, the effectiveness of the algorithm and its ease of implementation
may be even improved by introducing the novel concept of \textit{lazy
cohomology generators}."
"The frequency-domain fast boundary element method (BEM) combined with the
exponential window technique leads to an efficient yet simple method for
elastodynamic analysis. In this paper, the efficiency of this method is further
enhanced by three strategies. Firstly, we propose to use exponential window
with large damping parameter to improve the conditioning of the BEM matrices.
Secondly, the frequency domain windowing technique is introduced to alleviate
the severe Gibbs oscillations in time-domain responses caused by large damping
parameters. Thirdly, a solution extrapolation scheme is applied to obtain
better initial guesses for solving the sequential linear systems in the
frequency domain. Numerical results of three typical examples with the problem
size up to 0.7 million unknowns clearly show that the first and third
strategies can significantly reduce the computational time. The second strategy
can effectively eliminate the Gibbs oscillations and result in accurate
time-domain responses."
"We propose an ECG denoising method based on a feed forward neural network
with three hidden layers. Particulary useful for very noisy signals, this
approach uses the available ECG channels to reconstruct a noisy channel. We
tested the method, on all the records from Physionet MIT-BIH Arrhythmia
Database, adding electrode motion artifact noise. This denoising method
improved the perfomance of publicly available ECG analysis programs on noisy
ECG signals. This is an offline method that can be used to remove noise from
very corrupted Holter records."
"In early 1995, a DSM pilot initiative has been launched in the French islands
of Guadeloupe and Reunion through a partnership between several public and
private partners (the French Public Utility EDF, the University of Reunion
Island, low cost housing companies, architects, energy consultants, etc...) to
set up standards to improve thermal design of new residential buildings in
tropical climates. This partnership led to defining optimized bio-climatic
urban planning and architectural designs featuring the use of passive cooling
architectural principles (solar shading, natural ventilation) and components,
as well as energy efficient systems and technologies. The design and sizing of
each architectural component on internal thermal comfort in building has been
assessed with a validated thermal and airflow building simulation software
(CODYRUN). These technical specifications have been edited in a reference
document which has been used to build over 300 new pilot dwellings through the
years 1996-1998 in Reunion Island and in Guadeloupe. An experimental monitoring
has been made in these first ECODOM dwellings in 1998 and 1999. It will result
in experimental validation of impact of the passive cooling strategies on
thermal comfort of occupants leading to modify specifications if necessary. The
paper present all the methodology used for the elaboration of ECODOM, from the
simulations to the experimental results. This follow up is important, as the
setting up of the ECODOM standard will be the first step towards the setting up
of thermal regulations in the French overseas territories, by the year 2002."
"Over the last few years, several computational techniques have been devised
to recover protein complexes from the protein interaction (PPI) networks of
organisms. These techniques model ""dense"" subnetworks within PPI networks as
complexes. However, our comprehensive evaluations revealed that these
techniques fail to reconstruct many 'gold standard' complexes that are ""sparse""
in the networks (only 71 recovered out of 123 known yeast complexes embedded in
a network of 9704 interactions among 1622 proteins). In this work, we propose a
novel index called Component-Edge (CE) score to quantitatively measure the
notion of ""complex derivability"" from PPI networks. Using this index, we
theoretically categorize complexes as ""sparse"" or ""dense"" with respect to a
given network. We then devise an algorithm SPARC that selectively employs
functional interactions to improve the CE scores of predicted complexes, and
thereby elevates many of the ""sparse"" complexes to ""dense"". This empowers
existing methods to detect these ""sparse"" complexes. We demonstrate that our
approach is effective in reconstructing significantly many complexes missed
previously (104 recovered out of the 123 known complexes or ~47% improvement)."
"We investigate applicability of GPU to DEM. NVIDIA's code obtained superior
performance than CPU in computational time. A model of contact forces in
NVIDIA's code is too simple for practical use. We modify this model by
replacing it with the practical model. The simulation shows that the practical
model obtains the computing speed 6 times faster than the practical one on CPU
while 7 times slower than the simple one on GPU. The result are analyzed."
"In topology optimization, the design parameter with no contribution to the
objective function vanishes. This causes the stiffness matrix to become
singular. We show that a local optimal solution is obtained by Conjugate
Residual Method and Conjugate Gradient Method even if the stiffness matrix
becomes singular. We prove that CGMconverges to a local optimal solution in
that case. Computer simulation shows that CGM gives the same solutions obtained
by CRM in case of a cantilever beam problem."
"Present work presents a code written in the very simple programming language
MATLAB, for three dimensional linear elastostatics, using constant boundary
elements. The code, in full or in part, is not a translation or a copy of any
of the existing codes. Present paper explains how the code is written, and
lists all the formulae used. Code is verified by using the code to solve a
simple problem which has the well known approximate analytical solution. Of
course, present work does not make any contribution to research on boundary
elements, in terms of theory. But the work is justified by the fact that, to
the best of author's knowledge, as of now, one cannot find an open access
MATLAB code for three dimensional linear elastostatics using constant boundary
elements. Author hopes this paper to be of help to beginners who wish to
understand how a simple but complete boundary element code works, so that they
can build upon and modify the present open access code to solve complex
engineering problems quickly and easily. The code is available online for open
access (as supplementary file for the present paper), and may be downloaded
from the website for the present journal."
"Post buckling problem of a large deformed beam is analyzed using canonical
dual finite element method (CD-FEM). The feature of this method is to choose
correctly the canonical dual stress so that the original non-convex potential
energy functional is reformulated in a mixed complementary energy form with
both displacement and stress fields, and a pure complementary energy is
explicitly formulated in finite dimensional space. Based on the canonical
duality theory and the associated triality theorem, a primal-dual algorithm is
proposed, which can be used to find all possible solutions of this nonconvex
post-buckling problem. Numerical results show that the global maximum of the
pure-complementary energy leads to a stable buckled configuration of the beam.
While the local extrema of the pure-complementary energy present unstable
deformation states, especially. We discovered that the unstable buckled state
is very sensitive to the number of total elements and the external loads.
Theoretical results are verified through numerical examples and some
interesting phenomena in post-bifurcation of this large deformed beam are
observed."
"We formulate genome assembly problem as an optimization problem in which the
objective function is the likelihood of the assembly given the reads."
"Advances in bio-technology have made available massive amounts of functional,
structural and genomic data for many biological sequences. This increased
availability of heterogeneous biological data has resulted in biological
applications where a multiple sequence alignment (msa) is required for aligning
similar features, where a feature is described in structural, functional or
evolutionary terms. In these applications, for a given set of sequences,
depending on the feature of interest the optimal msa is likely to be different,
and sequence similarity can only be used as a rough initial estimate on the
accuracy of an msa. This has motivated the growth in template based heuristics
that supplement the sequence information with evolutionary, structural and
functional data and exploit feature similarity instead of sequence similarity
to construct multiple sequence alignments that are biologically more accurate.
However, current frameworks for designing template based heuristics do not
allow the user to explicitly specify information that can help to classify
features into types and associate weights signifying the relative importance of
a feature with respect to other features. In this paper, we first provide a
mechanism where as a part of the template information the user can explicitly
specify for each feature, its type, and weight. The type is to classify the
features into different categories based on their characteristics and the
weight signifies the relative importance of a feature with respect to other
features in that sequence. Second, we exploit the above information to define
scoring models for pair-wise sequence alignment that assume segment
conservation as opposed to single character (residue) conservation. Finally, we
present a fast progressive alignment based heuristic framework that helps in
constructing a global msa efficiently."
"One of the most powerful techniques to study protein structures is to look
for recurrent fragments (also called substructures or spatial motifs), then use
them as patterns to characterize the proteins under study. An emergent trend
consists in parsing proteins three-dimensional (3D) structures into graphs of
amino acids. Hence, the search of recurrent spatial motifs is formulated as a
process of frequent subgraph discovery where each subgraph represents a spatial
motif. In this scope, several efficient approaches for frequent subgraph
discovery have been proposed in the literature. However, the set of discovered
frequent subgraphs is too large to be efficiently analyzed and explored in any
further process. In this paper, we propose a novel pattern selection approach
that shrinks the large number of discovered frequent subgraphs by selecting the
representative ones. Existing pattern selection approaches do not exploit the
domain knowledge. Yet, in our approach we incorporate the evolutionary
information of amino acids defined in the substitution matrices in order to
select the representative subgraphs. We show the effectiveness of our approach
on a number of real datasets. The results issued from our experiments show that
our approach is able to considerably decrease the number of motifs while
enhancing their interestingness."
We introduce design and optimization considerations for the 'khmer' package.
"The ForMaRE project applies formal mathematical reasoning to economics. We
seek to increase confidence in economics' theoretical results, to aid in
discovering new results, and to foster interest in formal methods, i.e.
computer-aided reasoning, within economics. To formal methods, we seek to
contribute user experience feedback from new audiences, as well as new
challenge problems. In the first project year, we continued earlier game theory
studies but then focused on auctions, where we are building a toolbox of
formalisations, and have started to study matching and financial risk.
  In parallel to conducting research that connects economics and formal
methods, we organise events and provide infrastructure to connect both
communities, from fostering mutual awareness to targeted matchmaking. These
efforts extend beyond economics, towards generally enabling domain experts to
use mechanised reasoning."
"Petri-nets are a simple formalism for modeling concurrent computation.
Recently, they have emerged as a powerful tool for the modeling and analysis of
biochemical reaction networks, bridging the gap between purely qualitative and
quantitative models. These networks can be large and complex, which makes their
study difficult and computationally challenging. In this paper, we focus on two
structural properties of Petri-nets, siphons and traps, that bring us
information about the persistence of some molecular species. We present two
methods for enumerating all minimal siphons and traps of a Petri-net by
iterating the resolution of a boolean model interpreted as either a SAT or a
CLP(B) program. We compare the performance of these methods with a
state-of-the-art dedicated algorithm of the Petri-net community. We show that
the SAT and CLP(B) programs are both faster. We analyze why these programs
perform so well on the models of the repository of biological models
biomodels.net, and propose some hard instances for the problem of minimal
siphons enumeration."
"It is proposed to apply modern methods of nonlinear nonequilibrium
statistical mechanics to develop software algorithms that will optimally
respond to targets within short response times with minimal computer resources.
This Statistical Mechanics Algorithm for Response to Targets (SMART) can be
developed with a view towards its future implementation into a hardwired
Statistical Algorithm Multiprocessor (SAM) to enhance the efficiency and speed
of response to targets (SMART_SAM)."
"In this paper we present OSC, a scientific workflow specification language
based on software architecture principles. In contrast with other approaches,
OSC employs connectors as first-class constructs. In this way, we leverage
reusability and compositionality in the workflow modeling process, specially in
the configuration of mechanisms that manage non-functional attributes."
"Given the amino acid sequence of a protein, researchers often infer its
structure and function by finding homologous, or evolutionarily-related,
proteins of known structure and function. Since structure is typically more
conserved than sequence over long evolutionary distances, recognizing remote
protein homologs from their sequence poses a challenge.
  We first consider all proteins of known three-dimensional structure, and
explore how they cluster according to different levels of homology. An
automatic computational method reasonably approximates a human-curated
hierarchical organization of proteins according to their degree of homology.
  Next, we return to homology prediction, based only on the one-dimensional
amino acid sequence of a protein. Menke, Berger, and Cowen proposed a Markov
random field model to predict remote homology for beta-structural proteins, but
their formulation was computationally intractable on many beta-strand
topologies.
  We show two different approaches to approximate this random field, both of
which make it computationally tractable, for the first time, on all protein
folds. One method simplifies the random field itself, while the other retains
the full random field, but approximates the solution through stochastic search.
Both methods achieve improvements over the state of the art in remote homology
detection for beta-structural protein folds."
"Several experimental measurements are expressed in the form of
one-dimensional profiles, for which there is a scarcity of methodologies able
to classify the pertinence of a given result to a specific group. The
polarization curves that evaluate the corrosion kinetics of electrodes in
corrosive media are an application where the behavior is chiefly analyzed from
profiles. Polarization curves are indeed a classic method to determine the
global kinetics of metallic electrodes, but the strong nonlinearity from
different metals and alloys can overlap and the discrimination becomes a
challenging problem. Moreover, even finding a typical curve from replicated
tests requires subjective judgement. In this paper we used the so-called
multi-q approach based on the Tsallis statistics in a classification engine to
separate multiple polarization curve profiles of two stainless steels. We
collected 48 experimental polarization curves in aqueous chloride medium of two
stainless steel types, with different resistance against localized corrosion.
Multi-q pattern analysis was then carried out on a wide potential range, from
cathodic up to anodic regions. An excellent classification rate was obtained,
at a success rate of 90%, 80%, and 83% for low (cathodic), high (anodic), and
both potential ranges, respectively, using only 2% of the original profile
data. These results show the potential of the proposed approach towards
efficient, robust, systematic and automatic classification of highly non-linear
profile curves."
"The study focused on the machine learning analysis approaches to identify the
adulteration of 9 kinds of edible oil qualitatively and answered the following
three questions: Is the oil sample adulterant? How does it constitute? What is
the main ingredient of the adulteration oil? After extracting the
high-performance liquid chromatography (HPLC) data on triglyceride from 370 oil
samples, we applied the adaptive boosting with multi-class Hamming loss
(AdaBoost.MH) to distinguish the oil adulteration in contrast with the support
vector machine (SVM). Further, we regarded the adulterant oil and the pure oil
samples as ones with multiple labels and with only one label, respectively.
Then multi-label AdaBoost.MH and multi-label learning vector quantization
(ML-LVQ) model were built to determine the ingredients and their relative ratio
in the adulteration oil. The experimental results on six measures show that
ML-LVQ achieves better performance than multi-label AdaBoost.MH."
"The karyotype ontology describes the human chromosome complement as
determined cytogenetically, and is designed as an initial step toward the goal
of replacing the current system which is based on semantically meaningful
strings. This ontology uses a novel, semi-programmatic methodology based around
the tawny library to construct many classes rapidly. Here, we describe our use
case, methodology and the event-based approach that we use to represent
karyotypes.
  The ontology is available at http://www.purl.org/ontolink/karyotype/. The
clojure code is available at http://code.google.com/p/karyotype-clj/."
"A nonlinear Tracking-Differentiator is one-input-two-output system that can
generate smooth approximation of measured signals and get the derivatives of
the signals. The nonlinear tracking-Differentiator is explored to denoise and
generate the derivatives of the walks of the 3-periodicity of DNA sequences. An
improved algorithm for gene finding is presented using the nonlinear
Tracking-Differentiator. The gene finding algorithm employs the 3-base
periodicity of coding region. The 3-base periodicity DNA walks are denoised and
tracked using the nonlinear Tracking-Differentiator. Case studies demonstrate
that the nonlinear Tracking-Differentiator is an effective method to improve
the accuracy of the gene finding algorithm."
"This paper presents a practical computational approach to quantify the effect
of individual observations in estimating the state of a system. Such an
analysis can be used for pruning redundant measurements, and for designing
future sensor networks. The mathematical approach is based on computing the
sensitivity of the reanalysis (unconstrained optimization solution) with
respect to the data. The computational cost is dominated by the solution of a
linear system, whose matrix is the Hessian of the cost function, and is only
available in operator form. The right hand side is the gradient of a scalar
cost function that quantifies the forecast error of the numerical model. The
use of adjoint models to obtain the necessary first and second order
derivatives is discussed. We study various strategies to accelerate the
computation, including matrix-free iterative solvers, preconditioners, and an
in-house multigrid solver. Experiments are conducted on both a small-size
shallow-water equations model, and on a large-scale numerical weather
prediction model, in order to illustrate the capabilities of the new
methodology."
"In this paper, we develop an agent-based model which integrates four
important elements, i.e. organisational energy management policies/regulations,
energy management technologies, electric appliances and equipment, and human
behaviour, to simulate the electricity consumption in office buildings. Based
on a case study, we use this model to test the effectiveness of different
electricity management strategies, and solve practical office electricity
consumption problems. This paper theoretically contributes to an integration of
the four elements involved in the complex organisational issue of office
electricity consumption, and practically contributes to an application of an
agent-based approach for office building electricity consumption study."
"Many advances in research regarding immuno-interactions with cancer were
developed with the help of ordinary differential equation (ODE) models. These
models, however, are not effectively capable of representing problems involving
individual localisation, memory and emerging properties, which are common
characteristics of cells and molecules of the immune system. Agent-based
modelling and simulation is an alternative paradigm to ODE models that
overcomes these limitations. In this paper we investigate the potential
contribution of agent-based modelling and simulation when compared to ODE
modelling and simulation. We seek answers to the following questions: Is it
possible to obtain an equivalent agent-based model from the ODE formulation? Do
the outcomes differ? Are there any benefits of using one method compared to the
other? To answer these questions, we have considered three case studies using
established mathematical models of immune interactions with early-stage cancer.
These case studies were re-conceptualised under an agent-based perspective and
the simulation results were then compared with those from the ODE models. Our
results show that it is possible to obtain equivalent agent-based models (i.e.
implementing the same mechanisms); the simulation output of both types of
models however might differ depending on the attributes of the system to be
modelled. In some cases, additional insight from using agent-based modelling
was obtained. Overall, we can confirm that agent-based modelling is a useful
addition to the tool set of immunologists, as it has extra features that allow
for simulations with characteristics that are closer to the biological
phenomena."
"The use of genetic algorithms for the optimisation of magic angle spinning
NMR pulse sequences is discussed. The discussion uses as an example the
optimisation of the C7 dipolar recoupling pulse sequence, aiming to achieve
improved efficiency for spin systems characterised by large chemical shielding
anisotropies and/or small dipolar coupling interactions. The optimised pulse
sequence is found to be robust over a wide range of parameters, requires only
minimal a priori knowledge of the spin system for experimental implementations
with buildup rates being solely determined by the magnitude of the dipolar
coupling interaction, but is found to be less broadbanded than the original C7
pulse sequence. The optimised pulse sequence breaks the synchronicity between
r.f. pulses and sample spinning."
"The polar coordinate transformation (PCT) method has been extensively used to
treat various singular integrals in the boundary element method (BEM). However,
the resultant integrands of the PCT tend to become nearly singular when (1) the
aspect ratio of the element is large or (2) the field point is closed to the
element boundary; thus a large number of quadrature points are needed to
achieve a relatively high accuracy. In this paper, the first problem is
circumvented by using a conformal transformation so that the geometry of the
curved physical element is preserved in the transformed domain. The second
problem is alleviated by using a sigmoidal transformation, which makes the
quadrature points more concentrated around the near singularity.
  By combining the proposed two transformations with the Guiggiani's method in
[M. Guiggiani, \emph{et al}.
  A general algorithm for the numerical solution of hypersingular boundary
integral equations.
  \emph{ASME Journal of Applied Mechanics}, 59(1992), 604-614], one obtains an
efficient and robust numerical method for computing the weakly-, strongly- and
hyper-singular integrals in high-order BEM with curved elements. Numerical
integration results show that, compared with the original PCT, the present
method can reduce the number of quadrature points considerably, for given
accuracy. For further verification, the method is incorporated into a 2-order
Nystr\""om BEM code for solving acoustic Burton-Miller boundary integral
equation. It is shown that the method can retain the convergence rate of the
BEM with much less quadrature points than the existing PCT. The method is
implemented in C language and freely available."
"Transcriptome assembly from RNA-Seq reads is an active area of bioinformatics
research. The ever-declining cost and the increasing depth of RNA-Seq have
provided unprecedented opportunities to better identify expressed transcripts.
However, the nonlinear transcript structures and the ultra-high throughput of
RNA-Seq reads pose significant algorithmic and computational challenges to the
existing transcriptome assembly approaches, either reference-guided or de novo.
While reference-guided approaches offer good sensitivity, they rely on
alignment results of the splice-aware aligners and are thus unsuitable for
species with incomplete reference genomes. In contrast, de novo approaches do
not depend on the reference genome but face a computational daunting task
derived from the complexity of the graph built for the whole transcriptome. In
response to these challenges, we present a hybrid approach to exploit an
incomplete reference genome without relying on splice-aware aligners. We have
designed a split-and-align procedure to efficiently localize the reads to
individual genomic loci, which is followed by an accurate de novo assembly to
assemble reads falling into each locus. Using extensive simulation data, we
demonstrate a high accuracy and precision in transcriptome reconstruction by
comparing to selected transcriptome assembly tools. Our method is implemented
in assemblySAM, a GUI software freely available at
http://sammate.sourceforge.net."
"The noval method for mutational disease prediction using bioinformatics tools
and datasets for diagnosis the malignant mutations with powerful Artificial
Neural Network (Backpropagation Network) for classifying these malignant
mutations are related to gene(s) (like BRCA1 and BRCA2) cause a disease (breast
cancer). This noval method did not take in consideration just like adopted for
dealing, analyzing and treat the gene sequences for extracting useful
information from the sequence, also exceeded the environment factors which play
important roles in deciding and calculating some of genes features in order to
view its functional parts and relations to diseases. This paper is proposed an
enhancement of a novel method as a first way for diagnosis and prediction the
disease by mutations considering and introducing multi other features show the
alternations, changes in the environment as well as genes, comparing sequences
to gain information about the structure or function of a query sequence, also
proposing optimal and more accurate system for classification and dealing with
specific disorder using backpropagation with mean square rate 0.000000001.
Index Terms (Homology sequence, GC content and AT content, Bioinformatics,
Backpropagation Network, BLAST, DNA Sequence, Protein Sequence)"
"We extend the standard rough set-based approach to deal with huge amounts of
numeric attributes versus small amount of available objects. Here, a novel
approach of clustering along with dimensionality reduction; Hybrid Fuzzy C
Means-Quick Reduct (FCMQR) algorithm is proposed for single gene selection.
Gene selection is a process to select genes which are more informative. It is
one of the important steps in knowledge discovery. The problem is that all
genes are not important in gene expression data. Some of the genes may be
redundant, and others may be irrelevant and noisy. In this study, the entire
dataset is divided in proper grouping of similar genes by applying Fuzzy C
Means (FCM) algorithm. A high class discriminated genes has been selected based
on their degree of dependence by applying Quick Reduct algorithm based on Rough
Set Theory to all the resultant clusters. Average Correlation Value (ACV) is
calculated for the high class discriminated genes. The clusters which have the
ACV value a s 1 is determined as significant clusters, whose classification
accuracy will be equal or high when comparing to the accuracy of the entire
dataset. The proposed algorithm is evaluated using WEKA classifiers and
compared. Finally, experimental results related to the leukemia cancer data
confirm that our approach is quite promising, though it surely requires further
research."
"Although there are various types of cancer treatments, none of these
currently take into account the effect of ageing of the immune system and hence
altered responses to cancer. Recent studies have shown that in vitro
stimulation of T cells can help in the treatment of patients. There are many
factors that have to be considered when simulating an organism's
immunocompetence. Our particular interest lies in the study of loss of
immunocompetence with age. We are trying to answer questions such as: Given a
certain age of a patient, how fit is their immune system to fight cancer? Would
an immune boost improve the effectiveness of a cancer treatment given the
patient's immune phenotype and age? We believe that understanding the processes
of immune system ageing and degradation through computer simulation may help in
answering these questions. Specifically, we have decided to look at the change
in numbers of naive T cells with age, as they play a important role in
responses to cancer and anti-tumour vaccination. In this work we present an
agent-based simulation model to understand the interactions which influence the
naive T cell populations over time. Our agent model is based on existing
mathematical system dynamic model, but in comparisons offers better scope for
customisation and detailed analysis. We believe that the results obtained can
in future help with the modelling of T cell populations inside tumours."
"In this paper we present our ideas for conducting a cost benefit analysis by
using three different methods: scenario analysis, decision trees and
simulation. Then we introduce our case study and examine these methods in a
real world situation. We show how these tools can be used and what the results
are for each of them. Our aim is to conduct a comparison of these different
probabilistic methods of estimating costs for port security risk assessment
studies. Methodologically, we are trying to understand the limits of all the
tools mentioned above by focusing on rare events."
"System dynamics and agent based simulation models can both be used to model
and understand interactions of entities within a population. Our modeling work
presented here is concerned with understanding the suitability of the different
types of simulation for the immune system aging problems and comparing their
results. We are trying to answer questions such as: How fit is the immune
system given a certain age? Would an immune boost be of therapeutic value, e.g.
to improve the effectiveness of a simultaneous vaccination? Understanding the
processes of immune system aging and degradation may also help in development
of therapies that reverse some of the damages caused thus improving life
expectancy. Therefore as a first step our research focuses on T cells; major
contributors to immune system functionality. One of the main factors
influencing immune system aging is the output rate of naive T cells. Of further
interest is the number and phenotypical variety of these cells in an
individual, which will be the case study focused on in this paper."
"Next generation computer architectures will include order of magnitude more
intra-node parallelism; however, many application programmers have a difficult
time keeping their codes current with the state-of-the-art machines. In this
context, we analyze Hyperbolic PDE solvers, which are used in the solution of
many important applications in science and engineering. We present ManyClaw, a
project intended to explore the exploitation of intra-node parallelism in
hyperbolic PDE solvers via the Clawpack software package for solving hyperbolic
PDEs. Our goal is to separate the low level parallelism and the physical
equations thus providing users the capability to leverage intra-node
parallelism without explicitly writing code to take advantage of newer
architectures."
"A recent nature inspired optimization algorithm, Fish School Search (FSS) is
applied to the finite element model (FEM) updating problem. This method is
tested on a GARTEUR SM-AG19 aeroplane structure. The results of this algorithm
are compared with two other metaheuristic algorithms; Genetic Algorithm (GA)
and Particle Swarm Optimization (PSO). It is observed that on average, the FSS
and PSO algorithms give more accurate results than the GA. A minor modification
to the FSS is proposed. This modification improves the performance of FSS on
the FEM updating problem which has a constrained search space."