summary
"This paper describes how to implement a documentation technique that helps
readers to understand large programs or collections of programs, by providing
local indexes to all identifiers that are visible on every two-page spread. A
detailed example is given for a program that finds all Hamiltonian circuits in
an undirected graph."
"The Asynchronous pi-calculus, as recently proposed by Boudol and,
independently, by Honda and Tokoro, is a subset of the pi-calculus which
contains no explicit operators for choice and output-prefixing. The
communication mechanism of this calculus, however, is powerful enough to
simulate output-prefixing, as shown by Boudol, and input-guarded choice, as
shown recently by Nestmann and Pierce. A natural question arises, then, whether
or not it is possible to embed in it the full pi-calculus. We show that this is
not possible, i.e. there does not exist any uniform, parallel-preserving,
translation from the pi-calculus into the asynchronous pi-calculus, up to any
``reasonable'' notion of equivalence. This result is based on the incapablity
of the asynchronous pi-calculus of breaking certain symmetries possibly present
in the initial communication graph. By similar arguments, we prove a separation
result between the pi-calculus and CCS."
"The inclusion of universal quantification and a form of implication in goals
in logic programming is considered. These additions provide a logical basis for
scoping but they also raise new implementation problems. When universal and
existential quantifiers are permitted to appear in mixed order in goals, the
devices of logic variables and unification that are employed in solving
existential goals must be modified to ensure that constraints arising out of
the order of quantification are respected. Suitable modifications that are
based on attaching numerical tags to constants and variables and on using these
tags in unification are described. The resulting devices are amenable to an
efficient implementation and can, in fact, be assimilated easily into the usual
machinery of the Warren Abstract Machine (WAM). The provision of implications
in goals results in the possibility of program clauses being added to the
program for the purpose of solving specific subgoals. A naive scheme based on
asserting and retracting program clauses does not suffice for implementing such
additions for two reasons. First, it is necessary to also support the
resurrection of an earlier existing program in the face of backtracking.
Second, the possibility for implication goals to be surrounded by quantifiers
requires a consideration of the parameterization of program clauses by bindings
for their free variables. Devices for supporting these additional requirements
are described as also is the integration of these devices into the WAM. Further
extensions to the machine are outlined for handling higher-order additions to
the language. The ideas presented here are relevant to the implementation of
the higher-order logic programming language lambda Prolog."
"Reflective systems allow their own structures to be altered from within. Here
we are concerned with a style of reflection, called linguistic reflection,
which is the ability of a running program to generate new program fragments and
to integrate these into its own execution. In particular we describe how this
kind of reflection may be provided in the compiler-based, strongly typed
object-oriented programming language Java. The advantages of the programming
technique include attaining high levels of genericity and accommodating system
evolution. These advantages are illustrated by an example taken from persistent
programming which shows how linguistic reflection allows functionality (program
code) to be generated on demand (Just-In-Time) from a generic specification and
integrated into the evolving running program. The technique is evaluated
against alternative implementation approaches with respect to efficiency,
safety and ease of use."
"A polymorphic analysis is an analysis whose input and output contain
parameters which serve as placeholders for information that is unknown before
analysis but provided after analysis. In this paper, we present a polymorphic
groundness analysis that infers parameterised groundness descriptions of the
variables of interest at a program point. The polymorphic groundness analysis
is designed by replacing two primitive operators used in a monomorphic
groundness analysis and is shown to be as precise as the monomorphic groundness
analysis for any possible values for mode parameters. Experimental results of a
prototype implementation of the polymorphic groundness analysis are given."
"The universal object oriented languages made programming more simple and
efficient. In the article is considered possibilities of using similar methods
in computer algebra. A clear and powerful universal language is useful if
particular problem was not implemented in standard software packages like
REDUCE, MATHEMATICA, etc. and if the using of internal programming languages of
the packages looks not very efficient.
  Functional languages like LISP had some advantages and traditions for
algebraic and symbolic manipulations. Functional and object oriented
programming are not incompatible ones. An extension of the model of an object
for manipulation with pure functions and algebraic expressions is considered."
"By paying more attention to semantics-based tool generation, programming
language semantics can significantly increase its impact. Ultimately, this may
lead to ``Language Design Assistants'' incorporating substantial amounts of
semantic knowledge."
"With no intent of starting a holy war, this paper lists several annoying C++
birthmarks that the author has come across developing GUI class libraries.
C++'s view of classes, instances and hierarchies appears tantalizingly close to
GUI concepts of controls, widgets, window classes and subwindows. OO models of
C++ and of a window system are however different. C++ was designed to be a
""static"" language with a lexical name scoping, static type checking and
hierarchies defined at compile time. Screen objects on the other hand are
inherently dynamic; they usually live well beyond the procedure/block that
created them; the hierarchy of widgets is defined to a large extent by layout,
visibility and event flow. Many GUI fundamentals such as dynamic and geometric
hierarchies of windows and controls, broadcasting and percolation of events are
not supported directly by C++ syntax or execution semantics (or supported as
""exceptions"" -- pun intended). Therefore these features have to be emulated in
C++ GUI code. This leads to duplication of a graphical toolkit or a window
manager functionality, code bloat, engaging in unsafe practices and forgoing of
many strong C++ features (like scoping rules and compile-time type checking).
This paper enumerates a few major C++/GUI sores and illustrates them on simple
examples."
"Restructuring compilers use dependence analysis to prove that the meaning of
a program is not changed by a transformation. A well-known limitation of
dependence analysis is that it examines only the memory locations read and
written by a statement, and does not assume any particular interpretation for
the operations in that statement. Exploiting the semantics of these operations
enables a wider set of transformations to be used, and is critical for
optimizing important codes such as LU factorization with pivoting.
  Symbolic execution of programs enables the exploitation of such semantic
properties, but it is intractable for all but the simplest programs. In this
paper, we propose a new form of symbolic analysis for use in restructuring
compilers. Fractal symbolic analysis compares a program and its transformed
version by repeatedly simplifying these programs until symbolic analysis
becomes tractable, ensuring that equality of simplified programs is sufficient
to guarantee equality of the original programs. We present a prototype
implementation of fractal symbolic analysis, and show how it can be used to
optimize the cache performance of LU factorization with pivoting."
"The Task System and Item Architecture (TSIA) is a model for transparent
application execution. In many real-world projects, a TSIA provides a simple
application with a transparent reliable, distributed, heterogeneous, adaptive,
dynamic, real-time, parallel, secure or other execution. TSIA is suitable for
many applications, not just for the simple applications served to date. This
presentation shows that TSIA is a dataflow model - a long-standing model for
transparent parallel execution. The advances to the dataflow model include a
simple semantics, as well as support for input/output, for modifiable items and
for other such effects."
"In (Ferrucci, Pacini and Sessa, 1995) an extended form of resolution, called
Reduced SLD resolution (RSLD), is introduced. In essence, an RSLD derivation is
an SLD derivation such that redundancy elimination from resolvents is performed
after each rewriting step. It is intuitive that redundancy elimination may have
positive effects on derivation process. However, undesiderable effects are also
possible. In particular, as shown in this paper, program termination as well as
completeness of loop checking mechanisms via a given selection rule may be
lost. The study of such effects has led us to an analysis of selection rule
basic concepts, so that we have found convenient to move the attention from
rules of atom selection to rules of atom scheduling. A priority mechanism for
atom scheduling is built, where a priority is assigned to each atom in a
resolvent, and primary importance is given to the event of arrival of new atoms
from the body of the applied clause at rewriting time. This new computational
model proves able to address the study of redundancy elimination effects,
giving at the same time interesting insights into general properties of
selection rules. As a matter of fact, a class of scheduling rules, namely the
specialisation independent ones, is defined in the paper by using not trivial
semantic arguments. As a quite surprising result, specialisation independent
scheduling rules turn out to coincide with a class of rules which have an
immediate structural characterisation (named stack-queue rules). Then we prove
that such scheduling rules are tolerant to redundancy elimination, in the sense
that neither program termination nor completeness of equality loop check is
lost passing from SLD to RSLD."
"Forty years ago Dijkstra introduced the current conventional execution of
routines. It places activation frames onto a stack. Each frame is the internal
state of an executing routine. The resulting application execution is not
easily helped by an external system. This presentation proposes an alternative
execution of routines. It places task frames onto the stack. A task frame is
the call of a routine to be executed. The feasibility of the alternative
execution is demonstrated by a crude implementation. As described elsewhere, an
application which executes in terms of tasks can be provided by an external
system with a transparent reliable, distributed, heterogeneous, adaptive,
dynamic, real-time, parallel, secure or other execution. By extending the crude
implementation, this presentation outlines a simple transparent parallel
execution."
"While application software does the real work, domain-specific languages
(DSLs) are tools to help produce it efficiently, and language design assistants
in turn are meta-tools to help produce DSLs quickly. DSLs are already in wide
use (HTML for web pages, Excel macros for spreadsheet applications, VHDL for
hardware design, ...), but many more will be needed for both new as well as
existing application domains. Language design assistants to help develop them
currently exist only in the basic form of language development systems. After a
quick look at domain-specific languages, and especially their relationship to
application libraries, we survey existing language development systems and give
an outline of future language design assistants."
"The aim of this work is to define and implement an extended C++ language to
support the SIMD programming paradigm. The C++ programming language has been
extended to express all the potentiality of an abstract SIMD machine consisting
of a central Control Processor and a N-dimensional toroidal array of Numeric
Processors. Very few extensions have been added to the standard C++ with the
goal of minimising the effort for the programmer in learning a new language and
to keep very high the performance of the compiled code. The proposed language
has been implemented as a porting of the GNU C++ Compiler on a SIMD
supercomputer."
"The goal of this paper is the description and analysis of multimethod
implementation in a new object-oriented, class-based programming language
called OOLANG. The implementation of the multimethod typecheck and selection,
deeply analyzed in the paper, is performed in two phases in order to allow
static typechecking and separate compilation of modules. The first phase is
performed at compile time, while the second is executed at link time and does
not require the modules' source code. OOLANG has syntax similar to C++; the
main differences are the absence of pointers and the realization of
polymorphism through subsumption. It adopts the C++ object model and supports
multiple inheritance as well as virtual base classes. For this reason, it has
been necessary to define techniques for realigning argument and return value
addresses when performing multimethod invocations."
"Type classes are an elegant extension to traditional, Hindley-Milner based
typing systems. They are used in modern, typed languages such as Haskell to
support controlled overloading of symbols. Haskell 98 supports only
single-parameter and constructor type classes. Other extensions such as
multi-parameter type classes are highly desired but are still not officially
supported by Haskell. Subtle issues arise with extensions, which may lead to a
loss of feasible type inference or ambiguous programs. A proper logical basis
for type class systems seems to be missing. Such a basis would allow extensions
to be characterised and studied rigorously. We propose to employ Constraint
Handling Rules as a tool to study and develop type class systems in a uniform
way."
"Aldwych is proposed as the foundation of a general purpose language for
parallel applications. It works on a rule-based principle, and has aspects
variously of concurrent functional, logic and object-oriented languages, yet it
forms an integrated whole. It is intended to be applicable both for small-scale
parallel programming, and for large-scale open systems."
"We describe an approach to programming rule-based systems in Standard ML,
with a focus on so-called overlapping rules, that is rules that can still be
active when other rules are fired. Such rules are useful when implementing
rule-based reactive systems, and to that effect we show a simple implementation
of Loyall's Active Behavior Trees, used to control goal-directed agents in the
Oz virtual environment. We discuss an implementation of our framework using a
reactive library geared towards implementing those kind of systems."
"This paper illustrates how the diagram programming language DiaPlan can be
used to program visual systems. DiaPlan is a visual rule-based language that is
founded on the computational model of graph transformation. The language
supports object-oriented programming since its graphs are hierarchically
structured. Typing allows the shape of these graphs to be specified recursively
in order to increase program security. Thanks to its genericity, DiaPlan allows
to implement systems that represent and manipulate data in arbitrary diagram
notations. The environment for the language exploits the diagram editor
generator DiaGen for providing genericity, and for implementing its user
interface and type checker."
"We present a new approach to termination analysis of logic programs. The
essence of the approach is that we make use of general term-orderings (instead
of level mappings), like it is done in transformational approaches to logic
program termination analysis, but that we apply these orderings directly to the
logic program and not to the term-rewrite system obtained through some
transformation. We define some variants of acceptability, based on general
term-orderings, and show how they are equivalent to LD-termination. We develop
a demand driven, constraint-based approach to verify these
acceptability-variants.
  The advantage of the approach over standard acceptability is that in some
cases, where complex level mappings are needed, fairly simple term-orderings
may be easily generated. The advantage over transformational approaches is that
it avoids the transformation step all together."
"For logic programs with arithmetic predicates, showing termination is not
easy, since the usual order for the integers is not well-founded. A new method,
easily incorporated in the TermiLog system for automatic termination analysis,
is presented for showing termination in this case.
  The method consists of the following steps: First, a finite abstract domain
for representing the range of integers is deduced automatically. Based on this
abstraction, abstract interpretation is applied to the program. The result is a
finite number of atoms abstracting answers to queries which are used to extend
the technique of query-mapping pairs. For each query-mapping pair that is
potentially non-terminating, a bounded (integer-valued) termination function is
guessed. If traversing the pair decreases the value of the termination
function, then termination is established. Simple functions often suffice for
each query-mapping pair, and that gives our approach an edge over the classical
approach of using a single termination function for all loops, which must
inevitably be more complicated and harder to guess automatically. It is worth
noting that the termination of McCarthy's 91 function can be shown
automatically using our method.
  In summary, the proposed approach is based on combining a finite abstraction
of the integers with the technique of the query-mapping pairs, and is
essentially capable of dividing a termination proof into several cases, such
that a simple termination function suffices for each case. Consequently, the
whole process of proving termination can be done automatically in the framework
of TermiLog and similar systems."
"This paper describes a general framework for automatic termination analysis
of logic programs, where we understand by ``termination'' the finitenes s of
the LD-tree constructed for the program and a given query. A general property
of mappings from a certain subset of the branches of an infinite LD-tree into a
finite set is proved. From this result several termination theorems are
derived, by using different finite sets. The first two are formulated for the
predicate dependency and atom dependency graphs. Then a general result for the
case of the query-mapping pairs relevant to a program is proved (cf.
\cite{Sagiv,Lindenstrauss:Sagiv}). The correctness of the {\em TermiLog} system
described in \cite{Lindenstrauss:Sagiv:Serebrenik} follows from it. In this
system it is not possible to prove termination for programs involving
arithmetic predicates, since the usual order for the integers is not
well-founded. A new method, which can be easily incorporated in {\em TermiLog}
or similar systems, is presented, which makes it possible to prove termination
for programs involving arithmetic predicates. It is based on combining a finite
abstraction of the integers with the technique of the query-mapping pairs, and
is essentially capable of dividing a termination proof into several cases, such
that a simple termination function suffices for each case. Finally several
possible extensions are outlined."
"Complementation, the inverse of the reduced product operation, is a technique
for systematically finding minimal decompositions of abstract domains. File'
and Ranzato advanced the state of the art by introducing a simple method for
computing a complement. As an application, they considered the extraction by
complementation of the pair-sharing domain PS from the Jacobs and Langen's
set-sharing domain SH. However, since the result of this operation was still
SH, they concluded that PS was too abstract for this. Here, we show that the
source of this result lies not with PS but with SH and, more precisely, with
the redundant information contained in SH with respect to ground-dependencies
and pair-sharing. In fact, a proper decomposition is obtained if the
non-redundant version of SH, PSD, is substituted for SH. To establish the
results for PSD, we define a general schema for subdomains of SH that includes
PSD and Def as special cases. This sheds new light on the structure of PSD and
exposes a natural though unexpected connection between Def and PSD. Moreover,
we substantiate the claim that complementation alone is not sufficient to
obtain truly minimal decompositions of domains. The right solution to this
problem is to first remove redundancies by computing the quotient of the domain
with respect to the observable behavior, and only then decompose it by
complementation."
"In this paper we investigate the theoretical foundation of a new bottom-up
semantics for linear logic programs, and more precisely for the fragment of
LinLog that consists of the language LO enriched with the constant 1. We use
constraints to symbolically and finitely represent possibly infinite
collections of provable goals. We define a fixpoint semantics based on a new
operator in the style of Tp working over constraints. An application of the
fixpoint operator can be computed algorithmically. As sufficient conditions for
termination, we show that the fixpoint computation is guaranteed to converge
for propositional LO. To our knowledge, this is the first attempt to define an
effective fixpoint semantics for linear logic programs. As an application of
our framework, we also present a formal investigation of the relations between
LO and Disjunctive Logic Programming. Using an approach based on abstract
interpretation, we show that DLP fixpoint semantics can be viewed as an
abstraction of our semantics for LO. We prove that the resulting abstraction is
correct and complete for an interesting class of LO programs encoding Petri
Nets."
"It is important that practical data-flow analyzers are backed by reliably
proven theoretical results. Abstract interpretation provides a sound
mathematical framework and necessary generic properties for an abstract domain
to be well-defined and sound with respect to the concrete semantics. In logic
programming, the abstract domain Sharing is a standard choice for sharing
analysis for both practical work and further theoretical study. In spite of
this, we found that there were no satisfactory proofs for the key properties of
commutativity and idempotence that are essential for Sharing to be well-defined
and that published statements of the soundness of Sharing assume the
occurs-check. This paper provides a generalization of the abstraction function
for Sharing that can be applied to any language, with or without the
occurs-check. Results for soundness, idempotence and commutativity for abstract
unification using this abstraction function are proven."
"Prolog was once the main host for implementing constraint solvers.
  It seems that it is no longer so. To be useful, constraint solvers have to be
integrable into industrial applications written in imperative or
object-oriented languages; to be efficient, they have to interact with other
solvers. To meet these requirements, many solvers are now implemented in the
form of extensible object-oriented libraries. Following Pfister and Szyperski,
we argue that ``objects are not enough,'' and we propose to design solvers as
component-oriented libraries. We illustrate our approach by the description of
the architecture of a prototype, and we assess its strong points and
weaknesses."
"We propose an extension of the asynchronous pi-calculus with a notion of
random choice. We define an operational semantics which distinguishes between
probabilistic choice, made internally by the process, and nondeterministic
choice, made externally by an adversary scheduler. This distinction will allow
us to reason about the probabilistic correctness of algorithms under certain
schedulers. We show that in this language we can solve the electoral problem,
which was proved not possible in the asynchronous $\pi$-calculus. Finally, we
show an implementation of the probabilistic asynchronous pi-calculus in a
Java-like language."
"We consider a generalization of the dining philosophers problem to arbitrary
connection topologies. We focus on symmetric, fully distributed systems, and we
address the problem of guaranteeing progress and lockout-freedom, even in
presence of adversary schedulers, by using randomized algorithms. We show that
the well-known algorithms of Lehmann and Rabin do not work in the generalized
case, and we propose an alternative algorithm based on the idea of letting the
philosophers assign a random priority to their adjacent forks."
"ELAN is a powerful language and environment for specifying and prototyping
deduction systems in a language based on rewrite rules controlled by
strategies. Timed automata is a class of continuous real-time models of
reactive systems for which efficient model-checking algorithms have been
devised. In this paper, we show that these algorithms can very easily be
prototyped in the ELAN system. This paper argues through this example that
rewriting based systems relying on rules and strategies are a good framework to
prototype, study and test rather efficiently symbolic model-checking
algorithms, i.e. algorithms which involve combination of graph exploration
rules, deduction rules, constraint solving techniques and decision procedures."
"Nomadic applications create replicas of shared objects that evolve
independently while they are disconnected. When reconnecting, the system has to
reconcile the divergent replicas. In the log-based approach to reconciliation,
such as in the IceCube system, the input is a common initial state and logs of
actions that were performed on each replica. The output is a consistent global
schedule that maximises the number of accepted actions. The reconciler merges
the logs according to the schedule, and replays the operations in the merged
log against the initial state, yielding to a reconciled common final state.
  In this paper, we show the NP-completeness of the log-based reconciliation
problem and present two programs for solving it. Firstly, a constraint logic
program (CLP) that uses integer constraints for expressing precedence
constraints, boolean constraints for expressing dependencies between actions,
and some heuristics for guiding the search. Secondly, a stochastic local search
method with Tabu heuristic (LS), that computes solutions in an incremental
fashion but does not prove optimality. One difficulty in the LS modeling lies
in the handling of both boolean variables and integer variables, and in the
handling of the objective function which differs from a max-CSP problem.
Preliminary evaluation results indicate better performance for the CLP program
which, on somewhat realistic benchmarks, finds nearly optimal solutions up to a
thousands of actions and proves optimality up to a hundreds of actions."
"This paper focuses on the branching process for solving any constraint
satisfaction problem (CSP). A parametrised schema is proposed that (with
suitable instantiations of the parameters) can solve CSP's on both finite and
infinite domains. The paper presents a formal specification of the schema and a
statement of a number of interesting properties that, subject to certain
conditions, are satisfied by any instances of the schema.
  It is also shown that the operational procedures of many constraint systems
including cooperative systems) satisfy these conditions.
  Moreover, the schema is also used to solve the same CSP in different ways by
means of different instantiations of its parameters."
"The paper presents two CLP approaches to 2D angle placements, implemented in
CHIP v.5.3. The first is based on the classical (rectangular) cumulative global
constraint, the second on the new trapezoidal cumulative global constraint.
Both approaches are applied to a specific presented."
"In this paper we present the use of Constraint Programming for solving
balanced academic curriculum problems. We discuss the important role that
heuristics play when solving a problem using a constraint-based approach. We
also show how constraint solving techniques allow to very efficiently solve
combinatorial optimization problems that are too hard for integer programming
techniques."
"Homepage of the workshop proceedings, with links to all individually archived
papers"
"Compile-time garbage collection (CTGC) is still a very uncommon feature
within compilers. In previous work we have developed a compile-time structure
reuse system for Mercury, a logic programming language. This system indicates
which datastructures can safely be reused at run-time. As preliminary
experiments were promising, we have continued this work and have now a working
and well performing near-to-ship CTGC-system built into the Melbourne Mercury
Compiler (MMC).
  In this paper we present the multiple design decisions leading to this
system, we report the results of using CTGC for a set of benchmarks, including
a real-world program, and finally we discuss further possible improvements.
Benchmarks show substantial memory savings and a noticeable reduction in
execution time."
"Boolean functions can be used to express the groundness of, and trace
grounding dependencies between, program variables in (constraint) logic
programs. In this paper, a variety of issues pertaining to the efficient Prolog
implementation of groundness analysis are investigated, focusing on the domain
of definite Boolean functions, Def. The systematic design of the representation
of an abstract domain is discussed in relation to its impact on the algorithmic
complexity of the domain operations; the most frequently called operations
should be the most lightweight. This methodology is applied to Def, resulting
in a new representation, together with new algorithms for its domain operations
utilising previously unexploited properties of Def -- for instance,
quadratic-time entailment checking. The iteration strategy driving the analysis
is also discussed and a simple, but very effective, optimisation of induced
magic is described. The analysis can be implemented straightforwardly in Prolog
and the use of a non-ground representation results in an efficient, scalable
tool which does not require widening to be invoked, even on the largest
benchmarks. An extensive experimental evaluation is given"
"The semantics of the Prolog ``cut'' construct is explored in the context of
some desirable properties of logic programming systems, referred to as the
witness properties. The witness properties concern the operational consistency
of responses to queries. A generalization of Prolog with negation as failure
and cut is described, and shown not to have the witness properties. A
restriction of the system is then described, which preserves the choice and
first-solution behaviour of cut but allows the system to have the witness
properties.
  The notion of cut in the restricted system is more restricted than the Prolog
hard cut, but retains the useful first-solution behaviour of hard cut, not
retained by other proposed cuts such as the ``soft cut''. It is argued that the
restricted system achieves a good compromise between the power and utility of
the Prolog cut and the need for internal consistency in logic programming
systems. The restricted system is given an abstract semantics, which depends on
the witness properties; this semantics suggests that the restricted system has
a deeper connection to logic than simply permitting some computations which are
logical.
  Parts of this paper appeared previously in a different form in the
Proceedings of the 1995 International Logic Programming Symposium."
"This paper introduces a framework of parametric descriptive directional types
for constraint logic programming (CLP). It proposes a method for locating type
errors in CLP programs and presents a prototype debugging tool. The main
technique used is checking correctness of programs w.r.t. type specifications.
The approach is based on a generalization of known methods for proving
correctness of logic programs to the case of parametric specifications.
Set-constraint techniques are used for formulating and checking verification
conditions for (parametric) polymorphic type specifications. The specifications
are expressed in a parametric extension of the formalism of term grammars. The
soundness of the method is proved and the prototype debugging tool supporting
the proposed approach is illustrated on examples.
  The paper is a substantial extension of the previous work by the same authors
concerning monomorphic directional types."
"We study program refactoring while considering the language or even the
programming paradigm as a parameter. We use typed functional programs, namely
Haskell programs, as the specification medium for a corresponding refactoring
framework. In order to detach ourselves from language syntax, our
specifications adhere to the following style. (I) As for primitive algorithms
for program analysis and transformation, we employ generic function combinators
supporting generic traversal and polymorphic functions refined by ad-hoc cases.
(II) As for the language abstractions involved in refactorings, we design a
dedicated multi-parameter class. This class can be instantiated for
abstractions as present in various languages, e.g., Java, Prolog or Haskell."
"In order to improve precision and efficiency sharing analysis should track
both freeness and linearity. The abstract unification algorithms for these
combined domains are suboptimal, hence there is scope for improving precision.
This paper proposes three optimisations for tracing sharing in combination with
freeness and linearity. A novel connection between equations and sharing
abstractions is used to establish correctness of these optimisations even in
the presence of rational trees. A method for pruning intermediate sharing
abstractions to improve efficiency is also proposed. The optimisations are
lightweight and therefore some, if not all, of these optimisations will be of
interest to the implementor."
"In previous work, we have introduced functional strategies, that is,
first-class generic functions that can traverse into terms of any type while
mixing uniform and type-specific behaviour. In the present paper, we give a
detailed description of one particular Haskell-based model of functional
strategies. This model is characterised as follows. Firstly, we employ
first-class polymorphism as a form of second-order polymorphism as for the mere
types of functional strategies. Secondly, we use an encoding scheme of run-time
type case for mixing uniform and type-specific behaviour. Thirdly, we base all
traversal on a fundamental combinator for folding over constructor
applications.
  Using this model, we capture common strategic traversal schemes in a highly
parameterised style. We study two original forms of parameterisation. Firstly,
we design parameters for the specific control-flow, data-flow and traversal
characteristics of more concrete traversal schemes. Secondly, we use
overloading to postpone commitment to a specific type scheme of traversal. The
resulting portfolio of traversal schemes can be regarded as a challenging
benchmark for setups for typed generic programming.
  The way we develop the model and the suite of traversal schemes, it becomes
clear that parameterised + typed strategic programming is best viewed as a
potent combination of certain bits of parametric, intensional, polytypic, and
ad-hoc polymorphism."
"In previous work, we introduced the fundamentals and a supporting combinator
library for \emph{strategic programming}. This an idiom for generic programming
based on the notion of a \emph{functional strategy}: a first-class generic
function that cannot only be applied to terms of any type, but which also
allows generic traversal into subterms and can be customized with type-specific
behaviour.
  This paper seeks to provide practicing functional programmers with pragmatic
guidance in crafting their own strategic programs. We present the fundamentals
and the support from a user's perspective, and we initiate a catalogue of
\emph{strategy design patterns}. These design patterns aim at consolidating
strategic programming expertise in accessible form."
"We study one dimension in program evolution, namely the evolution of the
datatype declarations in a program. To this end, a suite of basic
transformation operators is designed. We cover structure-preserving
refactorings, but also structure-extending and -reducing adaptations. Both the
object programs that are subject to datatype transformations, and the meta
programs that encode datatype transformations are functional programs."
"A typed model of strategic term rewriting is developed. The key innovation is
that generic traversal is covered. To this end, we define a typed rewriting
calculus S'_{gamma}. The calculus employs a many-sorted type system extended by
designated generic strategy types gamma. We consider two generic strategy
types, namely the types of type-preserving and type-unifying strategies.
S'_{gamma} offers traversal combinators to construct traversals or schemes
thereof from many-sorted and generic strategies. The traversal combinators
model different forms of one-step traversal, that is, they process the
immediate subterms of a given term without anticipating any scheme of recursion
into terms. To inhabit generic types, we need to add a fundamental combinator
to lift a many-sorted strategy $s$ to a generic type gamma. This step is called
strategy extension. The semantics of the corresponding combinator states that s
is only applied if the type of the term at hand fits, otherwise the extended
strategy fails. This approach dictates that the semantics of strategy
application must be type-dependent to a certain extent. Typed strategic term
rewriting with coverage of generic term traversal is a simple but expressive
model of generic programming. It has applications in program transformation and
program analysis."
"Oz is a multiparadigm language that supports logic programming as one of its
major paradigms. A multiparadigm language is designed to support different
programming paradigms (logic, functional, constraint, object-oriented,
sequential, concurrent, etc.) with equal ease. This article has two goals: to
give a tutorial of logic programming in Oz and to show how logic programming
fits naturally into the wider context of multiparadigm programming. Our
experience shows that there are two classes of problems, which we call
algorithmic and search problems, for which logic programming can help formulate
practical solutions. Algorithmic problems have known efficient algorithms.
Search problems do not have known efficient algorithms but can be solved with
search. The Oz support for logic programming targets these two problem classes
specifically, using the concepts needed for each. This is in contrast to the
Prolog approach, which targets both classes with one set of concepts, which
results in less than optimal support for each class. To explain the essential
difference between algorithmic and search programs, we define the Oz execution
model. This model subsumes both concurrent logic programming
(committed-choice-style) and search-based logic programming (Prolog-style).
Instead of Horn clause syntax, Oz has a simple, fully compositional,
higher-order syntax that accommodates the abilities of the language. We
conclude with lessons learned from this work, a brief history of Oz, and many
entry points into the Oz literature."
"Dedicated to the memory of Edsger W.Dijkstra.
  Representation independence or relational parametricity formally
characterizes the encapsulation provided by language constructs for data
abstraction and justifies reasoning by simulation. Representation independence
has been shown for a variety of languages and constructs but not for shared
references to mutable state; indeed it fails in general for such languages.
This paper formulates representation independence for classes, in an
imperative, object-oriented language with pointers, subclassing and dynamic
dispatch, class oriented visibility control, recursive types and methods, and a
simple form of module. An instance of a class is considered to implement an
abstraction using private fields and so-called representation objects.
Encapsulation of representation objects is expressed by a restriction, called
confinement, on aliasing. Representation independence is proved for programs
satisfying the confinement condition. A static analysis is given for
confinement that accepts common designs such as the observer and factory
patterns. The formalization takes into account not only the usual interface
between a client and a class that provides an abstraction but also the
interface (often called ``protected'') between the class and its subclasses."
"In previous work, we introduced the notion of functional strategies:
first-class generic functions that can traverse terms of any type while mixing
uniform and type-specific behaviour. Functional strategies transpose the notion
of term rewriting strategies (with coverage of traversal) to the functional
programming paradigm. Meanwhile, a number of Haskell-based models and
combinator suites were proposed to support generic programming with functional
strategies.
  In the present paper, we provide a compact and matured reconstruction of
functional strategies. We capture strategic polymorphism by just two primitive
combinators. This is done without commitment to a specific functional language.
We analyse the design space for implementational models of functional
strategies. For completeness, we also provide an operational reference model
for implementing functional strategies (in Haskell). We demonstrate the
generality of our approach by reconstructing representative fragments of the
Strafunski library for functional strategies."
"Flavor (Formal Language for Audio-Visual Object Representation) has been
created as a language for describing coded multimedia bitstreams in a formal
way so that the code for reading and writing bitstreams can be automatically
generated. It is an extension of C++ and Java, in which the typing system
incorporates bitstream representation semantics. This allows describing in a
single place both the in-memory representation of data as well as their
bitstream-level (compressed) representation. Flavor also comes with a
translator that automatically generates standard C++ or Java code from the
Flavor source code so that direct access to compressed multimedia information
by application developers can be achieved with essentially zero programming.
Flavor has gone through many enhancements and this paper fully describes the
latest version of the language and the translator. The software has been made
into an open source project as of Version 4.1, and the latest downloadable
Flavor package is available at http://flavor.sourceforge.net."
"The past years have seen widening efforts at increasing Prolog's
declarativeness and expressiveness. Tabling has proved to be a viable technique
to efficiently overcome SLD's susceptibility to infinite loops and redundant
subcomputations. Our research demonstrates that implicit or-parallelism is a
natural fit for logic programs with tabling. To substantiate this belief, we
have designed and implemented an or-parallel tabling engine -- OPTYap -- and we
used a shared-memory parallel machine to evaluate its performance. To the best
of our knowledge, OPTYap is the first implementation of a parallel tabling
engine for logic programming systems. OPTYap builds on Yap's efficient
sequential Prolog engine. Its execution model is based on the SLG-WAM for
tabling, and on the environment copying for or-parallelism.
  Preliminary results indicate that the mechanisms proposed to parallelize
search in the context of SLD resolution can indeed be effectively and naturally
generalized to parallelize tabled computations, and that the resulting systems
can achieve good performance on shared-memory parallel machines. More
importantly, it emphasizes our belief that through applying or-parallelism and
tabling to logic programs the range of applications for Logic Programming can
be increased."
"We present cTI, the first system for universal left-termination inference of
logic programs. Termination inference generalizes termination analysis and
checking. Traditionally, a termination analyzer tries to prove that a given
class of queries terminates. This class must be provided to the system, for
instance by means of user annotations. Moreover, the analysis must be redone
every time the class of queries of interest is updated. Termination inference,
in contrast, requires neither user annotations nor recomputation. In this
approach, terminating classes for all predicates are inferred at once. We
describe the architecture of cTI and report an extensive experimental
evaluation of the system covering many classical examples from the logic
programming termination literature and several Prolog programs of respectable
size and complexity."
"A programming tactic involving polyhedra is reported that has been widely
applied in the polyhedral analysis of (constraint) logic programs. The method
enables the computations of convex hulls that are required for polyhedral
analysis to be coded with linear constraint solving machinery that is available
in many Prolog systems.
  To appear in Theory and Practice of Logic Programming (TPLP)"
"Program execution monitoring consists of checking whole executions for given
properties in order to collect global run-time information.
  Monitoring is very useful to maintain programs. However, application
developers face the following dilemma: either they use existing tools which
never exactly fit their needs, or they invest a lot of effort to implement
monitoring code.
  In this article we argue that, when an event-oriented tracer exists, the
compiler developers can enable the application developers to easily code their
own, relevant, monitors which will run efficiently.
  We propose a high-level operator, called foldt, which operates on execution
traces. One of the key advantages of our approach is that it allows a clean
separation of concerns; the definition of monitors is neither intertwined in
the user source code nor in the language compiler.
  We give a number of applications of the foldt operator to compute monitors
for Mercury program executions: execution profiles, graphical abstract views,
and two test coverage measurements. Each example is implemented by a few simple
lines of Mercury.
  Detailed measurements show acceptable performance of the basic mechanism of
foldt for executions of several millions of execution events."
"In this tool demonstration, we give an overview of the Chameleon type
debugger. The type debugger's primary use is to identify locations within a
source program which are involved in a type error. By further examining these
(potentially) problematic program locations, users gain a better understanding
of their program and are able to work towards the actual mistake which was the
cause of the type error. The debugger is interactive, allowing the user to
provide additional information to narrow down the search space. One of the
novel aspects of the debugger is the ability to explain erroneous-looking
types. In the event that an unexpected type is inferred, the debugger can
highlight program locations which contributed to that result. Furthermore, due
to the flexible constraint-based foundation that the debugger is built upon, it
can naturally handle advanced type system features such as Haskell's type
classes and functional dependencies."
"In this paper we would like to present a very short (possibly the shortest)
self-interpreter, based on a simplistic Turing-complete imperative language.
This interpreter explicitly processes the statements of the language, which
means the interpreter constitutes a description of the language inside that
same language. The paper does not require any specific knowledge; however,
experience in programming and a vivid imagination are beneficial."
"This paper focuses on the inference of modes for which a logic program is
guaranteed to terminate. This generalises traditional termination analysis
where an analyser tries to verify termination for a specified mode. Our
contribution is a methodology in which components of traditional termination
analysis are combined with backwards analysis to obtain an analyser for
termination inference. We identify a condition on the components of the
analyser which guarantees that termination inference will infer all modes which
can be checked to terminate. The application of this methodology to enhance a
traditional termination analyser to perform also termination inference is
demonstrated."
"An open ended list is a well known data structure in Prolog programs. It is
frequently used to represent a value changing over time, while this value is
referred to from several places in the data structure of the application. A
weak point in this technique is that the time complexity is linear in the
number of updates to the value represented by the open ended list. In this
programming pearl we present a variant of the open ended list, namely an open
ended tree, with an update and access time complexity logarithmic in the number
of updates to the value."
"It is well-known that freeness and linearity information positively interact
with aliasing information, allowing both the precision and the efficiency of
the sharing analysis of logic programs to be improved. In this paper we present
a novel combination of set-sharing with freeness and linearity information,
which is characterized by an improved abstract unification operator. We provide
a new abstraction function and prove the correctness of the analysis for both
the finite tree and the rational tree cases. Moreover, we show that the same
notion of redundant information as identified in (Bagnara et al. 2002;
Zaffanella et al. 2002) also applies to this abstract domain combination: this
allows for the implementation of an abstract unification operator running in
polynomial time and achieving the same precision on all the considered
observable properties."
"Sharing, an abstract domain developed by D. Jacobs and A. Langen for the
analysis of logic programs, derives useful aliasing information. It is
well-known that a commonly used core of techniques, such as the integration of
Sharing with freeness and linearity information, can significantly improve the
precision of the analysis. However, a number of other proposals for refined
domain combinations have been circulating for years. One feature that is common
to these proposals is that they do not seem to have undergone a thorough
experimental evaluation even with respect to the expected precision gains. In
this paper we experimentally evaluate: helping Sharing with the definitely
ground variables found using Pos, the domain of positive Boolean formulas; the
incorporation of explicit structural information; a full implementation of the
reduced product of Sharing and Pos; the issue of reordering the bindings in the
computation of the abstract mgu; an original proposal for the addition of a new
mode recording the set of variables that are deemed to be ground or free; a
refined way of using linearity to improve the analysis; the recovery of hidden
information in the combination of Sharing with freeness information. Finally,
we discuss the issue of whether tracking compoundness allows the computation of
more sharing information."
"Object-oriented programming languages such as Java and Objective C have
become popular for implementing agent-based and other object-based simulations
since objects in those languages can {\em reflect} (i.e. make runtime queries
of an object's structure). This allows, for example, a fairly trivial {\em
serialisation} routine (conversion of an object into a binary representation
that can be stored or passed over a network) to be written. However C++ does
not offer this ability, as type information is thrown away at compile time. Yet
C++ is often a preferred development environment, whether for performance
reasons or for its expressive features such as operator overloading.
  In this paper, we present the {\em Classdesc} system which brings many of the
benefits of object reflection to C++."
"The implementation of the compiler of the UPLNC language is presented with a
full source code listing."
"The paper describes the contributions of Alain Colmerauer to the areas of
logic programs (LP) and constraint logic programs (CLP)."
"Many functional logic languages are based on narrowing, a unification-based
goal-solving mechanism which subsumes the reduction mechanism of functional
languages and the resolution principle of logic languages. Needed narrowing is
an optimal evaluation strategy which constitutes the basis of modern
(narrowing-based) lazy functional logic languages. In this work, we present the
fundamentals of partial evaluation in such languages. We provide correctness
results for partial evaluation based on needed narrowing and show that the nice
properties of this strategy are essential for the specialization process. In
particular, the structure of the original program is preserved by partial
evaluation and, thus, the same evaluation strategy can be applied for the
execution of specialized programs. This is in contrast to other partial
evaluation schemes for lazy functional logic programs which may change the
program structure in a negative way. Recent proposals for the partial
evaluation of declarative multi-paradigm programs use (some form of) needed
narrowing to perform computations at partial evaluation time. Therefore, our
results constitute the basis for the correctness of such partial evaluators."
"We investigate a technique from the literature, called the phantom-types
technique, that uses parametric polymorphism, type constraints, and unification
of polymorphic types to model a subtyping hierarchy. Hindley-Milner type
systems, such as the one found in Standard ML, can be used to enforce the
subtyping relation, at least for first-order values. We show that this
technique can be used to encode any finite subtyping hierarchy (including
hierarchies arising from multiple interface inheritance). We formally
demonstrate the suitability of the phantom-types technique for capturing
first-order subtyping by exhibiting a type-preserving translation from a simple
calculus with bounded polymorphism to a calculus embodying the type system of
SML."
"The logic programming paradigm provides the basis for a new intensional view
of higher-order notions. This view is realized primarily by employing the terms
of a typed lambda calculus as representational devices and by using a richer
form of unification for probing their structures. These additions have
important meta-programming applications but they also pose non-trivial
implementation problems. One issue concerns the machine representation of
lambda terms suitable to their intended use: an adequate encoding must
facilitate comparison operations over terms in addition to supporting the usual
reduction computation. Another aspect relates to the treatment of a unification
operation that has a branching character and that sometimes calls for the
delaying of the solution of unification problems. A final issue concerns the
execution of goals whose structures become apparent only in the course of
computation. These various problems are exposed in this paper and solutions to
them are described. A satisfactory representation for lambda terms is developed
by exploiting the nameless notation of de Bruijn as well as explicit encodings
of substitutions. Special mechanisms are molded into the structure of
traditional Prolog implementations to support branching in unification and
carrying of unification problems over other computation steps; a premium is
placed in this context on exploiting determinism and on emulating usual
first-order behaviour. An extended compilation model is presented that treats
higher-order unification and also handles dynamically emergent goals. The ideas
described here have been employed in the Teyjus implementation of the Lambda
Prolog language, a fact that is used to obtain a preliminary assessment of
their efficacy."
"We propose a general framework for first-order functional logic programming,
supporting lazy functions, non-determinism and polymorphic datatypes whose data
constructors obey a set C of equational axioms. On top of a given C, we specify
a program as a set R of C-based conditional rewriting rules for defined
functions. We argue that equational logic does not supply the proper semantics
for such programs. Therefore, we present an alternative logic which includes
C-based rewriting calculi and a notion of model. We get soundness and
completeness for C-based rewriting w.r.t. models, existence of free models for
all programs, and type preservation results. As operational semantics, we
develop a sound and complete procedure for goal solving, which is based on the
combination of lazy narrowing with unification modulo C. Our framework is quite
expressive for many purposes, such as solving action and change problems, or
realizing the GAMMA computation model."
"This paper presents the multi-threading and internet message communication
capabilities of Qu-Prolog. Message addresses are symbolic and the
communications package provides high-level support that completely hides
details of IP addresses and port numbers as well as the underlying TCP/IP
transport layer. The combination of the multi-threads and the high level
inter-thread message communications provide simple, powerful support for
implementing internet distributed intelligent applications."
"Constraint Logic Programming (CLP) and Hereditary Harrop formulas (HH) are
two well known ways to enhance the expressivity of Horn clauses. In this paper,
we present a novel combination of these two approaches. We show how to enrich
the syntax and proof theory of HH with the help of a given constraint system,
in such a way that the key property of HH as a logic programming language
(namely, the existence of uniform proofs) is preserved. We also present a
procedure for goal solving, showing its soundness and completeness for
computing answer constraints. As a consequence of this result, we obtain a new
strong completeness theorem for CLP that avoids the need to build disjunctions
of computed answers, as well as a more abstract formulation of a known
completeness theorem for HH."
"Logic languages based on the theory of rational, possibly infinite, trees
have much appeal in that rational trees allow for faster unification (due to
the safe omission of the occurs-check) and increased expressivity (cyclic terms
can provide very efficient representations of grammars and other useful
objects). Unfortunately, the use of infinite rational trees has problems. For
instance, many of the built-in and library predicates are ill-defined for such
trees and need to be supplemented by run-time checks whose cost may be
significant. Moreover, some widely-used program analysis and manipulation
techniques are correct only for those parts of programs working over finite
trees. It is thus important to obtain, automatically, a knowledge of the
program variables (the finite variables) that, at the program points of
interest, will always be bound to finite terms. For these reasons, we propose
here a new data-flow analysis, based on abstract interpretation, that captures
such information."
"Higher-order representations of objects such as programs, proofs, formulas
and types have become important to many symbolic computation tasks. Systems
that support such representations usually depend on the implementation of an
intensional view of the terms of some variant of the typed lambda-calculus.
Various notations have been proposed for lambda-terms to explicitly treat
substitutions as basis for realizing such implementations. There are, however,
several choices in the actual reduction strategies. The most common strategy
utilizes such notations only implicitly via an incremental use of environments.
This approach does not allow the smaller substitution steps to be intermingled
with other operations of interest on lambda-terms. However, a naive strategy
explicitly using such notations can also be costly: each use of the
substitution propagation rules causes the creation of a new structure on the
heap that is often discarded in the immediately following step. There is thus a
tradeoff between these two approaches. This thesis describes the actual
realization of the two approaches, discusses their tradeoffs based on this and,
finally, offers an amalgamated approach that utilizes recursion in rewrite rule
application but also suspends substitution operations where necessary."
"We present a concurrent framework for Win32 programming based on Concurrent
ML, a concurrent language with higher-order functions, static typing,
lightweight threads and synchronous communication channels. The key points of
the framework are the move from an event loop model to a threaded model for the
processing of window messages, and the decoupling of controls notifications
from the system messages. This last point allows us to derive a general way of
writing controls that leads to easy composition, and can accommodate ActiveX
Controls in a transparent way."
"Reactive systems are systems that maintain an ongoing interaction with their
environment, activated by receiving input events from the environment and
producing output events in response. Modern programming languages designed to
program such systems use a paradigm based on the notions of instants and
activations. We describe a library for Standard ML that provides basic
primitives for programming reactive systems. The library is a low-level system
upon which more sophisticated reactive behaviors can be built, which provides a
convenient framework for prototyping extensions to existing reactive languages."
"A useful programming language needs to support writing programs that take
advantage of services and communication mechanisms supplied by the operating
system. We examine the problem of programming native Win32 applications under
Windows with Standard ML. We introduce an framework based on the IDL interface
language et a minimal foreign-functions interface to explore the Win32 API et
COM in the context of Standard ML."
"We present in this paper the preliminary design of a module system based on a
notion of components such as they are found in COM. This module system is
inspired from that of Standard ML, and features first-class instances of
components, first-class interfaces, and interface-polymorphic functions, as
well as allowing components to be both imported from the environment and
exported to the environment using simple mechanisms. The module system
automates the memory management of interfaces and hides the IUnknown interface
and QueryInterface mechanisms from the programmer, favoring instead a
higher-level approach to handling interfaces."
"Practical implementations of high-level languages must provide access to
libraries and system services that have APIs specified in a low-level language
(usually C). An important characteristic of such mechanisms is the
foreign-interface policy that defines how to bridge the semantic gap between
the high-level language and C. For example, IDL-based tools generate code to
marshal data into and out of the high-level representation according to user
annotations. The design space of foreign-interface policies is large and there
are pros and cons to each approach. Rather than commit to a particular policy,
we choose to focus on the problem of supporting a gamut of interoperability
policies. In this paper, we describe a framework for language interoperability
that is expressive enough to support very efficient implementations of a wide
range of different foreign-interface policies. We describe two tools that
implement substantially different policies on top of our framework and present
benchmarks that demonstrate their efficiency."
"Recent results of Bucciarelli show that the semilattice of degrees of
parallelism of first-order boolean functions in PCF has both infinite chains
and infinite antichains. By considering a simple subclass of Sieber's
sequentiality relations, we identify levels in the semilattice and derive
inexpressibility results concerning functions on different levels. This allows
us to further explore the structure of the semilattice of degrees of
parallelism: we identify semilattices characterized by simple level properties,
and show the existence of new infinite hierarchies which are in a certain sense
natural with respect to the levels."
"We describe a scheme for moving living code between a set of distributed
processes coordinated with unification based Linda operations, and its
application to building a comprehensive Logic programming based Internet
programming framework. Mobile threads are implemented by capturing first order
continuations in a compact data structure sent over the network. Code is
fetched lazily from its original base turned into a server as the continuation
executes at the remote site. Our code migration techniques, in combination with
a dynamic recompilation scheme, ensure that heavily used code moves up smoothly
on a speed hierarchy while volatile dynamic code is kept in a quickly updatable
form. Among the examples, we describe how to build programmable client and
server components (Web servers, in particular) and mobile agents."
"This paper presents a programming language which includes paradigms that are
usually associated with declarative languages, such as sets, rules and search,
into an imperative (functional) language. Although these paradigms are
separately well known and are available under various programming environments,
the originality of the CLAIRE language comes from the tight integration, which
yields interesting run-time performances, and from the richness of this
combination, which yields new ways in which to express complex algorithmic
patterns with few elegant lines. To achieve the opposite goals of a high
abstraction level (conciseness and readability) and run-time performance
(CLAIRE is used as a C++ preprocessor), we have developed two kinds of
compiler: first, a pattern pre-processor handles iterations over both concrete
and abstract sets (data types and program fragments), in a completely
user-extensible manner; secondly, an inference compiler transforms a set of
logical rules into a set of functions (demons that are used through procedural
attachment)."
"This paper presents a generic technique for improving hybrid algorithms
through the discovery of and tuning of meta-heuristics. The idea is to
represent a family of push/pull heuristics that are based upon inserting and
removing tasks in a current solution, with an algebra. We then let a learning
algorithm search for the best possible algebraic term, which represents a
hybrid algorithm for a given set of problems and an optimization criterion. In
a previous paper, we described this algebra in detail and provided a set of
preliminary results demonstrating the utility of this approach, using vehicle
routing with time windows (VRPTW) as a domain example. In this paper we expand
upon our results providing a more robust experimental framework and learning
algorithms, and report on some new results using the standard Solomon
benchmarks. In particular, we show that our learning algorithm is able to
achieve results similar to the best-published algorithms using only a fraction
of the CPU time. We also show that the automatic tuning of the best hybrid
combination of such techniques yields a better solution than hand tuning, with
considerably less effort."
"We present a prescriptive type system with parametric polymorphism and
subtyping for constraint logic programs. The aim of this type system is to
detect programming errors statically. It introduces a type discipline for
constraint logic programs and modules, while maintaining the capabilities of
performing the usual coercions between constraint domains, and of typing
meta-programming predicates, thanks to the flexibility of subtyping. The
property of subject reduction expresses the consistency of a prescriptive type
system w.r.t. the execution model: if a program is ""well-typed"", then all
derivations starting from a ""well-typed"" goal are again ""well-typed"". That
property is proved w.r.t. the abstract execution model of constraint
programming which proceeds by accumulation of constraints only, and w.r.t. an
enriched execution model with type constraints for substitutions. We describe
our implementation of the system for type checking and type inference. We
report our experimental results on type checking ISO-Prolog, the (constraint)
libraries of Sicstus Prolog and other Prolog programs."
"This note illustrates theoretical worst-case scenarios for groundness
analyses obtained through abstract interpretation over the abstract domains of
definite (Def) and positive (Pos) Boolean functions. For Def, an example is
given for which any Def-based abstract interpretation for groundness analysis
follows a chain which is exponential in the number of argument positions as
well as in the number of clauses but sub-exponential in the size of the
program. For Pos, we strengthen a previous result by illustrating an example
for which any Pos-based abstract interpretation for groundness analysis follows
a chain which is exponential in the size of the program. It remains an open
problem to determine if the worst case for Def is really as bad as that for
Pos."
"How to extract negative information from programs is an important issue in
logic programming. Here we address the problem for functional logic programs,
from a proof-theoretic perspective. The starting point of our work is CRWL
(Constructor based ReWriting Logic), a well established theoretical framework
for functional logic programming, whose fundamental notion is that of
non-strict non-deterministic function. We present a proof calculus, CRWLF,
which is able to deduce negative information from CRWL-programs. In particular,
CRWLF is able to prove finite failure of reduction within CRWL."
"LogicWeb mobile code consists of Prolog-like rules embedded in Web pages,
thereby adding logic programming behaviour to those pages. Since LogicWeb
programs are downloaded from foreign hosts and executed locally, there is a
need to protect the client from buggy or malicious code. A security model is
crucial for making LogicWeb mobile code safe to execute. This paper presents
such a model, which supports programs of varying trust levels by using
different resource access policies. The implementation of the model derives
from an extended operational semantics for the LogicWeb language, which
provides a precise meaning of safety."
"Imperative programmers often use cyclically linked trees in order to achieve
O(1) navigation time to neighbours. Some logic programmers believe that cyclic
terms are necessary to achieve the same in logic-based languages. An old but
little-known technique provides O(1) time and space navigation without cyclic
links, in the form of reversible predicates. A small modification provides O(1)
amortised time and space editing."
"This paper presents an algorithm that achieves hyper-arc consistency for the
soft alldifferent constraint. To this end, we prove and exploit the equivalence
with a minimum-cost flow problem. Consistency of the constraint can be checked
in O(nm) time, and hyper-arc consistency is achieved in O(m) time, where n is
the number of variables involved and m is the sum of the cardinalities of the
domains. It improves a previous method that did not ensure hyper-arc
consistency."
"In this paper we discuss the optimizing compilation of Constraint Handling
Rules (CHRs). CHRs are a multi-headed committed choice constraint language,
commonly applied for writing incremental constraint solvers. CHRs are usually
implemented as a language extension that compiles to the underlying language.
In this paper we show how we can use different kinds of information in the
compilation of CHRs in order to obtain access efficiency, and a better
translation of the CHR rules into the underlying language, which in this case
is HAL. The kinds of information used include the types, modes, determinism,
functional dependencies and symmetries of the CHR constraints. We also show how
to analyze CHR programs to determine this information about functional
dependencies, symmetries and other kinds of information supporting
optimizations."
"Recent constraint logic programming (CLP) languages, such as HAL and Mercury,
require type, mode and determinism declarations for predicates. This
information allows the generation of efficient target code and the detection of
many errors at compile-time. Unfortunately, mode checking in such languages is
difficult. One of the main reasons is that, for each predicate mode
declaration, the compiler is required to appropriately re-order literals in the
predicate's definition. The task is further complicated by the need to handle
complex instantiations (which interact with type declarations and higher-order
predicates) and automatic initialization of solver variables. Here we define
mode checking for strongly typed CLP languages which require reordering of
clause body literals. In addition, we show how to handle a simple case of
polymorphic modes by using the corresponding polymorphic types."
"This report presents Jartege, a tool which allows random generation of unit
tests for Java classes specified in JML. JML (Java Modeling Language) is a
specification language for Java which allows one to write invariants for
classes, and pre- and postconditions for operations. As in the JML-JUnit tool,
we use JML specifications on the one hand to eliminate irrelevant test cases,
and on the other hand as a test oracle. Jartege randomly generates test cases,
which consist of a sequence of constructor and method calls for the classes
under test. The random aspect of the tool can be parameterized by associating
weights to classes and operations, and by controlling the number of instances
which are created for each class under test. The practical use of Jartege is
illustrated by a small case study."
"We discuss the divergence problems recently identified in some extrapolation
operators for weakly-relational numeric domains. We identify the cause of the
divergences and point out that resorting to more concrete, syntactic domains
can be avoided by researching suitable algorithms for the elimination of
redundant constraints in the chosen representation."
"The higher-order pi-calculus is an extension of the pi-calculus to allow
communication of abstractions of processes rather than names alone. It has been
studied intensively by Sangiorgi in his thesis where a characterisation of a
contextual equivalence for higher-order pi-calculus is provided using labelled
transition systems and normal bisimulations. Unfortunately the proof technique
used there requires a restriction of the language to only allow finite types.
We revisit this calculus and offer an alternative presentation of the labelled
transition system and a novel proof technique which allows us to provide a
fully abstract characterisation of contextual equivalence using labelled
transitions and bisimulations for higher-order pi-calculus with recursive types
also."
This submission has been withdrawn at the request of the author.
This submission has been withdrawn at the request of the author.
"In this paper, we propose a new language, called AR ({\it Action Rules}), and
describe how various propagators for finite-domain constraints can be
implemented in it. An action rule specifies a pattern for agents, an action
that the agents can carry out, and an event pattern for events that can
activate the agents. AR combines the goal-oriented execution model of logic
programming with the event-driven execution model. This hybrid execution model
facilitates programming constraint propagators. A propagator for a constraint
is an agent that maintains the consistency of the constraint and is activated
by the updates of the domain variables in the constraint. AR has a much
stronger descriptive power than {\it indexicals}, the language widely used in
the current finite-domain constraint systems, and is flexible for implementing
not only interval-consistency but also arc-consistency algorithms. As examples,
we present a weak arc-consistency propagator for the {\tt all\_distinct}
constraint and a hybrid algorithm for n-ary linear equality constraints.
B-Prolog has been extended to accommodate action rules. Benchmarking shows that
B-Prolog as a CLP(FD) system significantly outperforms other CLP(FD) systems."
"Once a program file is modified, the recompilation time should be minimized,
without sacrificing execution speed or high level object oriented features. The
recompilation time is often a problem for the large graphical interactive
distributed applications tackled by modern OO languages. A compilation server
and fast code generator were developed and integrated with the SRC Modula-3
compiler and Linux ELF dynamic linker. The resulting compilation and
recompilation speedups are impressive. The impact of different language
features, processor speed, and application size are discussed."
"On one hand, termination analysis of logic programs is now a fairly
established research topic within the logic programming community. On the other
hand, non-termination analysis seems to remain a much less attractive subject.
If we divide this line of research into two kinds of approaches: dynamic versus
static analysis, this paper belongs to the latter. It proposes a criterion for
detecting non-terminating atomic queries with respect to binary CLP clauses,
which strictly generalizes our previous works on this subject. We give a
generic operational definition and a logical form of this criterion. Then we
show that the logical form is correct and complete with respect to the
operational definition."
"Haskell provides type-class-bounded and parametric polymorphism as opposed to
subtype polymorphism of object-oriented languages such as Java and OCaml. It is
a contentious question whether Haskell 98 without extensions, or with common
extensions, or with new extensions can fully support conventional
object-oriented programming with encapsulation, mutable state, inheritance,
overriding, statically checked implicit and explicit subtyping, and so on. We
systematically substantiate that Haskell 98, with some common extensions,
supports all the conventional OO features plus more advanced ones, including
first-class lexically scoped classes, implicitly polymorphic classes, flexible
multiple inheritance, safe downcasts and safe co-variant arguments. Haskell
indeed can support width and depth, structural and nominal subtyping. We
address the particular challenge to preserve Haskell's type inference even for
objects and object-operating functions. The OO features are introduced in
Haskell as the OOHaskell library. OOHaskell lends itself as a sandbox for typed
OO language design."
"We study embeddings of programming languages into one another that preserve
what reductions take place at compile-time, i.e., staging. A certain condition
-- what we call a `Turing complete kernel' -- is sufficient for a language to
be stage-universal in the sense that any language may be embedded in it while
preserving staging. A similar line of reasoning yields the notion of
safety-preserving embeddings, and a useful characterization of
safety-universality. Languages universal with respect to staging and safety are
good candidates for realizing domain-specific embedded languages (DSELs) and
`active libraries' that provide domain-specific optimizations and safety
checks."
"Datatype specialization is a form of subtyping that captures program
invariants on data structures that are expressed using the convenient and
intuitive datatype notation. Of particular interest are structural invariants
such as well-formedness. We investigate the use of phantom types for describing
datatype specializations. We show that it is possible to express
statically-checked specializations within the type system of Standard ML. We
also show that this can be done in a way that does not lose useful programming
facilities such as pattern matching in case expressions."
"We revisit the SL synchronous programming model introduced by Boussinot and
De Simone (IEEE, Trans. on Soft. Eng., 1996). We discuss an alternative design
of the model including thread spawning and recursive definitions and we explore
some basic properties of the revised model: determinism, reactivity, CPS
translation to a tail recursive form, computational expressivity, and a
compositional notion of program equivalence."
"I will present my implementation 'n-units' of physical units into C++
programs. It allows the compiler to check for dimensional consistency."
"We develop new methods to statically bound the resources needed for the
execution of systems of concurrent, interactive threads. Our study is concerned
with a \emph{synchronous} model of interaction based on cooperative threads
whose execution proceeds in synchronous rounds called instants. Our
contribution is a system of compositional static analyses to guarantee that
each instant terminates and to bound the size of the values computed by the
system as a function of the size of its parameters at the beginning of the
instant. Our method generalises an approach designed for first-order functional
languages that relies on a combination of standard termination techniques for
term rewriting systems and an analysis of the size of the computed values based
on the notion of quasi-interpretation. We show that these two methods can be
combined to obtain an explicit polynomial bound on the resources needed for the
execution of the system during an instant. As a second contribution, we
introduce a virtual machine and a related bytecode thus producing a precise
description of the resources needed for the execution of a system. In this
context, we present a suitable control flow analysis that allows to formulte
the static analyses for resource control at byte code level."
"In this note we revisit the so-called reactive programming style, which
evolves from the synchronous programming model of the Esterel language by
weakening the assumption that the absence of an event can be detected
instantaneously. We review some research directions that have been explored
since the emergence of the reactive model ten years ago. We shall also outline
some questions that remain to be investigated."
"The design of metaprogramming languages requires appreciation of the
tradeoffs that exist between important language characteristics such as safety
properties, expressive power, and succinctness. Unfortunately, such tradeoffs
are little understood, a situation we try to correct by embarking on a study of
metaprogramming language tradeoffs using tools from computability theory.
Safety properties of metaprograms are in general undecidable; for example, the
property that a metaprogram always halts and produces a type-correct instance
is $\Pi^0_2$-complete. Although such safety properties are undecidable, they
may sometimes be captured by a restricted language, a notion we adapt from
complexity theory. We give some sufficient conditions and negative results on
when languages capturing properties can exist: there can be no languages
capturing total correctness for metaprograms, and no `functional' safety
properties above $\Sigma^0_3$ can be captured. We prove that translating a
metaprogram from a general-purpose to a restricted metaprogramming language
capturing a property is tantamount to proving that property for the
metaprogram. Surprisingly, when one shifts perspective from programming to
metaprogramming, the corresponding safety questions do not become substantially
harder -- there is no `jump' of Turing degree for typical safety properties."
"The design and implementation of an incremental copying heap garbage
collector for WAM-based Prolog systems is presented. Its heap layout consists
of a number of equal-sized blocks. Other changes to the standard WAM allow
these blocks to be garbage collected independently. The independent collection
of heap blocks forms the basis of an incremental collecting algorithm which
employs copying without marking (contrary to the more frequently used mark&copy
or mark&slide algorithms in the context of Prolog). Compared to standard
semi-space copying collectors, this approach to heap garbage collection lowers
in many cases the memory usage and reduces pause times. The algorithm also
allows for a wide variety of garbage collection policies including generational
ones. The algorithm is implemented and evaluated in the context of hProlog."
"This paper presents Gom, a language for describing abstract syntax trees and
generating a Java implementation for those trees. Gom includes features
allowing the user to specify and modify the interface of the data structure.
These features provide in particular the capability to maintain the internal
representation of data in canonical form with respect to a rewrite system. This
explicitly guarantees that the client program only manipulates normal forms for
this rewrite system, a feature which is only implicitly used in many
implementations."
"The application of automatic transformation processes during the formal
development and optimization of programs can introduce encumbrances in the
generated code that programmers usually (or presumably) do not write. An
example is the introduction of redundant arguments in the functions defined in
the program. Redundancy of a parameter means that replacing it by any
expression does not change the result. In this work, we provide methods for the
analysis and elimination of redundant arguments in term rewriting systems as a
model for the programs that can be written in more sophisticated languages. On
the basis of the uselessness of redundant arguments, we also propose an erasure
procedure which may avoid wasteful computations while still preserving the
semantics (under ascertained conditions). A prototype implementation of these
methods has been undertaken, which demonstrates the practicality of our
approach."
"In this paper, we present our proposal to Constraint Functional Logic
Programming over Finite Domains (CFLP(FD)) with a lazy functional logic
programming language which seamlessly embodies finite domain (FD) constraints.
This proposal increases the expressiveness and power of constraint logic
programming over finite domains (CLP(FD)) by combining functional and
relational notation, curried expressions, higher-order functions, patterns,
partial applications, non-determinism, lazy evaluation, logical variables,
types, domain variables, constraint composition, and finite domain constraints.
  We describe the syntax of the language, its type discipline, and its
declarative and operational semantics. We also describe TOY(FD), an
implementation for CFLPFD(FD), and a comparison of our approach with respect to
CLP(FD) from a programming point of view, showing the new features we
introduce. And, finally, we show a performance analysis which demonstrates that
our implementation is competitive with respect to existing CLP(FD) systems and
that clearly outperforms the closer approach to CFLP(FD)."
"Semantics of logic programs has been given by proof theory, model theory and
by fixpoint of the immediate-consequence operator. If clausal logic is a
programming language, then it should also have a compositional semantics.
Compositional semantics for programming languages follows the abstract syntax
of programs, composing the meaning of a unit by a mathematical operation on the
meanings of its constituent units. The procedural interpretation of logic has
only yielded an incomplete abstract syntax for logic programs. We complete it
and use the result as basis of a compositional semantics. We present for
comparison Tarski's algebraization of first-order predicate logic, which is in
substance the compositional semantics for his choice of syntax. We characterize
our semantics by equivalence with the immediate-consequence operator."
"Functional programming comes in two flavours: one where ``functions are
first-class citizens'' (we call this applicative) and one which is based on
equations (we call this declarative). In relational programming clauses play
the role of equations. Hence Prolog is declarative. The purpose of this paper
is to provide in relational programming a mathematical basis for the relational
analog of applicative functional programming. We use the cylindric semantics of
first-order logic due to Tarski and provide a new notation for the required
cylinders that we call tables. We define the Table/Relation Algebra with
operators sufficient to translate Horn clauses into algebraic form. We
establish basic mathematical properties of these operators. We show how
relations can be first-class citizens, and devise mechanisms for modularity,
for local scoping of predicates, and for exporting/importing relations between
programs."
"Constraint Handling Rules (CHR) are a committed-choice declarative language
which has been designed for writing constraint solvers. A CHR program consists
of multi-headed guarded rules which allow one to rewrite constraints into
simpler ones until a solved form is reached.
  CHR has received a considerable attention, both from the practical and from
the theoretical side. Nevertheless, due the use of multi-headed clauses, there
are several aspects of the CHR semantics which have not been clarified yet. In
particular, no compositional semantics for CHR has been defined so far.
  In this paper we introduce a fix-point semantics which characterizes the
input/output behavior of a CHR program and which is and-compositional, that is,
which allows to retrieve the semantics of a conjunctive query from the
semantics of its components. Such a semantics can be used as a basis to define
incremental and modular analysis and verification tools."
"We propose a special-purpose class of compression algorithms for efficient
compression of Prolog programs. It is a dictionary-based compression method,
specially designed for the compression of Prolog code, and therefore we name it
PCA (Prolog Compression Algorithm). According to the experimental results this
method provides better compression than state-of-the-art general-purpose
compression algorithms. Since the algorithm works with Prolog syntactic
entities (e.g. atoms, terms, etc.) the implementation of a Prolog prototype is
straightforward and very easy to use in any Prolog application that needs
compression. Although the algorithm is designed for Prolog programs, the idea
can be easily applied for the compression of programs written in other (logic)
languages."
"We consider, as a means of making programming languages more flexible and
powerful, a parsing algorithm in which the parser may freely modify the grammar
while parsing. We are particularly interested in a modification of the
canonical LR(1) parsing algorithm in which, after the reduction of certain
productions, we examine the source sentence seen so far to determine the
grammar to use to continue parsing. A naive modification of the canonical LR(1)
parsing algorithm along these lines cannot be guaranteed to halt; as a result,
we develop a test which examines the grammar as it changes, stopping the parse
if the grammar changes in a way that would invalidate earlier assumptions made
by the parser. With this test in hand, we can develop our parsing algorithm and
prove that it is correct. That being done, we turn to earlier, related work;
the idea of programming languages which can be extended to include new
syntactic constructs has existed almost as long as the idea of high-level
programming languages. Early efforts to construct such a programming language
were hampered by an immature theory of formal languages. More recent efforts to
construct transformative languages relied either on an inefficient chain of
source-to-source translators; or they have a defect, present in our naive
parsing algorithm, in that they cannot be known to halt. The present algorithm
does not have these undesirable properties, and as such, it should prove a
useful foundation for a new kind of programming language."
"Escape analysis of object-oriented languages approximates the set of objects
which do not escape from a given context. If we take a method as context, the
non-escaping objects can be allocated on its activation stack; if we take a
thread, Java synchronisation locks on such objects are not needed. In this
paper, we formalise a basic escape domain e as an abstract interpretation of
concrete states, which we then refine into an abstract domain er which is more
concrete than e and, hence, leads to a more precise escape analysis than e. We
provide optimality results for both e and er, in the form of Galois insertions
from the concrete to the abstract domains and of optimal abstract operations.
The Galois insertion property is obtained by restricting the abstract domains
to those elements which do not contain garbage, by using an abstract garbage
collector. Our implementation of er is hence an implementation of a formally
correct escape analyser, able to detect the stack allocatable creation points
of Java (bytecode) applications.
  This report contains the proofs of results of a paper with the same title and
authors and to be published in the Journal ""Higher-Order Symbolic Computation""."
"In this article, we discuss a flow--sensitive analysis of equality
relationships for imperative programs. We describe its semantic domains,
general purpose operations over abstract computational states (term evaluation
and identification, semantic completion, widening operator, etc.) and semantic
transformers corresponding to program constructs. We summarize our experiences
from the last few years concerning this analysis and give attention to
applications of analysis of automatically generated code. Among other
illustrating examples, we consider a program for which the analysis diverges
without a widening operator and results of analyzing residual programs produced
by some automatic partial evaluator. An example of analysis of a program
generated by this evaluator is given."
"The complexity of round robin method of intraprocedural data flow analysis is
measured in number of iterations over the control flow graph. Existing
complexity bounds realistically explain the complexity of only Bit-vector
frameworks which are separable. In this paper we define the complexity bounds
for non-separable frameworks by quantifying the interdependences among the data
flow information of program entities using an Entity Dependence Graph."
"This paper presents a new type analysis for logic programs. The analysis is
performed with a priori type definitions; and type expressions are formed from
a fixed alphabet of type constructors. Non-discriminative union is used to join
type information from different sources without loss of precision. An operation
that is performed repeatedly during an analysis is to detect if a fixpoint has
been reached. This is reduced to checking the emptiness of types. Due to the
use of non-discriminative union, the fundamental problem of checking the
emptiness of types is more complex in the proposed type analysis than in other
type analyses with a priori type definitions. The experimental results,
however, show that use of tabling reduces the effect to a small fraction of
analysis time on a set of benchmarks.
  Keywords: Type analysis, Non-discriminative union, Abstract interpretation,
Tabling"
"One is interested here in the observation of dynamic processes starting from
the traces which they leave or those that one makes them produce. It is
considered here that it should be possible to make several observations
simultaneously, using a large variety of independently developed analyzers. For
this purpose, we introduce the original notion of ``full trace'' to capture the
idea that a process can be instrumented in such a way that it may broadcast all
information which could ever be requested by any kind of observer. Each
analyzer can then find in the full trace the data elements which it needs. This
approach uses what has been called a ""tracer driver"" which completes the tracer
and drives it to answer the requests of the analyzers. A tracer driver allows
to restrict the flow of information and makes this approach tractable. On the
other side, the potential size of a full trace seems to make the idea of full
trace unrealistic. In this work we explore the consequences of this notion in
term of potential efficiency, by analyzing the respective workloads between the
(full) tracer and many different analyzers, all being likely run in true
parallel environments. To illustrate this study, we use the example of the
observation of the resolution of constraints systems (proof-tree, search-tree
and propagation) using sophisticated visualization tools, as developed in the
project OADymPPaC (2001-2004). The processes considered here are computer
programs, but we believe the approach can be extended to many other kinds of
processes."
"This paper presents a logic based approach to debugging Java programs. In
contrast with traditional debugging we propose a debugging methodology for Java
programs using logical queries on individual execution states and also over the
history of execution. These queries were arrived at by a systematic study of
errors in object-oriented programs in our earlier research. We represent the
salient events during the execution of a Java program by a logic database, and
implement the queries as logic programs. Such an approach allows us to answer a
number of useful and interesting queries about a Java program, such as the
calling sequence that results in a certain outcome, the state of an object at a
particular execution point, etc. Our system also provides the ability to
compose new queries during a debugging session. We believe that logic
programming offers a significant contribution to the art of object-oriented
programs debugging."
"Effective static analyses have been proposed which infer bounds on the number
of resolutions or reductions. These have the advantage of being independent
from the platform on which the programs are executed and have been shown to be
useful in a number of applications, such as granularity control in parallel
execution. On the other hand, in distributed computation scenarios where
platforms with different capabilities come into play, it is necessary to
express costs in metrics that include the characteristics of the platform. In
particular, it is specially interesting to be able to infer upper and lower
bounds on actual execution times. With this objective in mind, we propose an
approach which combines compile-time analysis for cost bounds with a one-time
profiling of the platform in order to determine the values of certain
parameters for a given platform. These parameters calibrate a cost model which,
from then on, is able to compute statically time bound functions for procedures
and to predict with a significant degree of accuracy the execution times of
such procedures in the given platform. The approach has been implemented and
integrated in the CiaoPP system."
"We report on the development of a general tool called ExSched, implemented as
a plug-in for Microsoft Excel, for solving a class of constraint satisfaction
problems. The traditional spreadsheet paradigm is based on attaching arithmetic
expressions to individual cells and then evaluating them. The ExSched interface
generalizes the spreadsheet paradigm to allow finite domain constraints to be
attached to the individual cells that are then solved to get a solution. This
extension provides a user-friendly interface for solving constraint
satisfaction problems that can be modeled as 2D tables, such as scheduling
problems, timetabling problems, product configuration, etc. ExSched can be
regarded as a spreadsheet interface to CLP(FD) that hides the syntactic and
semantic complexity of CLP(FD) and enables novice users to solve many
scheduling and timetabling problems interactively."
"There are various kinds of type analysis of logic programs. These include for
example inference of types that describe an over-approximation of the success
set of a program, inference of well-typings, and abstractions based on given
types. Analyses can be descriptive or prescriptive or a mixture of both, and
they can be goal-dependent or goal-independent. We describe a prototype tool
that can be accessed from a web browser, allowing various type analyses to be
run. The first goal of the tool is to allow the analysis results to be examined
conveniently by clicking on points in the original program clauses, and to
highlight ill-typed program constructs, empty types or other type anomalies.
Secondly the tool allows combination of the various styles of analysis. For
example, a descriptive regular type can be automatically inferred for a given
program, and then that type can be used to generate the minimal ""domain model""
of the program with respect to the corresponding pre-interpretation, which can
give more precise information than the original descriptive type."
"Abstraction-Carrying Code (ACC) has recently been proposed as a framework for
proof-carrying code (PCC) in which the code supplier provides a program
together with an abstraction (or abstract model of the program) whose validity
entails compliance with a predefined safety policy. The abstraction thus plays
the role of safety certificate and its generation (and validation) is carried
out automatically by a fixed-point analyzer. Existing approaches for PCC are
developed under the assumption that the consumer reads and validates the entire
program w.r.t. the full certificate at once, in a non incremental way. In this
abstract, we overview the main issues on incremental ACC. In particular, in the
context of logic programming, we discuss both the generation of incremental
certificates and the design of an incremental checking algorithm for untrusted
updates of a (trusted) program, i.e., when a producer provides a modified
version of a previously validated program. By update, we refer to any arbitrary
change on a program, i.e., the extension of the program with new predicates,
the deletion of existing predicates and the replacement of existing predicates
by new versions for them. We also discuss how each kind of update affects the
incremental extension in terms of accuracy and correctness."
"We present CurryBrowser, a generic analysis environment for the declarative
multi-paradigm language Curry. CurryBrowser supports browsing through the
program code of an application written in Curry, i.e., the main module and all
directly or indirectly imported modules. Each module can be shown in different
formats (e.g., source code, interface, intermediate code) and, inside each
module, various properties of functions defined in this module can be analyzed.
In order to support the integration of various program analyses, CurryBrowser
has a generic interface to connect local and global analyses implemented in
Curry. CurryBrowser is completely implemented in Curry using libraries for GUI
programming and meta-programming."
"This volume contains the papers presented at WLPE'06: the 16th Workshop on
Logic-based Methods in Programming Environments held on August 16, 2006 in the
Seattle Sheraton Hotel and Towers, Seattle, Washington (USA). It was organised
as a satellite workshop of ICLP'06, the 22th International Conference on Logic
Programming."
"Macro tree transducers (mtt) are an important model that both covers many
useful XML transformations and allows decidable exact typechecking. This paper
reports our first step toward an implementation of mtt typechecker that has a
practical efficiency. Our approach is to represent an input type obtained from
a backward inference as an alternating tree automaton, in a style similar to
Tozawa's XSLT0 typechecking. In this approach, typechecking reduces to checking
emptiness of an alternating tree automaton. We propose several optimizations
(Cartesian factorization, state partitioning) on the backward inference process
in order to produce much smaller alternating tree automata than the naive
algorithm, and we present our efficient algorithm for checking emptiness of
alternating tree automata, where we exploit the explicit representation of
alternation for local optimizations. Our preliminary experiments confirm that
our algorithm has a practical performance that can typecheck simple
transformations with respect to the full XHTML in a reasonable time."
"This paper presents a new numerical abstract domain for static analysis by
abstract interpretation. This domain allows us to represent invariants of the
form (x-y<=c) and (+/-x<=c), where x and y are variables values and c is an
integer or real constant. Abstract elements are represented by Difference-Bound
Matrices, widely used by model-checkers, but we had to design new operators to
meet the needs of abstract interpretation. The result is a complete lattice of
infinite height featuring widening, narrowing and common transfer functions. We
focus on giving an efficient O(n2) representation and graph-based O(n3)
algorithms - where n is the number of variables|and claim that this domain
always performs more precisely than the well-known interval domain. To
illustrate the precision/cost tradeoff of this domain, we have implemented
simple abstract interpreters for toy imperative and parallel languages which
allowed us to prove some non-trivial algorithms correct."
"We propose a memory abstraction able to lift existing numerical static
analyses to C programs containing union types, pointer casts, and arbitrary
pointer arithmetics. Our framework is that of a combined points-to and
data-value analysis. We abstract the contents of compound variables in a
field-sensitive way, whether these fields contain numeric or pointer values,
and use stock numerical abstract domains to find an overapproximation of all
possible memory states--with the ability to discover relationships between
variables. A main novelty of our approach is the dynamic mapping scheme we use
to associate a flat collection of abstract cells of scalar type to the set of
accessed memory locations, while taking care of byte-level aliases - i.e., C
variables with incompatible types allocated in overlapping memory locations. We
do not rely on static type information which can be misleading in C programs as
it does not account for all the uses a memory zone may be put to. Our work was
incorporated within the Astr\'{e}e static analyzer that checks for the absence
of run-time-errors in embedded, safety-critical, numerical-intensive software.
It replaces the former memory domain limited to well-typed, union-free,
pointer-cast free data-structures. Early results demonstrate that this
abstraction allows analyzing a larger class of C programs, without much cost
overhead."
"This article presents the systematic design of a class of relational
numerical abstract domains from non-relational ones. Constructed domains
represent sets of invariants of the form (vj - vi in C), where vj and vi are
two variables, and C lives in an abstraction of P(Z), P(Q), or P(R). We will
call this family of domains weakly relational domains. The underlying concept
allowing this construction is an extension of potential graphs and
shortest-path closure algorithms in exotic-like algebras. Example constructions
are given in order to retrieve well-known domains as well as new ones. Such
domains can then be used in the Abstract Interpretation framework in order to
design various static analyses. Amajor benfit of this construction is its
modularity, allowing to quickly implement new abstract domains from existing
ones."
"We present lightweight and generic symbolic methods to improve the precison
of numerical static analyses based on Abstract Interpretation. The main idea is
to simplify numerical expressions before they are fed to abstract transfer
functions. An important novelty is that these simplifications are performed
on-the-fly, using information gathered dynamically by the analyzer. A first
method, called ""linearization,"" allows abstracting arbitrary expressions into
affine forms with interval coefficients while simplifying them. A second
method, called ""symbolic constant propagation,"" enhances the simplification
feature of the linearization by propagating assigned expressions in a symbolic
way. Combined together, these methods increase the relationality level of
numerical abstract domains and make them more robust against program
transformations. We show how they can be integrated within the classical
interval, octagon and polyhedron domains. These methods have been incorporated
within the Astr\'{e}e static analyzer that checks for the absence of run-time
errors in embedded critical avionics software. We present an experimental proof
of their usefulness."
"We present a new idea to adapt relational abstract domains to the analysis of
IEEE 754-compliant floating-point numbers in order to statically detect,
through abstract Interpretation-based static analyses, potential floating-point
run-time exceptions such as overflows or invalid operations. In order to take
the non-linearity of rounding into account, expressions are modeled as linear
forms with interval coefficients. We show how to extend already existing
numerical abstract domains, such as the octagon abstract domain, to efficiently
abstract transfer functions based on interval linear forms. We discuss specific
fixpoint stabilization techniques and give some experimental results."
"This article presents a new numerical abstract domain for static analysis by
abstract interpretation. It extends a former numerical abstract domain based on
Difference-Bound Matrices and allows us to represent invariants of the form
(+/-x+/-y<=c), where x and y are program variables and c is a real constant. We
focus on giving an efficient representation based on Difference-Bound Matrices
- O(n2) memory cost, where n is the number of variables - and graph-based
algorithms for all common abstract operators - O(n3) time cost. This includes a
normal form algorithm to test equivalence of representation and a widening
operator to compute least fixpoint approximations."
"Functional programming languages use garbage collection for heap memory
management. Ideally, garbage collectors should reclaim all objects that are
dead at the time of garbage collection. An object is dead at an execution
instant if it is not used in future. Garbage collectors collect only those dead
objects that are not reachable from any program variable. This is because they
are not able to distinguish between reachable objects that are dead and
reachable objects that are live.
  In this paper, we describe a static analysis to discover reachable dead
objects in programs written in first-order, eager functional programming
languages. The results of this technique can be used to make reachable dead
objects unreachable, thereby allowing garbage collectors to reclaim more dead
objects."
"The secure and robust functioning of a network relies on the defect-free
implementation of network applications. As network protocols have become
increasingly complex, however, hand-writing network message processing code has
become increasingly error-prone. In this paper, we present a domain-specific
language, Zebu, for describing protocol message formats and related processing
constraints. From a Zebu specification, a compiler automatically generates
stubs to be used by an application to parse network messages. Zebu is easy to
use, as it builds on notations used in RFCs to describe protocol grammars. Zebu
is also efficient, as the memory usage is tailored to application needs and
message fragments can be specified to be processed on demand. Finally,
Zebu-based applications are robust, as the Zebu compiler automatically checks
specification consistency and generates parsing stubs that include validation
of the message structure. Using a mutation analysis in the context of SIP and
RTSP, we show that Zebu significantly improves application robustness."
"In this paper, we propose a way of assigning static type information to
unmarshalling functions and we describe a verification technique for
unmarshalled data that preserves the execution safety provided by static type
checking. This technique, whose correctness is proven, relies on singleton
types whose values are transmitted to unmarshalling routines at runtime, and on
an efficient checking algorithm able to deal with sharing and cycles."
"We present a new code generator, called O'Jacare.net, to inter-operate
between C# and Objective Caml through their object models. O'Jacare.net defines
a basic IDL (Interface Definition Language) that describes classes and
interfaces in order to communicate between Objective Caml and C#. O'Jacare.net
generates all needed wrapper classes and takes advantage of static type
checking in both worlds. Although the IDL intersects these two object models,
O'Jacare.net allows to combine features from both."
"Modern compiler implementations use the Static Single Assignment
representation as a way to efficiently implement optimizing algorithms. However
this representation is not well adapted to architectures with a predicated
instruction set. The Psi-SSA representation extends the SSA representation such
that standard SSA algorithms can be easily adapted to an architecture with a
fully predicated instruction set. A new pseudo operation, the Psi operation, is
introduced to merge several conditional definitions into a unique definition."
"Array-OL is a high-level specification language dedicated to the definition
of intensive signal processing applications. Several tools exist for
implementing an Array-OL specification as a data parallel program. While
Array-OL can be used directly, it is often convenient to be able to deduce part
of the specification from a sequential version of the application. This paper
proposes such an analysis and examines its feasibility and its limits."
"Recently, the iterative approach named linear tabling has received
considerable attention because of its simplicity, ease of implementation, and
good space efficiency. Linear tabling is a framework from which different
methods can be derived based on the strategies used in handling looping
subgoals. One decision concerns when answers are consumed and returned. This
paper describes two strategies, namely, {\it lazy} and {\it eager} strategies,
and compares them both qualitatively and quantitatively. The results indicate
that, while the lazy strategy has good locality and is well suited for finding
all solutions, the eager strategy is comparable in speed with the lazy strategy
and is well suited for programs with cuts. Linear tabling relies on depth-first
iterative deepening rather than suspension to compute fixpoints. Each cluster
of inter-dependent subgoals as represented by a top-most looping subgoal is
iteratively evaluated until no subgoal in it can produce any new answers. Naive
re-evaluation of all looping subgoals, albeit simple, may be computationally
unacceptable. In this paper, we also introduce semi-naive optimization, an
effective technique employed in bottom-up evaluation of logic programs to avoid
redundant joins of answers, into linear tabling. We give the conditions for the
technique to be safe (i.e. sound and complete) and propose an optimization
technique called {\it early answer promotion} to enhance its effectiveness.
Benchmarking in B-Prolog demonstrates that with this optimization linear
tabling compares favorably well in speed with the state-of-the-art
implementation of SLG."
"Exploiting the full computational power of always deeper hierarchical
multiprocessor machines requires a very careful distribution of threads and
data among the underlying non-uniform architecture. The emergence of multi-core
chips and NUMA machines makes it important to minimize the number of remote
memory accesses, to favor cache affinities, and to guarantee fast completion of
synchronization steps. By using the BubbleSched platform as a threading backend
for the GOMP OpenMP compiler, we are able to easily transpose affinities of
thread teams into scheduling hints using abstractions called bubbles. We then
propose a scheduling strategy suited to nested OpenMP parallelism. The
resulting preliminary performance evaluations show an important improvement of
the speedup on a typical NAS OpenMP benchmark application."
"A rigid loop is a for-loop with a counter not accessible to the loop body or
any other part of a program. Special instructions for rigid loops are
introduced on top of the syntax of the program algebra PGA. Two different
semantic projections are provided and proven equivalent. One of these is taken
to have definitional status on the basis of two criteria: `normative semantic
adequacy' and `indicative algorithmic adequacy'."
"This paper is an introduction to Lambdix, a lazy Lisp interpreter implemented
at the Research Laboratory of Paris XI University (Laboratoire de Recherche en
Informatique, Orsay). Lambdix was devised in the course of an investigation
into the relationship between the semantics of programming languages and their
implementation; it was used to demonstrate that in the Lisp domain, semantic
correctness is consistent with efficiency, contrary to what has often been
claimed. The first part of the paper is an overview of well-known semantic
difficulties encountered by Lisp as well as an informal presentation of
Lambdix; it is shown that the difficulties which Lisp encouters do not arise in
Lambdix. The second part is about efficiency in implementation models. It
explains why Lambdix is better suited for lazy evaluation than previous models.
The section ends by giving comparative execution time tables."
"Cminor is a mid-level imperative programming language; there are
proved-correct optimizing compilers from C to Cminor and from Cminor to machine
language. We have redesigned Cminor so that it is suitable for Hoare Logic
reasoning and we have designed a Separation Logic for Cminor. In this paper, we
give a small-step semantics (instead of the big-step of the proved-correct
compiler) that is motivated by the need to support future concurrent
extensions. We detail a machine-checked proof of soundness of our Separation
Logic. This is the first large-scale machine-checked proof of a Separation
Logic w.r.t. a small-step semantics. The work presented in this paper has been
carried out in the Coq proof assistant. It is a first step towards an
environment in which concurrent Cminor programs can be verified using
Separation Logic and also compiled by a proved-correct compiler with formal
end-to-end correctness guarantees."
"In today's embedded applications a significant portion of energy is spent in
the memory subsystem. Several approaches have been proposed to minimize this
energy, including the use of scratch pad memories, with many based on static
analysis of a program. However, often it is not possible to perform static
analysis and optimization of a program's memory access behavior unless the
program is specifically written for this purpose. In this paper we introduce
the FORAY model of a program that permits aggressive analysis of the
application's memory behavior that further enables such optimizations since it
consists of 'for' loops and array accesses which are easily analyzable. We
present FORAY-GEN: an automated profile-based approach for extraction of the
FORAY model from the original program. We also demonstrate how FORAY-GEN
enhances applicability of other memory subsystem optimization approaches,
resulting in an average of two times increase in the number of memory
references that can be analyzed by existing static approaches."
"MANY TECHNIQUES for synthesizing digital hardware from C-like languages have
been proposed, but none have emerged as successful as Verilog or VHDL for
register-transfer-level design. This paper looks at two of the fundamental
challenges: concurrency and timing control."
"The aggressive application of scalar replacement to array references
substantially reduces the number of memory operations at the expense of a
possibly very large number of registers. In this paper we describe a register
allocation algorithm that assigns registers to scalar replaced array references
along the critical paths of a computation, in many cases exploiting the
opportunity for concurrent memory accesses. Experimental results, for a set of
image/signal processing code kernels, reveal that the proposed algorithm leads
to a substantial reduction of the number of execution cycles for the
corresponding hardware implementation on a contemporary
Field-Programmable-Gate-Array (FPGA) when compared to other greedy allocation
algorithms, in some cases, using even fewer number of registers."
"While loop restructuring based code optimization for array intensive
applications has been successful in the past, it has several problems such as
the requirement of checking dependences (legality issues) and transformation of
all of the array references within the loop body indiscriminately (while some
of the references can benefit from the transformation, others may not). As a
result, data transformations, i.e., transformations that modify memory layout
of array data instead of loop structure have been proposed. One of the problems
associated with data transformations is the difficulty of selecting a memory
layout for an array that is acceptable to the entire program (not just to a
single loop). In this paper, we formulate the problem of determining the memory
layouts of arrays as a constraint network, and explore several methods of
solution in a systematic way. Our experiments provide strong support in favor
of employing constraint processing, and point out future research directions."
"We study sequential programs that are instruction sequences with direct and
indirect jump instructions. The intuition is that indirect jump instructions
are jump instructions where the position of the instruction to jump to is the
content of some memory cell. We consider several kinds of indirect jump
instructions. For each kind, we define the meaning of programs with indirect
jump instructions of that kind by means of a translation into programs without
indirect jump instructions. For each kind, the intended behaviour of a program
with indirect jump instructions of that kind under execution is the behaviour
of the translated program under execution on interaction with some memory
device."
"This paper specifies an observational semantics and gives an original
presentation of the Byrd box model. The approach accounts for the semantics of
Prolog tracers independently of a particular Prolog implementation. Prolog
traces are, in general, considered as rather obscure and difficult to use. The
proposed formal presentation of its trace constitutes a simple and pedagogical
approach for teaching Prolog or for implementing Prolog tracers. It is a form
of declarative specification for the tracers. The trace model introduced here
is only one example to illustrate general problems relating to tracers and
observing processes. Observing processes know, from observed processes, only
their traces. The issue is then to be able to reconstitute, by the sole
analysis of the trace, part of the behaviour of the observed process, and if
possible, without any loss of information. As a matter of fact, our approach
highlights qualities of the Prolog resolution box model which made its success,
but also its insufficiencies."
"We study sequential programs that are instruction sequences with dynamically
instantiated instructions. We define the meaning of such programs in two
different ways. In either case, we give a translation by which each program
with dynamically instantiated instructions is turned into a program without
them that exhibits on execution the same behaviour by interaction with some
service. The complexity of the translations differ considerably, whereas the
services concerned are equally simple. However, the service concerned in the
case of the simpler translation is far more powerful than the service concerned
in the other case."
"In this paper we describe a new approach to programming which generalizes
object-oriented programming. It is based on using a new programming construct,
called concept, which generalizes classes. Concept is defined as a pair of two
classes: one reference class and one object class. Each concept has a parent
concept which is specified using inclusion relation generalizing inheritance.
We describe several important mechanisms such as reference resolution, context
stack, dual methods and life-cycle management, inheritance and polymorphism.
This approach to programming is positioned as a new programming paradigm and
therefore we formulate its main principles and rules."
"In the paper a new programming construct, called concept, is introduced.
Concept is pair of two classes: a reference class and an object class.
Instances of the reference classes are passed-by-value and are intended to
represent objects. Instances of the object class are passed-by-reference. An
approach to programming where concepts are used instead of classes is called
concept-oriented programming (CoP). In CoP objects are represented and accessed
indirectly by means of references. The structure of concepts describes a
hierarchical space with a virtual address system. The paper describes this new
approach to programming including such mechanisms as reference resolution,
complex references, method interception, dual methods, life-cycle management
inheritance and polymorphism."
"The paper describes a mechanism for indirect object representation and access
(ORA) in programming languages. The mechanism is based on using a new
programming construct which is referred to as concept. Concept consists of one
object class and one reference class both having their fields and methods. The
object class is the conventional class as defined in OOP with instances passed
by reference. Instances of the reference class are passed by value and are
intended to represent objects. The reference classes are used to describe how
objects have to be represented and accessed by providing custom format for
their identifiers and custom access procedures. Such an approach to programming
where concepts are used instead of classes is referred to as concept-oriented
programming. It generalizes OOP and its main advantage is that it allows the
programmer to describe not only the functionality of target objects but also
intermediate functions which are executed behind the scenes as an object is
being accessed."
"A fully-automated algorithm is developed able to show that evaluation of a
given untyped lambda-expression will terminate under CBV (call-by-value). The
``size-change principle'' from first-order programs is extended to arbitrary
untyped lambda-expressions in two steps. The first step suffices to show CBV
termination of a single, stand-alone lambda;-expression. The second suffices to
show CBV termination of any member of a regular set of lambda-expressions,
defined by a tree grammar. (A simple example is a minimum function, when
applied to arbitrary Church numerals.) The algorithm is sound and proven so in
this paper. The Halting Problem's undecidability implies that any sound
algorithm is necessarily incomplete: some lambda-expressions may in fact
terminate under CBV evaluation, but not be recognised as terminating.
  The intensional power of the termination algorithm is reasonably high. It
certifies as terminating many interesting and useful general recursive
algorithms including programs with mutual recursion and parameter exchanges,
and Colson's ``minimum'' algorithm. Further, our type-free approach allows use
of the Y combinator, and so can identify as terminating a substantial subset of
PCF."
"This paper describes an approach to creating textual syntax for Do-
main-Specific Languages (DSL). We consider target meta-model to be the main
artifact and hence to be developed first. The key idea is to represent analysis
of textual syntax as a sequence of transformations. This is made by explicit
opera- tions on abstract syntax trees (ATS), for which a simple language is
proposed. Text-to-model transformation is divided into two parts: text-to-AST
(developed by openArchitectureWare [1]) and AST-to-model (proposed by this
paper). Our approach simplifies semantic analysis and helps to generate as much
as possi- ble."
"PGA (ProGram Algebra) is an algebra of programs which concerns programs in
their simplest form: sequences of instructions. Molecular dynamics is a simple
model of computation developed in the setting of PGA, which bears on the use of
dynamic data structures in programming. We consider the programming of an
interpreter for a program notation that is close to existing assembly languages
using PGA with the primitives of molecular dynamics as basic instructions. It
happens that, although primarily meant for explaining programming language
features relating to the use of dynamic data structures, the collection of
primitives of molecular dynamics in itself is suited to our programming wants."
"In this paper, we study the phenomenon that instruction sequences are split
into fragments which somehow produce a joint behaviour. In order to bring this
phenomenon better into the picture, we formalize a simple mechanism by which
several instruction sequence fragments can produce a joint behaviour. We also
show that, even in the case of this simple mechanism, it is a non-trivial
matter to explain by means of a translation into a single instruction sequence
what takes place on execution of a collection of instruction sequence
fragments."
"The Resource Description Framework (RDF) is a semantic network data model
that is used to create machine-understandable descriptions of the world and is
the basis of the Semantic Web. This article discusses the application of RDF to
the representation of computer software and virtual computing machines. The
Semantic Web is posited as not only a web of data, but also as a web of
programs and processes."
"On the one hand, termination analysis of logic programs is now a fairly
established research topic within the logic programming community. On the other
hand, non-termination analysis seems to remain a much less attractive subject.
If we divide this line of research into two kinds of approaches: dynamic versus
static analysis, this paper belongs to the latter. It proposes a criterion for
detecting non-terminating atomic queries with respect to binary CLP rules,
which strictly generalizes our previous works on this subject. We give a
generic operational definition and an implemented logical form of this
criterion. Then we show that the logical form is correct and complete with
respect to the operational definition."
"Program transformation is an appealing technique which allows to improve
run-time efficiency, space-consumption and more generally to optimize a given
program. Essentially it consists of a sequence of syntactic program
manipulations which preserves some kind of semantic equivalence. One of the
basic operations which is used by most program transformation systems is
unfolding which consists in the replacement of a procedure call by its
definition. While there is a large body of literature on transformation and
unfolding of sequential programs, very few papers have addressed this issue for
concurrent languages and, to the best of our knowledge, no other has considered
unfolding of CHR programs.
  This paper defines a correct unfolding system for CHR programs. We define an
unfolding rule, show its correctness and discuss some conditions which can be
used to delete an unfolded rule while preserving the program meaning. We prove
that confluence and termination properties are preserved by the above
transformations."
"The points-to problem is the problem of determining the possible run-time
targets of pointer variables and is usually considered part of the more general
aliasing problem, which consists in establishing whether and when different
expressions can refer to the same memory address. Aliasing information is
essential to every tool that needs to reason about the semantics of programs.
However, due to well-known undecidability results, for all interesting
languages that admit aliasing, the exact solution of nontrivial aliasing
problems is not generally computable. This work focuses on approximated
solutions to this problem by presenting a store-based, flow-sensitive points-to
analysis, for applications in the field of automated software verification. In
contrast to software testing procedures, which heuristically check the program
against a finite set of executions, the methods considered in this work are
static analyses, where the computed results are valid for all the possible
executions of the analyzed program. We present a simplified programming
language and its execution model; then an approximated execution model is
developed using the ideas of abstract interpretation theory. Finally, the
soundness of the approximation is formally proved. The aim of developing a
realistic points-to analysis is pursued by presenting some extensions to the
initial simplified model and discussing the correctness of their formulation.
This work contains original contributions to the issue of points-to analysis,
as it provides a formulation of a filter operation on the points-to abstract
domain and a formal proof of the soundness of the defined abstract operations:
these, as far as we now, are lacking from the previous literature."
"We perceive programs as single-pass instruction sequences. A single-pass
instruction sequence under execution is considered to produce a behaviour to be
controlled by some execution environment. Threads as considered in basic thread
algebra model such behaviours. We show that all regular threads, i.e. threads
that can only be in a finite number of states, can be produced by single-pass
instruction sequences without jump instructions if use can be made of Boolean
registers. We also show that, in the case where goto instructions are used
instead of jump instructions, a bound to the number of labels restricts the
expressiveness."
"A program is a finite piece of data that produces a (possibly infinite)
sequence of primitive instructions. From scratch we develop a linear notation
for sequential, imperative programs, using a familiar class of primitive
instructions and so-called repeat instructions, a particular type of control
instructions. The resulting mathematical structure is a semigroup. We relate
this set of programs to program algebra (PGA) and show that a particular
subsemigroup is a carrier for PGA by providing axioms for single-pass
congruence, structural congruence, and thread extraction. This subsemigroup
characterizes periodic single-pass instruction sequences and provides a direct
basis for PGA's toolset."
"The problem of detecting of information and logically independent (DILD)
steps in programs is a key for equivalent program transformations. Here we are
considering the problem of independence of loop iterations, the concentration
of massive data processing and hence the most challenge construction for
parallelizing. We introduced a separated form of loops when loop's body is a
sequence of procedures each of them are used array's elements selected in a
previous procedure. We prove that any loop may be algorithmically represented
in this form and number of such procedures is invariant. We show that for this
form of loop the steps connections are determined with some integer equations
and hence the independence problem is algorithmically unsolvable if index
expressions are more complex than cubical. We suggest a modification of index
semantics that made connection equations trivial and loops iterations can be
executed in parallel."
"This article presents the formal semantics of a large subset of the C
language called Clight. Clight includes pointer arithmetic, ""struct"" and
""union"" types, C loops and structured ""switch"" statements. Clight is the source
language of the CompCert verified compiler. The formal semantics of Clight is a
big-step operational semantics that observes both terminating and diverging
executions and produces traces of input/output events. The formal semantics of
Clight is mechanized using the Coq proof assistant. In addition to the
semantics of Clight, this article describes its integration in the CompCert
verified compiler and several ways by which the semantics was validated."
"The advantages of tabled evaluation regarding program termination and
reduction of complexity are well known --as are the significant implementation,
portability, and maintenance efforts that some proposals (especially those
based on suspension) require. This implementation effort is reduced by program
transformation-based continuation call techniques, at some efficiency cost.
However, the traditional formulation of this proposal by Ramesh and Cheng
limits the interleaving of tabled and non-tabled predicates and thus cannot be
used as-is for arbitrary programs. In this paper we present a complete
translation for the continuation call technique which, using the runtime
support needed for the traditional proposal, solves these problems and makes it
possible to execute arbitrary tabled programs. We present performance results
which show that CCall offers a useful tradeoff that can be competitive with
state-of-the-art implementations."
"This paper formalizes and proves correct a compilation scheme for
mutually-recursive definitions in call-by-value functional languages. This
scheme supports a wider range of recursive definitions than previous methods.
We formalize our technique as a translation scheme to a lambda-calculus
featuring in-place update of memory blocks, and prove the translation to be
correct."
"Instruction sequences with direct and indirect jump instructions are as
expressive as instruction sequences with direct jump instructions only. We show
that, in the case where the number of instructions is not bounded, we are faced
with increases of the maximal internal delays of instruction sequences on
execution that are not bounded by a linear function if we strive for acceptable
increases of the lengths of instruction sequences on elimination of indirect
jump instructions."
"We define focus-method interfaces and some connections between such
interfaces and instruction sequences, giving rise to instruction sequence
components. We provide a flexible and practical notation for interfaces using
an abstract datatype specification comparable to that of basic process algebra
with deadlock. The structures thus defined are called progression rings. We
also define thread and service components. Two types of composition of
instruction sequences or threads and services (called `use' and `apply') are
lifted to the level of components."
"Coding standards and good practices are fundamental to a disciplined approach
to software projects, whatever programming languages they employ. Prolog
programming can benefit from such an approach, perhaps more than programming in
other languages. Despite this, no widely accepted standards and practices seem
to have emerged up to now. The present paper is a first step towards filling
this void: it provides immediate guidelines for code layout, naming
conventions, documentation, proper use of Prolog features, program development,
debugging and testing. Presented with each guideline is its rationale and,
where sensible options exist, illustrations of the relative pros and cons for
each alternative. A coding standard should always be selected on a per-project
basis, based on a host of issues pertinent to any given programming project;
for this reason the paper goes beyond the mere provision of normative
guidelines by discussing key factors and important criteria that should be
taken into account when deciding on a fully-fledged coding standard for the
project."
"This thesis concerns the implementation of Lambda Prolog, a higher-order
logic programming language that supports the lambda-tree syntax approach to
representing and manipulating formal syntactic objects. Lambda Prolog achieves
its functionality by extending a Prolog-like language by using typed lambda
terms as data structures that it then manipulates via higher-order unification
and some new program-level abstraction mechanisms. These additional features
raise new implementation questions that must be adequately addressed for Lambda
Prolog to be an effective programming tool. We consider these questions here,
providing eventually a virtual machine and compilation based realization. A key
idea is the orientation of the computation model of Lambda Prolog around a
restricted version of higher-order unification with nice algorithmic properties
and appearing to encompass most interesting applications. Our virtual machine
embeds a treatment of this form of unification within the structure of the
Warren Abstract Machine that is used in traditional Prolog implementations.
Along the way, we treat various auxiliary issues such as the low-level
representation of lambda terms, the implementation of reduction on such terms
and the optimized processing of types in computation. We also develop an actual
implementation of Lambda Prolog called Teyjus Version 2. A characteristic of
this system is that it realizes an emulator for the virtual machine in the C
language a compiler in the OCaml language. We present a treatment of the
software issues that arise from this kind of mixing of languages within one
system and we discuss issues relevant to the portability of our virtual machine
emulator across arbitrary architectures. Finally, we assess the the efficacy of
our various design ideas through experiments carried out using the system."
"The JSC language is a superset of JavaScript designed to ease the development
of large web applications. This language extends JavaScripts own object system
by isolating code in a class declaration, simplifying multiple inheritance and
using method implementation agreements."
"A unit test is a method for verifying the accuracy and the proper functioning
of a portion of a program. This work consists to study the relation and the
approaches to test Object-Oriented Programming (OOP) programs and to propose a
metamodel that enables the programmer to write the tests while writing the
source code to be tested by exploiting the key features of OOP programming
languages such as inheritance, polymorphism, etc."
"Pattern-matching programming is an example of a rule-based programming style
developed in functional languages. This programming style is intensively used
in dialects of ML but is restricted to algebraic data-types. This restriction
limits the field of application. However, as shown by Giavitto and Michel at
RULE'02, case-based function definitions can be extended to more general data
structures called topological collections. We show in this paper that this
extension retains the benefits of the typed discipline of the functional
languages. More precisely, we show that topological collections and the
rule-based definition of functions associated with them fit in a polytypic
extension of mini-ML where type inference is still possible."
"Topological collections allow to consider uniformly many data structures in
programming languages and are handled by functions defined by pattern matching
called transformations. We present two type systems for languages with
topological collections and transformations. The first one is a strong type
system \`a la Hindley/Milner which can be entirely typed at compile time. The
second one is a mixed static and dynamic type system allowing to handle
heterogeneous collections, that is collections which contain values with
different types. In the two cases, automatic type inference is possible."
"We previously developed a polymorphic type system and a type checker for a
multithreaded lock-based polymorphic typed assembly language (MIL) that ensures
that well-typed programs do not encounter race conditions. This paper extends
such work by taking into consideration deadlocks. The extended type system
verifies that locks are acquired in the proper order. Towards this end we
require a language with annotations that specify the locking order. Rather than
asking the programmer (or the compiler's backend) to specifically annotate each
newly introduced lock, we present an algorithm to infer the annotations. The
result is a type checker whose input language is non-decorated as before, but
that further checks that programs are exempt from deadlocks."
"The SPaCIFY project, which aims at bringing advances in MDE to the satellite
flight software industry, advocates a top-down approach built on a
domain-specific modeling language named Synoptic. In line with previous
approaches to real-time modeling such as Statecharts and Simulink, Synoptic
features hierarchical decomposition of application and control modules in
synchronous block diagrams and state machines. Its semantics is described in
the polychronous model of computation, which is that of the synchronous
language Signal."
"PGA, short for ProGram Algebra, describes sequential programs as finite or
infinite (repeating) sequences of instructions. The semigroup C of finite
instruction sequences was introduced as an equally expressive alternative to
PGA. PGA instructions are executed from left to right; most C instructions come
in a left-to-right as well as a right-to-left flavor. This thesis builds on C
by introducing an alternative semigroup Cg which employs label and goto
instructions instead of relative jump instructions as control structures. Cg
can be translated to C and vice versa (and is thus equally expressive). It is
shown that restricting the instruction sets of C and Cg to contain only
finitely many distinct jump, goto or label instructions in either or both
directions reduces their expressiveness. Instruction sets with an infinite
number of these instructions in both directions (not necessarily all such
instructions) do not suffer a loss of expressiveness."
"The C Object System (Cos) is a small C library which implements high-level
concepts available in Clos, Objc and other object-oriented programming
languages: uniform object model (class, meta-class and property-metaclass),
generic functions, multi-methods, delegation, properties, exceptions, contracts
and closures. Cos relies on the programmable capabilities of the C programming
language to extend its syntax and to implement the aforementioned concepts as
first-class objects. Cos aims at satisfying several general principles like
simplicity, extensibility, reusability, efficiency and portability which are
rarely met in a single programming language. Its design is tuned to provide
efficient and portable implementation of message multi-dispatch and message
multi-forwarding which are the heart of code extensibility and reusability.
With COS features in hand, software should become as flexible and extensible as
with scripting languages and as efficient and portable as expected with C
programming. Likewise, Cos concepts should significantly simplify adaptive and
aspect-oriented programming as well as distributed and service-oriented
computing"
"This article presents a complete scheme for the development of Critical
Embedded Systems with Multiple Real-Time Constraints. The system is programmed
with a language that extends the synchronous approach with high-level real-time
primitives. It enables to assemble in a modular and hierarchical manner several
locally mono-periodic synchronous systems into a globally multi-periodic
synchronous system. It also allows to specify flow latency constraints. A
program is translated into a set of real-time tasks. The generated code (\C\
code) can be executed on a simple real-time platform with a dynamic-priority
scheduler (EDF). The compilation process (each algorithm of the process, not
the compiler itself) is formally proved correct, meaning that the generated
code respects the real-time semantics of the original program (respect of
periods, deadlines, release dates and precedences) as well as its functional
semantics (respect of variable consumption)."
"The call-by-need lambda calculus provides an equational framework for
reasoning syntactically about lazy evaluation. This paper examines its
operational characteristics. By a series of reasoning steps, we systematically
unpack the standard-order reduction relation of the calculus and discover a
novel abstract machine definition which, like the calculus, goes ""under
lambdas."" We prove that machine evaluation is equivalent to standard-order
evaluation. Unlike traditional abstract machines, delimited control plays a
significant role in the machine's behavior. In particular, the machine replaces
the manipulation of a heap using store-based effects with disciplined
management of the evaluation stack using control-based effects. In short, state
is replaced with control. To further articulate this observation, we present a
simulation of call-by-need in a call-by-value language using delimited control
operations."
"This research paper is proposing the idea of pseudo code representation to
molecular programming used in designing molecular electronics devices. Already
the schematic representation of logical gates like AND, OR, NOT etc.from
molecular diodes or resonant tunneling diode are available. This paper is
setting a generic pseudo code model so that various logic gates can be
formulated. These molecular diodes have designed from organic molecules or
Bio-molecules. Our focus is on to give a scenario of molecular computation
through molecular programming. We have restricted our study to molecular
rectifying diode and logic device as AND gate from organic molecules only."
"A novel language system has given rise to promising alternatives to standard
formal and processor network models of computation. An interstring linked with
a abstract machine environment, shares sub-expressions, transfers data, and
spatially allocates resources for the parallel evaluation of dataflow. Formal
models called the a-Ram family are introduced, designed to support interstring
programming languages (interlanguages). Distinct from dataflow, graph
rewriting, and FPGA models, a-Ram instructions are bit level and execute in
situ. They support sequential and parallel languages without the space/time
overheads associated with the Turing Machine and lambda-calculus, enabling
massive programs to be simulated. The devices of one a-Ram model, called the
Synchronic A-Ram, are fully connected and simpler than FPGA LUT's. A compiler
for an interlanguage called Space, has been developed for the Synchronic A-Ram.
Space is MIMD. strictly typed, and deterministic. Barring memory allocation and
compilation, modules are referentially transparent. At a high level of
abstraction, modules exhibit a state transition system, aiding verification.
Data structures and parallel iteration are straightforward to implement, and
allocations of sub-processes and data transfers to resources are implicit.
Space points towards highly connected architectures called Synchronic Engines,
that are more general purpose than systolic arrays and GPUs, and bypass
programmability and conflict issues associated with multicores."
"A novel language system has given rise to promising alternatives to standard
formal and processor network models of computation. An interstring linked with
a abstract machine environment, shares sub-expressions, transfers data, and
spatially allocates resources for the parallel evaluation of dataflow. Formal
models called the a-Ram family are introduced, designed to support interstring
programming languages (interlanguages). Distinct from dataflow, graph
rewriting, and FPGA models, a-Ram instructions are bit level and execute in
situ. They support sequential and parallel languages without the space/time
overheads associated with the Turing Machine and l-calculus, enabling massive
programs to be simulated. The devices of one a-Ram model, called the Synchronic
A-Ram, are fully connected and simpler than FPGA LUT's. A compiler for an
interlanguage called Space, has been developed for the Synchronic A-Ram. Space
is MIMD. strictly typed, and deterministic. Barring memory allocation and
compilation, modules are referentially transparent. At a high level of
abstraction, modules exhibit a state transition system, aiding verification.
Data structures and parallel iteration are straightforward to implement, and
allocations of sub-processes and data transfers to resources are implicit.
Space points towards highly connected architectures called Synchronic Engines,
that scale in a GALS manner. Synchronic Engines are more general purpose than
systolic arrays and GPUs, and bypass programmability and conflict issues
associated with multicores."
"Previous deforestation and supercompilation algorithms may introduce
accidental termination when applied to call-by-value programs. This hides
looping bugs from the programmer, and changes the behavior of a program
depending on whether it is optimized or not. We present a supercompilation
algorithm for a higher-order call-by-value language and prove that the
algorithm both terminates and preserves termination properties. This algorithm
utilizes strictness information to decide whether to substitute or not and
compares favorably with previous call-by-name transformations."
"Object-oriented programming has been considered a most promising method in
program development and maintenance. An important feature of object-oriented
programs (OOPs) is their reusability which can be achieved through the
inheritance of classes or reusable components.Dynamic program slicing is an
effective technique for narrowing the errors to the relevant parts of a program
when debugging. Given a slicing criterion, the dynamic slice contains only
those statements that actually affect the variables in the slicing criterion.
This paper proposes a method to dynamically slice object-oriented (00) programs
based on dependence analysis. It uses the Control Dependency Graph for object
program and other static information to reduce the information to be traced
during program execution. In this paper we present a method to find the dynamic
Slice of object oriented programs where we are finding the slices for object
and in case of function overloading."
"(To appear in Theory and Practice of Logic Programming (TPLP)) We introduce a
systematic, concurrent execution scheme for Constraint Handling Rules (CHR)
based on a previously proposed sequential goal-based CHR semantics. We
establish strong correspondence results to the abstract CHR semantics, thus
guaranteeing that any answer in the concurrent, goal-based CHR semantics is
reproducible in the abstract CHR semantics. Our work provides the foundation to
obtain efficient, parallel CHR execution schemes."
"There are many programming languages in the world today.Each language has
their advantage and disavantage. In this paper, we will discuss ten programming
languages: C++, C#, Java, Groovy, JavaScript, PHP, Schalar, Scheme, Haskell and
AspectJ. We summarize and compare these ten languages on ten different
criterion. For example, Default more secure programming practices, Web
applications development, OO-based abstraction and etc. At the end, we will
give our conclusion that which languages are suitable and which are not for
using in some cases. We will also provide evidence and our analysis on why some
language are better than other or have advantages over the other on some
criterion."
"We report on a transformation from Sequential Function Charts of the IEC
61131-3 standard to BIP. Our presentation features a description of formal
syntax and semantics representation of the involved languages and
transformation rules. Furthermore, we present a formalism for describing
invariants of IEC 61131-3 systems and establish a notion of invariant
preservation between the two languages. For a subset of our transformation
rules we sketch a proof showing invariant preservation during the
transformation of IEC 61131-3 to BIP and vice versa."
"Two sinks drain precision from higher-order flow analyses: (1) merging of
argument values upon procedure call and (2) merging of return values upon
procedure return. To combat the loss of precision, these two sinks have been
addressed independently. In the case of procedure calls, abstract garbage
collection reduces argument merging; while in the case of procedure returns,
context-free approaches eliminate return value merging. It is natural to expect
a combined analysis could enjoy the mutually beneficial interaction between the
two approaches. The central contribution of this work is a direct product of
abstract garbage collection with context-free analysis. The central challenge
to overcome is the conflict between the core constraint of a pushdown system
and the needs of garbage collection: a pushdown system can only see the top of
the stack, yet garbage collection needs to see the entire stack during a
collection. To make the direct product computable, we develop ""stack
summaries,"" a method for tracking stack properties at each control state in a
pushdown analysis of higher-order programs."
"We develop a model of concurrent imperative programming with threads. We
focus on a small imperative language with cooperative threads which execute
without interruption until they terminate or explicitly yield control. We
define and study a trace-based denotational semantics for this language; this
semantics is fully abstract but mathematically elementary. We also give an
equational theory for the computational effects that underlie the language,
including thread spawning. We then analyze threads in terms of the free algebra
monad for this theory."
"Constraint Handling Rules (CHR) is a declarative committed-choice programming
language with a strong relationship to linear logic. Its generalization CHR
with Disjunction (CHRv) is a multi-paradigm declarative programming language
that allows the embedding of horn programs. We analyse the assets and the
limitations of the classical declarative semantics of CHR before we motivate
and develop a linear-logic declarative semantics for CHR and CHRv. We show how
to apply the linear-logic semantics to decide program properties and to prove
operational equivalence of CHRv programs across the boundaries of language
paradigms."
"Ariola and Felleisen's call-by-need {\lambda}-calculus replaces a variable
occurrence with its value at the last possible moment. To support this gradual
notion of substitution, function applications-once established-are never
discharged. In this paper we show how to translate this notion of reduction
into an abstract machine that resolves variable references via the control
stack. In particular, the machine uses the static address of a variable
occurrence to extract its current value from the dynamic control stack."
"We describe and compare design choices for meta-predicate semantics, as found
in representative Prolog module systems and in Logtalk. We look at the
consequences of these design choices from a pragmatic perspective, discussing
explicit qualification semantics, computational reflection support,
expressiveness of meta-predicate declarations, safety of meta-predicate
definitions, portability of meta-predicate definitions, and meta-predicate
performance. Our aim is to provide useful insight for debating meta-predicate
semantics and portability issues based on actual implementations and common
usage patterns."
"(Non-)portability of Prolog programs is widely considered as an important
factor in the lack of acceptance of the language. Since 1995, the core of the
language is covered by the ISO standard 13211-1. Since 2007, YAP and SWI-Prolog
have established a basic compatibility framework. This article describes and
evaluates this framework. The aim of the framework is running the same code on
both systems rather than migrating an application. We show that today, the
portability within the family of Edinburgh/Quintus derived Prolog
implementations is good enough to allow for maintaining portable real-world
applications."
"With the dissemination of affordable parallel and distributed hardware,
parallel and distributed constraint solving has lately been the focus of some
attention. To effectually apply the power of distributed computational systems,
there must be an effective sharing of the work involved in the search for a
solution to a Constraint Satisfaction Problem (CSP) between all the
participating agents, and it must happen dynamically, since it is hard to
predict the effort associated with the exploration of some part of the search
space. We describe and provide an initial experimental assessment of an
implementation of a work stealing-based approach to distributed CSP solving."
"Logic programming provides a very high-level view of programming, which comes
at the cost of some execution efficiency. Improving performance of logic
programs is thus one of the holy grails of Prolog system implementations and a
wide range of approaches have historically been taken towards this goal.
Designing computational models that both exploit the available parallelism in a
given application and that try hard to reduce the explored search space has
been an ongoing line of research for many years. These goals in particular have
motivated the design of several computational models, one of which is the
Extended Andorra Model (EAM). In this paper, we present a preliminary
specification and implementation of the EAM with Implicit Control, the WAM2EAM,
which supplies regular WAM instructions with an EAM-centered interpretation."
"Online proceedings of the Joint Workshop on Implementation of Constraint
Logic Programming Systems and Logic-based Methods in Programming Environments
(CICLOPS-WLPE 2010), Edinburgh, Scotland, U.K., July 15, 2010."
"We discuss the problem of building a compiler which can lift in a provably
correct way pieces of information on the execution cost of the object code to
cost annotations on the source code. To this end, we need a clear and flexible
picture of: (i) the meaning of cost annotations, (ii) the method to prove them
sound and precise, and (iii) the way such proofs can be composed. We propose a
so-called labelling approach to these three questions. As a first step, we
examine its application to a toy compiler. This formal study suggests that the
labelling approach has good compositionality and scalability properties. In
order to provide further evidence for this claim, we report our successful
experience in implementing and testing the labelling approach on top of a
prototype compiler written in OCAML for (a large fragment of) the C language."
"GCC has a new infrastructure to support a link time optimization (LTO). The
infrastructure is designed to allow linking of large applications using a
special mode (WHOPR) which support parallelization of the compilation process.
In this paper we present overview of the design and implementation of WHOPR and
present test results of its behavior when optimizing large applications. We
give numbers on compile time, memory usage and code quality comparisons to the
classical file by file based optimization model. In particular we focus on
Firefox web browser. We show main problems seen only when compiling a large
application, such as startup time and code size growth."
"A steering fragment of an instruction sequence consists of a sequence of
steering instructions. These are decision points involving the check of a
propositional statement in sequential logic. The question is addressed why
composed propositional statements occur in steering fragments given the fact
that a straightforward transformation allows their elimination. A survey is
provided of constraints that may be implicitly assumed when composed
propositional statements occur in a meaningful instruction sequence."
"Taha and Nielsen have developed a multi-stage calculus {\lambda}{\alpha} with
a sound type system using the notion of environment classifiers. They are
special identifiers, with which code fragments and variable declarations are
annotated, and their scoping mechanism is used to ensure statically that
certain code fragments are closed and safely runnable. In this paper, we
investigate the Curry-Howard isomorphism for environment classifiers by
developing a typed {\lambda}-calculus {\lambda}|>. It corresponds to
multi-modal logic that allows quantification by transition variables---a
counterpart of classifiers---which range over (possibly empty) sequences of
labeled transitions between possible worlds. This interpretation will reduce
the ""run"" construct---which has a special typing rule in
{\lambda}{\alpha}---and embedding of closed code into other code fragments of
different stages---which would be only realized by the cross-stage persistence
operator in {\lambda}{\alpha}---to merely a special case of classifier
application. {\lambda}|> enjoys not only basic properties including subject
reduction, confluence, and strong normalization but also an important property
as a multi-stage calculus: time-ordered normalization of full reduction. Then,
we develop a big-step evaluation semantics for an ML-like language based on
{\lambda}|> with its type system and prove that the evaluation of a well-typed
{\lambda}|> program is properly staged. We also identify a fragment of the
language, where erasure evaluation is possible. Finally, we show that the proof
system augmented with a classical axiom is sound and complete with respect to a
Kripke semantics of the logic."
"Carrying Code (ACC) has recently been proposed as a framework for mobile code
safety in which the code supplier provides a program together with an
abstraction (or abstract model of the program) whose validity entails
compliance with a predefined safety policy. The advantage of providing a
(fixpoint) abstraction to the code consumer is that its validity is checked in
a single pass (i.e., one iteration) of an abstract interpretation-based
checker. A main challenge to make ACC useful in practice is to reduce the size
of certificates as much as possible while at the same time not increasing
checking time. The intuitive idea is to only include in the certificate
information that the checker is unable to reproduce without iterating. We
introduce the notion of reduced certificate which characterizes the subset of
the abstraction which a checker needs in order to validate (and re-construct)
the full certificate in a single pass. Based on this notion, we instrument a
generic analysis algorithm with the necessary extensions in order to identify
the information relevant to the checker. Interestingly, the fact that the
reduced certificate omits (parts of) the abstraction has implications in the
design of the checker. We provide the sufficient conditions which allow us to
ensure that 1) if the checker succeeds in validating the certificate, then the
certificate is valid for the program (correctness) and 2) the checker will
succeed for any reduced certificate which is valid (completeness). Our approach
has been implemented and benchmarked within the ciaopp system. The experimental
results show that our proposal is able to greatly reduce the size of
certificates in practice.To appear in Theory and Practice of Logic Programming
(TPLP)."
"We present two novel approaches to parsing context-free languages. The first
approach is based on an extension of Brzozowski's derivative from regular
expressions to context-free grammars. The second approach is based on a
generalization of the derivative to parser combinators. The payoff of these
techniques is a small (less than 250 lines of code), easy-to-implement parsing
library capable of parsing arbitrary context-free grammars into lazy parse
forests. Implementations for both Scala and Haskell are provided. Preliminary
experiments with S-Expressions parsed millions of tokens per second, which
suggests this technique is efficient enough for use in practice."
"SAGAs calculi (or simply SAGAs) have been proposed by Bruni et al. as a model
for long-running transactions. The approach therein can be considered static,
while a dynamic approach has been proposed by Lanese and Zavattaro. In this
paper we first extend both static SAGAs (in the centralized interruption
policy) and dynamic SAGAs to deal with nesting, then we compare the two
approaches."
"The goal of this lecture is to show how modern theorem provers---in this
case, the Coq proof assistant---can be used to mechanize the specification of
programming languages and their semantics, and to reason over individual
programs and over generic program transformations, as typically found in
compilers. The topics covered include: operational semantics (small-step,
big-step, definitional interpreters); a simple form of denotational semantics;
axiomatic semantics and Hoare logic; generation of verification conditions,
with application to program proof; compilation to virtual machine code and its
proof of correctness; an example of an optimizing program transformation (dead
code elimination) and its proof of correctness."
"Intersection and union types denote conjunctions and disjunctions of
properties. Using bidirectional typechecking, intersection types are relatively
straightforward, but union types present challenges. For union types, we can
case-analyze a subterm of union type when it appears in evaluation position
(replacing the subterm with a variable, and checking that term twice under
appropriate assumptions). This technique preserves soundness in a call-by-value
semantics.
  Sadly, there are so many choices of subterms that a direct implementation is
not practical. But carefully transforming programs into let-normal form
drastically reduces the number of choices. The key results are soundness and
completeness: a typing derivation (in the system with too many subterm choices)
exists for a program if and only if a derivation exists for the let-normalized
program."
"This paper presents a type theory with a form of equality reflection:
provable equalities can be used to coerce the type of a term. Coercions and
other annotations, including implicit arguments, are dropped during reduction
of terms. We develop the metatheory for an undecidable version of the system
with unannotated terms. We then devise a decidable system with annotated terms,
justified in terms of the unannotated system. Finally, we show how the approach
can be extended to account for large eliminations, using what we call
quasi-implicit products."
"Contracts are a well-established approach for describing and analyzing
behavioral aspects of web service compositions. The theory of contracts comes
equipped with a notion of compatibility between clients and servers that
ensures that every possible interaction between compatible clients and servers
will complete successfully. It is generally agreed that real applications often
require the ability of exposing just partial descriptions of their behaviors,
which are usually known as abstract processes. We propose a formal
characterization of abstraction as an extension of the usual symbolic
bisimulation and we recover the notion of abstraction in the context of
contracts."
"Event-driven programming is used in many fields of modern Computer Science.
In event-driven programming languages user interacts with a program by
triggering the events. We propose a new approach that we denote command-event
driven programming in which the user interacts with a program by means of
events and commands. We describe a new programming language, T2Script, which is
based on command-event driven paradigm. T2Script has been already implemented
and used in one of industrial products. We describe the rationale, basic
concepts and advanced programming techniques of new T2Script language. We
evaluate the new language and show what advantages and limitations it has."
"Logic programming provides a high-level view of programming, giving
implementers a vast latitude into what techniques to explore to achieve the
best performance for logic programs. Towards obtaining maximum performance, one
of the holy grails of logic programming has been to design computational models
that could be executed efficiently and that would allow both for a reduction of
the search space and for exploiting all the available parallelism in the
application. These goals have motivated the design of the Extended Andorra
Model, a model where goals that do not constrain non-deterministic goals can
execute first.
  In this work we present and evaluate the Basic design for Extended Andorra
Model (BEAM), a system that builds upon David H. D. Warren's original EAM with
Implicit Control. We provide a complete description and implementation of the
BEAM System as a set of rewrite and control rules. We present the major data
structures and execution algorithms that are required for efficient execution,
and evaluate system performance.
  A detailed performance study of our system is included. Our results show that
the system achieves acceptable base performance, and that a number of
applications benefit from the advanced search inherent to the EAM."
"Context-oriented programming is an emerging paradigm addressing at the
language level the issue of dynamic software adaptation and modularization of
context-specific concerns. In this paper we propose JavaCtx, a tool which
employs coding conventions to generate the context-aware semantics for Java
programs and subsequently weave it into the application. The contribution of
JavaCtx is twofold: the design of a set of coding conventions which allow to
write context-oriented software in plain Java and the concept of
context-oriented semantics injection, which allows to introduce the
context-aware semantics without a source-to-source compilations process which
disrupts the structure of the code. Both these points allow to seamless
integrate JavaCtx in the existing industrial-strength appliances and by far
ease the development of context-oriented software."
"Dataflow languages provide natural support for specifying constraints between
objects in dynamic applications, where programs need to react efficiently to
changes of their environment. Researchers have long investigated how to take
advantage of dataflow constraints by embedding them into procedural languages.
Previous mixed imperative/dataflow systems, however, require syntactic
extensions or libraries of ad hoc data types for binding the imperative program
to the dataflow solver. In this paper we propose a novel approach that smoothly
combines the two paradigms without placing undue burden on the programmer. In
our framework, programmers can define ordinary commands of the host imperative
language that enforce constraints between objects stored in ""reactive"" memory
locations. Reactive objects can be of any legal type in the host language,
including primitive data types, pointers, arrays, and structures. Constraints
are automatically re-executed every time their input memory locations change,
letting a program behave like a spreadsheet where the values of some variables
depend upon the values of other variables. The constraint solving mechanism is
handled transparently by altering the semantics of elementary operations of the
host language for reading and modifying objects. We provide a formal semantics
and describe a concrete embodiment of our technique into C/C++, showing how to
implement it efficiently in conventional platforms using off-the-shelf
compilers. We discuss relevant applications to reactive scenarios, including
incremental computation, observer design pattern, and data structure repair.
The performance of our implementation is compared to ad hoc problem-specific
change propagation algorithms and to language-centric approaches such as
self-adjusting computation and subject/observer communication mechanisms,
showing that the proposed approach is efficient in practice."
"We demonstrate a method to infer polymorphically principal and
subtyping-minimal types for an ML-like core language by assigning ranges within
a lattice to type variables. We demonstrate the termination and completeness of
this algorithm, and proceed to show that it solves a broad special-case of the
generally-undecidable semi-unification problem. Our procedure requires no type
annotations, leaves no subtyping constraints in the inferred types, and
produces no proof obligations. We demonstrate the practical utility of our
technique by showing a type-preserving encoding of Featherweight Java into the
expression calculus over which we infer types."
"We present an algorithmic method for the quantitative, performance-aware
synthesis of concurrent programs. The input consists of a nondeterministic
partial program and of a parametric performance model. The nondeterminism
allows the programmer to omit which (if any) synchronization construct is used
at a particular program location. The performance model, specified as a
weighted automaton, can capture system architectures by assigning different
costs to actions such as locking, context switching, and memory and cache
accesses. The quantitative synthesis problem is to automatically resolve the
nondeterminism of the partial program so that both correctness is guaranteed
and performance is optimal. As is standard for shared memory concurrency,
correctness is formalized ""specification free"", in particular as race freedom
or deadlock freedom. For worst-case (average-case) performance, we show that
the problem can be reduced to 2-player graph games (with probabilistic
transitions) with quantitative objectives. While we show, using game-theoretic
methods, that the synthesis problem is NEXP-complete, we present an algorithmic
method and an implementation that works efficiently for concurrent programs and
performance models of practical interest. We have implemented a prototype tool
and used it to synthesize finite-state concurrent programs that exhibit
different programming patterns, for several performance models representing
different architectures."
"This paper presents language techniques for applying memoization selectively.
The techniques provide programmer control over equality, space usage, and
identification of precise dependences so that memoization can be applied
according to the needs of an application. Two key properties of the approach
are that it accepts and efficient implementation and yields programs whose
performance can be analyzed using standard analysis techniques. We describe our
approach in the context of a functional language called MFL and an
implementation as a Standard ML library. The MFL language employs a modal type
system to enable the programmer to express programs that reveal their true data
dependences when executed. We prove that the MFL language is sound by showing
that that MFL programs yield the same result as they would with respect to a
standard, non-memoizing semantics. The SML implementation cannot support the
modal type system of MFL statically but instead employs run-time checks to
ensure correct usage of primitives."
"This paper presents a semantics of self-adjusting computation and proves that
the semantics are correct and consistent. The semantics integrate change
propagation with the classic idea of memoization to enable reuse of
computations under mutation to memory. During evaluation, reuse of a
computation via memoization triggers a change propagation that adjusts the
reused computation to reflect the mutated memory. Since the semantics integrate
memoization and change-propagation, it involves both non-determinism (due to
memoization) and mutation (due to change propagation). Our consistency theorem
states that the non-determinism is not harmful: any two evaluations of the same
program starting at the same state yield the same result. Our correctness
theorem states that mutation is not harmful: self-adjusting programs are
consistent with purely functional programming. We formalize the semantics and
their meta-theory in the LF logical framework and machine check our proofs
using Twelf."
"Representation sharing can reduce the memory footprint of a program by
sharing one representation between duplicate terms. The most common
implementation of representation sharing in functional programming systems is
known as hash-consing. In the context of Prolog, representation sharing has
been given little attention. Some current techniques that deal with
representation sharing are reviewed. The new contributions are: (1) an easy
implementation of {\em input sharing} for {\em findall/3}; (2) a description of
a {\em sharer} module that introduces representation sharing at runtime. Their
realization is shown in the context of the WAM as implemented by hProlog. Both
can be adapted to any WAM-like Prolog implementation. The sharer works
independently of the garbage collector, but it can be made to cooperate with
the garbage collector. Benchmark results show that the sharer has a cost
comparable to the heap garbage collector, that its effectiveness is highly
application dependent, and that its policy must be tuned to the collector. To
appear in Theory and Practice of Logic Programming (TPLP)"
"When scripts in untyped languages grow into large programs, maintaining them
becomes difficult. A lack of explicit type annotations in typical scripting
languages forces programmers to must (re)discover critical pieces of design
information every time they wish to change a program. This analysis step both
slows down the maintenance process and may even introduce mistakes due to the
violation of undiscovered invariants.
  This paper presents Typed Scheme, an explicitly typed extension of PLT
Scheme, an untyped scripting language. Its type system is based on the novel
notion of occurrence typing, which we formalize and mechanically prove sound.
The implementation of Typed Scheme additionally borrows elements from a range
of approaches, including recursive types, true unions and subtyping, plus
polymorphism combined with a modicum of local inference.
  The formulation of occurrence typing naturally leads to a simple and
expressive version of predicates to describe refinement types. A Typed Scheme
program can use these refinement types to keep track of arbitrary classes of
values via the type system. Further, we show how the Typed Scheme type system,
in conjunction with simple recursive types, is able to encode refinements of
existing datatypes, thus expressing both proposed variations of refinement
types."
"Pattern matching is a widely used technique in functional languages,
especially those in the ML and Haskell traditions, where it is at the core of
the semantics. In languages in the Lisp tradition, in contrast, pattern
matching it typically provided by libraries built with macros. We present
match, a sophisticated pattern matcher for Racket, implemented as language
extension. using macros. The system supports novel and widely-useful
pattern-matching forms, and is itself extensible. The extensibility of match is
implemented via a general technique for creating extensible language
extensions."
"Nominal abstract syntax is a popular first-order technique for encoding, and
reasoning about, abstract syntax involving binders. Many of its applications
involve constraint solving. The most commonly used constraint solving algorithm
over nominal abstract syntax is the Urban-Pitts-Gabbay nominal unification
algorithm, which is well-behaved, has a well-developed theory and is applicable
in many cases. However, certain problems require a constraint solver which
respects the equivariance property of nominal logic, such as Cheney's
equivariant unification algorithm. This is more powerful but is more
complicated and computationally hard. In this paper we present a novel
algorithm for solving constraints over a simple variant of nominal abstract
syntax which we call non-permutative. This constraint problem has similar
complexity to equivariant unification but without many of the additional
complications of the equivariant unification term language. We prove our
algorithm correct, paying particular attention to issues of termination, and
present an explicit translation of name-name equivariant unification problems
into non-permutative constraints."
"We propose an optimizing transformation which reduces program runtime at the
expense of program size by eliminating conditional jumps."
"A software architecture describes the structure of a computing system by
specifying software components and their interactions. Mapping a software
architecture to an implementation is a well known challenge. A key element of
this mapping is the architecture's description of the data and control-flow
interactions between components. The characterization of these interactions can
be rather abstract or very concrete, providing more or less implementation
guidance, programming support, and static verification. In this paper, we
explore one point in the design space between abstract and concrete component
interaction specifications. We introduce a notion of interaction contract that
expresses allowed interactions between components, describing both data and
control-flow constraints. This declaration is part of the architecture
description, allows generation of extensive programming support, and enables
various verifications. We instantiate our approach in an architecture
description language for Sense/Compute/Control applications, and describe
associated compilation and verification strategies."
"Pull-tabbing is an evaluation approach for functional logic computations,
based on a graph transformation recently proposed, which avoids making
irrevocable non-deterministic choices that would jeopardize the completeness of
computations. In contrast to other approaches with this property, it does not
require an upfront cloning of a possibly large portion of the choice's context.
We formally define the pull-tab transformation, characterize the class of
programs for which the transformation is intended, extend the computations in
these programs to include the transformation, and prove the correctness of the
extended computations."
"Linear logic Concurrent Constraint programming (LCC) is an extension of
concurrent constraint programming (CC) where the constraint system is based on
Girard's linear logic instead of the classical logic. In this paper we address
the problem of program equivalence for this programming framework. For this
purpose, we present a structural operational semantics for LCC based on a label
transition system and investigate different notions of observational
equivalences inspired by the state of art of process algebras. Then, we
demonstrate that the asynchronous \pi-calculus can be viewed as simple
syntactical restrictions of LCC. Finally we show LCC observational equivalences
can be transposed straightforwardly to classical Concurrent Constraint
languages and Constraint Handling Rules, and investigate the resulting
equivalences."
"Over the past decades, coordination languages have emerged for the
specification and implementation of interaction protocols for communicating
software components. This class of languages includes Reo, a platform for
compositional construction of connectors. In recent years, various formalisms
for describing the behavior of Reo connectors have come to existence, each of
them serving its own purpose. Naturally, questions about how these models
relate to each other arise. From a theoretical point of view, answers to these
questions provide us with better insight into the fundamentals of Reo, while
from a more practical perspective, these answers broaden the applicability of
Reo's development tools. In this paper, we address one of these questions: we
investigate the equivalence between coloring models and constraint automata,
the two most dominant and practically relevant semantic models of Reo. More
specifically, we define operators that transform one model to the other (and
vice versa), prove their correctness, and show that they distribute over
composition. To ensure that the transformation operators map one-to-one
(instead of many-to-one), we extend coloring models with data constraints.
Though primarily a theoretical contribution, we sketch some potential
applications of our results: the broadening of the applicability of existing
tools for connector verification and animation."
"Points-to analysis is the problem of approximating run-time values of
pointers statically or at compile-time. Points-to sets are used to store the
approximated values of pointers during points-to analysis. Memory usage and
running time limit the ability of points-to analysis to analyze large programs.
  To our knowledge, works which have implemented a bit-vector representation of
points-to sets so far, allocates bits for each pointer without considering
pointer's type. By considering the type, we are able to allocate bits only for
a subset of all abstract objects which are of compatible type with the
pointer's type and as a consequence improve the memory usage and running time.
To achieve this goal, we number abstract objects in a way that all the abstract
objects of a type and all of its sub-types be consecutive in order.
  Our most efficient implementation uses about 2.5 times less memory than
hybrid points-to set (default points-to set in Spark) and also improves the
analysis time for sufficiently large programs."
"Self-adjusting computation offers a language-based approach to writing
programs that automatically respond to dynamically changing data. Recent work
made significant progress in developing sound semantics and associated
implementations of self-adjusting computation for high-level, functional
languages. These techniques, however, do not address issues that arise for
low-level languages, i.e., stack-based imperative languages that lack strong
type systems and automatic memory management.
  In this paper, we describe techniques for self-adjusting computation which
are suitable for low-level languages. Necessarily, we take a different approach
than previous work: instead of starting with a high-level language with
additional primitives to support self-adjusting computation, we start with a
low-level intermediate language, whose semantics is given by a stack-based
abstract machine. We prove that this semantics is sound: it always updates
computations in a way that is consistent with full reevaluation. We give a
compiler and runtime system for the intermediate language used by our abstract
machine. We present an empirical evaluation that shows that our approach is
efficient in practice, and performs favorably compared to prior proposals."
"Sequential programming and work-flow programming are two useful, but
radically different, ways of describing computational processing. Of the two,
it is sequential programming that we teach all programmers and support by
programming languages, whether in procedural, objectoriented, or functional
paradigms. We teach workflow as a secondary style of problem decomposition for
use in special situations, like distributed or networked processing. Both
styles offer complementary advantages, but the fact that they employ radically
different models for ownership of continuations interferes with our ability to
integrate them in a way that allows them to be taught and used in a single
programming language. This paper describes a programming language construct,
declarative future commitments, that permit better integration of the two."
"Debugging lazy functional programs poses serious challenges. In support of
the ""stop, examine, and resume"" debugging style of imperative languages, some
debugging tools abandon lazy evaluation. Other debuggers preserve laziness but
present it in a way that may confuse programmers because the focus of
evaluation jumps around in a seemingly random manner.
  In this paper, we introduce a supplemental tool, the algebraic program
stepper. An algebraic stepper shows computation as a mathematical calculation.
Algebraic stepping could be particularly useful for novice programmers or
programmers new to lazy programming. Mathematically speaking, an algebraic
stepper renders computation as the standard rewriting sequence of a lazy
lambda-calculus. Our novel lazy semantics introduces lazy evaluation as a form
of parallel program rewriting. It represents a compromise between Launchbury's
store-based semantics and a simple, axiomatic description of lazy computation
as sharing-via-parameters. Finally, we prove that the stepper's run-time
machinery correctly reconstructs the standard rewriting sequence."
"The usual advantages put forward for including nullability declarations in
the type systems of programming languages are that they improve program
reliability or performance. But there is another, entirely different, reason
for doing so. In the right context, this information enables the software
artifacts we produce, the objects and methods, to exhibit much greater
malleability. For declaratively typed languages, we can obtain greater software
malleability by extending the model of method call so that assurance of a
method's availability can be provided by any non-nullable parameter, not simply
the target parameter, and by allowing the method's implementation to reside in
classes or objects other than the target..
  This paper examines the question of whether this hypothetical improvement in
software malleability is consistent with existing programming practice by
examining the question of the extent to which methods in existing software have
multiplicities of non-nullable parameters. The circumstance occurs frequently
enough to provide an important reason to introduce declarations of nullability
into programming languages."
"Answer Set Programming (ASP) is a declarative logic programming formalism,
which is employed nowadays in both academic and industrial real-world
applications. Although some tools for supporting the development of ASP
programs have been proposed in the last few years, the crucial task of testing
ASP programs received less attention, and is an Achilles' heel of the available
programming environments.
  In this paper we present a language for specifying and running unit tests on
ASP programs. The testing language has been implemented in ASPIDE, a
comprehensive IDE for ASP, which supports the entire life-cycle of ASP
development with a collection of user-friendly graphical tools for program
composition, testing, debugging, profiling, solver execution configuration, and
output-handling."
"KiCS2 is a new system to compile functional logic programs of the source
language Curry into purely functional Haskell programs. The implementation is
based on the idea to represent the search space as a data structure and logic
variables as operations that generate their values. This has the advantage that
one can apply various, and in particular, complete search strategies to compute
solutions. However, the generation of all values for logic variables might be
inefficient for applications that exploit constraints on partially known
values. To overcome this drawback, we propose new techniques to implement
equational constraints in this framework. In particular, we show how
unification modulo function evaluation and functional patterns can be added
without sacrificing the efficiency of the kernel implementation."
"This volume is the proceedings of the second IFIP Working Conference on
Domain-Specific Languages (DSL 2011). It contains 2 abstracts of invited
presentations, 7 peer-reviewed articles selected by the program committee from
14 submissions, and 6 lecture notes for the distilled tutorials that we
solicited."
"Knowledge-based systems are suitable for realizing advanced functions that
require domain-specific expert knowledge, while knowledge representation
languages and their supporting environments are essential for realizing such
systems. Although Prolog is useful and effective in realizing such a supporting
environment, the language interoperability with other implementation languages,
such as Java, is often an important issue in practical application development.
This paper describes the techniques for translating a knowledge representation
language that is a nondeterministic functional language based on attribute
grammars into Java. The translation is based on binarization and the techniques
proposed for Prolog to Java translation although the semantics are different
from those of Prolog. A continuation unit is introduced to handle continuation
efficiently, while the variable and register management on backtracking is
simplified by using the single and unidirectional assignment features of
variables. An experimental translator written in the language itself
successfully generates Java code, while experimental results show that the
generated code is over 25 times faster than that of Prolog Cafe for
nondeterministic programs, and over 2 times faster for deterministic programs.
The generated code is also over 2 times faster than B-Prolog for
nondeterministic programs."
"We present an embedded DSL to support adaptation-based programming (ABP) in
Haskell. ABP is an abstract model for defining adaptive values, called
adaptives, which adapt in response to some associated feedback. We show how our
design choices in Haskell motivate higher-level combinators and constructs and
help us derive more complicated compositional adaptives.
  We also show an important specialization of ABP is in support of
reinforcement learning constructs, which optimize adaptive values based on a
programmer-specified objective function. This permits ABP users to easily
define adaptive values that express uncertainty anywhere in their programs.
Over repeated executions, these adaptive values adjust to more efficient ones
and enable the user's programs to self optimize.
  The design of our DSL depends significantly on the use of type classes. We
will illustrate, along with presenting our DSL, how the use of type classes can
support the gradual evolution of DSLs."
"Stencil computations, involving operations over the elements of an array, are
a common programming pattern in scientific computing, games, and image
processing. As a programming pattern, stencil computations are highly regular
and amenable to optimisation and parallelisation. However, general-purpose
languages obscure this regular pattern from the compiler, and even the
programmer, preventing optimisation and obfuscating (in)correctness. This paper
furthers our work on the Ypnos domain-specific language for stencil
computations embedded in Haskell. Ypnos allows declarative, abstract
specification of stencil computations, exposing the structure of a problem to
the compiler and to the programmer via specialised syntax. In this paper we
show the decidable safety guarantee that well-formed, well-typed Ypnos programs
cannot index outside of array boundaries. Thus indexing in Ypnos is safe and
run-time bounds checking can be eliminated. Program information is encoded as
types, using the advanced type-system features of the Glasgow Haskell Compiler,
with the safe-indexing invariant enforced at compile time via type checking."
"Domain-specific languages raise the level of abstraction in software
development. While it is evident that programmers can more easily reason about
very high-level programs, the same holds for compilers only if the compiler has
an accurate model of the application domain and the underlying target platform.
Since mapping high-level, general-purpose languages to modern, heterogeneous
hardware is becoming increasingly difficult, DSLs are an attractive way to
capitalize on improved hardware performance, precisely by making the compiler
reason on a higher level. Implementing efficient DSL compilers is a daunting
task however, and support for building performance-oriented DSLs is urgently
needed. To this end, we present the Delite Framework, an extensible toolkit
that drastically simplifies building embedded DSLs and compiling DSL programs
for execution on heterogeneous hardware. We discuss several building blocks in
some detail and present experimental results for the OptiML machine-learning
DSL implemented on top of Delite."
"We present a domain-specific embedded language (DSEL) in Haskell that
supports the philosophical study and practical explanation of causation. The
language provides constructs for modeling situations comprised of events and
functions for reliably determining the complex causal relationships that emerge
between these events. It enables the creation of visual explanations of these
causal relationships and a means to systematically generate alternative,
related scenarios, along with corresponding outcomes and causes. The DSEL is
based on neuron diagrams, a visual notation that is well established in
practice and has been successfully employed for causation explanation and
research. In addition to its immediate applicability by users of neuron
diagrams, the DSEL is extensible, allowing causation experts to extend the
notation to introduce special-purpose causation constructs. The DSEL also
extends the notation of neuron diagrams to operate over non-boolean values,
improving its expressiveness and offering new possibilities for causation
research and its applications."
"This paper is a short tutorial introduction to online partial evaluation. We
show how to write a simple online partial evaluator for a simple, pure,
first-order, functional programming language. In particular, we show that the
partial evaluator can be derived as a variation on a compositionally defined
interpreter. We demonstrate the use of the resulting partial evaluator for
program optimization in the context of model-driven development."
"In this tutorial, we program big-step and small-step total interpreters for
the While language extended with input and output primitives. While is a simple
imperative language consisting of skip, assignment, sequence, conditional and
loop. We first develop trace-based interpreters for While. Traces are
potentially infinite nonempty sequences of states. The interpreters assign
traces to While programs: for us, traces are denotations of While programs. The
trace is finite if the program is terminating and infinite if the program is
non-terminating. However, we cannot decide (i.e., write a program to
determine), for any given program, whether its trace is finite or infinite,
which amounts to deciding the halting problem. We then extend While with
interactive input/output primitives. Accordingly, we extend the interpreters by
generalizing traces to resumptions.
  The tutorial is based on our previous work with T. Uustalu on reasoning about
interactive programs in the setting of constructive type theory."
"In static analysis by abstract interpretation, one often uses widening
operators in order to enforce convergence within finite time to an inductive
invariant. Certain widening operators, including the classical one over finite
polyhedra, exhibit an unintuitive behavior: analyzing the program over a subset
of its variables may lead a more precise result than analyzing the original
program! In this article, we present simple workarounds for such behavior."
"We have implemented C like Continuation based programming language.
Continuation based C, CbC was implemented using micro-C on various
architecture, and we have tried several CbC programming experiments. Here we
report new implementation of CbC compiler based on GCC 4.2.3. Since it contains
full C capability, we can use CbC and C in a mixture."
"We design a family of program analyses for JavaScript that make no
approximation in matching calls with returns, exceptions with handlers, and
breaks with labels. We do so by starting from an established reduction
semantics for JavaScript and systematically deriving its intensional abstract
interpretation. Our first step is to transform the semantics into an equivalent
low-level abstract machine: the JavaScript Abstract Machine (JAM). We then give
an infinite-state yet decidable pushdown machine whose stack precisely models
the structure of the concrete program stack. The precise model of stack
structure in turn confers precise control-flow analysis even in the presence of
control effects, such as exceptions and finally blocks. We give pushdown
generalizations of traditional forms of analysis such as k-CFA, and prove the
pushdown framework for abstract interpretation is sound and computable."
"Matrix Code gives imperative programming a mathematical semantics and
heuristic power comparable in quality to functional and logic programming. A
program in Matrix Code is developed incrementally from a specification in
pre/post-condition form. The computations of a code matrix are characterized by
powers of the matrix when it is interpreted as a transformation in a space of
vectors of logical conditions. Correctness of a code matrix is expressed in
terms of a fixpoint of the transformation. The abstract machine for Matrix Code
is the dual-state machine, which we present as a variant of the classical
finite-state machine."
"We present a so-called labelling method to insert cost annotations in a
higher-order functional program, to certify their correctness with respect to a
standard compilation chain to assembly code including safe memory management,
and to reason on them in a higher-order Hoare logic."
"In object systems, classes take the role of modules, and interfaces consist
of methods. Because methods are encapsulated in objects, interfaces in object
systems do not allow abstracting over \emph{where} methods are implemented.
This implies that any change to the implementation structure may cause a
rippling effect. Sometimes this unduly restricts the scope of software
evolution, in particular for methods with multiple parameters where there is no
clear owner. We propose a simple scheme where symmetric methods may be defined
in the classes of any of their parameters. This allows client code to be
oblivious of what class contains a method implementation, and therefore immune
against it changing. When combined with multiple dynamic dispatch, this scheme
allows for modular extensibility where a method defined in one class is
overridden by a method defined in a class that is not its subtype. In this
paper, we illustrate the scheme by extending a core calculus of class-based
languages with these symmetric encapsulated multi-methods, and prove the result
sound."
"There is often a sort of a protocol associated to each class, stating when
and how certain methods should be called. Given that this protocol is, if at
all, described in the documentation accompanying the class, current mainstream
object-oriented languages cannot provide for the verification of client code
adherence against the sought class behaviour. We have defined a class-based
concurrent object-oriented language that formalises such protocols in the form
of usage types. Usage types are attached to class definitions, allowing for the
specification of (1) the available methods, (2) the tests clients must perform
on the result of methods, and (3) the object status - linear or shared - all of
which depend on the object's state. Our work extends the recent approach on
modular session types by eliminating channel operations, and defining the
method call as the single communication primitive in both sequential and
concurrent settings. In contrast to previous works, we define a single category
for objects, instead of distinct categories for linear and for shared objects,
and let linear objects evolve into shared ones. We introduce a standard sync
qualifier to prevent thread interference in certain operations on shared
objects. We formalise the language syntax, the operational semantics, and a
type system that enforces by static typing that methods are called only when
available, and by a single client if so specified in the usage type. We
illustrate the language via a complete example."
"We present an inference system for a version of the Pi-calculus in Haskell
for the session type proposed by Honda et al. The session type is very useful
in checking if the communications are well-behaved. The full session type
implementation in Haskell was first presented by Pucella and Tov, which is
'semi-automatic' in that the manual operations for the type representation was
necessary. We give an automatic type inference for the session type by using a
more abstract representation for the session type based on the 'de Bruijn
levels'. We show an example of the session type inference for a simple SMTP
client."
"X10 is a modern language built from the ground up to handle future parallel
systems, from multicore machines to cluster configurations. We take a closer
look at a pair of synchronisation mechanisms: finish and clocks. The former
waits for the termination of parallel computations, the latter allow multiple
concurrent activities to wait for each other at certain points in time. In
order to better understand these concepts we study a type system for a stripped
down version of X10. The main result assures that well typed programs do not
run into the errors identified in the X10 language reference, namely the
ClockUseException. The study will open, we hope, doors to a more flexible
utilisation of clocks in the X10 language."
"Emergence is the way complex systems arise out of a multiplicity of
relatively simple interactions between primitives. Since programming problems
become more and more complexes and transverses, our vision is that application
development should be process at two scales: micro- and macro-programming where
at the micro-level the paradigm is step-by-step and at macro-level the paradigm
is emergence. For micro-programming, which focuses on how things happen,
popular languages, Java, C++, Python, are imperative writing languages where
the code is a sequence of sentences executed by the computer. For
macro-programming, which focuses on how things connect, popular languages,
labVIEW, Blender, Simulink, are graphical data flow languages such that the
program is a composition of operators, a unit-process consuming input data and
producing output data, and connectors, a data-flow between an output data and
an input data of two operators. However, despite their fruitful applications,
these macro-languages are not transversal since different data-structures of
native data-structures cannot be integrated in their framework easily. Cameleon
language is a graphical data flow language following a two-scale paradigm. It
allows an easy up-scale that is the integration of any library writing in C++
in the data flow language. Cameleon language aims to democratize
macro-programming by an intuitive interaction between the human and the
computer where building an application based on a data-process and a GUI is a
simple task to learn and to do. Cameleon language allows conditional execution
and repetition to solve complex macro-problems. In this paper we introduce a
new model based on the extension of the petri net model for the description of
how the Cameleon language executes a composition."
"As recent programming languages provide improved conciseness and flexibility
of syntax, the development of embedded or internal Domain-Specific Languages
has increased. The field of Modeling and Simulation has had a long history of
innovation in programming languages (e.g. Simula-67, GPSS). Much effort has
gone into the development of Simulation Programming Languages.
  The ScalaTion project is working to develop an embedded or internal
Domain-Specific Language for Modeling and Simulation which could streamline
language innovation in this domain. One of its goals is to make the code
concise, readable, and in a form familiar to experts in the domain. In some
cases the code looks very similar to textbook formulas. To enhance readability
by domain experts, a version of ScalaTion is provided that heavily utilizes
Unicode.
  This paper discusses the development of the ScalaTion DSL and the underlying
features of Scala that make this possible. It then provides an overview of
ScalaTion highlighting some uses of Unicode. Statistical analysis capabilities
needed for Modeling and Simulation are presented in some detail. The notation
developed is clear and concise which should lead to improved usability and
extendibility."
"Tabling is an evaluation strategy for Prolog programs that works by storing
answers in a table space and then by using them in similar subgoals. Some
tabling engines use call by subsumption, where it is determined that a subgoal
will consume answers from a more general subgoal in order to reduce the search
space and increase efficiency. We designed an extension, named Retroactive Call
Subsumption (RCS), that implements call by subsumption independently of the
call order, thus allowing a more general subgoal to force previous called
subgoals to become answer consumers. For this extension, we propose a new table
space design, the Single Time Stamped Trie (STST), that is organized to make
answer sharing across subsumed/subsuming subgoals simple and efficient. In this
paper, we present the new STST table space design and we discuss the main
modifications made to the original Time Stamped Tries approach to
non-retroactive call by subsumption. In experimental results, with programs
that stress some deficiencies of the new STST design, some overheads may be
observed, however the results achieved with more realistic programs greatly
offset these overheads."
"A critical component in the implementation of an efficient tabling system is
the design of the table space. The most popular and successful data structure
for representing tables is based on a two-level trie data structure, where one
trie level stores the tabled subgoal calls and the other stores the computed
answers. The Global Trie (GT) is an alternative table space organization
designed with the intent to reduce the tables's memory usage, namely by storing
terms in a global trie, thus preventing repeated representations of the same
term in different trie data structures. In this paper, we propose an extension
to the GT organization, named Global Trie for Subterms (GT-ST), where compound
subterms in term arguments are represented as unique entries in the GT.
Experiments results using the YapTab tabling system show that GT-ST support has
potential to achieve significant reductions on memory usage, for programs with
increasing compound subterms in term arguments, without compromising the
execution time for other programs."
"e use Prolog as a flexible meta-language to provide executable specifications
of some fundamental mathematical objects and their transformations. In the
process, isomorphisms are unraveled between natural numbers and combinatorial
objects (rooted ordered trees representing hereditarily finite sequences and
rooted ordered binary trees representing G\""odel's System {\bf T} types).
  This paper focuses on an application that can be seen as an unexpected
""paradigm shift"": we provide recursive definitions showing that the resulting
representations are directly usable to perform symbolically arbitrary-length
integer computations.
  Besides the theoretically interesting fact of ""breaking the
arithmetic/symbolic barrier"", the arithmetic operations performed with symbolic
objects like trees or types turn out to be genuinely efficient -- we derive
implementations with asymptotic performance comparable to ordinary bitstring
implementations of arbitrary-length integer arithmetic.
  The source code of the paper, organized as a literate Prolog program, is
available at \url{http://logic.cse.unt.edu/tarau/research/2011/pPAR.pl}"
"We describe L-FLAT, a Logtalk Toolkit for teaching Formal Languages and
Automata Theory. L-FLAT supports the definition of \textsl{alphabets}, the
definition of \textsl{orders} over alphabet symbols, the partial definition of
\textsl{languages} using unit tests, and the definition of \textsl{mechanisms},
which implement language generators or language recognizers. Supported
mechanisms include \textsl{predicates}, \textsl{regular expressions},
\textsl{finite automata}, \textsl{context-free grammars}, \textsl{Turing
machines}, and \textsl{push-down automata}. L-FLAT entities are implemented
using the object-oriented features of Logtalk, providing a highly portable and
easily extendable framework. The use of L-FLAT in educational environments is
enhanced by supporting Mooshak, a web application that features automatic
grading of submitted programs."
"We describe an application of Prolog: a type checking tool for the Q
functional language. Q is a terse vector processing language, a descendant of
APL, which is getting more and more popular, especially in financial
applications. Q is a dynamically typed language, much like Prolog. Extending Q
with static typing improves both the readability of programs and programmer
productivity, as type errors are discovered by the tool at compile time, rather
than through debugging the program execution.
  The type checker uses constraints that are handled by Prolog Constraint
Handling Rules. During the analysis, we determine the possible type values for
each program expression and detect inconsistencies. As most built-in function
names of Q are overloaded, i.e. their meaning depends on the argument types, a
quite complex system of constraints had to be implemented."
"When doing inference in ProbLog, a probabilistic extension of Prolog, we
extend SLD resolution with some additional bookkeeping. This additional
information is used to compute the probabilistic results for a probabilistic
query. In Prolog's SLD, goals are nested very naturally. In ProbLog's SLD,
nesting probabilistic queries interferes with the probabilistic bookkeeping. In
order to support nested probabilistic inference we propose the notion of a
parametrised ProbLog engine. Nesting becomes possible by suspending and
resuming instances of ProbLog engines. With our approach we realise several
extensions of ProbLog such as meta-calls, negation, and answers of
probabilistic goals."
"A new high-level interface to multi-threading in Prolog, implemented in
hProlog, is described. Modern CPUs often contain multiple cores and through
high-level multi-threading a programmer can leverage this power without having
to worry about low-level details. Two common types of high-level explicit
parallelism are discussed: independent and-parallelism and competitive
or-parallelism. A new type of explicit parallelism, pipeline parallelism, is
proposed. This new type can be used in certain cases where independent
and-parallelism and competitive or-parallelism cannot be used."
"We present a technique exploiting Datalog with aggregates to improve the
performance of programs with arithmetic (in)equalities. Our approach employs a
source-to-source program transformation which approximates the propagation
technique from Constraint Programming. The experimental evaluation of the
approach shows good run time speed-ups on a range of non-recursive as well as
recursive programs. Furthermore, our technique improves upon the previously
reported in the literature constraint magic set transformation approach."
"We encode/decode Prolog terms as unique natural numbers. Our encodings have
the following properties: a) are bijective b) natural numbers always decode to
syntactically valid terms c) they work in low polynomial time in the bitsize of
the representations d) the bitsize of our encodings is within constant factor
of the syntactic representation of the input.
  We describe encodings of term algebras with finite signature as well as
algorithms that separate the ""structure"" of a term, a natural number encoding
of a list of balanced parenthesis, from its ""content"", a list of atomic terms
and Prolog variables. The paper is organized as a literate Prolog program
available from \url{http://logic.cse.unt.edu/tarau/research/2011/bijenc.pl}."
"Non-determinism is of great importance in functional logic programming. It
provides expressiveness and efficiency to functional logic computations. In
this paper we describe an implementation of the multi-paradigm functional logic
language Curry. The evaluation strategy employed by the implementation is based
on definitional trees and needed narrowing for deterministic operations, while
non-deterministic operations will depend on the graph transformation, bubbling.
Bubbling preserves the completeness of non-deterministic operations and avoids
unnecessary large-scale reconstruction of expressions done by other approaches."
"Mella is a minimalistic dependently typed programming language and
interactive theorem prover implemented in Haskell. Its main purpose is to
investigate the effective integration of automated theorem provers in a pure
and simple setting. Such integrations are essential for supporting program
development in dependently typed languages. We integrate the equational theorem
prover Waldmeister and test it on more than 800 proof goals from the TPTP
library. In contrast to previous approaches, the reconstruction of Waldmeister
proofs within Mella is quite robust and does not generate a significant
overhead to proof search. Mella thus yields a template for integrating more
expressive theorem provers in more sophisticated languages."
"We present Dependent JavaScript (DJS), a statically-typed dialect of the
imperative, object-oriented, dynamic language. DJS supports the particularly
challenging features such as run-time type-tests, higher-order functions,
extensible objects, prototype inheritance, and arrays through a combination of
nested refinement types, strong updates to the heap, and heap unrolling to
precisely track prototype hierarchies. With our implementation of DJS, we
demonstrate that the type system is expressive enough to reason about a variety
of tricky idioms found in small examples drawn from several sources, including
the popular book JavaScript: The Good Parts and the SunSpider benchmark suite."
"These are the revised versions of the papers presented at CICLOPS 2011, a
workshop colocated with ICLP 2011."
"Flow- and context-sensitive pointer analysis is generally considered too
expensive for large programs; most tools relax one or both of the requirements
for scalability. We formulate a flow- and context-sensitive points-to analysis
that is lazy in the following sense: points-to information is computed only for
live pointers and its propagation is sparse (restricted to live ranges of
respective pointers). Further, our analysis (a) uses strong liveness,
effectively including dead code elimination; (b) afterwards calculates
must-points-to information from may-points-to information instead of using a
mutual fixed-point; and (c) uses value-based termination of call strings during
interprocedural analysis (which reduces the number of call strings
significantly).
  A naive implementation of our analysis within GCC-4.6.0 gave analysis time
and size of points-to measurements for SPEC2006. Using liveness reduced the
amount of points-to information by an order of magnitude with no loss of
precision. For all programs under 30kLoC we found that the results were much
more precise than gcc's analysis. What comes as a pleasant surprise however, is
the fact that below this cross-over point, our naive linked-list implementation
is faster than a flow- and context-insensitive analysis which is primarily used
for efficiency. We speculate that lazy flow- and context-sensitive analyses may
be not only more precise, but also more efficient, than current approaches."
"Gecode is one of the most efficient libraries that can be used for constraint
solving. However, using it requires dealing with C++ programming details. On
the other hand several formats for representing constraint networks have been
proposed. Among them, XCSP has been proposed as a format based on XML which
allows us to represent constraints defined either extensionally or
intensionally, permits global constraints and has been the standard format of
the international competition of constraint satisfaction problems solvers. In
this paper we present a plug-in for solving problems specified in XCSP by
exploiting the Gecode solver. This is done by dynamically translating
constraints into Gecode library calls, thus avoiding the need to interact with
C++."
"Stack allocation and first-class functions don't naturally mix together. In
this paper we show that a type and effect system can be the detergent that
helps these features form a nice emulsion. Our interest in this problem comes
from our work on the Chapel language, but this problem is also relevant to
lambda expressions in C++ and blocks in Objective C. The difficulty in mixing
first-class functions and stack allocation is a tension between safety,
efficiency, and simplicity. To preserve safety, one must worry about functions
outliving the variables they reference: the classic upward funarg problem.
There are systems which regain safety but lose programmer-predictable
efficiency, and ones that provide both safety and efficiency, but give up
simplicity by exposing regions to the programmer. In this paper we present a
simple design that combines a type and effect system, for safety, with
function-local storage, for control over efficiency."
"This paper addresses the problem of specifying and parsing the syntax of
domain-specific languages (DSLs) in a modular, user-friendly way. That is, we
want to enable the design of composable DSLs that combine the natural syntax of
external DSLs with the easy implementation of internal DSLs. The challenge in
parsing composable DSLs is that the composition of several (individually
unambiguous) languages is likely to contain ambiguities. In this paper, we
present the design of a system that uses a type-oriented variant of island
parsing to efficiently parse the syntax of composable DSLs. In particular, we
show how type-oriented island parsing is constant time with respect to the
number of DSLs imported. We also show how to use our tool to implement DSLs on
top of a host language such as Typed Racket."
"C++0x is the working title for the revision of the ISO standard of the C++
programming language that was originally planned for release in 2009 but that
was delayed to 2011. The largest language extension in C++0x was ""concepts"",
that is, a collection of features for constraining template parameters. In
September of 2008, the C++ standards committee voted the concepts extension
into C++0x, but then in July of 2009, the committee voted the concepts
extension back out of C++0x.
  This article is my account of the technical challenges and debates within the
""concepts"" effort in the years 2003 to 2009. To provide some background, the
article also describes the design space for constrained parametric
polymorphism, or what is colloquially know as constrained generics. While this
article is meant to be generally accessible, the writing is aimed toward
readers with background in functional programming and programming language
theory. This article grew out of a lecture at the Spring School on Generic and
Indexed Programming at the University of Oxford, March 2010."
"We develop a behavioral theory for the untyped call-by-value lambda calculus
extended with the delimited-control operators shift and reset. For this
calculus, we discuss the possible observable behaviors and we define an
applicative bisimilarity that characterizes contextual equivalence. We then
compare the applicative bisimilarity and the CPS equivalence, a relation on
terms often used in studies of control operators. In the process, we illustrate
how bisimilarity can be used to prove equivalence of terms with
delimited-control effects."
"Expansion is an operation on typings (i.e., pairs of typing environments and
result types) defined originally in type systems for the lambda-calculus with
intersection types in order to obtain principal (i.e., most informative,
strongest) typings. In a type inference scenario, expansion allows postponing
choices for whether and how to use non-syntax-driven typing rules (e.g.,
intersection introduction) until enough information has been gathered to make
the right decision. Furthermore, these choices can be equivalent to inserting
uses of such typing rules at deeply nested positions in a typing derivation,
without needing to actually inspect or modify (or even have) the typing
derivation. Expansion has in recent years become simpler due to the use of
expansion variables (e.g., in System E).
  This paper extends expansion and expansion variables to systems with
forall-quantifiers. We present System Fs, an extension of System F with
expansion, and prove its main properties. This system turns type inference into
a constraint solving problem; this could be helpful to design a modular type
inference algorithm for System F types in the future."
"A lot of research work has been done in the area of Garbage collection for
both uniprocessor and distributed systems. Actors are associated with activity
(thread) and hence usual garbage collection algorithms cannot be applied for
them. Hence a separate algorithm should be used to collect them. If we
transform the active reference graph into a graph which captures all the
features of actors and looks like passive reference graph then any passive
reference graph algorithm can be applied for it. But the cost of transformation
and optimization are the core issues. An attempt has been made to walk through
these issues."
"The existing call-by-need lambda calculi describe lazy evaluation via
equational logics. A programmer can use these logics to safely ascertain
whether one term is behaviorally equivalent to another or to determine the
value of a lazy program. However, neither of the existing calculi models
evaluation in a way that matches lazy implementations.
  Both calculi suffer from the same two problems. First, the calculi never
discard function calls, even after they are completely resolved. Second, the
calculi include re-association axioms even though these axioms are merely
administrative steps with no counterpart in any implementation.
  In this paper, we present an alternative axiomatization of lazy evaluation
using a single axiom. It eliminates both the function call retention problem
and the extraneous re-association axioms. Our axiom uses a grammar of contexts
to describe the exact notion of a needed computation. Like its predecessors,
our new calculus satisfies consistency and standardization properties and is
thus suitable for reasoning about behavioral equivalence. In addition, we
establish a correspondence between our semantics and Launchbury's natural
semantics."
"We present a generalisation of King's symbolic execution technique called
compact symbolic execution. It proceeds in two steps. First, we analyse cyclic
paths in the control flow graph of a given program, independently from the rest
of the program. Our goal is to compute a so called template for each such a
cyclic path. A template is a declarative parametric description of all possible
program states, which may leave the analysed cyclic path after any number of
iterations along it. In the second step, we execute the program symbolically
with the templates in hand. The result is a compact symbolic execution tree. A
compact tree always carry the same information in all its leaves as the
corresponding classic symbolic execution tree. Nevertheless, a compact tree is
typically substantially smaller than the corresponding classic tree. There are
even programs for which compact symbolic execution trees are finite while
classic symbolic execution trees are infinite."
"We introduce a novel technique for finding real errors in programs. The
technique is based on a synergy of three well-known methods: metacompilation,
slicing, and symbolic execution. More precisely, we instrument a given program
with a code that tracks runs of state machines representing various kinds of
errors. Next we slice the program to reduce its size without affecting runs of
state machines. And then we symbolically execute the sliced program. Depending
on the kind of symbolic execution, the technique can be applied as a
stand-alone bug finding technique, or to weed out some false positives from an
output of another bug-finding tool. We provide several examples demonstrating
the practical applicability of our technique."
"Programming with dependent types is a blessing and a curse. It is a blessing
to be able to bake invariants into the definition of data-types: we can finally
write correct-by-construction software. However, this extreme accuracy is also
a curse: a data-type is the combination of a structuring medium together with a
special purpose logic. These domain-specific logics hamper any effort of code
reuse among similarly structured data.
  In this paper, we exorcise our data-types by adapting the notion of ornament
to our universe of inductive families. We then show how code reuse can be
achieved by ornamenting functions. Using these functional ornament, we capture
the relationship between functions such as the addition of natural numbers and
the concatenation of lists. With this knowledge, we demonstrate how the
implementation of the former informs the implementation of the latter: the user
can ask the definition of addition to be lifted to lists and she will only be
asked the details necessary to carry on adding lists rather than numbers.
  Our presentation is formalised in a type theory with a universe of data-types
and all our constructions have been implemented as generic programs, requiring
no extension to the type theory."
"Traversal strategies \'a la Stratego (also \'a la Strafunski and 'Scrap Your
Boilerplate') provide an exceptionally versatile and uniform means of querying
and transforming deeply nested and heterogeneously structured data including
terms in functional programming and rewriting, objects in OO programming, and
XML documents in XML programming. However, the resulting traversal programs are
prone to programming errors. We are specifically concerned with errors that go
beyond conservative type errors; examples we examine include divergent
traversals, prematurely terminated traversals, and traversals with dead code.
Based on an inventory of possible programming errors we explore options of
static typing and static analysis so that some categories of errors can be
avoided. This exploration generates suggestions for improvements to strategy
libraries as well as their underlying programming languages. Haskell is used
for illustrations and specifications with sufficient explanations to make the
presentation comprehensible to the non-specialist. The overall ideas are
language-agnostic and they are summarized accordingly."
"We develop a theory of contracting systems, where behavioural contracts may
be violated by dishonest participants after they have been agreed upon - unlike
in traditional approaches based on behavioural types. We consider the contracts
of \cite{CastagnaPadovaniGesbert09toplas}, and we embed them in a calculus that
allows distributed participants to advertise contracts, reach agreements, query
the fulfilment of contracts, and realise them (or choose not to).
  Our contract theory makes explicit who is culpable at each step of a
computation. A participant is honest in a given context S when she is not
culpable in each possible interaction with S. Our main result is a sufficient
criterion for classifying a participant as honest in all possible contexts."
"Developing an application with high performance through the code optimization
places a greater responsibility on the programmers. While most of the existing
compilers attempt to automatically optimize the program code, manual techniques
remain the predominant method for performing optimization. Deciding where to
try to optimize code is difficult, especially for large complex applications.
For manual optimization, the programmers can use his experiences in writing the
code, and then he can use a software profiler in order to collect and analyze
the performance data from the code. In this work, we have gathered the most
experiences which can be applied to improve the style of writing programs in C
language as well as we present an implementation of the manual optimization of
the codes using the Intel VTune profiler. The paper includes two case studies
to illustrate our optimization on the Heap Sort and Factorial functions."
"Global types are formal specifications that describe communication protocols
in terms of their global interactions. We present a new, streamlined language
of global types equipped with a trace-based semantics and whose features and
restrictions are semantically justified. The multi-party sessions obtained
projecting our global types enjoy a liveness property in addition to the
traditional progress and are shown to be sound and complete with respect to the
set of traces of the originating global type. Our notion of completeness is
less demanding than the classical ones, allowing a multi-party session to leave
out redundant traces from an underspecified global type. In addition to the
technical content, we discuss some limitations of our language of global types
and provide an extensive comparison with related specification languages
adopted in different communities."
"Region-based memory management (RBMM) is a form of compile time memory
management, well-known from the functional programming world. In this paper we
describe our work on implementing RBMM for the logic programming language
Mercury. One interesting point about Mercury is that it is designed with strong
type, mode, and determinism systems. These systems not only provide Mercury
programmers with several direct software engineering benefits, such as
self-documenting code and clear program logic, but also give language
implementors a large amount of information that is useful for program analyses.
In this work, we make use of this information to develop program analyses that
determine the distribution of data into regions and transform Mercury programs
by inserting into them the necessary region operations. We prove the
correctness of our program analyses and transformation. To execute the
annotated programs, we have implemented runtime support that tackles the two
main challenges posed by backtracking. First, backtracking can require regions
removed during forward execution to be ""resurrected""; and second, any memory
allocated during a computation that has been backtracked over must be recovered
promptly and without waiting for the regions involved to come to the end of
their life. We describe in detail our solution of both these problems. We study
in detail how our RBMM system performs on a selection of benchmark programs,
including some well-known difficult cases for RBMM. Even with these difficult
cases, our RBMM-enabled Mercury system obtains clearly faster runtimes for 15
out of 18 benchmarks compared to the base Mercury system with its Boehm runtime
garbage collector, with an average runtime speedup of 24%, and an average
reduction in memory requirements of 95%. In fact, our system achieves optimal
memory consumption in some programs."
"Eff is a programming language based on the algebraic approach to
computational effects, in which effects are viewed as algebraic operations and
effect handlers as homomorphisms from free algebras. Eff supports first-class
effects and handlers through which we may easily define new computational
effects, seamlessly combine existing ones, and handle them in novel ways. We
give a denotational semantics of eff and discuss a prototype implementation
based on it. Through examples we demonstrate how the standard effects are
treated in eff, and how eff supports programming techniques that use various
forms of delimited continuations, such as backtracking, breadth-first search,
selection functionals, cooperative multi-threading, and others."
"Interprocedural flow analysis can be used to eliminate otherwise unnecessary
heap allocated objects (unboxing), and in previous work we have shown how to do
so while maintaining correctness with respect to the garbage collector. In this
paper, we extend the notion of flow analysis to incorporate types, enabling
analysis and optimization of typed programs. We apply this typed analysis to
specify a type preserving interprocedural unboxing optimization, and prove that
the optimization preserves both type and GC safety along with program
semantics. We also show that the unboxing optimization can be applied
independently to separately compiled program modules, and prove via a
contextual equivalence result that unboxing a module in isolation preserves
program semantics."
"In first-year programming courses it is often difficult to show students how
an algorithm can be discovered. In this paper we present a program format that
supports the development from specification to code in small and obvious steps;
that is, a discovery process. The format, called Matrix Code, can be
interpreted as a proof according to the Floyd-Hoare program verification
method. The process consists of expressing the specification of a function body
as an initial code matrix and then growing the matrix by adding rows and
columns until the completed matrix is translated in a routine fashion to
compilable code. As worked example we develop a Java program that generates the
table of the first N prime numbers."
"Functional logic programming (FLP) languages use non-terminating and
non-confluent constructor systems (CS's) as programs in order to define
non-strict non-determi-nistic functions. Two semantic alternatives have been
usually considered for parameter passing with this kind of functions: call-time
choice and run-time choice. While the former is the standard choice of modern
FLP languages, the latter lacks some properties---mainly
compositionality---that have prevented its use in practical FLP systems.
Traditionally it has been considered that call-time choice induces a singular
denotational semantics, while run-time choice induces a plural semantics. We
have discovered that this latter identification is wrong when pattern matching
is involved, and thus we propose two novel compositional plural semantics for
CS's that are different from run-time choice.
  We study the basic properties of our plural semantics---compositionality,
polarity, monotonicity for substitutions, and a restricted form of the bubbling
property for constructor systems---and the relation between them and to
previous proposals, concluding that these semantics form a hierarchy in the
sense of set inclusion of the set of computed values. We have also identified a
class of programs characterized by a syntactic criterion for which the proposed
plural semantics behave the same, and a program transformation that can be used
to simulate one of them by term rewriting. At the practical level, we study how
to use the expressive capabilities of these semantics for improving the
declarative flavour of programs. We also propose a language which combines
call-time choice and our plural semantics, that we have implemented in Maude.
The resulting interpreter is employed to test several significant examples
showing the capabilities of the combined semantics.
  To appear in Theory and Practice of Logic Programming (TPLP)"
"Generic programming (GP) is an increasingly important trend in programming
languages. Well-known GP mechanisms, such as type classes and the C++0x
concepts proposal, usually combine two features: 1) a special type of
interfaces; and 2) implicit instantiation of implementations of those
interfaces.
  Scala implicits are a GP language mechanism, inspired by type classes, that
break with the tradition of coupling implicit instantiation with a special type
of interface. Instead, implicits provide only implicit instantiation, which is
generalized to work for any types. This turns out to be quite powerful and
useful to address many limitations that show up in other GP mechanisms.
  This paper synthesizes the key ideas of implicits formally in a minimal and
general core calculus called the implicit calculus, and it shows how to build
source languages supporting implicit instantiation on top of it. A novelty of
the calculus is its support for partial resolution and higher-order rules (a
feature that has been proposed before, but was never formalized or
implemented). Ultimately, the implicit calculus provides a formal model of
implicits, which can be used by language designers to study and inform
implementations of similar mechanisms in their own languages."
"The size-change abstraction (SCA) is an important program abstraction for
termination analysis, which has been successfully implemented in many tools for
functional and logic programs. In this paper, we demonstrate that SCA is also a
highly effective abstract domain for the bound analysis of imperative programs.
  We have implemented a bound analysis tool based on SCA for imperative
programs. We abstract programs in a pathwise and context dependent manner,
which enables our tool to analyze real-world programs effectively. Our work
shows that SCA captures many of the essential ideas of previous termination and
bound analysis and goes beyond in a conceptually simpler framework."
"Modern computer programming languages are governed by complex syntactic
rules. They are unlike natural languages; they require extensive manual work
and a significant amount of learning and practicing for an individual to become
skilled at and to write correct programs. Computer programming is a difficult,
complicated, unfamiliar, non-automated, and a challenging discipline for
everyone; especially, for students, new programmers and end-users. This paper
proposes a new programming language and an environment for writing computer
applications based on source-code generation. It is mainly a template-driven
automatic natural imperative programming language called MyProLang. It
harnesses GUI templates to generate proprietary natural language source-code,
instead of having computer programmers write the code manually. MyProLang is a
blend of five elements. A proprietary natural programming language with
unsophisticated grammatical rules and expressive syntax; automation templates
that automate the generation of instructions and thereby minimizing the
learning and training time; an NLG engine to generate natural instructions; a
source-to-source compiler that analyzes, parses, and build executables; and an
ergonomic IDE that houses diverse functions whose role is to simplify the
software development process. MyProLang is expected to make programming open to
everyone including students, programmers and end-users. In that sense, anyone
can start programming systematically, in an automated manner and in natural
language; without wasting time in learning how to formulate instructions and
arrange expressions, without putting up with unfamiliar structures and symbols,
and without being annoyed by syntax errors. In the long run, this increases the
productivity, quality and time-to-market in software development."
"In this article we develop a max-strategy improvement algorithm for computing
least fixpoints of operators on on the reals that are point-wise maxima of
finitely many monotone and order-concave operators. Computing the uniquely
determined least fixpoint of such operators is a problem that occurs frequently
in the context of numerical program/systems verification/analysis. As an
example for an application we discuss how our algorithm can be applied to
compute numerical invariants of programs by abstract interpretation based on
quadratic templates."
"We revisit known transformations from Jinja bytecode to rewrite systems from
the viewpoint of runtime complexity. Suitably generalising the constructions
proposed in the literature, we define an alternative representation of Jinja
bytecode (JBC) executions as ""computation graphs"" from which we obtain a novel
representation of JBC executions as ""constrained rewrite systems"". We prove
non-termination and complexity preservation of the transformation. We restrict
to well-formed JBC programs that only make use of non-recursive methods and
expect tree-shaped objects as input. Our approach allows for simplified
correctness proofs and provides a framework for the combination of the
computation graph method with standard techniques from static program analysis
like for example ""reachability analysis""."
"Exchanging mutable data objects with untrusted code is a delicate matter
because of the risk of creating a data space that is accessible by an attacker.
Consequently, secure programming guidelines for Java stress the importance of
using defensive copying before accepting or handing out references to an
internal mutable object. However, implementation of a copy method (like
clone()) is entirely left to the programmer. It may not provide a sufficiently
deep copy of an object and is subject to overriding by a malicious sub-class.
Currently no language-based mechanism supports secure object cloning. This
paper proposes a type-based annotation system for defining modular copy
policies for class-based object-oriented programs. A copy policy specifies the
maximally allowed sharing between an object and its clone. We present a static
enforcement mechanism that will guarantee that all classes fulfil their copy
policy, even in the presence of overriding of copy methods, and establish the
semantic correctness of the overall approach in Coq. The mechanism has been
implemented and experimentally evaluated on clone methods from several Java
libraries."
"CHR is a very versatile programming language that allows programmers to
declaratively specify constraint solvers. An important part of the development
of such solvers is in their testing and debugging phases. Current CHR
implementations support those phases by offering tracing facilities with
limited information. In this report, we propose a new trace for CHR which
contains enough information to analyze any aspects of \CHRv\ execution at some
useful abstract level, common to several implementations. %a large family of
rule based solvers. This approach is based on the idea of generic trace. Such a
trace is formally defined as an extension of the $\omega_r^\lor$ semantics of
CHR. We show that it can be derived form the SWI Prolog CHR trace."
"This paper provides an induction rule that can be used to prove properties of
data structures whose types are inductive, i.e., are carriers of initial
algebras of functors. Our results are semantic in nature and are inspired by
Hermida and Jacobs' elegant algebraic formulation of induction for polynomial
data types. Our contribution is to derive, under slightly different
assumptions, a sound induction rule that is generic over all inductive types,
polynomial or not. Our induction rule is generic over the kinds of properties
to be proved as well: like Hermida and Jacobs, we work in a general fibrational
setting and so can accommodate very general notions of properties on inductive
types rather than just those of a particular syntactic form. We establish the
soundness of our generic induction rule by reducing induction to iteration. We
then show how our generic induction rule can be instantiated to give induction
rules for the data types of rose trees, finite hereditary sets, and
hyperfunctions. The first of these lies outside the scope of Hermida and
Jacobs' work because it is not polynomial, and as far as we are aware, no
induction rules have been known to exist for the second and third in a general
fibrational framework. Our instantiation for hyperfunctions underscores the
value of working in the general fibrational setting since this data type cannot
be interpreted as a set."
"Recent mainstream programming languages such as Erlang or Scala have renewed
the interest on the Actor model of concurrency. However, the literature on the
static analysis of actor systems is still lacking of mature formal methods. In
this paper we present a minimal actor calculus that takes as primitive the
basic constructs of Scala's Actors API. More precisely, actors can send
asynchronous messages, process received messages according to a pattern
matching mechanism, and dynamically create new actors, whose scope can be
extruded by passing actor names as message parameters. Drawing inspiration from
the linear types and session type theories developed for process calculi, we
put forward a behavioural type system that addresses the key issues of an actor
calculus. We then study a safety property dealing with the determinism of
finite actor com- munication. More precisely, we show that well typed and
balanced actor systems are (i) deadlock-free and (ii) any message will
eventually be handled by the target actor, and dually no actor will
indefinitely wait for an expected message"
"A manual measuring time tool in mass sporting competitions would not be
imaginable nowadays, because many modern disciplines, such as IRONMAN, last a
long-time and, therefore, demand additional reliability. Moreover, automatic
timing-devices based on RFID technology, have become cheaper. However, these
devices cannot operate as stand-alone because they need a computer measuring
system that is capable of processing incoming events, encoding the results,
assigning them to the correct competitor, sorting the results according to the
achieved times, and then providing a printout of the results. This article
presents the domain-specific language EasyTime, which enables the controlling
of an agent by writing the events within a database. It focuses, in particular,
on the implementation of EasyTime with a LISA tool that enables the automatic
construction of compilers from language specifications, using Attribute
Grammars."
"In programming languages with dynamic use of memory, such as Java, knowing
that a reference variable x points to an acyclic data structure is valuable for
the analysis of termination and resource usage (e.g., execution time or memory
consumption). For instance, this information guarantees that the depth of the
data structure to which x points is greater than the depth of the data
structure pointed to by x.f for any field f of x. This, in turn, allows
bounding the number of iterations of a loop which traverses the structure by
its depth, which is essential in order to prove the termination or infer the
resource usage of the loop. The present paper provides an
Abstract-Interpretation-based formalization of a static analysis for inferring
acyclicity, which works on the reduced product of two abstract domains:
reachability, which models the property that the location pointed to by a
variable w can be reached by dereferencing another variable v (in this case, v
is said to reach w); and cyclicity, modeling the property that v can point to a
cyclic data structure. The analysis is proven to be sound and optimal with
respect to the chosen abstraction."
"A manually time-measuring tool in mass sporting competitions cannot be
imagined nowadays because many modern disciplines, such as IronMan, take a long
time and, therefore, demand additional reliability. Moreover, automatic timing
devices, based on RFID technology, have become cheaper. However, these devices
cannot operate stand-alone because they need a computer measuring system that
is capable of processing the incoming events, encoding the results, assigning
them to the correct competitor, sorting the results according to the achieved
times, and then providing a printout of the results. In this article, the
domain-specific language EasyTime is presented, which enables the controlling
of an agent by writing the events in a database. In particular, we are focused
on the implementation of EasyTime with a LISA tool that enables the automatic
construction of compilers from language specifications using Attribute
Grammars. By using of EasyTime, we can also decrease the number of measuring
devices. Furthermore, EasyTime is universal and can be applied to many
different sporting competitions in practice."
"We develop a static complexity analysis for a higher-order functional
language with structural list recursion. The complexity of an expression is a
pair consisting of a cost and a potential. The former is defined to be the size
of the expression's evaluation derivation in a standard big-step operational
semantics. The latter is a measure of the ""future"" cost of using the value of
that expression. A translation function tr maps target expressions to
complexities. Our main result is the following Soundness Theorem: If t is a
term in the target language, then the cost component of tr(t) is an upper bound
on the cost of evaluating t. The proof of the Soundness Theorem is formalized
in Coq, providing certified upper bounds on the cost of any expression in the
target language."
"Abstract interpretation techniques can be made more precise by distinguishing
paths inside loops, at the expense of possibly exponential complexity.
SMT-solving techniques and sparse representations of paths and sets of paths
avoid this pitfall. We improve previously proposed techniques for guided static
analysis and the generation of disjunctive invariants by combining them with
techniques for succinct representations of paths and symbolic representations
for transitions based on static single assignment. Because of the
non-monotonicity of the results of abstract interpretation with widening
operators, it is difficult to conclude that some abstraction is more precise
than another based on theoretical local precision results. We thus conducted
extensive comparisons between our new techniques and previous ones, on a
variety of open-source packages."
"Designing and implementing typed programming languages is hard. Every new
type system feature requires extending the metatheory and implementation, which
are often complicated and fragile. To ease this process, we would like to
provide general mechanisms that subsume many different features.
  In modern type systems, parametric polymorphism is fundamental, but
intersection polymorphism has gained little traction in programming languages.
Most practical intersection type systems have supported only refinement
intersections, which increase the expressiveness of types (more precise
properties can be checked) without altering the expressiveness of terms;
refinement intersections can simply be erased during compilation. In contrast,
unrestricted intersections increase the expressiveness of terms, and can be
used to encode diverse language features, promising an economy of both theory
and implementation.
  We describe a foundation for compiling unrestricted intersection and union
types: an elaboration type system that generates ordinary lambda-calculus
terms. The key feature is a Forsythe-like merge construct. With this construct,
not all reductions of the source program preserve types; however, we prove that
ordinary call-by-value evaluation of the elaborated program corresponds to a
type-preserving evaluation of the source program.
  We also describe a prototype implementation and applications of unrestricted
intersections and unions: records, operator overloading, and simulating dynamic
typing."
"Can we reuse some of the huge code-base developed in C to take advantage of
modern programming language features such as type safety, object-orientation,
and contracts? This paper presents a source-to-source translation of C code
into Eiffel, a modern object-oriented programming language, and the supporting
tool C2Eif. The translation is completely automatic and supports the entire C
language (ANSI, as well as many GNU C Compiler extensions, through CIL) as used
in practice, including its usage of native system libraries and inlined
assembly code. Our experiments show that C2Eif can handle C applications and
libraries of significant size (such as vim and libgsl), as well as challenging
benchmarks such as the GCC torture tests. The produced Eiffel code is
functionally equivalent to the original C code, and takes advantage of some of
Eiffel's object-oriented features to produce safe and easy-to-debug
translations."
"We prove that orthogonal constructor term rewrite systems and lambda-calculus
with weak (i.e., no reduction is allowed under the scope of a
lambda-abstraction) call-by-value reduction can simulate each other with a
linear overhead. In particular, weak call-by- value beta-reduction can be
simulated by an orthogonal constructor term rewrite system in the same number
of reduction steps. Conversely, each reduction in a term rewrite system can be
simulated by a constant number of beta-reduction steps. This is relevant to
implicit computational complexity, because the number of beta steps to normal
form is polynomially related to the actual cost (that is, as performed on a
Turing machine) of normalization, under weak call-by-value reduction.
Orthogonal constructor term rewrite systems and lambda-calculus are thus both
polynomially related to Turing machines, taking as notion of cost their natural
parameters."
"While methods of code abstraction and reuse are widespread and well
researched, methods of proof abstraction and reuse are still emerging. We
consider the use of dependent types for this purpose, introducing a completely
mechanical approach to proof composition. We show that common techniques for
abstracting algorithms over data structures naturally translate to abstractions
over proofs. We first introduce a language composed of a series of smaller
language components tied together by standard techniques from Malcom (1990). We
proceed by giving proofs of type preservation for each language component and
show that the basic ideas used in composing the syntactic data structures can
be applied to their semantics as well."
"Probabilistic transition system specifications (PTSSs) in the ntmufnu/ntmuxnu
format provide structural operational semantics for Segala-type systems that
exhibit both probabilistic and nondeterministic behavior and guarantee that
isimilarity is a congruence.Similar to the nondeterministic case of rule format
tyft/tyxt, we show that the well-foundedness requirement is unnecessary in the
probabilistic setting. To achieve this, we first define an extended version of
the ntmufnu/ntmuxnu format in which quantitative premises and conclusions
include nested convex combinations of distributions. This format also
guarantees that bisimilarity is a congruence. Then, for a given (possibly
non-well-founded) PTSS in the new format, we construct an equivalent
well-founded transition system consisting of only rules of the simpler
(well-founded) probabilistic ntree format. Furthermore, we develop a
proof-theoretic notion for these PTSSs that coincides with the existing
stratification-based meaning in case the PTSS is stratifiable. This continues
the line of research lifting structural operational semantic results from the
nondeterministic setting to systems with both probabilistic and
nondeterministic behavior."
"Quantum chromodynamics (QCD) is the theory of subnuclear physics, aiming at
mod- eling the strong nuclear force, which is responsible for the interactions
of nuclear particles. Lattice QCD (LQCD) is the corresponding discrete
formulation, widely used for simula- tions. The computational demand for the
LQCD is tremendous. It has played a role in the history of supercomputers, and
has also helped defining their future. Designing efficient LQCD codes that
scale well on large (probably hybrid) supercomputers requires to express many
levels of parallelism, and then to explore different algorithmic solutions.
While al- gorithmic exploration is the key for efficient parallel codes, the
process is hampered by the necessary coding effort. We present in this paper a
domain-specific language, QIRAL, for a high level expression of parallel
algorithms in LQCD. Parallelism is expressed through the mathematical struc-
ture of the sparse matrices defining the problem. We show that from these
expressions and from algorithmic and preconditioning formulations, a parallel
code can be automatically generated. This separates algorithms and mathematical
formulations for LQCD (that be- long to the field of physics) from the
effective orchestration of parallelism, mainly related to compilation and
optimization for parallel architectures."
"Measuring time in mass sports competitions is usually performed using
expensive measuring devices. Unfortunately, these solutions are not acceptable
by many organizers of sporting competitions. In order to make the measuring
time as cheap as possible, the domain-specific language (DSL) EasyTime was
proposed. In practice, it has been proven to be universal, flexible, and
efficient. It can even reduce the number of required measuring devices. On the
other hand, programming in EasyTime is not easy, because it requires a
domain-expert to program in a textual manner. In this paper, the
domain-specific modeling language (DSML) EasyTime II is proposed, which
simplifies the programming of the measuring system. First, the DSL EasyTime
domain analysis is presented. Then, the development of DSML is described in
detail. Finally, the DSML was tested by regular organizers of a sporting
competition. This test showed that DSML can be used by end-users without any
previous programming knowledge."
"Recently, data abstraction has been studied in the context of separation
logic, with noticeable practical successes: the developed logics have enabled
clean proofs of tricky challenging programs, such as subject-observer patterns,
and they have become the basis of efficient verification tools for Java
(jStar), C (VeriFast) and Hoare Type Theory (Ynot). In this paper, we give a
new semantic analysis of such logic-based approaches using Reynolds's
relational parametricity. The core of the analysis is our lifting theorems,
which give a sound and complete condition for when a true implication between
assertions in the standard interpretation entails that the same implication
holds in a relational interpretation. Using these theorems, we provide an
algorithm for identifying abstraction-respecting client-side proofs; the proofs
ensure that clients cannot distinguish two appropriately-related module
implementations."
"We present algorithms for checking and enforcing robustness of concurrent
programs against the Total Store Ordering (TSO) memory model. A program is
robust if all its TSO computations correspond to computations under the
Sequential Consistency (SC) semantics.
  We provide a complete characterization of non-robustness in terms of
so-called attacks: a restricted form of (harmful) out-of-program-order
executions. Then, we show that detecting attacks can be parallelized, and can
be solved using state reachability queries under SC semantics in a suitably
instrumented program obtained by a linear size source-to-source translation.
Importantly, the construction is valid for an arbitrary number of addresses and
an arbitrary number of parallel threads, and it is independent from the data
domain and from the size of store buffers in the TSO semantics. In particular,
when the data domain is finite and the number of addresses is fixed, we obtain
decidability and complexity results for robustness, even for an arbitrary
number of threads.
  As a second contribution, we provide an algorithm for computing an optimal
set of fences that enforce robustness. We consider two criteria of optimality:
minimization of program size and maximization of its performance. The
algorithms we define are implemented, and we successfully applied them to
analyzing and correcting several concurrent algorithms."
"This paper shows how a recently developed view of typing as small-step
abstract reduction, due to Kuan, MacQueen, and Findler, can be used to recast
the development of simple type theory from a rewriting perspective. We show how
standard meta-theoretic results can be proved in a completely new way, using
the rewriting view of simple typing. These meta-theoretic results include
standard type preservation and progress properties for simply typed lambda
calculus, as well as generalized versions where typing is taken to include both
abstract and concrete reduction. We show how automated analysis tools developed
in the term-rewriting community can be used to help automate the proofs for
this meta-theory. Finally, we show how to adapt a standard proof of
normalization of simply typed lambda calculus, for the rewriting approach to
typing."
"We propose a type system for a calculus of contracting processes. Processes
can establish sessions by stipulating contracts, and then can interact either
by keeping the promises made, or not. Type safety guarantees that a typeable
process is honest - that is, it abides by the contracts it has stipulated in
all possible contexts, even in presence of dishonest adversaries. Type
inference is decidable, and it allows to safely approximate the honesty of
processes using either synchronous or asynchronous communication."
"We address the design of distributed systems with synchronous dataflow
programming languages. As modular design entails handling both architectural
and functional modularity, our first contribution is to extend an existing
synchronous dataflow programming language with primitives allowing the
description of a distributed architecture and the localization of some
expressions onto some processors. We also present a distributed semantics to
formalize the distributed execution of synchronous programs. Our second
contribution is to provide a type system, in order to infer the localization of
non-annotated values by means of type inference and to ensure, at compilation
time, the consistency of the distribution. Our third contribution is to provide
a type-directed projection operation to obtain automatically,from a centralized
typed program, the local program to be executed by each computing resource. The
type system as well as the automatic distribution mechanism has been fully
implemented in the compiler of an existing synchronous data-flow programming
language."
"The technique of abstracting abstract machines (AAM) provides a systematic
approach for deriving computable approximations of evaluators that are easily
proved sound. This article contributes a complementary step-by-step process for
subsequently going from a naive analyzer derived under the AAM approach, to an
efficient and correct implementation. The end result of the process is a two to
three order-of-magnitude improvement over the systematically derived analyzer,
making it competitive with hand-optimized implementations that compute
fundamentally less precise results."
"We extend an off-the-shelf, executable formal semantics of C (Ellison and
Rosu's K Framework semantics) with the core features of CUDA-C. The hybrid
CPU/GPU computation model of CUDA-C presents challenges not just for
programmers, but also for practitioners of formal methods. Our formal semantics
helps expose and clarify these issues. We demonstrate the usefulness of our
semantics by generating a tool from it capable of detecting some race
conditions and deadlocks in CUDA-C programs. We discuss limitations of our
model and argue that its extensibility can easily enable a wider range of
verification tasks."
"Measuring time in mass sporting competitions is unthinkable manually today
because of their long duration and unreliability. Besides, automatic timing
devices based on the RFID technology have become cheaper. However, these
devices cannot operate stand-alone. To work efficiently, they need a computer
timing system for monitoring results. Such system should be capable of
processing the incoming events, encoding and assigning results to a individual
competitor, sorting results according to the achieved time and printing them.
In this paper, a domain-specific language named EasyTime will be defined. It
enables controlling an agent by writing events to a database. Using the agent,
the number of measuring devices can be reduced. Also, EasyTime is of a
universal type that can be applied to many different sporting competitions"
"Program transformation is an appealing technique which allows to improve
run-time efficiency, space-consumption, and more generally to optimize a given
program. Essentially, it consists of a sequence of syntactic program
manipulations which preserves some kind of semantic equivalence. Unfolding is
one of the basic operations which is used by most program transformation
systems and which consists in the replacement of a procedure call by its
definition. While there is a large body of literature on transformation and
unfolding of sequential programs, very few papers have addressed this issue for
concurrent languages.
  This paper defines an unfolding system for CHR programs. We define an
unfolding rule, show its correctness and discuss some conditions which can be
used to delete an unfolded rule while preserving the program meaning. We also
prove that, under some suitable conditions, confluence and termination are
preserved by the above transformation.
  To appear in Theory and Practice of Logic Programming (TPLP)"
"We present a Haskell library for first-order term rewriting covering basic
operations on positions, terms, contexts, substitutions and rewrite rules. This
effort is motivated by the increasing number of term rewriting tools that are
written in Haskell."
"Bounded linear types have proved to be useful for automated resource analysis
and control in functional programming languages. In this paper we introduce an
affine bounded linear typing discipline on a general notion of resource which
can be modeled in a semiring. For this type system we provide both a general
type-inference procedure, parameterized by the decision procedure of the
semiring equational theory, and a (coherent) categorical semantics. This is a
very useful type-theoretic and denotational framework for many applications to
resource-sensitive compilation, and it represents a generalization of several
existing type systems. As a non-trivial instance, motivated by our ongoing work
on hardware compilation, we present a complex new application to calculating
and controlling timing of execution in a (recursion-free) higher-order
functional programming language with local store."
"Speculative optimisation relies on the estimation of the probabilities that
certain properties of the control flow are fulfilled. Concrete or estimated
branch probabilities can be used for searching and constructing advantageous
speculative and bookkeeping transformations.
  We present a probabilistic extension of the classical equational approach to
data-flow analysis that can be used to this purpose. More precisely, we show
how the probabilistic information introduced in a control flow graph by branch
prediction can be used to extract a system of linear equations from a program
and present a method for calculating correct (numerical) solutions."
"Monotone frameworks is one of the most successful frameworks for
intraprocedural data flow analysis extending the traditional class of bitvector
frameworks (like live variables and available expressions). Weighted pushdown
systems is similarly one of the most general frameworks for interprocedural
analysis of programs. However, it makes use of idempotent semirings to
represent the sets of properties and unfortunately they do not admit analyses
whose transfer functions are not strict (e.g., classical bitvector frameworks).
This motivates the development of algorithms for backward and forward
reachability of pushdown systems using sets of properties forming so-called
flow algebras that weaken some of the assumptions of idempotent semirings. In
particular they do admit the bitvector frameworks, monotone frameworks, as well
as idempotent semirings. We show that the algorithms are sound under mild
assumptions on the flow algebras, mainly that the set of properties constitutes
a join semi-lattice, and complete provided that the transfer functions are
suitably distributive (but not necessarily strict)."
"In this study, we have investigated the adequacy of the PGAS parallel
language X10 to implement a Constraint-Based Local Search solver. We decided to
code in this language to benefit from the ease of use and architectural
independence from parallel resources which it offers. We present the
implementation strategy, in search of different sources of parallelism in the
context of an implementation of the Adaptive Search algorithm. We extensively
discuss the algorithm and its implementation. The performance evaluation on a
representative set of benchmarks shows close to linear speed-ups, in all the
problems treated."
"The tree based representation described in this paper, hereditarily binary
numbers, applies recursively a run-length compression mechanism that enables
computations limited by the structural complexity of their operands rather than
by their bitsizes. While within constant factors from their traditional
counterparts for their worst case behavior, our arithmetic operations open the
doors for interesting numerical computations, impossible with traditional
number representations. We provide a complete specification of our algorithms
in the form of a purely declarative Prolog program."
"This paper presents an implementation of interning of ground terms in the XSB
Tabled Prolog system. This is related to the idea of hash-consing. I describe
the concept of interning atoms and discuss the issues around interning ground
structured terms, motivating why tabling Prolog systems may change the
cost-benefit tradeoffs from those of traditional Prolog systems. I describe the
details of the implementation of interning ground terms in the XSB Tabled
Prolog System and show some of its performance properties. This implementation
achieves the effects of that of Zhou and Have but is tuned for XSB's
representations and is arguably simpler."
"We present a system, called CASS, for the analysis of functional logic
programs. The system is generic so that various kinds of analyses (e.g.,
groundness, non-determinism, demanded arguments) can be easily integrated. In
order to analyze larger applications consisting of dozens or hundreds of
modules, CASS supports a modular and incremental analysis of programs.
Moreover, it can be used by different programming tools, like documentation
generators, analysis environments, program optimizers, as well as Eclipse-based
development environments. For this purpose, CASS can also be invoked as a
server system to get a language-independent access to its functionality. CASS
is completely implemented in the functional logic language Curry as a
master/worker architecture to exploit parallel or distributed execution
environments."
"We propose in this article a framework for compilation of quantified
constraint satisfaction problems (QCSP). We establish the semantics of this
formalism by an interpretation to a QCSP. We specify an algorithm to compile a
QCSP embedded into a search algorithm and based on the inductive semantics of
QCSP. We introduce an optimality property and demonstrate the optimality of the
interpretation of the compiled QCSP."
"How can one recognize coordination languages and technologies? As this report
shows, the common approach that contrasts coordination with computation is
intellectually unsound: depending on the selected understanding of the word
""computation"", it either captures too many or too few programming languages.
Instead, we argue for objective criteria that can be used to evaluate how well
programming technologies offer coordination services. Of the various criteria
commonly used in this community, we are able to isolate three that are strongly
characterizing: black-box componentization, which we had identified previously,
but also interface extensibility and customizability of run-time optimization
goals. These criteria are well matched by Intel's Concurrent Collections and
AstraKahn, and also by OpenCL, POSIX and VMWare ESX."
"In previous work we describe a novel approach to dependent typing based on a
multivalued term language. In this technical report we formalise the runtime, a
kind of operational semantics, for that language. We describe a fairly
comprehensive core language, and then give a small-step operational semantics
based on an abstract machine. Errors are explicit in the semantics. We also
prove several simple properties: that every non-terminated machine state steps
to something and that reduction is deterministic once input is fixed."
"Behavioral software contracts are a widely used mechanism for governing the
flow of values between components. However, run-time monitoring and enforcement
of contracts imposes significant overhead and delays discovery of faulty
components to run-time.
  To overcome these issues, we present soft contract verification, which aims
to statically prove either complete or partial contract correctness of
components, written in an untyped, higher-order language with first-class
contracts. Our approach uses higher-order symbolic execution, leveraging
contracts as a source of symbolic values including unknown behavioral values,
and employs an updatable heap of contract invariants to reason about
flow-sensitive facts. We prove the symbolic execution soundly approximates the
dynamic semantics and that verified programs can't be blamed.
  The approach is able to analyze first-class contracts, recursive data
structures, unknown functions, and control-flow-sensitive refinements of
values, which are all idiomatic in dynamic languages. It makes effective use of
an off-the-shelf solver to decide problems without heavy encodings. The
approach is competitive with a wide range of existing tools---including type
systems, flow analyzers, and model checkers---on their own benchmarks."
"We investigate the expressive power of spreadsheets. We consider spreadsheets
which contain only formulas, and assume that they are small templates, which
can be filled to a larger area of the grid to process input data of variable
size. Therefore we can compare them to well-known machine models of
computation. We consider a number of classes of spreadsheets defined by
restrictions on their reference structure. Two of the classes correspond
closely to parallel complexity classes: we prove a direct correspondence
between the dimensions of the spreadsheet and amount of hardware and time used
by a parallel computer to compute the same function. As a tool, we produce
spreadsheets which are universal in these classes, i.e. can emulate any other
spreadsheet from them. In other cases we implement in the spreadsheets in
question instances of a polynomial-time complete problem, which indicates that
the the spreadsheets are unlikely to have efficient parallel evaluation
algorithms. Thus we get a picture how the computational power of spreadsheets
depends on their dimensions and structure of references."
"We present a formal framework for repairing infinite-state, imperative,
sequential programs, with (possibly recursive) procedures and multiple
assertions; the framework can generate repaired programs by modifying the
original erroneous program in multiple program locations, and can ensure the
readability of the repaired program using user-defined expression templates;
the framework also generates a set of inductive assertions that serve as a
proof of correctness of the repaired program. As a step toward integrating
programmer intent and intuition in automated program repair, we present a ""
cost-aware"" formulation - given a cost function associated with permissible
statement modifications, the goal is to ensure that the total program
modification cost does not exceed a given repair budget. As part of our
predicate abstraction-based solution framework, we present a sound and complete
algorithm for repair of Boolean programs. We have developed a prototype tool
based on SMT solving and used it successfully to repair diverse errors in
benchmark C programs."
"In functional programming languages, the classic form of annotation is a
single type constraint on a term. Intersection types add complications: a
single term may have to be checked several times against different types, in
different contexts, requiring annotation with several types. Moreover, it is
useful (in some systems, necessary) to indicate the context in which each such
type is to be used.
  This paper explores the technical design space of annotations in systems with
intersection types. Earlier work (Dunfield and Pfenning 2004) introduced
contextual typing annotations, which we now tease apart into more elementary
mechanisms: a ""right hand"" annotation (the standard form), a ""left hand""
annotation (the context in which a right-hand annotation is to be used), a
merge that allows for multiple annotations, and an existential binder for index
variables. The most novel element is the left-hand annotation, which guards
terms (and right-hand annotations) with a judgment that must follow from the
current context."
"This volume contains the proceedings of the 13th International Colloquium on
Implementation of Constraint and LOgic Programming Systems (CICLOPS 2013), held
in Istanbul, Turkey during August 25, 2013. CICLOPS is a well established line
of workshops, traditionally co-located with ICLP, that aims at discussing and
exchanging experience on the design, implementation, and optimization of
constraint and logic programming systems, and other systems based on logic as a
means of expressing computations. This year, CICLOPS received 8 paper
submissions. Each submission was reviewed by at least 3 Program Committee
members and, at the end, 6 papers were accepted for presentation at the
workshop. We would like to thank the ICLP organizers for their support, the
EasyChair conference management system for making the life of the program
chairs easier and arxiv.org for providing permanent hosting. Thanks should go
also to the authors of all submitted papers for their contribution to make
CICLOPS alive and to the participants for making the event a meeting point for
a fruitful exchange of ideas and feedback on recent developments. Finally, we
want to express our gratitude to the Program Committee members, as the
symposium would not have been possible without their dedicated work."
"Tabling in logic programming has been used to eliminate redundant computation
and also to stop infinite loop. In this paper we investigate another
possibility of tabling, i.e. to compute an infinite sum of probabilities for
probabilistic logic programs. Using PRISM, a logic-based probabilistic modeling
language with a tabling mechanism, we generalize prefix probability computation
for probabilistic context free grammars (PCFGs) to probabilistic logic
programs. Given a top-goal, we search for all proofs with tabling and obtain an
explanation graph which compresses them and may be cyclic. We then convert the
explanation graph to a set of linear probability equations and solve them by
matrix operation. The solution gives us the probability of the top-goal, which,
in nature, is an infinite sum of probabilities. Our general approach to prefix
probability computation through tabling not only allows to deal with non-PCFGs
such as probabilistic left-corner grammars (PLCGs) but has applications such as
plan recognition and probabilistic model checking and makes it possible to
compute probability for probabilistic models describing cyclic relations. To
appear in Theory and Practice of Logic Programming (TPLP)."
"Selection statements -- if-then-else, switch and try-catch -- are commonly
used in modern imperative programming languages. We propose another selection
statement called a {\it choice existentially quantified statement}. This
statement turns out to be quite useful for pattern matching among several
merits. Examples will be provided for this statement."
"This paper presents a review of some new futures introduced to C++ language
by ISO/IEC 14882:2011 standard (known as C++11). It describes the ideas of
r-values and move constructors.
  ----
  Niniejszy artyku{\l} jest jednym z serii artyku{\l}\'ow w kt\'orych zawarto
przegl{\ka}d nowych element\'ow j{\ke}zyka C++ wprowadzonych przez standard
ISO/IEC 14882:2011, znany pod nazw{\ka} C++11. W artykule przedstawiono nowe
mo\.zliwo\'sci zwi{\ka}zane z przekazywaniem parametr\'ow i pisaniem
konstruktor\'ow. Zawarto w nim dok{\l}adne om\'owienie idei r-warto\'sci i
przenoszenia obiekt\'ow."
"NOOP is a mathematical model of nominally-typed OOP that proves the
identification of inheritance and subtyping in mainstream nominally-typed OO
programming languages and the validity of this identification. This report
gives an overview of the main notions in OOP relevant to constructing a
mathematical model of OOP such as NOOP. The emphasis in this report is on
defining nominality, nominal typing and nominal subtyping of mainstream
nominally-typed OO languages, and on contrasting the three notions with their
counterparts in structurally-typed OO languages, i.e., with structurality,
structural typing and structural subtyping, respectively. An additional
appendix demonstrates these notions and other related notions, and the
differences between them, using some simple code examples. A detailed, more
technical comparison between nominal typing and structural typing in OOP is
presented in other publications."
"Writing accurate numerical software is hard because of many sources of
unavoidable uncertainties, including finite numerical precision of
implementations. We present a programming model where the user writes a program
in a real-valued implementation and specification language that explicitly
includes different types of uncertainties. We then present a compilation
algorithm that generates a conventional implementation that is guaranteed to
meet the desired precision with respect to real numbers. Our verification step
generates verification conditions that treat different uncertainties in a
unified way and encode reasoning about floating-point roundoff errors into
reasoning about real numbers. Such verification conditions can be used as a
standardized format for verifying the precision and the correctness of
numerical programs. Due to their often non-linear nature, precise reasoning
about such verification conditions remains difficult. We show that current
state-of-the art SMT solvers do not scale well to solving such verification
conditions. We propose a new procedure that combines exact SMT solving over
reals with approximate and sound affine and interval arithmetic. We show that
this approach overcomes scalability limitations of SMT solvers while providing
improved precision over affine and interval arithmetic. Using our initial
implementation we show the usefullness and effectiveness of our approach on
several examples, including those containing non-linear computation."
"In this preliminary note, we will illustrate our ideas on automated
mechanisms for termination and non-termination reasoning."
"Modern development environments handle information about the intent of the
programmer: for example, they use abstract syntax trees for providing
high-level code manipulation such as refactorings; nevertheless, they do not
keep track of this information in a way that would simplify code sharing and
change understanding. In most Smalltalk systems, source code modifications are
immediately registered in a transaction log often called a ChangeSet. Such
mechanism has proven reliability, but it has several limitations. In this paper
we analyse such limitations and describe scenarios and requirements for
tracking fine-grained code history with a semantic representation. We present
Epicea, an early prototype implementation. We want to enrich code sharing with
extra information from the IDE, which will help understanding the intention of
the changes and let a new generation of tools act in consequence."
"In this paper we provide a survey on the framework of abstract
non-interference. In particular, we describe a general formalization of
abstract non-interference by means of three dimensions (observation, protection
and semantics) that can be instantiated in order to obtain well known or even
new weakened non-interference properties. Then, we show that the notions of
abstract non-interference introduced in language-based security are instances
of this more general framework which allows to better understand the different
components of a non-interference policy. Finally, we consider two challenging
research fields concerning security where abstract non-interference seems a
promising approach providing new perspectives and new solutions to open
problems: Code injection and code obfuscation."
"Over the past two decades the notion of a strong monad has found wide
applicability in computing. Arising out of a need to interpret products in
computational and semantic settings, different approaches to this concept have
arisen. In this paper we introduce and investigate the connections between
these approaches and also relate the results to monad composition. We also
introduce new methods for checking and using the required laws associated with
such compositions, as well as provide examples illustrating problems and issues
that arise."
"In David Schmidt's PhD work he explored the use of denotational semantics as
a programming language. It was part of an effort to not only treat formal
semantics as specifications but also as interpreters and input to compiler
generators. The semantics itself can be seen as a program and one may examine
different programming styles and ways to represent states.
  Abstract interpretation is primarily a technique for derivation and
specification of program analysis. As with denotational semantics we may also
view abstract interpretations as programs and examine the implementation. The
main focus in this paper is to show that results from higher-order strictness
analysis may be used more generally as fixpoint operators for higher-order
functions over lattices and thus provide a technique for immediate
implementation of a large class of abstract interpretations. Furthermore, it
may be seen as a programming paradigm and be used to write programs in a
circular style."
"The Java virtual machine and the .NET common language runtime feature an
access control mechanism specified operationally in terms of run-time stack
inspection. We give a denotational semantics in ""eager"" form, and show that it
is equivalent to the ""lazy"" semantics using stack inspection. We give a static
analysis of safety, i.e., the absence of security errors, that is simpler than
previous proposals. We identify several program transformations that can be
used to remove run-time checks. We give complete, detailed proofs for safety of
the analysis and for the transformations, exploiting compositionality of the
eager semantics."
"This short note is written in celebration of David Schmidt's sixtieth
birthday. He has now been active in the program analysis research community for
over thirty years and we have enjoyed many interactions with him. His work on
characterising simulations between Kripke structures using Galois connections
was particularly influential in our own work on using probabilistic abstract
interpretation to study Larsen and Skou's notion of probabilistic bisimulation.
We briefly review this work and discuss some recent applications of these ideas
in a variety of different application areas."
"Programming for Mobile and Touch (PRoMoTo'13) was held at the 2013 ACM
SIGPLAN conference on Systems, Programming, Languages and Applications (SPLASH
2013), October 2013 in Indianapolis, USA. Submissions for this event were
invited in the general area of mobile and touch-oriented programming languages
and programming environments, and teaching of programming for mobile devices.
These are proceedings of the PRoMoTo'13."
"Increasing sharing in programs is desirable to compactify the code, and to
avoid duplication of reduction work at run-time, thereby speeding up execution.
We show how a maximal degree of sharing can be obtained for programs expressed
as terms in the lambda calculus with letrec. We introduce a notion of `maximal
compactness' for lambda-letrec-terms among all terms with the same infinite
unfolding. Instead of defined purely syntactically, this notion is based on a
graph semantics. lambda-letrec-terms are interpreted as first-order term graphs
so that unfolding equivalence between terms is preserved and reflected through
bisimilarity of the term graph interpretations. Compactness of the term graphs
can then be compared via functional bisimulation.
  We describe practical and efficient methods for the following two problems:
transforming a lambda-letrec-term into a maximally compact form; and deciding
whether two lambda-letrec-terms are unfolding-equivalent. The transformation of
a lambda-letrec-term $L$ into maximally compact form $L_0$ proceeds in three
steps:
  (i) translate L into its term graph $G = [[ L ]]$; (ii) compute the maximally
shared form of $G$ as its bisimulation collapse $G_0$; (iii) read back a
lambda-letrec-term $L_0$ from the term graph $G_0$ with the property $[[ L_0 ]]
= G_0$. This guarantees that $L_0$ and $L$ have the same unfolding, and that
$L_0$ exhibits maximal sharing.
  The procedure for deciding whether two given lambda-letrec-terms $L_1$ and
$L_2$ are unfolding-equivalent computes their term graph interpretations $[[
L_1 ]]$ and $[[ L_2 ]]$, and checks whether these term graphs are bisimilar.
  For illustration, we also provide a readily usable implementation."
"We present a new abstract machine, called DCESH, which describes the
execution of higher-order programs running in distributed architectures. DCESH
implements a generalised form of Remote Procedure Call that supports calling
higher-order functions across node boundaries, without sending actual code. Our
starting point is a variant of the SECD machine that we call the CES machine,
which implements reduction for untyped call-by-value PCF. We successively add
the features that we need for distributed execution and show the correctness of
each addition. First we add heaps, forming the CESH machine, which provides
features necessary for more efficient execution, and show that there is a
bisimulation between the CES and the CESH machine. Then we construct a
two-level operational semantics, where the high level is a network of
communicating machines, and the low level is given by local machine
transitions. Using these networks, we arrive at our final system, the
distributed CESH machine (DCESH). We show that there is a bisimulation relation
also between the CESH machine and the DCESH machine. All the technical results
have been formalised and proved correct in Agda, and a prototype compiler has
been developed."
"We introduce a fully automated static analysis that takes a sequential Java
bytecode program P as input and attempts to prove that there exists an infinite
execution of P. The technique consists in compiling P into a constraint logic
program P_CLP and in proving non-termination of P_CLP; when P consists of
instructions that are exactly compiled into constraints, the non-termination of
P_CLP entails that of P. Our approach can handle method calls; to the best of
our knowledge, it is the first static approach for Java bytecode able to prove
the existence of infinite recursions. We have implemented our technique inside
the Julia analyser. We have compared the results of Julia on a set of 113
programs with those provided by AProVE and Invel, the only freely usable
non-termination analysers comparable to ours that we are aware of. Only Julia
could detect non-termination due to infinite recursion."
"Wadler and Thiemann unified type-and-effect systems with monadic semantics
via a syntactic correspondence and soundness results with respect to an
operational semantics. They conjecture that a general, ""coherent"" denotational
semantics can be given to unify effect systems with a monadic-style semantics.
We provide such a semantics based on the novel structure of an indexed monad,
which we introduce. We redefine the semantics of Moggi's computational
lambda-calculus in terms of (strong) indexed monads which gives a one-to-one
correspondence between indices of the denotations and the effect annotations of
traditional effect systems. Dually, this approach yields indexed comonads which
gives a unified semantics and effect system to contextual notions of effect
(called coeffects), which we have previously described."
"SMT-based verifiers have long been an effective means of ensuring safety
properties of programs. While these techniques are well understood, we show
that they implicitly require eager semantics; directly applying them to a lazy
language is unsound due to the presence of divergent sub-computations. We
recover soundness by composing the safety analysis with a termination analysis.
Of course, termination is itself a challenging problem, but we show how the
safety analysis can be used to ensure termination, thereby bootstrapping
soundness for the entire system. Thus, while safety invariants have long been
required to prove termination, we show how termination proofs can be to soundly
establish safety. We have implemented our approach in liquidHaskell, a
Refinement Type-based verifier for Haskell. We demonstrate its effectiveness
via an experimental evaluation using liquidHaskell to verify safety, functional
correctness and termination properties of real-world Haskell libraries,
totaling over 10,000 lines of code."
"Algebraic specifications of data types provide a natural basis for testing
data types implementations. In this framework, the conformance relation is
based on the satisfaction of axioms. This makes it possible to formally state
the fundamental concepts of testing: exhaustive test set, testability
hypotheses, oracle. Various criteria for selecting finite test sets have been
proposed. They depend on the form of the axioms, and on the possibilities of
observation of the implementation under test. This last point is related to the
well-known oracle problem. As the main interest of algebraic specifications is
data type abstraction, testing a concrete implementation raises the issue of
the gap between the abstract description and the concrete representation. The
observational semantics of algebraic specifications bring solutions on the
basis of the so-called observable contexts. After a description of testing
methods based on algebraic specifications, the chapter gives a brief
presentation of some tools and case studies, and presents some applications to
other formal methods involving datatypes."
"Quantum computer programming is emerging as a new subject domain from
multidisciplinary research in quantum computing, computer science, mathematics
(especially quantum logic, lambda calculi, and linear logic), and engineering
attempts to build the first non-trivial quantum computer. This paper briefly
surveys the history, methods, and proposed tools for programming quantum
computers circa late 2007. It is intended to provide an extensive but
non-exhaustive look at work leading up to the current state-of-the-art in
quantum computer programming. Further, it is an attempt to analyze the needed
programming tools for quantum programmers, to use this analysis to predict the
direction in which the field is moving, and to make recommendations for further
development of quantum programming language tools."
"Object-oriented programming (OOP) is aimed at describing the structure and
behaviour of objects by hiding the mechanism of their representation and access
in primitive references. In this article we describe an approach, called
concept-oriented programming (COP), which focuses on modelling references
assuming that they also possess application-specific structure and behaviour
accounting for a great deal or even most of the overall program complexity.
References in COP are completely legalized and get the same status as objects
while the functions are distributed among both objects and references. In order
to support this design we introduce a new programming construct, called
concept, which generalizes conventional classes and concept inclusion relation
generalizing class inheritance. The main advantage of COP is that it allows
programmers to describe two sides of any program: explicitly used functions of
objects and intermediate functionality of references having cross-cutting
nature and executed implicitly behind the scenes during object access."
"REC (Regular Expression Compiler) is a concise programming language
development in mayor Mexican Universities at end of 60s which allows students
to write programs without knowledge of the complicated syntax of languages like
FORTRAN and ALGOL. The language is recursive and contains only four elements
for control. This paper describes use of the interpreter of REC written in
FORTRAN on IBM1130 Simulator from -Computer History Simulation- Project."
"The # component model was proposed to improve the practice of parallel
programming. This paper introduces a type system for # programming systems,
aiming to lift the abstraction and safety of programming for parallel computing
architectures by introducing a notion of abstract component based on universal
and existential bounded quantification. Issues about the implementation of such
type system in HPE, a # programming system, are also discussed."
"This paper concerns instruction sequences that contain probabilistic
instructions, i.e. instructions that are themselves probabilistic by nature. We
propose several kinds of probabilistic instructions, provide an informal
operational meaning for each of them, and discuss related work. On purpose, we
refrain from providing an ad hoc formal meaning for the proposed kinds of
instructions. We also discuss the approach of projection semantics, which was
introduced in earlier work on instruction sequences, in the light of
probabilistic instruction sequences."
"The General Intensional Programming System (GIPSY) has been built around the
Lucid family of intensional programming languages that rely on the higher-order
intensional logic (HOIL) to provide context-oriented multidimensional reasoning
of intensional expressions. HOIL combines functional programming with various
intensional logics to allow explicit context expressions to be evaluated as
first-class values that can be passed as parameters to functions and return as
results with an appropriate set of operators defined on contexts. GIPSY's
frameworks are implemented in Java as a collection of replaceable components
for the compilers of various Lucid dialects and the demand-driven eductive
evaluation engine that can run distributively. GIPSY provides support for
hybrid programming models that couple intensional and imperative languages for
a variety of needs. Explicit context expressions limit the scope of evaluation
of math expressions (effectively a Lucid program is a mathematics or physics
expression constrained by the context) in tensor physics, regular math in
multiple dimensions, etc., and for cyberforensic reasoning as one of the
use-cases of interest. Thus, GIPSY is a support testbed for HOIL-based
languages some of which enable such reasoning, as in formal cyberforensic case
analysis with event reconstruction. In this paper we discuss the GIPSY
architecture, its evaluation engine and example use-cases."
"Constraint Handling Rules (CHR) is a high-level programming language based on
multi-headed multiset rewrite rules. Originally designed for writing
user-defined constraint solvers, it is now recognized as an elegant general
purpose language. CHR-related research has surged during the decade following
the previous survey by Fruehwirth. Covering more than 180 publications, this
new survey provides an overview of recent results in a wide range of research
areas, from semantics and analysis to systems, extensions and applications."
"We describe a process calculus featuring high level constructs for
component-oriented programming in a distributed setting. We propose an
extension of the higher-order pi-calculus intended to capture several important
mechanisms related to component-based programming, such as dynamic update,
reconfiguration and code migration. In this paper, we are primarily concerned
with the possibility to build a distributed implementation of our calculus.
Accordingly, we define a low-level calculus, that describes how the high-level
constructs are implemented, as well as details of the data structures
manipulated at runtime. We also discuss current and future directions of
research in relation to our analysis of component-based programming."
"Programming languages are expected to support programmer's effort to
structure program code. The ML module system, object systems and mixins are
good examples of language constructs promoting modular programming. Among the
three, mixins can be thought of as a generalization of the two others in the
sense that mixins can incorporate features of ML modules and objects with a set
of primitive operators with clean semantics. Much work has been devoted to
build mixin-based module systems for practical programming languages. In
respect of the operational semantics, previous work notably investigated mixin
calculi in call-by-name and call-by-value evaluation settings. In this paper we
examine a mixin calculus in a call-by-need, or lazy, evaluation setting. We
demonstrate how lazy mixins can be interesting in practice with a series of
examples, and formalize the operational semantics by adapting Ancona and
Zucca's concise formalization of call-by-name mixins. We then extend the
semantics with constraints to control the evaluation order of components of
mixins in several ways. The main motivation for considering the constraints is
to produce side effects in a more explicit order than in a purely lazy,
demand-driven setting. We explore the design space of possibly interesting
constraints and consider two examples in detail."
"Often, when modelling a system there are properties and operations that are
related to a group of objects rather than to a single object. In this paper we
extend Java with Swarm Behavior, a new composition operator that associates
behavior with a collection of instances. The lookup resolution of swarm
behavior is based on the element type of a collection and is thus orthogonal to
the collection hierarchy."
"Lecture notes for the Comparative Studies of Programming Languages course,
COMP6411, taught at the Department of Computer Science and Software
Engineering, Faculty of Engineering and Computer Science, Concordia University,
Montreal, QC, Canada. These notes include a compiled book of primarily related
articles from the Wikipedia, the Free Encyclopedia, as well as Comparative
Programming Languages book and other resources, including our own. The original
notes were compiled by Dr. Paquet."
"The idea of functional programming has played a big role in shaping today's
landscape of mainstream programming languages. Another concept that dominates
the current programming style is Dijkstra's structured programming. Both
concepts have been successfully married, for example in the programming
language Scala. This paper proposes how the same can be achieved for structured
programming and PURELY functional programming via the notion of LINEAR SCOPE.
One advantage of this proposal is that mainstream programmers can reap the
benefits of purely functional programming like easily exploitable parallelism
while using familiar structured programming syntax and without knowing concepts
like monads. A second advantage is that professional purely functional
programmers can often avoid hard to read functional code by using structured
programming syntax that is often easier to parse mentally."
"Sun and the CERT recommend for secure Java development to not allow partially
initialized objects to be accessed. The CERT considers the severity of the
risks taken by not following this recommendation as high. The solution
currently used to enforce object initialization is to implement a coding
pattern proposed by Sun, which is not formally checked. We propose a modular
type system to formally specify the initialization policy of libraries or
programs and a type checker to statically check at load time that all loaded
classes respect the policy. This allows to prove the absence of bugs which have
allowed some famous privilege escalations in Java. Our experimental results
show that our safe default policy allows to prove 91% of classes of java.lang,
java.security and javax.security safe without any annotation and by adding 57
simple annotations we proved all classes but four safe. The type system and its
soundness theorem have been formalized and machine checked using Coq."
"We present a non-null annotations inferencer for the Java bytecode language.
We previously proposed an analysis to infer non-null annotations and proved it
soundness and completeness with respect to a state of the art type system. This
paper proposes extensions to our former analysis in order to deal with the Java
bytecode language. We have implemented both analyses and compared their
behaviour on several benchmarks. The results show a substantial improvement in
the precision and, despite being a whole-program analysis, production
applications can be analyzed within minutes."
"Although in most cases class initialization works as expected, some static
fields may be read before being initialized, despite being initialized in their
corresponding class initializer. We propose an analysis which compute, for each
program point, the set of static fields that must have been initialized and
discuss its soundness. We show that such an analysis can be directly applied to
identify the static fields that may be read before being initialized and to
improve the precision while preserving the soundness of a null-pointer
analysis."
"State of the art analyzers in the Logic Programming (LP) paradigm are
nowadays mature and sophisticated. They allow inferring a wide variety of
global properties including termination, bounds on resource consumption, etc.
The aim of this work is to automatically transfer the power of such analysis
tools for LP to the analysis and verification of Java bytecode (JVML). In order
to achieve our goal, we rely on well-known techniques for meta-programming and
program specialization. More precisely, we propose to partially evaluate a JVML
interpreter implemented in LP together with (an LP representation of) a JVML
program and then analyze the residual program. Interestingly, at least for the
examples we have studied, our approach produces very simple LP representations
of the original JVML programs. This can be seen as a decompilation from JVML to
high-level LP source. By reasoning about such residual programs, we can
automatically prove in the CiaoPP system some non-trivial properties of JVML
programs such as termination, run-time error freeness and infer bounds on its
resource consumption. We are not aware of any other system which is able to
verify such advanced properties of Java bytecode."
"Static analysis is a powerful technique for automatic verification of
programs but raises major engineering challenges when developing a full-fledged
analyzer for a realistic language such as Java. This paper describes the Sawja
library: a static analysis framework fully compliant with Java 6 which provides
OCaml modules for efficiently manipulating Java bytecode programs. We present
the main features of the library, including (i) efficient functional
data-structures for representing program with implicit sharing and lazy
parsing, (ii) an intermediate stack-less representation, and (iii) fast
computation and manipulation of complete programs."
"One of the differences among the various approaches to suspension-based
tabled evaluation is the scheduling strategy. The two most popular strategies
are local and batched evaluation.
  The former collects all the solutions to a tabled predicate before making any
one of them available outside the tabled computation. The latter returns
answers one by one before computing them all, which in principle is better if
only one answer (or a subset of the answers) is desired.
  Batched evaluation is closer to SLD evaluation in that it computes solutions
lazily as they are demanded, but it may need arbitrarily more memory than local
evaluation, which is able to reclaim memory sooner. Some programs which in
practice can be executed under the local strategy quickly run out of memory
under batched evaluation. This has led to the general adoption of local
evaluation at the expense of the more depth-first batched strategy.
  In this paper we study the reasons for the high memory consumption of batched
evaluation and propose a new scheduling strategy which we have termed swapping
evaluation. Swapping evaluation also returns answers one by one before
completing a tabled call, but its memory usage can be orders of magnitude less
than batched evaluation. An experimental implementation in the XSB system shows
that swapping evaluation is a feasible memory-scalable strategy that need not
compromise execution speed."
"Context-free approaches to static analysis gain precision over classical
approaches by perfectly matching returns to call sites---a property that
eliminates spurious interprocedural paths. Vardoulakis and Shivers's recent
formulation of CFA2 showed that it is possible (if expensive) to apply
context-free methods to higher-order languages and gain the same boost in
precision achieved over first-order programs.
  To this young body of work on context-free analysis of higher-order programs,
we contribute a pushdown control-flow analysis framework, which we derive as an
abstract interpretation of a CESK machine with an unbounded stack. One
instantiation of this framework marks the first polyvariant pushdown analysis
of higher-order programs; another marks the first polynomial-time analysis. In
the end, we arrive at a framework for control-flow analysis that can
efficiently compute pushdown generalizations of classical control-flow
analyses."
"We describe a derivational approach to abstract interpretation that yields
novel and transparently sound static analyses when applied to well-established
abstract machines. To demonstrate the technique and support our claim, we
transform the CEK machine of Felleisen and Friedman, a lazy variant of
Krivine's machine, and the stack-inspecting CM machine of Clements and
Felleisen into abstract interpretations of themselves. The resulting analyses
bound temporal ordering of program events; predict return-flow and
stack-inspection behavior; and approximate the flow and evaluation of by-need
parameters. For all of these machines, we find that a series of well-known
concrete machine refactorings, plus a technique we call store-allocated
continuations, leads to machines that abstract into static analyses simply by
bounding their stores. We demonstrate that the technique scales up uniformly to
allow static analysis of realistic language features, including tail calls,
conditionals, side effects, exceptions, first-class continuations, and even
garbage collection."
"We introduce streaming data string transducers that map input data strings to
output data strings in a single left-to-right pass in linear time. Data strings
are (unbounded) sequences of data values, tagged with symbols from a finite
set, over a potentially infinite data domain that supports only the operations
of equality and ordering. The transducer uses a finite set of states, a finite
set of variables ranging over the data domain, and a finite set of variables
ranging over data strings. At every step, it can make decisions based on the
next input symbol, updating its state, remembering the input data value in its
data variables, and updating data-string variables by concatenating data-string
variables and new symbols formed from data variables, while avoiding
duplication. We establish that the problems of checking functional equivalence
of two streaming transducers, and of checking whether a streaming transducer
satisfies pre/post verification conditions specified by streaming acceptors
over input/output data-strings, are in PSPACE. We identify a class of
imperative and a class of functional programs, manipulating lists of data
items, which can be effectively translated to streaming data-string
transducers. The imperative programs dynamically modify a singly-linked heap by
changing next-pointers of heap-nodes and by adding new nodes. The main
restriction specifies how the next-pointers can be used for traversal. We also
identify an expressively equivalent fragment of functional programs that
traverse a list using syntactically restricted recursive calls. Our results
lead to algorithms for assertion checking and for checking functional
equivalence of two programs, written possibly in different programming styles,
for commonly used routines such as insert, delete, and reverse."
"An important issue towards a broader acceptance of answer-set programming
(ASP) is the deployment of tools which support the programmer during the coding
phase. In particular, methods for debugging an answer-set program are
recognised as a crucial step in this regard. Initial work on debugging in ASP
mainly focused on propositional programs, yet practical debuggers need to
handle programs with variables as well. In this paper, we discuss a debugging
technique that is directly geared towards non-ground programs. Following
previous work, we address the central debugging question why some
interpretation is not an answer set. The explanations provided by our method
are computed by means of a meta-programming technique, using a uniform encoding
of a debugging request in terms of ASP itself. Our method also permits programs
containing comparison predicates and integer arithmetics, thus covering a
relevant language class commonly supported by all state-of-the-art ASP solvers."
"Page switching is a technique that increases the memory in microcontrollers
without extending the address buses. This technique is widely used in the
design of 8-bit MCUs. In this paper, we present an algorithm to reduce the
overhead of page switching. To pursue small code size, we place the emphasis on
the allocation of functions into suitable pages with a heuristic algorithm,
thereby the cost-effective placement of page selection instructions. Our
experimental results showed the optimization achieved a reduction in code size
of 13.2 percent."
"The intention of these notes is to give a mathematical account of how I
believe students could be taught to think about functional programming
languages and to explain how such languages work."
"With the advent of numerous languages it is difficult to realize the edge of
one language in a particular scope over another one. We are making an effort,
realizing these few issues and comparing some main stream languages like Java,
Scala, C++, Haskell, VB .NET, AspectJ, Perl, Ruby, PHP and Scheme keeping in
mind some core issues in program development."
"Comparison of programming languages is a common topic of discussion among
software engineers. Few languages ever become sufficiently popular that they
are used by more than a few people or find their niche in research or
education; but professional programmers can easily use dozens of different
languages during their career. Multiple programming languages are designed,
specified, and implemented every year in order to keep up with the changing
programming paradigms, hardware evolution, etc. In this paper we present a
comparative study between ten programming languages: Haskell, Java, Perl, C++,
AspectJ, COBOL, Ruby, PHP, Bash Scripts, and Scheme; with respect of the
following criteria: Secure programming practices, web applications development,
web services design and composition, object oriented-based abstraction,
reflection, aspect-orientation, functional programming, declarative
programming, batch scripting, and user interface prototype design."
"This is a survey on the programming languages: C++, JavaScript, AspectJ, C#,
Haskell, Java, PHP, Scala, Scheme, and BPEL. Our survey work involves a
comparative study of these ten programming languages with respect to the
following criteria: secure programming practices, web application development,
web service composition, OOP-based abstractions, reflection, aspect
orientation, functional programming, declarative programming, batch scripting,
and UI prototyping. We study these languages in the context of the above
mentioned criteria and the level of support they provide for each one of them."
"Grammar convergence is a method that helps discovering relationships between
different grammars of the same language or different language versions. The key
element of the method is the operational, transformation-based representation
of those relationships. Given input grammars for convergence, they are
transformed until they are structurally equal. The transformations are composed
from primitive operators; properties of these operators and the composed chains
provide quantitative and qualitative insight into the relationships between the
grammars at hand. We describe a refined method for grammar convergence, and we
use it in a major study, where we recover the relationships between all the
grammars that occur in the different versions of the Java Language
Specification (JLS). The relationships are represented as grammar
transformation chains that capture all accidental or intended differences
between the JLS grammars. This method is mechanized and driven by nominal and
structural differences between pairs of grammars that are subject to
asymmetric, binary convergence steps. We present the underlying operator suite
for grammar transformation in detail, and we illustrate the suite with many
examples of transformations on the JLS grammars. We also describe the
extraction effort, which was needed to make the JLS grammars amenable to
automated processing. We include substantial metadata about the convergence
process for the JLS so that the effort becomes reproducible and transparent."
"This paper presents the current state of an ongoing research project to
improve the performance of the OCaml byte-code interpreter using Just-In-Time
native code generation. Our JIT engine OCamlJIT2 currently runs on x86-64
processors, mimicing precisely the behavior of the OCaml virtual machine. Its
design and implementation is described, and performance measures are given."
"In this paper, we introduce Continuation Passing C (CPC), a programming
language for concurrent systems in which native and cooperative threads are
unified and presented to the programmer as a single abstraction. The CPC
compiler uses a compilation technique, based on the CPS transform, that yields
efficient code and an extremely lightweight representation for contexts. We
provide a proof of the correctness of our compilation scheme. We show in
particular that lambda-lifting, a common compilation technique for functional
languages, is also correct in an imperative language like C, under some
conditions enforced by the CPC compiler. The current CPC compiler is mature
enough to write substantial programs such as Hekate, a highly concurrent
BitTorrent seeder. Our benchmark results show that CPC is as efficient, while
using significantly less space, as the most efficient thread libraries
available."
"SWI-Prolog is neither a commercial Prolog system nor a purely academic
enterprise, but increasingly a community project. The core system has been
shaped to its current form while being used as a tool for building research
prototypes, primarily for \textit{knowledge-intensive} and \textit{interactive}
systems. Community contributions have added several interfaces and the
constraint (CLP) libraries. Commercial involvement has created the initial
garbage collector, added several interfaces and two development tools: PlDoc (a
literate programming documentation system) and PlUnit (a unit testing
environment).
  In this article we present SWI-Prolog as an integrating tool, supporting a
wide range of ideas developed in the Prolog community and acting as glue
between \textit{foreign} resources. This article itself is the glue between
technical articles on SWI-Prolog, providing context and experience in applying
them over a longer period."
"SICStus Prolog has evolved for nearly 25 years. This is an appropriate point
in time for revisiting the main language and design decisions, and try to
distill some lessons. SICStus Prolog was conceived in a context of multiple,
conflicting Prolog dialect camps and a fledgling standardization effort. We
reflect on the impact of this effort and role model implementations on our
development. After summarizing the development history, we give a guided tour
of the system anatomy, exposing some designs that were not published before. We
give an overview of our new interactive development environment, and describe a
sample of key applications. Finally, we try to identify key good and not so
good design decisions."
"Concurrency has been rapidly gaining importance in general-purpose computing,
caused by the recent turn towards multicore processing architectures. As a
result, an increasing number of developers have to learn to write concurrent
programs, a task that is known to be hard even for the expert. Language
designers are therefore working on languages that promise to make concurrent
programming ""easier"" than using traditional thread libraries. However, the
claim that a new language is more usable than another cannot be supported by
purely theoretical considerations, but calls for empirical studies. In this
paper, we present the design of a study to compare concurrent programming
languages with respect to comprehending and debugging existing programs and
writing correct new programs. A critical challenge for such a study is avoiding
the bias that might be introduced during the training phase and when
interpreting participants' solutions. We address these issues by the use of
self-study material and an evaluation scheme that exposes any subjective
decisions of the corrector, or eliminates them altogether. We apply our design
to a comparison of two object-oriented languages for concurrency, multithreaded
Java and SCOOP (Simple Concurrent Object-Oriented Programming), in an academic
setting. We obtain results in favor of SCOOP even though the study participants
had previous training in Java Threads."
"Threads are a convenient and modular abstraction for writing concurrent
programs, but often fairly expensive. The standard alternative to threads,
event-loop programming, allows much lighter units of concurrency, but leads to
code that is difficult to write and even harder to understand. Continuation
Passing C (CPC) is a translator that converts a program written in threaded
style into a program written with events and native system threads, at the
programmer's choice. Together with two undergraduate students, we taught
ourselves how to program in CPC by writing Hekate, a massively concurrent
network server designed to efficiently handle tens of thousands of
simultaneously connected peers. In this paper, we describe a number of
programming idioms that we learnt while writing Hekate; while some of these
idioms are specific to CPC, many should be applicable to other programming
systems with sufficiently cheap threads."
"We describe the BinProlog system's compilation technology, runtime system and
its extensions supporting first-class Logic Engines while providing a short
history of its development, details of some of its newer re-implementations as
well as an overview of the most important architectural choices involved in
their design.
  With focus on its differences with conventional WAM implementations, we
explain key details of BinProlog's compilation technique, which replaces the
WAM with a simplified continuation passing runtime system (the ""BinWAM""), based
on a mapping of full Prolog to binary logic programs. This is followed by a
description of a term compression technique using a ""tag-on-data""
representation.
  Later derivatives, the Java-based Jinni Prolog compiler and the recently
developed Lean Prolog system refine the BinProlog architecture with first-class
Logic Engines, made generic through the use of an Interactor interface. An
overview of their applications with focus on the ability to express at source
level a wide variety of Prolog built-ins and extensions, covers these newer
developments."
"Existing technology can parse arbitrary context-free grammars, but only a
single, static grammar per input. In order to support more powerful
syntax-extension systems, we propose reflective grammars, which can modify
their own syntax during parsing. We demonstrate and prove the correctness of an
algorithm for parsing reflective grammars. The algorithm is based on Earley's
algorithm, and we prove that it performs asymptotically no worse than Earley's
algorithm on ordinary context-free grammars."
"The use of programming languages such as Java and C in Open Source Software
(OSS) has been well studied. However, many other popular languages such as XSL
or XML have received minor attention. In this paper, we discuss some trends in
OSS development that we observed when considering multiple programming language
evolution of OSS. Based on the revision data of 22 OSS projects, we tracked the
evolution of language usage and other artefacts such as documentation files,
binaries and graphics files. In these systems several different languages and
artefact types including C/C++, Java, XML, XSL, Makefile, Groovy, HTML, Shell
scripts, CSS, Graphics files, JavaScript, JSP, Ruby, Phyton, XQuery,
OpenDocument files, PHP, etc. have been used. We found that the amount of code
written in different languages differs substantially. Some of our findings can
be summarized as follows: (1) JavaScript and CSS files most often co-evolve
with XSL; (2) Most Java developers but only every second C/C++ developer work
with XML; (3) and more generally, we observed a significant increase of usage
of XML and XSL during recent years and found that Java or C are hardly ever the
only language used by a developer. In fact, a developer works with more than 5
different artefact types (or 4 different languages) in a project on average."
"Predicate abstraction is a key enabling technology for applying finite-state
model checkers to programs written in mainstream languages. It has been used
very successfully for debugging sequential system-level C code. Although model
checking was originally designed for analyzing concurrent systems, there is
little evidence of fruitful applications of predicate abstraction to
shared-variable concurrent software. The goal of this paper is to close this
gap. We have developed a symmetry-aware predicate abstraction strategy: it
takes into account the replicated structure of C programs that consist of many
threads executing the same procedure, and generates a Boolean program template
whose multi-threaded execution soundly overapproximates the concurrent C
program. State explosion during model checking parallel instantiations of this
template can now be absorbed by exploiting symmetry. We have implemented our
method in the SATABS predicate abstraction framework, and demonstrate its
superior performance over alternative approaches on a large range of
synchronization programs."
"We study the correspondence between a concurrent lambda-calculus in
administrative, continuation passing style and a pi-calculus and we derive a
termination result for the latter."
"For the lambda-calculus with letrec we develop an optimisation, which is
based on the contraction of a certain class of 'future' (also: virtual)
redexes.
  In the implementation of functional programming languages it is common
practice to perform beta-reductions at compile time whenever possible in order
to produce code that requires fewer reductions at run time. This is, however,
in principle limited to redexes and created redexes that are 'visible' (in the
sense that they can be contracted without the need for unsharing), and cannot
generally be extended to redexes that are concealed by sharing constructs such
as letrec. In the case of recursion, concealed redexes become visible only
after unwindings during evaluation, and then have to be contracted time and
again.
  We observe that in some cases such redexes exhibit a certain form of
repetitive behaviour at run time. We describe an analysis for identifying
binders that give rise to such repetitive reduction patterns, and eliminate
them by a sort of predictive contraction. Thereby these binders are lifted out
of recursive positions or eliminated altogether, as a result alleviating the
amount of beta-reductions required for each recursive iteration.
  Both our analysis and simplification are suitable to be integrated into
existing compilers for functional programming languages as an additional
optimisation phase. With this work we hope to contribute to increasing the
efficiency of executing programs written in such languages."
"In a functional language, the dominant control-flow mechanism is function
call and return. Most higher-order flow analyses, including k-CFA, do not
handle call and return well: they remember only a bounded number of pending
calls because they approximate programs with control-flow graphs. Call/return
mismatch introduces precision-degrading spurious control-flow paths and
increases the analysis time. We describe CFA2, the first flow analysis with
precise call/return matching in the presence of higher-order functions and tail
calls. We formulate CFA2 as an abstract interpretation of programs in
continuation-passing style and describe a sound and complete summarization
algorithm for our abstract semantics. A preliminary evaluation shows that CFA2
gives more accurate data-flow information than 0CFA and 1CFA."
"Yet Another Prolog (YAP) is a Prolog system originally developed in the
mid-eighties and that has been under almost constant development since then.
This paper presents the general structure and design of the YAP system,
focusing on three important contributions to the Logic Programming community.
First, it describes the main techniques used in YAP to achieve an efficient
Prolog engine. Second, most Logic Programming systems have a rather limited
indexing algorithm. YAP contributes to this area by providing a dynamic
indexing mechanism, or just-in-time indexer (JITI). Third, a important
contribution of the YAP system has been the integration of both or-parallelism
and tabling in a single Logic Programming system."
"Linear logic provides a framework to control the complexity of higher-order
functional programs. We present an extension of this framework to programs with
multithreading and side effects focusing on the case of elementary time. Our
main contributions are as follows. First, we provide a new combinatorial proof
of termination in elementary time for the functional case. Second, we develop
an extension of the approach to a call-by-value $lambda$-calculus with
multithreading and side effects. Third, we introduce an elementary affine type
system that guarantees the standard subject reduction and progress properties.
Finally, we illustrate the programming of iterative functions with side effects
in the presented formalism."
"We provide an overall description of the Ciao multiparadigm programming
system emphasizing some of the novel aspects and motivations behind its design
and implementation. An important aspect of Ciao is that, in addition to
supporting logic programming (and, in particular, Prolog), it provides the
programmer with a large number of useful features from different programming
paradigms and styles, and that the use of each of these features (including
those of Prolog) can be turned on and off at will for each program module.
Thus, a given module may be using, e.g., higher order functions and
constraints, while another module may be using assignment, predicates, Prolog
meta-programming, and concurrency. Furthermore, the language is designed to be
extensible in a simple and modular way. Another important aspect of Ciao is its
programming environment, which provides a powerful preprocessor (with an
associated assertion language) capable of statically finding non-trivial bugs,
verifying that programs comply with specifications, and performing many types
of optimizations (including automatic parallelization). Such optimizations
produce code that is highly competitive with other dynamic languages or, with
the (experimental) optimizing compiler, even that of static languages, all
while retaining the flexibility and interactive development of a dynamic
language. This compilation architecture supports modularity and separate
compilation throughout. The environment also includes a powerful
auto-documenter and a unit testing framework, both closely integrated with the
assertion system. The paper provides an informal overview of the language and
program development environment. It aims at illustrating the design philosophy
rather than at being exhaustive, which would be impossible in a single journal
paper, pointing instead to previous Ciao literature."
"Multidimensional Retiming is one of the most important optimization
techniques to improve timing parameters of nested loops. It consists in
exploring the iterative and recursive structures of loops to redistribute
computation nodes on cycle periods, and thus to achieve full parallelism.
However, this technique introduces a large overhead in a loop generation due to
the loop transformation. The provided solutions are generally characterized by
an important cycle number and a great code size. It represents the most
limiting factors while implementing them in embedded systems. In this paper, we
present a new Multidimensional Retiming technique, called ""Optimal
Multidimensional Retiming"" (OMDR). It reveals the timing and data dependency
characteristics of nodes, to minimize the overhead. The experimental results
show that the average improvement on the execution time of the nested loops by
our technique is 19.31% compared to the experiments provided by an existent
Multidimensional Retiming Technique. The average code size is reduced by 43.53%
compared to previous experiments."
"Session types allow communication protocols to be specified
type-theoretically so that protocol implementations can be verified by static
type checking. We extend previous work on session types for distributed
object-oriented languages in three ways. (1) We attach a session type to a
class definition, to specify the possible sequences of method calls. (2) We
allow a session type (protocol) implementation to be modularized, i.e.
partitioned into separately-callable methods. (3) We treat session-typed
communication channels as objects, integrating their session types with the
session types of classes. The result is an elegant unification of communication
channels and their session types, distributed object-oriented programming, and
a form of typestate supporting non-uniform objects, i.e. objects that
dynamically change the set of available methods. We define syntax, operational
se-mantics, a sound type system, and a sound and complete type checking
algorithm for a small distributed class-based object-oriented language with
structural subtyping. Static typing guarantees that both sequences of messages
on channels, and sequences of method calls on objects, conform to
type-theoretic specifications, thus ensuring type-safety. The language includes
expected features of session types, such as delegation, and expected features
of object-oriented programming, such as encapsulation of local state."
"Interpreters have a bad reputation for having lower performance than
just-in-time compilers. We present a new way of building high performance
interpreters that is particularly effective for executing dynamically typed
programming languages. The key idea is to combine speculative staging of
optimized interpreter instructions with a novel technique of incrementally and
iteratively concerting them at run-time.
  This paper introduces the concepts behind deriving optimized instructions
from existing interpreter instructions---incrementally peeling off layers of
complexity. When compiling the interpreter, these optimized derivatives will be
compiled along with the original interpreter instructions. Therefore, our
technique is portable by construction since it leverages the existing
compiler's backend. At run-time we use instruction substitution from the
interpreter's original and expensive instructions to optimized instruction
derivatives to speed up execution.
  Our technique unites high performance with the simplicity and portability of
interpreters---we report that our optimization makes the CPython interpreter up
to more than four times faster, where our interpreter closes the gap between
and sometimes even outperforms PyPy's just-in-time compiler."
"Modern languages are typically supported by managed runtimes (Virtual
Machines). Since VMs have to deal with many concepts such as memory management,
abstract execution model and scheduling, they tend to be very complex.
Additionally, VMs have to meet strong performance requirements. This demand of
performance is one of the main reasons why many VMs are built statically. Thus,
design decisions are frozen at compile time preventing changes at runtime. One
clear example is the impossibility to dynamically adapt or change primitives of
the VM once it has been compiled. In this work we present a toolchain that
allows for altering and configuring components such as primitives and plug-ins
at runtime. The main contribution is Waterfall, a dynamic and reflective
translator from Slang, a restricted subset of Smalltalk, to native code.
Waterfall generates primitives on demand and executes them on the fly. We
validate our approach by implementing dynamic primitive modification and
runtime customization of VM plug-ins."
"Coroutines and events are two common abstractions for writing concurrent
programs. Because coroutines are often more convenient, but events more
portable and efficient, it is natural to want to translate the former into the
latter. CPC is such a source-to-source translator for C programs, based on a
partial conversion into continuation-passing style (CPS conversion) of
functions annotated as cooperative.
  In this article, we study the application of the CPC translator to QEMU, an
open-source machine emulator which also uses annotated coroutine functions for
concurrency. We first propose a new type of annotations to identify functions
which never cooperate, and we introduce CoroCheck, a tool for the static
analysis and inference of cooperation annotations. Then, we improve the CPC
translator, defining CPS conversion as a calling convention for the C language,
with support for indirect calls to CPS-converted function through function
pointers. Finally, we apply CoroCheck and CPC to QEMU (750 000 lines of C
code), fixing hundreds of missing annotations and comparing performance of the
translated code with existing implementations of coroutines in QEMU.
  Our work shows the importance of static annotation checking to prevent actual
concurrency bugs, and demonstrates that CPS conversion is a flexible, portable,
and efficient compilation technique, even for very large programs written in an
imperative language."
"The purpose of a program analysis is to compute an abstract meaning for a
program which approximates its dynamic behaviour. A compositional program
analysis accomplishes this task with a divide-and-conquer strategy: the meaning
of a program is computed by dividing it into sub-programs, computing their
meaning, and then combining the results. Compositional program analyses are
desirable because they can yield scalable (and easily parallelizable) program
analyses.
  This paper presents algebraic framework for designing, implementing, and
proving the correctness of compositional program analyses. A program analysis
in our framework defined by an algebraic structure equipped with sequencing,
choice, and iteration operations. From the analysis design perspective, a
particularly interesting consequence of this is that the meaning of a loop is
computed by applying the iteration operator to the loop body. This style of
compositional loop analysis can yield interesting ways of computing loop
invariants that cannot be defined iteratively. We identify a class of
algorithms, the so-called path-expression algorithms [Tarjan1981,Scholz2007],
which can be used to efficiently implement analyses in our framework. Lastly,
we develop a theory for proving the correctness of an analysis by establishing
an approximation relationship between an algebra defining a concrete semantics
and an algebra defining an analysis."
"This article shows a correspondence between abstract interpretation of
imperative programs and the refinement calculus: in the refinement calculus, an
abstract interpretation of a program is a specification which is a function.
  This correspondence can be used to guide the design of mechanically verified
static analyses, keeping the correctness proof well separated from the
heuristic parts of the algorithms."
"Language-integrated query techniques have been explored in a number of
different language designs. We consider two different, type-safe approaches
employed by Links and F#. Both approaches provide rich dynamic query generation
capabilities, and thus amount to a form of heterogeneous staged computation,
but to date there has been no formal investigation of their relative
expressiveness. We present two core calculi Eff and Quot, respectively
capturing the essential aspects of language-integrated querying using effects
in Links and quotation in LINQ. We show via translations from Eff to Quot and
back that the two approaches are equivalent in expressiveness. Based on the
translation from Eff to Quot, we extend a simple Links compiler to handle
queries."
"Modern computer systems are awash in a sea of asynchronous events. There is
an increasing need for a declarative language that can permit business users to
specify complex event-processing rules. Such rules should be able to correlate
different event streams, detect absence of events (negative information),
permit aggregations over sliding windows, specify dependent sliding windows
etc. For instance it should be possible to precisely state a rule such as
""Every seventh trading session that DowJones has risen consecutively, and IBM's
stock is off 3% over its average in this period, evaluate IBM position"",
""Declare the sensor as faulty if no reading has been received for 500 ms"", etc.
Further, the language should be implementable efficiently in an event-driven
fashion.
  We propose the Timed (Default) Concurrent Constraint, TCC, programming
framework as a foundation for such complex event processing. While very rich,
the TCC framework ""forgets"" information from one instant to the next. We make
two extensions. First, we extend the TCC model to carry the store from previous
time instants as ""past"" information in the current time instant. This permits
rules to to be written with rich queries over the past. Second, we show that
many of the powerful properties of the agent language can be folded into the
query language by permitting agents and queries to be defined mutually
recursively, building on the testing interpretation of intuitionistic logic
described in RCC \cite{radha-fsttcs05}. We show that this permits queries to
move ""back and forth"" in the past, e.g.{} ""Order a review if the last time that
IBM stock price dropped by 10% in a day, there was more than 20% increase in
trading volume for Oracle the following day.""
  We provide a formal semantics for TCC + Histories and establish some basic
properties."
"Provenance is an increasing concern due to the ongoing revolution in sharing
and processing scientific data on the Web and in other computer systems. It is
proposed that many computer systems will need to become provenance-aware in
order to provide satisfactory accountability, reproducibility, and trust for
scientific or other high-value data. To date, there is not a consensus
concerning appropriate formal models or security properties for provenance. In
previous work, we introduced a formal framework for provenance security and
proposed formal definitions of properties called disclosure and obfuscation.
  In this article, we study refined notions of positive and negative disclosure
and obfuscation in a concrete setting, that of a general-purpose programing
language. Previous models of provenance have focused on special-purpose
languages such as workflows and database queries. We consider a higher-order,
functional language with sums, products, and recursive types and functions, and
equip it with a tracing semantics in which traces themselves can be replayed as
computations. We present an annotation-propagation framework that supports many
provenance views over traces, including standard forms of provenance studied
previously. We investigate some relationships among provenance views and
develop some partial solutions to the disclosure and obfuscation problems,
including correct algorithms for disclosure and positive obfuscation based on
trace slicing."
"A proxy object is a surrogate or placeholder that controls access to another
target object. Proxy objects are a widely used solution for different scenarios
such as remote method invocation, future objects, behavioral reflection, object
databases, inter-languages communications and bindings, access control, lazy or
parallel evaluation, security, among others. Most proxy implementations support
proxies for regular objects but are unable to create proxies for objects with
an important role in the runtime infrastructure such as classes or methods.
Proxies can be complex to install, they can have a significant overhead, they
can be limited to certain kind of classes, etc. Moreover, proxy implementations
are often not stratified and they do not have a clear separation between
proxies (the objects intercepting messages) and handlers (the objects handling
interceptions). In this paper, we present Ghost: a uniform and general-purpose
proxy implementation for the Pharo programming language. Ghost provides low
memory consuming proxies for regular objects as well as for classes and
methods. When a proxy takes the place of a class, it intercepts both the
messages received by the class and the lookup of methods for messages received
by its instances. Similarly, if a proxy takes the place of a method, then the
method execution is intercepted too."
"Compensation is a technique to roll-back a system to a consistent state in
case of failure. Recovery mechanisms for compensating calculi specify the order
of execution of compensation sequences. Dynamic recovery means that the order
of execution is determined at runtime. In this paper, we define an extension of
Compensating CSP, called DEcCSP, with general dynamic recovery. We provide a
formal, operational semantics for the calculus, and illustrate its expressive
power with a case study. In contrast with previous versions of Compensating
CSP, DEcCSP provides mechanisms to replace or discard compensations at runtime.
Additionally, we bring back to DEcCSP standard CSP operators that are not
available in other compensating CSP calculi, and introduce channel
communication."
"Scenario-aware dataflow (SADF) is a prominent tool for modeling and analysis
of dynamic embedded dataflow applications. In SADF the application is
represented as a finite collection of synchronous dataflow (SDF) graphs, each
of which represents one possible application behaviour or scenario. A finite
state machine (FSM) specifies the possible orders of scenario occurrences. The
SADF model renders the tightest possible performance guarantees, but is limited
by its finiteness. This means that from a practical point of view, it can only
handle dynamic dataflow applications that are characterized by a reasonably
sized set of possible behaviours or scenarios. In this paper we remove this
limitation for a class of SADF graphs by means of SADF model parametrization in
terms of graph port rates and actor execution times. First, we formally define
the semantics of the model relevant for throughput analysis based on (max,+)
linear system theory and (max,+) automata. Second, by generalizing some of the
existing results, we give the algorithms for worst-case throughput analysis of
parametric rate and parametric actor execution time acyclic SADF graphs with a
fully connected, possibly infinite state transition system. Third, we
demonstrate our approach on a few realistic applications from digital signal
processing (DSP) domain mapped onto an embedded multi-processor architecture."
"We study, formally and experimentally, the trade-off in temporal and spatial
overhead when managing contiguous blocks of memory using the explicit, dynamic
and real-time heap management system Compact-fit (CF). The key property of CF
is that temporal and spatial overhead can be bounded, related, and predicted in
constant time through the notion of partial and incremental compaction. Partial
compaction determines the maximally tolerated degree of memory fragmentation.
Incremental compaction of objects, introduced here, determines the maximal
amount of memory involved in any, logically atomic, portion of a compaction
operation. We explore CF's potential application space on (1) multiprocessor
and multicore systems as well as on (2) memory-constrained uniprocessor
systems. For (1), we argue that little or no compaction is likely to avoid the
worst case in temporal as well as spatial overhead but also observe that
scalability only improves by a constant factor. Scalability can be further
improved significantly by reducing overall data sharing through separate
instances of Compact-fit. For (2), we observe that incremental compaction can
effectively trade-off throughput and memory fragmentation for lower latency."
"Presented simple extensions to scala language related to import statements:
exported imports, which provide ability to reuse sequence of import clauses in
composable form and default rewriters, which provide mechanism for pluggable
macro-based AST transformation of overall compilation unit, activated by import
of library object. Using these facilities not only allows more compact code, it
prevents application programmer from producing certain type of errors too and
allows to implement local language extension as libraries on top of standard
compiler. Part of discussed extensions is submitted to scala language committee
as pre-sip \cite{ai-presip} and can be used as first step for refining imports
semantics in the future version of scala language."
"Higher-order constructs extend the expressiveness of first-order (Constraint)
Logic Programming ((C)LP) both syntactically and semantically. At the same time
assertions have been in use for some time in (C)LP systems helping programmers
detect errors and validate programs. However, these assertion-based extensions
to (C)LP have not been integrated well with higher-order to date. This paper
contributes to filling this gap by extending the assertion-based approach to
error detection and program validation to the higher-order context within
(C)LP. We propose an extension of properties and assertions as used in (C)LP in
order to be able to fully describe arguments that are predicates. The extension
makes the full power of the assertion language available when describing
higher-order arguments. We provide syntax and semantics for (higher-order)
properties and assertions, as well as for programs which contain such
assertions, including the notions of error and partial correctness and provide
some formal results. We also discuss several alternatives for performing
run-time checking of such programs."
"We introduce an object-oriented framework for parallel programming, which is
based on the observation that programming objects can be naturally interpreted
as processes. A parallel program consists of a collection of persistent
processes that communicate by executing remote methods. We discuss code
parallelization and process persistence, and explain the main ideas in the
context of computations with very large data objects."
"Program transformations in terms of abstract syntax trees compromise
referential integrity by introducing variable capture. Variable capture occurs
when in the generated program a variable declaration accidentally shadows the
intended target of a variable reference. Existing transformation systems either
do not guarantee the avoidance of variable capture or impair the implementation
of transformations.
  We present an algorithm called name-fix that automatically eliminates
variable capture from a generated program by systematically renaming variables.
name-fix is guided by a graph representation of the binding structure of a
program, and requires name-resolution algorithms for the source language and
the target language of a transformation. name-fix is generic and works for
arbitrary transformations in any transformation system that supports origin
tracking for names. We verify the correctness of name-fix and identify an
interesting class of transformations for which name-fix provides hygiene. We
demonstrate the applicability of name-fix for implementing capture-avoiding
substitution, inlining, lambda lifting, and compilers for two domain-specific
languages."
"Jython is a Java based Python implementation and the most seamless way to
integrate Python and Java. However, it does not support native extensions
written for CPython like NumPy or SciPy. Since most scientific Python code
fundamentally depends on exactly such native extensions directly or indirectly,
it usually cannot be run with Jython. JyNI (Jython Native Interface) aims to
close this gap. It is a layer that enables Jython users to load native CPython
extensions and access them from Jython the same way as they would do in
CPython. In order to leverage the JyNI functionality, you just have to put it
on the Java classpath when Jython is launched. It neither requires you to
recompile the extension code, nor to build a customized Jython fork. That
means, it is binary compatible with existing extension builds. At the time of
writing, JyNI does not fully implement the Python C-API and it is only capable
of loading simple examples that only involve most basic built-in types. The
concept is rather complete though and our goal is to provide the C-API needed
to load NumPy as soon as possible. After that we will focus on SciPy and
others. We expect that our work will also enable Java developers to use CPython
extensions like NumPy in their Java code."
"External or internal domain-specific languages (DSLs) or (fluent) APIs?
Whoever you are -- a developer or a user of a DSL -- you usually have to choose
your side; you should not! What about metamorphic DSLs that change their shape
according to your needs? We report on our 4-years journey of providing the
""right"" support (in the domain of feature modeling), leading us to develop an
external DSL, different shapes of an internal API, and maintain all these
languages. A key insight is that there is no one-size-fits-all solution or no
clear superiority of a solution compared to another. On the contrary, we found
that it does make sense to continue the maintenance of an external and internal
DSL. The vision that we foresee for the future of software languages is their
ability to be self-adaptable to the most appropriate shape (including the
corresponding integrated development environment) according to a particular
usage or task. We call metamorphic DSL such a language, able to change from one
shape to another shape."
"Information-flow control mechanisms are difficult both to design and to prove
correct. To reduce the time wasted on doomed proof attempts due to broken
definitions, we advocate modern random testing techniques for finding
counterexamples during the design process. We show how to use QuickCheck, a
property-based random-testing tool, to guide the design of increasingly complex
information-flow abstract machines, leading up to a sophisticated register
machine with a novel and highly permissive flow-sensitive dynamic enforcement
mechanism that is sound in the presence of first-class public labels. We find
that both sophisticated strategies for generating well-distributed random
programs and readily falsifiable formulations of noninterference properties are
critically important for efficient testing. We propose several approaches and
evaluate their effectiveness on a collection of injected bugs of varying
subtlety. We also present an effective technique for shrinking large
counterexamples to minimal, easily comprehensible ones. Taken together, our
best methods enable us to quickly and automatically generate simple
counterexamples for more than 45 bugs. Moreover, we show how testing guides the
discovery of the sophisticated invariants needed for the noninterference proof
of our most complex machine."
"In this paper, we compose six different Python and Prolog VMs into 4 pairwise
compositions: one using C interpreters; one running on the JVM; one using
meta-tracing interpreters; and one using a C interpreter and a meta-tracing
interpreter. We show that programs that cross the language barrier frequently
execute faster in a meta-tracing composition, and that meta-tracing imposes a
significantly lower overhead on composed programs relative to mono-language
programs."
"Most program profiling methods output the execution time of one specific
program execution, but not its computational complexity class in terms of the
big-O notation. Perfrewrite is a tool based on LLVM's Clang compiler to rewrite
a program such that it tracks semantic information while the program executes
and uses it to guess memory usage, communication and computational complexity.
While source code instrumentation is a standard technique for profiling, using
it for deriving formulas is an uncommon approach."
"Although the HO/N games are fully abstract for PCF, the traditional notion of
innocence (which underpins these games) is not satisfactory for such language
features as non-determinism and probabilistic branching, in that there are
stateless terms that are not innocent. Based on a category of P-visible plays
with a notion of embedding as morphisms, we propose a natural generalisation by
viewing innocent strategies as sheaves over (a site of) plays, echoing a slogan
of Hirschowitz and Pous. Our approach gives rise to fully complete game models
in each of the three cases of deterministic, nondeterministic and probabilistic
branching. To our knowledge, in the second and third cases, ours are the first
such factorisation-free constructions."
"Statically reasoning in the presence of exceptions and about the effects of
exceptions is challenging: exception-flows are mutually determined by
traditional control-flow and points-to analyses. We tackle the challenge of
analyzing exception-flows from two angles. First, from the angle of pruning
control-flows (both normal and exceptional), we derive a pushdown framework for
an object-oriented language with full-featured exceptions. Unlike traditional
analyses, it allows precise matching of throwers to catchers. Second, from the
angle of pruning points-to information, we generalize abstract garbage
collection to object-oriented programs and enhance it with liveness analysis.
We then seamlessly weave the techniques into enhanced reachability computation,
yielding highly precise exception-flow analysis, without becoming intractable,
even for large applications. We evaluate our pruned, pushdown exception-flow
analysis, comparing it with an established analysis on large scale standard
Java benchmarks. The results show that our analysis significantly improves
analysis precision over traditional analysis within a reasonable analysis time."
"We present necessary and sufficient conditions for the termination of linear
homogeneous programs. We also develop a complete method to check termination
for this class of programs. Our complete characterization of termination for
such programs is based on linear algebraic methods. We reduce the verification
of the termination problem to checking the orthogonality of a well determined
vector space and a certain vector, both related to loops in the program.
Moreover, we provide theoretical results and symbolic computational methods
guaranteeing the soundness, completeness and numerical stability of the
approach. Finally, we show that it is enough to interpret variable values over
a specific countable number field, or even over its ring of integers, when one
wants to check termination over the reals."
"In this paper we address the deadlock detection problem in the context of
SCOOP - an OO-programming model for concurrency, recently formalized in Maude.
We present the integration of a deadlock detection mechanism on top of the
aforementioned formalization and analyze how an abstract semantics of SCOOP
based on a notion of ""may alias expressions"" can contribute to improving the
deadlock detection procedure."
"Register allocation and instruction scheduling are two central compiler
back-end problems that are critical for quality. In the last two decades,
combinatorial optimization has emerged as an alternative approach to
traditional, heuristic algorithms for these problems. Combinatorial approaches
are generally slower but more flexible than their heuristic counterparts and
have the potential to generate optimal code. This paper surveys existing
literature on combinatorial register allocation and instruction scheduling. The
survey covers approaches that solve each problem in isolation as well as
approaches that integrate both problems. The latter have the potential to
generate code that is globally optimal by capturing the trade-off between
conflicting register allocation and instruction scheduling decisions."
"There is an apparent similarity between the descriptions of small-step
operational semantics of imperative programs and the semantics of finite
automata, so defining an abstraction mapping from semantics to automata and
proving a simulation property seems to be easy. This paper aims at identifying
the reasons why simple proofs break, among them artifacts in the semantics that
lead to stuttering steps in the simulation. We then present a semantics based
on the zipper data structure, with a direct interpretation of evaluation as
navigation in the syntax tree. The abstraction function is then defined by
equivalence class construction."
"We study sequential programs that are instruction sequences with jump-shift
instructions in the setting of PGA (ProGram Algebra). Jump-shift instructions
preceding a jump instruction increase the position to jump to. The jump-shift
instruction is not found in programming practice. Its merit is that the
expressive power of PGA extended with the jump-shift instruction, is not
reduced if the reach of jump instructions is bounded. This is used to show that
there exists a finite-state execution mechanism that by making use of a counter
can produce each finite-state thread from some program that is a finite or
periodic infinite sequence of instructions from a finite set."
"Tabled Constraint Logic Programming is a powerful execution mechanism for
dealing with Constraint Logic Programming without worrying about fixpoint
computation. Various applications, e.g in the fields of program analysis and
model checking, have been proposed. Unfortunately, a high-level system for
developing new applications is lacking, and programmers are forced to resort to
complicated ad hoc solutions.
  This papers presents TCHR, a high-level framework for tabled Constraint Logic
Programming. It integrates in a light-weight manner Constraint Handling Rules
(CHR), a high-level language for constraint solvers, with tabled Logic
Programming. The framework is easily instantiated with new application-specific
constraint domains. Various high-level operations can be instantiated to
control performance. In particular, we propose a novel, generalized technique
for compacting answer sets."
"Using a call-by-value functional language as an example, this article
illustrates the use of coinductive definitions and proofs in big-step
operational semantics, enabling it to describe diverging evaluations in
addition to terminating evaluations. We formalize the connections between the
coinductive big-step semantics and the standard small-step semantics, proving
that both semantics are equivalent. We then study the use of coinductive
big-step semantics in proofs of type soundness and proofs of semantic
preservation for compilers. A methodological originality of this paper is that
all results have been proved using the Coq proof assistant. We explain the
proof-theoretic presentation of coinductive definitions and proofs offered by
Coq, and show that it facilitates the discovery and the presentation of the
results."
"The problem of space-optimal jump encoding in the x86 instruction set, also
known as branch displacement optimization, is described, and a linear-time
algorithm is given that uses no complicated data structures, no recursion, and
no randomization. The only assumption is that there are no array declarations
whose size depends on the negative of the size of a section of code (Hyde
2006), which is reasonable for real code."
"This volume contains the papers presented at WLPE 2008: the 18th Workshop on
Logic-based Methods in Programming Environments held on 12th December, 2008 in
Udine, Italy. It was held as a satellite workshop of ICLP 2008, the 24th
International Conference on Logic Programming."
"Some recent approaches for scalable offline partial evaluation of logic
programs include a size-change analysis for ensuring both so called local and
global termination. In this work|inspired by experimental evaluation|we
introduce several improvements that may increase the accuracy of the analysis
and, thus, the quality of the associated specialized programs. We aim to
achieve this while maintaining the same complexity and scalability of the
recent works."
"The use of non-deterministic functions is a distinctive feature of modern
functional logic languages. The semantics commonly adopted is call-time choice,
a notion that at the operational level is related to the sharing mechanism of
lazy evaluation in functional languages. However, there are situations where
run-time choice, closer to ordinary rewriting, is more appropriate. In this
paper we propose an extension of existing call-time choice based languages, to
provide support for run-time choice in localized parts of a program. The
extension is remarkably simple at three relevant levels: syntax, formal
operational calculi and implementation, which is based on the system Toy."
"Logic programming is sometimes described as relational programming: a
paradigm in which the programmer specifies and composes n-ary relations using
systems of constraints. An advanced logic programming environment will provide
tools that abstract these relations to transform, optimise, or even verify the
correctness of a logic program. This talk will show that these concepts, namely
relations, constraints and abstractions, turn out to also be important in the
reverse engineer process that underpins the discovery of bugs within the
security industry."
"We present an extension of System F with call-by-name exceptions. The type
system is enriched with two syntactic constructs: a union type for programs
whose execution may raise an exception at top level, and a corruption type for
programs that may raise an exception in any evaluation context (not necessarily
at top level). We present the syntax and reduction rules of the system, as well
as its typing and subtyping rules. We then study its properties, such as
confluence. Finally, we construct a realizability model using orthogonality
techniques, from which we deduce that well-typed programs are weakly
normalizing and that the ones who have the type of natural numbers really
compute a natural number, without raising exceptions."
"A computational abstract machine based on two operations: referencing and bit
copying is presented. These operations are sufficient for carrying out any
computation. They can be used as the primitives for a Turing-complete
programming language. The interesting point is that the computation can be done
without logic operations such as AND or OR. The compiler and emulator of this
language with sample programs are available on the Internet."
"We present natural semantics for acyclic as well as cyclic call-by-need
lambda calculi, which are proved equivalent to the reduction semantics given by
Ariola and Felleisen. The natural semantics are big-step and use global heaps,
where evaluation is suspended and memorized. The reduction semantics are
small-step and evaluation is suspended and memorized locally in let-bindings.
Thus two styles of formalization describe the call-by-need strategy from
different angles.
  The natural semantics for the acyclic calculus is revised from the previous
presentation by Maraist et al. and its adequacy is ascribed to its
correspondence with the reduction semantics, which has been proved equivalent
to call-by-name by Ariola and Felleisen. The natural semantics for the cyclic
calculus is inspired by that of Launchbury and Sestoft and we state its
adequacy using a denotational semantics in the style of Launchbury; adequacy of
the reduction semantics for the cyclic calculus is in turn ascribed to its
correspondence with the natural semantics."
"A program is usually represented as a word chain. It is exactly a word chain
that appears as the lexical analyzer output and is parsed. The work shows that
a program can be syntactically represented as an oriented word tree, that is a
syntactic program tree, program words being located both in tree nodes and on
tree arrows. The basic property of a tree is that arrows starting from each
node are marked by different words (including an empty word). Semantics can
then be directly specified on such tree using either requirements or additional
links, and adding instructions to some tree nodes enables program execution
specification."
"Software products evolve over time. Sometimes they evolve by adding new
features, and sometimes by either fixing bugs or replacing outdated
implementations with new ones. When software engineers fail to anticipate such
evolution during development, they will eventually be forced to re-architect or
re-build from scratch. Therefore, it has been common practice to prepare for
changes so that software products are extensible over their lifetimes. However,
making software extensible is challenging because it is difficult to anticipate
successive changes and to provide adequate abstraction mechanisms over
potential changes. Such extensibility mechanisms, furthermore, should not
compromise any existing functionality during extension. Software engineers
would benefit from a tool that provides a way to add extensions in a reliable
way. It is natural to expect programming languages to serve this role.
Extensible programming is one effort to address these issues.
  In this thesis, we present type safe extensible programming using the MLPolyR
language. MLPolyR is an ML-like functional language whose type system provides
type-safe extensibility mechanisms at several levels. After presenting the
language, we will show how these extensibility mechanisms can be put to good
use in the context of product line engineering. Product line engineering is an
emerging software engineering paradigm that aims to manage variations, which
originate from successive changes in software."
"We propose a method for encoding iterators (and recursion operators in
general) using interaction nets (INs). There are two main applications for
this: the method can be used to obtain a visual nota- tion for functional
programs; and it can be used to extend the existing translations of the
lambda-calculus into INs to languages with recursive types."
"A great variety of static analyses that compute safety properties of
single-thread programs have now been developed. This paper presents a
systematic method to extend a class of such static analyses, so that they
handle programs with multiple POSIX-style threads. Starting from a pragmatic
operational semantics, we build a denotational semantics that expresses
reasoning a la assume-guarantee. The final algorithm is then derived by
abstract interpretation. It analyses each thread in turn, propagating
interferences between threads, in addition to other semantic information. The
combinatorial explosion, ensued from the explicit consideration of all
interleavings, is thus avoided. The worst case complexity is only increased by
a factor n compared to the single-thread case, where n is the number of
instructions in the program. We have implemented prototype tools, demonstrating
the practicality of the approach."
"In this report, we show how to use the Simple Fluent Calculus (SFC) to
specify generic tracers, i.e. tracers which produce a generic trace. A generic
trace is a trace which can be produced by different implementations of a
software component and used independently from the traced component. This
approach is used to define a method for extending a java based CHRor platform
called CHROME (Constraint Handling Rule Online Model-driven Engine) with an
extensible generic tracer. The method includes a tracer specification in SFC, a
methodology to extend it, and the way to integrate it with CHROME, resulting in
the platform CHROME-REF (for Reasoning Explanation Facilities), which is a
constraint solving and rule based reasoning engine with explanatory traces."
"Provenance, or information about the sources, derivation, custody or history
of data, has been studied recently in a number of contexts, including
databases, scientific workflows and the Semantic Web. Many provenance
mechanisms have been developed, motivated by informal notions such as
influence, dependence, explanation and causality. However, there has been
little study of whether these mechanisms formally satisfy appropriate policies
or even how to formalize relevant motivating concepts such as causality. We
contend that mathematical models of these concepts are needed to justify and
compare provenance techniques. In this paper we review a theory of causality
based on structural models that has been developed in artificial intelligence,
and describe work in progress on a causal semantics for provenance graphs."
"Garbage collectors are notoriously hard to verify, due to their low-level
interaction with the underlying system and the general difficulty in reasoning
about reachability in graphs. Several papers have presented verified
collectors, but either the proofs were hand-written or the collectors were too
simplistic to use on practical applications. In this work, we present two
mechanically verified garbage collectors, both practical enough to use for
real-world C# benchmarks. The collectors and their associated allocators
consist of x86 assembly language instructions and macro instructions, annotated
with preconditions, postconditions, invariants, and assertions. We used the
Boogie verification generator and the Z3 automated theorem prover to verify
this assembly language code mechanically. We provide measurements comparing the
performance of the verified collector with that of the standard Bartok
collectors on off-the-shelf C# benchmarks, demonstrating their competitiveness."
"Optimizations in a traditional compiler are applied sequentially, with each
optimization destructively modifying the program to produce a transformed
program that is then passed to the next optimization. We present a new approach
for structuring the optimization phase of a compiler. In our approach,
optimizations take the form of equality analyses that add equality information
to a common intermediate representation. The optimizer works by repeatedly
applying these analyses to infer equivalences between program fragments, thus
saturating the intermediate representation with equalities. Once saturated, the
intermediate representation encodes multiple optimized versions of the input
program. At this point, a profitability heuristic picks the final optimized
program from the various programs represented in the saturated representation.
Our proposed way of structuring optimizers has a variety of benefits over
previous approaches: our approach obviates the need to worry about optimization
ordering, enables the use of a global optimization heuristic that selects among
fully optimized programs, and can be used to perform translation validation,
even on compilers other than our own. We present our approach, formalize it,
and describe our choice of intermediate representation. We also present
experimental results showing that our approach is practical in terms of time
and space overhead, is effective at discovering intricate optimization
opportunities, and is effective at performing translation validation for a
realistic optimizer."
"We present Babel-17, the first programming language for purely functional
structured programming (PFSP). Earlier work illustrated PFSP in the framework
of a toy research language. Babel-17 takes this earlier work to a new level by
showing how PFSP can be combined with pattern matching, object oriented
programming, and features like concurrency, lazy evaluation, memoization and
support for lenses."
"ECLiPSe is a Prolog-based programming system, aimed at the development and
deployment of constraint programming applications. It is also used for teaching
most aspects of combinatorial problem solving, e.g. problem modelling,
constraint programming, mathematical programming, and search techniques. It
uses an extended Prolog as its high-level modelling and control language,
complemented by several constraint solver libraries, interfaces to third-party
solvers, an integrated development environment and interfaces for embedding
into host environments. This paper discusses language extensions,
implementation aspects, components and tools that we consider relevant on the
way from Logic Programming to Constraint Logic Programming."
"Correctness of program transformations in extended lambda calculi with a
contextual semantics is usually based on reasoning about the operational
semantics which is a rewrite semantics. A successful approach to proving
correctness is the combination of a context lemma with the computation of
overlaps between program transformations and the reduction rules, and then of
so-called complete sets of diagrams. The method is similar to the computation
of critical pairs for the completion of term rewriting systems. We explore
cases where the computation of these overlaps can be done in a first order way
by variants of critical pair computation that use unification algorithms. As a
case study we apply the method to a lambda calculus with recursive
let-expressions and describe an effective unification algorithm to determine
all overlaps of a set of transformations with all reduction rules. The
unification algorithm employs many-sorted terms, the equational theory of
left-commutativity modelling multi-sets, context variables of different kinds
and a mechanism for compactly representing binding chains in recursive
let-expressions."
"This paper proposes a type-and-effect system called Teqt, which distinguishes
terminating terms and total functions from possibly diverging terms and partial
functions, for a lambda calculus with general recursion and equality types. The
central idea is to include a primitive type-form ""Terminates t"", expressing
that term t is terminating; and then allow terms t to be coerced from possibly
diverging to total, using a proof of Terminates t. We call such coercions
termination casts, and show how to implement terminating recursion using them.
For the meta-theory of the system, we describe a translation from Teqt to a
logical theory of termination for general recursive, simply typed functions.
Every typing judgment of Teqt is translated to a theorem expressing the
appropriate termination property of the computational part of the Teqt term."
"The paradigm of Tabled Logic Programming (TLP) is now supported by a number
of Prolog systems, including XSB, YAP Prolog, B-Prolog, Mercury, ALS, and Ciao.
The reasons for this are partly theoretical: tabling ensures termination and
optimal known complexity for queries to a large class of programs. However the
overriding reasons are practical. TLP allows sophisticated programs to be
written concisely and efficiently, especially when mechanisms such as tabled
negation and call and answer subsumption are supported. As a result TLP has now
been used in a variety of applications from program analysis to querying over
the semantic web. This paper provides a survey of TLP and its applications as
implemented in XSB Prolog, along with discussion of how XSB supports tabling
with dynamically changing code, and in a multi-threaded environment."
"This index covers the lecture notes and the final course project reports for
COMP6411 Summer 2010 at Concordia University, Montreal, Canada, Comparative
Study of Programming Languages by 4 teams trying compare a set of common
criteria and their applicability to about 10 distinct programming languages,
where 5 language choices were provided by the instructor and five were picked
by each team and each student individually compared two of the 10 and then the
team did a summary synthesis across all 10 languages. Their findings are posted
here for further reference, comparative studies, and analysis."
"We describe a strategy language to control the application of graph rewriting
rules, and show how this language can be used to write high-level declarative
programs in several application areas. This language is part of a graph-based
programming tool built within the port-graph transformation and visualisation
environment PORGY."
"B-Prolog is a high-performance implementation of the standard Prolog language
with several extensions including matching clauses, action rules for event
handling, finite-domain constraint solving, arrays and hash tables, declarative
loop constructs, and tabling. The B-Prolog system is based on the TOAM
architecture which differs from the WAM mainly in that (1) arguments are passed
old-fashionedly through the stack, (2) only one frame is used for each
predicate call, and (3) instructions are provided for encoding matching trees.
The most recent architecture, called TOAM Jr., departs further from the WAM in
that it employs no registers for arguments or temporary variables, and provides
variable-size instructions for encoding predicate calls. This paper gives an
overview of the language features and a detailed description of the TOAM Jr.
architecture, including architectural support for action rules and tabling."
"We present a new approach to automated reasoning about higher-order programs
by extending symbolic execution to use behavioral contracts as symbolic values,
enabling symbolic approximation of higher-order behavior.
  Our approach is based on the idea of an abstract reduction semantics that
gives an operational semantics to programs with both concrete and symbolic
components. Symbolic components are approximated by their contract and our
semantics gives an operational interpretation of contracts-as-values. The
result is a executable semantics that soundly predicts program behavior,
including contract failures, for all possible instantiations of symbolic
components. We show that our approach scales to an expressive language of
contracts including arbitrary programs embedded as predicates, dependent
function contracts, and recursive contracts. Supporting this feature-rich
language of specifications leads to powerful symbolic reasoning using existing
program assertions.
  We then apply our approach to produce a verifier for contract correctness of
components, including a sound and computable approximation to our semantics
that facilitates fully automated contract verification. Our implementation is
capable of verifying contracts expressed in existing programs, and of
justifying valuable contract-elimination optimizations."
"This paper gives two new categorical characterisations of lenses: one as a
coalgebra of the store comonad, and the other as a monoidal natural
transformation on a category of a certain class of coalgebras. The store
comonad of the first characterisation can be generalized to a Cartesian store
comonad, and the coalgebras of this Cartesian store comonad turn out to be
exactly the Biplates of the Uniplate generic programming library. On the other
hand, the monoidal natural transformations on functors can be generalized to
work on a category of more specific coalgebras. This generalization turns out
to be the type of compos from the Compos generic programming library. A
theorem, originally conjectured by van Laarhoven, proves that these two
generalizations are isomorphic, thus the core data types of the Uniplate and
Compos libraries supporting generic program on single recursive types are the
same. Both the Uniplate and Compos libraries generalize this core functionality
to support mutually recursive types in different ways. This paper proposes a
third extension to support mutually recursive data types that is as powerful as
Compos and as easy to use as Uniplate. This proposal, called Multiplate, only
requires rank 3 polymorphism in addition to the normal type class mechanism of
Haskell."
"We describe a framework to support the implementation of web-based systems
intended to manipulate data stored in relational databases. Since the
conceptual model of a relational database is often specified as an
entity-relationship (ER) model, we propose to use the ER model to generate a
complete implementation in the declarative programming language Curry. This
implementation contains operations to create and manipulate entities of the
data model, supports authentication, authorization, session handling, and the
composition of individual operations to user processes. Furthermore, the
implementation ensures the consistency of the database w.r.t. the data
dependencies specified in the ER model, i.e., updates initiated by the user
cannot lead to an inconsistent state of the database. In order to generate a
high-level declarative implementation that can be easily adapted to individual
customer requirements, the framework exploits previous works on declarative
database programming and web user interface construction in Curry."
"Programs written in dynamic languages make heavy use of features --- run-time
type tests, value-indexed dictionaries, polymorphism, and higher-order
functions --- that are beyond the reach of type systems that employ either
purely syntactic or purely semantic reasoning. We present a core calculus,
System D, that merges these two modes of reasoning into a single powerful
mechanism of nested refinement types wherein the typing relation is itself a
predicate in the refinement logic. System D coordinates SMT-based logical
implication and syntactic subtyping to automatically typecheck sophisticated
dynamic language programs. By coupling nested refinements with McCarthy's
theory of finite maps, System D can precisely reason about the interaction of
higher-order functions, polymorphism, and dictionaries. The addition of type
predicates to the refinement logic creates a circularity that leads to unique
technical challenges in the metatheory, which we solve with a novel
stratification approach that we use to prove the soundness of System D."
"We develop a framework for computing two foundational analyses for concurrent
higher-order programs: (control-)flow analysis (CFA) and may-happen-in-parallel
analysis (MHP). We pay special attention to the unique challenges posed by the
unrestricted mixture of first-class continuations and dynamically spawned
threads. To set the stage, we formulate a concrete model of concurrent
higher-order programs: the P(CEK*)S machine. We find that the systematic
abstract interpretation of this machine is capable of computing both flow and
MHP analyses. Yet, a closer examination finds that the precision for MHP is
poor. As a remedy, we adapt a shape analytic technique-singleton abstraction-to
dynamically spawned threads (as opposed to objects in the heap). We then show
that if MHP analysis is not of interest, we can substantially accelerate the
computation of flow analysis alone by collapsing thread interleavings with a
second layer of abstraction."
"Dynamic software adaptability is one of the central features leveraged by
autonomic computing. However, developing software that changes its behavior at
run time adapting to the operational conditions is a challenging task. Several
approaches have been proposed in the literature to attack this problem at
different and complementary abstraction levels: software architecture,
middleware, and programming level. We focus on the support that ad-hoc
programming language constructs may provide to support dynamically adaptive
behaviors. We introduce context-oriented programming languages and we present a
framework that positions the supported paradigm in the MAPE-K autonomic loop.
We discuss the advantages of using context-oriented programming languages
instead of other mainstream approaches based on dynamic aspect oriented
programming languages and present a case study that shows how the proposed
programming style naturally fits dynamic adaptation requirements. Finally, we
discuss some known problems and outline a number of open research challenges."
"Problems in program analysis can be solved by developing novel program
semantics and deriving abstractions conventionally. For over thirty years,
higher-order program analysis has been sold as a hard problem. Its solutions
have required ingenuity and complex models of approximation. We claim that this
difficulty is due to premature focus on abstraction and propose a new approach
that emphasizes semantics. Its simplicity enables new analyses that are beyond
the current state of the art."
"Predictive models are fundamental to engineering reliable software systems.
However, designing conservative, computable approximations for the behavior of
programs (static analyses) remains a difficult and error-prone process for
modern high-level programming languages. What analysis designers need is a
principled method for navigating the gap between semantics and analytic models:
analysis designers need a method that tames the interaction of complex
languages features such as higher-order functions, recursion, exceptions,
continuations, objects and dynamic allocation.
  We contribute a systematic approach to program analysis that yields novel and
transparently sound static analyses. Our approach relies on existing
derivational techniques to transform high-level language semantics into
low-level deterministic state-transition systems (with potentially infinite
state spaces). We then perform a series of simple machine refactorings to
obtain a sound, computable approximation, which takes the form of a
non-deterministic state-transition systems with finite state spaces. The
approach scales up uniformly to enable program analysis of realistic language
features, including higher-order functions, tail calls, conditionals, side
effects, exceptions, first-class continuations, and even garbage collection."
"Step-indexed semantic models of types were proposed as an alternative to
purely syntactic safety proofs using subject-reduction. Building upon the work
by Appel and others, we introduce a generalized step-indexed model for the
call-by-name lambda calculus. We also show how to prove type safety of general
recursion in our call-by-name model."
"Modern high-end machines feature multiple processor packages, each of which
contains multiple independent cores and integrated memory controllers connected
directly to dedicated physical RAM. These packages are connected via a shared
bus, creating a system with a heterogeneous memory hierarchy. Since this shared
bus has less bandwidth than the sum of the links to memory, aggregate memory
bandwidth is higher when parallel threads all access memory local to their
processor package than when they access memory attached to a remote package.
This bandwidth limitation has traditionally limited the scalability of modern
functional language implementations, which seldom scale well past 8 cores, even
on small benchmarks.
  This work presents a garbage collector integrated with our strict, parallel
functional language implementation, Manticore, and shows that it scales
effectively on both a 48-core AMD Opteron machine and a 32-core Intel Xeon
machine."
"This work is multifold. We review the historical literature on the Lucid
programming language, its dialects, intensional logic, intensional programming,
the implementing systems, and context-oriented and context-aware computing and
so on that provide a contextual framework for the converging Core Lucid
standard programming model. We are designing a standard specification of a
baseline Lucid virtual machine for generic execution of Lucid programs. The
resulting Core Lucid language would inherit the properties of generalization
attempts of GIPL (1999-2013) and TransLucid (2008-2013) for all future and
recent Lucid implementing systems to follow. We also maintain this work across
local research group in order to foster deeper collaboration, maintain a list
of recent and historical bibliography and a reference manual and reading list
for students. We form a (for now informal) SIGLUCID group to keep track of this
standard and historical records with eventual long-term goal through iterative
revisions for this work to become a book or an encyclopedia of the referenced
topics, and perhaps, an RFC. We first begin small with this initial set of
notes."
"Symbolic execution is a successful and very popular technique used in
software verification and testing. A key limitation of symbolic execution is in
dealing with code containing loops. The problem is that even a single loop can
generate a huge number of different symbolic execution paths, corresponding to
different number of loop iterations and taking various paths through the loop.
  We introduce a technique which, given a start location above some loops and a
target location anywhere below these loops, returns a feasible path between
these two locations, if such a path exists. The technique infers a collection
of constraint systems from the program and uses them to steer the symbolic
execution towards the target. On reaching a loop it iteratively solves the
appropriate constraint system to find out which path through this loop to take,
or, alternatively, whether to continue below the loop. To construct the
constraint systems we express the values of variables modified in a loop as
functions of the number of times a given path through the loop was executed.
  We have built a prototype implementation of our technique and compared it to
state-of-the-art symbolic execution tools on simple programs with loops. The
results show significant improvements in the running time. We found instances
where our algorithm finished in seconds, whereas the other tools did not finish
within an hour. Our approach also shows very good results in the case when the
target location is not reachable by any feasible path."
"Emerging GPU architectures for high performance computing are well suited to
a data-parallel programming model. This paper presents preliminary work
examining a programming methodology that provides Fortran programmers with
access to these emerging systems. We use array constructs in Fortran to show
how this infrequently exploited, standardized language feature is easily
transformed to lower-level accelerator code. The transformations in ForOpenCL
are based on a simple mapping from Fortran to OpenCL. We demonstrate, using a
stencil code solving the shallow-water fluid equations, that the performance of
the ForOpenCL compiler-generated transformations is comparable with that of
hand-optimized OpenCL code."
"REC (REGULAR EXPRESSION COMPILER) is a programming language of simple
structure developed originally for the PDP-8 computer of the Digital Equipment,
Corporation, but readily adaptable to any other general purpose computer. It
has been used extensively in teaching Algebra and Numerical Analysis in the
Escuela Superior de F\'isica y Matem\'aticas of the Instituto Polit\'ecnico
Nacional. Moreover, the fact that the same control language, REC, is equally
applicable and equally efficient over the whole range of computer facilities
available to the students gives a very welcome coherence to the entire teaching
program, including the course of Mathematical Logic which is devoted to the
theoretical aspects of such matters.
  REC; derives its appeal from the fact that computers can be regarded
reasonably well as Turing Machines. The REC notation is simply a manner of
writing regular expression, somewhat more amenable to programming the Turing
Machine which they control. If one does not wish to think so strictly in terms
of Turing Machines, REC expressions still provide a means of defining the flow
of control in a program which is quite convenient for many applications."
"A new behavior descriptive entity type called spec is proposed, which
combines the traditional interface with test rules and test cases, to
completely specify the desired behavior of each method, and to enforce the
behavior-wise correctness of all compiled units. Using spec, a new programming
paradigm is proposed, which allows the separation programming space into 1) a
behavior domain to aggregate all behavior programming in the format of specs,
2) a object domain to bind each concrete spec to its data representation in a
particular address space, and 3) a realization domain to connect the behavior
domain and the object domain. Such separation guarantees the strictness of
behavior satisfaction at compile time, while allows flexibility of dynamical
binding of actual implementation at runtime. A new convention call type
expressiveness to allow data exchange between different programming languages
and between different software environments is also proposed."
"We describe a derivational approach to abstract interpretation that yields
novel and transparently sound static analyses when applied to well-established
abstract machines for higher-order and imperative programming languages. To
demonstrate the technique and support our claim, we transform the CEK machine
of Felleisen and Friedman, a lazy variant of Krivine's machine, and the
stack-inspecting CM machine of Clements and Felleisen into abstract
interpretations of themselves. The resulting analyses bound temporal ordering
of program events; predict return-flow and stack-inspection behavior; and
approximate the flow and evaluation of by-need parameters. For all of these
machines, we find that a series of well-known concrete machine refactorings,
plus a technique of store-allocated continuations, leads to machines that
abstract into static analyses simply by bounding their stores. We demonstrate
that the technique scales up uniformly to allow static analysis of realistic
language features, including tail calls, conditionals, side effects,
exceptions, first-class continuations, and even garbage collection. In order to
close the gap between formalism and implementation, we provide translations of
the mathematics as running Haskell code for the initial development of our
method."
"Prolog's very useful expressive power is not captured by traditional logic
programming semantics, due mainly to the cut and goal and clause order. Several
alternative semantics have been put forward, exposing operational details of
the computation state. We propose instead to redesign Prolog around structured
alternatives to the cut and clauses, keeping the expressive power and
computation model but with a compositional denotational semantics over much
simpler states-just variable bindings. This considerably eases reasoning about
programs, by programmers and tools such as a partial evaluator, with safe
unfolding of calls through predicate definitions. An if-then-else across
clauses replaces most uses of the cut, but the cut's full power is achieved by
an until construct. Disjunction, conjunction and until, along with unification,
are the primitive goal types with a compositional semantics yielding sequences
of variable-binding solutions. This extends to programs via the usual technique
of a least fixpoint construction. A simple interpreter for Prolog in the
alternative language, and a definition of until in Prolog, establish the
identical expressive power of the two languages. Many useful control constructs
are derivable from the primitives, and the semantic framework illuminates the
discussion of alternative ones. The formalisation rests on a term language with
variable abstraction as in the {\lambda}-calculus. A clause is an abstraction
on the call arguments, a continuation, and the local variables. It can be
inclusive or exclusive, expressing a local case bound to a continuation by
either a disjunction or an if-then-else. Clauses are open definitions, composed
(and closed) with simple functional application ({\beta}-reduction). This paves
the way for a simple account of flexible module composition mechanisms. Cube, a
concrete language with the exposed principles, has been implemented."
"Tabled evaluation is an implementation technique that solves some problems of
traditional Prolog systems in dealing with recursion and redundant
computations. Most tabling engines determine if a tabled subgoal will produce
or consume answers by using variant checks. A more refined method, named call
subsumption, considers that a subgoal A will consume from a subgoal B if A is
subsumed by (an instance of) B, thus allowing greater answer reuse. We recently
developed an extension, called Retroactive Call Subsumption, that improves upon
call subsumption by supporting bidirectional sharing of answers between
subsumed/subsuming subgoals. In this paper, we present both an algorithm and an
extension to the table space data structures to efficiently implement instance
retrieval of subgoals for subsumptive tabled evaluation of logic programs.
Experiments results using the YapTab tabling system show that our
implementation performs quite well on some complex benchmarks and is robust
enough to handle a large number of subgoals without performance degradation."
"The C++ Standard Template Library is the flagship example for libraries based
on the generic programming paradigm. The usage of this library is intended to
minimize the number of classical C/C++ errors, but does not warrant bug-free
programs. Furthermore, many new kinds of errors may arise from the inaccurate
use of the generic programming paradigm, like dereferencing invalid iterators
or misunderstanding remove-like algorithms. In this paper we present some
typical scenarios that may cause runtime or portability problems. We emit
warnings and errors while these risky constructs are used. We also present a
general approach to emit ""customized"" warnings. We support the so-called
""believe-me marks"" to disable warnings. We present another typical usage of our
technique, when classes become deprecated during the software lifecycle."
"An optimizing compiler consists of a front end parsing a textual programming
language into an intermediate representation (IR), a middle end performing
optimizations on the IR, and a back end lowering the IR to a target
representation (TR) built of operations supported by the target hardware. In
modern compiler construction graph-based IRs are employed. Optimization and
lowering tasks can then be implemented with graph transformation rules. This
case provides two compiler tasks to evaluate the participating tools regarding
performance."
"The challenge of the Compiler Optimization Case is to perform local
optimizations and instruction selection on the graph-based intermediate
representation of a compiler. The case is designed to compare participating
tools regarding their performance. We tackle this task employing the general
purpose graph rewrite system GrGen.NET (www.grgen.net)."
"The challenge of the Reengineering Case is to extract a state machine model
out of the abstract syntax graph of a Java program. The extracted state machine
offers a reduced view on the full program graph and thus helps to understand
the program regarding the question of interest. We tackle this task employing
the general purpose graph rewrite system GrGen.NET (www.grgen.net)."
"We introduce the graph transformation tool GrGen.NET (www.grgen.net) by
solving the Hello World Case of the Transformation Tool Contest 2011 which
consists of a collection of small transformation tasks; for each task a section
is given explaining our implementation."
"Robustness is a standard correctness property which intuitively means that if
the input to the program changes less than a fixed small amount then the output
changes only slightly. This notion is useful in the analysis of rounding error
for floating point programs because it helps to establish bounds on output
errors introduced by both measurement errors and by floating point computation.
Compositional methods often do not work since key constructs---like the
conditional and the while-loop---are not robust. We propose a method for
proving the robustness of a while-loop. This method is non-local in the sense
that instead of breaking the analysis down to single lines of code, it checks
certain global properties of its structure. We show the applicability of our
method on two standard algorithms: the CORDIC computation of the cosine and
Dijkstra's shortest path algorithm."
"We present a calculus that models a form of process interaction based on
copyless message passing, in the style of Singularity OS. The calculus is
equipped with a type system ensuring that well-typed processes are free from
memory faults, memory leaks, and communication errors. The type system is
essentially linear, but we show that linearity alone is inadequate, because it
leaves room for scenarios where well-typed processes leak significant amounts
of memory. We address these problems basing the type system upon an original
variant of session types."
"Is there any Cartesian-closed category of continuous domains that would be
closed under Jones and Plotkin's probabilistic powerdomain construction? This
is a major open problem in the area of denotational semantics of probabilistic
higher-order languages. We relax the question, and look for quasi-continuous
dcpos instead. We introduce a natural class of such quasi-continuous dcpos, the
omega-QRB-domains. We show that they form a category omega-QRB with pleasing
properties: omega-QRB is closed under the probabilistic powerdomain functor,
under finite products, under taking bilimits of expanding sequences, under
retracts, and even under so-called quasi-retracts. But... omega-QRB is not
Cartesian closed. We conclude by showing that the QRB domains are just one half
of an FS-domain, merely lacking control."
"This paper defines a notion of binding trees that provide a suitable model
for second-order type systems with F-bounded quantifiers and equirecursive
types. It defines a notion of regular binding trees that correspond in the
right way to notions of regularity in the first-order case. It defines a notion
of subtyping on these trees and proves various properties of the subtyping
relation. It defines a mapping from types to trees and shows that types produce
regular binding trees. It presents a set of type equality and subtyping rules,
and proves them sound and complete with respect to the tree interpretation. It
defines a notion of binding-tree automata and how these generate regular
binding trees. It gives a polynomial-time algorithm for deciding when two
automata's trees are in the subtyping relation."
"In previous work we have illustrated the benefits that compositional data
types (CDTs) offer for implementing languages and in general for dealing with
abstract syntax trees (ASTs). Based on Swierstra's data types \'a la carte,
CDTs are implemented as a Haskell library that enables the definition of
recursive data types and functions on them in a modular and extendable fashion.
Although CDTs provide a powerful tool for analysing and manipulating ASTs, they
lack a convenient representation of variable binders. In this paper we remedy
this deficiency by combining the framework of CDTs with Chlipala's parametric
higher-order abstract syntax (PHOAS). We show how a generalisation from
functors to difunctors enables us to capture PHOAS while still maintaining the
features of the original implementation of CDTs, in particular its modularity.
Unlike previous approaches, we avoid so-called exotic terms without resorting
to abstract types: this is crucial when we want to perform transformations on
CDTs that inspect the recursively computed CDTs, e.g. constant folding."
"Traversals of data structures are ubiquitous in programming. Consequently, it
is important to be able to characterise those structures that are traversable
and understand their algebraic properties. Traversable functors have been
characterised by McBride and Paterson as those equipped with a distributive law
over arbitrary applicative functors; however, laws that fully capture the
intuition behind traversals are missing. This article is an attempt to remedy
this situation by proposing laws for characterising traversals that capture the
intuition behind them. To support our claims, we prove that finitary containers
are traversable in our sense and argue that elements in a traversable structure
are visited exactly once."
"Datatype-generic programming increases program abstraction and reuse by
making functions operate uniformly across different types. Many approaches to
generic programming have been proposed over the years, most of them for
Haskell, but recently also for dependently typed languages such as Agda.
Different approaches vary in expressiveness, ease of use, and implementation
techniques.
  Some work has been done in comparing the different approaches informally.
However, to our knowledge there have been no attempts to formally prove
relations between different approaches. We thus present a formal comparison of
generic programming libraries. We show how to formalise different approaches in
Agda, including a coinductive representation, and then establish theorems that
relate the approaches to each other. We provide constructive proofs of
inclusion of one approach in another that can be used to convert between
approaches, helping to reduce code duplication across different libraries. Our
formalisation also helps in providing a clear picture of the potential of each
approach, especially in relating different generic views and their
expressiveness."
"Monads have become a powerful tool for structuring effectful computations in
functional programming, because they make the order of effects explicit. When
translating pure code to a monadic version, we need to specify evaluation order
explicitly. Two standard translations give call-by-value and call-by-name
semantics. The resulting programs have different structure and types, which
makes revisiting the choice difficult.
  In this paper, we translate pure code to monadic using an additional
operation malias that abstracts out the evaluation strategy. The malias
operation is based on computational comonads; we use a categorical framework to
specify the laws that are required to hold about the operation.
  For any monad, we show implementations of malias that give call-by-value and
call-by-name semantics. Although we do not give call-by-need semantics for all
monads, we show how to turn certain monads into an extended monad with
call-by-need semantics, which partly answers an open question. Moreover, using
our unified translation, it is possible to change the evaluation strategy of
functional code translated to the monadic form without changing its structure
or types."
"In functional programming, monads are supposed to encapsulate computations,
effectfully producing the final result, but keeping to themselves the means of
acquiring it. For various reasons, we sometimes want to reveal the internals of
a computation. To make that possible, in this paper we introduce monad
transformers that add the ability to automatically accumulate observations
about the course of execution as an effect. We discover that if we treat the
resulting trace as the actual result of the computation, we can find new
functionality in existing monads, notably when working with non-terminating
computations."
"We present a full-spectrum dependently typed core language which includes
both nontermination and computational irrelevance (a.k.a. erasure), a
combination which has not been studied before. The two features interact: to
protect type safety we must be careful to only erase terminating expressions.
Our language design is strongly influenced by the choice of CBV evaluation, and
by our novel treatment of propositional equality which has a heterogeneous,
completely erased elimination form."
"This paper presents the derivation of an executable Krivine abstract machine
from a small step interpreter for the simply typed lambda calculus in the
dependently typed programming language Agda."
"This paper is a companion technical report to the article
""Continuation-Passing C: from threads to events through continuations"". It
contains the complete version of the proofs of correctness of lambda-lifting
and CPS-conversion presented in the article."
"Register allocation has long been formulated as a graph coloring problem,
coloring the conflict graph with physical registers. Such a formulation does
not fully capture the goal of the allocation, which is to minimize the traffic
between registers and memory. Linear scan has been proposed as an alternative
to graph coloring, but in essence, it can be viewed as a greedy algorithm for
graph coloring: coloring the vertices not in the order of their degrees, but in
the order of their occurence in the program. Thus it suffers from almost the
same constraints as graph coloring. In this article, I propose a new method of
register allocation based on the ideas of model transformer semantics (MTS) and
static cache replacement (SCR). Model transformer semantics captures the
semantics of registers and the stack. Static cache replacement relaxes the
assumptions made by graph coloring and linear scan, aiming directly at reducing
register-memory traffic. The method explores a much larger solution space than
that of graph coloring and linear scan, thus providing more opportunities of
optimization. It seamlessly performs live range splitting, an optimization
found in extensions to graph coloring and linear scan. Also, it simplifies the
compiler, and its semantics-based approach provides possibilities of
simplifying the formal verification of compilers."
"We define a notion of normal form bisimilarity for the untyped call-by-value
lambda calculus extended with the delimited-control operators shift and reset.
Normal form bisimilarities are simple, easy-to-use behavioral equivalences
which relate terms without having to test them within all contexts (like
contextual equivalence), or by applying them to function arguments (like
applicative bisimilarity). We prove that the normal form bisimilarity for shift
and reset is sound but not complete w.r.t. contextual equivalence and we define
up-to techniques that aim at simplifying bisimulation proofs. Finally, we
illustrate the simplicity of the techniques we develop by proving several
equivalences on terms."
"Model-based parser generators decouple language specification from language
processing. The model-driven approach avoids the limitations that conventional
parser generators impose on the language designer. Conventional tools require
the designed language grammar to conform to the specific kind of grammar
supported by the particular parser generator (being LL and LR parser generators
the most common). Model-driven parser generators, like ModelCC, do not require
a grammar specification, since that grammar can be automatically derived from
the language model and, if needed, adapted to conform to the requirements of
the given kind of parser, all of this without interfering with the conceptual
design of the language and its associated applications. Moreover, model-driven
tools such as ModelCC are able to automatically resolve references between
language elements, hence producing abstract syntax graphs instead of abstract
syntax trees as the result of the parsing process. Such graphs are not confined
to directed acyclic graphs and they can contain cycles, since ModelCC supports
anaphoric, cataphoric, and recursive references."
"In the static analysis of functional programs, pushdown flow analysis and
abstract garbage collection skirt just inside the boundaries of soundness and
decidability. Alone, each method reduces analysis times and boosts precision by
orders of magnitude. This work illuminates and conquers the theoretical
challenges that stand in the way of combining the power of these techniques.
The challenge in marrying these techniques is not subtle: computing the
reachable control states of a pushdown system relies on limiting access during
transition to the top of the stack; abstract garbage collection, on the other
hand, needs full access to the entire stack to compute a root set, just as
concrete collection does. \emph{Introspective} pushdown systems resolve this
conflict. Introspective pushdown systems provide enough access to the stack to
allow abstract garbage collection, but they remain restricted enough to compute
control-state reachability, thereby enabling the sound and precise product of
pushdown analysis and abstract garbage collection. Experiments reveal
synergistic interplay between the techniques, and the fusion demonstrates
""better-than-both-worlds"" precision."
"We propose two operations to prevent sharing in Haskell that do not require
modifying the data generating code, demonstrate their use and usefulness, and
compare them to other approaches to preventing sharing. Our claims are
supported by a formal semantics and a prototype implementation."
"Previous approaches to systematic state-space exploration for testing
multi-threaded programs have proposed context-bounding and depth-bounding to be
effective ranking algorithms for testing multithreaded programs. This paper
proposes two new metrics to rank thread schedules for systematic state-space
exploration. Our metrics are based on characterization of a concurrency bug
using v (the minimum number of distinct variables that need to be involved for
the bug to manifest) and t (the minimum number of distinct threads among which
scheduling constraints are required to manifest the bug). Our algorithm is
based on the hypothesis that in practice, most concurrency bugs have low v
(typically 1- 2) and low t (typically 2-4) characteristics. We iteratively
explore the search space of schedules in increasing orders of v and t. We show
qualitatively and empirically that our algorithm finds common bugs in fewer
number of execution runs, compared with previous approaches. We also show that
using v and t improves the lower bounds on the probability of finding bugs
through randomized algorithms.
  Systematic exploration of schedules requires instrumenting each variable
access made by a program, which can be very expensive and severely limits the
applicability of this approach. Previous work [5, 19] has avoided this problem
by interposing only on synchronization operations (and ignoring other variable
accesses). We demonstrate that by using variable bounding (v) and a static
imprecise alias analysis, we can interpose on all variable accesses (and not
just synchronization operations) at 10-100x less overhead than previous
approaches."
"The aim of this paper is to alter the abstract definition of the program of
the theoretical programming model which has been developed at Eotvos Lorand
University for many years in order to investigate methods that support
designing correct programs. The motivation of this modification was that the
dynamic properties of programs appear in the model. This new definition of the
program gives a hand to extend the model with the concept of subprograms while
the earlier results of the original programming model are preserved."
"We show how logic programs with ""delays"" can be transformed to programs
without delays in a way which preserves information concerning floundering
(also known as deadlock). This allows a declarative (model-theoretic),
bottom-up or goal independent approach to be used for analysis and debugging of
properties related to floundering. We rely on some previously introduced
restrictions on delay primitives and a key observation which allows properties
such as groundness to be analysed by approximating the (ground) success set.
This paper is to appear in Theory and Practice of Logic Programming (TPLP).
  Keywords: Floundering, delays, coroutining, program analysis, abstract
interpretation, program transformation, declarative debugging"
"We consider the problem of computing numerical invariants of programs, for
instance bounds on the values of numerical program variables. More
specifically, we study the problem of performing static analysis by abstract
interpretation using template linear constraint domains. Such invariants can be
obtained by Kleene iterations that are, in order to guarantee termination,
accelerated by widening operators. In many cases, however, applying this form
of extrapolation leads to invariants that are weaker than the strongest
inductive invariant that can be expressed within the abstract domain in use.
Another well-known source of imprecision of traditional abstract interpretation
techniques stems from their use of join operators at merge nodes in the control
flow graph. The mentioned weaknesses may prevent these methods from proving
safety properties. The technique we develop in this article addresses both of
these issues: contrary to Kleene iterations accelerated by widening operators,
it is guaranteed to yield the strongest inductive invariant that can be
expressed within the template linear constraint domain in use. It also eschews
join operators by distinguishing all paths of loop-free code segments. Formally
speaking, our technique computes the least fixpoint within a given template
linear constraint domain of a transition relation that is succinctly expressed
as an existentially quantified linear real arithmetic formula. In contrast to
previously published techniques that rely on quantifier elimination, our
algorithm is proved to have optimal complexity: we prove that the decision
problem associated with our fixpoint problem is in the second level of the
polynomial-time hierarchy."
"We present a procedure for splitting processes in a process algebra with
multi-actions (a subset of the specification language mCRL2). This splitting
procedure cuts a process into two processes along a set of actions A: roughly,
one of these processes contains no actions from A, while the other process
contains only actions from A. We state and prove a theorem asserting that the
parallel composition of these two processes equals the original process under
appropriate synchronization.
  We apply our splitting procedure to the process algebraic semantics of the
coordination language Reo: using this procedure and its related theorem, we
formally establish the soundness of splitting Reo connectors along the
boundaries of their (a)synchronous regions in implementations of Reo. Such
splitting can significantly improve the performance of connectors."
"Non-confluent and non-terminating constructor-based term rewrite systems are
useful for the purpose of specification and programming. In particular,
existing functional logic languages use such kind of rewrite systems to define
possibly non-strict non-deterministic functions. The semantics adopted for
non-determinism is call-time choice, whose combination with non-strictness is a
non trivial issue, addressed years ago from a semantic point of view with the
Constructor-based Rewriting Logic (CRWL), a well-known semantic framework
commonly accepted as suitable semantic basis of modern functional logic
languages. A drawback of CRWL is that it does not come with a proper notion of
one-step reduction, which would be very useful to understand and reason about
how computations proceed. In this paper we develop thoroughly the theory for
the first order version of let-rewriting, a simple reduction notion close to
that of classical term rewriting, but extended with a let-binding construction
to adequately express the combination of call-time choice with non-strict
semantics. Let-rewriting can be seen as a particular textual presentation of
term graph rewriting. We investigate the properties of let-rewriting, most
remarkably their equivalence with respect to a conservative extension of the
CRWL-semantics coping with let-bindings, and we show by some case studies that
having two interchangeable formal views (reduction/semantics) of the same
language is a powerful reasoning tool. After that, we provide a notion of
let-narrowing which is adequate for call-time choice as proved by soundness and
completeness results of let-narrowing with respect to let-rewriting. Moreover,
we relate those let-rewriting and let-narrowing relations (and hence CRWL) with
ordinary term rewriting and narrowing (..)
  To appear in Theory and Practice of Logic Programming (TPLP)."
"The framework of Light Logics has been extensively studied to control the
complexity of higher-order functional programs. We propose an extension of this
framework to multithreaded programs with side effects, focusing on the case of
polynomial time. After introducing a modal \lambda-calculus with parallel
composition and regions, we prove that a realistic call-by-value evaluation
strategy can be computed in polynomial time for a class of well-formed
programs. The result relies on the simulation of call-by-value by a polynomial
shallow-first strategy which preserves the evaluation order of side effects.
Then, we provide a polynomial type system that guarantees that well-typed
programs do not go wrong. Finally, we illustrate the expressivity of the type
system by giving a programming example of concurrent iteration producing side
effects over an inductive data structure."
"Current tabling systems suffer from an increase in space complexity, time
complexity or both when dealing with sequences due to the use of data
structures for tabled subgoals and answers and the need to copy terms into and
from the table area. This symptom can be seen in not only B-Prolog, which uses
hash tables, but also systems that use tries such as XSB and YAP. In this
paper, we apply hash-consing to tabling structured data in B-Prolog. While
hash-consing can reduce the space consumption when sharing is effective, it
does not change the time complexity. We enhance hash-consing with two
techniques, called input sharing and hash code memoization, for reducing the
time complexity by avoiding computing hash codes for certain terms. The
improved system is able to eliminate the extra linear factor in the old system
for processing sequences, thus significantly enhancing the scalability of
applications such as language parsing and bio-sequence analysis applications.
We confirm this improvement with experimental results."
"We present an implementation in the Coq proof assistant of type directed
partial evaluation (TDPE) algorithms for call-by-name and call-by-value
versions of shift and reset delimited control operators, and in presence of
strong sum types. We prove that the algorithm transforms well-typed programs to
ones in normal form. These normal forms can not always be arrived at using the
so far known equational theories. The typing system does not allow answer-type
modification for function types and allows delimiters to be set on at most one
atomic type. The semantic domain for evaluation is expressed in Constructive
Type Theory as a dependently typed monadic structure combining Kripke models
and continuation passing style translations."
"Multi-threading is currently supported by several well-known Prolog systems
providing a highly portable solution for applications that can benefit from
concurrency. When multi-threading is combined with tabling, we can exploit the
power of higher procedural control and declarative semantics. However, despite
the availability of both threads and tabling in some Prolog systems, the
implementation of these two features implies complex ties to each other and to
the underlying engine. Until now, XSB was the only Prolog system combining
multi-threading with tabling. In XSB, tables may be either private or shared
between threads. While thread-private tables are easier to implement, shared
tables have all the associated issues of locking, synchronization and potential
deadlocks. In this paper, we propose an alternative view to XSB's approach. In
our proposal, each thread views its tables as private but, at the engine level,
we use a common table space where tables are shared among all threads. We
present three designs for our common table space approach: No-Sharing (NS)
(similar to XSB's private tables), Subgoal-Sharing (SS) and Full-Sharing (FS).
The primary goal of this work was to reduce the memory usage for the table
space but, our experimental results, using the YapTab tabling system with a
local evaluation strategy, show that we can also achieve significant reductions
on running time."
"Confluence is a fundamental property of Constraint Handling Rules (CHR)
since, as in other rewriting formalisms, it guarantees that the computations
are not dependent on rule application order, and also because it implies the
logical consistency of the program declarative view. In this paper we are
concerned with proving the confluence of non-terminating CHR programs. For this
purpose, we derive from van Oostrom's decreasing diagrams method a novel
criterion on CHR critical pairs that generalizes all preexisting criteria. We
subsequently improve on a result on the modularity of CHR confluence, which
permits modular combinations of possibly non-terminating confluent programs,
without loss of confluence."
"We present and evaluate a compiler from Prolog (and extensions) to JavaScript
which makes it possible to use (constraint) logic programming to develop the
client side of web applications while being compliant with current industry
standards. Targeting JavaScript makes (C)LP programs executable in virtually
every modern computing device with no additional software requirements from the
point of view of the user. In turn, the use of a very high-level language
facilitates the development of high-quality, complex software. The compiler is
a back end of the Ciao system and supports most of its features, including its
module system and its rich language extension mechanism based on packages. We
present an overview of the compilation process and a detailed description of
the run-time system, including the support for modular compilation into
separate JavaScript code. We demonstrate the maturity of the compiler by
testing it with complex code such as a CLP(FD) library written in Prolog with
attributed variables. Finally, we validate our proposal by measuring the
performance of some LP and CLP(FD) benchmarks running on top of major
JavaScript engines."
"Threads and events are two common abstractions for writing concurrent
programs. Because threads are often more convenient, but events more efficient,
it is natural to want to translate the former into the latter. However, whereas
there are many different event-driven styles, existing translators often apply
ad-hoc rules which do not reflect this diversity. We analyse various
control-flow and data-flow encodings in real-world event-driven code, and we
observe that it is possible to generate any of these styles automatically from
threaded code, by applying certain carefully chosen classical program
transformations. In particular, we implement two of these transformations,
lambda lifting and environments, in CPC, an extension of the C language for
writing concurrent systems. Finally, we find out that, although rarely used in
real-world programs because it is tedious to perform manually, lambda lifting
yields better performance than environments in most of our benchmarks."
"Constraint Handling Rules (CHRs) are a high-level rule-based programming
language for specification and implementation of constraint solvers. CHR
manipulates a global store representing a flat conjunction of constraints. By
default, CHR does not support goals with a more complex propositional structure
including disjunction, negation, etc., or CHR relies on the host system to
provide such features. In this paper we introduce Satisfiability Modulo
Constraint Handling Rules (SMCHR): a tight integration of CHR with a modern
Boolean Satisfiability (SAT) solver for quantifier-free formulae with an
arbitrary propositional structure. SMCHR is essentially a Satisfiability Modulo
Theories (SMT) solver where the theory T is implemented in CHR. The execution
algorithm of SMCHR is based on lazy clause generation, where a new clause for
the SAT solver is generated whenever a rule is applied. We shall also explore
the practical aspects of building an SMCHR system, including extending a
""built-in"" constraint solver supporting equality with unification and
justifications."
"While generalized abstract datatypes (GADT) are now considered
well-understood, adding them to a language with a notion of subtyping comes
with a few surprises. What does it mean for a GADT parameter to be covariant?
The answer turns out to be quite subtle. It involves fine-grained properties of
the subtyping relation that raise interesting design questions. We allow
variance annotations in GADT definitions, study their soundness, and present a
sound and complete algorithm to check them. Our work may be applied to
real-world ML-like languages with explicit subtyping such as OCaml, or to
languages with general subtyping constraints."
"By considering an essential subset of the BPEL orchestration language, we
define SeB, a session based style of this subset. We discuss the formal
semantics of SeB and we present its main properties. We use a new approach to
address the formal semantics, based on a translation into so-called control
graphs. Our semantics handles control links and addresses the static semantics
that prescribes the valid usage of variables. We also provide the semantics of
collections of networked services.
  Relying on these semantics, we define precisely what is meant by interaction
safety, paving the way to the formal analysis of safe interactions between BPEL
services."
"We present an elaboration of inductive definitions down to a universe of
datatypes. The universe of datatypes is an internal presentation of strictly
positive families within type theory. By elaborating an inductive definition --
a syntactic artifact -- to its code -- its semantics -- we obtain an
internalized account of inductives inside the type theory itself: we claim that
reasoning about inductive definitions could be carried in the type theory, not
in the meta-theory as it is usually the case. Besides, we give a formal
specification of that elaboration process. It is therefore amenable to formal
reasoning too. We prove the soundness of our translation and hint at its
correctness with respect to Coq's Inductive definitions.
  The practical benefits of this approach are numerous. For the type theorist,
this is a small step toward bootstrapping, ie. implementing the inductive
fragment in the type theory itself. For the programmer, this means better
support for generic programming: we shall present a lightweight deriving
mechanism, entirely definable by the programmer and therefore not requiring any
extension to the type theory."
"There is a plethora of research articles describing the deep semantics of
JavaScript. Nevertheless, such articles are often difficult to grasp for
readers not familiar with formal semantics. In this report, we propose a digest
of the semantics of JavaScript centered around security concerns. This document
proposes an overview of the JavaScript language and the misleading semantic
points in its design. The first part of the document describes the main
characteristics of the language itself. The second part presents how those
characteristics can lead to problems. It finishes by showing some coding
patterns to avoid certain traps and presents some ECMAScript 5 new features."
"Reductionism is a viable strategy for designing and implementing practical
programming languages, leading to solutions which are easier to extend,
experiment with and formally analyze. We formally specify and implement an
extensible programming language, based on a minimalistic first-order imperative
core language plus strong abstraction mechanisms, reflection and
self-modification features. The language can be extended to very high levels:
by using Lisp-style macros and code-to-code transforms which automatically
rewrite high-level expressions into core forms, we define closures and
first-class continuations on top of the core. Non-self-modifying programs can
be analyzed and formally reasoned upon, thanks to the language simple
semantics. We formally develop a static analysis and prove a soundness property
with respect to the dynamic semantics. We develop a parallel garbage collector
suitable to multi-core machines to permit efficient execution of parallel
programs."
"To deal with failures as simply as possible, we propose a new foun- dation
for the core (untyped) C, which is based on a new logic called task logic or
imperative logic. We then introduce a sequential-disjunctive statement of the
form S : R. This statement has the following semantics: execute S and R
sequentially. It is considered a success if at least one of S;R is a success.
This statement is useful for dealing with inessential errors without explicitly
catching them."
"Parallel hardware makes concurrency mandatory for efficient program
execution. However, writing concurrent software is both challenging and
error-prone. C++11 provides standard facilities for multiprogramming, such as
atomic operations with acquire/release semantics and RAII mutex locking, but
these primitives remain too low-level. Using them both correctly and
efficiently still requires expert knowledge and hand-crafting. The actor model
replaces implicit communication by sharing with an explicit message passing
mechanism. It applies to concurrency as well as distribution, and a lightweight
actor model implementation that schedules all actors in a properly
pre-dimensioned thread pool can outperform equivalent thread-based
applications. However, the actor model did not enter the domain of native
programming languages yet besides vendor-specific island solutions. With the
open source library libcppa, we want to combine the ability to build reliable
and distributed systems provided by the actor model with the performance and
resource-efficiency of C++11."
"While generalized algebraic datatypes (\GADTs) are now considered
well-understood, adding them to a language with a notion of subtyping comes
with a few surprises. What does it mean for a \GADT parameter to be covariant?
The answer turns out to be quite subtle. It involves fine-grained properties of
the subtyping relation that raise interesting design questions. We allow
variance annotations in \GADT definitions, study their soundness, and present a
sound and complete algorithm to check them. Our work may be applied to
real-world ML-like languages with explicit subtyping such as OCaml, or to
languages with general subtyping constraints."
"Automatic code transformation in which transformations are tuned for specific
applications and contexts are difficult to achieve in an accessible manner. In
this paper, we present an approach to build application specific code
transformations. Our approach is based on analysis of the abstract syntax
representation of exemplars of the essential change to the code before and
after the transformation is applied. This analysis entails a sequence of steps
to identify the change, determine how to generalize it, and map it to term
rewriting rules for the Stratego term rewriting system. The methods described
in this paper assume programs are represented in a language-neutral term
format, allowing tools based on our methods to be applied to programs written
in the major languages used by computational scientists utilizing high
performance computing systems."
"We report on the implementation of a certified compiler for a high-level
hardware description language (HDL) called Fe-Si (FEatherweight SynthesIs).
Fe-Si is a simplified version of Bluespec, an HDL based on a notion of guarded
atomic actions. Fe-Si is defined as a dependently typed deep embedding in Coq.
The target language of the compiler corresponds to a synthesisable subset of
Verilog or VHDL. A key aspect of our approach is that input programs to the
compiler can be defined and proved correct inside Coq. Then, we use extraction
and a Verilog back-end (written in OCaml) to get a certified version of a
hardware design."
"ModelCC is a model-based parser generator that decouples language design from
language processing. ModelCC provides two different mechanisms to specify the
mapping from an abstract syntax model to a concrete syntax model: metadata
annotations defined on top of the abstract syntax model specification and a
domain-specific language for defining ASM-CSM mappings. Using a domain-specific
language to specify the mapping from abstract to concrete syntax models allows
the definition of multiple concrete syntax models for the same abstract syntax
model. In this paper, we describe the ModelCC domain-specific language for
abstract syntax model to concrete syntax model mappings and we showcase its
capabilities by providing a meta-definition of that domain-specific language."
"Single inheritance has been widely accepted in the current programming
practice to avoid the complication that incurred by multiple inheritance.
Single inheritance enhances the reusability of codes and eliminates the
confusion of identical methods that possibly defined in two superclasses.
However, the mechanism of inner class in Java potentially reintroduces the
problems encountered by multiple inheritance. When the depth of Java inner
class is increased, the problem becomes severe. This paper aims at exposing the
problems of inheritance at the Java inner class. In addition, a measure is
proposed to evaluate the potential problem of inheritance for Java inner class"
"To represent interactive objects, we propose a choice-disjunctive declaration
statement of the form S R where S;R are the (procedure or field) declaration
statements within a class. This statement has the following semantics: request
the user to choose one between S and R when an object of this class is created.
This statement is useful for representing interactive objects that require
interactions with the user."
"Mode-directed tabling is an extension to the tabling technique that supports
the definition of mode operators for specifying how answers are inserted into
the table space. In this paper, we focus our discussion on the efficient
support for mode directed-tabling in the YapTab tabling system. We discuss 7
different mode operators and explain how we have extended and optimized
YapTab's table space organization to support them. Initial experimental results
show that our implementation compares favorably with the B-Prolog and XSB
state-of-the-art Prolog tabling systems."
"Many or-parallel Prolog models exploiting implicit parallelism have been
proposed in the past. Arguably, one of the most successful models is
environment copying for shared memory architectures. With the increasing
availability and popularity of multicore architectures, it makes sense to
recover the body of knowledge there is in this area and re-engineer prior
computational models to evaluate their performance on newer architectures. In
this work, we focus on the implementation of splitting strategies for
or-parallel Prolog execution on multicores and, for that, we develop a
framework, on top of the YapOr system, that integrates and supports five
alternative splitting strategies. Our implementation shares the underlying
execution environment and most of the data structures used to implement
or-parallelism in YapOr. In particular, we took advantage of YapOr's
infrastructure for incremental copying and scheduling support, which we used
with minimal modifications. We thus argue that all these common support
features allow us to make a first and fair comparison between these five
alternative splitting strategies and, therefore, better understand their
advantages and weaknesses."
"A range of methodologies and techniques are available to guide the design and
implementation of language extensions and domain-specific languages. A simple
yet powerful technique is based on source-to-source transformations interleaved
across the compilation passes of a base language. Despite being a successful
approach, it has the main drawback that the input source code is lost in the
process. When considering the whole workflow of program development (warning
and error reporting, debugging, or even program analysis), program translations
are no more powerful than a glorified macro language. In this paper, we propose
an augmented approach to language extensions for Prolog, where symbolic
annotations are included in the target program. These annotations allow
selectively reversing the translated code. We illustrate the approach by
showing that coupling it with minimal extensions to a generic Prolog debugger
allows us to provide users with a familiar, source-level view during the
debugging of programs which use a variety of language extensions, such as
functional notation, DCGs, or CLP{Q,R}."
"We present a new free library for Constraint Logic Programming over Finite
Domains, included with the Ciao Prolog system. The library is entirely written
in Prolog, leveraging on Ciao's module system and code transformation
capabilities in order to achieve a highly modular design without compromising
performance. We describe the interface, implementation, and design rationale of
each modular component.
  The library meets several design goals: a high level of modularity, allowing
the individual components to be replaced by different versions;
high-efficiency, being competitive with other FD implementations; a glass-box
approach, so the user can specify new constraints at different levels; and a
Prolog implementation, in order to ease the integration with Ciao's code
analysis components.
  The core is built upon two small libraries which implement integer ranges and
closures. On top of that, a finite domain variable datatype is defined, taking
care of constraint reexecution depending on range changes. These three
libraries form what we call the FD kernel of the library.
  This FD kernel is used in turn to implement several higher-level finite
domain constraints, specified using indexicals. Together with a labeling module
this layer forms what we name \emph{the FD solver}. A final level integrates
the clp(FD) paradigm with our FD solver. This is achieved using attributed
variables and a compiler from the clp(FD) language to the set of constraints
provided by the solver.
  It should be noted that the user of the library is encouraged to work in any
of those levels as seen convenient: from writing a new range module to
enriching the set of FD constraints by writing new indexicals."
"Statically reasoning in the presence of and about exceptions is challenging:
exceptions worsen the well-known mutual recursion between data-flow and
control-flow analysis. The recent development of pushdown control-flow analysis
for the {\lambda}-calculus hints at a way to improve analysis of exceptions: a
pushdown stack can precisely match catches to throws in the same way it matches
returns to calls. This work generalizes pushdown control-flow analysis to
object-oriented programs and to exceptions. Pushdown analysis of exceptions
improves precision over the next best analysis, Bravenboer and Smaragdakis's
Doop, by orders of magnitude. By then generalizing abstract garbage collection
to object-oriented programs, we reduce analysis time by half over pure pushdown
analysis. We evaluate our implementation for Dalvik bytecode on standard
benchmarks as well as several Android applications."
"Web applications written in JavaScript are regularly used for dealing with
sensitive or personal data. Consequently, reasoning about their security
properties has become an important problem, which is made very difficult by the
highly dynamic nature of the language, particularly its support for runtime
code generation. As a first step towards dealing with this, we propose to
investigate security analyses for languages with more principled forms of
dynamic code generation. To this end, we present a static information flow
analysis for a dynamically typed functional language with prototype-based
inheritance and staged metaprogramming. We prove its soundness, implement it
and test it on various examples designed to show its relevance to proving
security properties, such as noninterference, in JavaScript. To our knowledge,
this is the first fully static information flow analysis for a language with
staged metaprogramming, and the first formal soundness proof of a CFA-based
information flow analysis for a functional programming language."
"This paper describes an operational semantics for futures, with the primary
target on energy efficiency. The work in progress is built around an insight
that different threads can coordinate by running at different ""paces,"" so that
the time for synchronization and the resulting wasteful energy consumption can
be reduced. We exploit several inherent characteristics of futures to determine
how the paces of involving threads can be coordinated. The semantics is
inspired by recent advances in computer architectures, where the frequencies of
CPU cores can be adjusted dynamically. The work is a first-step toward a
direction where variant frequencies are directly modeled as an essential
semantic feature in concurrent programming languages."
"We identify three problems with current techniques for implementing protocols
among threads, which complicate and impair the scalability of multicore
software development: implementing synchronization, implementing coordination,
and modularizing protocols. To mend these deficiencies, we argue for the use of
domain-specific languages (DSL) based on existing models of concurrency. To
demonstrate the feasibility of this proposal, we explain how to use the model
of concurrency Reo as a high-level protocol DSL, which offers appropriate
abstractions and a natural separation of protocols and computations. We
describe a Reo-to-Java compiler and illustrate its use through examples."
"Object-oriented programming is inextricably linked to the pioneering work of
Ole-Johan Dahl and Kristen Nygaard on the design of the Simula language, which
started at the Norwegian Computing Centre in the Spring of 1961. However,
object-orientation, as we think of it today---fifty years later---is the result
of a complex interplay of ideas, constraints and people. Dahl and Nygaard would
certainly recognise it as their progeny, but might also be amazed at how much
it has grown up.
  This article is based on a lecture given on 22nd August 2011, on the occasion
of the scientific opening of the Ole-Johan Dahl hus at the University of Oslo.
It looks at the foundational ideas from Simula that stand behind
object-orientation, how those ideas have evolved to become the dominant
programming paradigm, and what they have to offer as we approach the challenges
of the next fifty years of informatics."
"EasyTime is a domain-specific language (DSL) for measuring time during sports
competitions. A distinguishing feature of DSLs is that they are much more
amenable to change, and EasyTime is no exception in this regard. This paper
introduces two new EasyTime features: classifications of competitors into
categories, and the inclusion of competitions where the number of laps must be
dynamically determined. It shows how such extensions can be incrementally added
into the base-language reusing most of the language specifications. Two case
studies are presented showing the suitability of this approach."
"This paper is aimed to present the importance and implementation of an
incremental call graph plugin. An algorithm is proposed for the call graph
implementation which has better overall performance than the algorithm that has
been proposed previously. In addition to this, the algorithm has been
empirically proved to have excellent performance on recursive codes. The
algorithm also readily checks for function skip and returns exceptions."
"Global Value Numbering(GVN) is a method for detecting redundant computations
in programs. Here, we introduce the problem of Global Value Numbering in its
original form, as conceived by Kildall(1973), and present an algorithm which is
a simpler variant of Kildall's. The algorithm uses the concept of value
expression - an abstraction of a set of expressions - enabling a representation
of the equivalence information which is compact and simple to manipulate."
"This paper presents an approach to verify safety properties of Erlang-style,
higher-order concurrent programs automatically. Inspired by Core Erlang, we
introduce Lambda-Actor, a prototypical functional language with
pattern-matching algebraic data types, augmented with process creation and
asynchronous message-passing primitives. We formalise an abstract model of
Lambda-Actor programs called Actor Communicating System (ACS) which has a
natural interpretation as a vector addition system, for which some verification
problems are decidable. We give a parametric abstract interpretation framework
for Lambda-Actor and use it to build a polytime computable, flow-based,
abstract semantics of Lambda-Actor programs, which we then use to bootstrap the
ACS construction, thus deriving a more accurate abstract model of the input
program. We have constructed Soter, a tool implementation of the verification
method, thereby obtaining the first fully-automatic, infinite-state model
checker for a core fragment of Erlang. We find that in practice our abstraction
technique is accurate enough to verify an interesting range of safety
properties. Though the ACS coverability problem is Expspace-complete, Soter can
analyse these verification problems surprisingly efficiently."
"A new theory of programming is proposed. The theory consists of OE (Operation
Expression), SP (Semantic Predicate) and A (Axiom), abbreviated as OESPA. OE is
for programming: its syntax is given by BNF formulas and its semantics is
defined by axioms on these formulas. Similar to predicates in logic, SP is for
describing properties of OE (i.e. programs) and for program property analysis.
But SP is different from predicates, it directly relates the final values of
variables upon termination of a given OE with initial values of these variables
before the same OE. As such, it is feasible to prove or disprove whether a
given SP is a property of a given OE by computation based on A (Axioms). SP
calculus is proposed for program specification and specification analysis, that
is missing in software engineering."
"The definitional equality of an intensional type theory is its test of type
compatibility. Today's systems rely on ordinary evaluation semantics to compare
expressions in types, frustrating users with type errors arising when
evaluation fails to identify two `obviously' equal terms. If only the machine
could decide a richer theory! We propose a way to decide theories which
supplement evaluation with `$\nu$-rules', rearranging the neutral parts of
normal forms, and report a successful initial experiment.
  We study a simple -calculus with primitive fold, map and append operations on
lists and develop in Agda a sound and complete decision procedure for an
equational theory enriched with monoid, functor and fusion laws."
"Productivity languages such as NumPy and Matlab make it much easier to
implement data-intensive numerical algorithms. However, these languages can be
intolerably slow for programs that don't map well to their built-in primitives.
In this paper, we discuss locality optimizations for our system Parakeet, a
just-in-time compiler and runtime system for an array-oriented subset of
Python. Parakeet dynamically compiles whole user functions to high performance
multi-threaded native code. Parakeet makes extensive use of the classic data
parallel operators Map, Reduce, and Scan. We introduce a new set of data
parallel operators,TiledMap, TiledReduce, and TiledScan, that break up their
computations into local pieces of bounded size so as better to make use of
small fast memories. We introduce a novel tiling transformation to generate
tiled operators automatically. Applying this transformation once tiles the
program for cache, and applying it again enables tiling for registers. The
sizes for cache tiles are left unspecified until runtime, when an autotuning
search is performed. Finally, we evaluate our optimizations on benchmarks and
show significant speedups on programs that exhibit data locality."
"Static analyzers based on abstract interpretation are complex pieces of
software implementing delicate algorithms. Even if static analysis techniques
are well understood, their implementation on real languages is still
error-prone. This paper presents a formal verification using the Coq proof
assistant: a formalization of a value analysis (based on abstract
interpretation), and a soundness proof of the value analysis. The formalization
relies on generic interfaces. The mechanized proof is facilitated by a
translation validation of a Bourdoncle fixpoint iterator. The work has been
integrated into the CompCert verified C-compiler. Our verified analysis
directly operates over an intermediate language of the compiler having the same
expressiveness as C. The automatic extraction of our value analysis into OCaml
yields a program with competitive results, obtained from experiments on a
number of benchmarks and comparisons with the Frama-C tool."
"An approximate program transformation is a transformation that can change the
semantics of a program within a specified empirical error bound. Such
transformations have wide applications: they can decrease computation time,
power consumption, and memory usage, and can, in some cases, allow
implementations of incomputable operations. Correctness proofs of approximate
program transformations are by definition quantitative. Unfortunately, unlike
with standard program transformations, there is as of yet no modular way to
prove correctness of an approximate transformation itself. Error bounds must be
proved for each transformed program individually, and must be re-proved each
time a program is modified or a different set of approximations are applied. In
this paper, we give a semantics that enables quantitative reasoning about a
large class of approximate program transformations in a local, composable way.
Our semantics is based on a notion of distance between programs that defines
what it means for an approximate transformation to be correct up to an error
bound. The key insight is that distances between programs cannot in general be
formulated in terms of metric spaces and real numbers. Instead, our semantics
admits natural notions of distance for each type construct; for example,
numbers are used as distances for numerical data, functions are used as
distances for functional data, an polymorphic lambda-terms are used as
distances for polymorphic data. We then show how our semantics applies to two
example approximations: replacing reals with floating-point numbers, and loop
perforation."
"A visual programming language uses pictorial tools such as diagrams to
represent its structural units and control stream. It is useful for enhancing
understanding, maintenance, verification, testing, and parallelism. This paper
proposes a diagrammatic methodology that produces a conceptual representation
of instructions for programming source codes. Without loss of generality in the
potential for using the methodology in a wider range of applications, this
paper focuses on using these diagrams in teaching of C++ programming. C++
programming constructs are represented in the proposed method in order to show
that it can provide a foundation for understanding the behavior of running
programs. Applying the method to actual C++ classes demonstrates that it
improves understanding of the activities in the computer system corresponding
to a C++ program."
"An interprocedural analysis is precise if it is flow sensitive and fully
context-sensitive even in the presence of recursion. Many methods of
interprocedural analysis sacrifice precision for scalability while some are
precise but limited to only a certain class of problems.
  Soot currently supports interprocedural analysis of Java programs using graph
reachability. However, this approach is restricted to IFDS/IDE problems, and is
not suitable for general data flow frameworks such as heap reference analysis
and points-to analysis which have non-distributive flow functions.
  We describe a general-purpose interprocedural analysis framework for Soot
using data flow values for context-sensitivity. This framework is not
restricted to problems with distributive flow functions, although the lattice
must be finite. It combines the key ideas of the tabulation method of the
functional approach and the technique of value-based termination of call string
construction.
  The efficiency and precision of interprocedural analyses is heavily affected
by the precision of the underlying call graph. This is especially important for
object-oriented languages like Java where virtual method invocations cause an
explosion of spurious call edges if the call graph is constructed naively. We
have instantiated our framework with a flow and context-sensitive points-to
analysis in Soot, which enables the construction of call graphs that are far
more precise than those constructed by Soot's SPARK engine."
"We address a problem connected to the unfolding semantics of functional
programming languages: give a useful characterization of those infinite
lambda-terms that are lambda_{letrec}-expressible in the sense that they arise
as infinite unfoldings of terms in lambda_{letrec}, the lambda-calculus with
letrec. We provide two characterizations, using concepts we introduce for
infinite lambda-terms: regularity, strong regularity, and binding-capturing
chains. It turns out that lambda_{letrec}-expressible infinite lambda-terms
form a proper subclass of the regular infinite lambda-terms. In this paper we
establish these characterizations only for expressibility in lambda_{mu}, the
lambda-calculus with explicit mu-recursion. We show that for all infinite
lambda-terms T the following are equivalent: (i): T is lambda_{mu}-expressible;
(ii): T is strongly regular; (iii): T is regular, and it only has finite
binding-capturing chains.
  We define regularity and strong regularity for infinite lambda-terms as two
different generalizations of regularity for infinite first-order terms: as the
existence of only finitely many subterms that are defined as the reducts of two
rewrite relations for decomposing lambda-terms. These rewrite relations act on
infinite lambda-terms furnished with a marked prefix of abstractions for
collecting decomposed lambda-abstractions and keeping the terms closed under
decomposition. They differ in how vacuous abstractions in the prefix are
removed.
  This report accompanies the article with the same title for the proceedings
of the conference RTA 2013, and mainly differs from that by providing the proof
of the characterization of lambda_{mu}-expressibility with binding-capturing
chains."
"This paper presents a review of some new futures introduced to C++ language
by ISO/IEC 14882:2011 standard (known as C++11). It describes new language
elements which allow to easier expressed of types of variables: auto and
decltype keywords, new function declaration syntax, and tools which are
included in type_traits header.
  -----
  Niniejszy artyku{\l} jest jednym z serii artyku{\l}\'ow w kt\'orych zawarto
przegl{\ka}d nowych element\'ow j{\ke}zyka C++ wprowadzonych przez standard
ISO/IEC 14882:2011, znany pod nazw{\ka} C++11. W artykule przedstawiono nowe
mo\.zliwo\'sci zwi{\ka}zane ze wskazywaniem typ\'ow zmiennych. Opisano s{\l}owa
kluczowe auto i decltype, now{\ka} sk{\l}adnie deklarowania funkcji/metod oraz
narz{\ke}dzia zawarte w pliku nag{\l}\'owkowym <type_traits>."
"The strength of a dynamic language is also its weakness: run-time flexibility
comes at the cost of compile-time predictability. Many of the hallmarks of
dynamic languages such as closures, continuations, various forms of reflection,
and a lack of static types make many programmers rejoice, while compiler
writers, tool developers, and verification engineers lament. The dynamism of
these features simply confounds statically reasoning about programs that use
them. Consequently, static analyses for dynamic languages are few, far between,
and seldom sound.
  The ""abstracting abstract machines"" (AAM) approach to constructing static
analyses has recently been proposed as a method to ameliorate the difficulty of
designing analyses for such language features. The approach, so called because
it derives a function for the sound and computable approximation of program
behavior starting from the abstract machine semantics of a language, provides a
viable approach to dynamic language analysis since all that is required is a
machine description of the interpreter.
  The original AAM recipe produces finite state abstractions, which cannot
faithfully represent an interpreter's control stack. Recent advances have shown
that higher-order programs can be approximated with pushdown systems. However,
these automata theoretic models either break down on features that inspect or
modify the control stack.
  In this paper, we tackle the problem of bringing pushdown flow analysis to
the domain of dynamic language features. We revise the abstracting abstract
machines technique to target the stronger computational model of pushdown
systems. In place of automata theory, we use only abstract machines and
memoization. As case studies, we show the technique applies to a language with
closures, garbage collection, stack-inspection, and first-class composable
continuations."
"We describe the design and implementation of GNU Guix, a purely functional
package manager designed to support a complete GNU/Linux distribution. Guix
supports transactional upgrades and roll-backs, unprivileged package
management, per-user profiles, and garbage collection. It builds upon the
low-level build and deployment layer of the Nix package manager. Guix uses
Scheme as its programming interface. In particular, we devise an embedded
domain-specific language (EDSL) to describe and compose packages. We
demonstrate how it allows us to benefit from the host general-purpose
programming language while not compromising on expressiveness. Second, we show
the use of Scheme to write build programs, leading to ""two-tier"" programming
system."
"We present a functional programming language for specifying constraints over
tree-shaped data. The language allows for Haskell-like algebraic data types and
pattern matching. Our constraint compiler CO4 translates these programs into
satisfiability problems in propositional logic. We present an application from
the area of automated analysis of (non-)termination of rewrite systems."
"Adding versatile interactions to goals and queries in logic programming is an
essential task. Unfortunately, existing logic languages can take input from the
user only via the $read$ construct.
  We propose to add a new interactive goal to allow for more controlled and
more guided participation from the user. We illustrate our idea via \muprolog,
an extension of Prolog with bounded choice goals."
"Dependency analysis is a program analysis that determines potential data flow
between program points. While it is not a security analysis per se, it is a
viable basis for investigating data integrity, for ensuring confidentiality,
and for guaranteeing sanitization. A noninterference property can be stated and
proved for the dependency analysis. We have designed and implemented a
dependency analysis for JavaScript. We formalize this analysis as an
abstraction of a tainting semantics. We prove the correctness of the tainting
semantics, the soundness of the abstraction, a noninterference property, and
the termination of the analysis."
"This is the manual of Cyan, a prototype-based object-oriented language. Cyan
supports static typing, single inheritance, mixin objects (similar to mixin
classes with mixin inheritance), generic prototypes, and Java-like interfaces.
The language has several innovations: a completely object-oriented exception
system, statically-typed closures, a kind of graphical metaobjects called
codegs, optional dynamic typing, user-defined literal objects (an innovative
way of creating objects), context objects (a generalization of closures), and
grammar methods and message sends (which makes it easy to define Domain
Specific Languages)."
"Program termination is a hot research topic in program analysis. The last few
years have witnessed the development of termination analyzers for programming
languages such as C and Java with remarkable precision and performance. These
systems are largely based on techniques and tools coming from the field of
declarative constraint programming. In this paper, we first recall an algorithm
based on Farkas' Lemma for discovering linear ranking functions proving
termination of a certain class of loops. Then we propose an extension of this
method for showing the existence of eventual linear ranking functions, i.e.,
linear functions that become ranking functions after a finite unrolling of the
loop. We show correctness and completeness of this algorithm."
"A long-standing practical challenge in the optimization of higher-order
languages is inlining functions with free variables. Inlining code statically
at a function call site is safe if the compiler can guarantee that the free
variables have the same bindings at the inlining point as they do at the point
where the function is bound as a closure (code and free variables). There have
been many attempts to create a heuristic to check this correctness condition,
from Shivers' kCFA-based reflow analysis to Might's Delta-CFA and anodization,
but all of those have performance unsuitable for practical compiler
implementations. In practice, modern language implementations rely on a series
of tricks to capture some common cases (e.g., closures whose free variables are
only top-level identifiers such as +) and rely on hand-inlining by the
programmer for anything more complicated.
  This work provides the first practical, general approach for inlining
functions with free variables. We also provide a proof of correctness, an
evaluation of both the execution time and performance impact of this
optimization, and some tips and tricks for implementing an efficient and
precise control-flow analysis."
"In the analysis of logic programs, abstract domains for detecting sharing
properties are widely used. Recently the new domain $\Linp$ has been introduced
to generalize both sharing and linearity information. This domain is endowed
with an optimal abstract operator for single-binding unification. The authors
claim that the repeated application of this operator is also optimal for
multi-binding unification. This is the proof of such a claim."
"We present an extension to the labelling approach, a technique for lifting
resource consumption information from compiled to source code. This approach,
which is at the core of the annotating compiler from a large fragment of C to
8051 assembly of the CerCo project, looses preciseness when differences arise
as to the cost of the same portion of code, whether due to code transformation
such as loop optimisations or advanced architecture features (e.g. cache). We
propose to address this weakness by formally indexing cost labels with the
iterations of the containing loops they occur in. These indexes can be
transformed during the compilation, and when lifted back to source code they
produce dependent costs.
  The proposed changes have been implemented in CerCo's untrusted prototype
compiler from a large fragment of C to 8051 assembly."
"The recent rise of interest in bidirectional transformations (BXs) has led to
the development of many BX frameworks, originating in diverse computer science
disciplines. From a user perspective, these frameworks vary significantly in
both interface and predictability of the underlying bidirectionalization
technique. In this paper we start by presenting a generic BX scheme that can be
instantiated to different concrete interfaces, by plugging-in the desired
notion of update and traceability. Based on that scheme, we then present
several desirable generic properties that may characterize a BX framework, and
show how they can be instantiated to concrete interfaces. This generic
presentation is useful when exploring the BX design space: it might help
developers when designing new frameworks and end-users when comparing existing
ones. We support the latter claim, by applying it in a comparative survey of
popular existing BX frameworks."
"We propose a bridge between functional and object-oriented programming in the
first-year curriculum. Traditionally, curricula that begin with functional
programming transition to a professional, usually object-oriented, language in
the second course. This transition poses obstacles for students, and often
results in confusing the details of development environments, syntax, and
libraries with the fundamentals of OO programming that the course should focus
on. Instead, we propose to begin the second course with a sequence of custom
teaching languages which minimize the transition from the first course, and
allow students to focus on core ideas. After working through the sequence of
pedagogical languages, we then transition to Java, at which point students have
a strong command of the basic principles. We have 3 years of experience with
this course, with notable success."
"Instruction selection is one of three optimisation problems involved in the
code generator backend of a compiler. The instruction selector is responsible
of transforming an input program from its target-independent representation
into a target-specific form by making best use of the available machine
instructions. Hence instruction selection is a crucial part of efficient code
generation.
  Despite on-going research since the late 1960s, the last, comprehensive
survey on the field was written more than 30 years ago. As new approaches and
techniques have appeared since its publication, this brings forth a need for a
new, up-to-date review of the current body of literature. This report addresses
that need by performing an extensive review and categorisation of existing
research. The report therefore supersedes and extends the previous surveys, and
also attempts to identify where future research should be directed."
"Error detection facilities for dynamic languages are often based on unit
testing. Thus, the advantage of rapid prototyping and flexibility must be
weighed against cumbersome and time consuming test suite development. Lindahl
and Sagonas' success typings provide a means of static must-fail detection in
Erlang. Due to the constraint-based nature of the approach, some errors
involving nested tuples and recursion cannot be detected.
  We propose an approach that uses an extension of model checking for
pattern-matching recursion schemes with context-aware ranked tree automata to
provide improved success typings for a constructor-based first-order prototype
language."
"C++ does not support run-time resolution of template type arguments. To
circumvent this restriction, we can instantiate a template for all possible
combinations of type arguments at compile time and then select the proper
instance at run time by evaluation of some provided conditions. However, for
templates with multiple type parameters such a solution may easily result in a
branching code bloat. We present a template metaprogramming algorithm called
for_id that allows the user to select the proper template instance at run time
with theoretical minimum sustained complexity of the branching code."
"Green is a statically-typed object-oriented language that separates subtyping
from inheritance. It supports garbage collection, classes as first-class
objects, parameterized classes, introspective reflection and a kind of run-time
metaobjects called shells."
"This is a preliminary version of the language report. It contains key
definitions, specifications and some examples, but lacks completeness. The full
document will include Chapter 3 (Data and Instrumentation Layer) and will
comprise an appendix giving the complete syntax and some whole program
examples. The purpose of the present document is to fix the concepts and major
features of the language and to enable the production of the definition
document that is required for implementation."
"Bidirectional typechecking, in which terms either synthesize a type or are
checked against a known type, has become popular for its scalability (unlike
Damas-Milner type inference, bidirectional typing remains decidable even for
very expressive type systems), its error reporting, and its relative ease of
implementation. Following design principles from proof theory, bidirectional
typing can be applied to many type constructs. The principles underlying a
bidirectional approach to polymorphism, however, are less obvious. We give a
declarative, bidirectional account of higher-rank polymorphism, grounded in
proof theory; this calculus enjoys many properties such as eta-reduction and
predictability of annotations. We give an algorithm for implementing the
declarative system; our algorithm is remarkably simple and well-behaved,
despite being both sound and complete."
"Python is a popular dynamic language with a large part of its appeal coming
from powerful libraries and extension modules. These augment the language and
make it a productive environment for a wide variety of tasks, ranging from web
development (Django) to numerical analysis (NumPy). Unfortunately, Python's
performance is quite poor when compared to modern implementations of languages
such as Lua and JavaScript.
  Why does Python lag so far behind these other languages? As we show, the very
same API and extension libraries that make Python a powerful language also make
it very difficult to efficiently execute. Given that we want to retain access
to the great extension libraries that already exist for Python, how fast can we
make it?
  To evaluate this, we designed and implemented Falcon, a high-performance
bytecode interpreter fully compatible with the standard CPython interpreter.
Falcon applies a number of well known optimizations and introduces several new
techniques to speed up execution of Python bytecode. In our evaluation, we
found Falcon an average of 25% faster than the standard Python interpreter on
most benchmarks and in some cases about 2.5X faster."
"In heap-based languages, knowing that a variable x points to an acyclic data
structure is useful for analyzing termination: this information guarantees that
the depth of the data structure to which x points is greater than the depth of
the structure pointed to by x.fld, and allows bounding the number of iterations
of a loop which traverses the data structure on fld. In general, proving
termination needs acyclicity, unless program-specific or non-automated
reasoning is performed. However, recent work could prove that certain loops
terminate even without inferring acyclicity, because they traverse data
structures ""acyclically"". Consider a double-linked list: if it is possible to
demonstrate that every cycle involves both the ""next"" and the ""prev"" field,
then a traversal on ""next"" terminates since no cycle will be traversed
completely. This paper develops a static analysis inferring field-sensitive
reachability and cyclicity information, which is more general than existing
approaches. Propositional formulae are computed, which describe which fields
may or may not be traversed by paths in the heap. Consider a tree with edges
""left"" and ""right"" to the left and right sub-trees, and ""parent"" to the parent
node: termination of a loop traversing leaf-up cannot be guaranteed by
state-of-the-art analyses. Instead, propositional formulae computed by this
analysis indicate that cycles must traverse ""parent"" and at least one between
""left"" and ""right"": termination is guaranteed as no cycle is traversed
completely. This paper defines the necessary abstract domains and builds an
abstract semantics on them. A prototypical implementation provides the expected
result on relevant examples."
"In this tutorial I will present how a combination of linear and dependent
type can be useful to describe different properties about higher order
programs. Linear types have been proved particularly useful to express
properties of functions; dependent types are useful to describe the behavior of
the program in terms of its control flow. This two ideas fits together well
when one is interested in analyze properties of functions depending on the
control flow of the program. I will present these ideas with example taken by
complexity analysis and sensitivity analysis. I will conclude the tutorial by
arguing about the generality of this approach."
"Adding versatile interactions to imperative programming -- C, Java and
Android -- is an essential task. Unfortunately, existing languages provide only
limited constructs for user interaction. These constructs are usually in the
form of $unbounded$ quantification. For example, existing languages can take
the keyboard input from the user only via the $read(x)/scan(x)$ construct. Note
that the value of $x$ is unbounded in the sense that $x$ can have any value.
This construct is thus not useful for applications with bounded inputs. To
support bounded choices, we propose new bounded-choice statements for user
interation. Each input device (the keyboard, the mouse, the touch, $...$)
naturally requires a new bounded-choice statement. To make things simple,
however, we focus on a bounded-choice statement for keyboard -- kchoose -- to
allow for more controlled and more guided participation from the user. It is
straightforward to adjust our idea to other input devices. We illustrate our
idea via Java(BI), an extension of the core Java with a new bounded-choice
statement for the keyboard."
"This volume contains the papers presented at the 23rd Workshop on Logic-based
Methods in Programming Environments (WLPE 2013), which was held in Istanbul,
Turkey, on August 24 & 25 2013 as a satellite event of the 29th International
Conference on Logic Programming, (ICLP 2013)."
"We present an automatic analysis technique for checking data races on OpenCL
kernels. Our method defines symbolic execution techniques based on separation
logic with suitable abstractions to automatically detect non-benign racy
behaviours on kernel"
"BEE is a compiler which facilitates solving finite domain constraints by
encoding them to CNF and applying an underlying SAT solver. In BEE constraints
are modeled as Boolean functions which propagate information about equalities
between Boolean literals. This information is then applied to simplify the CNF
encoding of the constraints. We term this process equi-propagation. A key
factor is that considering only a small fragment of a constraint model at one
time enables to apply stronger, and even complete reasoning to detect
equivalent literals in that fragment. Once detected, equivalences propagate to
simplify the entire constraint model and facilitate further reasoning on other
fragments. BEE is described in several recent papers. In this paper, after a
quick review of BEE, we elaborate on two undocumented details of the
implementation: the hybrid encoding of cardinality constraints and complete
equi-propagation. We thendescribe on-going work aimed to extend BEE to consider
binary representation of numbers."
"In this paper we investigate XSB-Prolog as a static analysis engine for data
represented by medium-sized graphs. We use XSB-Prolog to automatically identify
function dependencies in the Linux Kernel---queries that are difficult to
implement efficiently in a commodity database and that developers often have to
identify manually. This project illustrates that Prolog systems are ideal for
building tools for use in other disciplines that require sophisticated
inferences, because Prolog is both declarative and can efficiently implement
complex problem specifications through tabling and indexing."
"Programming languages and techniques based on logic and constraints, such as
the Constraint Handling Rules (CHR), can support many common programming tasks
that can be expressed in the form of a search for feasible or optimal
solutions. Developing new constraint solvers using CHR is especially
interesting in configuration management for large scale, distributed and
dynamic cloud applications, where dynamic configuration and component selection
is an integral part of the programming environment. Writing CHR-style
constraint solvers in a domain-specific language which is a subset of Java --
instead of using a separate language layer -- solves many integration,
development cycle disruption, testing and debugging problems that discourage or
make difficult the adoption of the CHR-based approach in the mainstream
programming environments. Besides, the prototype implementation exposes a
well-defined API that supports transactional store behavior, safe termination,
and debugging via event notifications."
"We present a novel general resource analysis for logic programs based on
sized types.Sized types are representations that incorporate structural (shape)
information and allow expressing both lower and upper bounds on the size of a
set of terms and their subterms at any position and depth. They also allow
relating the sizes of terms and subterms occurring at different argument
positions in logic predicates. Using these sized types, the resource analysis
can infer both lower and upper bounds on the resources used by all the
procedures in a program as functions on input term (and subterm) sizes,
overcoming limitations of existing analyses and enhancing their precision. Our
new resource analysis has been developed within the abstract interpretation
framework, as an extension of the sized types abstract domain, and has been
integrated into the Ciao preprocessor, CiaoPP. The abstract domain operations
are integrated with the setting up and solving of recurrence equations for
both, inferring size and resource usage functions. We show that the analysis is
an improvement over the previous resource analysis present in CiaoPP and
compares well in power to state of the art systems."
"Prolog's support for dynamic programming, meta programming and text
processing using context free grammars make the language highly suitable for
defining domain specific languages (DSL) as well as analysing, refactoring or
generating expression states in other (programming) languages. Well known DSLs
are the DCG (Definite Clause Grammar) notation and constraint languages such as
CHR. These extensions use Prolog operator declarations and the {...} notation
to realise a good syntax. When external languages, such as HTML, SQL or
JavaScript enter the picture, operators no longer satisfy for embedding
snippets of these languages into a Prolog source file. In addition, Prolog has
poor support for quoting long text fragments.
  Haskell introduced quasi quotationsto resolve this problem. In this paper we
`ported' the Haskell mechanism for quasi quoting to Prolog. We show that this
can be done cleanly and that quasi quoting can solve the above mentioned
problems."
"Diverse selection statements -- if-then-else, switch and try-catch -- are
commonly used in modern programming languages. To make things simple, we
propose a unifying statement for selection. This statement is of the form
schoose(G_1,...,G_n) where each $G_i$ is a statement. It has a a simple
semantics: sequentially choose the first successful statement $G_i$ and then
proceeds with executing $G_i$. Examples will be provided for this statement."
"This dissertation explores classes of compiler optimization techniques that
are applicable late in the compilation process, after all executable code for a
program has been linked. I concentrate on techniques which, for various
reasons, cannot be applied earlier in the compilation process. In addition to a
theoretical treatment of this class of optimization techniques, this
dissertation reports on an implementation of these techniques in a production
environment. I describe the details of the implementation which allows these
techniques to be re-targeted easily and report on improvements gained when
optimizing production software."
"A compact version of MACRO SPITBOL, a compiler/ interpreter for a variant of
SNOBOL4, has been developed for use on microcomputer systems. The techniques
for producing an implementation are largely automatic in order to preserve the
integrity and portability of the SPITBOL system. These techniques are discussed
along with a description of an initial implementation on a 65K byte
minicomputer. An interesting theoretical problem which arises when using
procedures which compact the interpretive object code is also analyzed."
"We present abstract acceleration techniques for computing loop invariants for
numerical programs with linear assignments and conditionals. Whereas abstract
interpretation techniques typically over-approximate the set of reachable
states iteratively, abstract acceleration captures the effect of the loop with
a single, non-iterative transfer function applied to the initial states at the
loop head. In contrast to previous acceleration techniques, our approach
applies to any linear loop without restrictions. Its novelty lies in the use of
the Jordan normal form decomposition of the loop body to derive symbolic
expressions for the entries of the matrix modeling the effect of n>=0
iterations of the loop. The entries of such a matrix depend on $n$ through
complex polynomial, exponential and trigonometric functions. Therefore, we
introduces an abstract domain for matrices that captures the linear inequality
relations between these complex expressions. This results in an abstract matrix
for describing the fixpoint semantics of the loop.
  Our approach integrates smoothly into standard abstract interpreters and can
handle programs with nested loops and loops containing conditional branches. We
evaluate it over small but complex loops that are commonly found in control
software, comparing it with other tools for computing linear loop invariants.
The loops in our benchmarks typically exhibit polynomial, exponential and
oscillatory behaviors that present challenges to existing approaches. Our
approach finds non-trivial invariants to prove useful bounds on the values of
variables for such loops, clearly outperforming the existing approaches in
terms of precision while exhibiting good performance."
"The main purpose of this article is to describe the taxonomy of computer
languages according to the levels of abstraction. There exists so many computer
languages because of so many reasons like the evolution of better computer
languages over the time; the socio-economic factors as the proprietary
interests, commercial advantages; expressive power; ease of use of novice;
orientation toward special purposes; orientation toward special hardware; and
diverse ideas about most suitability. Moreover, the important common properties
of most of these languages are discussed here. No programming language is
designed in a vacuity, but it solves some specific kinds of problems. There is
a different framework for each problem and best suitable framework for each
problem. A single framework is not best for all types of problems. So, it is
important to select vigilantly the frameworks supported by the language. The
five generation of the computer programming languages are explored in this
paper to some extent."
"Today's mobile platforms provide only coarse-grained permissions to users
with regard to how third- party applications use sensitive private data.
Unfortunately, it is easy to disguise malware within the boundaries of
legitimately-granted permissions. For instance, granting access to ""contacts""
and ""internet"" may be necessary for a text-messaging application to function,
even though the user does not want contacts transmitted over the internet. To
understand fine-grained application use of permissions, we need to statically
analyze their behavior. Even then, malware detection faces three hurdles: (1)
analyses may be prohibitively expensive, (2) automated analyses can only find
behaviors that they are designed to find, and (3) the maliciousness of any
given behavior is application-dependent and subject to human judgment. To
remedy these issues, we propose semantic-based program analysis, with a human
in the loop as an alternative approach to malware detection. In particular, our
analysis allows analyst-crafted semantic predicates to search and filter
analysis results. Human-oriented semantic-based program analysis can
systematically, quickly and concisely characterize the behaviors of mobile
applications. We describe a tool that provides analysts with a library of the
semantic predicates and the ability to dynamically trade speed and precision.
It also provides analysts the ability to statically inspect details of every
suspicious state of (abstract) execution in order to make a ruling as to
whether or not the behavior is truly malicious with respect to the intent of
the application. In addition, permission and profiling reports are generated to
aid analysts in identifying common malicious behaviors."
"Low-level program analysis is a fundamental problem, taking the shape of
""flow analysis"" in functional languages and ""points-to"" analysis in imperative
and object-oriented languages. Despite the similarities, the vocabulary and
results in the two communities remain largely distinct, with limited
cross-understanding. One of the few links is Shivers's $k$-CFA work, which has
advanced the concept of ""context-sensitive analysis"" and is widely known in
both communities.
  Recent results indicate that the relationship between the functional and
object-oriented incarnations of $k$-CFA is not as well understood as thought.
Van Horn and Mairson proved $k$-CFA for $k \geq 1$ to be EXPTIME-complete;
hence, no polynomial-time algorithm can exist. Yet, there are several
polynomial-time formulations of context-sensitive points-to analyses in
object-oriented languages. Thus, it seems that functional $k$-CFA may actually
be a profoundly different analysis from object-oriented $k$-CFA. We resolve
this paradox by showing that the exact same specification of $k$-CFA is
polynomial-time for object-oriented languages yet exponential- time for
functional ones: objects and closures are subtly different, in a way that
interacts crucially with context-sensitivity and complexity. This illumination
leads to an immediate payoff: by projecting the object-oriented treatment of
objects onto closures, we derive a polynomial-time hierarchy of
context-sensitive CFAs for functional programs."
"This dissertation proves lower bounds on the inherent difficulty of deciding
flow analysis problems in higher-order programming languages. We give exact
characterizations of the computational complexity of 0CFA, the $k$CFA
hierarchy, and related analyses. In each case, we precisely capture both the
expressiveness and feasibility of the analysis, identifying the elements
responsible for the trade-off.
  0CFA is complete for polynomial time. This result relies on the insight that
when a program is linear (each bound variable occurs exactly once), the
analysis makes no approximation; abstract and concrete interpretation coincide,
and therefore pro- gram analysis becomes evaluation under another guise.
Moreover, this is true not only for 0CFA, but for a number of further
approximations to 0CFA. In each case, we derive polynomial time completeness
results.
  For any $k > 0$, $k$CFA is complete for exponential time. Even when $k = 1$,
the distinction in binding contexts results in a limited form of closures,
which do not occur in 0CFA. This theorem validates empirical observations that
$k$CFA is intractably slow for any $k > 0$. There is, in the worst case---and
plausibly, in practice---no way to tame the cost of the analysis. Exponential
time is required. The empirically observed intractability of this analysis can
be understood as being inherent in the approximation problem being solved,
rather than reflecting unfortunate gaps in our programming abilities."
"We give an exact characterization of the computational complexity of the
$k$CFA hierarchy. For any $k > 0$, we prove that the control flow decision
problem is complete for deterministic exponential time. This theorem validates
empirical observations that such control flow analysis is intractable. It also
provides more general insight into the complexity of abstract interpretation."
"Flow analysis is a ubiquitous and much-studied component of compiler
technology---and its variations abound. Amongst the most well known is Shivers'
0CFA; however, the best known algorithm for 0CFA requires time cubic in the
size of the analyzed program and is unlikely to be improved. Consequently,
several analyses have been designed to approximate 0CFA by trading precision
for faster computation. Henglein's simple closure analysis, for example,
forfeits the notion of directionality in flows and enjoys an ""almost linear""
time algorithm. But in making trade-offs between precision and complexity, what
has been given up and what has been gained? Where do these analyses differ and
where do they coincide?
  We identify a core language---the linear $\lambda$-calculus---where 0CFA,
simple closure analysis, and many other known approximations or restrictions to
0CFA are rendered identical. Moreover, for this core language, analysis
corresponds with (instrumented) evaluation. Because analysis faithfully
captures evaluation, and because the linear $\lambda$-calculus is complete for
PTIME, we derive PTIME-completeness results for all of these analyses."
"When programmers want to prove strong program invariants, they are usually
faced with a choice between using theorem provers and using traditional
programming languages. The former requires them to provide program proofs,
which, for many applications, is considered a heavy burden. The latter provides
less guarantees and the programmer usually has to write run-time assertions to
compensate for the lack of suitable invariants expressible in the type system.
  We introduce Mezzo, a programming language in the tradition of ML, in which
the usual concept of a type is replaced by a more precise notion of a
permission. Programs written in Mezzo usually enjoy stronger guarantees than
programs written in pure ML. However, because Mezzo is based on a type system,
the reasoning requires no user input. In this paper, we illustrate the key
concepts of Mezzo, highlighting the static guarantees our language provides."
"In this article the function overloading in object-oriented programming is
elaborated and how they are implemented in C++. The language supports a variety
of programming styles. Here we are describing the polymorphism and its types in
brief. The main stress is given on the function overloading implementation
styles in the language. The polymorphic nature of languages has advantages like
that we can add new code without requiring changes to the other classes and
interfaces (in Java language) are easily implemented. In this technique, the
run-time overhead is also introduced in dynamic binding which increases the
execution time of the software. But there are no such types of overheads in
this static type of polymorphism because everything is resolved at the time of
compile time. Polymorphism; Function Overloading; Static Polymorphism;
Overloading; Compile Time Polymorphism."
"We present Mezzo, a typed programming language of ML lineage. Mezzo is
equipped with a novel static discipline of duplicable and affine permissions,
which controls aliasing and ownership. This rules out certain mistakes,
including representation exposure and data races, and enables new idioms, such
as gradual initialization, memory re-use, and (type)state changes. Although the
core static discipline disallows sharing a mutable data structure, Mezzo offers
several ways of working around this restriction, including a novel dynamic
ownership control mechanism which we dub ""adoption and abandon""."
"This is a talk proposal for HOPE 2013. Using iteration over a collection as a
case study, we wish to illustrate the strengths and weaknesses of the prototype
programming language Mezzo."
"Type systems hide data that is captured by function closures in function
types. In most cases this is a beneficial design that favors simplicity and
compositionality. However, some applications require explicit information about
the data that is captured in closures. This paper introduces open closure
types, that is, function types that are decorated with type contexts. They are
used to track data-flow from the environment into the function closure. A
simply-typed lambda calculus is used to study the properties of the type theory
of open closure types. A distinctive feature of this type theory is that an
open closure type of a function can vary in different type contexts. To present
an application of the type theory, it is shown that a type derivation
establishes a simple non-interference property in the sense of information-flow
theory. A publicly available prototype implementation of the system can be used
to experiment with type derivations for example programs."
"Rich applications merge classical computing, client-server concurrency,
web-based interfaces, and the complex time- and event-based reactive
programming found in embedded systems. To handle them, we extend the Hop web
programming platform by HipHop, a domain-specific language dedicated to
event-based process orchestration. Borrowing the synchronous reactive model of
Esterel, HipHop is based on synchronous concurrency and preemption primitives
that are known to be key components for the modular design of complex reactive
behaviors. HipHop departs from Esterel by its ability to handle the dynamicity
of Web applications, thanks to the reflexivity of Hop. Using a music player
example, we show how to modularly build a non-trivial Hop application using
HipHop orchestration code."
"In the work we discuss the benefit of using bitwise operations in
programming. Some interesting examples in this respect have been shown. What is
described in detail is an algorithm for sorting an integer array with the
substantial use of the bitwise operations. Besides its correctness we strictly
prove that the described algorithm works in time O(n). In the work during the
realisation of each of the examined algorithms we use the apparatus of the
object-oriented programming with the syntax and the semantics of the
programming language C++."
"This paper describes a case study for the sixth Transformation Tool Contest.
The case is based on a mapping from Petri-Nets to statecharts (i.e., from flat
process models to hierarchical ones). The case description separates a simple
mapping phase from a phase that involves the step by step destruction Petri-Net
elements and the corresponding construction of a hierarchy of statechart
elements. Although the focus of this case study is on the comparison of the
runtime performance of solutions, we also include correctness tests as well as
bonus criteria for evaluating transformation language and tool features."
"If the result of an expensive computation is invalidated by a small change to
the input, the old result should be updated incrementally instead of
reexecuting the whole computation. We incrementalize programs through their
derivative. A derivative maps changes in the program's input directly to
changes in the program's output, without reexecuting the original program. We
present a program transformation taking programs to their derivatives, which is
fully static and automatic, supports first-class functions, and produces
derivatives amenable to standard optimization.
  We prove the program transformation correct in Agda for a family of
simply-typed {\lambda}-calculi, parameterized by base types and primitives. A
precise interface specifies what is required to incrementalize the chosen
primitives.
  We investigate performance by a case study: We implement in Scala the program
transformation, a plugin and improve performance of a nontrivial program by
orders of magnitude."
"We describe an alternative approach to handling mutable references (aka.
pointers) within a gradually typed language that has different efficiency
characteristics than the prior approach of Herman et al. [2010]. In particular,
we reduce the costs of reading and writing through references in statically
typed regions of code. We reduce the costs to be the same as they would in a
statically typed language, that is, simply the cost of a load or store
instruction (for primitive data types). This reduction in cost is especially
important for programmers who would like to use gradual typing to facilitate
transitioning from a dynamically-typed prototype of an algorithm to a
statically-typed, high-performance implementation. The programmers we have in
mind are scientists and engineers who currently prototype in Matlab and then
manually translate their algorithms into Fortran. We present the static and
dynamic semantics for mutable references and a mechanized proof of type safety
using the Isabelle proof assistant."
"The Karatsuba multiplication algorithm is an algorithm for computing the
product of two natural numbers represented in the binary number system. This
means that the algorithm actually computes a function on bit strings. The
restriction of this function to bit strings of any given length can be computed
according to the Karatsuba multiplication algorithm by a finite instruction
sequence that contains only instructions to set and get the content of Boolean
registers, forward jump instructions, and a termination instruction. We
describe the instruction sequences concerned for the restrictions to bit
strings of the different lengths by uniform terms from an algebraic theory."
"For each function on bit strings, its restriction to bit strings of any given
length can be computed by a finite instruction sequence that contains only
instructions to set and get the content of Boolean registers, forward jump
instructions, and a termination instruction. Backward jump instructions are not
necessary for this, but instruction sequences can be significantly shorter with
them. We take the function on bit strings that models the multiplication of
natural numbers on their representation in the binary number system to
demonstrate this by means of a concrete example. The example is reason to
discuss points concerning the halting problem and the concept of an algorithm."
"PLACES 2013 (full title: Programming Language Approaches to Concurrency- and
Communication-cEntric Software) was the sixth edition of the PLACES workshop
series. After the first PLACES, which was affiliated to DisCoTec in 2008, the
workshop has been part of ETAPS every year since 2009 and is now an established
part of the ETAPS satellite events. This year, PLACES was the best attended
workshop at ETAPS 2013.
  The workshop series was started in order to promote the application of novel
programming language ideas to the increasingly important problem of developing
software for systems in which concurrency and communication are intrinsic
aspects. This includes software for multi- and many-core systems, accelerators
and large-scale distributed and/or service-oriented systems. The scope of
PLACES includes new programming language features, whole new programming
language designs, new type systems, new semantic approaches, new program
analysis techniques, and new implementation mechanisms."
"This paper presents the first implementation of session types in a
dynamically-typed language - Python. Communication safety of the whole system
is guaranteed at runtime by monitors that check the execution traces comply
with an associated protocol. Protocols are written in Scribble, a choreography
description language based on multiparty session types, with addition of logic
formulas for more precise behaviour properties. The presented framework
overcomes the limitations of previous works on the session types where all
endpoints should be statically typed so that they do not permit
interoperability with untyped participants. The advantages, expressiveness and
performance of dynamic protocol checking are demonstrated through use case and
benchmarks."
"Co-operative and pre-emptive scheduling are usually considered to be
complementary models of threading. In the case of virtual machines, we show
that they can be unified using a single concept, the bounded execution of a
thread of control, essentially providing a first-class representation of a
computation as it is reduced. Furthermore this technique can be used to surface
the thread scheduler of a language into the language itself, allowing programs
to provide their own schedulers without any additional support in the virtual
machine, and allowing the same virtual machine to support different thread
models simultaneously and without re-compilation."
"JSConTest introduced the notions of effect monitoring and dynamic effect
inference for JavaScript. It enables the description of effects with path
specifications resembling regular expressions. It is implemented by an offline
source code transformation.
  To overcome the limitations of the JSConTest implementation, we redesigned
and reimplemented effect monitoring by taking advantange of JavaScript proxies.
Our new design avoids all drawbacks of the prior implementation. It guarantees
full interposition; it is not restricted to a subset of JavaScript; it is
self-maintaining; and its scalability to large programs is significantly better
than with JSConTest.
  The improved scalability has two sources. First, the reimplementation is
significantly faster than the original, transformation-based implementation.
Second, the reimplementation relies on the fly-weight pattern and on trace
reduction to conserve memory. Only the combination of these techniques enables
monitoring and inference for large programs."
"We present a formal small-step structural operational semantics for a large
fragment of X10, unifying past work. The fragment covers multiple places,
mutable objects on the heap, sequencing, \code{try/catch}, \code{async},
\code{finish}, and \code{at} constructs. This model accurately captures the
behavior of a large class of concurrent, multi-place X10 programs. Further, we
introduce a formal model of resilience in X10. During execution of an X10
program, a place may fail for many reasons. Resilient X10 permits the program
to continue executing, losing the data at the failed place, and most of the
control state, and repairing the global control state in such a way that key
semantic principles hold, the Invariant Happens Before Principle, and the
Failure Masking Principle. These principles permit an X10 programmer to write
clean code that continues to work in the presence of place failure. The given
semantics have additionally been mechanized in Coq."
"A multitude of different probabilistic programming languages exists today,
all extending a traditional programming language with primitives to support
modeling of complex, structured probability distributions. Each of these
languages employs its own probabilistic primitives, and comes with a particular
syntax, semantics and inference procedure. This makes it hard to understand the
underlying programming concepts and appreciate the differences between the
different languages. To obtain a better understanding of probabilistic
programming, we identify a number of core programming concepts underlying the
primitives used by various probabilistic languages, discuss the execution
mechanisms that they require and use these to position state-of-the-art
probabilistic languages and their implementation. While doing so, we focus on
probabilistic extensions of logic programming languages such as Prolog, which
have been developed since more than 20 years."
"We present a parallel implementation of Coalgebraic Logic Programming (CoALP)
in the programming language Go. CoALP was initially introduced to reflect
coalgebraic semantics of logic programming, with coalgebraic derivation
algorithm featuring both corecursion and parallelism. Here, we discuss how the
coalgebraic semantics influenced our parallel implementation of logic
programming."
"A proxy, in general, is an object mediating access to an arbitrary target
object. The proxy is then intended to be used in place of the target object.
Ideally, a proxy is not distinguishable from other objects. Running a program
with a proxy leads to the same outcome as running the program with the target
object. Even though the approach provides a lot of power to the user, proxies
come with a limitation. Because a proxy, wrapping a target object, is a new
object and different from its target, the interposition changes the behaviour
of some core components. For distinct proxies the double == and triple ===
equal operator returns false, even if the target object is the same. More
precisely, the expected result depends on use case. To overcome this limitation
we will discuss alternatives."
"Coinductive definitions, such as that of an infinite stream, may often be
described by elegant logic programs, but ones for which SLD-refutation is of no
value as SLD-derivations fall into infinite loops. Such definitions give rise
to questions of lazy corecursive derivations and parallelism, as execution of
such logic programs can have both recursive and corecursive features at once.
Observational and coalgebraic semantics have been used to study them
abstractly. The programming developments have often occurred separately and
have usually been implementation-led. Here, we give a coherent semantics-led
account of the issues, starting with abstract category theoretic semantics,
developing coalgebra to characterise naturally arising trees, and proceeding
towards implementation of a new dialect, CoALP, of logic programming,
characterised by guarded lazy corecursion and parallelism."
"The verification community has studied dynamic data structures primarily in a
bottom-up way by analyzing pointers and the shapes induced by them. Recent work
in fields such as separation logic has made significant progress in extracting
shapes from program source code. Many real world programs however manipulate
complex data whose structure and content is most naturally described by
formalisms from object oriented programming and databases. In this paper, we
look at the verification of programs with dynamic data structures from the
perspective of content representation. Our approach is based on description
logic, a widely used knowledge representation paradigm which gives a logical
underpinning for diverse modeling frameworks such as UML and ER. Technically,
we assume that we have separation logic shape invariants obtained from a shape
analysis tool, and requirements on the program data in terms of description
logic. We show that the two-variable fragment of first order logic with
counting and trees %(whose decidability was proved at LICS 2013) can be used as
a joint framework to embed suitable fragments of description logic and
separation logic."
"In programmers point of view, Datatypes in programming language level have a
simple description but inside hardware, huge machine codes are responsible to
describe type features. Datatype architecture design is a novel approach to
match programming features along with hardware design. In this paper a novel
Data type-Based Code Reducer (TYPELINE) architecture is proposed and
implemented according to significant data types (SDT) of programming languages.
TYPELINE uses TEUs for processing various SDT operations. This architecture
design leads to reducing the number of machine codes, and increases execution
speed, and also improves some parallelism level. This is because this
architecture supports some operation for the execution of Abstract Data Types
in parallel. Also it ensures to maintain data type features and entire
application level specifications using the proposed type conversion unit. This
framework includes compiler level identifying execution modes and memory
management unit for decreasing object read/write in heap memory by ISA support.
This energy-efficient architecture is completely compatible with object
oriented programming languages and in combination mode it can process complex
C++ data structures with respect to parallel TYPELINE architecture support."
"Application Specific Instruction-set Processor (ASIP) is one of the popular
processor design techniques for embedded systems which allows customizability
in processor design without overly hindering design flexibility. Multi-pipeline
ASIPs were proposed to improve the performance of such systems by compromising
between speed and processor area. One of the problems in the multi-pipeline
design is the limited inherent instruction level parallelism (ILP) available in
applications. The ILP of application programs can be improved via a compiler
optimization technique known as loop unrolling. In this paper, we present how
loop unrolling effects the performance of multi-pipeline ASIPs. The
improvements in performance average around 15% for a number of benchmark
applications with the maximum improvement of around 30%. In addition, we
analyzed the variable of performance against loop unrolling factor, which is
the amount of unrolling we perform."
"Nowadays, we are to find out solutions to huge computing problems very
rapidly. It brings the idea of parallel computing in which several machines or
processors work cooperatively for computational tasks. In the past decades,
there are a lot of variations in perceiving the importance of parallelism in
computing machines. And it is observed that the parallel computing is a
superior solution to many of the computing limitations like speed and density;
non-recurring and high cost; and power consumption and heat dissipation etc.
The commercial multiprocessors have emerged with lower prices than the
mainframe machines and supercomputers machines. In this article the high
performance computing (HPC) through parallel programming paradigms (PPPs) are
discussed with their constructs and design approaches."
"Representation theorems relate seemingly complex objects to concrete, more
tractable ones.
  In this paper, we take advantage of the abstraction power of category theory
and provide a general representation theorem for a wide class of second-order
functionals which are polymorphic over a class of functors. Types polymorphic
over a class of functors are easily representable in languages such as Haskell,
but are difficult to analyse and reason about. The concrete representation
provided by the theorem is easier to analyse, but it might not be as convenient
to implement. Therefore, depending on the task at hand, the change of
representation may prove valuable in one direction or the other.
  We showcase the usefulness of the representation theorem with a range of
examples. Concretely, we show how the representation theorem can be used to
show that traversable functors are finitary containers, how parameterised
coalgebras relate to very well-behaved lenses, and how algebraic effects might
be implemented in a functional language."
"Linearizability is the de facto correctness criterion for concurrent data
structures. Unfortunately, linearizability imposes a performance penalty which
scales linearly in the number of contending threads. Quiescent consistency is
an alternative criterion which guarantees that a concurrent data structure
behaves correctly when accessed sequentially. Yet quiescent consistency says
very little about executions that have any contention.
  We define quantitative quiescent consistency (QQC), a relaxation of
linearizability where the degree of relaxation is proportional to the degree of
contention. When quiescent, no relaxation is allowed, and therefore QQC refines
quiescent consistency, unlike other proposed relaxations of linearizability. We
show that high performance counters and stacks designed to satisfy quiescent
consistency continue to satisfy QQC. The precise assumptions under which QQC
holds provides fresh insight on these structures. To demonstrate the robustness
of QQC, we provide three natural characterizations and prove compositionality."
"A problem that is constantly cropping up in designing even the simplest
algorithm or a program is dealing with +-1 bug when we calculate positions
within an array, very noticeably while splitting it in half. This bug is often
found in buffer overflow type of bugs. While designing one complicated
algorithm, we needed various ways of splitting an array, and we found lack of
general guidance for this apparently minor problem. We present an exercise that
tracks the cause of the problem and leads to the solution. This problem looks
trivial because it seems obvious or insignificant, however treating it without
outmost precision can lead to subtle bugs, unbalanced solution, not transparent
expressions for various languages. Basically, the exercise is about dealing
with <= < as well as n/2, n/2-1, (n+1)/2, n-1 and similar expressions when they
are rounded down to the nearest integer and used to define a range."
"We investigate the phenomenon that ""every monad is a linear state monad"". We
do this by studying a fully-complete state-passing translation from an impure
call-by-value language to a new linear type theory: the enriched call-by-value
calculus. The results are not specific to store, but can be applied to any
computational effect expressible using algebraic operations, even to effects
that are not usually thought of as stateful. There is a bijective
correspondence between generic effects in the source language and state access
operations in the enriched call-by-value calculus. From the perspective of
categorical models, the enriched call-by-value calculus suggests a refinement
of the traditional Kleisli models of effectful call-by-value languages. The new
models can be understood as enriched adjunctions."
"This paper introduces a new metaobject, the generalizer, which complements
the existing specializer metaobject. With the help of examples, we show that
this metaobject allows for the efficient implementation of complex
non-class-based dispatch within the framework of existing metaobject protocols.
We present our modifications to the generic function invocation protocol from
the Art of the Metaobject Protocol; in combination with previous work, this
produces a fully-functional extension of the existing mechanism for method
selection and combination, including support for method combination completely
independent from method selection. We discuss our implementation, within the
SBCL implementation of Common Lisp, and in that context compare the performance
of the new protocol with the standard one, demonstrating that the new protocol
can be tolerably efficient."
"This dissertation introduces executable refinement types, which refine
structural types by semi-decidable predicates, and establishes their metatheory
and accompanying implementation techniques. These results are useful for
undecidable type systems in general.
  Particular contributions include: (1) Type soundness and a logical relation
for extensional equivalence for executable refinement types (though type
checking is undecidable); (2) hybrid type checking for executable refinement
types, which blends static and dynamic checks in a novel way, in some sense
performing better statically than any decidable approximation; (3) a type
reconstruction algorithm - reconstruction is decidable even though type
checking is not, when suitably redefined to apply to undecidable type systems;
(4) a novel use of existential types with dependent types to ensure that the
language of logical formulae is closed under type checking (5) a prototype
implementation, Sage, of executable refinement types such that all dynamic
errors are communicated back to the compiler and are thenceforth static errors."
"We describe JSAI, an abstract interpreter for JavaScript. JSAI uses novel
abstract domains to compute a reduced product of type inference, pointer
analysis, string analysis, integer and boolean constant propagation, and
control-flow analysis. In addition, JSAI allows for analysis control-flow
sensitivity (i.e., context-, path-, and heap-sensitivity) to be modularly
configured without requiring any changes to the analysis implementation. JSAI
is designed to be provably sound with respect to a specific concrete semantics
for JavaScript, which has been extensively tested against existing
production-quality JavaScript implementations.
  We provide a comprehensive evaluation of JSAI's performance and precision
using an extensive benchmark suite. This benchmark suite includes real-world
JavaScript applications, machine-generated JavaScript code via Emscripten, and
browser addons. We use JSAI's configurability to evaluate a large number of
analysis sensitivities (some well-known, some novel) and observe some
surprising results. We believe that JSAI's configurability and its formal
specifications position it as a useful research platform to experiment on novel
sensitivities, abstract domains, and client analyses for JavaScript."
"Providing feedback on programming assignments manually is a tedious, error
prone, and time-consuming task. In this paper, we motivate and address the
problem of generating feedback on performance aspects in introductory
programming assignments. We studied a large number of functionally correct
student solutions to introductory programming assignments and observed: (1)
There are different algorithmic strategies, with varying levels of efficiency,
for solving a given problem. These different strategies merit different
feedback. (2) The same algorithmic strategy can be implemented in countless
different ways, which are not relevant for reporting feedback on the student
program.
  We propose a light-weight programming language extension that allows a
teacher to define an algorithmic strategy by specifying certain key values that
should occur during the execution of an implementation. We describe a dynamic
analysis based approach to test whether a student's program matches a teacher's
specification. Our experimental results illustrate the effectiveness of both
our specification language and our dynamic analysis. On one of our benchmarks
consisting of 2316 functionally correct implementations to 3 programming
problems, we identified 16 strategies that we were able to describe using our
specification language (in 95 minutes after inspecting 66, i.e., around 3%,
implementations). Our dynamic analysis correctly matched each implementation
with its corresponding specification, thereby automatically producing the
intended feedback."
"Heap data is potentially unbounded and seemingly arbitrary. As a consequence,
unlike stack and static memory, heap memory cannot be abstracted directly in
terms of a fixed set of source variable names appearing in the program being
analysed. This makes it an interesting topic of study and there is an abundance
of literature employing heap abstractions. Although most studies have addressed
similar concerns, their formulations and formalisms often seem dissimilar and
some times even unrelated. Thus, the insights gained in one description of heap
abstraction may not directly carry over to some other description. This survey
is a result of our quest for a unifying theme in the existing descriptions of
heap abstractions. In particular, our interest lies in the abstractions and not
in the algorithms that construct them.
  In our search of a unified theme, we view a heap abstraction as consisting of
two features: a heap model to represent the heap memory and a summarization
technique for bounding the heap representation. We classify the models as
storeless, store based, and hybrid. We describe various summarization
techniques based on k-limiting, allocation sites, patterns, variables, other
generic instrumentation predicates, and higher-order logics. This approach
allows us to compare the insights of a large number of seemingly dissimilar
heap abstractions and also paves way for creating new abstractions by
mix-and-match of models and summarization techniques."
"Software-defined networking (SDN) is revolutionizing the networking industry,
but current SDN programming platforms do not provide automated mechanisms for
updating global configurations on the fly. Implementing updates by hand is
challenging for SDN programmers because networks are distributed systems with
hundreds or thousands of interacting nodes. Even if initial and final
configurations are correct, naively updating individual nodes can lead to
incorrect transient behaviors, including loops, black holes, and access control
violations. This paper presents an approach for automatically synthesizing
updates that are guaranteed to preserve specified properties. We formalize
network updates as a distributed programming problem and develop a synthesis
algorithm based on counterexample-guided search and incremental model checking.
We describe a prototype implementation, and present results from experiments on
real-world topologies and properties demonstrating that our tool scales to
updates involving over one-thousand nodes."
"Data-flow analyses usually associate information with control flow regions.
Informally, if these regions are too small, like a point between two
consecutive statements, we call the analysis dense. On the other hand, if these
regions include many such points, then we call it sparse. This paper presents a
systematic method to build program representations that support sparse
analyses. To pave the way to this framework we clarify the bibliography about
well-known intermediate program representations. We show that our approach, up
to parameter choice, subsumes many of these representations, such as the SSA,
SSI and e-SSA forms. In particular, our algorithms are faster, simpler and more
frugal than the previous techniques used to construct SSI - Static Single
Information - form programs. We produce intermediate representations isomorphic
to Choi et al.'s Sparse Evaluation Graphs (SEG) for the family of data-flow
problems that can be partitioned per variables. However, contrary to SEGs, we
can handle - sparsely - problems that are not in this family."
"Both uppermost open source compilers, GCC and LLVM, are mature enough to
link-time optimize large applications. In case of large applications, we must
take into account, except standard speed efficiency and memory consumption,
different aspects. We focus on size of the code, cold start-up time, etc.
Developers of applications often come up with ad-hoc solutions such as Elfhack
utility, start-up of an application via a pre-loading utility and dlopen;
prelinking and variety of different tools that reorder functions to fit the
order of execution. The goal of the thesis is to analyse all existing
techniques of optimization, evaluate their efficiency and design new solutions
based on the link-time optimization platform."
"These are the proceedings of the 6th European Conference on Python in
Science, EuroSciPy 2013, that was held in Brussels (21-25 August 2013)."
"In the Object-Oriented Programming Systems (OOPS), these two concepts namely
function overloading and function overriding are a bit confusing to the
programmers. In this article this confusion is tried to be removed. Both of
these are the concepts which come under the polymorphism (poly means many and
morph mean forms). In the article the comparison is done in between them. For
the new programmers and the learners, it is important to understand them. The
function overloading [1] is achieved at the time of the compile and the
function overriding is achieved at the run time. The function overriding always
takes place in inheritance, but the function overloading can also take place
without inheritance."
"Picat, a new member of the logic programming family, follows a different
doctrine than Prolog in offering the core logic programming concepts: arrays
and maps as built-in data types; implicit pattern matching with explicit
unification and explicit non-determinism; functions for deterministic
computations; and loops for convenient scripting and modeling purposes. Picat
provides facilities for solving combinatorial search problems, including a
common interface with CP, SAT, and MIP solvers, tabling for dynamic
programming, and a module for planning. Picat's planner module, which is
implemented by the use of tabling, has produced surprising and encouraging
results. Thanks to term-sharing and resource-bounded tabled search, Picat
overwhelmingly outperforms the cutting-edge ASP and PDDL planners on the
planning benchmarks used in recent ASP competitions."
"To appear in Theory and Practice of Logic Programming (TPLP). Several Prolog
interpreters are based on the Warren Abstract Machine (WAM), an elegant model
to compile Prolog programs. In order to improve the performance several
strategies have been proposed, such as: optimize the selection of clauses,
specialize the unification, global analysis, native code generation and
tabling. This paper proposes a different strategy to implement an efficient
Prolog System, the creation of specialized emulators on the fly. The proposed
strategy was implemented and evaluated at YAP Prolog System, and the
experimental evaluation showed interesting results."
"Integration techniques for combining programs written in distinct language
paradigms facilitate the implementation of specialised modules in the best
language for their task. In the case of Java-Prolog integration, a known
problem is the proper representation of references to Java objects on the
Prolog side. To solve it adequately, multiple dimensions should be considered,
including reference representation, opacity of the representation, identity
preservation, reference life span, and scope of the inter-language conversion
policies. This paper presents an approach that addresses all these dimensions,
generalising and building on existing representation patterns of foreign
references in Prolog, and taking inspiration from similar inter-language
representation techniques found in other domains. Our approach maximises
portability by making few assumptions about the Prolog engine interacting with
Java (e.g., embedded or executed as an external process). We validate our work
by extending JPC, an open-source integration library, with features supporting
our approach. Our JPC library is currently compatible with three different open
source Prolog engines (SWI, YAP} and XSB) by means of drivers. To appear in
Theory and Practice of Logic Programming (TPLP)."
"To appear in Theory and Practice of Logic Programming (TPLP). Tabling is a
commonly used technique in logic programming for avoiding cyclic behavior of
logic programs and enabling more declarative program definitions. Furthermore,
tabling often improves computational performance. Rational term are terms with
one or more infinite sub-terms but with a finite representation. Rational terms
can be generated in Prolog by omitting the occurs check when unifying two
terms. Applications of rational terms include definite clause grammars,
constraint handling systems, and coinduction. In this paper, we report our
extension of YAP's Prolog tabling mechanism to support rational terms. We
describe the internal representation of rational terms within the table space
and prove its correctness. We then use this extension to implement a tabling
based approach to coinduction. We compare our approach with current coinductive
transformations and describe the implementation. In addition, we present an
algorithm that ensures a canonical representation for rational terms."
"A critical component in the implementation of a concurrent tabling system is
the design of the table space. One of the most successful proposals for
representing tables is based on a two-level trie data structure, where one trie
level stores the tabled subgoal calls and the other stores the computed
answers. In this work, we present a simple and efficient lock-free design where
both levels of the tries can be shared among threads in a concurrent
environment. To implement lock-freedom we took advantage of the CAS atomic
instruction that nowadays can be widely found on many common architectures. CAS
reduces the granularity of the synchronization when threads access concurrent
areas, but still suffers from low-level problems such as false sharing or cache
memory side-effects. In order to be as effective as possible in the concurrent
search and insert operations over the table space data structures, we based our
design on a hash trie data structure in such a way that it minimizes potential
low-level synchronization problems by dispersing as much as possible the
concurrent areas. Experimental results in the Yap Prolog system show that our
new lock-free hash trie design can effectively reduce the execution time and
scale better than previous designs."
"This introduction to Haskell is written to optimize learning by programmers
who already know OCaml."
"In his seminal paper ""A Natural Semantics for Lazy Evaluation"", John
Launchbury proves his semantics correct with respect to a denotational
semantics. We machine-checked the proof and found it to fail, and provide two
ways to fix it: One by taking a detour via a modified natural semantics with an
explicit stack, and one by adjusting the denotational semantics of heaps."
"C++ 98/03 already has a reputation for overwhelming complexity compared to
other programming languages. The raft of new features in C++ 11/14 suggests
that the complexity in the next generation of C++ code bases will overwhelm
still further. The planned C++ 17 will probably worsen matters in ways
difficult to presently imagine.
  Countervailing against this rise in software complexity is the hard
de-exponentialisation of computer hardware capacity growth expected no later
than 2020, and which will have even harder to imagine consequences on all
computer software. WG21 C++ 17 study groups SG2 (Modules), SG7 (Reflection),
SG8 (Concepts), and to a lesser extent SG10 (Feature Test) and SG12 (Undefined
Behaviour), are all fundamentally about significantly improving complexity
management in C++ 17, yet WG21's significant work on improving C++ complexity
management is rarely mentioned explicitly.
  This presentation pitches a novel implementation solution for some of these
complexity scaling problems, tying together SG2 and SG7 with parts of SG3
(Filesystem): a standardised but very lightweight transactional graph database
based on Boost.ASIO, Boost.AFIO and Boost.Graph at the very core of the C++
runtime, making future C++ codebases considerably more tractable and affordable
to all users of C++."
"An FOL-program consists of a background theory in a decidable fragment of
first-order logic and a collection of rules possibly containing first-order
formulas. The formalism stems from recent approaches to tight integrations of
ASP with description logics. In this paper, we define a well-founded semantics
for FOL-programs based on a new notion of unfounded sets on consistent as well
as inconsistent sets of literals, and study some of its properties. The
semantics is defined for all FOL-programs, including those where it is
necessary to represent inconsistencies explicitly. The semantics supports a
form of combined reasoning by rules under closed world as well as open world
assumptions, and it is a generalization of the standard well-founded semantics
for normal logic programs. We also show that the well-founded semantics defined
here approximates the well-supported answer set semantics for normal DL
programs."
"We consider the application of Constraint Handling Rules (CHR) for the
specification of type inference systems, such as that used by Haskell.
Confluence of CHR guarantees that the answer provided by type inference is
correct and consistent. The standard method for establishing confluence relies
on an assumption that the CHR program is terminating. However, many examples in
practice give rise to non-terminating CHR programs, rendering this method
inapplicable. Despite no guarantee of termination or confluence, the Glasgow
Haskell Compiler (GHC) supports options that allow the user to proceed with
type inference anyway, e.g. via the use of the UndecidableInstances flag. In
this paper we formally identify and verify a set of relaxed criteria, namely
range-restrictedness, local confluence, and ground termination, that ensure the
consistency of CHR-based type inference that maps to potentially
non-terminating CHR programs."
"Resolution-based Knowledge Representation and Reasoning (KRR) systems, such
as Flora-2, Silk or Ergo, can scale to tens or hundreds of millions of facts,
while supporting reasoning that includes Hilog, inheritance, defeasibility
theories, and equality theories. These systems handle the termination and
complexity issues that arise from the use of these features by a heavy use of
tabled resolution. In fact, such systems table by default all rules defined by
users, unless they are simple facts.
  Performing dynamic updates within such systems is nearly impossible unless
the tables themselves can be made to react to changes. Incremental tabling as
first implemented in XSB (Saha 2006) partially addressed this problem, but the
implementation was limited in scope and not always easy to use. In this paper,
we introduce transparent incremental tabling which at the semantic level
supports updates in the 3-valued well-founded semantics, while guaranteeing
full consistency of all tabled queries. Transparent incremental tabling also
has significant performance improvements over previous implementations,
including lazy recomputation, and control over the dependency structures used
to determine how tables are updated."
"We have designed a new logic programming language called LM (Linear Meld) for
programming graph-based algorithms in a declarative fashion. Our language is
based on linear logic, an expressive logical system where logical facts can be
consumed. Because LM integrates both classical and linear logic, LM tends to be
more expressive than other logic programming languages. LM programs are
naturally concurrent because facts are partitioned by nodes of a graph data
structure. Computation is performed at the node level while communication
happens between connected nodes. In this paper, we present the syntax and
operational semantics of our language and illustrate its use through a number
of examples."
"Automatic techniques for program verification usually suffer the well-known
state explosion problem. Most of the classical approaches are based on browsing
the structure of some form of model (which represents the behavior of the
program) to check if a given specification is valid. This implies that a part
of the model has to be built, and sometimes the needed fragment is quite huge.
  In this work, we provide an alternative automatic decision method to check
whether a given property, specified in a linear temporal logic, is valid w.r.t.
a tccp program. Our proposal (based on abstract interpretation techniques) does
not require to build any model at all. Our results guarantee correctness but,
as usual when using an abstract semantics, completeness is lost."
"We present the new ASP system clingo 4. Unlike its predecessors, being mere
monolithic combinations of the grounder gringo with the solver clasp, the new
clingo 4 series offers high-level constructs for realizing complex reasoning
processes. Among others, such processes feature advanced forms of search, as in
optimization or theory solving, or even interact with an environment, as in
robotics or query-answering. Common to them is that the problem specification
evolves during the reasoning process, either because data or constraints are
added, deleted, or replaced. In fact, clingo 4 carries out such complex
reasoning within a single integrated ASP grounding and solving process. This
avoids redundancies in relaunching grounder and solver programs and benefits
from the solver's learning capacities. clingo 4 accomplishes this by
complementing ASP's declarative input language by control capacities expressed
via the embedded scripting languages lua and python. On the declarative side,
clingo 4 offers a new directive that allows for structuring logic programs into
named and parameterizable subprograms. The grounding and integration of these
subprograms into the solving process is completely modular and fully
controllable from the procedural side, viz. the scripting languages. By
strictly separating logic and control programs, clingo 4 also abolishes the
need for dedicated systems for incremental and reactive reasoning, like iclingo
and oclingo, respectively, and its flexibility goes well beyond the advanced
yet still rigid solving processes of the latter."
"Two-dimensional patterns are used in many research areas in computer science,
ranging from image processing to specification and verification of complex
software systems (via scenarios). The contribution of this paper is twofold.
First, we present the basis of a new formal representation of two-dimensional
patterns based on contours and their compositions. Then, we present efficient
algorithms to verify correctness of the contour-representation. Finally, we
briefly discuss possible applications, in particular using them as a basic
instrument in developing software tools for handling two dimensional words."
"Constraint Handling Rules (CHR) has matured into a general purpose language
over the past two decades. Any general purpose language requires its own
development tools. Visualization tools, in particular, facilitate many tasks
for programmers as well as beginners to the language. The article presents
on-going work towards the visualization of CHR programs. The process is done
through source-to-source transformation. It aims towards reaching a generic
transformer to visualize different algorithms implemented in CHR. Note: An
extended abstract / full version of a paper accepted to be presented at the
Doctoral Consortium of the 30th International Conference on Logic Programming
(ICLP 2014), July 19-22, Vienna, Austria."
"This publication is to present a summary of research (referred as Klabs -
http://www.kappalabs.org) carried out in author's Ph.D studies on topic of
application of Logic Programming as scripting language for virtual character
behavior control in First Person Shooter (FPS) games. The research goal is to
apply reasoning and knowledge representation techniques to create character
behavior, which results in increased players' engagement. An extended abstract
/ full version of a paper accepted to be presented at the Doctoral Consortium
of the 30th International Conference on Logic Programming (ICLP 2014), July
19-22, Vienna, Austria"
"Several techniques and tools have been developed for verification of
properties expressed as Horn clauses with constraints over a background theory
(CHC). Current CHC verification tools implement intricate algorithms and are
often limited to certain subclasses of CHC problems. Our aim in this work is to
investigate the use of a combination of off-the-shelf techniques from the
literature in analysis and transformation of Constraint Logic Programs (CLPs)
to solve challenging CHC verification problems. We find that many problems can
be solved using a combination of tools based on well-known techniques from
abstract interpretation, semantics-preserving transformations, program
specialisation and query-answer transformations. This gives insights into the
design of automatic, more general CHC verification tools based on a library of
components."
"When developing a (web) interface for a deductive database, functionality
required by the client is provided by means of HTTP handlers that wrap the
logical data access predicates. These handlers are responsible for converting
between client and server data representations and typically include options
for paginating results. Designing the web accessible API is difficult because
it is hard to predict the exact requirements of clients. Pengines changes this
picture. The client provides a Prolog program that selects the required data by
accessing the logical API of the server. The pengine infrastructure provides
general mechanisms for converting Prolog data and handling Prolog
non-determinism. The Pengines library is small (2000 lines Prolog, 150 lines
JavaScript). It greatly simplifies defining an AJAX based client for a Prolog
program and provides non-deterministic RPC between Prolog processes as well as
interaction with Prolog engines similar to Paul Tarau's engines. Pengines are
available as a standard package for SWI-Prolog 7."
"Domain-specific languages (DSLs) are routinely created to simplify difficult
or specialized programming tasks. They expose useful abstractions and design
patterns in the form of language constructs, provide static semantics to
eagerly detect misuse of these constructs, and dynamic semantics to completely
define how language constructs interact. However, implementing and composing
DSLs is a non-trivial task, and there is a lack of tools and techniques.
  We address this problem by presenting a complete module system over LP for
DSL construction, reuse, and composition. LP is already useful for DSL design,
because it supports executable language specifications using notations familiar
to language designers. We extend LP with a module system that is simple (with a
few concepts), succinct (for key DSL specification scenarios), and composable
(on the level of languages, compilers, and programs). These design choices
reflect our use of LP for industrial DSL design. Our module system has been
implemented in the FORMULA language, and was used to build key Windows 8 device
drivers via DSLs. Though we present our module system as it actually appears in
our FORMULA language, our emphasis is on concepts adaptable to other LP
languages."
"Energy models can be constructed by characterizing the energy consumed by
executing each instruction in a processor's instruction set. This can be used
to determine how much energy is required to execute a sequence of assembly
instructions, without the need to instrument or measure hardware.
  However, statically analyzing low-level program structures is hard, and the
gap between the high-level program structure and the low-level energy models
needs to be bridged. We have developed techniques for performing a static
analysis on the intermediate compiler representations of a program.
Specifically, we target LLVM IR, a representation used by modern compilers,
including Clang. Using these techniques we can automatically infer an estimate
of the energy consumed when running a function under different platforms, using
different compilers.
  One of the challenges in doing so is that of determining an energy cost of
executing LLVM IR program segments, for which we have developed two different
approaches. When this information is used in conjunction with our analysis, we
are able to infer energy formulae that characterize the energy consumption for
a particular program. This approach can be applied to any languages targeting
the LLVM toolchain, including C and XC or architectures such as ARM Cortex-M or
XMOS xCORE, with a focus towards embedded platforms. Our techniques are
validated on these platforms by comparing the static analysis results to the
physical measurements taken from the hardware. Static energy consumption
estimation enables energy-aware software development, without requiring
hardware knowledge."
"We present a language to specify syntax guided synthesis (SyGuS) problems.
Syntax guidance is a prominent theme in contemporary program synthesis
approaches, and SyGuS was first described in [1]. This paper describes
concretely the input format of a SyGuS solver.
  [1] Rajeev Alur, Rastislav Bodik, Garvit Juniwal, Milo M. K. Martin, Mukund
Raghothaman, Sanjit A. Seshia, Rishabh Singh, Armando Solar-Lezama, Emina
Torlak, and Abhishek Udupa. Syntax-guided synthesis. In FMCAD, pages 1--17,
2013."
"Higher-order constructs extend the expressiveness of first-order (Constraint)
Logic Programming ((C)LP) both syntactically and semantically. At the same time
assertions have been in use for some time in (C)LP systems helping programmers
detect errors and validate programs. However, these assertion-based extensions
to (C)LP have not been integrated well with higher order to date. Our work
contributes to filling this gap by extending the assertion-based approach to
error detection and program validation to the higher-order context, within
(C)LP. It is based on an extension of properties and assertions as used in
(C)LP in order to be able to fully describe arguments that are predicates."
"Register allocation is a much studied problem. A particularly important
context for optimizing register allocation is within loops, since a significant
fraction of the execution time of programs is often inside loop code. A variety
of algorithms have been proposed in the past for register allocation, but the
complexity of the problem has resulted in a decoupling of several important
aspects, including loop unrolling, register promotion, and instruction
reordering. In this paper, we develop an approach to register allocation and
promotion in a unified optimization framework that simultaneously considers the
impact of loop unrolling and instruction scheduling. This is done via a novel
instruction tiling approach where instructions within a loop are represented
along one dimension and innermost loop iterations along the other dimension. By
exploiting the regularity along the loop dimension, and imposing essential
dependence based constraints on intra-tile execution order, the problem of
optimizing register pressure is cast in a constraint programming formalism.
Experimental results are provided from thousands of innermost loops extracted
from the SPEC benchmarks, demonstrating improvements over the current
state-of-the-art."
"Unification of logic variables instantly connects present and future
observations of their value, independently of their location in the data areas
of the runtime system. The paper extends this property to ""interclausal logic
variables"", an easy to implement Prolog extension that supports instant global
information exchanges without dynamic database updates. We illustrate their
usefulness with two of algorithms, {\em graph coloring} and {\em minimum
spanning tree}. Implementations of interclausal variables as source-level
transformations and as abstract machine adaptations are given. To address the
need for globally visible chained transitions of logic variables we describe a
DCG-based program transformation that extends the functionality of interclausal
variables."
"This volume contains the papers presented at the eleventh Workshop on
Constraint Handling Rules (CHR 2014), which will be held in Vienna at the
occasion of the Vienna Summer of Logic (VSL)"
"This paper extends Escardo and Oliva's selection monad to the selection monad
transformer, a general monadic framework for expressing backtracking search
algorithms in Haskell. The use of the closely related continuation monad
transformer for similar purposes is also discussed, including an implementation
of a DPLL-like SAT solver with no explicit recursion. Continuing a line of work
exploring connections between selection functions and game theory, we use the
selection monad transformer with the nondeterminism monad to obtain an
intuitive notion of backward induction for a certain class of nondeterministic
games."
"Monads are a popular tool for the working functional programmer to structure
effectful computations. This paper presents polymonads, a generalization of
monads. Polymonads give the familiar monadic bind the more general type forall
a,b. L a -> (a -> M b) -> N b, to compose computations with three different
kinds of effects, rather than just one. Polymonads subsume monads and
parameterized monads, and can express other constructions, including precise
type-and-effect systems and information flow tracking; more generally,
polymonads correspond to Tate's productoid semantic model. We show how to equip
a core language (called lambda-PM) with syntactic support for programming with
polymonads. Type inference and elaboration in lambda-PM allows programmers to
write polymonadic code directly in an ML-like syntax--our algorithms compute
principal types and produce elaborated programs wherein the binds appear
explicitly. Furthermore, we prove that the elaboration is coherent: no matter
which (type-correct) binds are chosen, the elaborated program's semantics will
be the same. Pleasingly, the inferred types are easy to read: the polymonad
laws justify (sometimes dramatic) simplifications, but with no effect on a
type's generality."
"We propose a programming model where effects are treated in a disciplined
way, and where the potential side-effects of a function are apparent in its
type signature. The type and effect of expressions can also be inferred
automatically, and we describe a polymorphic type inference system based on
Hindley-Milner style inference. A novel feature is that we support polymorphic
effects through row-polymorphism using duplicate labels. Moreover, we show that
our effects are not just syntactic labels but have a deep semantic connection
to the program. For example, if an expression can be typed without an exn
effect, then it will never throw an unhandled exception. Similar to Haskell's
`runST` we show how we can safely encapsulate stateful operations. Through the
state effect, we can also safely combine state with let-polymorphism without
needing either imperative type variables or a syntactic value restriction.
Finally, our system is implemented fully in a new language called Koka and has
been used successfully on various small to medium-sized sample programs ranging
from a Markdown processor to a tier-splitted chat application. You can try out
Koka live at www.rise4fun.com/koka/tutorial."
"CHR is a declarative, concurrent and committed choice rule-based constraint
programming language. We extend CHR with multiset comprehension patterns,
providing the programmer with the ability to write multiset rewriting rules
that can match a variable number of constraints in the store. This enables
writing more readable, concise and declarative code for algorithms that
coordinate large amounts of data or require aggregate operations. We call this
extension $\mathit{CHR}^\mathit{cp}$. We give a high-level abstract semantics
of $\mathit{CHR}^\mathit{cp}$, followed by a lower-level operational semantics.
We then show the soundness of this operational semantics with respect to the
abstract semantics."
"It is well-known that many environment-based abstract machines can be seen as
strategies in lambda calculi with explicit substitutions (ES). Recently,
graphical syntaxes and linear logic led to the linear substitution calculus
(LSC), a new approach to ES that is halfway between big-step calculi and
traditional calculi with ES. This paper studies the relationship between the
LSC and environment-based abstract machines. While traditional calculi with ES
simulate abstract machines, the LSC rather distills them: some transitions are
simulated while others vanish, as they map to a notion of structural
congruence. The distillation process unveils that abstract machines in fact
implement weak linear head reduction, a notion of evaluation having a central
role in the theory of linear logic. We show that such a pattern applies
uniformly in call-by-name, call-by-value, and call-by-need, catching many
machines in the literature. We start by distilling the KAM, the CEK, and the
ZINC, and then provide simplified versions of the SECD, the lazy KAM, and
Sestoft's machine. Along the way we also introduce some new machines with
global environments. Moreover, we show that distillation preserves the time
complexity of the executions, i.e. the LSC is a complexity-preserving
abstraction of abstract machines."
"The paper introduces a modular extension (plugin) for Java language compilers
and Integrated Development Environments (IDE) which adds operator overloading
feature to Java language while preserving backward compatibility.
  The extension use the idea of library-based language extensibility similar to
SugarJ. But unlike most language extensions, it works directly inside the
compiler and does not have any external preprocessors. This gives much faster
compilation, better language compatibility and support of native developer
tools (IDE, build tools).
  The extension plugs into javac and Eclipse Java compilers as well as in all
tools whose use the compilers such as IDEs (Netbeans, Eclipse, IntelliJ IDEA),
build tools (ant, maven, gradle), etc. No compiler, IDE, build tools
modification needed. Just add a jar library to classpath and/or install a
plugin to your IDE.
  The paper also discuss on how to build such Java compiler extensions.
  The extension source code is open on http://amelentev.github.io/java-oo/"
"In the static analysis of functional programs, pushdown flow analysis and
abstract garbage collection push the boundaries of what we can learn about
programs statically. This work illuminates and poses solutions to theoretical
and practical challenges that stand in the way of combining the power of these
techniques. Pushdown flow analysis grants unbounded yet computable polyvariance
to the analysis of return-flow in higher-order programs. Abstract garbage
collection grants unbounded polyvariance to abstract addresses which become
unreachable between invocations of the abstract contexts in which they were
created. Pushdown analysis solves the problem of precisely analyzing recursion
in higher-order languages; abstract garbage collection is essential in solving
the ""stickiness"" problem. Alone, our benchmarks demonstrate that each method
can reduce analysis times and boost precision by orders of magnitude. We
combine these methods. The challenge in marrying these techniques is not
subtle: computing the reachable control states of a pushdown system relies on
limiting access during transition to the top of the stack; abstract garbage
collection, on the other hand, needs full access to the entire stack to compute
a root set, just as concrete collection does. Conditional pushdown systems were
developed for just such a conundrum, but existing methods are ill-suited for
the dynamic nature of garbage collection. We show fully precise and approximate
solutions to the feasible paths problem for pushdown garbage-collecting
control-flow analysis. Experiments reveal synergistic interplay between garbage
collection and pushdown techniques, and the fusion demonstrates
""better-than-both-worlds"" precision."
"Program behavior may depend on parameters, which are either configured before
compilation time, or provided at run-time, e.g., by sensors or other input
devices. Parametric program analysis explores how different parameter settings
may affect the program behavior.
  In order to infer invariants depending on parameters, we introduce parametric
strategy iteration. This algorithm determines the precise least solution of
systems of integer equations depending on surplus parameters. Conceptually, our
algorithm performs ordinary strategy iteration on the given integer system for
all possible parameter settings in parallel. This is made possible by means of
region trees to represent the occurring piecewise affine functions. We indicate
that each required operation on these trees is polynomial-time if only
constantly many parameters are involved.
  Parametric strategy iteration for systems of integer equations allows to
construct parametric integer interval analysis as well as parametric analysis
of differences of integer variables. It thus provides a general technique to
realize precise parametric program analysis if numerical properties of integer
variables are of concern."
"Monotonicity in concurrent systems stipulates that, in any global state,
extant system actions remain executable when new processes are added to the
state. This concept is not only natural and common in multi-threaded software,
but also useful: if every thread's memory is finite, monotonicity often
guarantees the decidability of safety property verification even when the
number of running threads is unknown. In this paper, we show that the act of
obtaining finite-data thread abstractions for model checking can be at odds
with monotonicity: Predicate-abstracting certain widely used monotone software
results in non-monotone multi-threaded Boolean programs - the monotonicity is
lost in the abstraction. As a result, well-established sound and complete
safety checking algorithms become inapplicable; in fact, safety checking turns
out to be undecidable for the obtained class of unbounded-thread Boolean
programs. We demonstrate how the abstract programs can be modified into
monotone ones, without affecting safety properties of the non-monotone
abstraction. This significantly improves earlier approaches of enforcing
monotonicity via overapproximations."
"We developed a Functional object-oriented Parallel framework (FooPar) for
high-level high-performance computing in Scala. Central to this framework are
Distributed Memory Parallel Data structures (DPDs), i.e., collections of data
distributed in a shared nothing system together with parallel operations on
these data. In this paper, we first present FooPar's architecture and the idea
of DPDs and group communications. Then, we show how DPDs can be implemented
elegantly and efficiently in Scala based on the Traversable/Builder pattern,
unifying Functional and Object-Oriented Programming. We prove the correctness
and safety of one communication algorithm and show how specification testing
(via ScalaCheck) can be used to bridge the gap between proof and
implementation. Furthermore, we show that the group communication operations of
FooPar outperform those of the MPJ Express open source MPI-bindings for Java,
both asymptotically and empirically. FooPar has already been shown to be
capable of achieving close-to-optimal performance for dense matrix-matrix
multiplication via JNI. In this article, we present results on a parallel
implementation of the Floyd-Warshall algorithm in FooPar, achieving more than
94 % efficiency compared to the serial version on a cluster using 100 cores for
matrices of dimension 38000 x 38000."
"The introduction of lambdas in Java 8 completes the slate of
statically-typed, mainstream languages with both object-oriented and functional
features. The main motivation for lambdas in Java has been to facilitate
stream-based declarative APIs, and, therefore, easier parallelism. In this
paper, we evaluate the performance impact of lambda abstraction employed in
stream processing, for a variety of high-level languages that run on a virtual
machine (C#, F#, Java and Scala) and runtime platforms (JVM on Linux and
Windows, .NET CLR for Windows, Mono for Linux). Furthermore, we evaluate the
performance gain that two optimizing libraries (ScalaBlitz and LinqOptimizer)
can offer for C#, F# and Scala. Our study is based on small-scale
throughput-benchmarking, with significant care to isolate different factors,
consult experts on the systems involved, and identify causes and opportunities.
We find that Java exhibits high implementation maturity, which is a dominant
factor in benchmarks. At the same time, optimizing frameworks can be highly
effective for common query patterns."
"Domain theory is `a mathematical theory that serves as a foundation for the
semantics of programming languages'. Domains form the basis of a theory of
partial information, which extends the familiar notion of partial function to
encompass a whole spectrum of ""degrees of definedness"", so as to model
incremental higher-order computation (i.e., computing with infinite data
values, such as functions defined over an infinite domain like the domain of
integers, infinite trees, and such as objects of object-oriented programming).
General considerations from recursion theory dictate that partial functions are
unavoidable in any discussion of computability. Domain theory provides an
appropriately abstract setting in which the notion of a partial function can be
lifted and used to give meaning to higher types, recursive types, etc. NOOP is
a domain-theoretic model of nominally-typed OOP. NOOP was used to prove the
identification of inheritance and subtyping in mainstream nominally-typed OO
programming languages and the validity of this identification. In this report
we first present the definitions of basic domain theoretic notions and domain
constructors used in the construction of NOOP, then we present the construction
of a simple structural model of OOP called COOP as a step towards the
construction of NOOP. Like the construction of NOOP, the construction of COOP
uses earlier presented domain constructors."
"This paper proposes a pattern-matching system that enables non-linear
pattern-matching against unfree data types. The system allows multiple
occurrences of the same variables in a pattern, multiple results of
pattern-matching and modularization of the way of pattern-matching for each
data type at the same time. It enables us to represent pattern-matching against
not only algebraic data types but also unfree data types such as sets, graphs
and any other data types whose data have no canonical form and multiple ways of
decomposition. I have realized that with a rule that pattern-matching is
executed from the left side of a pattern and a rule that a binding to a
variable in a pattern can be referred to in its right side of the pattern.
Furthermore, I have realized modularization of these patterns with lexical
scoping. In my system, a pattern is not a first class object, but a
pattern-function that obtains only patterns and returns a pattern is a first
class object. This restriction simplifies the non-linear pattern-matching
system with lexical scoping. I have already implemented the pattern-matching
system in the Egison programming language."
"Programming distributed applications free from communication deadlocks and
races is complex. Preserving these properties when applications are updated at
runtime is even harder. We present DIOC, a language for programming distributed
applications that are free from deadlocks and races by construction. A DIOC
program describes a whole distributed application as a unique entity
(choreography). DIOC allows the programmer to specify which parts of the
application can be updated. At runtime, these parts may be replaced by new DIOC
fragments from outside the application. DIOC programs are compiled, generating
code for each site, in a lower-level language called DPOC. We formalise both
DIOC and DPOC semantics as labelled transition systems and prove the
correctness of the compilation as a trace equivalence result. As corollaries,
DPOC applications are free from communication deadlocks and races, even in
presence of runtime updates."
"We present AIOCJ, a framework for programming distributed adaptive
applications. Applications are programmed using AIOC, a choreographic language
suited for expressing patterns of interaction from a global point of view. AIOC
allows the programmer to specify which parts of the application can be adapted.
Adaptation takes place at runtime by means of rules, which can change during
the execution to tackle possibly unforeseen adaptation needs. AIOCJ relies on a
solid theory that ensures applications to be deadlock-free by construction also
after adaptation. We describe the architecture of AIOCJ, the design of the AIOC
language, and an empirical validation of the framework."
"Specifications in the Twelf system are based on a logic programming
interpretation of the Edinburgh Logical Framework or LF. We consider an
approach to animating such specifications using a Lambda Prolog implementation.
This approach is based on a lossy translation of the dependently typed LF
expressions into the simply typed lambda calculus (STLC) terms of Lambda Prolog
and a subsequent encoding of lost dependency information in predicates that are
defined by suitable clauses. To use this idea in an implementation of logic
programming a la Twelf, it is also necessary to translate the results found for
Lambda Prolog queries back into LF expressions. We describe such an inverse
translation and show that it has the necessary properties to facilitate an
emulation of Twelf behavior through our translation of LF specifications into
Lambda Prolog programs. A characteristic of Twelf is that it permits queries to
consist of types which have unspecified parts represented by meta-variables for
which values are to be found through computation. We show that this capability
can be supported within our translation based approach to animating Twelf
specifications."
"In this paper, we study the interleaving -- or pure merge -- operator that
most often characterizes parallelism in concurrency theory. This operator is a
principal cause of the so-called combinatorial explosion that makes very hard -
at least from the point of view of computational complexity - the analysis of
process behaviours e.g. by model-checking. The originality of our approach is
to study this combinatorial explosion phenomenon on average, relying on
advanced analytic combinatorics techniques. We study various measures that
contribute to a better understanding of the process behaviours represented as
plane rooted trees: the number of runs (corresponding to the width of the
trees), the expected total size of the trees as well as their overall shape.
Two practical outcomes of our quantitative study are also presented: (1) a
linear-time algorithm to compute the probability of a concurrent run prefix,
and (2) an efficient algorithm for uniform random sampling of concurrent runs.
These provide interesting responses to the combinatorial explosion problem."
"Arrays are such a rich and fundamental data type that they tend to be built
into a language, either in the compiler or in a large low-level library.
Defining this functionality at the user level instead provides greater
flexibility for application domains not envisioned by the language designer.
Only a few languages, such as C++ and Haskell, provide the necessary power to
define $n$-dimensional arrays, but these systems rely on compile-time
abstraction, sacrificing some flexibility. In contrast, dynamic languages make
it straightforward for the user to define any behavior they might want, but at
the possible expense of performance.
  As part of the Julia language project, we have developed an approach that
yields a novel trade-off between flexibility and compile-time analysis. The
core abstraction we use is multiple dispatch. We have come to believe that
while multiple dispatch has not been especially popular in most kinds of
programming, technical computing is its killer application. By expressing key
functions such as array indexing using multi-method signatures, a surprising
range of behaviors can be obtained, in a way that is both relatively easy to
write and amenable to compiler analysis. The compact factoring of concerns
provided by these methods makes it easier for user-defined types to behave
consistently with types in the standard library."
"This article provides an introduction to Rust, a systems language by Mozilla,
to programmers already familiar with Haskell, OCaml or other functional
languages."
"We consider the problem of automated reasoning about dynamically manipulated
data structures. The state-of-the-art methods are limited to the
unfold-and-match (U+M) paradigm, where predicates are transformed via
(un)folding operations induced from their definitions before being treated as
uninterpreted. However, proof obligations from verifying programs with
iterative loops and multiple function calls often do not succumb to this
paradigm. Our contribution is a proof method which -- beyond U+M -- performs
automatic formula re-writing by treating previously encountered obligations in
each proof path as possible induction hypotheses. This enables us, for the
first time, to systematically reason about a wide range of obligations, arising
from practical program verification. We demonstrate the power of our proof
rules on commonly used lemmas, thereby close the remaining gaps in existing
state-of-the-art systems. Another impact, probably more important, is that our
method regains the power of compositional reasoning, and shows that the usage
of user-provided lemmas is no longer needed for the existing set of benchmarks.
This not only removes the burden of coming up with the appropriate lemmas, but
also significantly boosts up the verification process, since lemma
applications, coupled with unfolding, often induce very large search space."
"Transactional Lock Elision (TLE) uses Hardware Transactional Memory (HTM) to
execute unmodified critical sections concurrently, even if they are protected
by the same lock. To ensure correctness, the transactions used to execute these
critical sections ""subscribe"" to the lock by reading it and checking that it is
available. A recent paper proposed using the tempting ""lazy subscription""
optimization for a similar technique in a different context, namely
transactional systems that use a single global lock (SGL) to protect all
transactional data. We identify several pitfalls that show that lazy
subscription \emph{is not safe} for TLE because unmodified critical sections
executing before subscribing to the lock may behave incorrectly in a number of
subtle ways. We also show that recently proposed compiler support for modifying
transaction code to ensure subscription occurs before any incorrect behavior
could manifest is not sufficient to avoid all of the pitfalls we identify. We
further argue that extending such compiler support to avoid all pitfalls would
add substantial complexity and would usually limit the extent to which
subscription can be deferred, undermining the effectiveness of the
optimization. Hardware extensions suggested in the recent proposal also do not
address all of the pitfalls we identify. In this extended version of our WTTM
2014 paper, we describe hardware extensions that make lazy subscription safe,
both for SGL-based transactional systems and for TLE, without the need for
special compiler support. We also explain how nontransactional loads can be
exploited, if available, to further enhance the effectiveness of lazy
subscription."
"Some formats of well-behaved operational specifications, correspond to
natural transformations of certain types (for example, GSOS and coGSOS laws).
These transformations have a common generalization: distributive laws of monads
over comonads. We prove that this elegant theoretical generalization has
limited practical benefits: it does not translate to any concrete rule format
that would be complete for specifications that contain both GSOS and coGSOS
rules. This is shown for the case of labeled transition systems and
deterministic stream systems."
"We present a parametric abstract domain for array content analysis. The
method maintains invariants for contiguous regions of the array, similar to the
methods of Gopan, Reps and Sagiv, and of Halbwachs and Peron. However, it
introduces a novel concept of an array content graph, avoiding the need for an
up-front factorial partitioning step. The resulting analysis can be used with
arbitrary numeric relational abstract domains; we evaluate the domain on a
range of array manipulating program fragments."
"Computer programming is among the fundamental aspects of computer science
curriculum. Many students first introduced to introductory computer programming
courses experience difficulties in learning and comprehending. Vast amount of
researches have revealed that, generally programming courses are regarded as
difficult and challenging and thus often have the highest dropout rates.
Moreover, numerous researches have devoted in delivering new approaches and
tools in enhancing the process of teaching and learning computer programming to
novice programmers. One among the tools that have emerged to offer positive
results is Program Visualization tool (Visualizer). Visualizers have shown
remarkable contributions in facilitating novices to learn and comprehend
computer programming. In addition to that, an approach to visualize codes
execution, Memory Transfer Language (MTL), allows a novice to animate the code
through paper and pencil mechanism without actively involving the machine. MTL
depends on the concepts of RAM (Random Access Memory) to interpret the code
line by line. Programming requires effort and special approach in the way it is
learned and taught, thus this paper aimed at presenting a proposed framework
for developing a visualizer that employs the use of MTL to enhance teaching and
learning programming."
"Session types model structured communication-based programming. In
particular, binary session types for the pi-calculus describe communication
between exactly two participants in a distributed scenario. Adding sessions to
the pi-calculus means augmenting it with type and term constructs. In a
previous paper, we tried to understand to which extent the session constructs
are more complex and expressive than the standard pi-calculus constructs. Thus,
we presented an encoding of binary session pi-calculus to the standard typed
pi-calculus by adopting linear and variant types and the continuation-passing
principle. In the present paper, we focus on recursive session types and we
present an encoding into recursive linear pi-calculus. This encoding is a
conservative extension of the former in that it preserves the results therein
obtained. Most importantly, it adopts a new treatment of the duality relation,
which in the presence of recursive types has been proven to be quite
challenging."
"Numerical software, common in scientific computing or embedded systems,
inevitably uses an approximation of the real arithmetic in which most
algorithms are designed. In many domains, roundoff errors are not the only
source of inaccuracy and measurement and other input errors further increase
the uncertainty of the computed results. Adequate tools are needed to help
users select suitable approximations, especially for safety-critical
applications.
  We present the source-to-source compiler Rosa which takes as input a
real-valued program with error specifications and synthesizes code over an
appropriate floating-point or fixed-point data type. The main challenge of such
a compiler is a fully automated, sound and yet accurate enough numerical error
estimation. We present a unified technique for floating-point and fixed-point
arithmetic of various precisions which can handle nonlinear arithmetic,
determine closed- form symbolic invariants for unbounded loops and quantify the
effects of discontinuities on numerical errors. We evaluate Rosa on a number of
benchmarks from scientific computing and embedded systems and, comparing it to
state-of-the-art in automated error estimation, show it presents an interesting
trade-off between accuracy and performance."
"The standard algorithm for higher-order contract checking can lead to
unbounded space consumption and can destroy tail recursion, altering a
program's asymptotic space complexity. While space efficiency for gradual
types---contracts mediating untyped and typed code---is well studied, sound
space efficiency for manifest contracts---contracts that check stronger
properties than simple types, e.g., ""is a natural"" instead of ""is an
integer""---remains an open problem.
  We show how to achieve sound space efficiency for manifest contracts with
strong predicate contracts. The essential trick is breaking the contract
checking down into coercions: structured, blame-annotated lists of checks. By
carefully preventing duplicate coercions from appearing, we can restore space
efficiency while keeping the same observable behavior.
  Along the way, we define a framework for space efficiency, traversing the
design space with three different space-efficient manifest calculi. We examine
the diverse correctness criteria for contract semantics; we conclude with a
coercion-based language whose contracts enjoy (galactically) bounded, sound
space consumption---they are observationally equivalent to the standard,
space-inefficient semantics."
"Looping is one of the fundamental logical instructions used for repeating a
block of code. It is used in programs across all programming languages.
Traditionally, in languages like C, the for loop is used extensively for
repeated execution of a block of code, due to its ease for use and simplified
representation. This paper proposes a new way of representing the for loop to
improve its runtime efficiency and compares the experimental statistics with
the traditional for loop representation. It is found that for small number of
iterations, the difference in computational time may not be considerable. But
given any large number of iterations, the difference is noticeable."
"In software engineering, taking a good election between recursion and
iteration is essential because their efficiency and maintenance are different.
In fact, developers often need to transform iteration into recursion (e.g., in
debugging, to decompose the call graph into iterations); thus, it is quite
surprising that there does not exist a public transformation from loops to
recursion that handles all kinds of loops. This article describes a
transformation able to transform iterative loops into equivalent recursive
methods. The transformation is described for the programming language Java, but
it is general enough as to be adapted to many other languages that allow
iteration and recursion. We describe the changes needed to transform loops of
types while/do/for/foreach into recursion. Each kind of loop requires a
particular treatment that is described and exemplified."
"We present a new technique called type targeted testing, which translates
precise refinement types into comprehensive test-suites. The key insight behind
our approach is that through the lens of SMT solvers, refinement types can also
be viewed as a high-level, declarative, test generation technique, wherein
types are converted to SMT queries whose models can be decoded into concrete
program inputs. Our approach enables the systematic and exhaustive testing of
implementations from high-level declarative specifications, and furthermore,
provides a gradual path from testing to full verification. We have implemented
our approach as a Haskell testing tool called TARGET, and present an evaluation
that shows how TARGET can be used to test a wide variety of properties and how
it compares against state-of-the-art testing approaches."
"Polymorphism in programming languages enables code reuse. Here, we show that
polymorphism has broad applicability far beyond computations for technical
computing: parallelism in distributed computing, presentation of visualizations
of runtime data flow, and proofs for formal verification of correctness. The
ability to reuse a single codebase for all these purposes provides new ways to
understand and verify parallel programs."
"In our ongoing work, we use constraint automata to compile protocol
specifications expressed as Reo connectors into efficient executable code,
e.g., in C. We have by now studied this automata based compilation approach
rather well, and have devised effective solutions to some of its problems.
Because our approach is based on constraint automata, the approach, its
problems, and our solutions are in fact useful and relevant well beyond the
specific case of compiling Reo. In this short paper, we identify and analyze
two such rather unexpected problems."
"Dynamically typed programming languages such as JavaScript and Python defer
type checking to run time. In order to maximize performance, dynamic language
VM implementations must attempt to eliminate redundant dynamic type checks.
However, type inference analyses are often costly and involve tradeoffs between
compilation time and resulting precision. This has lead to the creation of
increasingly complex multi-tiered VM architectures.
  This paper introduces lazy basic block versioning, a simple JIT compilation
technique which effectively removes redundant type checks from critical code
paths. This novel approach lazily generates type-specialized versions of basic
blocks on-the-fly while propagating context-dependent type information. This
does not require the use of costly program analyses, is not restricted by the
precision limitations of traditional type analyses and avoids the
implementation complexity of speculative optimization techniques.
  We have implemented intraprocedural lazy basic block versioning in a
JavaScript JIT compiler. This approach is compared with a classical flow-based
type analysis. Lazy basic block versioning performs as well or better on all
benchmarks. On average, 71% of type tests are eliminated, yielding speedups of
up to 50%. We also show that our implementation generates more efficient
machine code than TraceMonkey, a tracing JIT compiler for JavaScript, on
several benchmarks. The combination of implementation simplicity, low
algorithmic complexity and good run time performance makes basic block
versioning attractive for baseline JIT compilers."
"The design and implementation of static analyzers has become increasingly
systematic. Yet for a given language or analysis feature, it often requires
tedious and error prone work to implement an analyzer and prove it sound. In
short, static analysis features and their proofs of soundness do not compose
well, causing a dearth of reuse in both implementation and metatheory.
  We solve the problem of systematically constructing static analyzers by
introducing Galois transformers: monad transformers that transport Galois
connection properties. In concert with a monadic interpreter, we define a
library of monad transformers that implement building blocks for classic
analysis parameters like context, path, and heap (in)sensitivity. Moreover,
these can be composed together independent of the language being analyzed.
  Significantly, a Galois transformer can be proved sound once and for all,
making it a reusable analysis component. As new analysis features and
abstractions are developed and mixed in, soundness proofs need not be
reconstructed, as the composition of a monad transformer stack is sound by
virtue of its constituents. Galois transformers provide a viable foundation for
reusable and composable metatheory for program analysis.
  Finally, these Galois transformers shift the level of abstraction in analysis
design and implementation to a level where non-specialists have the ability to
synthesize sound analyzers over a number of parameters."
"In this paper, we study the problem of generating inputs to a higher-order
program causing it to error. We first study the problem in the setting of PCF,
a typed, core functional language and contribute the first relatively complete
method for constructing counterexamples for PCF programs. The method is
relatively complete in the sense of Hoare logic; completeness is reduced to the
completeness of a first-order solver over the base types of PCF. In practice,
this means an SMT solver can be used for the effective, automated generation of
higher-order counterexamples for a large class of programs.
  We achieve this result by employing a novel form of symbolic execution for
higher-order programs. The remarkable aspect of this symbolic execution is that
even though symbolic higher-order inputs and values are considered, the path
condition remains a first-order formula. Our handling of symbolic function
application enables the reconstruction of higher-order counterexamples from
this first-order formula.
  After establishing our main theoretical results, we sketch how to apply the
approach to untyped, higher-order, stateful languages with first-class
contracts and show how counterexample generation can be used to detect contract
violations in this setting. To validate our approach, we implement a tool
generating counterexamples for erroneous modules written in Racket."
"To represent mutually exclusive procedures, we propose a choice-conjunctive
declaration statement of the form $uchoo(S,R)$ where $S, R$ are the procedure
declaration statements within a module. This statement has the following
semantics: request the machine to choose a successful one between $S$ and $R$.
This statement is useful for representing objects with mutually exclusive
procedures. We illustrate our idea via C^uchoo, an extension of the core C with
a new statement."
"The original liveness based flow and context sensitive points-to analysis
(LFCPA) is restricted to scalar pointer variables and scalar pointees on stack
and static memory. In this paper, we extend it to support heap memory and
pointer expressions involving structures, unions, arrays, and pointer
arithmetic. The key idea behind these extensions involves constructing bounded
names for locations in terms of compile time constants (names and fixed
offsets), and introducing sound approximations when it is not possible to do
so. We achieve this by defining a grammar for pointer expressions, suitable
memory models and location naming conventions, and some key evaluations of
pointer expressions that compute the named locations. These extensions preserve
the spirit of the original LFCPA which is evidenced by the fact that although
the lattices and flow functions change, the overall data flow equations remain
unchanged."
"In order to achieve competitive performance, abstract machines for Prolog and
related languages end up being large and intricate, and incorporate
sophisticated optimizations, both at the design and at the implementation
levels. At the same time, efficiency considerations make it necessary to use
low-level languages in their implementation. This makes them laborious to code,
optimize, and, especially, maintain and extend. Writing the abstract machine
(and ancillary code) in a higher-level language can help tame this inherent
complexity. We show how the semantics of most basic components of an efficient
virtual machine for Prolog can be described using (a variant of) Prolog. These
descriptions are then compiled to C and assembled to build a complete bytecode
emulator. Thanks to the high level of the language used and its closeness to
Prolog, the abstract machine description can be manipulated using standard
Prolog compilation and optimization techniques with relative ease. We also show
how, by applying program transformations selectively, we obtain abstract
machine implementations whose performance can match and even exceed that of
state-of-the-art, highly-tuned, hand-crafted emulators."
"Profile-Guided Optimization (PGO) is an excellent means to improve the
performance of a compiled program. Indeed, the execution path data it provides
helps the compiler to generate better code and better cacheline packing.
  At the time of this writing, compilers only support instrumentation-based
PGO. This proved effective for optimizing programs. However, few projects use
it, due to its complicated dual-compilation model and its high overhead. Our
solution of sampling Hardware Performance Counters overcome these drawbacks. In
this paper, we propose a PGO solution for GCC by sampling Last Branch Record
(LBR) events and using debug symbols to recreate source locations of binary
instructions.
  By using LBR-Sampling, the generated profiles are very accurate. This
solution achieved an average of 83% of the gains obtained with
instrumentation-based PGO and 93% on C++ benchmarks only. The profiling
overhead is only 1.06% on average whereas instrumentation incurs a 16% overhead
on average."
"Tracing just-in-time compilation is a popular compilation technique for the
efficient implementation of dynamic languages, which is commonly used for
JavaScript, Python and PHP. We provide a formal model of tracing JIT
compilation of programs using abstract interpretation. Hot path detection
corresponds to an abstraction of the trace semantics of the program. The
optimization phase corresponds to a transform of the original program that
preserves its trace semantics up to an observation modeled by some abstraction.
We provide a generic framework to express dynamic optimizations and prove them
correct. We instantiate it to prove the correctness of dynamic type
specialization and constant variable folding. We show that our framework is
more general than the model of tracing compilation introduced by Guo and
Palsberg [2011] based on operational bisimulations."
"Avoiding access conflicts is a major challenge in the design of
multi-threaded programs. In the context of real-time systems, the absence of
conflicts can be guaranteed by ensuring that no two potentially conflicting
accesses are ever scheduled concurrently.In this paper, we analyze programs
that carry time annotations specifying the time for executing each statement.
We propose a technique for verifying that a multi-threaded program with time
annotations is free of access conflicts. In particular, we generate constraints
that reflect the possible schedules for executing the program and the required
properties. We then invoke an SMT solver in order to verify that no execution
gives rise to concurrent conflicting accesses. Otherwise, we obtain a trace
that exhibits the access conflict."
"Sequential tasks cannot be effectively handled in logic programming based on
classical logic or linear logic. This limitation can be addressed by using a
fragment of Japaridze'sSequential tasks cannot be effectively handled in logic
programming based on classical logic or linear logic. This limitation can be
addressed by using a fragment of Japaridze's computability logic. We propose
\seqweb, an extension to LogicWeb with sequential goal formulas. SeqWeb extends
the LogicWeb by allowing goals of the form $G\seqand G$ and $G\seqor G$ where
$G$ is a goal. These goals allow us to specify both sequential-conjunctive and
sequential-disjunctive tasks. computability logic. We propose \seqweb, an
extension to LogicWeb with sequential goal formulas. SeqWeb extends the
LogicWeb by allowing goals of the form $G\seqand G$ and $G\seqor G$ where $G$
is a goal. These goals allow us to specify both sequential-conjunctive and
sequential-disjunctive tasks."
"This document contains a description of a Common Lisp extension that allows a
programmer to write functional programs that use ""normal order"" evaluation, as
in ""non-strict"" languages like Haskell. The extension is relatively
straightforward, and it appears to be the first one such that is integrated in
the overall Common Lisp framework."
"In this paper we demonstrate several examples of solving challenging
algorithmic problems from the Google Code Jam programming contest with the
Prolog-based ECLiPSe system using declarative techniques like constraint logic
programming and linear (integer) programming. These problems were designed to
be solved by inventing clever algorithms and efficiently implementing them in a
conventional imperative programming language, but we present relatively simple
declarative programs in ECLiPSe that are fast enough to find answers within the
time limit imposed by the contest rules. We claim that declarative programming
with ECLiPSe is better suited for solving certain common kinds of programming
problems offered in Google Code Jam than imperative programming. We show this
by comparing the mental steps required to come up with both kinds of solutions."
"We present a set of rules for compiling a Dalvik bytecode program into a
logic program with array constraints. Non-termination of the resulting program
entails that of the original one, hence the techniques we have presented before
for proving non-termination of constraint logic programs can be used for
proving non-termination of Dalvik programs."
"Many probabilistic programming languages allow programs to be run under
constraints in order to carry out Bayesian inference. Running programs under
constraints could enable other uses such as rare event simulation and
probabilistic verification---except that all such probabilistic languages are
necessarily limited because they are defined or implemented in terms of an
impoverished theory of probability. Measure-theoretic probability provides a
more general foundation, but its generality makes finding computational content
difficult.
  We develop a measure-theoretic semantics for a first-order probabilistic
language with recursion, which interprets programs as functions that compute
preimages. Preimage functions are generally uncomputable, so we derive an
abstract semantics. We implement the abstract semantics and use the
implementation to carry out Bayesian inference, stochastic ray tracing (a rare
event simulation), and probabilistic verification of floating-point error
bounds."
"This report presents the general picture of how Control Network Programming
can be effectively used for implementing various search strategies, both blind
and informed. An interesting possibility is non - procedural solutions that can
be developed for most local search algorithms. A generic solution is described
for procedural implementations."
"This report presents the Dafny language and verifier, with a focus on
describing the main features of the language, including pre- and
postconditions, assertions, loop invariants, termination metrics, quantifiers,
predicates and frames. Examples of Dafny code are provided to illustrate the
use of each feature, and an overview of how Dafny translates programming code
into a mathematical proof of functional verification is presented. The report
also includes references to useful resources on Dafny, with mentions of related
works in the domain of specification languages."
"Locks have been widely used as an effective synchronization mechanism among
processes and threads. However, we observe that a large number of false
inter-thread dependencies (i.e., unnecessary lock contentions) exist during the
program execution on multicore processors, thereby incurring significant
performance overhead. This paper presents a performance debugging framework,
PERFPLAY, to facilitate a comprehensive and in-depth understanding of the
performance impact of unnecessary lock contentions. The core technique of our
debugging framework is trace replay. Specifically, PERFPLAY records the program
execution trace, on the basis of which the unnecessary lock contentions can be
identified through trace analysis. We then propose a novel technique of trace
transformation to transform these identified unnecessary lock contentions in
the original trace into the correct pattern as a new trace free of unnecessary
lock contentions. Through replaying both traces, PERFPLAY can quantify the
performance impact of unnecessary lock contentions. To demonstrate the
effectiveness of our debugging framework, we study five real-world programs and
PARSEC benchmarks. Our experimental results demonstrate the significant
performance overhead of unnecessary lock contentions, and the effectiveness of
PERFPLAY in identifying the performance critical unnecessary lock contentions
in real applications."
"Reducing energy consumption is one of the key challenges in computing
technology. One factor that contributes to high energy consumption is that all
parts of the program are considered equally significant for the accuracy of the
end-result. However, in many cases, parts of computations can be performed in
an approximate way, or even dropped, without affecting the quality of the final
output to a significant degree.
  In this paper, we introduce a task-based programming model and runtime system
that exploit this observation to trade off the quality of program outputs for
increased energy-efficiency. This is done in a structured and flexible way,
allowing for easy exploitation of different execution points in the
quality/energy space, without code modifications and without adversely
affecting application performance. The programmer specifies the significance of
tasks, and optionally provides approximations for them. Moreover, she provides
hints to the runtime on the percentage of tasks that should be executed
accurately in order to reach the target quality of results. The runtime system
can apply a number of different policies to decide whether it will execute each
individual less-significant task in its accurate form, or in its approximate
version. Policies differ in terms of their runtime overhead but also the degree
to which they manage to execute tasks according to the programmer's
specification.
  The results from experiments performed on top of an Intel-based
multicore/multiprocessor platform show that, depending on the runtime policy
used, our system can achieve an energy reduction of up to 83% compared with a
fully accurate execution and up to 35% compared with an approximate version
employing loop perforation. At the same time, our approach always results in
graceful quality degradation."
"Buffer overflow detection and mitigation for C programs has been an important
concern for a long time. This paper defines a string buffer overflow analysis
for C programs. The key ideas of our formulation are (a) separating buffers
from the pointers that point to them, (b) modelling buffers in terms of sizes
and sets of positions of null characters, and (c) defining stateless functions
to compute the sets of null positions and mappings between buffers and
pointers.
  This exercise has been carried out to test the feasibility of describing such
an analysis in terms of lattice valued functions and relations to facilitate
automatic construction of an analyser without the user having to write
C/C++/Java code. This is facilitated by devising stateless formulations because
stateful formulations combine features through side effects in states raising a
natural requirement of C/C++/Java code to be written to describe them. Given
the above motivation, the focus of this paper is not to build good static
approximations for buffer overflow analysis but to show how given static
approximations could be formalized in terms of stateless formulations so that
they become amenable to automatic construction of analysers."
"An Application Specific Instruction set Processor (ASIP) is an important
component in designing embedded systems. One of the problems in designing an
instruction set for such processors is determining the number of registers is
needed in the processor that will optimize the computational time and the cost.
The performance of a processor may fall short due to register spilling, which
is caused by the lack of available registers in a processor. In the design
perspective, it will result in processors with great performance and low power
consumption if we can avoid register spilling by deciding a value for the
number of registers needed in an ASIP. However, as of now, it has not clearly
been recognized how the number of registers changes with different application
domains. In this paper, we evaluated whether different application domains have
any significant effect on register spilling and therefore the performance of a
processor so that we could use different number of registers when building
ASIPs for different application domains rather than using a constant set of
registers. Such utilization of registers will result in processors with high
performance, low cost and low power consumption."
"Graph processing is used extensively in areas from social networking mining
to web indexing. We demonstrate that the performance and dependability of such
applications critically hinges on the graph data structure used, because a
fixed, compile-time choice of data structure can lead to poor performance or
applications unable to complete. To address this problem, we introduce an
approach that helps programmers transform regular, off-the-shelf graph
applications into adaptive, more dependable applications where adaptations are
performed via runtime selection from alternate data structure representations.
Using our approach, applications dynamically adapt to the input graph's
characteristics and changes in available memory so they continue to run when
faced with adverse conditions such as low memory. Experiments with graph
algorithms on real-world (e.g., Wikipedia metadata, Gnutella topology) and
synthetic graph datasets show that our adaptive applications run to completion
with lower execution time and/or memory utilization in comparison to their
non-adaptive versions."
"Asynchronous programming has appeared as a programming style that overcomes
undesired properties of concurrent programming. Typically in asynchronous
models of programming, methods are posted into a post list for latter
execution. The order of method executions is serial, but nondeterministic. This
paper presents a new and simple, yet powerful, model for asynchronous
programming. The proposed model consists of two components; a context-free
grammar and an operational semantics. The model is supported by the ability to
express important applications. An advantage of our model over related work is
that the model simplifies the way posted methods are assigned priorities.
Another advantage is that the operational semantics uses the simple concept of
singly linked list to simulate the prioritized process of methods posting and
execution. The simplicity and expressiveness make it relatively easy for
analysis algorithms to disclose the otherwise un-captured programming bugs in
asynchronous programs."
"For the past several decades, programmers have been modeling things in the
world with trees using hierarchies of classes and object-oriented programming
(OOP) languages. In this paper, we describe a novel approach to programming,
called concept-oriented programming (COP), which generalizes classes and
inheritance by introducing concepts and inclusion, respectively."
"Narrowing is a procedure that was first studied in the context of equational
E-unification and that has been used in a wide range of applications. The
classic completeness result due to Hullot states that any term rewriting
derivation starting from an instance of an expression can be ""lifted"" to a
narrowing derivation, whenever the substitution employed is normalized. In this
paper we adapt the generator- based extra-variables-elimination transformation
used in functional-logic programming to overcome that limitation, so we are
able to lift term rewriting derivations starting from arbitrary instances of
expressions. The proposed technique is limited to left-linear constructor
systems and to derivations reaching a ground expression. We also present a
Maude-based implementation of the technique, using natural rewriting for the
on-demand evaluation strategy."
"We address the problem of checking state reachability for programs running
under Total Store Order (TSO). The problem has been shown to be decidable but
the cost is prohibitive, namely non-primitive recursive. We propose here to
give up completeness. Our contribution is a new algorithm for TSO reachability:
it uses the standard SC semantics and introduces the TSO semantics lazily and
only where needed. At the heart of our algorithm is an iterative refinement of
the program of interest. If the program's goal state is SC-reachable, we are
done. If the goal state is not SC-reachable, this may be due to the fact that
SC under-approximates TSO. We employ a second algorithm that determines TSO
computations which are infeasible under SC, and hence likely to lead to new
states. We enrich the program to emulate, under SC, these TSO computations.
Altogether, this yields an iterative under-approximation that we prove sound
and complete for bug hunting, i.e., a semi-decision procedure halting for
positive cases of reachability. We have implemented the procedure as an
extension to the tool Trencher and compared it to the Memorax and CBMC model
checkers."
"Exact polyhedral model (PM) can be built in the general case if the only
control structures are {\tt do}-loops and structured {\tt if}s, and if loop
counter bounds, array subscripts and {\tt if}-conditions are affine expressions
of enclosing loop counters and possibly some integer constants. In more general
dynamic control programs, where arbitrary {\tt if}s and {\tt while}s are
allowed, in the general case the usual dataflow analysis can be only fuzzy.
This is not a problem when PM is used just for guiding the parallelizing
transformations, but is insufficient for transforming source programs to other
computation models (CM) relying on the PM, such as our version of dataflow CM
or the well-known KPN.
  The paper presents a novel way of building the exact polyhedral model and an
extension of the concept of the exact PM, which allowed us to add in a natural
way all the processing related to the data dependent conditions. Currently, in
our system, only arbirary {\tt if}s (not {\tt while}s) are allowed in input
programs. The resulting polyhedral model can be easily put out as an equivalent
program with the dataflow computation semantics."
"Programs that process data that reside in files are widely used in varied
domains, such as banking, healthcare, and web-traffic analysis. Precise static
analysis of these programs in the context of software verification and
transformation tasks is a challenging problem. Our key insight is that static
analysis of file-processing programs can be made more useful if knowledge of
the input file formats of these programs is made available to the analysis. We
propose a generic framework that is able to perform any given underlying
abstract interpretation on the program, while restricting the attention of the
analysis to program paths that are potentially feasible when the program's
input conforms to the given file format specification. We describe an
implementation of our approach, and present empirical results using real and
realistic programs that show how our approach enables novel verification and
transformation tasks, and also improves the precision of standard analysis
problems."
"To prevent concurrency errors, programmers need to obey a locking discipline.
Annotations that specify that discipline, such as Java's @GuardedBy, are
already widely used. Unfortunately, their semantics is expressed informally and
is consequently ambiguous. This article highlights such ambiguities and
formalizes the semantics of @GuardedBy in two alternative ways, building on an
operational semantics for a small concurrent fragment of a Java-like language.
It also identifies when such annotations are actual guarantees against data
races. Our work aids in understanding the annotations and supports the
development of sound formal tools that verify or infer such annotations."
"This paper presents a formalized framework for defining corecursive functions
safely in a total setting, based on corecursion up-to and relational
parametricity. The end product is a general corecursor that allows corecursive
(and even recursive) calls under well-behaved operations, including
constructors. Corecursive functions that are well behaved can be registered as
such, thereby increasing the corecursor's expressiveness. The metatheory is
formalized in the Isabelle proof assistant and forms the core of a prototype
tool. The corecursor is derived from first principles, without requiring new
axioms or extensions of the logic."
"The rapidly increasing number of cores available in multicore processors does
not necessarily lead directly to a commensurate increase in performance:
programs written in conventional languages, such as C, need careful
restructuring, preferably automatically, before the benefits can be observed in
improved run-times. Even then, much depends upon the intrinsic capacity of the
original program for concurrent execution. The subject of this paper is the
performance gains from the combined effect of the complementary techniques of
the Decoupled Software Pipeline (DSWP) and (backward) slicing. DSWP extracts
threadlevel parallelism from the body of a loop by breaking it into stages
which are then executed pipeline style: in effect cutting across the control
chain. Slicing, on the other hand, cuts the program along the control chain,
teasing out finer threads that depend on different variables (or locations).
parts that depend on different variables. The main contribution of this paper
is to demonstrate that the application of DSWP, followed by slicing offers
notable improvements over DSWP alone, especially when there is a loop-carried
dependence that prevents the application of the simpler DOALL optimization.
Experimental results show an improvement of a factor of ?1.6 for DSWP + slicing
over DSWP alone and a factor of ?2.4 for DSWP + slicing over the original
sequential code."
"In previous work carried out in the setting of program algebra, including
work in the area of instruction sequence size complexity, we chose instruction
sets for Boolean registers that contain only instructions of a few of the
possible kinds. In the current paper, we study instruction sequence size
bounded functional completeness of all possible instruction sets for Boolean
registers. We expect that the results of this study will turn out to be useful
to adequately assess results of work that is concerned with lower bounds of
instruction sequence size complexity."
"Dynamic languages are praised for their flexibility and expressiveness, but
static analysis often yields many false positives and verification is
cumbersome for lack of structure. Hence, unit testing is the prevalent
incomplete method for validating programs in such languages.
  Falsification is an alternative approach that uncovers definite errors in
programs. A falsifier computes a set of inputs that definitely crash a program.
  Success typing is a type-based approach to document programs in dynamic
languages. We demonstrate that success typing is, in fact, an instance of
falsification by mapping success (input) types into suitable logic formulae.
Output types are represented by recursive types. We prove the correctness of
our mapping (which establishes that success typing is falsification) and we
report some experiences with a prototype implementation."
"We present an overview of some recent efforts aimed at the development of
Choreographic Programming, a programming paradigm for the production of
concurrent software that is guaranteed to be correct by construction from
global descriptions of communication behaviour."
"Linear and substructural types are powerful tools, but adding them to
standard functional programming languages often means introducing extra
annotations and typing machinery. We propose a lightweight substructural type
system design that recasts the structural rules of weakening and contraction as
type classes; we demonstrate this design in a prototype language, Clamp.
  Clamp supports polymorphic substructural types as well as an expressive
system of mutable references. At the same time, it adds little additional
overhead to a standard Damas-Hindley-Milner type system enriched with type
classes. We have established type safety for the core model and implemented a
type checker with type inference in Haskell."
"Despite widespread interest in multicore computing, concur- rency models in
mainstream languages often lead to subtle, error-prone code.
  Observationally Cooperative Multithreading (OCM) is a new approach to
shared-memory parallelism. Programmers write code using the well-understood
cooperative (i.e., nonpreemptive) multithreading model for uniprocessors. OCM
then allows threads to run in parallel, so long as results remain consistent
with the cooperative model.
  Programmers benefit because they can reason largely sequentially. Remaining
interthread interactions are far less chaotic than in other models, permitting
easier reasoning and debugging. Programmers can also defer the choice of
concurrency-control mechanism (e.g., locks or transactions) until after they
have written their programs, at which point they can compare
concurrency-control strategies and choose the one that offers the best
performance. Implementers and researchers also benefit from the agnostic nature
of OCM -- it provides a level of abstraction to investigate, compare, and
combine a variety of interesting concurrency-control techniques."
"We developed StarL as a framework for programming, simulating, and verifying
distributed systems that interacts with physical processes. StarL framework has
(a) a collection of distributed primitives for coordination, such as mutual
exclusion, registration and geocast that can be used to build sophisticated
applications, (b) theory libraries for verifying StarL applications in the PVS
theorem prover, and (c) an execution environment that can be used to deploy the
applications on hardware or to execute them in a discrete event simulator. The
primitives have (i) abstract, nondeterministic specifications in terms of
invariants, and assume-guarantee style progress properties, (ii)
implementations in Java/Android that always satisfy the invariants and attempt
progress using best effort strategies. The PVS theories specify the invariant
and progress properties of the primitives, and have to be appropriately
instantiated and composed with the application's state machine to prove
properties about the application. We have built two execution environments: one
for deploying applications on Android/iRobot Create platform and a second one
for simulating large instantiations of the applications in a discrete even
simulator. The capabilities are illustrated with a StarL application for
vehicle to vehicle coordination in a automatic intersection that uses
primitives for point-to-point motion, mutual exclusion, and registration."
"A comparison of Landin's form of lambda calculus with Church's shows that,
independently of the lambda calculus, there exists a mechanism for converting
functions with arguments indexed by variables to the usual kind of function
where the arguments are indexed numerically. We call this the ""lambda
mechanism"" and show how it can be used in other calculi. In first-order
predicate logic it can be used to define new functions and new predicates in
terms of existing ones. In a purely imperative programming language it can be
used to provide an Algol-like procedure facility."
"We present a new CSP- and SAT-based approach for coordinating interfaces of
distributed stream-connected components provided as closed-source services. The
Kahn Process Network (KPN) is taken as a formal model of computation and a
Message Definition Language (MDL) is introduced to describe the format of
messages communicated between the processes. MDL links input and output
interfaces of a node to support flow inheritance and contextualisation. Since
interfaces can also be linked by the existence of a data channel between them,
the match is generally not only partial but also substantially nonlocal. The
KPN communication graph thus becomes a graph of interlocked constraints to be
satisfied by specific instances of the variables. We present an algorithm that
solves the CSP by iterative approximation while generating an adjunct Boolean
SAT problem on the way. We developed a solver in OCaml as well as tools that
analyse the source code of KPN vertices to derive MDL terms and automatically
modify the code by propagating type definitions back to the vertices after the
CSP has been solved. Techniques and approaches are illustrated on a KPN
implementing an image processing algorithm as a running example."
"Non-trivial analysis problems require posets with infinite ascending and
descending chains. In order to compute reasonably precise post-fixpoints of the
resulting systems of equations, Cousot and Cousot have suggested accelerated
fixpoint iteration by means of widening and narrowing.
  The strict separation into phases, however, may unnecessarily give up
precision that cannot be recovered later, as over-approximated interim results
have to be fully propagated through the equation the system. Additionally,
classical two-phased approach is not suitable for equation systems with
infinitely many unknowns---where demand driven solving must be used.
Construction of an intertwined approach must be able to answer when it is safe
to apply narrowing---or when widening must be applied. In general, this is a
difficult problem. In case the right-hand sides of equations are monotonic,
however, we can always apply narrowing whenever we have reached a post-fixpoint
for an equation. The assumption of monotonicity, though, is not met in presence
of widening. It is also not met by equation systems corresponding to
context-sensitive inter-procedural analysis, possibly combining
context-sensitive analysis of local information with flow-insensitive analysis
of globals.
  As a remedy, we present a novel operator that combines a given widening
operator with a given narrowing operator. We present adapted versions of
round-robin as well as of worklist iteration, local and side-effecting solving
algorithms for the combined operator and prove that the resulting solvers
always return sound results and are guaranteed to terminate for monotonic
systems whenever only finitely many unknowns (constraint variables) are
encountered. Practical remedies are proposed for termination in the
non-monotonic case."
"Family-based (lifted) data-flow analysis for Software Product Lines (SPLs) is
capable of analyzing all valid products (variants) without generating any of
them explicitly. It takes as input only the common code base, which encodes all
variants of a SPL, and produces analysis results corresponding to all variants.
However, the computational cost of the lifted analysis still depends inherently
on the number of variants (which is exponential in the number of features, in
the worst case). For a large number of features, the lifted analysis may be too
costly or even infeasible. In this paper, we introduce variability abstractions
defined as Galois connections and use abstract interpretation as a formal
method for the calculational-based derivation of approximate (abstracted)
lifted analyses of SPL programs, which are sound by construction. Moreover,
given an abstraction we define a syntactic transformation that translates any
SPL program into an abstracted version of it, such that the analysis of the
abstracted SPL coincides with the corresponding abstracted analysis of the
original SPL. We implement the transformation in a tool, reconfigurator that
works on Object-Oriented Java program families, and evaluate the practicality
of this approach on three Java SPL benchmarks."
"We present a new type system combining refinement types and the
expressiveness of intersection type discipline. The use of such features makes
it possible to derive more precise types than in the original refinement
system. We have been able to prove several interesting properties for our
system (including subject reduction) and developed an inference algorithm,
which we proved to be sound."
"Static analysers search for overapproximating proofs of safety commonly known
as safety invariants. Fundamentally, such analysers summarise traces into sets
of states, thus trading the ability to distinguish traces for computational
tractability. Conversely, static bug finders (e.g. Bounded Model Checking) give
evidence for the failure of an assertion in the form of a counterexample, which
can be inspected by the user. However, static bug finders fail to scale when
analysing programs with bugs that require many iterations of a loop as the
computational effort grows exponentially with the depth of the bug. We propose
a novel approach for finding bugs, which delivers the performance of abstract
interpretation together with the concrete precision of BMC. To do this, we
introduce the concept of danger invariants -- the dual to safety invariants.
Danger invariants summarise sets of traces that are guaranteed to reach an
error state. This summarisation allows us to find deep bugs without false
alarms and without explicitly unwinding loops. We present a second-order
formulation of danger invariants and use the Second-Order SAT solver described
in previous work to compute danger invariants for intricate programs taken from
the literature."
"A computation in the continuation monad returns a final result given a
continuation, ie. it is a function with type $(X \to R) \to R$. If we instead
return the intermediate result at $X$ then our computation is called a
selection function. Selection functions appear in diverse areas of mathematics
and computer science (especially game theory, proof theory and topology) but
the existing literature does not heavily emphasise the fact that the selection
monad is a CPS translation. In particular it has so far gone unnoticed that the
selection monad has a call/cc-like operator with interesting similarities and
differences to the usual call/cc, which we explore experimentally using
Haskell.
  Selection functions can be used whenever we find the intermediate result more
interesting than the final result. For example a SAT solver computes an
assignment to a boolean function, and then its continuation decides whether it
is a satisfying assignment, and we find the assignment itself more interesting
than the fact that it is or is not satisfying. In game theory we find the move
chosen by a player more interesting than the outcome that results from that
move. The author and collaborators are developing a theory of games in which
selection functions are viewed as generalised notions of rationality, used to
model players. By realising that strategic contexts in game theory are examples
of continuations we can see that classical game theory narrowly misses being in
CPS, and that a small change of viewpoint yields a theory of games that is
better behaved, and especially more compositional."
"Despite the conceptual simplicity of sequential consistency (SC), the
semantics of SC atomic operations and fences in the C11 and OpenCL memory
models is subtle, leading to convoluted prose descriptions that translate to
complex axiomatic formalisations. We conduct an overhaul of SC atomics in C11,
reducing the associated axioms in both number and complexity. A consequence of
our simplification is that the SC operations in an execution no longer need to
be totally ordered. This relaxation enables, for the first time, efficient and
exhaustive simulation of litmus tests that use SC atomics. We extend our
improved C11 model to obtain the first rigorous memory model formalisation for
OpenCL (which extends C11 with support for heterogeneous many-core
programming). In the OpenCL setting, we refine the SC axioms still further to
give a sensible semantics to SC operations that employ a 'memory scope' to
restrict their visibility to specific threads. Our overhaul requires slight
strengthenings of both the C11 and the OpenCL memory models, causing some
behaviours to become disallowed. We argue that these strengthenings are
natural, and that all of the formalised C11 and OpenCL compilation schemes of
which we are aware (Power and x86 CPUs for C11, AMD GPUs for OpenCL) remain
valid in our revised models. Using the Herd memory model simulator, we show
that our overhaul leads to an exponential improvement in simulation time for
C11 litmus tests compared with the original model, making exhaustive simulation
competitive, time-wise, with the non-exhaustive CDSChecker tool."
"Over the past thirty years, there has been significant progress in developing
general-purpose, language-based approaches to incremental computation, which
aims to efficiently update the result of a computation when an input is
changed. A key design challenge in such approaches is how to provide efficient
incremental support for a broad range of programs. In this paper, we argue that
first-class names are a critical linguistic feature for efficient incremental
computation. Names identify computations to be reused across differing runs of
a program, and making them first class gives programmers a high level of
control over reuse. We demonstrate the benefits of names by presenting NOMINAL
ADAPTON, an ML-like language for incremental computation with names. We
describe how to use NOMINAL ADAPTON to efficiently incrementalize several
standard programming patterns---including maps, folds, and unfolds---and show
how to build efficient, incremental probabilistic trees and tries. Since
NOMINAL ADAPTON's implementation is subtle, we formalize it as a core calculus
and prove it is from-scratch consistent, meaning it always produces the same
answer as simply re-running the computation. Finally, we demonstrate that
NOMINAL ADAPTON can provide large speedups over both from-scratch computation
and ADAPTON, a previous state-of-the-art incremental computation system."
"Although run-time language composition is common, it normally takes the form
of a crude Foreign Function Interface (FFI). While useful, such compositions
tend to be coarse-grained and slow. In this paper we introduce a novel
fine-grained syntactic composition of PHP and Python which allows users to
embed each language inside the other, including referencing variables across
languages. This composition raises novel design and implementation challenges.
We show that good solutions can be found to the design challenges; and that the
resulting implementation imposes an acceptable performance overhead of, at
most, 2.6x."
"We present the linear first-order intermediate language IL for verified
compilers. IL is a functional language with calls to a nondeterministic
environment. We give IL terms a second, imperative semantic interpretation and
obtain a register transfer language. For the imperative interpretation we
establish a notion of live variables. Based on live variables, we formulate a
decidable property called coherence ensuring that the functional and the
imperative interpretation of a term coincide. We formulate a register
assignment algorithm for IL and prove its correctness. The algorithm translates
a functional IL program into an equivalent imperative IL program. Correctness
follows from the fact that the algorithm reaches a coherent program after
consistently renaming local variables. We prove that the maximal number of live
variables in the initial program bounds the number of different variables in
the final coherent program. The entire development is formalized in Coq."
"We demonstrate that general-purpose memory allocation involving many threads
on many cores can be done with high performance, multicore scalability, and low
memory consumption. For this purpose, we have designed and implemented scalloc,
a concurrent allocator that generally performs and scales in our experiments
better than other allocators while using less memory, and is still competitive
otherwise. The main ideas behind the design of scalloc are: uniform treatment
of small and big objects through so-called virtual spans, efficiently and
effectively reclaiming free memory through fast and scalable global data
structures, and constant-time (modulo synchronization) allocation and
deallocation operations that trade off memory reuse and spatial locality
without being subject to false sharing."
"Transactional memory has arisen as a good way for solving many of the issues
of lock-based programming. However, most implementations admit isolated
transactions only, which are not adequate when we have to coordinate
communicating processes. To this end, in this paper we present OCTM, an
Haskell-like language with open transactions over shared transactional memory:
processes can join transactions at runtime just by accessing to shared
variables. Thus a transaction can co-operate with the environment through
shared variables, but if it is rolled-back, also all its effects on the
environment are retracted. For proving the expressive power of TCCS we give an
implementation of TCCS, a CCS-like calculus with open transactions."
"We investigate the semantic intricacies of conditioning, a main feature in
probabilistic programming. We provide a weakest (liberal) pre-condition (w(l)p)
semantics for the elementary probabilistic programming language pGCL extended
with conditioning. We prove that quantitative weakest (liberal) pre-conditions
coincide with conditional (liberal) expected rewards in Markov chains and show
that semantically conditioning is a truly conservative extension. We present
two program transformations which entirely eliminate conditioning from any
program and prove their correctness using the w(l)p-semantics. Finally, we show
how the w(l)p-semantics can be used to determine conditional probabilities in a
parametric anonymity protocol and show that an inductive w(l)p-semantics for
conditioning in non-deterministic probabilistic programs cannot exist."
"A paper has recently been published in SIAM-JC. This paper is faulty: 1) The
standard requirements about the definition of an algorithm are not respected,
2) The main point in the complexity study, namely the functional programming
component, is absent. The Editorial Board of the SIAM JC had been warned a
confirmed publication would be openly commented, it is the role of this text."
"Comparison of programming languages is a common topic of discussion among
software engineers. Multiple programming languages are designed, specified, and
implemented every year in order to keep up with the changing programming
paradigms, hardware evolution, etc. In this paper we present a comparative
study between six programming languages: C++, PHP, C#, Java, Python, VB ; These
languages are compared under the characteristics of reusability, reliability,
portability, availability of compilers and tools, readability, efficiency,
familiarity and expressiveness."
"In this paper we present several examples of solving algorithmic problems
from the Google Code Jam programming contest with Picat programming language
using declarative techniques: constraint logic programming and tabled logic
programming. In some cases the use of Picat simplifies the implementation
compared to conventional imperative programming languages, while in others it
allows to directly convert the problem statement into an efficiently solvable
declarative problem specification without inventing an imperative algorithm."
"Android is an operating system that has been used in a majority of mobile
devices. Each application in Android runs in an instance of the Dalvik virtual
machine, which is a register-based virtual machine (VM). Most applications for
Android are developed using Java, compiled to Java bytecode and then translated
to DEX bytecode using the dx tool in the Android SDK. In this work, we aim to
develop a type-based method for certifying non-interference properties of DEX
bytecode, following a methodology that has been developed for Java bytecode
certification by Barthe et al. To this end, we develop a formal operational
semantics of the Dalvik VM, a type system for DEX bytecode, and prove the
soundness of the type system with respect to a notion of non-interference. We
then study the translation process from Java bytecode to DEX bytecode, as
implemented in the dx tool in the Android SDK. We show that an abstracted
version of the translation from Java bytecode to DEX bytecode preserves the
non-interference property. More precisely, we show that if the Java bytecode is
typable in Barthe et al's type system (which guarantees non-interference) then
its translation is typable in our type system. This result opens up the
possibility to leverage existing bytecode verifiers for Java to certify
non-interference properties of Android bytecode."
"Due to their ""inherent parallelism"", interaction nets have since their
introduction been considered as an attractive implementation mechanism for
functional programming. We show that a simple highly-concurrent implementation
in Haskell can achieve promising speed-ups on multiple cores."
"GP 2 is an experimental programming language for computing by graph
transformation. An initial interpreter for GP 2, written in the functional
language Haskell, provides a concise and simply structured reference
implementation. Despite its simplicity, the performance of the interpreter is
sufficient for the comparative investigation of a range of test programs. It
also provides a platform for the development of more sophisticated
implementations."
"Global Value Numbering (GVN) is an important static analysis to detect
equivalent expressions in a program. We present an iterative data-flow analysis
GVN algorithm in SSA for the purpose of detecting total redundancies. The
central challenge is defining a join operation to detect equivalences at a join
point in polynomial time such that later occurrences of redundant expressions
could be detected. For this purpose, we introduce the novel concept of value
$\phi$-function. We claim the algorithm is precise and takes only polynomial
time."
"In this paper I introduce a mechanism to derive program transforma- tions
from order-preserving transformations of vector types. The purpose of this work
is to allow automatic generation of correct-by-construction instances of
programs in a streaming data processing paradigm suitable for FPGA processing.
We show that for it is possible to automatically derive instances for programs
based on combinations of opaque element- processing functions combined using
foldl and map, purely from the type transformations."
"We classify programming languages according to evaluation order: each
language fixes one evaluation order as the default, making it transparent to
program in that evaluation order, and troublesome to program in the other.
  This paper develops a type system that is impartial with respect to
evaluation order. Evaluation order is implicit in terms, and explicit in types,
with by-value and by-name versions of type connectives. A form of intersection
type quantifies over evaluation orders, describing code that is agnostic over
(that is, polymorphic in) evaluation order. By allowing such generic code,
programs can express the by-value and by-name versions of a computation without
code duplication.
  We also formulate a type system that only has by-value connectives, plus a
type that generalizes the difference between by-value and by-name connectives:
it is either a suspension (by name) or a ""no-op"" (by value). We show a
straightforward encoding of the impartial type system into the more economical
one. Then we define an elaboration from the economical language to a
call-by-value semantics, and prove that elaborating a well-typed source
program, where evaluation order is implicit, produces a well-typed target
program where evaluation order is explicit. We also prove a simulation between
evaluation of the target program and reductions (either by-value or by-name) in
the source program.
  Finally, we prove that typing, elaboration, and evaluation are faithful to
the type annotations given in the source program: if the programmer only writes
by-value types, no by-name reductions can occur at run time."
"We propose a light-weight client-server model of communication between
existing implementations of different program analyses. The communication is
on-line and anonymous which means that all analyses simultaneously analyse the
same program and an analysis does not know what other analyses participate in
the communication. The anonymity and model's strong emphasis on independence of
analyses allow to preserve almost everything in existing implementations. An
analysis only has to add an implementation of a proposed communication
protocol, determine places in its code where information from others would
help, and then check whether there is no communication scenario, which would
corrupt its result. We demonstrate functionality and effectiveness of the
proposed communication model in a detailed case study with three analyses: two
abstract interpreters and the classic symbolic execution. Results of the
evaluation on SV-COMP benchmarks show impressive improvements in computed
invariants and increased counts of successfully analysed benchmarks."
"Static program analysis is a valuable tool for any programming language that
people write programs in. The prevalence of scripting languages in the world
suggests programming language interpreters are relatively easy to write. Users
of these languages lament their inability to analyze their code, therefore
programming language analyzers are not easy to write. This thesis investigates
a systematic method of creating abstract interpreters from traditional
interpreters, called Abstracting Abstract Machines.
  Abstract interpreters are difficult to develop due to technical, theoretical,
and pragmatic problems. Technical problems include engineering data structures
and algorithms. I show that modest and simple changes to the mathematical
presentation of abstract machines result in 1000 times better running time -
just seconds for moderately sized programs.
  In the theoretical realm, abstraction can make correctness difficult to
ascertain. I provide proof techniques for proving the correctness of regular,
pushdown, and stack-inspecting pushdown models of abstract computation by
leaving computational power to an external factor: allocation. Even if we don't
trust the proof, we can run models concretely against test suites to better
trust them.
  In the pragmatic realm, I show that the systematic process of abstracting
abstract machines is automatable. I develop a meta-language for expressing
abstract machines similar to other semantics engineering languages. The
language's special feature is that it provides an interface to abstract
allocation. The semantics guarantees that if allocation is finite, then the
semantics is a sound and computable approximation of the concrete semantics."
"A key challenge when statically typing so-called dynamic languages is the
ubiquity of value-based overloading, where a given function can dynamically
reflect upon and behave according to the types of its arguments. Thus, to
establish basic types, the analysis must reason precisely about values, but in
the presence of higher-order functions and polymorphism, this reasoning itself
can require basic types. In this paper we address this chicken-and-egg problem
by introducing the framework of two-phased typing. The first ""trust"" phase
performs classical, i.e. flow-, path- and value-insensitive type checking to
assign basic types to various program expressions. When the check inevitably
runs into ""errors"" due to value-insensitivity, it wraps problematic expressions
with DEAD-casts, which explicate the trust obligations that must be discharged
by the second phase. The second phase uses refinement typing, a flow- and
path-sensitive analysis, that decorates the first phase's types with logical
predicates to track value relationships and thereby verify the casts and
establish other correctness properties for dynamically typed languages."
"Proxies are the swiss army knives of object adaptation. They introduce a
level of indirection to intercept select operations on a target object and
divert them as method calls to a handler. Proxies have many uses like
implementing access control, enforcing contracts, virtualizing resources.
  One important question in the design of a proxy API is whether a proxy object
should inherit the identity of its target. Apparently proxies should have their
own identity for security-related applications whereas other applications, in
particular contract systems, require transparent proxies that compare equal to
their target objects.
  We examine the issue with transparency in various use cases for proxies,
discuss different approaches to obtain transparency, and propose two designs
that require modest modifications in the JavaScript engine and cannot be
bypassed by the programmer.
  We implement our designs in the SpiderMonkey JavaScript interpreter and
bytecode compiler. Our evaluation shows that these modifications of have no
statistically significant impact on the benchmark performance of the JavaScript
engine. Furthermore, we demonstrate that contract systems based on wrappers
require transparent proxies to avoid interference with program execution in
realistic settings."
"TreatJS is a language embedded, higher-order contract system for JavaScript
which enforces contracts by run-time monitoring. Beyond providing the standard
abstractions for building higher-order contracts (base, function, and object
contracts), TreatJS's novel contributions are its guarantee of non-interfering
contract execution, its systematic approach to blame assignment, its support
for contracts in the style of union and intersection types, and its notion of a
parameterized contract scope, which is the building block for composable
run-time generated contracts that generalize dependent function contracts.
  TreatJS is implemented as a library so that all aspects of a contract can be
specified using the full JavaScript language. The library relies on JavaScript
proxies to guarantee full interposition for contracts. It further exploits
JavaScript's reflective features to run contracts in a sandbox environment,
which guarantees that the execution of contract code does not modify the
application state. No source code transformation or change in the JavaScript
run-time system is required.
  The impact of contracts on execution speed is evaluated using the Google
Octane benchmark."
"In this paper, we demonstrate the effectiveness of Cheney's Copy Algorithm
for a Lisp-like system and experimentally show the infeasability of developing
an optimal garbage collector for general use. We summarize and compare several
garbage-collection algorithms including Cheney's Algorithm, the canonical Mark
and Sweep Algorithm, and Knuth's Classical Lisp 2 Algorithm. We implement and
analyze these three algorithms in the context of a custom MicroLisp
environment. We conclude and present the core considerations behind the
development of a garbage collector---specifically for Lisp---and make an
attempt to investigate these issues in depth. We also discuss experimental
results that imply the effectiveness of Cheney's algorithm over Mark-Sweep for
Lisp-like languages."
"We present Alias Refinement Types (ART), a new approach to the verification
of correctness properties of linked data structures. While there are many
techniques for checking that a heap-manipulating program adheres to its
specification, they often require that the programmer annotate the behavior of
each procedure, for example, in the form of loop invariants and pre- and
post-conditions. Predicate abstraction would be an attractive abstract domain
for performing invariant inference, existing techniques are not able to reason
about the heap with enough precision to verify functional properties of data
structure manipulating programs. In this paper, we propose a technique that
lifts predicate abstraction to the heap by factoring the analysis of data
structures into two orthogonal components: (1) Alias Types, which reason about
the physical shape of heap structures, and (2) Refinement Types, which use
simple predicates from an SMT decidable theory to capture the logical or
semantic properties of the structures. We prove ART sound by translating types
into separation logic assertions, thus translating typing derivations in ART
into separation logic proofs. We evaluate ART by implementing a tool that
performs type inference for an imperative language, and empirically show, using
a suite of data-structure benchmarks, that ART requires only 21% of the
annotations needed by other state-of-the-art verification techniques."
"Differentially private mechanisms enjoy a variety of composition properties.
Leveraging these, McSherry introduced PINQ (SIGMOD 2009), a system empowering
non-experts to construct new differentially private analyses. PINQ is an
LINQ-like API which provides automatic privacy guarantees for all programs
which use it to mediate sensitive data manipulation. In this work we introduce
featherweight PINQ, a formal model capturing the essence of PINQ. We prove that
any program interacting with featherweight PINQ's API is differentially
private."
"We propose a novel method for inferring refinement types of higher-order
functional programs. The main advantage of the proposed method is that it can
infer maximally preferred (i.e., Pareto optimal) refinement types with respect
to a user-specified preference order. The flexible optimization of refinement
types enabled by the proposed method paves the way for interesting
applications, such as inferring most-general characterization of inputs for
which a given program satisfies (or violates) a given safety (or termination)
property. Our method reduces such a type optimization problem to a Horn
constraint optimization problem by using a new refinement type system that can
flexibly reason about non-determinism in programs. Our method then solves the
constraint optimization problem by repeatedly improving a current solution
until convergence via template-based invariant generation. We have implemented
a prototype inference system based on our method, and obtained promising
results in preliminary experiments."
"We present a computer-aided programming approach to concurrency. The approach
allows programmers to program assuming a friendly, non-preemptive scheduler,
and our synthesis procedure inserts synchronization to ensure that the final
program works even with a preemptive scheduler. The correctness specification
is implicit, inferred from the non-preemptive behavior. Let us consider
sequences of calls that the program makes to an external interface. The
specification requires that any such sequence produced under a preemptive
scheduler should be included in the set of such sequences produced under a
non-preemptive scheduler. The solution is based on a finitary abstraction, an
algorithm for bounded language inclusion modulo an independence relation, and
rules for inserting synchronization. We apply the approach to device-driver
programming, where the driver threads call the software interface of the device
and the API provided by the operating system. Our experiments demonstrate that
our synthesis method is precise and efficient, and, since it does not require
explicit specifications, is more practical than the conventional approach based
on user-provided assertions."
"Given a specification and a set of candidate programs (program space), the
program synthesis problem is to find a candidate program that satisfies the
specification. We present the synthesis through unification (STUN) approach,
which is an extension of the counter-example guided inductive synthesis (CEGIS)
approach. In CEGIS, the synthesizer maintains a subset S of inputs and a
candidate program Prog that is correct for S. The synthesizer repeatedly checks
if there exists a counter-example input c such that the execution of Prog is
incorrect on c. If so, the synthesizer enlarges S to include c, and picks a
program from the program space that is correct for the new set S.
  The STUN approach extends CEGIS with the idea that given a program Prog that
is correct for a subset of inputs, the synthesizer can try to find a program
Prog' that is correct for the rest of the inputs. If Prog and Prog' can be
unified into a program in the program space, then a solution has been found. We
present a generic synthesis procedure based on the STUN approach and specialize
it for three different domains by providing the appropriate unification
operators. We implemented these specializations in prototype tools, and we show
that our tools often per- forms significantly better on standard benchmarks
than a tool based on a pure CEGIS approach."
"Golo is a simple dynamically-typed language for the Java Virtual Machine.
Initially implemented as a ahead-of-time compiler to JVM bytecode, it leverages
invokedy-namic and JSR 292 method handles to implement a reasonably efficient
runtime. Truffle is emerging as a framework for building interpreters for JVM
languages with self-specializing AST nodes. Combined with the Graal compiler,
Truffle offers a simple path towards writing efficient interpreters while
keeping the engineering efforts balanced. The Golo project is interested in
experimenting with a Truffle interpreter in the future, as it would provides
interesting comparison elements between invokedynamic versus Truffle for
building a language runtime."
"The actor model of computation has gained significant popularity over the
last decade. Its high level of abstraction makes it appealing for concurrent
applications in parallel and distributed systems. However, designing a
real-world actor framework that subsumes full scalability, strong reliability,
and high resource efficiency requires many conceptual and algorithmic additives
to the original model.
  In this paper, we report on designing and building CAF, the ""C++ Actor
Framework"". CAF targets at providing a concurrent and distributed native
environment for scaling up to very large, high-performance applications, and
equally well down to small constrained systems. We present the key
specifications and design concepts---in particular a message-transparent
architecture, type-safe message interfaces, and pattern matching
facilities---that make native actors a viable approach for many robust,
elastic, and highly distributed developments. We demonstrate the feasibility of
CAF in three scenarios: first for elastic, upscaling environments, second for
including heterogeneous hardware like GPGPUs, and third for distributed runtime
systems. Extensive performance evaluations indicate ideal runtime behaviour for
up to 64 cores at very low memory footprint, or in the presence of GPUs. In
these tests, CAF continuously outperforms the competing actor environments
Erlang, Charm++, SalsaLite, Scala, ActorFoundry, and even the OpenMPI."
"Despite its old age, Lisp remains mysterious to many of its admirers. The
mysteries on one hand fascinate the language, on the other hand also obscure
it. Following Stoyan but paying attention to what he has neglected or omitted,
in this first essay of a series intended to unravel these mysteries, we trace
the development of Lisp back to its origin, revealing how the language has
evolved into its nowadays look and feel. The insights thus gained will not only
enhance existent understanding of the language but also inspires further
improvement of it."
"All modern web browsers - Internet Explorer, Firefox, Chrome, Opera, and
Safari - have a core rendering engine written in C++. This language choice was
made because it affords the systems programmer complete control of the
underlying hardware features and memory in use, and it provides a transparent
compilation model.
  Servo is a project started at Mozilla Research to build a new web browser
engine that preserves the capabilities of these other browser engines but also
both takes advantage of the recent trends in parallel hardware and is more
memory-safe. We use a new language, Rust, that provides us a similar level of
control of the underlying system to C++ but which builds on many concepts
familiar to the functional programming community, forming a novelty - a useful,
safe systems programming language.
  In this paper, we show how a language with an affine type system, regions,
and many syntactic features familiar to functional language programmers can be
successfully used to build state-of-the-art systems software. We also outline
several pitfalls encountered along the way and describe some potential areas
for future research."
"A central method for analyzing the asymptotic complexity of a functional
program is to extract and then solve a recurrence that expresses evaluation
cost in terms of input size. The relevant notion of input size is often
specific to a datatype, with measures including the length of a list, the
maximum element in a list, and the height of a tree. In this work, we give a
formal account of the extraction of cost and size recurrences from higher-order
functional programs over inductive datatypes. Our approach allows a wide range
of programmer-specified notions of size, and ensures that the extracted
recurrences correctly predict evaluation cost. To extract a recurrence from a
program, we first make costs explicit by applying a monadic translation from
the source language to a complexity language, and then abstract datatype values
as sizes. Size abstraction can be done semantically, working in models of the
complexity language, or syntactically, by adding rules to a preorder judgement.
We give several different models of the complexity language, which support
different notions of size. Additionally, we prove by a logical relations
argument that recurrences extracted by this process are upper bounds for
evaluation cost; the proof is entirely syntactic and therefore applies to all
of the models we consider."
"Expressive static typing disciplines are a powerful way to achieve
high-quality software. However, the adoption cost of such techniques should not
be under-estimated. Just like gradual typing allows for a smooth transition
from dynamically-typed to statically-typed programs, it seems desirable to
support a gradual path to certified programming. We explore gradual certified
programming in Coq, providing the possibility to postpone the proofs of
selected properties, and to check ""at runtime"" whether the properties actually
hold. Casts can be integrated with the implicit coercion mechanism of Coq to
support implicit cast insertion a la gradual typing. Additionally, when
extracting Coq functions to mainstream languages, our encoding of casts
supports lifting assumed properties into runtime checks. Much to our surprise,
it is not necessary to extend Coq in any way to support gradual certified
programming. A simple mix of type classes and axioms makes it possible to bring
gradual certified programming to Coq in a straightforward manner."
"This paper introduces the Egison programming language whose feature is strong
pattern-matching facility against not only algebraic data types but also
non-free data types whose data have multiple ways of representation such as
sets and graphs. Our language supports multiple occurrences of the same
variables in a pattern, multiple results of pattern-matching, polymorphism of
pattern-constructors and loop-patterns, patterns that contain ""and-so-forth""
whose repeat count can be changed by the parameter. This paper proposes the way
to design expressions that have all these features and demonstrates how these
features are useful to express programs concise. Egison has already implemented
in Haskell."
"Logic programming has traditionally lacked devices for expressing mutually
exclusive modules. We address this limitation by adopting choice-conjunctive
modules of the form $D_0 \& D_1$ where $D_0, D_1$ are a conjunction of Horn
clauses and $\&$ is a linear logic connective. Solving a goal $G$ using $D_0 \&
D_1$ -- $exec(D_0 \& D_1,G)$ -- has the following operational semantics:
$choose$ a successful one between $exec(D_0,G)$ and $exec(D_1,G)$. In other
words, if $D_0$ is chosen in the course of solving $G$, then $D_1$ will be
discarded and vice versa. Hence, the class of choice-conjunctive modules can
capture the notion of mutually exclusive modules."
"We extend prior work on class-morphing to provide a more expressive
pattern-based compile-time reflection language. Our MorphJ language offers a
disciplined form of metaprogramming that produces types by statically iterating
over and pattern-matching on fields and methods of other types. We expand such
capabilities with ""universal morphing"", which also allows pattern-matching over
types (e.g., all classes nested in another, all supertypes of a class) while
maintaining modular type safety for our meta-programs. We present informal
examples of the functionality and discuss a design for adding universal
morphing to Java."
"Given a program and a time deadline, does the program finish before the
deadline when executed on a given platform? With the requirement to produce a
test case when such a violation can occur, we refer to this problem as the
worst-case execution-time testing (WCETT) problem.
  In this paper, we present an approach for solving the WCETT problem for
loop-free programs by timing the execution of a program on a small number of
carefully calculated inputs. We then create a sequence of integer linear
programs the solutions of which encode the best timing model consistent with
the measurements. By solving the programs we can find the worst-case input as
well as estimate execution time of any other input. Our solution is more
accurate than previous approaches and, unlikely previous work, by increasing
the number of measurements we can produce WCETT bounds up to any desired
accuracy.
  Timing of a program depends on the properties of the platform it executes on.
We further show how our approach can be used to quantify the timing
repeatability of the underlying platform."
"High-level programming languages play a key role in a growing number of
networking platforms, streamlining application development and enabling precise
formal reasoning about network behavior. Unfortunately, current compilers only
handle ""local"" programs that specify behavior in terms of hop-by-hop forwarding
behavior, or modest extensions such as simple paths. To encode richer ""global""
behaviors, programmers must add extra state -- something that is tricky to get
right and makes programs harder to write and maintain. Making matters worse,
existing compilers can take tens of minutes to generate the forwarding state
for the network, even on relatively small inputs. This forces programmers to
waste time working around performance issues or even revert to using
hardware-level APIs.
  This paper presents a new compiler for the NetKAT language that handles rich
features including regular paths and virtual networks, and yet is several
orders of magnitude faster than previous compilers. The compiler uses symbolic
automata to calculate the extra state needed to implement ""global"" programs,
and an intermediate representation based on binary decision diagrams to
dramatically improve performance. We describe the design and implementation of
three essential compiler stages: from virtual programs (which specify behavior
in terms of virtual topologies) to global programs (which specify network-wide
behavior in terms of physical topologies), from global programs to local
programs (which specify behavior in terms of single-switch behavior), and from
local programs to hardware-level forwarding tables. We present results from
experiments on real-world benchmarks that quantify performance in terms of
compilation time and forwarding table size."
"Web sites routinely incorporate JavaScript programs from several sources into
a single page. These sources must be protected from one another, which requires
robust sandboxing. The many entry-points of sandboxes and the subtleties of
JavaScript demand robust verification of the actual sandbox source. We use a
novel type system for JavaScript to encode and verify sandboxing properties.
The resulting verifier is lightweight and efficient, and operates on actual
source. We demonstrate the effectiveness of our technique by applying it to
ADsafe, which revealed several bugs and other weaknesses."
"Mobile agents represent a new model for network computing. Many different
languages have been used to implement mobile agents. The characteristics that
make a language useful for writing mobile agents are: (1) their support of
agent migration, (2) their support for agent-to-agent communication, (3) how
they allow agents to interact with local resources, (4) security mechanisms,
(5) execution efficiency, (6) language implementation across multiple
platforms, and (7) the language's ease of programming of the tasks mobile
agents perform."
"Typical JavaScript (JS) programs feature a large number of object property
accesses. Hence, fast property reads and writes are crucial for good
performance. Unfortunately, many (often redundant) dynamic checks are implied
in each property access and the semantic complexity of JS makes it difficult to
optimize away these tests through program analysis. We introduce two techniques
to effectively eliminate a large proportion of dynamic checks related to object
property accesses.
  Typed shapes enable code specialization based on object property types
without potentially complex and expensive analyses. Shape propagation allows
the elimination of redundant shape checks in inline caches. These two
techniques combine particularly well with Basic Block Versioning (BBV), but
should be easily adaptable to tracing Just-In-Time (JIT) compilers and method
JITs with type feedback.
  To assess the effectiveness of the techniques presented, we have implemented
them in Higgs, a type-specializing JIT compiler for JS. The techniques are
compared to a baseline using polymorphic Inline Caches (PICs), as well as
commercial JS implementations. Empirical results show that across the 26
benchmarks tested, these techniques eliminate on average 48% of type tests,
reduce code size by 17% and reduce execution time by 25%. On several
benchmarks, Higgs performs better than current production JS virtual machines"
"Direct manipulation interfaces and programmatic systems have distinct and
complementary strengths. The former provide intuitive, immediate visual
feedback and enable rapid prototyping, whereas the latter enable complex,
reusable abstractions. Unfortunately, existing systems typically force users
into just one of these two interaction modes.
  We present a system called Sketch-n-Sketch that integrates programmatic and
direct manipulation for the particular domain of Scalable Vector Graphics
(SVG). In Sketch-n-Sketch, the user writes a program to generate an output SVG
canvas. Then the user may directly manipulate the canvas while the system
immediately infers a program update in order to match the changes to the
output, a workflow we call live synchronization. To achieve this, we propose
(i) a technique called trace-based program synthesis that takes program
execution history into account in order to constrain the search space and (ii)
heuristics for dealing with ambiguities. Based on our experience with examples
spanning 2,000 lines of code and from the results of a preliminary user study,
we believe that Sketch-n-Sketch provides a novel workflow that can augment
traditional programming systems. Our approach may serve as the basis for live
synchronization in other application domains, as well as a starting point for
yet more ambitious ways of combining programmatic and direct manipulation."
"Traditional control-flow analysis (CFA) for higher-order languages, whether
implemented by constraint-solving or abstract interpretation, introduces
spurious connections between callers and callees. Two distinct invocations of a
function will necessarily pollute one another's return-flow. Recently, three
distinct approaches have been published which provide perfect call-stack
precision in a computable manner: CFA2, PDCFA, and AAC. Unfortunately, CFA2 and
PDCFA are difficult to implement and require significant engineering effort.
Furthermore, all three are computationally expensive; for a monovariant
analysis, CFA2 is in $O(2^n)$, PDCFA is in $O(n^6)$, and AAC is in $O(n^9 log
n)$.
  In this paper, we describe a new technique that builds on these but is both
straightforward to implement and computationally inexpensive. The crucial
insight is an unusual state-dependent allocation strategy for the addresses of
continuation. Our technique imposes only a constant-factor overhead on the
underlying analysis and, with monovariance, costs only O(n3) in the worst case.
  This paper presents the intuitions behind this development, a proof of the
precision of this analysis, and benchmarks demonstrating its efficacy."
"We present an approach for dynamic information flow control across the
application and database. Our approach reduces the amount of policy code
required, yields formal guarantees across the application and database, works
with existing relational database implementations, and scales for realistic
applications. In this paper, we present a programming model that factors out
information flow policies from application code and database queries, a dynamic
semantics for the underlying {\lambda}^JDB core language, and proofs of
termination-insensitive non-interference and policy compliance for the
semantics. We implement these ideas in Jacqueline, a Python web framework, and
demonstrate feasibility through three application case studies: a course
manager, a health record system, and a conference management system used to run
an academic workshop. We show that in comparison to traditional applications
with hand-coded policy checks, Jacqueline applications have 1) a smaller
trusted computing base, 2) fewer lines of policy code, and 2) reasonable, often
negligible, additional overheads."
"Calculational abstract interpretation, long advocated by Cousot, is a
technique for deriving correct-by-construction abstract interpreters from the
formal semantics of programming languages.
  This paper addresses the problem of deriving correct-by-verified-construction
abstract interpreters with the use of a proof assistant. We identify several
technical challenges to overcome with the aim of supporting verified
calculational abstract interpretation that is faithful to existing
pencil-and-paper proofs, supports calculation with Galois connections
generally, and enables the extraction of verified static analyzers from these
proofs. To meet these challenges, we develop a theory of Galois connections in
monadic style that include a specification effect. Effectful calculations may
reason classically, while pure calculations have extractable computational
content. Moving between the worlds of specification and implementation is
enabled by our metatheory.
  To validate our approach, we give the first mechanically verified proof of
correctness for Cousot's ""Calculational design of a generic abstract
interpreter."" Our proof ""by calculus"" closely follows the original
paper-and-pencil proof and supports the extraction of a verified static
analyzer."
"Sketch-based synthesis, epitomized by the SKETCH tool, lets developers
synthesize software starting from a partial program, also called a sketch or
template. This paper presents JSKETCH, a tool that brings sketch-based
synthesis to Java. JSKETCH's input is a partial Java program that may include
holes, which are unknown constants, expression generators, which range over
sets of expressions, and class generators, which are partial classes. JSKETCH
then translates the synthesis problem into a SKETCH problem; this translation
is complex because SKETCH is not object-oriented. Finally, JSKETCH synthesizes
an executable Java program by interpreting the output of SKETCH."
"We present a new approach to automated reasoning about higher-order programs
by endowing symbolic execution with a notion of higher-order, symbolic values.
Our approach is sound and relatively complete with respect to a first-order
solver for base type values. Therefore, it can form the basis of automated
verification and bug-finding tools for higher-order programs.
  To validate our approach, we use it to develop and evaluate a system for
verifying and refuting behavioral software contracts of components in a
functional language, which we call soft contract verification. In doing so, we
discover a mutually beneficial relation between behavioral contracts and
higher-order symbolic execution.
  Our system uses higher-order symbolic execution, leveraging contracts as a
source of symbolic values including unknown behavioral values, and employs an
updatable heap of contract invariants to reason about flow-sensitive facts.
Whenever a contract is refuted, it reports a concrete counterexample
reproducing the error, which may involve solving for an unknown function. The
approach is able to analyze first-class contracts, recursive data structures,
unknown functions, and control-flow-sensitive refinements of values, which are
all idiomatic in dynamic languages. It makes effective use of an off-the-shelf
solver to decide problems without heavy encodings. The approach is competitive
with a wide range of existing tools---including type systems, flow analyzers,
and model checkers---on their own benchmarks. We have built a tool which
analyzes programs written in Racket, and report on its effectiveness in
verifying and refuting contracts."
"Software testing is one of the most popular validation techniques in the
software industry. Surprisingly, we can only find a few approaches to testing
in the context of logic programming. In this paper, we introduce a systematic
approach for dynamic testing that combines both concrete and symbolic
execution. Our approach is fully automatic and guarantees full path coverage
when it terminates. We prove some basic properties of our technique and
illustrate its practical usefulness through a prototype implementation."
"Recent work has proposed a promising approach to improving scalability of
program synthesis by allowing the user to supply a syntactic template that
constrains the space of potential programs. Unfortunately, creating templates
often requires nontrivial effort from the user, which impedes the usability of
the synthesizer. We present a solution to this problem in the context of
recursive transformations on algebraic data-types. Our approach relies on
polymorphic synthesis constructs: a small but powerful extension to the
language of syntactic templates, which makes it possible to define a program
space in a concise and highly reusable manner, while at the same time retains
the scalability benefits of conventional templates. This approach enables
end-users to reuse predefined templates from a library for a wide variety of
problems with little effort. The paper also describes a novel optimization that
further improves the performance and scalability of the system. We evaluated
the approach on a set of benchmarks that most notably includes desugaring
functions for lambda calculus, which force the synthesizer to discover Church
encodings for pairs and boolean operations."
"Many recent analyses for conventional imperative programs begin by
transforming programs into logic programs, capitalising on existing LP analyses
and simple LP semantics. We propose using logic programs as an intermediate
program representation throughout the compilation process. With restrictions
ensuring determinism and single-modedness, a logic program can easily be
transformed to machine language or other low-level language, while maintaining
the simple semantics that makes it suitable as a language for program analysis
and transformation. We present a simple LP language that enforces determinism
and single-modedness, and show that it makes a convenient program
representation for analysis and transformation."
"The use of annotations, referred to as assertions or contracts, to describe
program properties for which run-time tests are to be generated, has become
frequent in dynamic programing languages. However, the frameworks proposed to
support such run-time testing generally incur high time and/or space overheads
over standard program execution. We present an approach for reducing this
overhead that is based on the use of memoization to cache intermediate results
of check evaluation, avoiding repeated checking of previously verified
properties. Compared to approaches that reduce checking frequency, our proposal
has the advantage of being exhaustive (i.e., all tests are checked at all
points) while still being much more efficient than standard run-time checking.
Compared to the limited previous work on memoization, it performs the task
without requiring modifications to data structure representation or checking
code. While the approach is general and system-independent, we present it for
concreteness in the context of the Ciao run-time checking framework, which
allows us to provide an operational semantics with checks and caching. We also
report on a prototype implementation and provide some experimental results that
support that using a relatively small cache leads to significant decreases in
run-time checking overhead."
"This paper defines the syntax and semantics of the input language of the ASP
grounder GRINGO. The definition covers several constructs that were not
discussed in earlier work on the semantics of that language, including
intervals, pools, division of integers, aggregates with non-numeric values, and
lparse-style aggregate expressions. The definition is abstract in the sense
that it disregards some details related to representing programs by strings of
ASCII characters. It serves as a specification for GRINGO from Version 4.5 on."
"Partial functions are common abstractions in formal specification notations
such as Z, B and Alloy. Conversely, executable programming languages usually
provide little or no support for them. In this paper we propose to add partial
functions as a primitive feature to a Constraint Logic Programming (CLP)
language, namely {log}. Although partial functions could be programmed on top
of {log}, providing them as first-class citizens adds valuable flexibility and
generality to the form of set-theoretic formulas that the language can safely
deal with. In particular, the paper shows how the {log} constraint solver is
naturally extended in order to accommodate for the new primitive constraints
dealing with partial functions. Efficiency of the new version is empirically
assessed by running a number of non-trivial set-theoretical goals involving
partial functions, obtained from specifications written in Z."
"Any file is fundamentally a binary data stream. A practical solution was
achieved to interpret binary data stream. A new scripting language named Data
Format Scripting Language (DFSL) was developed to describe the physical layout
of the data in a structural, more intelligible way. On the basis of the
solution, a generic software application was implemented; it parses various
binary data streams according to their respective DFSL scripts and generates
human-readable result and XML document for data sharing. Our solution helps
eliminate the error-prone low-level programming, especially in the hardware
devices or network protocol development/debugging processes."
"Software-defined networking (SDN) programs must simultaneously describe
static forwarding behavior and dynamic updates in response to events.
Event-driven updates are critical to get right, but difficult to implement
correctly due to the high degree of concurrency in networks. Existing SDN
platforms offer weak guarantees that can break application invariants, leading
to problems such as dropped packets, degraded performance, security violations,
etc. This paper introduces EVENT-DRIVEN CONSISTENT UPDATES that are guaranteed
to preserve well-defined behaviors when transitioning between configurations in
response to events. We propose NETWORK EVENT STRUCTURES (NESs) to model
constraints on updates, such as which events can be enabled simultaneously and
causal dependencies between events. We define an extension of the NetKAT
language with mutable state, give semantics to stateful programs using NESs,
and discuss provably-correct strategies for implementing NESs in SDNs. Finally,
we evaluate our approach empirically, demonstrating that it gives well-defined
consistency guarantees while avoiding expensive synchronization and packet
buffering."
"We describe a new approach to domain specific languages (DSLs), called Quoted
DSLs (QDSLs), that resurrects two old ideas: quotation, from McCarthy's Lisp of
1960, and the subformula property, from Gentzen's natural deduction of 1935.
Quoted terms allow the DSL to share the syntax and type system of the host
language. Normalising quoted terms ensures the subformula property, which
guarantees that one can use higher-order types in the source while guaranteeing
first-order types in the target, and enables using types to guide fusion. We
test our ideas by re-implementing Feldspar, which was originally implemented as
an Embedded DSL (EDSL), as a QDSL; and we compare the QDSL and EDSL variants."
"In this paper we examine how concurrency has been embodied in mainstream
programming languages. In particular, we rely on the evolutionary talking
borrowed from biology to discuss major historical landmarks and crucial
concepts that shaped the development of programming languages. We examine the
general development process, occasionally deepening into some language, trying
to uncover evolutionary lineages related to specific programming traits. We
mainly focus on concurrency, discussing the different abstraction levels
involved in present-day concurrent programming and emphasizing the fact that
they correspond to different levels of explanation. We then comment on the role
of theoretical research on the quest for suitable programming abstractions,
recalling the importance of changing the working framework and the way of
looking every so often. This paper is not meant to be a survey of modern
mainstream programming languages: it would be very incomplete in that sense. It
aims instead at pointing out a number of remarks and connect them under an
evolutionary perspective, in order to grasp a unifying, but not simplistic,
view of the programming languages development process."
"Tabling is probably the most widely studied extension of Prolog. But despite
its importance and practicality, tabling is not implemented by most Prolog
systems. Existing approaches require substantial changes to the Prolog engine,
which is an investment out of reach of most systems. To enable more widespread
adoption, we present a new implementation of tabling in under 600 lines of
Prolog code. Our lightweight approach relies on delimited control and provides
reasonable performance."
"Easily programming behaviors is one major issue of a large and reconfigurable
deployment in the Internet of Things. Such kind of devices often requires to
externalize part of their behavior such as the sensing, the data aggregation or
the code offloading. Most existing context-oriented programming languages
integrate in the same class or close layers the whole behavior. We propose to
abstract and separate the context tracking from the decision process, and to
use event-based handlers to interconnect them. We keep a very easy declarative
and non-layered programming model. We illustrate by defining an extension to
Golo-a JVM-based dynamic language."
"We address a declarative construction of abstract syntax trees with Parsing
Expression Grammars. AST operators (constructor, connector, and tagging) are
newly defined to specify flexible AST constructions. A new challenge coming
with PEGs is the consistency management of ASTs in backtracking and packrat
parsing. We make the transaction AST machine in order to perform AST operations
in the context of the speculative parsing of PEGs. All the consistency control
is automated by the analysis of AST operators. The proposed approach is
implemented in the Nez parser, written in Java. The performance study shows
that the transactional AST machine requires 25\% approximately more time in
CSV, XML, and C grammars."
"Logic programming such as Prolog is often sequential and slow because each
execution step processes only a single, $micro$ connective. To fix this
problem, we propose to use $macro$ connectives as the means of improving both
readability and performance."
"The goal of the DSLDI workshop is to bring together researchers and
practitioners interested in sharing ideas on how DSLs should be designed,
implemented, supported by tools, and applied in realistic application contexts.
We are both interested in discovering how already known domains such as graph
processing or machine learning can be best supported by DSLs, but also in
exploring new domains that could be targeted by DSLs. More generally, we are
interested in building a community that can drive forward the development of
modern DSLs. These informal post-proceedings contain the submitted talk
abstracts to the 3rd DSLDI workshop (DSLDI'15), and a summary of the panel
discussion on Language Composition."
"Adding versatile interactions to imperative programming -- C, Java and
Android -- is an essential task. Unfortunately, existing languages provide
little constructs for user interaction.
  We propose a computability-logical approach to user interaction. We
illustrate our idea via C^I, an extension of the core C with a new choice
statement."
"Coordination languages simplify design and development of concurrent systems.
Particularly, exogenous coordination languages, like BIP and Reo, enable system
designers to express the interactions among components in a system explicitly.
In this paper we establish a formal relation between BI(P) (i.e., BIP without
the priority layer) and Reo, by defining transformations between their semantic
models. We show that these transformations preserve all properties expressible
in a common semantics. This formal relation comprises the basis for a solid
comparison and consolidation of the fundamental coordination concepts behind
these two languages. Moreover, this basis offers translations that enable users
of either language to benefit from the toolchains of the other."
"Difference constraints have been used for termination analysis in the
literature, where they denote relational inequalities of the form x' <= y + c,
and describe that the value of x in the current state is at most the value of y
in the previous state plus some integer constant c. In this paper, we argue
that the complexity of imperative programs typically arises from counter
increments and resets, which can be modeled naturally by difference
constraints. We present the first practical algorithm for the analysis of
difference constraint programs and describe how C programs can be abstracted to
difference constraint programs. Our approach contributes to the field of
automated complexity and (resource) bound analysis by enabling automated
amortized complexity analysis for a new class of programs and providing a
conceptually simple program model that relates invariant- and bound analysis.
We demonstrate the effectiveness of our approach through a thorough
experimental comparison on real world C code: our tool Loopus computes the
complexity for considerably more functions in less time than related tools from
the literature."
"While event handling is a key element in modern interactive programming, it
is unfortunate that its theoretical foundation is rather weak. To solve this
problem, we propose to adopt a game-logical approach of computability logic
\cite{Jap08} to event handling."
"Compilers for statically typed functional programming languages are notorious
for generating confusing type error messages. When the compiler detects a type
error, it typically reports the program location where the type checking failed
as the source of the error. Since other error sources are not even considered,
the actual root cause is often missed. A more adequate approach is to consider
all possible error sources and report the most useful one subject to some
usefulness criterion. In our previous work, we showed that this approach can be
formulated as an optimization problem related to satisfiability modulo theories
(SMT). This formulation cleanly separates the heuristic nature of usefulness
criteria from the underlying search problem. Unfortunately, algorithms that
search for an optimal error source cannot directly use principal types which
are crucial for dealing with the exponential-time complexity of the decision
problem of polymorphic type checking. In this paper, we present a new algorithm
that efficiently finds an optimal error source in a given ill-typed program.
Our algorithm uses an improved SMT encoding to cope with the high complexity of
polymorphic typing by iteratively expanding the typing constraints from which
principal types are derived. The algorithm preserves the clean separation
between the heuristics and the actual search. We have implemented our algorithm
for OCaml. In our experimental evaluation, we found that the algorithm reduces
the running times for optimal type error localization from minutes to seconds
and scales better than previous localization algorithms."
"Interacting with computers is a ubiquitous activity for millions of people.
Repetitive or specialized tasks often require creation of small, often one-off,
programs. End-users struggle with learning and using the myriad of
domain-specific languages (DSLs) to effectively accomplish these tasks.
  We present a general framework for constructing program synthesizers that
take natural language (NL) inputs and produce expressions in a target DSL. The
framework takes as input a DSL definition and training data consisting of
NL/DSL pairs. From these it constructs a synthesizer by learning optimal
weights and classifiers (using NLP features) that rank the outputs of a
keyword-programming based translation. We applied our framework to three
domains: repetitive text editing, an intelligent tutoring system, and flight
information queries. On 1200+ English descriptions, the respective synthesizers
rank the desired program as the top-1 and top-3 for 80% and 90% descriptions
respectively."
"Parsing Expression Grammars (PEGs) define languages by specifying
recursive-descent parser that recognises them. The PEG formalism exhibits
desirable properties, such as closure under composition, built-in
disambiguation, unification of syntactic and lexical concerns, and closely
matching programmer intuition. Unfortunately, state of the art PEG parsers
struggle with left-recursive grammar rules, which are not supported by the
original definition of the formalism and can lead to infinite recursion under
naive implementations. Likewise, support for associativity and explicit
precedence is spotty. To remedy these issues, we introduce Autumn, a general
purpose PEG library that supports left-recursion, left and right associativity
and precedence rules, and does so efficiently. Furthermore, we identify infix
and postfix operators as a major source of inefficiency in left-recursive PEG
parsers and show how to tackle this problem. We also explore the extensibility
of the PEG paradigm by showing how one can easily introduce new parsing
operators and how our parser accommodates custom memoization and error handling
strategies. We compare our parser to both state of the art and battle-tested
PEG and CFG parsers, such as Rats!, Parboiled and ANTLR."
"The Curry-Howard correspondence is about a relationship between types and
programs on the one hand and propositions and proofs on the other. The
implications for programming language design and program verification is an
active field of research.
  Transformer-like semantics of internal definitions that combine a defining
computation and an application will be presented. By specialisation for a given
defining computation one can derive inference rules for applications of defined
operations.
  With semantics of that kind for every operation, each application identifies
an axiom in a logic defined by the programming language, so a language can be
considered a theory."
"This thesis explores the teleo-reactive programming paradigm for controlling
autonomous agents, such as robots. Teleo-reactive programming provides a
robust, opportunistic method for goal-directed programming that continuously
reacts to the sensed environment. In particular, the TR and TeleoR systems are
investigated. They influence the design of a teleo-reactive system programming
in Python, for controlling autonomous agents via the Pedro communications
architecture. To demonstrate the system, it is used as a controller in a simple
game."
"Large-scale data centers and cloud computing have turned system configuration
into a challenging problem. Several widely-publicized outages have been blamed
not on software bugs, but on configuration bugs. To cope, thousands of
organizations use system configuration languages to manage their computing
infrastructure. Of these, Puppet is the most widely used with thousands of
paying customers and many more open-source users. The heart of Puppet is a
domain-specific language that describes the state of a system. Puppet already
performs some basic static checks, but they only prevent a narrow range of
errors. Furthermore, testing is ineffective because many errors are only
triggered under specific machine states that are difficult to predict and
reproduce. With several examples, we show that a key problem with Puppet is
that configurations can be non-deterministic.
  This paper presents Rehearsal, a verification tool for Puppet configurations.
Rehearsal implements a sound, complete, and scalable determinacy analysis for
Puppet. To develop it, we (1) present a formal semantics for Puppet, (2) use
several analyses to shrink our models to a tractable size, and (3) frame
determinism-checking as decidable formulas for an SMT solver. Rehearsal then
leverages the determinacy analysis to check other important properties, such as
idempotency. Finally, we apply Rehearsal to several real-world Puppet
configurations."
"SAFE is a clean-slate design for a highly secure computer system, with
pervasive mechanisms for tracking and limiting information flows. At the lowest
level, the SAFE hardware supports fine-grained programmable tags, with
efficient and flexible propagation and combination of tags as instructions are
executed. The operating system virtualizes these generic facilities to present
an information-flow abstract machine that allows user programs to label
sensitive data with rich confidentiality policies. We present a formal,
machine-checked model of the key hardware and software mechanisms used to
dynamically control information flow in SAFE and an end-to-end proof of
noninterference for this model.
  We use a refinement proof methodology to propagate the noninterference
property of the abstract machine down to the concrete machine level. We use an
intermediate layer in the refinement chain that factors out the details of the
information-flow control policy and devise a code generator for compiling such
information-flow policies into low-level monitor code. Finally, we verify the
correctness of this generator using a dedicated Hoare logic that abstracts from
low-level machine instructions into a reusable set of verified structured code
generators."
"We present a C-language implementation of the lambda-pi calculus by extending
the (call-by-need) stack machine of Ariola, Chang and Felleisen to hold types,
using a typeless- tagless- final interpreter strategy. It has the advantage of
expressing all operations as folds over terms, including by-need evaluation,
recovery of the initial syntax-tree encoding for any term, and eliminating most
garbage-collection tasks. These are made possible by a disciplined approach to
handling the spine of each term, along with a robust stack-based API. Type
inference is not covered in this work, but also derives several advantages from
the present stack transformation. Timing and maximum stack space usage results
for executing benchmark problems are presented. We discuss how the design
choices for this interpreter allow the language to be used as a high-level
scripting language for automatic distributed parallel execution of common
scientific computing workflows."
"Softwares source code is becoming large and complex. Compilation of large
base code is a time consuming process. Parallel compilation of code will help
in reducing the time complexity. Parsing is one of the phases in compiler in
which significant amount of time of compilation is spent. Techniques have
already been developed to extract the parallelism available in parser. Current
LR(k) parallel parsing techniques either face difficulty in creating Abstract
Syntax Tree or requires modification in the grammar or are specific to less
expressive grammars. Most of the programming languages like C, ALGOL are
block-structured, and in most languages grammars the grammar of different
blocks is independent, allowing different blocks to be parsed in parallel. We
are proposing a block level parallel parser derived from Incremental Jump Shift
Reduce Parser by [13]. Block Parallelized Parser (BPP) can even work as a block
parallel incremental parser. We define a set of Incremental Categories and
create the partitions of a grammar based on a rule. When parser reaches the
start of the block symbol it will check whether the current block is related to
any incremental category. If block parallel parser find the incremental
category for it, parser will parse the block in parallel. Block parallel parser
is developed for LR(1) grammar. Without making major changes in Shift Reduce
(SR) LR(1) parsing algorithm, block parallel parser can create an Abstract
Syntax tree easily. We believe this parser can be easily extended to LR (k)
grammars and also be converted to an LALR (1) parser. We implemented BPP and SR
LR(1) parsing algorithm for C Programming Language. We evaluated performance of
both techniques by parsing 10 random files from Linux Kernel source. BPP showed
28% and 52% improvement in the case of including header files and excluding
header files respectively."
"The aim of a probabilistic output analysis is to derive a probability
distribution of possible output values for a program from a probability
distribution of its input. We present a method for performing static output
analysis, based on program transformation techniques. It generates a
probability function as a possibly uncomputable expression in an intermediate
language. This program is then analyzed, transformed, and approximated. The
result is a closed form expression that computes an over approximation of the
output probability distribution for the program. We focus on programs where the
possible input follows a known probability distribution. Tests in programs are
not assumed to satisfy the Markov property of having fixed branching
probabilities independently of previous history."
"We reduce JavaScript to a core calculus structured as a small-step
operational semantics. We present several peculiarities of the language and
show that our calculus models them. We explicate the desugaring process that
turns JavaScript programs into ones in the core. We demonstrate faithfulness to
JavaScript using real-world test suites. Finally, we illustrate utility by
defining a security property, implementing it as a type system on the core, and
extending it to the full language."
"We extend the linear {\pi}-calculus with composite regular types in such a
way that data containing linear values can be shared among several processes,
if there is no overlapping access to such values. We describe a type
reconstruction algorithm for the extended type system and discuss some
practical aspects of its implementation."
"Choreographic Programming is a programming paradigm for building concurrent
software that is deadlock-free by construction, by disallowing mismatched I/O
operations in the language used to write programs (called choreographies).
Previous models for choreographic programming are either trivially Turing
complete, because they include arbitrary local computations at each process, or
trivially Turing incomplete, e.g., because termination is decidable.
  In this work, we explore the core expressivity of choreographies, by
introducing a minimal language (AC) with restricted local computation (zero,
successor, and equality). AC is Turing complete purely by virtue of the
communication structures that can be written in it. We show that a
Turing-complete fragment of AC can be correctly projected to an actor-like
process calculus (AP), thus identifying a process language that is both
deadlock-free and Turing-complete. By embedding AC into CC, a standard model
for choreographies based on sessions, we also characterise a Turing-complete
fragment of CC, showing that the local computation primitives found in previous
works do not add expressive power. As a corollary, we identify a fragment of
the session-based pi-calculus that is both deadlock-free and Turing complete."
"Choreographic Programming is a methodology for the development of concurrent
software based on a correctness-by-construction approach which, given a global
description of a system (a choreography), automatically generates deadlock-free
communicating programs via an EndPoint Projection (EPP). Previous works use
target-languages for EPP that, like their source choreography languages, model
communications using channel names (e.g., variants of CCS and {\pi}-calculus).
This leaves a gap between such models and real-world implementations, where
communications are concretely supported by low-level mechanisms for message
routing.
  We bridge this gap by developing Applied Choreographies (AC), a new model for
choreographic programming. AC brings the correctness-by-construction
methodology of choreographies down to the level of a real executable language.
The key feature of AC is that its semantics is based on message correlation ---
a standard technique in Service-Oriented Computing --- while retaining the
usual simple and intuitive syntax of choreography languages. We provide AC with
a typing discipline that ensures the correct use of the low-level mechanism of
message correlation, thus avoiding communication errors. We also define a
two-step compilation from AC to a low-level Correlation Calculus, which is the
basis of a real executable language (Jolie). Finally, we prove an operational
correspondence theorem, which ensures that compiled programs behave as the
original choreography. This is the first result of such correct- ness property
in the case of a real-world implemented language."
"Types are an important part of any modern programming language, but we often
forget that the concept of type we understand nowadays is not the same it was
perceived in the sixties. Moreover, we conflate the concept of ""type"" in
programming languages with the concept of the same name in mathematical logic,
an identification that is only the result of the convergence of two different
paths, which started apart with different aims. The paper will present several
remarks (some historical, some of more conceptual character) on the subject, as
a basis for a further investigation. The thesis we will argue is that there are
three different characters at play in programming languages, all of them now
called types: the technical concept used in language design to guide
implementation; the general abstraction mechanism used as a modelling tool; the
classifying tool inherited from mathematical logic. We will suggest three
possible dates ad quem for their presence in the programming language
literature, suggesting that the emergence of the concept of type in computer
science is relatively independent from the logical tradition, until the
Curry-Howard isomorphism will make an explicit bridge between them."
"We propose a type-based analysis to infer the session protocols of channels
in an ML-like concurrent functional language. Combining and extending
well-known techniques, we develop a type-checking system that separates the
underlying ML type system from the typing of sessions. Without using linearity,
our system guarantees communication safety and partial lock freedom. It also
supports provably complete session inference for finite sessions with no
programmer annotations. We exhibit the usefulness of our system with
interesting examples, including one which is not typable in substructural type
systems."
"Programming languages are engineered languages that allow to instruct a
machine and share algorithmic information; they have a great influence on the
society since they underlie almost every information technology artefact, and
they are at the core of the current explosion of software technology. The
history of programming languages is marked by innovations, diversifications,
lateral transfers and social influences; moreover, it represents an
intermediate case study between the evolution of human languages and the
evolution of technology. In this paper we study the application of the
Darwinian explanation to the programming languages evolution by discussing to
what extent the evolutionary mechanisms distinctive of biology can be applied
to this area. We show that a number of evolutionary building blocks can be
recognised in the realm of computer languages, but we also identify critical
issues. Far from being crystal clear, this fine-grained study shows to be a
useful tool to assess recent results about programming languages phylogenies.
Finally, we show that rich evolutionary patterns, such as co-evolution,
macro-evolutionary trends, niche construction and exaptation, can be
effectively applied to programming languages and provide for interesting
explanatory tools."
"Scala's type system unifies ML modules, object-oriented, and functional
programming. The Dependent Object Types (DOT) family of calculi has been
proposed as a new foundation for Scala and similar languages. Unfortunately, it
is not clear how DOT relates to any well-known type systems, and type soundness
has only been established for very restricted subsets. In fact, important Scala
features are known to break at least one key metatheoretic property such as
environment narrowing or subtyping transitivity, which are usually required for
a type soundness proof.
  First, and, perhaps surprisingly, we show how rich DOT calculi can still be
proved sound. The key insight is that narrowing and subtyping transitivity only
need to hold for runtime objects, but not for code that is never executed.
Alas, the dominant method of proving type soundness, Wright and Felleisen's
syntactic approach, is based on term rewriting, which does not a priori make a
distinction between runtime and type assignment time.
  Second, we demonstrate how type soundness can be proved for advanced,
polymorphic, type systems with respect to high-level, definitional
interpreters, implemented in Coq. We present the first mechanized soundness
proof in this style for System F<: and several extensions, including mutable
references. Our proofs use only simple induction: another surprising result, as
the combination of big-step semantics, mutable references, and polymorphism is
commonly believed to require co-inductive proof techniques.
  Third, we show how DOT-like calculi emerge as generalizations of F<:,
exposing a rich design space of calculi with path-dependent types which we
collectively call System D. Armed with insights from the definitional
interpreter semantics, we also show how equivalent small-step semantics and
soundness proofs in Wright-Felleisen-style can be derived for these systems."
"In functional programming, point-free relation calculi have been fruitful for
general theories of program construction, but for specific applications
pointwise expressions can be more convenient and comprehensible. In imperative
programming, refinement calculi have been tied to pointwise expression in terms
of state variables, with the curious exception of the ubiquitous but invisible
heap. To integrate pointwise with point-free, de Moor and Gibbons extended
lambda calculus with non-injective pattern matching interpreted using
relations. This article gives a semantics of that language using ``ideal
relations'' between partial orders, and a second semantics using predicate
transformers. The second semantics is motivated by its potential use with
separation algebra, for pattern matching in programs acting on the heap. Laws
including lax beta and eta are proved in these models and a number of open
problems are posed."
"There is growing interest in lowering the energy consumption of computation.
Energy transparency is a concept that makes a program's energy consumption
visible from software to hardware through the different system layers. Such
transparency can enable energy optimizations at each layer and between layers,
and help both programmers and operating systems make energy aware decisions.
The common methodology of extracting the energy consumption of a program is
through direct measurement of the target hardware. This usually involves
specialized equipment and knowledge most programmers do not have. In this
paper, we examine how existing methods for static resource analysis and energy
modeling can be utilized to perform Energy Consumption Static Analysis (ECSA)
for deeply embedded programs. To investigate this, we have developed ECSA
techniques that work at the instruction set level and at a higher level, the
LLVM IR, through a novel mapping technique. We apply our ECSA to a
comprehensive set of mainly industrial benchmarks, including single-threaded
and also multi-threaded embedded programs from two commonly used concurrency
patterns, task farms and pipelines. We compare our ECSA results to hardware
measurements and predictions obtained based on simulation traces. We discuss a
number of application scenarios for which ECSA results can provide energy
transparency and conclude with a set of new research questions for future work."
"When optimizing a thread in a concurrent program (either done manually or by
the compiler), it must be guaranteed that the resulting thread is a refinement
of the original thread. Most theories of valid optimizations are formulated in
terms of valid syntactic transformations on the program code, or in terms of
valid transformations on thread execution traces. We present a new theory
formulated instead in terms of the state of threads at synchronization
operations, and show that it provides several advantages: it supports more
optimizations, and leads to more efficient and simpler procedures for
refinement checking. We develop the theory for the SC-for-DRF execution model
(using locks for synchronization), and show that its application in a compiler
testing setting leads to large performance improvements."
"We propose a method that transforms a C program manipulating containers using
low-level pointer statements into an equivalent program where the containers
are manipulated via calls of standard high-level container operations like
push_back or pop_front. The input of our method is a C program annotated by a
special form of shape invariants which can be obtained from current automatic
shape analysers after a slight modification. The resulting program where the
low-level pointer statements are summarized into high-level container
operations is more understandable and (among other possible benefits) better
suitable for program analysis. We have implemented our approach and
successfully tested it through a number of experiments with list-based
containers, including experiments with simplification of program analysis by
separating shape analysis from analysing data-related properties."
"Software synthesis - the process of generating complete, general-purpose
programs from specifications - has become a hot research topic in the past few
years. For decades the problem was thought to be insurmountable: the search
space of possible programs is far too massive to efficiently traverse. Advances
in efficient constraint solving have overcome this barrier, enabling a new
generation of effective synthesis systems. Most existing systems compile
synthesis tasks down to low-level SMT instances, sacrificing high-level
semantic information while solving only first-order problems (i.e., filling
integer holes). Recent work takes an alternative approach, using the
Curry-Howard isomorphism and techniques from automated theorem proving to
construct higher-order programs with algebraic datatypes.
  My thesis involved extending this type-directed synthesis engine to handle
product types, which required significant modifications to both the underlying
theory and the tool itself. Product types streamline other language features,
eliminating variable-arity constructors among other workarounds employed in the
original synthesis system. A form of logical conjunction, products are
invertible, making it possible to equip the synthesis system with an efficient
theorem-proving technique called focusing that eliminates many of the
nondeterministic choices inherent in proof search. These theoretical
enhancements informed a new version of the type-directed synthesis prototype
implementation, which remained performance-competitive with the original
synthesizer. A significant advantage of the type-directed synthesis framework
is its extensibility; this thesis is a roadmap for future such efforts to
increase the expressive power of the system."
"We present a method for synthesizing recursive functions that provably
satisfy a given specification in the form of a polymorphic refinement type. We
observe that such specifications are particularly suitable for program
synthesis for two reasons. First, they offer a unique combination of expressive
power and decidability, which enables automatic verification---and hence
synthesis---of nontrivial programs. Second, a type-based specification for a
program can often be effectively decomposed into independent specifications for
its components, causing the synthesizer to consider fewer component
combinations and leading to a combinatorial reduction in the size of the search
space. At the core of our synthesis procedure is a new algorithm for refinement
type checking, which supports specification decomposition.
  We have evaluated our prototype implementation on a large set of synthesis
problems and found that it exceeds the state of the art in terms of both
scalability and usability. The tool was able to synthesize more complex
programs than those reported in prior work (several sorting algorithms and
operations on balanced search trees), as well as most of the benchmarks tackled
by existing synthesizers, often starting from a more concise and intuitive user
input."
"We propose a novel notion of pointer race for concurrent programs
manipulating a shared heap. A pointer race is an access to a memory address
which was freed, and it is out of the accessor's control whether or not the
cell has been re-allocated. We establish two results. (1) Under the assumption
of pointer race freedom, it is sound to verify a program running under explicit
memory management as if it was running with garbage collection. (2) Even the
requirement of pointer race freedom itself can be verified under the
garbage-collected semantics. We then prove analogues of the theorems for a
stronger notion of pointer race needed to cope with performance-critical code
purposely using racy comparisons and even racy dereferences of pointers. As a
practical contribution, we apply our results to optimize a thread-modular
analysis under explicit memory management. Our experiments confirm a speed-up
of up to two orders of magnitude."
"Languages like F#, C#, and recently also Scala, provide ""Async"" programming
models which aim to make asynchronous programming easier by avoiding an
inversion of control that is inherent in callback-based programming models.
This paper presents a novel approach to integrate the Async model with
observable streams of the Reactive Extensions model. Reactive Extensions are
best-known from the .NET platform, and widely-used implementations of its
programming model exist also for Java, Ruby, and other languages. This paper
contributes a formalization of the unified ""Reactive Async"" model in the
context of an object-based core calculus. Our formal model captures the essence
of the protocol of asynchronous observables using a heap evolution property. We
prove a subject reduction theorem; the theorem implies that reduction preserves
the heap evolution property. Thus, for well-typed programs our calculus ensures
the protocol of asynchronous observables."
"We extend abstract interpretation for the purpose of verifying hybrid
systems. Abstraction has been playing an important role in many verification
methodologies for hybrid systems, but some special care is needed for
abstraction of continuous dynamics defined by ODEs. We apply Cousot and
Cousot's framework of abstract interpretation to hybrid systems, almost as it
is, by regarding continuous dynamics as an infinite iteration of infinitesimal
discrete jumps. This extension follows the recent line of work by Suenaga,
Hasuo and Sekine, where deductive verification is extended for hybrid systems
by 1) introducing a constant dt for an infinitesimal value; and 2) employing
Robinson's nonstandard analysis (NSA) to define mathematically rigorous
semantics. Our theoretical results include soundness and termination via
uniform widening operators; and our prototype implementation successfully
verifies some benchmark examples."
"Traditional Answer Set Programming (ASP) rests upon one-shot solving. A logic
program is fed into an ASP system and its stable models are computed. The high
practical relevance of dynamic applications led to the development of
multi-shot solving systems. An operative system solves continuously changing
logic programs. Although this was primarily aiming at dynamic applications in
assisted living, robotics, or stream reasoning, where solvers interact with an
environment, it also opened up the opportunity of interactive ASP, where a
solver interacts with a user. We begin with a formal characterization of
interactive ASP in terms of states and operations on them. In turn, we describe
the interactive ASP shell aspic along with its basic functionalities."
"Abstracting Gradual Typing (AGT) is an approach to systematically deriving
gradual counterparts to static type disciplines. The approach consists of
defining the semantics of gradual types by interpreting them as sets of static
types, and then defining an optimal abstraction back to gradual types. These
operations are used to lift the static discipline to the gradual setting. The
runtime semantics of the gradual language then arises as reductions on gradual
typing derivations.
  To demonstrate the flexibility of AGT, we gradualize $\lambda_\text{SEC}$,
the prototypical security-typed language, with respect to only security labels
rather than entire types, yielding a type system that ranges gradually from
simply-typed to securely-typed. We establish noninterference for the gradual
language, called $\lambda_{\widetilde{\text{SEC}}}$, using Zdancewic's logical
relation proof method. Whereas prior work presents gradual security cast
languages, which require explicit security casts, this work yields the first
gradual security source language, which requires no explicit casts."
"The static estimation of the energy consumed by program executions is an
important challenge, which has applications in program optimization and
verification, and is instrumental in energy-aware software development. Our
objective is to estimate such energy consumption in the form of functions on
the input data sizes of programs. We have developed a tool for experimentation
with static analysis which infers such energy functions at two levels, the
instruction set architecture (ISA) and the intermediate code (LLVM IR) levels,
and reflects it upwards to the higher source code level. This required the
development of a translation from LLVM IR to an intermediate representation and
its integration with existing components, a translation from ISA to the same
representation, a resource analyzer, an ISA-level energy model, and a mapping
from this model to LLVM IR. The approach has been applied to programs written
in the XC language running on XCore architectures, but is general enough to be
applied to other languages. Experimental results show that our LLVM IR level
analysis is reasonably accurate (less than 6.4% average error vs. hardware
measurements) and more powerful than analysis at the ISA level. This paper
provides insights into the trade-off of precision versus analyzability at these
levels."
"In this paper, we analyze the complexity of functional programs written in
the interaction-net computation model, an asynchronous, parallel and confluent
model that generalizes linear-logic proof nets. Employing user-defined sized
and scheduled types, we certify concrete time, space and space-time complexity
bounds for both sequential and parallel reductions of interaction-net programs
by suitably assigning complexity potentials to typed nodes. The relevance of
this approach is illustrated on archetypal programming examples. The provided
analysis is precise, compositional and is, in theory, not restricted to
particular complexity classes."
"We propose a light-weight client-server model of communication between
program analyses. Clients are individual analyses and the server mediates their
communication. A client cannot see properties of any other and the
communication is anonymous. There is no central algorithm standing above
clients which would tell them when to communicate what information. Clients
communicate with others spontaneously, according to their actual personal
needs. The model is based on our observation that a piece of information
provided to an analysis at a right place may (substantially) improve its
result. We evaluated the proposed communication model for all possible
combinations of three clients on more than 400 benchmarks and the results show
that the communication model performs well in practice."
"Microservices is an architectural style inspired by service-oriented
computing that has recently started gaining popularity. Jolie is a programming
language based on the microservices paradigm: the main building block of Jolie
systems are services, in contrast to, e.g., functions or objects. The
primitives offered by the Jolie language elicit many of the recurring patterns
found in microservices, like load balancers and structured processes. However,
Jolie still lacks some useful constructs for dealing with message types and
data manipulation that are present in service-oriented computing. In this
paper, we focus on the possibility of expressing choices at the level of data
types, a feature well represented in standards for Web Services, e.g., WSDL. We
extend Jolie to support such type choices and show the impact of our
implementation on some of the typical scenarios found in microservice systems.
This shows how computation can move from a process-driven to a data-driven
approach, and leads to the preliminary identification of recurring
communication patterns that can be shaped as design patterns."
"The abundance of poorly optimized mobile applications coupled with their
increasing centrality in our digital lives make a framework for mobile app
optimization an imperative. While tuning strategies for desktop and server
applications have a long history, it is difficult to adapt them for use on
mobile phones.
  Reference inputs which trigger behavior similar to a mobile application's
typical are hard to construct. For many classes of applications the very
concept of typical behavior is nonexistent, each user interacting with the
application in very different ways. In contexts like this, optimization
strategies need to evaluate their effectiveness against real user input, but
doing so online runs the risk of user dissatisfaction when suboptimal
optimizations are evaluated.
  In this paper we present an iterative compiler which employs a novel capture
and replay technique in order to collect real user input and use it later to
evaluate different transformations offline. The proposed mechanism identifies
and stores only the set of memory pages needed to replay the most heavily used
functions of the application. At idle periods, this minimal state is combined
with different binaries of the application, each one build with different
optimizations enabled. Replaying the targeted functions allows us to evaluate
the effectiveness of each set of optimizations for the actual way the user
interacts with the application.
  For the BEEBS benchmark suite, our approach was able to improve performance
by up to 57%, while keeping the slowdown experienced by the user on average at
0.8%. By focusing only on heavily used functions, we are able to conserve
storage space by between two and three orders of magnitude compared to typical
capture and replay implementations."
"Dynamically typed programming languages such as Python and JavaScript defer
type checking to run time. VM implementations can improve performance by
eliminating redundant dynamic type checks. However, type inference analyses are
often costly and involve tradeoffs between compilation time and resulting
precision. This has lead to the creation of increasingly complex multi-tiered
VM architectures.
  Lazy basic block versioning is a simple JIT compilation technique which
effectively removes redundant type checks from critical code paths. This novel
approach lazily generates type-specialized versions of basic blocks on-the-fly
while propagating context-dependent type information. This approach does not
require the use of costly program analyses, is not restricted by the precision
limitations of traditional type analyses.
  This paper extends lazy basic block versioning to propagate type information
interprocedurally, across function call boundaries. Our implementation in a
JavaScript JIT compiler shows that across 26 benchmarks, interprocedural basic
block versioning eliminates more type tag tests on average than what is
achievable with static type analysis without resorting to code transformations.
On average, 94.3% of type tag tests are eliminated, yielding speedups of up to
56%. We also show that our implementation is able to outperform Truffle/JS on
several benchmarks, both in terms of execution time and compilation time."
"Event-driven multi-threaded programming is fast becoming a preferred style of
developing efficient and responsive applications. In this concurrency model,
multiple threads execute concurrently, communicating through shared objects as
well as by posting asynchronous events that are executed in their order of
arrival. In this work, we consider partial order reduction (POR) for
event-driven multi-threaded programs. The existing POR techniques treat event
queues associated with threads as shared objects and thereby, reorder every
pair of events handled on the same thread even if reordering them does not lead
to different states. We do not treat event queues as shared objects and propose
a new POR technique based on a novel backtracking set called the
dependence-covering set. Events handled by the same thread are reordered by our
POR technique only if necessary. We prove that exploring dependence-covering
sets suffices to detect all deadlock cycles and assertion violations defined
over local variables. To evaluate effectiveness of our POR scheme, we have
implemented a dynamic algorithm to compute dependence-covering sets. On
execution traces obtained from a few Android applications, we demonstrate that
our technique explores many fewer transitions ---often orders of magnitude
fewer--- compared to exploration based on persistent sets, wherein, event
queues are considered as shared objects."
"PEGs are a formal grammar foundation for describing syntax, and are not hard
to generate parsers with a plain recursive decent parsing. However, the large
amount of C-stack consumption in the recursive parsing is not acceptable
especially in resource-restricted embedded systems. Alternatively, we have
attempted the machine virtualization approach to PEG-based parsing. MiniNez,
our implemented virtual machine, is presented in this paper with several
downsizing techniques, including instruction specialization, inline expansion
and static flow analysis. As a result, the MiniNez machine achieves both a very
small footprint and competitive performance to generated C parsers. We have
demonstrated the experimental results by comparing on two major embedded
platforms: Cortex-A7 and Intel Atom processor."
"This paper presents incremental verification-validation, a novel approach for
checking rich data structure invariants expressed as separation logic
assertions. Incremental verification-validation combines static verification of
separation properties with efficient, short-circuiting dynamic validation of
arbitrarily rich data constraints. A data structure invariant checker is an
inductive predicate in separation logic with an executable interpretation; a
short-circuiting checker is an invariant checker that stops checking whenever
it detects at run time that an assertion for some sub-structure has been fully
proven statically. At a high level, our approach does two things: it statically
proves the separation properties of data structure invariants using a static
shape analysis in a standard way but then leverages this proof in a novel
manner to synthesize short-circuiting dynamic validation of the data
properties. As a consequence, we enable dynamic validation to make up for
imprecision in sound static analysis while simultaneously leveraging the static
verification to make the remaining dynamic validation efficient. We show
empirically that short-circuiting can yield asymptotic improvements in dynamic
validation, with low overhead over no validation, even in cases where static
verification is incomplete."
"We present a framework for statically detecting deadlocks in a concurrent
object-oriented language with asynchronous method calls and cooperative
scheduling of method activations. Since this language features recursion and
dynamic resource creation, deadlock detection is extremely complex and
state-of-the-art solutions either give imprecise answers or do not scale. In
order to augment precision and scalability we propose a modular framework that
allows several techniques to be combined. The basic component of the framework
is a front-end inference algorithm that extracts abstract behavioural
descriptions of methods, called contracts, which retain resource dependency
information. This component is integrated with a number of possible different
back-ends that analyse contracts and derive deadlock information. As a
proof-of-concept, we discuss two such back-ends: (i) an evaluator that computes
a fixpoint semantics and (ii) an evaluator using abstract model checking."
"We study the problem of automatically computing the time complexity of
concurrent object-oriented programs. To determine this complexity we use
intermediate abstract descriptions that record relevant information for the
time analysis (cost of statements, creations of objects, and concurrent
operations), called behavioural types. Then, we define a translation function
that takes behavioural types and makes the parallelism explicit into so-called
cost equations, which are fed to an automatic off-the-shelf solver for
obtaining the time complexity."
"We describe an alternative solution to the impedance-mismatch problem between
programming and query languages: rather than embed queries in a programming
language, as done in LINQ systems, we embed programs in a query language, and
dub the result QINL."
"Galois connections are a foundational tool for structuring abstraction in
semantics and their use lies at the heart of the theory of abstract
interpretation. Yet, mechanization of Galois connections remains limited to
restricted modes of use, preventing their general application in mechanized
metatheory and certified programming.
  This paper presents constructive Galois connections, a variant of Galois
connections that is effective both on paper and in proof assistants; is
complete with respect to a large subset of classical Galois connections; and
enables more general reasoning principles, including the ""calculational"" style
advocated by Cousot.
  To design constructive Galois connection we identify a restricted mode of use
of classical ones which is both general and amenable to mechanization in
dependently-typed functional programming languages. Crucial to our metatheory
is the addition of monadic structure to Galois connections to control a
""specification effect"". Effectful calculations may reason classically, while
pure calculations have extractable computational content. Explicitly moving
between the worlds of specification and implementation is enabled by our
metatheory.
  To validate our approach, we provide two case studies in mechanizing existing
proofs from the literature: one uses calculational abstract interpretation to
design a static analyzer, the other forms a semantic basis for gradual typing.
Both mechanized proofs closely follow their original paper-and-pencil
counterparts, employ reasoning principles not captured by previous
mechanization approaches, support the extraction of verified algorithms, and
are novel."
"We present a new type system combining occurrence typing, previously used to
type check programs in dynamically-typed languages such as Racket, JavaScript,
and Ruby, with dependent refinement types. We demonstrate that the addition of
refinement types allows the integration of arbitrary solver-backed reasoning
about logical propositions from external theories. By building on occurrence
typing, we can add our enriched type system as an extension of Typed
Racket---adding dependency and refinement reuses the existing formalism while
increasing its expressiveness.
  Dependent refinement types allow Typed Racket programmers to express rich
type relationships, ranging from data structure invariants such as red-black
tree balance to preconditions such as vector bounds. Refinements allow
programmers to embed the propositions that occurrence typing in Typed Racket
already reasons about into their types. Further, extending occurrence typing to
refinements allows us to make the underlying formalism simpler and more
powerful.
  In addition to presenting the design of our system, we present a formal model
of the system, show how to integrate it with theories over both linear
arithmetic and bitvectors, and evaluate the system in the context of the full
Typed Racket implementation. Specifically, we take safe vector access as a case
study, and examine all vector accesses in a 56,000 line corpus of Typed Racket
programs. Our system is able to prove that 50% of these are safe with no new
annotation, and with a few annotations and modifications, we can capture close
to 80%."
"Given a multithreaded program written assuming a friendly, non-preemptive
scheduler, the goal of synchronization synthesis is to automatically insert
synchronization primitives to ensure that the modified program behaves
correctly, even with a preemptive scheduler. In this work, we focus on the
quality of the synthesized solution: we aim to infer synchronization placements
that not only ensure correctness, but also meet some quantitative objectives
such as optimal program performance on a given computing platform.
  The key step that enables solution optimization is the construction of a set
of global constraints over synchronization placements such that each model of
the constraints set corresponds to a correctness-ensuring synchronization
placement. We extract the global constraints from generalizations of
counterexample traces and the control-flow graph of the program. The global
constraints enable us to choose from among the encoded synchronization
solutions using an objective function. We consider two types of objective
functions: ones that are solely dependent on the program (e.g., minimizing the
size of critical sections) and ones that are also dependent on the computing
platform. For the latter, given a program and a computing platform, we
construct a performance model based on measuring average contention for
critical sections and the average time taken to acquire and release a lock
under a given average contention.
  We empirically evaluated that our approach scales to typical module sizes of
many real world concurrent programs such as device drivers and multithreaded
servers, and that the performance predictions match reality. To the best of our
knowledge, this is the first comprehensive approach for optimizing the
placement of synthesized synchronization."
"Separation Logic (SL) was a significant advance in program verification of
data structures. It used a ""separating"" conjoin operator in data structure
specifications to construct heaps from disjoint subheaps, and a \emph{frame
rule} to very elegantly realize local reasoning. Consequently, when a program
is verified in SL, the proof is very natural and succinct. In this paper, we
present a new program verification framework whose first motivation is to
maintain the essential advantage of SL, that of expressing separation and then
using framing to obtain local reasoning. Our framework comprises two new
facets. First, we begin with a new domain of discourse of \emph{explicit
subheaps} with \emph{recursive definitions}. The resulting specification
language can describe arbitrary data structures, and arbitratry \emph{sharing}
therein. This enables a very precise specification of frames. Second, we
perform program verification by using a strongest postcondition propagation in
symbolic execution, and this provides a basis for \emph{automation}. Finally,
we present an implementation of our verifier, and demonstrate automation on a
number of representative programs. In particular, we present the first
automatic proof of a classic graph marking algorithm."
"Nez is a PEG(Parsing Expressing Grammar)-based open grammar language that
allows us to describe complex syntax constructs without action code. Since open
grammars are declarative and free from a host programming language of parsers,
software engineering tools and other parser applications can reuse once-defined
grammars across programming languages.
  A key challenge to achieve practical open grammars is the expressiveness of
syntax constructs and the resulting parser performance, as the traditional
action code approach has provided very pragmatic solutions to these two issues.
In Nez, we extend the symbol-based state management to recognize
context-sensitive language syntax, which often appears in major programming
languages. In addition, the Abstract Syntax Tree constructor allows us to make
flexible tree structures, including the left-associative pair of trees. Due to
these extensions, we have demonstrated that Nez can parse not all but many
grammars.
  Nez can generate various types of parsers since all Nez operations are
independent of a specific parser language. To highlight this feature, we have
implemented Nez with dynamic parsing, which allows users to integrate a Nez
parser as a parser library that loads a grammar at runtime. To achieve its
practical performance, Nez operators are assembled into low-level virtual
machine instructions, including automated state modifications when
backtracking, transactional controls of AST construction, and efficient
memoization in packrat parsing. We demonstrate that Nez dynamic parsers achieve
very competitive performance compared to existing efficient parser generators."
"Parsing Expression Grammars are a popular foundation for describing syntax.
Unfortunately, several syntax of programming languages are still hard to
recognize with pure PEGs. Notorious cases appears: typedef-defined names in
C/C++, indentation-based code layout in Python, and HERE document in many
scripting languages. To recognize such PEG-hard syntax, we have addressed a
declarative extension to PEGs. The ""declarative"" extension means no programmed
semantic actions, which are traditionally used to realize the extended parsing
behavior. Nez is our extended PEG language, including symbol tables and
conditional parsing. This paper demonstrates that the use of Nez Extensions can
realize many practical programming languages, such as C, C\#, Ruby, and Python,
which involve PEG-hard syntax."
"This volume collects the extended versions of selected papers originally
presented at the two ACM SIGPLAN workshops: ML Family Workshop 2014 and OCaml
2014. Both were affiliated with ICFP 2014 and took place on two consecutive
days, on September 4 and 5, 2014 in Gothenburg, Sweden.
  The ML Family workshop aims to recognize the entire extended family of ML and
ML-like languages: languages that are Higher-order, Typed, Inferred, and
Strict. It provides the forum to discuss common issues, both practical
(compilation techniques, implementations of concurrency and parallelism,
programming for the Web) and theoretical (fancy types, module systems,
metaprogramming). The scope of the workshop includes all aspects of the design,
semantics, theory, application, implementation, and teaching of the members of
the ML family.
  The OCaml workshop is more specifically targeted at the OCaml community, with
an emphasis on new proposals and tools aiming to improve OCaml, its
environment, and the functioning of the community. As such, it is interested in
works on the type system, language extensions, compiler and optimizations,
applications, tools, and experience reports of exciting uses."
"In this paper, we argue that modern functional programming languages - in
particular, FSharp on the .NET platform - are well suited for the development
of distributed, web and cloud applications on the Internet. We emphasize that
FSharp can be successfully used in a range of scenarios - starting from simple
ASP.NET web applications, and including cloud data processing tasks and
data-driven web applications. In particular, we show how some of the FSharp
features (eg. quotations) can be effectively used to develop a distributed web
system using single code-base, and describe the commercial WebSharper project
by Intellifactory for building distributed client-server web applications, as
well as research library that uses Windows Azure for parametric sweep
computational tasks."
"We present modular implicits, an extension to the OCaml language for ad-hoc
polymorphism inspired by Scala implicits and modular type classes. Modular
implicits are based on type-directed implicit module parameters, and elaborate
straightforwardly into OCaml's first-class functors. Basing the design on
OCaml's modules leads to a system that naturally supports many features from
other languages with systematic ad-hoc overloading, including inheritance,
instance constraints, constructor classes and associated types."
"Cryptic type error messages are a major obstacle to learning OCaml or other
ML-based languages. In many cases, error messages cannot be interpreted without
a sufficiently-precise model of the type inference algorithm. The problem of
improving type error messages in ML has received quite a bit of attention over
the past two decades, and many different strategies have been considered. The
challenge is not only to produce error messages that are both sufficiently
concise and systematically useful to the programmer, but also to handle a
full-blown programming language and to cope with large-sized programs
efficiently.
  In this work, we present a modification to the traditional ML type inference
algorithm implemented in OCaml that, by significantly reducing the
left-to-right bias, allows us to report error messages that are more helpful to
the programmer. Our algorithm remains fully predictable and continues to
produce fairly concise error messages that always help making some progress
towards fixing the code. We implemented our approach as a patch to the OCaml
compiler in just a few hundred lines of code. We believe that this patch should
benefit not just to beginners, but also to experienced programs developing
large-scale OCaml programs."
"Linear algebra is a major field of numerical computation and is widely
applied. Most linear algebra libraries (in most programming languages) do not
statically guarantee consistency of the dimensions of vectors and matrices,
causing runtime errors. While advanced type systems--specifically, dependent
types on natural numbers--can ensure consistency among the sizes of collections
such as lists and arrays, such type systems generally require non-trivial
changes to existing languages and application programs, or tricky type-level
programming.
  We have developed a linear algebra library interface that verifies the
consistency (with respect to dimensions) of matrix operations by means of
generative phantom types, implemented via fairly standard ML types and module
system. To evaluate its usability, we ported to it a practical machine learning
library from a traditional linear algebra library. We found that most of the
changes required for the porting could be made mechanically, and changes that
needed human thought are minor."
"Programs that transform other programs often require access to the internal
structure of the program to be transformed. This is at odds with the usual
extensional view of functional programming, as embodied by the lambda calculus
and SK combinator calculus. The recently-developed SF combinator calculus
offers an alternative, intensional model of computation that may serve as a
foundation for developing principled languages in which to express intensional
computation, including program transformation. Until now there have been no
static analyses for reasoning about or verifying programs written in
SF-calculus. We take the first step towards remedying this by developing a
formulation of the popular control flow analysis 0CFA for SK-calculus and
extending it to support SF-calculus. We prove its correctness and demonstrate
that the analysis is invariant under the usual translation from SK-calculus
into SF-calculus."
"Context-Oriented programming languages provide us with primitive constructs
to adapt program behaviour depending on the evolution of their operational
environment, namely the context. In previous work we proposed ML_CoDa, a
context-oriented language with two-components: a declarative constituent for
programming the context and a functional one for computing. This paper
describes the implementation of ML_CoDa as an extension of F#."
"We develop the operational semantics of an untyped probabilistic
lambda-calculus with continuous distributions, as a foundation for universal
probabilistic programming languages such as Church, Anglican, and Venture. Our
first contribution is to adapt the classic operational semantics of
lambda-calculus to a continuous setting via creating a measure space on terms
and defining step-indexed approximations. We prove equivalence of big-step and
small-step formulations of this distribution-based semantics. To move closer to
inference techniques, we also define the sampling-based semantics of a term as
a function from a trace of random samples to a value. We show that the
distribution induced by integrating over all traces equals the
distribution-based semantics. Our second contribution is to formalize the
implementation technique of trace Markov chain Monte Carlo (MCMC) for our
calculus and to show its correctness. A key step is defining sufficient
conditions for the distribution induced by trace MCMC to converge to the
distribution-based semantics. To the best of our knowledge, this is the first
rigorous correctness proof for trace MCMC for a higher-order functional
language."
"We consider two classes of stream-based computations which admit taking
linear combinations of execution runs: probabilistic sampling and generalized
animation. The dataflow architecture is a natural platform for programming with
streams. The presence of linear combinations allows us to introduce the notion
of almost continuous transformation of dataflow graphs. We introduce a new
approach to higher-order dataflow programming: a dynamic dataflow program is a
stream of dataflow graphs evolving by almost continuous transformations. A
dynamic dataflow program would typically run while it evolves. We introduce
Fluid, an experimental open source system for programming with dataflow graphs
and almost continuous transformations."
"We consider dataflow architecture for two classes of computations which admit
taking linear combinations of execution runs: probabilistic sampling and
generalized animation. We improve the earlier technique of almost continuous
program transformations by adopting a discipline of bipartite graphs linking
nodes obtained via general transformations and nodes obtained via linear
transformations which makes it possible to develop and evolve dataflow programs
over these classes of computations by continuous program transformations. The
use of bipartite graphs allows us to represent the dataflow programs from this
class as matrices of real numbers and evolve and modify programs by continuous
change of these numbers.
  We develop a formalism for higher-order dataflow programming for this class
of dataflow graphs based on the higher-order matrix elements. Some of our
software experiments are briefly discussed."
"Programming languages serve a dual purpose: to communicate programs to
computers, and to communicate programs to humans. Indeed, it is this dual
purpose that makes programming language design a constrained and challenging
problem. Inheritance is an essential aspect of that second purpose: it is a
tool to improve communication. Humans understand new concepts most readily by
first looking at a number of concrete examples, and later abstracting over
those examples. The essence of inheritance is that it mirrors this process: it
provides a formal mechanism for moving from the concrete to the abstract."
"Bidirectional transformations (bx) have primarily been modeled as pure
functions, and do not account for the possibility of the side-effects that are
available in most programming languages. Recently several formulations of bx
that use monads to account for effects have been proposed, both among
practitioners and in academic research. The combination of bx with effects
turns out to be surprisingly subtle, leading to problems with some of these
proposals and increasing the complexity of others. This paper reviews the
proposals for monadic lenses to date, and offers some improved definitions,
paying particular attention to the obstacles to naively adding monadic effects
to existing definitions of pure bx such as lenses and symmetric lenses, and the
subtleties of equivalence of symmetric bidirectional transformations in the
presence of effects."
"High Level Language Virtual Machines is a core topic of interest for the
researchers who are into virtual execution environments. As an open source
virtual machine released to 16 universities, as early as 2001, Jikes RVM has
been a major drive for many researches. While working on this project, we
studied the JIT compilation of Jikes RVM as well as the Garbage Collection (GC)
which is handled by the Memory Management Toolkit (MMTk), a part of the Jikes
RVM. We also studied the Compressor Mark-Compact Collector algorithm and
implemented it for MMTk. We have also implemented a micro-benchmark for the GC
algorithms in Java, named ""XPDBench"", for benchmarking the implementations."
"There has been much recent interest in adopting functional and reactive
programming for use in real-time system design. Moving toward a more
declarative methodology for developing real-time systems purports to improve
the fidelity of software. To study the benefits of functional and reactive
programming for real-time systems, real-time aware functional compilers and
language runtimes are required. In this paper we examine the necessary changes
to a modern Standard ML compiler, MLton, to provide basic support for real-time
execution. We detail our current progress in modifying MLton with a threading
model that supports priorities, a chunked object model to support real-time
garbage collection, and low level modification to execute on top of a real-time
operating system. We present preliminary numbers and our work in progress
prototype, which is able to boot ML programs compiled with MLton on x86
machines."
"In most constraint programming systems, a limited number of search engines is
offered while the programming of user-customized search algorithms requires
low-level efforts, which complicates the deployment of such algorithms. To
alleviate this limitation, concepts such as computation spaces have been
developed. Computation spaces provide a coarse-grained restoration mechanism,
because they store all information contained in a search tree node. Other
granularities are possible, and in this paper we make the case for dynamically
adapting the restoration granularity during search. In order to elucidate
programmable restoration granularity, we present restoration as an aspect of a
constraint programming system, using the model of aspect-oriented programming.
A proof-of-concept implementation using Gecode shows promising results."
"Search is a key service within constraint programming systems, and it demands
the restoration of previously accessed states during the exploration of a
search tree. Restoration proceeds either bottom-up within the tree to roll back
previously performed operations using a trail, or top-down to redo them,
starting from a previously stored state and using suitable information stored
along the way. In this paper, we elucidate existing restoration techniques
using a pair of abstract methods and employ them to present a new technique
that we call recollection. The proposed technique stores the variables that
were affected by constraint propagation during fix points reasoning steps, and
it conducts neither operation roll-back nor recomputation, while consuming much
less memory than storing previous visited states. We implemented this idea as a
prototype within the Gecode solver. An empirical evaluation reveals that
constraint problems with expensive propagation and frequent failures can
benefit from recollection with respect to runtime at the expense of a marginal
increase in memory consumption, comparing with the most competitive variant of
recomputation."
"Virtual Machines (VMs) with Just-In-Time (JIT) compilers are traditionally
thought to execute programs in two phases: the initial warmup phase determines
which parts of a program would most benefit from dynamic compilation, before
JIT compiling them into machine code; subsequently the program is said to be at
a steady state of peak performance. Measurement methodologies almost always
discard data collected during the warmup phase such that reported measurements
focus entirely on peak performance. We introduce a fully automated statistical
approach, based on changepoint analysis, which allows us to determine if a
program has reached a steady state and, if so, whether that represents peak
performance or not. Using this, we show that even when run in the most
controlled of circumstances, small, deterministic, widely studied
microbenchmarks often fail to reach a steady state of peak performance on a
variety of common VMs. Repeating our experiment on 3 different machines, we
found that at most 43.5% of <VM, benchmark> pairs consistently reach a steady
state of peak performance."
"Running distributed applications in the cloud involves deployment. That is,
distribution and configuration of application services and middleware
infrastructure. The considerable complexity of these tasks resulted in the
emergence of declarative JSON-based domain-specific deployment languages to
develop deployment programs. However, existing deployment programs unsafely
compose artifacts written in different languages, leading to bugs that are hard
to detect before run time. Furthermore, deployment languages do not provide
extension points for custom implementations of existing cloud services such as
application-specific load balancing policies.
  To address these shortcomings, we propose CPL (Cloud Platform Language), a
statically-typed core language for programming both distributed applications as
well as their deployment on a cloud platform. In CPL, application services and
deployment programs interact through statically typed, extensible interfaces,
and an application can trigger further deployment at run time. We provide a
formal semantics of CPL and demonstrate that it enables type-safe, composable
and extensible libraries of service combinators, such as load balancing and
fault tolerance."
"""The von Neumann vicious circle"" means that non-von Neumann computer
architectures cannot be developed because of the lack of widely available and
effective non-von Neumann languages. New languages cannot be created because of
lack of conceptual foundations for non-von Neumann architectures. The reason is
that programming languages are high-level abstract isomorphic copies of von
Neumann computer architectures. This constitutes the current paradigm in
Computer Science. The paradigm is equivalent to the predominant view that
computations on higher order objects (functionals) can be done only
symbolically, i.e. by term rewriting. The paper is a short introduction to the
papers arXiv:1501.03043 and arXiv:1510.02787 trying to break the paradigm by
introducing a framework that may be seen as a higher order functional HDL
(Hardware Description Language)."
"Side effects are a core part of practical programming. However, they are
often hard to reason about, particularly in a concurrent setting. We propose a
foundation for reasoning about concurrent side effects using sessions.
Primarily, we show that session types are expressive enough to encode an effect
system for stateful processes. This is formalised via an effect-preserving
encoding of a simple imperative language with an effect system into the
pi-calculus with session primitives and session types (into which we encode
effect specifications). This result goes towards showing a connection between
the expressivity of session types and effect systems. We briefly discuss how
the encoding could be extended and applied to reason about and control
concurrent side effects."
"Concurrent programs executing on NUMA architectures consist of concurrent
entities (e.g. threads, actors) and data placed on different nodes. Execution
of these concurrent entities often reads or updates states from remote nodes.
The performance of such systems depends on the extent to which the concurrent
entities can be executing in parallel, and on the amount of the remote reads
and writes.
  We consider an actor-based object oriented language, and propose a type
system which expresses the topology of the program (the placement of the actors
and data on the nodes), and an effect system which characterises remote reads
and writes (in terms of which node reads/writes from which other nodes). We use
a variant of ownership types for the topology, and a combination of behavioural
and ownership types for the effect system."
"A recent study of bugs in real-world concurrent and distributed systems found
that, while implementations of individual protocols tend to be robust, the
composition of multiple protocols and its interplay with internal computation
is the culprit for most errors. Multiparty Session Types and Choreographic
Programming are methodologies for developing correct-by-construction concurrent
and distributed software, based on global descriptions of communication flows.
However, protocol composition is either limited or left unchecked. Inspired by
these two methodologies, in this work we present a new language model for the
safe composition of protocols, called Procedural Choreographies (PC). Protocols
in PC are procedures, parameterised on the processes that enact them.
Procedures define communications declaratively using global descriptions, and
programs are written by invoking and composing these procedures. An
implementation in terms of a process model is then mechanically synthesised,
guaranteeing correctness and deadlock-freedom. We study PC in the settings of
synchronous and asynchronous communications, and illustrate its expressivity
with some representative examples."
"Transactional memory (TM) has emerged as a promising abstraction for
concurrent programming alternative to lock-based synchronizations. However,
most TM models admit only isolated transactions, which are not adequate in
multi-threaded programming where transactions have to interact via shared data
before committing. In this paper, we present Open Transactional Memory (OTM), a
programming abstraction supporting safe, data-driven interactions between
composable memory transactions. This is achieved by relaxing isolation between
transactions, still ensuring atomicity: threads of different transactions can
interact by accessing shared variables, but then their transactions have to
commit together-actually, these transactions are transparently merged. This
model allows for loosely-coupled interactions since transaction merging is
driven only by accesses to shared data, with no need to specify participants
beforehand. In this paper we provide a specification of the OTM in the setting
of Concurrent Haskell, showing that it is a conservative extension of current
STM abstraction. In particular, we provide a formal semantics, which allows us
to prove that OTM satisfies the \emph{opacity} criterion."
"Homogeneous generative meta-programming (HGMP) enables the generation of
program fragments at compile-time or run-time. We present the first
foundational calculus which can model powerful HGMP languages such as Template
Haskell. The calculus is designed such that we can gradually enhance it with
the features needed to model many of the advanced features of real languages.
As a demonstration of the flexibility of our approach, we also provide a simple
type system for the calculus."
"This paper addresses the problem of creating simplifiers for logic formulas
based on conditional term rewriting. In particular, the paper focuses on a
program synthesis application where formula simplifications have been shown to
have a significant impact. We show that by combining machine learning
techniques with constraint-based synthesis, it is possible to synthesize a
formula simplifier fully automatically from a corpus of representative
problems, making it possible to create formula simplifiers tailored to specific
problem domains. We demonstrate the benefits of our approach for synthesis
benchmarks from the SyGuS competition and automated grading."
"We introduce a trace semantics for a call-by-value language with full
polymorphism and higher-order references. This is an operational game semantics
model based on a nominal interpretation of parametricity whereby polymorphic
values are abstracted with special kinds of names. The use of polymorphic
references leads to violations of parametricity which we counter by closely
recoding the disclosure of typing information in the semantics. We prove the
model sound for the full language and strengthen our result to full abstraction
for a large fragment where polymorphic references obey specific inhabitation
conditions."
"Choreographic Programming is a development methodology for concurrent
software that guarantees correctness by construction. The key to this paradigm
is to disallow mismatched I/O operations in programs, called choreographies,
and then mechanically synthesise distributed implementations in terms of
standard process models via a mechanism known as EndPoint Projection (EPP).
  Despite the promise of choreographic programming, there is still a lack of
practical evaluations that illustrate the applicability of choreographies to
concrete computational problems with standard concurrent solutions. In this
work, we explore the potential of choreographies by using Procedural
Choreographies (PC), a model that we recently proposed, to write distributed
algorithms for sorting (Quicksort), solving linear equations (Gaussian
elimination), and computing Fast Fourier Transform. We discuss the lessons
learned from this experiment, giving possible directions for the usage and
future improvements of choreography languages."
"GitHub is the most widely used social, distributed version control system. It
has around 10 million registered users and hosts over 16 million public
repositories. Its user base is also very active as GitHub ranks in the top 100
Alexa most popular websites. In this study, we collect GitHub's state in its
entirety. Doing so, allows us to study new aspects of the ecosystem. Although
GitHub is the home to millions of users and repositories, the analysis of
users' activity time-series reveals that only around 10% of them can be
considered active. The collected dataset allows us to investigate the
popularity of programming languages and existence of pattens in the relations
between users, repositories, and programming languages.
  By, applying a k-means clustering method to the users-repositories commits
matrix, we find that two clear clusters of programming languages separate from
the remaining. One cluster forms for ""web programming"" languages (Java Script,
Ruby, PHP, CSS), and a second for ""system oriented programming"" languages (C,
C++, Python). Further classification, allow us to build a phylogenetic tree of
the use of programming languages in GitHub. Additionally, we study the main and
the auxiliary programming languages of the top 1000 repositories in more
detail. We provide a ranking of these auxiliary programming languages using
various metrics, such as percentage of lines of code, and PageRank."
"Obtaining good performance when programming heterogeneous computing platforms
poses significant challenges for the programmer. We present a program
transformation environment, implemented in Haskell, where architecture-agnostic
scientific C code with semantic annotations is transformed into functionally
equivalent code better suited for a given platform. The transformation steps
are formalized (and implemented) as rules which can be fired when certain
syntactic and semantic conditions are met. These conditions are to be fulfilled
by program properties which can be automatically inferred or, alternatively,
stated as annotations in the source code. Rule selection can be guided by
heuristics derived from a machine learning procedure which tries to capture how
run-time characteristics (e.g., resource consumption or performance) are
affected by the transformation steps."
"The current trend in next-generation exascale systems goes towards
integrating a wide range of specialized (co-)processors into traditional
supercomputers. However, the integration of different specialized devices
increases the degree of heterogeneity and the complexity in programming such
type of systems. Due to the efficiency of heterogeneous systems in terms of
Watt and FLOPS per surface unit, opening the access of heterogeneous platforms
to a wider range of users is an important problem to be tackled. In order to
bridge the gap between heterogeneous systems and programmers, in this paper we
propose a machine learning-based approach to learn heuristics for defining
transformation strategies of a program transformation system. Our approach
proposes a novel combination of reinforcement learning and classification
methods to efficiently tackle the problems inherent to this type of systems.
Preliminary results demonstrate the suitability of the approach for easing the
programmability of heterogeneous systems."
"Providing feedback on programming assignments is a tedious task for the
instructor, and even impossible in large Massive Open Online Courses with
thousands of students. In this paper, we present a novel technique for
automatic feedback generation: (1) For a given programming assignment, we
automatically cluster the correct student attempts using a dynamic program
analysis. From each cluster we select one student attempt as a specification.
(2) Given an incorrect student attempt we run a repair procedure against all
specifications, and automatically generate the minimal repair based on one of
them. We implemented the proposed approach in a publicly available tool and
evaluated it on a large number of existing student attempts, and additionally
performed a user study about usefulness of provided feedback, and established
that: our tool can completely automatically, in real time, repair a large
number of student attempts, including complicated and larger repairs, while
preliminary results show feedback to be useful."
"Observational determinism is a security property that characterizes secure
information flow for multithreaded programs. Most of the methods that have been
used to verify observational determinism are based on either type systems or
conventional model checking techniques. A conventional model checker is
stateful and often verifies a system model usually constructed manually. As
these methods are based on stateful model checking, they are confronted with
the state space explosion problem. In order to verify and test computer
programs, stateless code model checking is more appropriate than conventional
techniques. It is an effective method for systematic testing of large and
complicated concurrent programs, and for exploring the state space of such
programs. In this paper, we propose a new method for verifying information flow
security in concurrent programs. For the first time, we use stateless code
model checking to verify observational determinism."
"Verification of large and complicated concurrent programs is an important
issue in the software world. Stateless model checking is an appropriate method
for systematically and automatically testing of large programs, which has
proved its power in verifying code of large programs. Another well-known method
in this area is runtime verification. Both stateless model checking and runtime
verification are similar in some ways. One common approach in runtime
verification is to construct runtime monitors for properties expressed in
linear temporal logic. Currently, there are some semantics to check linear
temporal logic formulae on finite paths proposed in the field of runtime
verification, which can also be applied to stateless model checking. However,
existing stateless model checkers do not support LTL formulae. In some
settings, it is more advantageous to make use of stateless model checking
instead of runtime verification. This paper proposes a novel encoding of one of
the recent LTL semantics on finite paths into an actor-based system. We take a
truly parallel approach without saving any program states or traces, which not
only addresses important problems in runtime verification, but can also be
applied to stateless model checking."
"Stateless code model checking is an effective verification technique, which
is more applicable than stateful model checking to the software world. Existing
stateless model checkers support the verification of neither LTL formulae nor
the information flow security properties. This paper proposes a distributed
stateless code model checker (DSCMC) designed based on the Actor model, and has
the capability of verifying code written in different programming languages.
This tool is implemented using Erlang, which is an actor-based programming
language. DSCMC is able to detect deadlocks, livelocks, and data races
automatically. In addition, the tool can verify information flow security and
the properties specified in LTL. Thanks to its actor-based architecture, DSCMC
provides a wide range of capabilities. The parallel architecture of the tool
exploiting the rich concurrency model of Erlang is suited to the time-intensive
process of stateless code model checking."
"We extend Levy's call-by-push-value (CBPV) analysis from simple to dependent
type theory (DTT) in order to study the interaction between computational
effects and dependent types. We define the naive system of dependently typed
CBPV, dCBPV-, and its extension with a principle of Kleisli extensions for
dependent functions, dCBPV+. We investigate these systems from the points of
view of syntax, categorical semantics, concrete models and operational
semantics, in presence of a range of effects. We observe that, while the
expressive power of dCBPV+ is needed if we want well-defined call-by-value
(CBV) and call-by-name (CBN) translations of DTT, it is a less straightforward
system than dCBPV-, in presence of some effects. Indeed, to be able to
construct specific models and to retain the subject reduction property in the
operational semantics, we are required to impose certain subtyping conditions,
the idea being that the type of a computation may only become more (not less)
specified as certain effects are executed."
"This paper presents the insight that practical embedding techniques, commonly
used for implementing Domain-Specific Languages, correspond to theoretical
Normalisation-By-Evaluation (NBE) techniques, commonly used for deriving
canonical form of terms with respect to an equational theory.
  NBE constitutes of four components: a syntactic domain, a semantic domain,
and a pair of translations between the two. Embedding also often constitutes of
four components: an object language, a host language, encoding of object terms
in the host, and extraction of object code from the host.
  The correspondence is deep in that all four components in embedding and NBE
correspond to each other. Based on this correspondence, this paper introduces
Embedding-By-Normalisation (EBN) as a principled approach to study and
structure embedding.
  The correspondence is useful in that solutions from NBE can be borrowed to
solve problems in embedding. In particular, based on NBE techniques, such as
Type-Directed Partial Evaluation, this paper presents a solution to the problem
of extracting object code from embedded programs involving sum types, such as
conditional expressions, and primitives, such as literals and operations on
them."
"This paper explains why the CVC Verilog hardware description language (HDL)
optimized flow graph compiled simulator is fast. CVC is arguably the fastest
full IEEE 1364 2005 standard compiled Verilog simulator available yet consists
of only 95,000 lines of C code and was developed by only two people. The paper
explains how CVC validates the anti-formalism computer science methodology best
expressed by Peter Naur's datalogy and provides specific guidelines for
applying the method. CVC development history from a slow interpreter into a
fast flow graph based machine code compiled simulator is described. The failure
of initial efforts that tried to convert CVC into interpreted execution of
possibly auto generated virtual machines is discussed. The paper presents
evidence showing CVC's speed by comparing CVC against the open source Icarus
simulator. CVC is normally 35 to 45 times faster than Icarus, but can be as
fast as 111 times or as slow as 30 times. The paper then criticizes the
competing Allen and Kennedy theory from their ""Optimizing Compilers"" book that
argues fast Verilog simulation requires detail removing high level abstraction.
CVC speed comes from efficient low level usage of microprocessor instruction
parallelism. The paper concludes with a discussion of why the author believes
special purpose full accurate delay 1364 standard hardware Verilog simulators
and parallel Verilog simulation distributed over many processors can not be
faster. CVC is available as open source software."
"Mainstream object-oriented programming languages such as Java, C#, C++ and
Scala are all almost entirely nominally-typed. NOOP is a recently developed
domain-theoretic model of OOP that was designed to include full nominal
information found in nominally-typed OOP. This paper compares NOOP to the most
widely known domain-theoretic models of OOP, namely, the models developed by
Cardelli and Cook, which were structurally-typed models. Leveraging the
development of NOOP, the comparison presented in this paper provides a clear
and precise mathematical account for the relation between nominal and
structural OO type systems."
"This article presents a formalisation of a simple imperative programming
language. The objective is to study and develop ""hands-on"" a formal
specifcation of a programming language, namely its syntax, operational
semantics and type system. To have an executable version of the language, we
implemented in Racket its operational semantics and type system."
"To support growing massive parallelism, functional components and also the
capabilities of current processors are changing and continue to do so. Todays
computers are built upon multiple processing cores and run applications
consisting of a large number of threads, making runtime thread management a
complex process. Further, each core can support multiple, concurrent thread
execution. Hence, hardware and software support for threads is more and more
needed to improve peak-performance capacity, overall system throughput, and has
therefore been the subject of much research. This paper surveys, many of the
proposed or currently available solutions for executing, distributing and
managing threads both in hardware and software. The nature of current
applications is diverse. To increase the system performance, all programming
models may not be suitable to harness the built-in massive parallelism of
multicore processors. Due to the heterogeneity in hardware, hybrid programming
model (which combines the features of shared and distributed model) currently
has become very promising. In this paper, first, we have given an overview of
threads, threading mechanisms and its management issues during execution. Next,
we discuss about different parallel programming models considering to their
explicit thread support. We also review the programming models with respect to
their support to shared-memory, distributed-memory and heterogeneity. Hardware
support at execution time is very crucial to the performance of the system,
thus different types of hardware support for threads also exist or have been
proposed, primarily based on widely used programming models. We also further
discuss on software support for threads, to mainly increase the deterministic
behavior during runtime. Finally, we conclude the paper by discussing some
common issues related to the thread management."
"Optimizing floating-point arithmetic is vital because it is ubiquitous,
costly, and used in compute-heavy workloads. Implementing precise optimizations
correctly, however, is difficult, since developers must account for all the
esoteric properties of floating-point arithmetic to ensure that their
transformations do not alter the output of a program. Manual reasoning is error
prone and stifles incorporation of new optimizations. We present an approach to
automate reasoning about floating-point optimizations using satisfiability
modulo theories (SMT) solvers. We implement the approach in LifeJacket, a
system for automatically verifying precise floating-point optimizations for the
LLVM assembly language. We have used LifeJacket to verify 43 LLVM optimizations
and to discover eight incorrect ones, including three previously unreported
problems. LifeJacket is an open source extension of the Alive system for
optimization verification."
"Computing precise (fully flow-sensitive and context-sensitive) and exhaustive
points-to information is computationally expensive. Many practical tools
approximate the points-to information trading precision for efficiency. This
has adverse impact on computationally intensive analyses such as model
checking. Past explorations in top-down approaches of fully flow- and
context-sensitive points-to analysis (FCPA) have not scaled. We explore the
alternative of bottom-up interprocedural approach which constructs summary flow
functions for procedures to represent the effect of their calls. This approach
has been effectively used for many analyses. However, it is computationally
expensive for FCPA which requires modelling unknown locations accessed
indirectly through pointers. Such accesses are commonly handled by using
placeholders to explicate unknown locations or by using multiple call-specific
summary flow functions.
  We generalize the concept of points-to relations by using the counts of
indirection levels leaving the unknown locations implicit. This allows us to
create summary flow functions in the form of generalized points-to graphs
(GPGs) without the need of placeholders. By design, GPGs represent both memory
(in terms of classical points-to facts) and memory transformers (in terms of
generalized points-to facts). We perform FCPA by progressively reducing
generalized points-to facts to classical points-to facts. GPGs distinguish
between may and must pointer updates thereby facilitating strong updates within
calling contexts.
  The size of GPG is linearly bounded by the number of variables and is
independent of the number of statements in the procedure. Empirical
measurements on SPEC benchmarks show that GPGs are indeed compact in spite of
large procedure sizes. This allows us to scale FCPA to 158 kLoC using GPGs
(compared to 35 kLoC reported by liveness-based FCPA)."
"The article gives a brief overview of the current state of programming
language Dino in order to see where its stands between other dynamic
programming languages. Then it describes the current implementation, used tools
and major implementation decisions including how to implement a stable,
portable and simple JIT compiler.
  We study the effect of major implementation decisions on the performance of
Dino on x86-64, AARCH64, and Powerpc64. In brief, the performance of some model
benchmark on x86-64 was improved by $\textbf{3.1}$ times after moving from a
stack based virtual machine to a register-transfer architecture, a further
$\textbf{1.5}$ times by adding byte code combining, a further $\textbf{2.3}$
times through the use of JIT, and a further $\textbf{4.4}$ times by performing
type inference with byte code specialization, with a resulting overall
performance improvement of about $\textbf{47}$ times. To put these results in
context, we include performance comparisons of Dino with widely used
implementations of Ruby, Python 3, PyPy and JavaScript on the three platforms
mentioned above.
  The goal of this article is to share the experience of Dino implementation
with other dynamic language implementors in hope that it can help them to
improve implementation of popular dynamic languages to make them probably
faster and more portable, using less developer resources, and may be to avoid
some mistakes and wrong directions which were experienced during Dino
development."
"Standard higher-order contract monitoring breaks tail recursion and leads to
space leaks that can change a program's asymptotic complexity; space-efficiency
restores tail recursion and bounds the amount of space used by contracts.
Space-efficient contract monitoring for contracts enforcing simple type
disciplines (a/k/a gradual typing) is well studied. Prior work establishes a
space-efficient semantics for manifest contracts without dependency (Greenberg
2015); we adapt that work to a latent calculus with dependency. We guarantee
space efficiency when no dependency is used; we cannot generally guarantee
space efficiency when dependency is used, but instead offer a framework for
making such programs space efficient on a case-by-case basis."
"We present Refined TypeScript (RSC), a lightweight refinement type system for
TypeScript, that enables static verification of higher-order, imperative
programs. We develop a formal core of RSC that delineates the interaction
between refinement types and mutability. Next, we extend the core to account
for the imperative and dynamic features of TypeScript. Finally, we evaluate RSC
on a set of real world benchmarks, including parts of the Octane benchmarks,
D3, Transducers, and the TypeScript compiler."
"Traditionally, each party in a (dyadic or multiparty) session implements
exactly one role specified in the type of the session. We refer to this kind of
session as an individual session (i-session). As a generalization of i-session,
a group session (g-session) is one in which each party may implement a group of
roles based on one channel. In particular, each of the two parties involved in
a dyadic g-session implements either a group of roles or its complement. In
this paper, we present a formalization of g-sessions in a multi-threaded
lambda-calculus (MTLC) equipped with a linear type system, establishing for the
MTLC both type preservation and global progress. As this formulated MTLC can be
readily embedded into ATS, a full-fledged language with a functional
programming core that supports both dependent types (of DML-style) and linear
types, we obtain a direct implementation of linearly typed g-sessions in ATS.
The primary contribution of the paper lies in both of the identification of
g-sessions as a fundamental building block for multiparty sessions and the
theoretical development in support of this identification."
"There are billions of lines of sequential code inside nowadays' software
which do not benefit from the parallelism available in modern multicore
architectures. Automatically parallelizing sequential code, to promote an
efficient use of the available parallelism, has been a research goal for some
time now. This work proposes a new approach for achieving such goal. We created
a new parallelizing compiler that analyses the read and write instructions, and
control-flow modifications in programs to identify a set of dependencies
between the instructions in the program. Afterwards, the compiler, based on the
generated dependencies graph, rewrites and organizes the program in a
task-oriented structure. Parallel tasks are composed by instructions that
cannot be executed in parallel. A work-stealing-based parallel runtime is
responsible for scheduling and managing the granularity of the generated tasks.
Furthermore, a compile-time granularity control mechanism also avoids creating
unnecessary data-structures. This work focuses on the Java language, but the
techniques are general enough to be applied to other programming languages. We
have evaluated our approach on 8 benchmark programs against OoOJava, achieving
higher speedups. In some cases, values were close to those of a manual
parallelization. The resulting parallel code also has the advantage of being
readable and easily configured to improve further its performance manually."
"Dynamic languages such as Ruby, Python, and JavaScript have many compelling
benefits, but the lack of static types means subtle errors can remain latent in
code for a long time. While many researchers have developed various systems to
bring some of the benefits of static types to dynamic languages, prior
approaches have trouble dealing with metaprogramming, which generates code as
the program executes. In this paper, we propose Hummingbird, a new system that
uses a novel technique, just-in-time static type checking, to type check Ruby
code even in the presence of metaprogramming. In Hummingbird, method type
signatures are gathered dynamically at run-time, as those methods are created.
When a method is called, Hummingbird statically type checks the method body
against current type signatures. Thus, Hummingbird provides thorough static
checks on a per-method basis, while also allowing arbitrarily complex
metaprogramming. For performance, Hummingbird memoizes the static type checking
pass, invalidating cached checks only if necessary. We formalize Hummingbird
using a core, Ruby-like language and prove it sound. To evaluate Hummingbird,
we applied it to six apps, including three that use Ruby on Rails, a powerful
framework that relies heavily on metaprogramming. We found that all apps
typecheck successfully using Hummingbird, and that Hummingbird's performance
overhead is reasonable. We applied Hummingbird to earlier versions of one Rails
app and found several type errors that had been introduced and then fixed.
Lastly, we demonstrate using Hummingbird in Rails development mode to typecheck
an app as live updates are applied to it."
"Current algorithms for context-free parsing inflict a trade-off between ease
of understanding, ease of implementation, theoretical complexity, and practical
performance. No algorithm achieves all of these properties simultaneously.
  Might et al. (2011) introduced parsing with derivatives, which handles
arbitrary context-free grammars while being both easy to understand and simple
to implement. Despite much initial enthusiasm and a multitude of independent
implementations, its worst-case complexity has never been proven to be better
than exponential. In fact, high-level arguments claiming it is fundamentally
exponential have been advanced and even accepted as part of the folklore.
Performance ended up being sluggish in practice, and this sluggishness was
taken as informal evidence of exponentiality.
  In this paper, we reexamine the performance of parsing with derivatives. We
have discovered that it is not exponential but, in fact, cubic. Moreover,
simple (though perhaps not obvious) modifications to the implementation by
Might et al. (2011) lead to an implementation that is not only easy to
understand but also highly performant in practice."
"Inference algorithms in probabilistic programming languages (PPLs) can be
thought of as interpreters, since an inference algorithm traverses a model
given evidence to answer a query. As with interpreters, we can improve the
efficiency of inference algorithms by compiling them once the model, evidence
and query are known. We present SIMPL, a domain specific language for inference
algorithms, which uses this idea in order to automatically specialize annotated
inference algorithms. Due to the approach of specialization, unlike a
traditional compiler, with SIMPL new inference algorithms can be added easily,
and still be optimized using domain-specific information. We evaluate SIMPL and
show that partial evaluation gives a 2-6x speedup, caching provides an
additional 1-1.5x speedup, and generating C code yields an additional 13-20x
speedup, for an overall speedup of 30-150x for several inference algorithms and
models."
"Secure compilation studies compilers that generate target-level components
that are as secure as their source-level counterparts. Full abstraction is the
most widely-proven property when defining a secure compiler. A compiler is
modular if it allows different components to be compiled independently and then
to be linked together to form a whole program. Unfortunately, many existing
fully-abstract compilers to untyped machine code are not modular. So, while
fully-abstractly compiled components are secure from malicious attackers, if
they are linked against each other the resulting component may become
vulnerable to attacks. This paper studies how to devise modular, fully-abstract
compilers. It first analyses the attacks arising when compiled programs are
linked together, identifying security threats that are due to linking. Then, it
defines a compiler from an object-based language with method calls and dynamic
memory allocation to untyped assembly language extended with a memory isolation
mechanism. The paper provides a proof sketch that the defined compiler is
fully-abstract and modular, so its output can be linked together without
introducing security violations."
"We consider the problem of reducing the memory required to run lazy
first-order functional programs. Our approach is to analyze programs for
liveness of heap-allocated data. The result of the analysis is used to preserve
only live data---a subset of reachable data---during garbage collection. The
result is an increase in the garbage reclaimed and a reduction in the peak
memory requirement of programs. While this technique has already been shown to
yield benefits for eager first-order languages, the lack of a statically
determinable execution order and the presence of closures pose new challenges
for lazy languages. These require changes both in the liveness analysis itself
and in the design of the garbage collector.
  To show the effectiveness of our method, we implemented a copying collector
that uses the results of the liveness analysis to preserve live objects, both
evaluated (i.e., in WHNF) and closures. Our experiments confirm that for
programs running with a liveness-based garbage collector, there is a
significant decrease in peak memory requirements. In addition, a sizable
reduction in the number of collections ensures that in spite of using a more
complex garbage collector, the execution times of programs running with
liveness and reachability-based collectors remain comparable."
"We present here in a thorough analysis of the Mool language, covering not
only its implementation but also the formalisation (syntax, operational
semantics, and type system). The objective is to detect glitches in both the
implementation and in the formal definitions, proposing as well new features
and added expressiveness. To test our proposals we implemented the revision
developed in the Racket platform."
"We consider nondeterministic probabilistic programs with the most basic
liveness property of termination. We present efficient methods for termination
analysis of nondeterministic probabilistic programs with polynomial guards and
assignments. Our approach is through synthesis of polynomial ranking
supermartingales, that on one hand significantly generalizes linear ranking
supermartingales and on the other hand is a counterpart of polynomial
ranking-functions for proving termination of nonprobabilistic programs. The
approach synthesizes polynomial ranking-supermartingales through
Positivstellensatz's, yielding an efficient method which is not only sound, but
also semi-complete over a large subclass of programs. We show experimental
results to demonstrate that our approach can handle several classical programs
with complex polynomial guards and assignments, and can synthesize efficient
quadratic ranking-supermartingales when a linear one does not exist even for
simple affine programs."
"We present PrivInfer, an expressive framework for writing and verifying
differentially private Bayesian machine learning algorithms. Programs in
PrivInfer are written in a rich functional probabilistic programming language
with constructs for performing Bayesian inference. Then, differential privacy
of programs is established using a relational refinement type system, in which
refinements on probability types are indexed by a metric on distributions. Our
framework leverages recent developments in Bayesian inference, probabilistic
programming languages, and in relational refinement types. We demonstrate the
expressiveness of PrivInfer by verifying privacy for several examples of
private Bayesian inference."
"In functional logic programs, rules are applicable independently of textual
order, i.e., any rule can potentially be used to evaluate an expression. This
is similar to logic languages and contrary to functional languages, e.g.,
Haskell enforces a strict sequential interpretation of rules. However, in some
situations it is convenient to express alternatives by means of compact default
rules. Although default rules are often used in functional programs, the
non-deterministic nature of functional logic programs does not allow to
directly transfer this concept from functional to functional logic languages in
a meaningful way. In this paper we propose a new concept of default rules for
Curry that supports a programming style similar to functional programming while
preserving the core properties of functional logic programming, i.e.,
completeness, non-determinism, and logic-oriented use of functions. We discuss
the basic concept and propose an implementation which exploits advanced
features of functional logic languages."
"Recent years have seen growing interest in the retrofitting of type systems
onto dynamically-typed programming languages, in order to improve type safety,
programmer productivity, or performance. In such cases, type system developers
must strike a delicate balance between disallowing certain coding patterns to
keep the type system simple, or including them at the expense of additional
complexity and effort. Thus far, the process for designing retrofitted type
systems has been largely ad hoc, because evaluating multiple variations of a
type system on large bodies of existing code is a significant undertaking.
  We present trace typing: a framework for automatically and quantitatively
evaluating variations of a retrofitted type system on large code bases. The
trace typing approach involves gathering traces of program executions,
inferring types for instances of variables and expressions occurring in a
trace, and merging types according to merge strategies that reflect specific
(combinations of) choices in the source-level type system design space.
  We evaluated trace typing through several experiments. We compared several
variations of type systems retrofitted onto JavaScript, measuring the number of
program locations with type errors in each case on a suite of over fifty
thousand lines of JavaScript code. We also used trace typing to validate and
guide the design of a new retrofitted type system that enforces fixed object
layout for JavaScript objects. Finally, we leveraged the types computed by
trace typing to automatically identify tag tests --- dynamic checks that refine
a type --- and examined the variety of tests identified."
"This article reports on steps towards building a simple and accurate
domain-theoretic model of generic nominally-typed OOP."
"Compilers are used to run programs that are written in a range of designs
from text to executable formats. With the advent of the internet, studies
related to the development of cloud based compilers are being carried out.
There is a considerable increase of on-line compilers enabling on-line
compilation of user programs without any mandate to. This study is specific to
on-line C compilers to investigate the correctness, issues and limitations."
"The syntax and semantics of user-supplied hypothesis names in tactic
languages is a thorny problem, because the binding structure of a proof is a
function of the goal at which a tactic script is executed. We contribute a new
language to deal with the dynamic and interactive character of names in tactic
scripts called Nominal LCF, and endow it with a denotational semantics in
dI-domains. A large fragment of Nominal LCF has already been implemented and
used to great effect in the new RedPRL proof assistant."
"Structural operational semantic specifications come in different styles:
small-step and big-step. A problem with the big-step style is that specifying
divergence and abrupt termination gives rise to annoying duplication. We
present a novel approach to representing divergence and abrupt termination in
big-step semantics using status flags. This avoids the duplication problem, and
uses fewer rules and premises for representing divergence than previous
approaches in the literature."
"Most modern applications interact with external services and access data in
structured formats such as XML, JSON and CSV. Static type systems do not
understand such formats, often making data access more cumbersome. Should we
give up and leave the messy world of external data to dynamic typing and
runtime checks? Of course, not!
  We present F# Data, a library that integrates external structured data into
F#. As most real-world data does not come with an explicit schema, we develop a
shape inference algorithm that infers a shape from representative sample
documents. We then integrate the inferred shape into the F# type system using
type providers. We formalize the process and prove a relative type soundness
theorem.
  Our library significantly reduces the amount of data access code and it
provides additional safety guarantees when contrasted with the widely used
weakly typed techniques."
"We present a new algorithm for computing upper bounds on the number of
executions of each program instruction during any single program run. The upper
bounds are expressed as functions of program input values. The algorithm is
primarily designed to produce bounds that are relatively tight, i.e. not
unnecessarily blown up. The upper bounds for instructions allow us to infer
loop bounds, i.e.~upper bounds on the number of loop iterations. Experimental
results show that the algorithm implemented in a prototype tool Looperman often
produces tighter bounds than current tools for loop bound analysis."
"In this paper we investigate the applicability of standard model checking
approaches to verifying properties in probabilistic programming. As the
operational model for a standard probabilistic program is a potentially
infinite parametric Markov decision process, no direct adaption of existing
techniques is possible. Therefore, we propose an on-the-fly approach where the
operational model is successively created and verified via a step-wise
execution of the program. This approach enables to take key features of many
probabilistic programs into account: nondeterminism and conditioning. We
discuss the restrictions and demonstrate the scalability on several benchmarks."
"This paper describes a reduction from the halting problem of Turing machines
to subtype checking in Java. It follows that subtype checking in Java is
undecidable, which answers a question posed by Kennedy and Pierce in 2007. It
also follows that Java's type checker can recognize any recursive language,
which improves a result of Gil and Levy from 2016. The latter point is
illustrated by a parser generator for fluent interfaces."
"We introduce a new programming language for expressing reversibility,
Energy-Efficient Language (Eel), geared toward algorithm design and
implementation. Eel is the first language to take advantage of a partially
reversible computation model, where programs can be composed of both reversible
and irreversible operations. In this model, irreversible operations cost energy
for every bit of information created or destroyed. To handle programs of
varying degrees of reversibility, Eel supports a log stack to automatically
trade energy costs for space costs, and introduces many powerful control logic
operators including protected conditional, general conditional, protected
loops, and general loops. In this paper, we present the design and compiler for
the three language levels of Eel along with an interpreter to simulate and
annotate incurred energy costs of a program."
"Polymorphic variants are a useful feature of the OCaml language whose current
definition and implementation rely on kinding constraints to simulate a
subtyping relation via unification. This yields an awkward formalization and
results in a type system whose behaviour is in some cases unintuitive and/or
unduly restrictive. In this work, we present an alternative formalization of
poly-morphic variants, based on set-theoretic types and subtyping, that yields
a cleaner and more streamlined system. Our formalization is more expressive
than the current one (it types more programs while preserving type safety), it
can internalize some meta-theoretic properties, and it removes some
pathological cases of the current implementation resulting in a more intuitive
and, thus, predictable type system. More generally, this work shows how to add
full-fledged union types to functional languages of the ML family that usually
rely on the Hindley-Milner type system. As an aside, our system also improves
the theory of semantic subtyping, notably by proving completeness for the type
reconstruction algorithm."
"In this work, we present a family of operational semantics that gradually
approximates the realistic program behaviors in the C/C++11 memory model. Each
semantics in our framework is built by elaborating and combining two simple
ingredients: viewfronts and operation buffers. Viewfronts allow us to express
the spatial aspect of thread interaction, i.e., which values a thread can read,
while operation buffers enable manipulation with the temporal execution aspect,
i.e., determining the order in which the results of certain operations can be
observed by concurrently running threads.
  Starting from a simple abstract state machine, through a series of gradual
refinements of the abstract state, we capture such language aspects and
synchronization primitives as release/acquire atomics, sequentially-consistent
and non-atomic memory accesses, also providing a semantics for relaxed atomics,
while avoiding the Out-of-Thin-Air problem. To the best of our knowledge, this
is the first formal and executable operational semantics of C11 capable of
expressing all essential concurrent aspects of the standard.
  We illustrate our approach via a number of characteristic examples, relating
the observed behaviors to those of standard litmus test programs from the
literature. We provide an executable implementation of the semantics in PLT
Redex, along with a number of implemented litmus tests and examples, and
showcase our prototype on a large case study: randomized testing and debugging
of a realistic Read-Copy-Update data structure."
"Multi-language virtual machines have a number of advantages. They allow
software developers to use software libraries that were written for different
programming languages. Furthermore, language implementors do not have to bother
with low-level VM functionality and their implementation can benefit from
optimizations in existing virtual machines. MagLev is an implementation of the
Ruby programming language on top of the GemStone/S virtual machine for the
Smalltalk programming language. In this work, we present how software
components written in both languages can interact. We show how MagLev unifies
the Smalltalk and the Ruby object model, taking into account Smalltalk meta
classes, Ruby singleton classes, and Ruby modules. Besides, we show how we can
call Ruby methods from Smalltalk and vice versa. We also present MagLev's
concept of bridge methods for implementing Ruby method calling conventions.
Finally, we compare our solution to other language implementations and virtual
machines."
"The statements `inheritance is not subtyping' and `mainstream OO languages
unnecessarily place restrictions over inheritance' have rippled as mantras
through the PL research community for years. Many mainstream OO developers and
OO language designers however do not accept these statements. In
\emph{nominally-typed} OO languages that these developers and language
designers are dearly familiar with, inheritance simply is subtyping; and they
believe OO type inheritance is an inherently nominal notion not a structural
one.
  Nominally-typed OO languages are among the most used programming languages
today. However, the value of nominal typing to mainstream OO developers, as a
means for designing robust OO software, seems to be in wait for full
appreciation among PL researchers--thereby perpetuating an unnecessary schism
between many OO developers and language designers and many OO PL researchers,
with each side discounting, if not even disregarding, the views of the other.
  In this essay we strengthen earlier efforts to demonstrate the semantic value
of nominal typing by presenting a technical comparison between nominal OO type
systems and structural OO type systems. Recently, a domain-theoretic model of
nominally-typed OOP was compared to well-known models of structurally-typed
OOP. Combined, these comparisons provide a clear and deep account for the
relation between nominal and structural OO type systems that has not been
presented before, and they help demonstrate the key value of nominal typing and
nominal subtyping to OO developers and language designers.
  We believe a clearer understanding of the key semantic advantage of pure
nominal OO typing over pure structural OO typing can help remedy the existing
schism. We believe future foundational OO PL research, to further its relevance
to mainstream OOP, should be based less on structural models of OOP and more on
nominal ones instead."
"Logic programming has traditiLogic programming has traditionally lacked
devices for expressing iterative tasks. To overcome this problem, this paper
proposes iterative goal formulas of the form $\seqandq{x}{L} G$ where $G$ is a
goal, $x$ is a variable, and $L$ is a list. $\seqandq{x}{L}$ is called a
parallel bounded quantifier. These goals allow us to specify the following
task: iterate $G$ with $x$ ranging over all the elements of $L$. onally lacked
devices for expressing iterative tasks. To overcome this problem, this paper
proposes iterative goal formulas of the form $\seqandq{x}{L} G$ where $G$ is a
goal, $x$ is a variable, and $L$ is a list. $\seqandq{x}{L}$ is called a
parallel bounded quantifier. These goals allow us to specify the following
task: iterate $G$ with $x$ ranging over all the elements of $L$."
"Context-Oriented Programming languages provide us with primitive constructs
to adapt program behaviour depending on the evolution of their operational
environment, namely the context. In previous work we proposed ML_CoDa, a
context-oriented language with two-components: a declarative constituent for
programming the context and a functional one for computing. This paper
describes an extension of ML_CoDa to deal with adaptation to unpredictable
context changes notified by asynchronous events."
"Syndicate is a new coordinated, concurrent programming language. It occupies
a novel point on the spectrum between the shared-everything paradigm of threads
and the shared-nothing approach of actors. Syndicate actors exchange messages
and share common knowledge via a carefully controlled database that clearly
scopes conversations. This approach clearly simplifies coordination of
concurrent activities. Experience in programming with Syndicate, however,
suggests a need to raise the level of linguistic abstraction. In addition to
writing event handlers and managing event subscriptions directly, the language
will have to support a reactive style of programming. This paper presents
event-oriented Syndicate programming and then describes a preliminary design
for augmenting it with new reactive programming constructs."
"Weak-head normalization is inconsistent with functional extensionality in the
call-by-name $\lambda$-calculus. We explore this problem from a new angle via
the conflict between extensionality and effects. Leveraging ideas from work on
the $\lambda$-calculus with control, we derive and justify alternative
operational semantics and a sequence of abstract machines for performing head
reduction. Head reduction avoids the problems with weak-head reduction and
extensionality, while our operational semantics and associated abstract
machines show us how to retain weak-head reduction's ease of implementation."
"Push/enter and eval/apply are two calling conventions used in implementations
of functional languages. In this paper, we explore the following observation:
when considering functions with multiple arguments, the stack under the
push/enter and eval/apply conventions behaves similarly to two particular
implementations of the list datatype: the regular cons-list and a form of lists
with lazy concatenation respectively. Along the lines of Danvy et al.'s
functional correspondence between definitional interpreters and abstract
machines, we use this observation to transform an abstract machine that
implements push/enter into an abstract machine that implements eval/apply. We
show that our method is flexible enough to transform the push/enter Spineless
Tagless G-machine (which is the semantic core of the GHC Haskell compiler) into
its eval/apply variant."
"It has been an open question as to whether the Modular Structural Operational
Semantics framework can express the dynamic semantics of call/cc. This paper
shows that it can, and furthermore, demonstrates that it can express the more
general delimited control operators control and shift."
"The performance of value classes is highly dependent on how they are
represented in the virtual machine. Value class instances are immutable, have
no identity, and can only refer to other value objects or primitive values and
since they should be very lightweight and fast, it is important to optimize
them carefully. In this paper we present a technique to detect and compress
common patterns of value class usage to improve memory usage and performance.
The technique identifies patterns of frequent value object references and
introduces abbreviated forms for them. This allows to store multiple
inter-referenced value objects in an inlined memory representation, reducing
the overhead stemming from meta-data and object references. Applied to a small
prototype and an implementation of the Racket language, we found improvements
in memory usage and execution time for several micro-benchmarks."
"Static type errors are a common stumbling block for newcomers to typed
functional languages. We present a dynamic approach to explaining type errors
by generating counterexample witness inputs that illustrate how an ill-typed
program goes wrong. First, given an ill-typed function, we symbolically execute
the body to dynamically synthesize witness values that can make the program go
wrong. We prove that our procedure synthesizes general witnesses in that if a
witness is found, then for all inhabited input types, there exist values that
can make the function go wrong. Second, we show how to extend the above
procedure to produce a reduction graph that can be used to interactively
visualize and debug witness executions. Third, we evaluate the coverage of our
approach on two data sets comprising over 4,500 ill-typed student programs. Our
technique is able to generate witnesses for 88% of the programs, and our
reduction graph yields small counterexamples for 81% of the witnesses. Finally,
we evaluate the utility of our witnesses in helping students understand and fix
type errors, and find that students presented with our witnesses consistently
show a greater understanding of type errors than those presented with a
standard error message."
"Interprocedural analysis by means of partial tabulation of summary functions
may not terminate when the same procedure is analyzed for infinitely many
abstract calling contexts or when the abstract domain has infinite strictly
ascending chains. As a remedy, we present a novel local solver for general
abstract equation systems, be they monotonic or not, and prove that this solver
fails to terminate only when infinitely many variables are encountered. We
clarify in which sense the computed results are sound. Moreover, we show that
interprocedural analysis performed by this novel local solver, is guaranteed to
terminate for all non-recursive programs --- irrespective of whether the
complete lattice is infinite or has infinite strictly ascending or descending
chains."
"Purl is a language to be used for modular definition and verification of
knitting patterns. The syntax is similar to the standard knitting pattern
notation provided by the Craft Yarn Council. Purl provides constructs not
available in the standard notation to allow reuse of segments of patterns. This
report describes the basics of knitting and hand-knitting patterns. A knitting
pattern language more terse than the standard notation is presented with the
implementation of a compiler to this standard."
"Jython is a Java-based Python implementation and the most seamless way to
integrate Python and Java. It achieves high efficiency by compiling Python code
to Java bytecode and thus letting Java's JIT optimize it - an approach that
enables Python code to call Java functions or to subclass Java classes. It
enables Python code to leverage Java's multithreading features and utilizes
Java's built-in garbage collection (GC). However, it currently does not support
CPython's C-API and thus does not support native extensions like NumPy and
SciPy. Since most scientific code depends on such extensions, it is not
runnable with Jython. Jython Native Interface (JyNI) is a compatibility layer
that aims to provide CPython's native C extension API on top of Jython. JyNI is
implemented using the Java Native Interface (JNI) and its native part is
designed to be binary compatible with existing extension builds [...]."
"Many Prolog programs are unnecessarily impure because of inadequate means to
express syntactic inequality. While the frequently provided built-in `dif/2` is
able to correctly describe expected answers, its direct use in programs often
leads to overly complex and inefficient definitions --- mainly due to the lack
of adequate indexing mechanisms. We propose to overcome these problems by using
a new predicate that subsumes both equality and inequality via reification.
Code complexity is reduced with a monotonic, higher-order if-then-else
construct based on `call/N`. For comparable correct uses of impure definitions,
our approach is as determinate and similarly efficient as its impure
counterparts."
"Most integrated development environments are shipped with refactoring tools.
However, their refactoring operations are often known to be unreliable. As a
consequence, developers have to test their code after applying an automatic
refactoring. In this article, we consider a refactoring operation (renaming of
global variables in C), and we prove that its core implementation preserves the
set of possible behaviors of transformed programs. That proof of correctness
relies on the operational semantics of C provided by CompCert C in Coq."
"Algorithmic skeletons are used as building-blocks to ease the task of
parallel programming by abstracting the details of parallel implementation from
the developer. Most existing libraries provide implementations of skeletons
that are defined over flat data types such as lists or arrays. However,
skeleton-based parallel programming is still very challenging as it requires
intricate analysis of the underlying algorithm and often uses inefficient
intermediate data structures. Further, the algorithmic structure of a given
program may not match those of list-based skeletons. In this paper, we present
a method to automatically transform any given program to one that is defined
over a list and is more likely to contain instances of list-based skeletons.
This facilitates the parallel execution of a transformed program using existing
implementations of list-based parallel skeletons. Further, by using an existing
transformation called distillation in conjunction with our method, we produce
transformed programs that contain fewer inefficient intermediate data
structures."
"One of the difficulties in developing collective adaptive systems is the
challenge of simultaneously engineering both the desired resilient behaviour of
the collective and the details of its implementation on individual devices.
Aggregate computing simplifies this problem by separating these aspects into
different layers of abstraction by means of a unifying notion of computational
field and a functional computational model. We review the state of the art in
aggregate computing, discuss the various resiliency properties it supports, and
develop a roadmap of foundational problems still needing to be addressed in the
continued development of this emerging discipline."
"In quantitative program analysis, values are assigned to execution traces to
represent a quality measure. Such analyses cover important applications, e.g.
resource usage. Examining all traces is well known to be intractable and
therefore traditional algorithms reason over an over-approximated set.
Typically, inaccuracy arises due to inclusion of infeasible paths in this set.
Thus path-sensitivity is one cure. However, there is another reason for the
inaccuracy: that the cost model, i.e., the way in which the analysis of each
trace is quantified, is dynamic. That is, the cost of a trace is dependent on
the context in which the trace is executed. Thus the goal of accurate analysis,
already challenged by path-sensitivity, is now further challenged by
context-sensitivity.
  In this paper, we address the problem of quantitative analysis defined over a
dynamic cost model. Our algorithm is an ""anytime"" algorithm: it generates an
answer quickly, but if the analysis resource budget allows, it progressively
produces better solutions via refinement iterations. The result of each
iteration remains sound, but importantly, must converge to an exact analysis
when given an unlimited resource budget. In order to be scalable, our algorithm
is designed to be incremental. We finally give evidence that a new level of
practicality is achieved by an evaluation on a realistic collection of
benchmarks."
"Typestate-oriented programming is an extension of the OO paradigm in which
objects are modeled not just in terms of interfaces but also in terms of their
usage protocols, describing legal sequences of method calls, possibly depending
on the object's internal state. We argue that the Actor Model allows
typestate-OOP in an inherently distributed setting, whereby objects/actors can
be accessed concurrently by several processes, and local entities cooperate to
carry out a communication protocol. In this article we illustrate the approach
by means of a number of examples written in Scala Akka. We show that Scala's
abstractions support clean and natural typestate-oriented actor programming
with the usual asynchronous and non-blocking semantics. We also show that the
standard type system of Scala and a typed wrapping of usual (untyped) Akka's
ActorRef are enough to provide rich forms of type safety so that well-typed
actors respect their intended communication protocols. This approach draws on a
solid theoretical background, consisting of a sound behavioral type system for
the Join Calculus, that is a foundational calculus of distributed asynchronous
processes whose semantics is based on the Chemical Abstract Machine, that
unveiled its strong connections with typestate-oriented programming of both
concurrent objects and actors."
"We present Lifty, a language that uses type-driven program repair to enforce
information flow policies. In Lifty, the programmer specifies a policy by
annotating the source of sensitive data with a refinement type, and the system
automatically inserts access checks necessary to enforce this policy across the
code. This is a significant improvement over current practice, where
programmers manually implement access checks, and any missing check can cause
an information leak. To support this programming model, we have developed (1)
an encoding of information flow security in terms of decidable refinement types
that enables fully automatic verification and (2) a program repair algorithm
that localizes unsafe accesses to sensitive data and replaces them with
provably secure alternatives. We formalize the encoding and prove its
noninterference guarantee. Our experience using Lifty to implement a conference
management system shows that it decreases policy burden and is able to
efficiently synthesize all necessary access checks, including those required to
prevent a set of reported real-world information leaks."
"Structure editors allow programmers to edit the tree structure of a program
directly. This can have cognitive benefits, particularly for novice and
end-user programmers (as evidenced by the popularity of structure editors like
Scratch.) It also simplifies matters for tool designers, because they do not
need to contend with malformed program text.
  This paper defines Hazelnut, a structure editor based on a small
bidirectionally typed lambda calculus extended with holes and a cursor (a la
Huet's zipper.) Hazelnut goes one step beyond syntactic well-formedness: it's
edit actions operate over statically meaningful (i.e. well-typed) terms.
Naively, this prohibition on ill-typed edit states would force the programmer
to construct terms in a rigid ""outside-in"" manner. To avoid this problem, the
action semantics automatically places terms assigned a type that is
inconsistent with the expected type inside a hole. This safely defers the type
consistency check until the term inside the hole is finished.
  Hazelnut is a foundational type-theoretic account of typed structure editing,
rather than an end-user tool itself. To that end, we describe how Hazelnut's
rich metatheory, which we have mechanized in Agda, guides the definition of an
extension to the calculus. We also discuss various plausible evaluation
strategies for terms with holes, and in so doing reveal connections with
gradual typing and contextual modal type theory (the Curry-Howard
interpretation of contextual modal logic.) Finally, we discuss how Hazelnut's
semantics lends itself to implementation as a functional reactive program. Our
reference implementation is written using js_of_ocaml."
"Data oriented applications, usually written in a high-level, general-purpose
programming language (such as Java) interact with database through a coarse
interface. Informally, the text of a query is built on the application side
(either via plain string concatenation or through an abstract notion of
statement) and shipped to the database over the wire where it is executed. The
results are then serialized and sent back to the ""client-code"" where they are
translated in the language's native datatypes. This round trip is detrimental
to performances but, worse, such a programming model prevents one from having
richer queries, namely queries containing user-defined functions (that is
functions defined by the programmer and used e.g. in the filter condition of a
SQL query). While some databases also possess a ""server-side"" language (e.g.
PL/SQL in Oracle database), its integration with the very-optimized query
execution engine is still minimal and queries containing (PL/SQL) user-defined
functions remain notoriously inefficient. In this setting, we reviewed existing
language-integrated query frameworks, highlighting that existing database query
languages (including SQL) share high-level querying primitives (e.g.,
filtering, joins, aggregation) that can be represented by operators, but differ
widely regarding the semantics of their expression language. In order to
represent queries in an application language- and database-agnostic manner, we
designed a small calculus, dubbed ""QIR"" for Query Intermediate Representation.
QIR contains expressions, corresponding to a small extension of the pure
lambda-calculus, and operators to represent usual querying primitives. In the
effort to send efficient queries to the database, we abstracted the idea of
""good"" query representations in a measure on QIR terms. Then, we designed an
evaluation strategy rewriting QIR query representations into ""better"" ones."
"A bit layout is a sequence of fields of certain bit lengths that specifies
how to interpret a serial stream, e.g., the MP3 audio format. A layout with
variable length fields needs to include meta-information to help the parser
interpret unambiguously the rest of the stream; e.g. a field providing the
length of a following variable length field. If no such information is
available, then the layout is ambiguous. I present a linear-time algorithm to
determine whether a layout is ambiguous or not by modelling the behaviour of a
serial parser reading the stream as forward chaining reasoning on a collection
of Horn clauses."
"Property-based random testing a la QuickCheck requires building efficient
generators for well-distributed random data satisfying complex logical
predicates, but writing these generators can be difficult and error prone. We
propose a domain-specific language in which generators are conveniently
expressed by decorating predicates with lightweight annotations to control both
the distribution of generated values and the amount of constraint solving that
happens before each variable is instantiated. This language, called Luck, makes
generators easier to write, read, and maintain.
  We give Luck a formal semantics and prove several fundamental properties,
including the soundness and completeness of random generation with respect to a
standard predicate semantics. We evaluate Luck on common examples from the
property-based testing literature and on two significant case studies, showing
that it can be used in complex domains with comparable bug-finding
effectiveness and a significant reduction in testing code size compared to
handwritten generators."
"Aliasing is a known source of challenges in the context of imperative
object-oriented languages, which have led to important advances in type systems
for aliasing control. However, their large-scale adoption has turned out to be
a surprisingly difficult challenge. While new language designs show promise,
they do not address the need of aliasing control in existing languages.
  This paper presents a new approach to isolation and uniqueness in an
existing, widely-used language, Scala. The approach is unique in the way it
addresses some of the most important obstacles to the adoption of type system
extensions for aliasing control. First, adaptation of existing code requires
only a minimal set of annotations. Only a single bit of information is required
per class. Surprisingly, the paper shows that this information can be provided
by the object-capability discipline, widely-used in program security. We
formalize our approach as a type system and prove key soundness theorems. The
type system is implemented for the full Scala language, providing, for the
first time, a sound integration with Scala's local type inference. Finally, we
empirically evaluate the conformity of existing Scala open-source code on a
corpus of over 75,000 LOC."
"The IrGL intermediate representation is an explicitly parallel representation
for irregular programs that targets GPUs. In this report, we describe IrGL
constructs, examples of their use and how IrGL is compiled to CUDA by the
Galois GPU compiler."
"ProbNetKAT is a probabilistic extension of NetKAT with a denotational
semantics based on Markov kernels. The language is expressive enough to
generate continuous distributions, which raises the question of how to compute
effectively in the language. This paper gives an new characterization of
ProbNetKAT's semantics using domain theory, which provides the foundation
needed to build a practical implementation. We show how to use the semantics to
approximate the behavior of arbitrary ProbNetKAT programs using distributions
with finite support. We develop a prototype implementation and show how to use
it to solve a variety of problems including characterizing the expected
congestion induced by different routing schemes and reasoning probabilistically
about reachability in a network."
"We present a static deadlock analysis approach for C/pthreads. The design of
our method has been guided by the requirement to analyse real-world code. Our
approach is sound (i.e., misses no deadlocks) for programs that have defined
behaviour according to the C standard, and precise enough to prove
deadlock-freedom for a large number of programs. The method consists of a
pipeline of several analyses that build on a new context- and thread-sensitive
abstract interpretation framework. We further present a lightweight dependency
analysis to identify statements relevant to deadlock analysis and thus speed up
the overall analysis. In our experimental evaluation, we succeeded to prove
deadlock-freedom for 262 programs from the Debian GNU/Linux distribution with
in total 2.6 MLOC in less than 11 hours."
"This paper presents a software-based technique to recover control-flow errors
in multithreaded programs. Control-flow error recovery is achieved through
inserting additional instructions into multithreaded program at compile time
regarding to two dependency graphs. These graphs are extracted to model
control-flow and data dependencies among basic blocks and thread interactions
between different threads of a program. In order to evaluate the proposed
technique, three multithreaded benchmarks quick sort, matrix multiplication and
linked list utilized to run on a multi-core processor, and a total of 5000
transient faults has been injected into several executable points of each
program. The results show that this technique detects and corrects between
91.9% and 93.8% of the injected faults with acceptable performance and memory
overheads."
"A fundamental challenge in synthesis from examples is designing a learning
algorithm that poses the minimal number of questions to an end user while
guaranteeing that the target hypothesis is discovered. Such guarantees are
practically important because they ensure that end users will not be
overburdened with unnecessary questions.
  We present SPEX -- a learning algorithm that addresses the above challenge.
SPEX considers the hypothesis space of formulas over first-order predicates and
learns the correct hypothesis by only asking the user simple membership queries
for concrete examples. Thus, SPEX is directly applicable to any learning
problem that fits its hypothesis space and uses membership queries.
  SPEX works by iteratively eliminating candidate hypotheses from the space
until converging to the target hypothesis. The main idea is to use the
implication order between hypotheses to guarantee that in each step the
question presented to the user obtains maximal pruning of the space. This
problem is particularly challenging when predicates are potentially correlated.
  To show that SPEX is practically useful, we expressed two rather different
applications domains in its framework: learning programs for the domain of
technical analysts (stock trading) and learning data structure specifications.
The experimental results show that SPEX's optimality guarantee is effective: it
drastically reduces the number of questions posed to the user while
successfully learning the exact hypothesis."
"Tabling is a powerful resolution mechanism for logic programs that captures
their least fixed point semantics more faithfully than plain Prolog. In many
tabling applications, we are not interested in the set of all answers to a
goal, but only require an aggregation of those answers. Several works have
studied efficient techniques, such as lattice-based answer subsumption and
mode-directed tabling, to do so for various forms of aggregation.
  While much attention has been paid to expressivity and efficient
implementation of the different approaches, soundness has not been considered.
This paper shows that the different implementations indeed fail to produce
least fixed points for some programs. As a remedy, we provide a formal
framework that generalises the existing approaches and we establish a soundness
criterion that explains for which programs the approach is sound.
  This article is under consideration for acceptance in TPLP."
"Recent work has provided delimited control for Prolog to dynamically
manipulate the program control-flow, and to implement a wide range of
control-flow and dataflow effects on top of. Unfortunately, delimited control
is a rather primitive language feature that is not easy to use.
  As a remedy, this work introduces algebraic effect handlers for Prolog, as a
high-level and structured way of defining new side-effects in a modular
fashion. We illustrate the expressive power of the feature and provide an
implementation by means of elaboration into the delimited control primitives.
  The latter add a non-negligible performance overhead when used extensively.
To address this issue, we present an optimised compilation approach that
combines partial evaluation with dedicated rewrite rules. The rewrite rules are
driven by a lightweight effect inference that analyses what effect operations
may be called by a goal. We illustrate the effectiveness of this approach on a
range of benchmarks. This article is under consideration for acceptance in
TPLP."
"The runtime system of dynamic languages such as Prolog or Lisp and their
derivatives contain a symbol table, in Prolog often called the atom table. A
simple dynamically resizing hash-table used to be an adequate way to implement
this table. As Prolog becomes fashionable for 24x7 server processes we need to
deal with atom garbage collection and concurrent access to the atom table.
Classical lock-based implementations to ensure consistency of the atom table
scale poorly and a stop-the-world approach to implement atom garbage collection
quickly becomes a bottle-neck, making Prolog unsuitable for soft real-time
applications. In this article we describe a novel implementation for the atom
table using lock-free techniques where the atom-table remains accessible even
during atom garbage collection. Relying only on CAS (Compare And Swap) and not
on external libraries, the implementation is straightforward and portable.
  Under consideration for acceptance in TPLP."
"The aim of a probabilistic resource analysis is to derive a probability
distribution of possible resource usage for a program from a probability
distribution of its input. We present an automated multi- phase rewriting based
method to analyze programs written in a subset of C. It generates a probability
distribution of the resource usage as a possibly uncomputable expression and
then transforms it into a closed form expression using over-approximations. We
present the technique, outline the implementation and show results from
experiments with the system."
"We present an algorithm for synthesizing a context-free grammar encoding the
language of valid program inputs from a set of input examples and blackbox
access to the program. Our algorithm addresses shortcomings of existing grammar
inference algorithms, which both severely overgeneralize and are prohibitively
slow. Our implementation, GLADE, leverages the grammar synthesized by our
algorithm to fuzz test programs with structured inputs. We show that GLADE
substantially increases the incremental coverage on valid inputs compared to
two baseline fuzzers."
"Programmers currently enjoy access to a very high number of code repositories
and libraries of ever increasing size. The ensuing potential for reuse is
however hampered by the fact that searching within all this code becomes an
increasingly difficult task. Most code search engines are based on syntactic
techniques such as signature matching or keyword extraction. However, these
techniques are inaccurate (because they basically rely on documentation) and at
the same time do not offer very expressive code query languages. We propose a
novel approach that focuses on querying for semantic characteristics of code
obtained automatically from the code itself. Program units are pre-processed
using static analysis techniques, based on abstract interpretation, obtaining
safe semantic approximations. A novel, assertion-based code query language is
used to express desired semantic characteristics of the code as partial
specifications. Relevant code is found by comparing such partial specifications
with the inferred semantics for program elements. Our approach is fully
automatic and does not rely on user annotations or documentation. It is more
powerful and flexible than signature matching because it is parametric on the
abstract domain and properties, and does not require type definitions. Also, it
reasons with relations between properties, such as implication and abstraction,
rather than just equality. It is also more resilient to syntactic code
differences. We describe the approach and report on a prototype implementation
within the Ciao system.
  Under consideration for acceptance in TPLP."
"We implement a library for encoding constructor classes in Standard ML,
including elaboration from minimal definitions, and automatic instantiation of
superclasses."
"Partial evaluation (PE) is a powerful and general program optimization
technique with many successful applications. However, it has never been
investigated in the context of expressive rule-based languages like Maude,
CafeOBJ, OBJ, ASF+SDF, and ELAN, which support: 1) rich type structures with
sorts, subsorts and overloading; 2) equational rewriting modulo axioms such as
commutativity, associativity-commutativity, and
associativity-commutativity-identity. In this extended abstract, we illustrate
the key concepts by showing how they apply to partial evaluation of expressive
rule-based programs written in Maude. Our partial evaluation scheme is based on
an automatic unfolding algorithm that computes term variants and relies on
equational least general generalization for ensuring global termination. We
demonstrate the use of the resulting partial evaluator for program optimization
on several examples where it shows significant speed-ups."
"Strings are extensively used in modern programming languages and constraints
over strings of unknown length occur in a wide range of real-world applications
such as software analysis and verification, testing, model checking, and web
security. Nevertheless, practically no CP solver natively supports string
constraints. We introduce string variables and a suitable set of string
constraints as builtin features of the MiniZinc modelling language.
Furthermore, we define an interpreter for converting a MiniZinc model with
strings into a FlatZinc instance relying on only integer variables. This
provides a user-friendly interface for modelling combinatorial problems with
strings, and enables both string and non-string solvers to actually solve such
problems."
"We introduce a new native code compiler for Curry codenamed Sprite. Sprite is
based on the Fair Scheme, a compilation strategy that provides instructions for
transforming declarative, non-deterministic programs of a certain class into
imperative, deterministic code. We outline salient features of Sprite, discuss
its implementation of Curry programs, and present benchmarking results. Sprite
is the first-to-date operationally complete implementation of Curry.
Preliminary results show that ensuring this property does not incur a
significant penalty."
"Automated analysis of recursive derivations in logic programming is known to
be a hard problem. Both termination and non-termination are undecidable
problems in Turing-complete languages. However, some declarative languages
offer a practical work-around for this problem, by making a clear distinction
between whether a program is meant to be understood inductively or
coinductively. For programs meant to be understood inductively, termination
must be guaranteed, whereas for programs meant to be understood coinductively,
productive non-termination (or ""productivity"") must be ensured. In practice,
such classification helps to better understand and implement some
non-terminating computations.
  Logic programming was one of the first declarative languages to make this
distinction: in the 1980's, Lloyd and van Emden's ""computations at infinity""
captured the big-step operational semantics of derivations that produce
infinite terms as answers. In modern terms, computations at infinity describe
""global productivity"" of computations in logic programming. Most programming
languages featuring coinduction also provide an observational, or small-step,
notion of productivity as a computational counterpart to global productivity.
This kind of productivity is ensured by checking that finite initial fragments
of infinite computations can always be observed to produce finite portions of
their infinite answer terms.
  In this paper we introduce a notion of observational productivity for logic
programming as an algorithmic approximation of global productivity, give an
effective procedure for semi-deciding observational productivity, and offer an
implemented automated observational productivity checker for logic programs."
"Constraint automata (CA) constitute a coordination model based on finite
automata on infinite words. Originally introduced for modeling of coordinators,
an interesting new application of CAs is implementing coordinators (i.e.,
compiling CAs into executable code). Such an approach guarantees
correctness-by-construction and can even yield code that outperforms
hand-crafted code. The extent to which these two potential advantages
materialize depends on the smartness of CA-compilers and the existence of
proofs of their correctness.
  Every transition in a CA is labeled by a ""data constraint"" that specifies an
atomic data-flow between coordinated processes as a first-order formula. At
run-time, compiler-generated code must handle data constraints as efficiently
as possible. In this paper, we present, and prove the correctness of two
optimization techniques for CA-compilers related to handling of data
constraints: a reduction to eliminate redundant variables and a translation
from (declarative) data constraints to (imperative) data commands expressed in
a small sequential language. Through experiments, we show that these
optimization techniques can have a positive impact on performance of generated
executable code."
"Puppet is a popular declarative framework for specifying and managing complex
system configurations. The Puppet framework includes a domain-specific language
with several advanced features inspired by object-oriented programming,
including user-defined resource types, 'classes' with a form of inheritance,
and dependency management. Like most real-world languages, the language has
evolved in an ad hoc fashion, resulting in a design with numerous features,
some of which are complex, hard to understand, and difficult to use correctly.
  We present an operational semantics for $\mu$Puppet, a representative subset
of the Puppet language that covers the distinctive features of Puppet, while
excluding features that are either deprecated or work-in-progress. Formalising
the semantics sheds light on difficult parts of the language, identifies
opportunities for future improvements, and provides a foundation for future
analysis or debugging techniques, such as static typechecking or provenance
tracking. Our semantics leads straightforwardly to a reference implementation
in Haskell. We also discuss some of Puppet's idiosyncrasies, particularly its
handling of classes and scope, and present an initial corpus of test cases
supported by our formal semantics."
"Horn clauses and first-order resolution are commonly used to implement type
classes in Haskell. Several corecursive extensions to type class resolution
have recently been proposed, with the goal of allowing (co)recursive dictionary
construction where resolution does not termi- nate. This paper shows, for the
first time, that corecursive type class resolution and its extensions are
coinductively sound with respect to the greatest Herbrand models of logic
programs and that they are induc- tively unsound with respect to the least
Herbrand models. We establish incompleteness results for various fragments of
the proof system."
"Anglican is a probabilistic programming system designed to interoperate with
Clojure and other JVM languages. We introduce the programming language
Anglican, outline our design choices, and discuss in depth the implementation
of the Anglican language and runtime, including macro-based compilation,
extended CPS-based evaluation model, and functional representations for
probabilistic paradigms, such as a distribution, a random process, and an
inference algorithm.
  We show that a probabilistic functional language can be implemented
efficiently and integrated tightly with a conventional functional language with
only moderate computational overhead. We also demonstrate how advanced
probabilistic modeling concepts are mapped naturally to the functional
foundation."
"Today's programmers face a false choice between creating software that is
extensible and software that is correct. Specifically, dynamic languages permit
software that is richly extensible (via dynamic code loading, dynamic object
extension, and various forms of reflection), and today's programmers exploit
this flexibility to ""bring their own language features"" to enrich extensible
languages (e.g., by using common JavaScript libraries). Meanwhile, such
library-based language extensions generally lack enforcement of their
abstractions, leading to programming errors that are complex to avoid and
predict.
  To offer verification for this extensible world, we propose online
verification-validation (OVV), which consists of language and VM design that
enables a ""phaseless"" approach to program analysis, in contrast to the standard
static-dynamic phase distinction. Phaseless analysis freely interposes abstract
interpretation with concrete execution, allowing analyses to use dynamic
(concrete) information to prove universal (abstract) properties about future
execution.
  In this paper, we present a conceptual overview of OVV through a motivating
example program that uses a hypothetical database library. We present a generic
semantics for OVV, and an extension to this semantics that offers a simple
gradual type system for the database library primitives. The result of
instantiating this gradual type system in an OVV setting is a checker that can
progressively type successive continuations of the program until a continuation
is fully verified. To evaluate the proposed vision of OVV for this example, we
implement the VM semantics (in Rust), and show that this design permits
progressive typing in this manner."
"Dijkstra monads enable a dependent type theory to be enhanced with support
for specifying and verifying effectful code via weakest preconditions. Together
with their closely related counterparts, Hoare monads, they provide the basis
on which verification tools like F*, Hoare Type Theory (HTT), and Ynot are
built.
  We show that Dijkstra monads can be derived ""for free"" by applying a
continuation-passing style (CPS) translation to the standard monadic
definitions of the underlying computational effects. Automatically deriving
Dijkstra monads in this way provides a correct-by-construction and efficient
way of reasoning about user-defined effects in dependent type theories.
  We demonstrate these ideas in EMF*, a new dependently typed calculus,
validating it via both formal proof and a prototype implementation within F*.
Besides equipping F* with a more uniform and extensible effect system, EMF*
enables a novel mixture of intrinsic and extrinsic proofs within F*."
"We provide the syntax and semantics of the LISA (for ""Litmus Instruction Set
Architecture"") language. The parallel assembly language LISA is implemented in
the herd7 tool (http://virginia.cs.ucl.ac.uk/herd/) for simulating weak
consistency models."
"Writing functions in R is an important skill for anyone using R. S3 methods
allow for functions to be generalised across different classes and are easy to
implement. Whilst many R users are be adept at creating their own functions, it
seems that there is room for many more to take advantage of R's S3 methods.
This paper provides a simple and targeted guide to explain what S3 methods are,
why people should them, and how they can do it."
"In this extended abstract we present our current work on leveraging Standard
ML for developing embedded and real-time systems. Specifically we detail our
experiences in modifying MLton, a whole program, optimizing compiler for
Standard ML, for use in such contexts. We focus primarily on the language
runtime, re-working the threading subsystem and garbage collector, as well as
necessary changes for integrating MLton generated programs into a light weight
operating system kernel. We compare and contrast these changes to our previous
work on extending MLton for multicore systems, which focused around acheiving
scalability."
"We present a type system and inference algorithm for a rich subset of
JavaScript equipped with objects, structural subtyping, prototype inheritance,
and first-class methods. The type system supports abstract and recursive
objects, and is expressive enough to accommodate several standard benchmarks
with only minor workarounds. The invariants enforced by the types enable an
ahead-of-time compiler to carry out optimizations typically beyond the reach of
static compilers for dynamic languages. Unlike previous inference techniques
for prototype inheritance, our algorithm uses a combination of lower and upper
bound propagation to infer types and discover type errors in all code,
including uninvoked functions. The inference is expressed in a simple
constraint language, designed to leverage off-the-shelf fixed point solvers. We
prove soundness for both the type system and inference algorithm. An
experimental evaluation showed that the inference is powerful, handling the
aforementioned benchmarks with no manual type annotation, and that the inferred
types enable effective static compilation."
"We provide the syntax and semantics of the cat language, a domain specific
language to describe consistency properties of parallel/distributed programs.
The language is implemented in the herd7 too
(http://diy.inria.fr/doc/herd.html)l."
"Command-line tools are confusing and hard to use for novice programmers due
to their cryptic error messages and lack of documentation. Novice users often
resort to online help-forums for finding corrections to their buggy commands,
but have a hard time in searching precisely for posts that are relevant to
their problem and then applying the suggested solutions to their buggy command.
  We present a tool, NoFAQ, that uses a set of rules to suggest possible fixes
when users write buggy commands that trigger commonly occurring errors. The
rules are expressed in a language called FixIt and each rule pattern-matches
against the user's buggy command and the corresponding error message, and uses
these inputs to produce a possible fixed command. Our main contribution is an
algorithm based on lazy VSA for synthesizing FixIt rules from examples of buggy
and repaired commands. The algorithm allows users to add new rules in NoFAQ
without having to manually encode them. We present the evaluation of NoFAQ on
92 benchmark problems and show that NoFAQ is able to instantly synthesize rules
for 81 benchmark problems in real time using just 2 to 5 input-output examples
for each rule."
"We consider the problem of static deadlock detection for programs in the Go
programming language which make use of synchronous channel communications. In
our analysis, regular expressions extended with a fork operator capture the
communication behavior of a program. Starting from a simple criterion that
characterizes traces of deadlock-free programs, we develop automata-based
methods to check for deadlock-freedom. The approach is implemented and
evaluated with a series of examples."
"We sketch a simple language of concurrent objects which explores the design
space between type systems and continuous testing. In our language, programs
are collections of communicating automata checked automatically for multiparty
compatibility. This property, taken from the session types literature but here
applied to terms rather than types, guarantees that no state-related errors
arise during execution: no object gets stuck because it was sent the wrong
message, and every message is processed."
"We introduce HONEY; a new specialized programming language designed to
facilitate the processing of multivariate, asynchronous and non-uniformly
sampled symbolic and scalar time sequences. When compiled, a Honey program is
transformed into a static process flow diagram, which is then executed by a
virtual machine. Honey's most notable features are: (1) Honey introduces a new,
efficient and non-prone to error paradigm for defining recursive process flow
diagrams from text input with the mindset of imperative programming. Honey's
specialized, high level and concise syntax allows fast and easy writing,
reading and maintenance of complex processing of large scalar symbolic time
sequence datasets. (2) Honey guarantees programs will be executed similarly on
static or real-time streaming datasets. (3) Honey's IDE includes an interactive
visualization tool which allows for an interactive exploration of the
intermediate and final outputs. This combination enables fast incremental
prototyping, debugging, monitoring and maintenance of complex programs. (4) In
case of large datasets (larger than the available memory), Honey programs can
be executed to process input greedily. (5) The graphical structure of a
compiled program provides several desirable properties, including distributed
and/or paralleled execution, memory optimization, and program structure
visualization. (6) Honey contains a large library of both common and novel
operators developed through various research projects. An open source C++
implementation of Honey as well as the Honey IDE and the interactive data
visualizer are publicly available."
"We explore the use of liveness for interactive program verification for a
simple concurrent object language. Our experimental IDE integrates two
(formally dual) kinds of continuous testing into the development environment:
compatibility-checking, which verifies an object's use of other objects, and
compliance-checking, which verifies an object's claim to refine the behaviour
of another object. Source code errors highlighted by the IDE are not static
type errors but the reflection back to the source of runtime errors that occur
in some execution of the system. We demonstrate our approach, and discuss
opportunities and challenges."
"We describe a complete Scheme implementation of miniAdapton, which implements
the core functionality of the Adapton system for incremental computation (also
known as self-adjusting computation). Like Adapton, miniAdapton allows
programmers to safely combine mutation and memoization. miniAdapton is built on
top of an even simpler system, microAdapton. Both miniAdapton and microAdapton
are designed to be easy to understand, extend, and port to host languages other
than Scheme. We also present adapton variables, a new interface in Adapton for
variables intended to represent expressions."
"Historically, true context-sensitive parsing has seldom been applied to
programming languages, due to its inherent complexity. However, many mainstream
programming and markup languages (C, Haskell, Python, XML, and more) possess
context-sensitive features. These features are traditionally handled with
ad-hoc code (e.g., custom lexers), outside of the scope of parsing theory.
  Current grammar formalisms struggle to express context-sensitive features.
Most solutions lack context transparency: they make grammars hard to write,
maintain and compose by hardwiring context through the entire grammar. Instead,
we approach context-sensitive parsing through the idea that parsers may recall
previously matched input (or data derived therefrom) in order to make parsing
decisions. We make use of mutable parse state to enable this form of recall.
  We introduce principled stateful parsing as a new transactional discipline
that makes state changes transparent to parsing mechanisms such as backtracking
and memoization. To enforce this discipline, users specify parsers using
formally specified primitive state manipulation operations.
  Our solution is available as a parsing library named Autumn. We illustrate
our solution by implementing some practical context-sensitive grammar features
such as significant whitespace handling and namespace classification."
"Actor coordination armoured with a suitable protocol description language has
been a pressing problem in the actors community. We study the applicability of
multiparty session type (MPST) protocols for verification of actor programs. We
incorporate sessions to actors by introducing minimum additions to the model
such as the notion of actor roles and protocol mailboxes. The framework uses
Scribble, which is a protocol description language based on multiparty session
types. Our programming model supports actor-like syntax and runtime
verification mechanism guaranteeing communication safety of the participating
entities. An actor can implement multiple roles in a similar way as an object
can implement multiple interfaces. Multiple roles allow for cooperative
inter-concurrency in a single actor. We demonstrate our framework by designing
and implementing a session actor library in Python and its runtime verification
mechanism. Benchmark results demonstrate that the runtime checks induce
negligible overhead. We evaluate the applicability of our verification
framework to specify actor interactions by implementing twelve examples from an
actor benchmark suit."
"In this paper we describe how to leverage higher-order unification to type
check a dependently typed language with meta-variables. The literature usually
presents the unification algorithm as a standalone component, however the need
to check definitional equality of terms while type checking gives rise to a
tight interplay between type checking and unification. This interplay is a
major source of complexity in the type-checking algorithm for existing
dependently typed programming languages. We propose an algorithm that encodes a
type-checking problem entirely in the form of unification constraints, reducing
the complexity of the type-checking code by taking advantage of higher order
unification, which is already part of the implementation of many dependently
typed languages."
"Jolie is a programming language that follows the microservices paradigm. As
an open source project, it has built a community of developers worldwide - both
in the industry as well as in academia - taken care of the development,
continuously improved its usability, and therefore broadened the adoption. In
this paper, we present some of the most recent results and work in progress
that has been made within our research team."
"State-of-the-art programming language techniques for incremental computation
employ programmer-specified names, whose role is critical for the asymptotic
efficiency of many online algorithms. These names identify ""cache locations""
for dynamic data and sub-computations---the program's dynamic dependencies. To
work well, or at all, these names must be precise, meaning that for an
evaluation derivation in question, each name identifies at most one value or
subcomputation; we call all other names imprecise, or ambiguous.
  The precise name problem consists of statically verifying that a program
allocates names precisely, for all inputs. Past theoretical work ignores this
problem by not permitting programs to use names directly, and past
implementations, which necessarily permit explicit names, employ ad hoc
workarounds for imprecise incremental programs.
  In this work, we give the first static verification approach to the precise
naming problem. Specifically, we define a refinement type system that gives
name term and index term sub-languages for approximating programmer-specified
names of dynamic data and sub-computations. We prove that our type system
enforces that these names are precise. We demonstrate the practical value of
our proposed type system by verifying examples of incremental sequences and
sets from a recent collections library, including both library client and
implementation code.
  Drawing closer to an implementation of our type system, we derive a
bidirectional version, and prove that it corresponds to our declarative type
system. A key challenge in implementing the bidirectional system is handling
constraints over names, name terms and name sets; toward this goal, we give
decidable, syntactic rules to guide these checks."
"Dataflow matrix machines are self-referential generalized recurrent neural
nets. The self-referential mechanism is provided via a stream of matrices
defining the connectivity and weights of the network in question. A natural
question is: what should play the role of untyped lambda-calculus for this
programming architecture? The proposed answer is a discipline of programming
with only one kind of streams, namely the streams of appropriately shaped
matrices. This yields Pure Dataflow Matrix Machines which are networks of
transformers of streams of matrices capable of defining a pure dataflow matrix
machine."
"The ability to record and replay program executions with low overhead enables
many applications, such as reverse-execution debugging, debugging of
hard-to-reproduce test failures, and ""black box"" forensic analysis of failures
in deployed systems. Existing record-and-replay approaches rely on recording an
entire virtual machine (which is heavyweight), modifying the OS kernel (which
adds deployment and maintenance costs), or pervasive code instrumentation
(which imposes significant performance and complexity overhead). We
investigated whether it is possible to build a practical record-and-replay
system avoiding all these issues. The answer turns out to be yes --- if the CPU
and operating system meet certain non-obvious constraints. Fortunately modern
Intel CPUs, Linux kernels and user-space frameworks meet these constraints,
although this has only become true recently. With some novel optimizations, our
system RR records and replays real-world workloads with low overhead with an
entirely user-space implementation running on stock hardware and operating
systems. RR forms the basis of an open-source reverse-execution debugger seeing
significant use in practice. We present the design and implementation of RR,
describe its performance on a variety of workloads, and identify constraints on
hardware and operating system design required to support our approach."
"The octagon abstract domain is a widely used numeric abstract domain
expressing relational information between variables whilst being both
computationally efficient and simple to implement. Each element of the domain
is a system of constraints where each constraint takes the restricted form $\pm
x_i \pm x_j \leq d$. A key family of operations for the octagon domain are
closure algorithms, which check satisfiability and provide a normal form for
octagonal constraint systems. We present new quadratic incremental algorithms
for closure, strong closure and integer closure and proofs of their
correctness. We highlight the benefits and measure the performance of these new
algorithms."
"A program can be viewed as a syntactic structure P (syntactic skeleton)
parameterized by a collection of the identifiers V (variable names). This paper
introduces the skeletal program enumeration (SPE) problem: Given a fixed
syntactic skeleton P and a set of variables V , enumerate a set of programs P
exhibiting all possible variable usage patterns within P. It proposes an
effective realization of SPE for systematic, rigorous compiler testing by
leveraging three important observations: (1) Programs with different variable
usage patterns exhibit diverse control- and data-dependence information, and
help exploit different compiler optimizations and stress-test compilers; (2)
most real compiler bugs were revealed by small tests (i.e., small-sized P) ---
this ""small-scope"" observation opens up SPE for practical compiler validation;
and (3) SPE is exhaustive w.r.t. a given syntactic skeleton and variable set,
and thus can offer a level of guarantee that is absent from all existing
compiler testing techniques.
  The key challenge of SPE is how to eliminate the enormous amount of
equivalent programs w.r.t. $\alpha$-conversion. Our main technical contribution
is a novel algorithm for computing the canonical (and smallest) set of all
non-$\alpha$-equivalent programs. We have realized our SPE technique and
evaluated it using syntactic skeletons derived from GCC's testsuite. Our
evaluation results on testing GCC and Clang are extremely promising. In less
than six months, our approach has led to 217 confirmed bug reports, 104 of
which have already been fixed, and the majority are long latent bugs despite
the extensive prior efforts of automatically testing both compilers (e.g.,
Csmith and EMI). The results also show that our algorithm for enumerating
non-$\alpha$-equivalent programs provides six orders of magnitude reduction,
enabling processing the GCC test-suite in under a month."
"Mobile applications today often fail to be context aware when they also need
to be customizable and efficient at run-time. Context-oriented programming
allows programmers to develop applications that are more context aware. Its
central construct, the so-called layer, however, is not customizable. We
propose to use novel persistent contextual values for mobile development.
Persistent contextual values automatically adapt their value to the context.
Furthermore they provide access without overhead. Key-value configuration files
contain the specification of contextual values and the persisted contextual
values themselves. By modifying the configuration files, the contextual values
can easily be customized for every context. From the specification, we generate
code to simplify development. Our implementation, called Elektra, permits
development in several languages including C++ and Java. In a benchmark we
compare layer activations between threads and between applications. In a case
study involving a web-server on a mobile embedded device the performance
overhead is minimal, even with many context switches."
"Programmers routinely trade space for time to increase performance, often in
the form of caching or memoization. In managed languages like Java or
JavaScript, however, this space-time tradeoff is complex. Using more space
translates into higher garbage collection costs, especially at the limit of
available memory. Existing runtime systems provide limited support for
space-sensitive algorithms, forcing programmers into difficult and often
brittle choices about provisioning.
  This paper presents prioritized garbage collection, a cooperative programming
language and runtime solution to this problem. Prioritized GC provides an
interface similar to soft references, called priority references, which
identify objects that the collector can reclaim eagerly if necessary. The key
difference is an API for defining the policy that governs when priority
references are cleared and in what order. Application code specifies a priority
value for each reference and a target memory bound. The collector reclaims
references, lowest priority first, until the total memory footprint of the
cache fits within the bound. We use this API to implement a space-aware
least-recently-used (LRU) cache, called a Sache, that is a drop-in replacement
for existing caches, such as Google's Guava library. The garbage collector
automatically grows and shrinks the Sache in response to available memory and
workload with minimal provisioning information from the programmer. Using a
Sache, it is almost impossible for an application to experience a memory leak,
memory pressure, or an out-of-memory crash caused by software caching."
"We study the notion of extensibility in functional data types, as a new
approach to the problem of decorating abstract syntax trees with additional
sets of information. We observed the need for such extensibility while
redesigning the data types representing Haskell abstract syntax inside GHC.
  Specifically, we describe our approach to the tree-decoration problem using a
novel syntactic machinery in Haskell for expressing extensible data types. We
show that the syntactic machinery is complete in that it can express all the
syntactically possible forms of extensions to algebraic data type declarations.
Then, we describe an encoding of the syntactic machinery based on the existing
features in Glasgow Haskell Compiler(GHC)."
"The construction of GNOOP as a domain-theoretic model of generic
nominally-typed OOP is currently underway. This extended abstract presents the
concepts of `nominal intervals' and `full generication' that are likely to help
in building GNOOP as an accurate mathematical model of generic nominally-typed
OOP. The abstract also presents few related category-theoretic suggestions. The
presented concepts and suggestions are particularly geared towards enabling
GNOOP to offer a precise and simple view of so-far-hard-to-analyze features of
generic OOP such as variance annotations (e.g., Java wildcard types) and erased
generics (e.g., Java type erasure)."
"We investigate the relationship between finite terms in {\lambda}-letrec, the
{\lambda}-calculus with letrec, and the infinite {\lambda}-terms they express.
We say that a lambda-letrec term expresses a lambda-term if the latter can be
obtained as an infinite unfolding of the former. Unfolding is the process of
substituting occurrences of function variables by the right-hand side of their
definition.
  We consider the following questions: (i) How can we characterise those
infinite {\lambda}-terms that are {\lambda}-letrec-expressible? (ii) Given two
{\lambda}-letrec terms, how can we determine whether they have the same
unfolding? (iii) Given a {\lambda}-letrec term, can we find a more compact
version of the term with the same unfolding? To tackle these questions we
introduce and study the following formalisms: (i) a rewriting system for
unfolding {\lambda}-letrec terms into {\lambda}-terms (ii) a rewriting system
for `observing' {\lambda}-terms by dissecting their term structure (iii)
higher-order and first-order graph formalisms together with translations
between them as well as translations from and to {\lambda}-letrec.
  We identify a first-order term graph formalism on which bisimulation
preserves and reflects the unfolding semantics of {\lambda}-letrec and which is
closed under functional bisimulation. From this we derive efficient methods to
determine whether two terms are equivalent under infinite unfolding and to
compute the maximally shared form of a given {\lambda}-letrec term."
"Verification problems of programs written in various paradigms (such as
imperative, logic, concurrent, functional, and object-oriented ones) can be
reduced to problems of solving Horn clause constraints on predicate variables
that represent unknown inductive invariants. This paper presents a novel Horn
constraint solving method based on inductive theorem proving: the method
reduces Horn constraint solving to validity checking of first-order formulas
with inductively defined predicates, which are then checked by induction on the
derivation of the predicates. To automate inductive proofs, we introduce a
novel proof system tailored to Horn constraint solving and use an SMT solver to
discharge proof obligations arising in the proof search. The main advantage of
the proposed method is that it can verify relational specifications across
programs in various paradigms where multiple function calls need to be analyzed
simultaneously. The class of specifications includes practically important ones
such as functional equivalence, associativity, commutativity, distributivity,
monotonicity, idempotency, and non-interference. Furthermore, our novel
combination of Horn clause constraints with inductive theorem proving enables
us to naturally and automatically axiomatize recursive functions that are
possibly non-terminating, non-deterministic, higher-order, exception-raising,
and over non-inductively defined data types. We have implemented a relational
verification tool for the OCaml functional language based on the proposed
method and obtained promising results in preliminary experiments."
"Semantic data fuels many different applications, but is still lacking proper
integration into programming languages. Untyped access is error-prone while
mapping approaches cannot fully capture the conceptualization of semantic data.
In this paper, we present $\lambda_{DL}$,a $\lambda$-calculus with a modified
type system to provide type-safe integration of semantic data. This is achieved
by the integration of description logics into the $\lambda$-calculus for typing
and data access. It is centered around several key design principles. Among
these are (1) the usage of semantic conceptualizations as types, (2) subtype
inference for these types, and (3) type-checked query access to the data by
both ensuring the satisfiability of queries as well as typing query results
precisely in $\lambda_{DL}$. The paper motivates the use of a modified type
system for semantic data and it provides the theoretic foundation for the
integration of description logics as well as the core formal specifications of
$\lambda_{DL}$ including a proof of type safety."
"In this paper, we prove correctness of parallelizing a string matcher using
Haskell as a theorem prover. We use refinement types to specify correctness
properties, Haskell terms to express proofs and Liquid Haskell to check
correctness of proofs. First, we specify and prove that a class of monoid
morphisms can be parallelized via parallel monoid concatenation. Then, we
encode string matching as a morphism to get a provably correct parallel
transformation. Our 1839LoC prototype proof shows that Liquid Haskell can be
used as a fully expressive theorem prover on realistic Haskell implementations."
"This volume contains the post-proceedings of the 14th International Workshop
on Quantitative Aspects of Programming Languages and Systems (QAPL), held as a
satellite workshop of ETAPS 2016 in Eindhoven, The Netherlands, on 2-3 April
2016."
"Linearisability is a central notion for verifying concurrent libraries: a
given library is proven safe if its operational history can be rearranged into
a new sequential one which, in addition, satisfies a given specification.
Linearisability has been examined for libraries in which method arguments and
method results are of ground type, including libraries parameterised with such
methods. In this paper we extend linearisability to the general higher-order
setting: methods can be passed as arguments and returned as values. A library
may also depend on abstract methods of any order. We use this generalised
notion to show correctness of several higher-order example libraries."
"Haskell, as implemented in the Glasgow Haskell Compiler (GHC), has been
adding new type-level programming features for some time. Many of these
features---chiefly: generalized algebraic datatypes (GADTs), type families,
kind polymorphism, and promoted datatypes---have brought Haskell to the
doorstep of dependent types. Many dependently typed programs can even currently
be encoded, but often the constructions are painful.
  In this dissertation, I describe Dependent Haskell, which supports full
dependent types via a backward-compatible extension to today's Haskell. An
important contribution of this work is an implementation, in GHC, of a portion
of Dependent Haskell, with the rest to follow. The features I have implemented
are already released, in GHC 8.0. This dissertation contains several practical
examples of Dependent Haskell code, a full description of the differences
between Dependent Haskell and today's Haskell, a novel type-safe dependently
typed lambda-calculus (called Pico) suitable for use as an intermediate
language for compiling Dependent Haskell, and a type inference and elaboration
algorithm, Bake, that translates Dependent Haskell to type-correct Pico."
"Gradual typing combines static and dynamic typing in the same language,
offering the benefits of both to programmers. Static typing provides error
detection and strong guarantees while dynamic typing enables rapid prototyping
and flexible programming idioms. For programmers to fully take advantage of a
gradual type system, however, they must be able to trust their type
annotations, and so runtime checks must be performed at the boundaries of
static and dynamic code to ensure that static types are respected. Higher order
and mutable values cannot be completely checked at these boundaries, and so
additional checks must be performed at their use sites. Traditionally, this has
been achieved by installing wrappers or proxies on such values that moderate
the flow of data between static and dynamic, but these can cause problems if
the language supports comparison of object identity or has a foreign function
interface.
  Reticulated Python is a gradually typed variant of Python implemented via a
source-to-source translator for Python 3. It implements a proxy-free
alternative design named transient casts. This paper presents a formal
semantics for transient casts and shows that not only are they sound, but they
work in an open-world setting in which the Reticulated translator has only been
applied to some of the program; the rest is untranslated Python. We formalize
this open world soundness property and use Coq to prove that it holds for
Anthill Python, a calculus that models Reticulated Python."
"In order to tackle the development of concurrent and distributed systems, the
active object programming model provides a high-level abstraction to program
concurrent behaviours. There exists already a variety of active object
frameworks targeted at a large range of application domains: modelling,
verification, efficient execution. However, among these frameworks, very few of
them consider a multi-threaded execution of active objects. Introducing a
controlled parallelism within active objects enables overcoming some of their
limitations. In this paper, we present a complete framework around the
multiactive object programming model. We present it through ProActive, the Java
library that offers multiactive objects, and through MultiASP, the programming
language that allows the formalisation of our developments. We then use
multiactive objects to compile a cooperative active object language with
different synchronisation primitives into our programming model. This paper
also presents different use cases and the development support to illustrate the
practical usability of our language. Formalisation of our work provides the
programmer with guarantees on the behaviour of the multiactive object
programming model and of the compiler."
"Choreographies are global descriptions of interactions among concurrent
components, most notably used in the settings of verification (e.g., Multiparty
Session Types) and synthesis of correct-by-construction software (Choreographic
Programming). They require a top-down approach: programmers first write
choreographies, and then use them to verify or synthesize their programs.
However, most existing software does not come with choreographies yet, which
prevents their application.
  To attack this problem, we propose a novel methodology (called choreography
extraction) that, given a set of programs or protocol specifications,
automatically constructs a choreography that describes their behavior. The key
to our extraction is identifying a set of paths in a graph that represents the
symbolic execution of the programs of interest. Our method improves on previous
work in several directions: we can now deal with programs that are equipped
with a state and internal computation capabilities; time complexity is
dramatically better; we capture programs that are correct but not necessarily
synchronizable, i.e., they work because they exploit asynchronous
communication."
"Virtual machines have been widely adapted for high-level programming language
implementations and for providing a degree of platform neutrality. As the
overall use and adaptation of virtual machines grow, the overall performance of
virtual machines has become a widely-discussed topic. In this paper, we present
a survey on the performance differences of the two most widely adapted types of
virtual machines - the stack-based virtual machine and the register-based
virtual machine - using various benchmark programs. Additionally, we adopted a
new approach of measuring performance by measuring the overall dispatch time,
amount of dispatches, fetch time, and execution time while running benchmarks
on custom-implemented, lightweight virtual machines. Finally, we present two
lightweight, custom-designed, Turing-equivalent virtual machines that are
specifically designed in benchmarking virtual machine performance - the
""Conceptum"" stack-based virtual machine, and the ""Inertia"" register-based
virtual machine. Our result showed that while on average the register machine
spends 20.39% less time in executing benchmarks than the stack machine, the
stack-based virtual machine is still faster than the virtual machine regarding
the instruction fetch time."
"Cala-gopher is a library-level Scala implementation of communication sequence
process constructs: channels, selectors (similar to analogical constructs in
Limbo or Go) transputers (similar to Occam proc) and a set of high-level
operations on top of akka and SIP-22 async. The framework integrates CSP-style
programming into standard Scala concurrency environment via idiomatic API. This
allows usage of communication patterns, well known in Go world, but not easy
expressable in mainstream scala concurrency frameworks, along with algebraic
approach for composing computation builders. Besides, we want to discuss
current implementation issues and future directions in the context of evolving
of compiler and libraries ecosystem."
"This article presents a resource analysis system for OCaml programs. This
system automatically derives worst-case resource bounds for higher-order
polymorphic programs with user-defined inductive types. The technique is
parametric in the resource and can derive bounds for time, memory allocations
and energy usage. The derived bounds are multivariate resource polynomials
which are functions of different size parameters that depend on the standard
OCaml types. Bound inference is fully automatic and reduced to a linear
optimization problem that is passed to an off-the-shelf LP solver. Technically,
the analysis system is based on a novel multivariate automatic amortized
resource analysis (AARA). It builds on existing work on linear AARA for
higher-order programs with user-defined inductive types and on multivariate
AARA for first-order programs with built-in lists and binary trees. For the
first time, it is possible to automatically derive polynomial bounds for
higher-order functions and polynomial bounds that depend on user-defined
inductive types. Moreover, the analysis handles programs with side effects and
even outperforms the linear bound inference of previous systems. At the same
time, it preserves the expressivity and efficiency of existing AARA techniques.
The practicality of the analysis system is demonstrated with an implementation
and integration with Inria's OCaml compiler. The implementation is used to
automatically derive resource bounds for 411 functions and 6018 lines of code
derived from OCaml libraries, the CompCert compiler, and implementations of
textbook algorithms. In a case study, the system infers bounds on the number of
queries that are sent by OCaml programs to DynamoDB, a commercial NoSQL cloud
database service."
"We describe a programming abstraction for heterogeneous parallel hardware,
designed to capture a wide range of popular parallel hardware, including GPUs,
vector instruction sets and multicore CPUs. Our abstraction, which we call
HPVM, is a hierarchical dataflow graph with shared memory and vector
instructions. We use HPVM to define both a virtual instruction set (ISA) and
also a compiler intermediate representation (IR). The virtual ISA aims to
achieve both functional portability and performance portability across
heterogeneous systems, while the compiler IR aims to enable effective code
generation and optimization for such systems.
  HPVM effectively supports all forms of parallelism used to achieve
computational speedups (as opposed to concurrency), including task parallelism,
coarse-grain data parallelism, fine-grain data parallelism, and pipelined
parallelism. HPVM also enables flexible scheduling and tiling: different nodes
in the dataflow graph can be mapped flexibly to different combinations of
compute units, and the graph hierarchy expresses memory tiling, essential for
achieving high performance on GPU and CPU targets."
"Termination is one of the basic liveness properties, and we study the
termination problem for probabilistic programs with real-valued variables.
Previous works focused on the qualitative problem that asks whether an input
program terminates with probability~1 (almost-sure termination). A powerful
approach for this qualitative problem is the notion of ranking supermartingales
with respect to a given set of invariants. The quantitative problem
(probabilistic termination) asks for bounds on the termination probability. A
fundamental and conceptual drawback of the existing approaches to address
probabilistic termination is that even though the supermartingales consider the
probabilistic behavior of the programs, the invariants are obtained completely
ignoring the probabilistic aspect.
  In this work we address the probabilistic termination problem for
linear-arithmetic probabilistic programs with nondeterminism. We define the
notion of {\em stochastic invariants}, which are constraints along with a
probability bound that the constraints hold. We introduce a concept of {\em
repulsing supermartingales}. First, we show that repulsing supermartingales can
be used to obtain bounds on the probability of the stochastic invariants.
Second, we show the effectiveness of repulsing supermartingales in the
following three ways: (1)~With a combination of ranking and repulsing
supermartingales we can compute lower bounds on the probability of termination;
(2)~repulsing supermartingales provide witnesses for refutation of almost-sure
termination; and (3)~with a combination of ranking and repulsing
supermartingales we can establish persistence properties of probabilistic
programs.
  We also present results on related computational problems and an experimental
evaluation of our approach on academic examples."
"The C and C++ high-level languages provide programmers with atomic operations
for writing high-performance concurrent code. At the assembly language level, C
and C++ atomics get mapped down to individual instructions or combinations of
instructions by compilers, depending on the ordering guarantees and
synchronization instructions provided by the underlying architecture. These
compiler mappings must uphold the ordering guarantees provided by C/C++ atomics
or the compiled program will not behave according to the C/C++ memory model. In
this paper we discuss two counterexamples to the well-known trailing-sync
compiler mappings for the Power and ARMv7 architectures that were previously
thought to be proven correct. In addition to the counterexamples, we discuss
the loophole in the proof of the mappings that allowed the incorrect mappings
to be proven correct. We also discuss the current state of compilers and
architectures in relation to the bug."
"At the allocation and deallocation of small objects with fixed size, the
standard allocator of the runtime system has commonly a worse time performance
compared to allocators adapted for a special application field. We propose a
memory allocator, originally developed for mesh primitives but also usable for
any other small equally sized objects. For a large amount of objects it leads
to better results than allocating data with the C ++new instruction and behaves
nowhere worse. The proposed synchronization approach for this allocator behaves
lock-free in practical scenarios without using machine instructions, such as
compare-and-swap. A traversal structure is integrated requiring less memory
than using containers such as STL-vectors or lists, but with comparable time
performance."
"A long-standing shortcoming of statically typed functional languages is that
type checking does not rule out pattern-matching failures (run-time match
exceptions). Refinement types distinguish different values of datatypes; if a
program annotated with refinements passes type checking, pattern-matching
failures become impossible. Unfortunately, refinement is a monolithic property
of a type, exacerbating the difficulty of adding refinement types to nontrivial
programs.
  Gradual typing has explored how to incrementally move between static typing
and dynamic typing. We develop a type system of gradual sums that combines
refinement with imprecision. Then, we develop a bidirectional version of the
type system, which rules out excessive imprecision, and give a type-directed
translation to a target language with explicit casts. We prove that the static
sublanguage cannot have match failures, that a well-typed program remains
well-typed if its type annotations are made less precise, and that making
annotations less precise causes target programs to fail later. Several of these
results correspond to criteria for gradual typing given by Siek et al. (2015)."
"Session types are used to describe communication protocols in distributed
systems and, as usual in type theories, session subtyping characterizes
substitutability of the communicating processes. We investigate the
(un)decidability of subtyping for session types in asynchronously communicating
systems. We first devise a core undecidable subtyping relation that is obtained
by imposing limitations on the structure of types. Then, as a consequence of
this initial undecidability result, we show that (differently from what stated
or conjectured in the literature) the three notions of asynchronous subtyping
defined so far for session types are all undecidable. Namely, we consider the
asynchronous session subtyping by Mostrous and Yoshida for binary sessions, the
relation by Chen et al. for binary sessions under the assumption that every
message emitted is eventually consumed, and the one by Mostrous et al. for
multiparty session types. Finally, by showing that two fragments of the core
subtyping relation are decidable, we evince that further restrictions on the
structure of types make our core subtyping relation decidable."
"Type soundness is an important property of modern programming languages. In
this paper we explore the idea that ""well-typed languages are sound"": the idea
that the appropriate typing discipline over language specifications guarantees
that the language is type sound. We instantiate this idea for a certain class
of languages defined using small step operational semantics by ensuring the
progress and preservation theorems. Our first contribution is a syntactic
discipline for organizing and restricting language specifications so that they
automatically satisfy the progress theorem. This discipline is not novel but
makes explicit the way expert language designers have been organizing a certain
class of languages for long time. We give a formal account of this discipline
by representing language specifications as (higher-order) logic programs and by
giving a meta type system over that collection of formulas. Our second
contribution is a methodology and meta type system for guaranteeing that
languages satisfy the preservation theorem. Ultimately, we proved that language
specifications that conform to our meta type systems are guaranteed to be type
sound. We have implemented these ideas in the TypeSoundnessCertifier, a tool
that takes language specifications in the form of logic programs and type
checks them according to our meta type systems. For those languages that pass
our type checker, our tool automatically produces a proof of type soundness
that can be machine-checked by the Abella proof assistant. For those languages
that fail our type checker, the tool pinpoints the design mistakes that hinder
type soundness. We have applied the TypeSoundnessCertifier to a large number of
programming languages, including those with recursive types, polymorphism,
letrec, exceptions, lists and other common types and operators."
"Choreographic programming is a programming-language design approach that
drives error-safe protocol development in distributed systems. Starting from a
global specification (choreography) one can generate distributed
implementations. The advantages of this top-down approach lie in the
correctness-by-design principle, where implementations (endpoints) generated
from a choreography behave according to the strict control flow described in
the choreography, and do not deadlock. Motivated by challenging scenarios in
Cyber-Physical Systems (CPS), we study how choreographic programming can cater
for dynamic infrastructures where not all endpoints are always available. We
introduce the Global Quality Calculus ($GC_q$), a variant of choreographic
programming for the description of communication systems where some of the
components involved in a communication might fail. GCq features novel operators
for multiparty, partial and collective communications. This paper studies the
nature of failure-aware communication: First, we introduce $GC_q$ syntax,
semantics and examples of its use. The interplay between failures and
collective communications in a choreography can lead to choreographies that
cannot progress due to absence of resources. In our second contribution, we
provide a type system that ensures that choreographies can be realized despite
changing availability conditions. A specification in $GC_q$ guides the
implementation of distributed endpoints when paired with global (session)
types. Our third contribution provides an endpoint-projection based methodology
for the generation of failure-aware distributed processes. We show the
correctness of the projection, and that well-typed choreographies with
availability considerations enjoy progress."
"Peephole optimizations are a common source of compiler bugs. Compiler
developers typically transform an incorrect peephole optimization into a valid
one by strengthening the precondition. This process is challenging and tedious.
This paper proposes ALIVE-INFER, a data-driven approach that infers
preconditions for peephole optimizations expressed in Alive. ALIVE-INFER
generates positive and negative examples for an optimization, enumerates
predicates on-demand, and learns a set of predicates that separate the positive
and negative examples. ALIVE-INFER repeats this process until it finds a
precondition that ensures the validity of the optimization. ALIVE-INFER reports
both a weakest precondition and a set of succinct partial preconditions to the
developer. Our prototype generates preconditions that are weaker than LLVM's
preconditions for 73 optimizations in the Alive suite. We also demonstrate the
applicability of this technique to generalize 54 optimization patterns
generated by Souper, an LLVM~IR--based superoptimizer."
"Channel- and actor-based programming languages are both used in practice, but
the two are often confused. Languages such as Go provide anonymous processes
which communicate using buffers or rendezvous points---known as
channels---while languages such as Erlang provide addressable processes---known
as actors---each with a single incoming message queue. The lack of a common
representation makes it difficult to reason about translations that exist in
the folklore. We define a calculus $\lambda_{\textrm{ch}}$ for typed
asynchronous channels, and a calculus $\lambda_{\textrm{act}}$ for typed
actors. We define translations from $\lambda_{\textrm{act}}$ into
$\lambda_{\textrm{ch}}$ and $\lambda_{\textrm{ch}}$ into
$\lambda_{\textrm{act}}$ and prove that both are type- and
semantics-preserving. We show that our approach accounts for synchronisation
and selective receive in actor systems and discuss future extensions to support
guarded choice and behavioural types."
"This paper presents an example-driven synthesis technique for automating a
large class of data preparation tasks that arise in data science. Given a set
of input tables and an out- put table, our approach synthesizes a table
transformation program that performs the desired task. Our approach is not
restricted to a fixed set of DSL constructs and can synthesize programs from an
arbitrary set of components, including higher-order combinators. At a
high-level, our approach performs type-directed enumerative search over partial
pro- grams but incorporates two key innovations that allow it to scale: First,
our technique can utilize any first-order specification of the components and
uses SMT-based deduction to reject partial programs. Second, our algorithm uses
partial evaluation to increase the power of deduction and drive enumerative
search. We have evaluated our synthesis algorithm on dozens of data preparation
tasks obtained from on-line forums, and we show that our approach can
automatically solve a large class of problems encountered by R users."
"The Dependent Object Types (DOT) calculus aims to model the essence of Scala,
with a focus on abstract type members, path-dependent types, and subtyping.
Other Scala features could be defined by translation to DOT. Mutation is a
fundamental feature of Scala currently missing in DOT. Mutation in DOT is
needed not only to model effectful computation and mutation in Scala programs,
but even to precisely specify how Scala initializes immutable variables and
fields (vals). We present an extension to DOT that adds typed mutable reference
cells. We have proven the extension sound with a mechanized proof in Coq. We
present the key features of our extended calculus and its soundness proof, and
discuss the challenges that we encountered in our search for a sound design and
the alternative solutions that we considered."
"We explore the design and implementation of Frank, a strict functional
programming language with a bidirectional effect type system designed from the
ground up around a novel variant of Plotkin and Pretnar's effect handler
abstraction.
  Effect handlers provide an abstraction for modular effectful programming: a
handler acts as an interpreter for a collection of commands whose interfaces
are statically tracked by the type system. However, Frank eliminates the need
for an additional effect handling construct by generalising the basic mechanism
of functional abstraction itself. A function is simply the special case of a
Frank operator that interprets no commands. Moreover, Frank's operators can be
multihandlers which simultaneously interpret commands from several sources at
once, without disturbing the direct style of functional programming with
values.
  Effect typing in Frank employs a novel form of effect polymorphism which
avoid mentioning effect variables in source code. This is achieved by
propagating an ambient ability inwards, rather than accumulating unions of
potential effects outwards.
  We introduce Frank by example, and then give a formal account of the Frank
type system and its semantics. We introduce Core Frank by elaborating Frank
operators into functions, case expressions, and unary handlers, and then give a
sound small-step operational semantics for Core Frank.
  Programming with effects and handlers is in its infancy. We contribute an
exploration of future possibilities, particularly in combination with other
forms of rich type system."
"We study induction on the program structure as a proof method for
bisimulation-based compiler correctness. We consider a first-order language
with mutually recursive function definitions, system calls, and an environment
semantics. The proof method relies on a generalization of compatibility of
function definition with the bisimulation. We use the inductive method to show
correctness of a form of dead code elimination. This is an interesting case
study because the transformation removes function, variable, and parameter
definitions from the program. While such transformations require modification
of the simulation in a coinductive proof, the inductive method deals with them
naturally. All our results are formalized in Coq."
"We present sound and complete environmental bisimilarities for a variant of
Dybvig et al.'s calculus of multi-prompted delimited-control operators with
dynamic prompt generation. The reasoning principles that we obtain generalize
and advance the existing techniques for establishing program equivalence in
calculi with single-prompted delimited control.
  The basic theory that we develop is presented using Madiot et al.'s framework
that allows for smooth integration and composition of up-to techniques
facilitating bisimulation proofs. We also generalize the framework in order to
express environmental bisimulations that support equivalence proofs of
evaluation contexts representing continuations. This change leads to a novel
and powerful up-to technique enhancing bisimulation proofs in the presence of
control operators."
"We introduce Dynamic SOS as a framework for describing semantics of
programming languages that include dynamic software upgrades. Dynamic SOS
(DSOS) is built on top of the Modular SOS of P. Mosses, with an underlying
category theory formalization. The idea of Dynamic SOS is to bring out the
essential differences between dynamic upgrade constructs and program execution
constructs. The important feature of Modular SOS (MSOS) that we exploit in DSOS
is the sharp separation of the program execution code from the additional
(data) structures needed at run-time. In DSOS we aim to achieve the same
modularity and decoupling for dynamic software upgrades. This is partly
motivated by the long term goal of having machine-checkable proofs for general
results like type safety. We exemplify Dynamic SOS on two languages supporting
dynamic software upgrades, namely the C-like PROTEUS, which supports updating
of variables, functions, records, or types at specific program points, and
CREOL, which supports dynamic class upgrades in the setting of concurrent
objects. Existing type analyses for software upgrades can be done on top of
DSOS too, as we illustrate for PROTEUS. A second contribution is the definition
of a general encapsulating construction on Modular SOS useful in situations
where a form of encapsulation of the execution is needed. We show how to apply
this in the setting of concurrent object-oriented programming with active
objects and asynchronous method calls."
"Today's JavaScript applications are composed of scripts from different
origins that are loaded at run time. As not all of these origins are equally
trusted, the execution of these scripts should be isolated from one another.
However, some scripts must access the application state and some may be allowed
to change it, while preserving the confidentiality and integrity constraints of
the application.
  This paper presents design and implementation of DecentJS, a
language-embedded sandbox for full JavaScript. It enables scripts to run in a
configurable degree of isolation with fine-grained access control. It provides
a transactional scope in which effects are logged for review by the access
control policy. After inspection of the log, effects can be committed to the
application state or rolled back.
  The implementation relies on JavaScript proxies to guarantee full
interposition for the full language and for all code, including dynamically
loaded scripts and code injected via eval. Its only restriction is that scripts
must be compliant with JavaScript's strict mode."
"Functional languages with strong static type systems have beneficial
properties to help ensure program correctness and reliability. Surprisingly,
their practical significance in applications is low relative to other languages
lacking in those dimensions. In this paper, the programs-as-proofs analogy is
taken seriously to gain speculative insights by analysis of creation habits in
the proof-centric discipline of mathematics. Viewed in light of this analogy, a
sampling of mathematicians' attitudes towards formal proof suggests that the
crucial role of intuition and experimentation in programming tasks may be under
appreciated, hinting at a possible explanation of the challenges rigorously
disciplined languages face in practical applications."