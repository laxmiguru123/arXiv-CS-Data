summary
"In this paper we analyze the development of interactive TV in the U.S. and
Western Europe. We argue that despite the nascent character of the market there
are important regulatory issues at stake, as exemplified by the AOL/TW merger
and the British Interactive Broadcasting case. Absent rules that provide for
non-discriminatory access to network components (including terminal equipment
specifications), dominant platform operators are likely to leverage ownership
of delivery infrastructure into market power over interactive TV services.
While integration between platform operator, service provider and terminal
vendor may facilitate the introduction of services in the short-term, the
lasting result will be a collection of fragmented ""walled gardens"" offering
limited content and applications. Would interactive TV develop under such
model, the exciting opportunities for broad-based innovation and extended
access to multiple information, entertainment and educational services opened
by the new generation of broadcasting technologies will be foregone"
"The modern multimedia technologies based on the whole palette of hardware and
software facilities of real-time high-speed information processing, in a
combination with effective facilities of the remote access to information
resources, allow us to visualize diverse types of information. Data
visualization facilities &#8211; is the face of the Automated Control System on
whom often judge about their efficiency. They take a special place, providing
visualization of the diverse information necessary for decision-making by a
final control link - the person allocated by certain powers."
"Many current watermarking algorithms insert data in the spatial or transform
domains like the discrete cosine, the discrete Fourier, and the discrete
wavelet transforms. In this paper, we present a data-hiding algorithm that
exploits the singular value decomposition (SVD) representation of the data. We
compute the SVD of the host image and the watermark and embed the watermark in
the singular vectors of the host image. The proposed method leads to an
imperceptible scheme for digital images, both in grey scale and color and is
quite robust against attacks like noise and JPEG compression."
"In the context of human action recognition in video sequences, a temporal
belief filter is presented. It allows to cope with human action disparity and
low quality videos. The whole system of action recognition is based on the
Transferable Belief Model (TBM) proposed by P. Smets. The TBM allows to
explicitly model the doubt between actions. Furthermore, the TBM emphasizes the
conflict which is exploited for action recognition. The filtering performance
is assessed on real video sequences acquired by a moving camera and under
several unknown view angles."
"In this paper, we propose a fast 2-D block-based motion estimation algorithm
called Particle Swarm Optimization - Zero-motion Prejudgment(PSO-ZMP) which
consists of three sequential routines: 1)Zero-motion prejudgment. The routine
aims at finding static macroblocks(MB) which do not need to perform remaining
search thus reduces the computational cost; 2)Predictive image coding and 3)PSO
matching routine. Simulation results obtained show that the proposed PSO-ZMP
algorithm achieves over 10 times of computation less than Diamond Search(DS)
and 5 times less than the recent proposed Adaptive Rood Pattern
Searching(ARPS). Meanwhile the PSNR performances using PSO-ZMP are very close
to that using DS and ARPS in some less-motioned sequences. While in some
sequences containing dense and complex motion contents, the PSNR performances
of PSO-ZMP are several dB lower than that using DS and ARPS but in an
acceptable degree."
"Motion estimation is the most critical process in video coding systems. First
of all, it has a definitive impact on the rate-distortion performance given by
the video encoder. Secondly, it is the most computationally intensive process
within the encoding loop. For these reasons, the design of high-performance
low-cost motion estimators is a crucial task in the video compression field. An
adaptive cost block matching (ACBM) motion estimation technique is presented in
this paper, featuring an excellent tradeoff between the quality of the
reconstructed video sequences and the computational effort. Simulation results
demonstrate that the ACBM algorithm achieves a slight better rate-distortion
performance than the one given by the well-known full search algorithm block
matching algorithm with reductions of up to 95% in the computational load."
"This paper surveys the characteristics of multimedia systems. Multimedia
applications today are dominated by compression and decompression, but
multimedia devices must also implement many other functions such as security
and file management. We introduce some basic concepts of multimedia algorithms
and the larger set of functions that multimedia systems-on-chips must
implement."
"Visual information processing will play an increasingly important role in
future electronics systems. In many applications, e.g. video surveillance
cameras, data throughput of microprocessors is not sufficient and power
consumption is too high. Instruction profiling on a typical test algorithm has
shown that pixel address calculations are the dominant operations to be
optimized. Therefore AddressLib, a structured scheme for pixel addressing was
developed, that can be accelerated by AddressEngine, a coprocessor for visual
information processing. In this paper, the architectural design of
AddressEngine is described, which in the first step supports a subset of the
AddressLib. Dataflow and memory organization are optimized during architectural
design. AddressEngine was implemented in a FPGA and was tested with MPEG-7
Global Motion Estimation algorithm. Results on processing speed and circuit
complexity are given and compared to a pure software implementation. The next
step will be the support for the full AddressLib, including segment addressing.
An outlook on further investigations on dynamic reconfiguration capabilities is
given."
"In this paper, we propose an export architecture that provides a clear
separation of authoring services from publication services. We illustrate this
architecture with the LimSee3 authoring tool and several standard publication
formats: Timesheets, SMIL, and XHTML."
"Initial offset placement in p2p streaming systems is studied in this paper.
Proportional placement (PP) scheme is proposed. In this scheme, peer places the
initial offset as the offset reported by other reference peer with a shift
proportional to the buffer width or offset lag of this reference peer. This
will introduce a stable placement that supports larger buffer width for peers
and small buffer width for tracker. Real deployed placement method in PPLive is
studied through measurement. It shows that, instead of based on offset lag, the
placement is based on buffer width of the reference peer to facilitate the
initial chunk fetching. We will prove that, such a PP scheme may not be stable
under arbitrary buffer occupation in the reference peer. The required average
buffer width then is derived. A simple good peer selection mechanism to check
the buffer occupation of reference peer is proposed for a stable PP scheme
based on buffer width"
"No single information source can be good enough to satisfy the divergent and
dynamic needs of users all the time. Integrating information from divergent
sources can be a solution to deficiencies in information content. We present
how Information from multimedia document can be collected based on associating
a generic database to a federated database. Information collected in this way
is brought into relevance by integrating the parameters of usage and user's
parameter for decision making. We identified seven different classifications of
multimedia document."
"A single queue incorporating a retransmission protocol is investigated,
assuming that the sequence of per effort success probabilities in the Automatic
Retransmission reQuest (ARQ) chain is a priori defined and no channel state
information at the transmitter is available. A Markov Decision Problem with an
average cost criterion is formulated where the possible actions are to either
continue the retransmission process of an erroneous packet at the next time
slot or to drop the packet and move on to the next packet awaiting for
transmission. The cost per slot is a linear combination of the current queue
length and a penalty term in case dropping is chosen as action. The
investigation seeks policies that provide the best possible average packet
delay-dropping trade-off for Quality of Service guarantees. An optimal
deterministic stationary policy is shown to exist, several structural
properties of which are obtained. Based on that, a class of suboptimal
<L,K>-policies is introduced. These suggest that it is almost optimal to use a
K-truncated ARQ protocol as long as the queue length is lower than L, else send
all packets in one shot. The work concludes with an evaluation of the optimal
delay-dropping tradeoff using dynamic programming and a comparison between the
optimal and suboptimal policies."
"The intent of the H.264 AVC project was to create a standard capable of
providing good video quality at substantially lower bit rates than previous
standards without increasing the complexity of design so much that it would be
impractical or excessively expensive to implement. An additional goal was to
provide enough flexibility to allow the standard to be applied to a wide
variety of applications. To achieve better coding efficiency, H.264 AVC uses
several techniques such as inter mode and intra mode prediction with variable
size motion compensation, which adopts Rate Distortion Optimization (RDO). This
increases the computational complexity of the encoder especially for devices
with lower processing capabilities such as mobile and other handheld devices.
In this paper, we propose an algorithm to reduce the number of mode and sub
mode evaluations in inter mode prediction. Experimental results show that this
fast intra mode selection algorithm can lessen about 75 percent encoding time
with little loss of bit rate and visual quality."
"Playout buffers are used in VoIP systems to compensate for network delay
jitter by making a trade-off between delay and loss. In this work we propose a
playout buffer algorithm that makes the trade-off based on maximization of
conversational speech quality, aiming to keep the computational complexity
lowest possible. We model the network delay using a Pareto distribution and
show that it is a good compromise between providing an appropriate fit to the
network delay characteristics and yielding a low arithmetical complexity. We
use the ITU-T E-Model as the quality model and simplify its delay impairment
function. The proposed playout buffer algorithm finds the optimum playout delay
using a closed-form solution that minimizes the sum of the simplified delay
impairment factor and the loss-dependent equipment impairment factor of the
E-model. The simulation results show that our proposed algorithm outperforms
existing state-of-the-art algorithms with a reduced complexity for a
quality-based algorithm."
"A novel video watermarking system operating in the three dimensional wavelet
transform is here presented. Specifically the video sequence is partitioned
into spatio temporal units and the single shots are projected onto the 3D
wavelet domain. First a grayscale watermark image is decomposed into a series
of bitplanes that are preprocessed with a random location matrix. After that
the preprocessed bitplanes are adaptively spread spectrum and added in 3D
wavelet coefficients of the video shot. Our video watermarking algorithm is
robust against the attacks of frame dropping, averaging and swapping.
Furthermore, it allows blind retrieval of embedded watermark which does not
need the original video and the watermark is perceptually invisible. The
algorithm design, evaluation, and experimentation of the proposed scheme are
described in this paper."
"In this paper we present the new genre of interactive operas implemented on
personal computers. They differ from traditional ones not only because they are
virtual, but mainly because they offer to composers and listeners new
perspectives of combinations and interactions between music, text and visual
aspects."
"In this paper we have proposed a dynamic buffer allocation algorithm for the
prefix, based on the popularity of the videos. More cache blocks are allocated
for most popular videos and a few cache blocks are allocated for less popular
videos. Buffer utilization is also maximized irrespective of the load on the
Video-on-Demand system. Overload can lead the server getting slowed down. By
storing the first few seconds of popular video clips, a multimedia local server
can shield the users from the delay, throughput, and loss properties of the
path between the local server and the central server. The key idea of
controlled multicast is used to allow clients to share a segment of a video
stream even when the requests arrive at different times. This dynamic buffer
allocation algorithm is simulated and its performance is evaluated based on the
buffer utilization by multimedia servers and average buffer allocation for the
most popular videos. Our simulation results shows efficient utilization of
network bandwidth and reduced hard disk utilization hence resulting in increase
in the number of requests being served."
"This paper presents a gradient based motion estimation algorithm based on
shape-motion prediction, which takes advantage of the correlation between
neighboring Binary Alpha Blocks (BABs), to match with the Mpeg-4 shape coding
case and speed up the estimation process. The PSNR and computation time
achieved by the proposed algorithm seem to be better than those obtained by
most popular motion estimation techniques."
"In a video on demand system, the main video repository may be far away from
the user and generally has limited streaming capacities. Since a high quality
video's size is huge, it requires high bandwidth for streaming over the
internet. In order to achieve a higher video hit ratio, reduced client waiting
time, distributed server's architecture can be used, in which multiple local
servers are placed close to clients and, based on their regional demands video
contents are cached dynamically from the main server. As the cost of proxy
server is decreasing and demand for reduced waiting time is increasing day by
day, newer architectures are explored, innovative schemes are arrived at. In
this paper we present novel 3 layer architecture, includes main multimedia
server, a Tracker and Proxy servers. This architecture targets to optimize the
client waiting time. We also propose an efficient prefix caching and load
sharing algorithm at the proxy server to allocate the cache according to
regional popularity of the video. The simulation results demonstrate that it
achieves significantly lower client's waiting time, when compared to the other
existing algorithms."
"Image Compression plays a very important role in image processing especially
when we are to send the image on the internet. The threat to the information on
the internet increases and image is no exception. Generally the image is sent
on the internet as the compressed image to optimally use the bandwidth of the
network. But as we are on the network, at any intermediate level the image can
be changed intentionally or unintentionally. To make sure that the correct
image is being delivered at the other end we embed the water mark to the image.
The watermarked image is then compressed and sent on the network. When the
image is decompressed at the other end we can extract the watermark and make
sure that the image is the same that was sent by the other end. Though
watermarking the image increases the size of the uncompressed image but that
has to done to achieve the high degree of robustness i.e. how an image sustains
the attacks on it. The present paper is an attempt to make transmission of the
images secure from the intermediate attacks by applying the generally used
compression transforms."
"In this paper we propose scalable proxy servers cluster architecture of
interconnected proxy servers for high quality and high availability services.
We also propose an optimal regional popularity based video prefix replication
strategy and a scene change based replica caching algorithm that utilizes the
zipf-like video popularity distribution to maximize the availability of videos
closer to the client and request-servicing rate thereby reducing the client
rejection ratio and the response time for the client. The simulation results of
our proposed architecture and algorithm show the greater achievement in
maximizing the availability of videos, client request-servicing rate and in
reduction of initial start-up latency and client rejection ratio."
"Digital processing of speech signal and voice recognition algorithm is very
important for fast and accurate automatic voice recognition technology. The
voice is a signal of infinite information. A direct analysis and synthesizing
the complex voice signal is due to too much information contained in the
signal. Therefore the digital signal processes such as Feature Extraction and
Feature Matching are introduced to represent the voice signal. Several methods
such as Liner Predictive Predictive Coding (LPC), Hidden Markov Model (HMM),
Artificial Neural Network (ANN) and etc are evaluated with a view to identify a
straight forward and effective method for voice signal. The extraction and
matching process is implemented right after the Pre Processing or filtering
signal is performed. The non-parametric method for modelling the human auditory
perception system, Mel Frequency Cepstral Coefficients (MFCCs) are utilize as
extraction techniques. The non linear sequence alignment known as Dynamic Time
Warping (DTW) introduced by Sakoe Chiba has been used as features matching
techniques. Since it's obvious that the voice signal tends to have different
temporal rate, the alignment is important to produce the better
performance.This paper present the viability of MFCC to extract features and
DTW to compare the test patterns."
"Tag recommendation is a common way to enrich the textual annotation of
multimedia contents. However, state-of-the-art recommendation methods are built
upon the pair-wised tag relevance, which hardly capture the context of the web
video, i.e., when who are doing what at where. In this paper we propose the
context-oriented tag recommendation (CtextR) approach, which expands tags for
web videos under the context-consistent constraint. Given a web video, CtextR
first collects the multi-form WWW resources describing the same event with the
video, which produce an informative and consistent context; and then, the tag
recommendation is conducted based on the obtained context. Experiments on an
80,031 web video collection show CtextR recommends various relevant tags to web
videos. Moreover, the enriched tags improve the performance of web video
categorization."
"Among the areas, most demanding in terms of calculation is the
telecommunication and video applications are now included in several
telecommunication devices such as set-top boxes, mobile phones. Embedded videos
applications in new generations of telecommunication devices need a processing
capacity that can not be achieved by the conventional processor, to work around
this problem the use of programmable technology has a lot of interest. First,
Field Programmable Gate Arrays (FPGAs) present many performance benefits for
real-time image processing applications. The FPGA structure is able to exploit
spatial and temporal parallelism. In this paper, we present a new method for
implementation of the Color Structure Descriptor (CSD) using the FPGA circuit.
In fact the (CSD) provides satisfactory image indexing and retrieval results
among all colorbased descriptors in MPEG-7. But the real time implementation of
this descriptor is still having problems. In this paper we propose a method for
adapting this descriptor for possible implementation under the constraints of
the video processing in real time. We have verified the real-time
implementation of the (CSD) with an image size of 120*80 pixels."
"Most P2P VoD schemes focused on service architectures and overlays
optimization without considering segments rarity and the performance of
prefetching strategies. As a result, they cannot better support VCRoriented
service in heterogeneous environment having clients using free VCR controls.
Despite the remarkable popularity in VoD systems, there exist no prior work
that studies the performance gap between different prefetching strategies. In
this paper, we analyze and understand the performance of different prefetching
strategies. Our analytical characterization brings us not only a better
understanding of several fundamental tradeoffs in prefetching strategies, but
also important insights on the design of P2P VoD system. On the basis of this
analysis, we finally proposed a cooperative prefetching strategy called
""cooching"". In this strategy, the requested segments in VCR interactivities are
prefetched into session beforehand using the information collected through
gossips. We evaluate our strategy through extensive simulations. The results
indicate that the proposed strategy outperforms the existing prefetching
mechanisms."
"With the rapid development of computer technology, computer music has begun
to appear in the laboratory. Many potential utility of computer music is
gradually increasing. The purpose of this paper is attempted to analyze the
possibility of integrating multimodal interaction such as vision-based hand
gesture and speech interaction into musical conducting education. To achieve
this purpose, this paper is focus on discuss some related research and the
traditional musical conducting education. To do so, six musical conductors had
been interviewed to share their musical conducting learning/ teaching
experience. These interviews had been analyzed in this paper to show the
syllabus and the focus of musical conducting education for beginners."
"In this paper, we propose an efficient client-to-client streaming approach to
cooperatively stream the video using chaining technique with unicast
communication among the clients. This approach considers two major issues of
VoD 1) Prefix caching scheme to accommodate more number of videos closer to
client, so that the request-service delay for the user can be minimized. 2)
Cooperative proxy and client chaining scheme for streaming the videos using
unicasting. This approach minimizes the client rejection rate and bandwidth
requirement on server to proxy and proxy to client path. Our simulation results
show that the proposed approach achieves reduced client waiting time and
optimal prefix caching of videos minimizing server to proxy path bandwidth
usage by utilizing the client to client bandwidth, which is occasionally used
when compared to busy server to proxy path bandwidth."
"This paper presents an efficient method for approximation of temporal video
data using linear Bezier fitting. For a given sequence of frames, the proposed
method estimates the intensity variations of each pixel in temporal dimension
using linear Bezier fitting in Euclidean space. Fitting of each segment ensures
upper bound of specified mean squared error. Break and fit criteria is employed
to minimize the number of segments required to fit the data. The proposed
method is well suitable for lossy compression of temporal video data and
automates the fitting process of each pixel. Experimental results show that the
proposed method yields good results both in terms of objective and subjective
quality measurement parameters without causing any blocking artifacts."
"Image steganography is the art of hiding information into a cover image. This
paper presents a novel technique for Image steganography based on Block-DCT,
where DCT is used to transform original image (cover image) blocks from spatial
domain to frequency domain. Firstly a gray level image of size M x N is divided
into no joint 8 x 8 blocks and a two dimensional Discrete Cosine Transform (2-d
DCT) is performed on each of the P = MN / 64 blocks. Then Huffman encoding is
also performed on the secret messages/images before embedding and each bit of
Huffman code of secret message/image is embedded in the frequency domain by
altering the least significant bit of each of the DCT coefficients of cover
image blocks. The experimental results show that the algorithm has a high
capacity and a good invisibility. Moreover PSNR of cover image with stego-image
shows the better results in comparison with other existing steganography
approaches. Furthermore, satisfactory security is maintained since the secret
message/image cannot be extracted without knowing decoding rules and Huffman
table."
"In this paper the background detection in images in poor lighting can be done
by the use of morphological filters. Lately contrast image enhancement
technique is used to detect the background in image which uses Weber's Law. The
proposed technique is more effective one in which the background detection in
image can be done in color images. The given image obtained in this method is
very effective one. More enhancement can be obtained while comparing the
results. In this technique compressed domain enhancement has been used for
better result."
"In this paper a novel hybrid approach for compensating the distortion of any
interpolation has been proposed. In this hybrid method, a modular approach was
incorporated in an iterative fashion. By using this approach we can get drastic
improvement with less computational complexity. The extension of the proposed
approach to two dimensions was also studied. Both the simulation results and
mathematical analyses confirmed the superiority of the hybrid method. The
proposed method was also shown to be robust against additive noise."
"Now-a-days internet has become a vast source of entertainment & new services
are available in quick succession which provides entertainment to the users.
One of this service i.e. Video-on-Demand is most hyped service in this context.
Transferring the video over the network with less error is the main objective
of the service providers. In this paper we present an algorithm for routing the
video to the user in an effective manner along with a method that ensures less
error rate than others."
"Information security has become a cause of concern because of the electronic
eavesdropping. Capacity, robustness and invisibility are important parameters
in information hiding and are quite difficult to achieve in a single algorithm.
This paper proposes a novel steganography technique for digital color image
which achieves the purported targets. The professed methodology employs a
complete random scheme for pixel selection and embedding of data. Of the three
colour channels (Red, Green, Blue) in a given colour image, the least two
significant bits of any one of the channels of the color image is used to
channelize the embedding capacity of the remaining two channels. We have
devised three approaches to achieve various levels of our desired targets. In
the first approach, Red is the default guide but it results in localization of
MSE in the remaining two channels, which makes it slightly vulnerable. In the
second approach, user gets the liberty to select the guiding channel (Red,
Green or Blue) to guide the remaining two channels. It will increase the
robustness and imperceptibility of the embedded image however the MSE factor
will still remain as a drawback. The third approach improves the performance
factor as a cyclic methodology is employed and the guiding channel is selected
in a cyclic fashion. This ensures the uniform distribution of MSE, which gives
better robustness and imperceptibility along with enhanced embedding capacity.
The imperceptibility has been enhanced by suitably adapting optimal pixel
adjustment process (OPAP) on the stego covers."
"With the increasing growth of technology and the entrance into the digital
age, we have to handle a vast amount of information every time which often
presents difficulties. So, the digital information must be stored and retrieved
in an efficient and effective manner, in order for it to be put to practical
use. Wavelets provide a mathematical way of encoding information in such a way
that it is layered according to level of detail. This layering facilitates
approximations at various intermediate stages. These approximations can be
stored using a lot less space than the original data. Here a low complex 2D
image compression method using wavelets as the basis functions and the approach
to measure the quality of the compressed image are presented. The particular
wavelet chosen and used here is the simplest wavelet form namely the Haar
Wavelet. The 2D discret wavelet transform (DWT) has been applied and the detail
matrices from the information matrix of the image have been estimated. The
reconstructed image is synthesized using the estimated detail matrices and
information matrix provided by the Wavelet transform. The quality of the
compressed images has been evaluated using some factors like Compression Ratio
(CR), Peak Signal to Noise Ratio (PSNR), Mean Opinion Score (MOS), Picture
Quality Scale (PQS) etc."
"This document supplements an experimental Jitter / Max/MSP collection of
implementation patches that set its goal to simulate an alchemical process for
a person standing in front of a mirror-like screen while interacting with it.
The work involved takes some patience and has three stages to go through. At
the final stage the ""alchemist"" in the mirror wearing sharp-colored gloves (for
motion tracking) is to extract the final ultimate shining sparkle (FFT-based
visualization) in the nexus of the hands. The more the hands are apart, the
large the sparkle should be. Moving hands around should make the sparkle
follow. To achieve the desired visual effect and the feedback mechanism, the
Jitter lattice-based intensional programming model is used to work on
4-dimensional (A+R+G+B) video matrices and sound signals in order to apply some
well-known alchemical techniques to the video at real-time to get a mirror
effect and accompanying transmutation and transformation stages of the video
based on the stability of the sound produced for some duration of time in
real-time. There is an accompanying video of the result with the interaction
with the tool and the corresponding programming patches."
"This paper addresses the need for geospatial consumers (either humans or
machines) to visualize multicolored elevation contour poly lines with respect
their different contour intervals and control the visual portrayal of the data
with which they work. The current OpenGIS Web Map Service (WMS) specification
supports the ability for an information provider to specify very basic styling
options by advertising a preset collection of visual portrayals for each
available data set. However, while a WMS currently can provide the user with a
choice of style options, the WMS can only tell the user the name of each style.
It cannot tell the user what portrayal will look like on the map. More
importantly, the user has no way of defining their own styling rules. The
ability for a human or machine client to define these rules requires a styling
language that the client and server can both understand. Defining this
language, called the StyledLayerDescriptor (SLD), is the main focus of this
paper, and it can be used to portray the output of Web Map Servers, Web Feature
Servers and Web Coverage Servers."
"This paper presents a method for indexing human ac- tivities in videos
captured from a wearable camera being worn by patients, for studies of
progression of the dementia diseases. Our method aims to produce indexes to
facilitate the navigation throughout the individual video recordings, which
could help doctors search for early signs of the dis- ease in the activities of
daily living. The recorded videos have strong motion and sharp lighting
changes, inducing noise for the analysis. The proposed approach is based on a
two steps analysis. First, we propose a new approach to segment this type of
video, based on apparent motion. Each segment is characterized by two original
motion de- scriptors, as well as color, and audio descriptors. Second, a
Hidden-Markov Model formulation is used to merge the multimodal audio and video
features, and classify the test segments. Experiments show the good properties
of the ap- proach on real data."
"Video transmission over mobile ad-hoc networks is becoming important as these
networks become more widely used in the wireless networks. We propose a
routing-aware multiple description video coding approach to support video
transmission over mobile ad-hoc networks with single and multiple path
transport. We build a model to estimate the packet loss probability of each
packet transmitted over the network based on the standard ad-hoc routing
messages and network parameters without losing the RERR message. We then
calculate the frame loss probability in order to eliminate error without any
loss of data."
"Modern developments in digital media technologies has made transmitting and
storing large amounts of multi/rich media data (e.g. text, images, music, video
and their combination) more feasible and affordable than ever before. However,
the state of the art techniques to process, mining and manage those rich media
are still in their infancy. Advances developments in multimedia acquisition and
storage technology the rapid progress has led to the fast growing incredible
amount of data stored in databases. Useful information to users can be revealed
if these multimedia files are analyzed. Multimedia mining deals with the
extraction of implicit knowledge, multimedia data relationships, or other
patterns not explicitly stored in multimedia files. Also in retrieval, indexing
and classification of multimedia data with efficient information fusion of the
different modalities is essential for the system's overall performance. The
purpose of this paper is to provide a systematic overview of multimedia mining.
This article is also represents the issues in the application process component
for multimedia mining followed by the multimedia mining models."
"The multilayer secured DWT-DCT and YIQ color space based image watermarking
technique with robustness and better correlation is presented here. The
security levels are increased by using multiple pn sequences, Arnold
scrambling, DWT domain, DCT domain and color space conversions. Peak signal to
noise ratio and Normalized correlations are used as measurement metrics. The
512x512 sized color images with different histograms are used for testing and
watermark of size 64x64 is embedded in HL region of DWT and 4x4 DCT is used.
'Haar' wavelet is used for decomposition and direct flexing factor is used. We
got PSNR value is 63.9988 for flexing factor k=1 for Lena image and the maximum
NC 0.9781 for flexing factor k=4 in Q color space. The comparative performance
in Y, I and Q color space is presented. The technique is robust for different
attacks like scaling, compression, rotation etc."
"In this paper, we propose a new multi-layer structural approach for the task
of object based image retrieval. In our work we tackle the problem of
structural organization of local features. The structural features we propose
are nested multi-layered local graphs built upon sets of SURF feature points
with Delaunay triangulation. A Bag-of-Visual-Words (BoVW) framework is applied
on these graphs, giving birth to a Bag-of-Graph-Words representation. The
multi-layer nature of the descriptors consists in scaling from trivial Delaunay
graphs - isolated feature points - by increasing the number of nodes layer by
layer up to graphs with maximal number of nodes. For each layer of graphs its
own visual dictionary is built. The experiments conducted on the SIVAL and
Caltech-101 data sets reveal that the graph features at different layers
exhibit complementary performances on the same content and perform better than
baseline BoVW approach. The combination of all existing layers, yields
significant improvement of the object recognition performance compared to
single level approaches."
"This paper addresses the automatic classification of X-rated videos by
analyzing its obscene sounds. In this paper, obscene sounds refer to audio
signals generated from sexual moans and screams during sexual scenes. By
analyzing various sound samples, we determined the distinguishable
characteristics of obscene sounds and propose a repeated curve-like spectrum
feature that represents the characteristics of such sounds. We constructed
6,269 audio clips to evaluate the proposed feature, and separately constructed
1,200 X-rated and general videos for classification. The proposed feature has
an F1-score, precision, and recall rate of 96.6%, 98.2%, and 95.2%,
respectively, for the original dataset, and 92.6%, 97.6%, and 88.0% for a noisy
dataset of 5dB SNR. And, in classifying videos, the feature has more than a 90%
F1-score, 97% precision, and an 84% recall rate. From the measured performance,
X-rated videos can be classified with only the audio features and the repeated
curve-like spectrum feature is suitable to detect obscene sounds."
"As the rapid progress of the media streaming applications such as video
streaming can be classified into two types of streaming, Live video streaming,
Video on Demand (VoD). Live video streaming is a service which allows the
clients to watch many TV channels over the internet and the clients able to use
one operation to perform is to switch the channels. Video on Demand (VoD) is
one of the most important applications for the internet of the future and has
become an interactive multimedia service which allows the users to start
watching the video of their choice at anytime and anywhere, especially after
the rapid deployment of the wireless networks and mobile devices. In this paper
provide statistical information about the Internet, communications and mobile
devices etc. This has led to an increased demand for the development,
communication and computational powers of many of the mobile wireless
subscribers/mobile devices such as laptops, PDAs, smart phones and notebook.
These techniques are utilized to obtain a video on demand service with higher
resolution and quality. Another objective in this paper is to see Malaysia
ranked as a fully developed country by the year 2020."
"A cost effective, gesture based modelling technique called Virtual
Interactive Prototyping (VIP) is described in this paper. Prototyping is
implemented by projecting a virtual model of the equipment to be prototyped.
Users can interact with the virtual model like the original working equipment.
For capturing and tracking the user interactions with the model image and sound
processing techniques are used. VIP is a flexible and interactive prototyping
method that has much application in ubiquitous computing environments.
Different commercial as well as socio-economic applications and extension to
interactive advertising of VIP are also discussed."
"In this research paper, the authors propose a new approach to digital image
compression using crack coding This method starts with the original image and
develop crack codes in a recursive manner, marking the pixels visited earlier
and expanding the entropy in four directions. The proposed method is
experimented with sample bitmap images and results are tabulated. The method is
implemented in uni-processor machine using C language source code."
"We consider the problem of energy-efficient on-line scheduling for
slice-parallel video decoders on multicore systems. We assume that each of the
processors are Dynamic Voltage Frequency Scaling (DVFS) enabled such that they
can independently trade off performance for power, while taking the video
decoding workload into account. In the past, scheduling and DVFS policies in
multi-core systems have been formulated heuristically due to the inherent
complexity of the on-line multicore scheduling problem. The key contribution of
this report is that we rigorously formulate the problem as a Markov decision
process (MDP), which simultaneously takes into account the on-line scheduling
and per-core DVFS capabilities; the power consumption of the processor cores
and caches; and the loss tolerant and dynamic nature of the video decoder's
traffic. In particular, we model the video traffic using a Direct Acyclic Graph
(DAG) to capture the precedence constraints among frames in a Group of Pictures
(GOP) structure, while also accounting for the fact that frames have different
display/decoding deadlines and non-deterministic decoding complexities. The
objective of the MDP is to minimize long-term power consumption subject to a
minimum Quality of Service (QoS) constraint related to the decoder's
throughput. Although MDPs notoriously suffer from the curse of dimensionality,
we show that, with appropriate simplifications and approximations, the
complexity of the MDP can be mitigated. We implement a slice-parallel version
of H.264 on a multiprocessor ARM (MPARM) virtual platform simulator, which
provides cycle-accurate and bus signal-accurate simulation for different
processors. We use this platform to generate realistic video decoding traces
with which we evaluate the proposed on-line scheduling algorithm in Matlab."
"Multiview video with interactive and smooth view switching at the receiver is
a challenging application with several issues in terms of effective use of
storage and bandwidth resources, reactivity of the system, quality of the
viewing experience and system complexity. The classical decoding system for
generating virtual views first projects a reference or encoded frame to a given
viewpoint and then fills in the holes due to potential occlusions. This last
step still constitutes a complex operation with specific software or hardware
at the receiver and requires a certain quantity of information from the
neighboring frames for insuring consistency between the virtual images. In this
work we propose a new approach that shifts most of the burden due to
interactivity from the decoder to the encoder, by anticipating the navigation
of the decoder and sending auxiliary information that guarantees temporal and
interview consistency. This leads to an additional cost in terms of
transmission rate and storage, which we minimize by using optimization
techniques based on the user behavior modeling. We show by experiments that the
proposed system represents a valid solution for interactive multiview systems
with classical decoders."
"Paper presents the way of transferring stereo images using SMS over GSM
network. Generally, Stereo image is composed of two stereoscopic images in such
way that gives three dimensional affect when viewed. GSM have two short
messaging services, which can transfer images and sounds etc. Such services are
known as; MMS (Multimedia Messaging Service) and EMS (Extended Messaging
Service). EMS can send Predefined sounds, animation and images but have
limitation that it does not support widely. MMS can send much higher contents
than EMS but need 3G and other network capability in order to send large size
data up to 1000 bytes. Other limitations are Portability, content adaption etc.
Our major aim in this paper is to provide an alternative way of sending stereo
images over SMS which is widely supported than EMS. We develop an application
using J2ME Platform."
"Scene mining is a subset of image mining in which scenes are classified to a
distinct set of classes based on analysis of their content. In other word in
scene mining, a label is given to visual content of scene, for example,
mountain, beach. Scene mining is used in applications such as medicine, movie,
information retrieval, computer vision, recognition of traffic scene. Reviewing
of represented methods shows there are various methods in scene mining. Scene
mining applications extension and existence of various scenes, make comparison
of methods hard. Scene mining can be followed by identifying scene mining
components and representing a framework to analyzing and evaluating methods. In
this paper, at first, components of scene mining are introduced, then a
framework based on extracted features of scene is represented to classify scene
mining methods. Finally, these methods are analyzed and evaluated via a
proposal framework."
"Convolution and cross-correlation are the basis of filtering and pattern or
template matching in multimedia signal processing. We propose two throughput
scaling options for any one-dimensional convolution kernel in programmable
processors by adjusting the imprecision (distortion) of computation. Our
approach is based on scalar quantization, followed by two forms of tight
packing in floating-point (one of which is proposed in this paper) that allow
for concurrent calculation of multiple results. We illustrate how our approach
can operate as an optional pre- and post-processing layer for off-the-shelf
optimized convolution routines. This is useful for multimedia applications that
are tolerant to processing imprecision and for cases where the input signals
are inherently noisy (error tolerant multimedia applications). Indicative
experimental results with a digital music matching system and an MPEG-7 audio
descriptor system demonstrate that the proposed approach offers up to 175%
increase in processing throughput against optimized (full-precision)
convolution with virtually no effect in the accuracy of the results. Based on
marginal statistics of the input data, it is also shown how the throughput and
distortion can be adjusted per input block of samples under constraints on the
signal-to-noise ratio against the full-precision convolution."
"One of the difficulties of teaching English is the prosody, including the
stress. French learners have difficulties to encode this information about the
word because it is irrelevant for them. Therefore, they have difficulty to
produce this stress when they speak that language. Studies in this area have
concluded that the dual-coding approach (auditory and visual) of a phonetic
phenomenon helps a lot to improve its perception and memorization for novice
learners. The aim of our work is to provide English teachers with an authoring
named SaCoPh for editing multimedia courses that support this approach. This
course is based on a template that fits the educational aspects of phonetics,
exploiting the features of version 3.0 of the standard SMIL (Synchronized
Multimedia Integration Language) for the publication of this course on the web."
"Insertion of a text message, audio data or/and an image into another image or
3D model is called as a watermarking process. Watermarking has variety of
applications like: Copyright Protection, Owner Identification, Copy Protection
and Data Hiding etc., depending upon the type of watermark insertion algorithm.
Watermark remains in the content after applying various attacks without any
distortions. The blind watermarking method used in the system is based on a
wavelet transform, a fuzzy inference system and a multi-resolution
representation (MRR) of the 3d model. The watermark scrambled by Arnold
Transform is embedded in the wavelet coefficients at third resolution level of
the MRR. Fuzzy logic approach used in the method makes it to approximate the
best possible gain with an accurate scaling factor so that the watermark
remains invisible. The fuzzy input variables are computed for each wavelet
coefficient in the 3D model. The output of the fuzzy system is a single value
which is a perceptual value for each corresponding wavelet coefficient. Thus,
the fuzzy perceptual mask combines all these non-linear variables to build a
simple, easy to use HVS model. Results shows that the system is robust against
affine transformations, smoothing, cropping and noise attacks."
"Mutiscale analysis represents multiresolution scrutiny of a signal to improve
its signal quality. Multiresolution analysis of 1D voice signal and 2D image is
conducted using DCT, FFT and different wavelets such as Haar, Deubachies,
Morlet, Cauchy, Shannon, Biorthogonal, Symmlet and Coiflet deploying the
cascaded filter banks based decomposition and reconstruction. The outstanding
quantitative analysis of the specified wavelets is done to investigate the
signal quality, mean square error, entropy and peak-to-peak SNR at multiscale
stage-4 for both 1D voice signal and 2D image. In addition, the 2D image
compression performance is significantly found 93.00% in DB-4, 93.68% in
bior-4.4, 93.18% in Sym-4 and 92.20% in Coif-2 during the multiscale analysis."
"Multimedia Information security becomes a important part for the
organization's intangible assets. Level of confidence and stakeholder trusted
are performance indicator as successes organization, it is imperative for
organizations to use Information Security Management System (ISMS) to
effectively manage their multimedia information assets. The main objective of
this paper is to Provide a novel practical framework approach to the
development of ISMS, Called by the I-SolFramework, implemented in multimedia
information security architecture (MISA), it divides a problem into six object
domains or six layers, namely organization,stakeholders, tool & technology,
policy, knowledge, and culture. In addition, this framework also introduced
novelty algorithm and mathematic models as measurement and assessment tools of
MISA parameters."
"Despite the fact that Solid State Disk (SSD) data storage media had offered a
revolutionary property storages community, but the unavailability of a
comprehensive allocation strategy in SSDs storage media, leads to consuming the
available space, random writing processes, time-consuming reading processes,
and system resources consumption. In order to overcome these challenges, an
efficient allocation algorithm is a desirable option. In this paper, we had
executed an intensive investigation on the SSD-based allocation algorithms that
had been proposed by the knowledge community. An explanatory comparison had
been made between these algorithms. We reviewed these algorithms in order to
building advanced knowledge armature that would help in inventing new
allocation algorithms for this type of storage media."
"In this paper, the perceptual quality difference between scalable and
single-layer videos coded at the same spatial, temporal and amplitude
resolution (STAR) is investigated through a subjective test using a mobile
platform. Three source videos are considered and for each source video
single-layer and scalable video are compared at 9 different STARs. We utilize
paired comparison methods with and without tie option. Results collected from
10 subjects in the without ""tie"" option and 6 subjects in the with ""tie"" option
show that there is no significant quality difference between scalable and
singlelayer video when coded at the same STAR. An analysis of variance (ANOVA)
test is also performed to further confirm the finding."
"In this paper, we investigate the impact of spatial, temporal and amplitude
resolution (STAR) on the perceptual quality of a compressed video. Subjective
quality tests were carried out on a mobile device. Seven source sequences are
included in the tests and for each source sequence we have 27 test
configurations generated by JSVM encoder (3 QP levels, 3 spatial resolutions,
and 3 temporal resolutions), resulting a total of 189 processed video sequences
(PVSs). Videos coded at different spatial resolutions are displayed at the full
screen size of the mobile platform. Subjective data reveal that the impact of
spatial resolution (SR), temporal resolution (TR) and quantization stepsize
(QS) can each be captured by a function with a single content-dependent
parameter. The joint impact of SR, TR and QS can be accurately modeled by the
product of these three functions with only three parameters. We further find
that the quality decay rates with SR and QS, respectively are independent of
TR, and likewise, the decay rate with TR is independent of SR and QS,
respectively. However, there is a significant interaction between the effects
of SR and QS. The overall quality model is further validated on five other
datasets with very high accuracy. The complete model correlates well with the
subjective ratings with a Pearson Correlation Coefficient (PCC) of 0.991."
"In this paper, we investigate the impacts of spatial, temporal and amplitude
resolution (STAR) on the bit rate of a compressed video. We propose an
analytical rate model in terms of the quantization stepsize, frame size and
frame rate. Experimental results reveal that the increase of the video rate as
the individual resolution increases follows a power function. Hence, the
proposed model expresses the rate as the product of power functions of the
quantization stepsize, frame size and frame rate, respectively. The proposed
rate model is analytically tractable, requiring only four content dependent
parameters. We also propose methods for predicting the model parameters from
content features that can be computed from original video. Simulation results
show that model predicted rates fit the measured data very well with high
Pearson correlation (PC) and small relative root mean square error (RRMSE). The
same model function works for different coding scenarios (including scalable
and non-scalable video, temporal prediction using either hierarchical B or IPPP
structure, etc.) with very high accuracy (average PC $>$ 0.99), but the values
of model parameters differ. Using the proposed rate model and the quality model
introduced in a separate work, we show how to optimize the STAR for a given
rate constraint, which is important for both encoder rate control and scalable
video adaptation. Furthermore, we demonstrate how to order the spatial,
temporal and amplitude layers of a scalable video in a rate-quality optimized
way."
"Most existing image denoising algorithms can only deal with a single type of
noise, which violates the fact that the noisy observed images in practice are
often suffered from more than one type of noise during the process of
acquisition and transmission. In this paper, we propose a new variational
algorithm for mixed Gaussian-impulse noise removal by exploiting image local
consistency and nonlocal consistency simultaneously. Specifically, the local
consistency is measured by a hyper-Laplace prior, enforcing the local
smoothness of images, while the nonlocal consistency is measured by
three-dimensional sparsity of similar blocks, enforcing the nonlocal
self-similarity of natural images. Moreover, a Split-Bregman based technique is
developed to solve the above optimization problem efficiently. Extensive
experiments for mixed Gaussian plus impulse noise show that significant
performance improvements over the current state-of-the-art schemes have been
achieved, which substantiates the effectiveness of the proposed algorithm."
"Nowadays, the use of mobile applications and terminals faces fundamental
challenges related to energy constraint. This is due to the limited battery
lifetime as compared to the increasing hardware evolution. Video streaming is
one of the most energy consuming applications in a mobile system because of its
intensive use of bandwidth, memory and processing power. In this work, we aim
to propose a methodology for building and validating a high level global power
consumption model including a hardware and software elements. Our approach is
based on exploiting the interactions between power consumption sub-models of
standalone systems in the perspective to build more accurate global model. The
interactions are studied within the exclusive context of video streaming
applications that are one of the most used mobile applications."
"We study an interactive live streaming scenario where multiple peers pull
streams of the same free viewpoint video that are synchronized in time but not
necessarily in view. In free viewpoint video, each user can periodically select
a virtual view between two anchor camera views for display. The virtual view is
synthesized using texture and depth videos of the anchor views via
depth-image-based rendering (DIBR). In general, the distortion of the virtual
view increases with the distance to the anchor views, and hence it is
beneficial for a peer to select the closest anchor views for synthesis. On the
other hand, if peers interested in different virtual views are willing to
tolerate larger distortion in using more distant anchor views, they can
collectively share the access cost of common anchor views.
  Given anchor view access cost and synthesized distortion of virtual views
between anchor views, we study the optimization of anchor view allocation for
collaborative peers. We first show that, if the network reconfiguration costs
due to view-switching are negligible, the problem can be optimally solved in
polynomial time using dynamic programming. We then consider the case of
non-negligible reconfiguration costs (e.g., large or frequent view-switching
leading to anchor-view changes). In this case, the view allocation problem
becomes NP-hard. We thus present a locally optimal and centralized allocation
algorithm inspired by Lloyd's algorithm in non-uniform scalar quantization. We
also propose a distributed algorithm with guaranteed convergence where each
peer group independently make merge-and-split decisions with a well-defined
fairness criteria. The results show that depending on the problem settings, our
proposed algorithms achieve respective optimal and close-to-optimal performance
in terms of total cost, and outperform a P2P scheme without collaborative
anchor selection."
"With the rapid development of information technology and multimedia, the use
of digital data is increasing day by day. So it becomes very essential to
protect multimedia information from piracy and also it is challenging. A great
deal of Copyright owners is worried about protecting any kind of illegal
repetition of their information. Hence, facing all these kinds of problems
development of the techniques is very important. Digital watermarking
considered as a solution to prevent the multimedia data. In this paper, an idea
of watermarking is proposed and implemented. In proposed watermarking method,
the original image is rearranged using zigzag sequence and DWT is applied on
rearranged image. Then DCT and SVD are applied on all high bands LH, HL and HH.
Watermark is then embedded by modifying the singular values of these bands.
Extraction of watermark is performed by the inversion of watermark embedding
process. For choosing of these three bands it gives facility of mid-band and
pure high band that ensures good imperceptibility and more robustness against
different kinds of attacks."
"This paper presents a hybrid digital image watermarking based on Discrete
Wavelet Transform (DWT), Discrete Cosine Transform (DCT) and Singular Value
Decomposition (SVD) in a zigzag order. From DWT we choose the high band to
embed the watermark that facilities to add more information, gives more
invisibility and robustness against some attacks. Such as geometric attack.
Zigzag method is applied to map DCT coefficients into four quadrants that
represent low, mid and high bands. Finally, SVD is applied to each quadrant."
"A new technique for data hiding in digital image is proposed in this paper.
Steganography is a well known technique for hiding data in an image, but
generally the format of image plays a pivotal role in it, and the scheme is
format dependent. In this paper we will discuss a new technique where
irrespective of the format of image, we can easily hide a large amount of data
without deteriorating the quality of the image. The data to be hidden is
enciphered with the help of a secret key. This enciphered data is then embedded
at the end of the image. The enciphered data bits are extracted and then
deciphered with the help of same key used for encryption. Simulation results
show that Image Quality Measures of this proposed scheme are better than the
conventional LSB replacing technique. The proposed method is simple and is easy
to implement."
"Steganography derives from the Greek word steganos, meaning covered or
secret, and graphy (writing or drawing). Steganography is a technology where
modern data compression, information theory, spread spectrum, and cryptography
technologies are brought together to satisfy the need for privacy on the
Internet. This paper is an attempt to analyse the various techniques used in
steganography and to identify areas in which this technique can be applied, so
that the human race can be benefited at large."
"This paper proposes a control scheme for the quality-fair delivery of several
encoded video streams to mobile users sharing a common wireless resource. Video
quality fairness, as well as similar delivery delays are targeted among
streams. The proposed controller is implemented within some aggregator located
near the bottleneck of the network. The transmission rate among streams is
adapted based on the quality of the already encoded and buffered packets in the
aggregator. Encoding rate targets are evaluated by the aggregator and fed back
to each remote video server (fully centralized solution), or directly evaluated
by each server in a distributed way (partially distributed solution). Each
encoding rate target is adjusted for each stream independently based on the
corresponding buffer level or buffering delay in the aggregator. Communication
delays between the servers and the aggregator are taken into account. The
transmission and encoding rate control problems are studied with a
control-theoretic perspective. The system is described with a multi-input
multi-output model. Proportional Integral (PI) controllers are used to adjust
the video quality and control the aggregator buffer levels. The system
equilibrium and stability properties are studied. This provides guidelines for
choosing the parameters of the PI controllers. Experimental results show the
convergence of the proposed control system and demonstrate the improvement in
video quality fairness compared to a classical transmission rate fair streaming
solution and to a utility max-min fair approach."
"The Voice over Internet Protocol (VoIP) is becoming a more available and
popular way of communicating for Internet users. This also applies to
Peer-to-Peer (P2P) systems and merging these two have already proven to be
successful (e.g. Skype). Even the existing standards of VoIP provide an
assurance of security and Quality of Service (QoS), however, these features are
usually optional and supported by limited number of implementations. As a
result, the lack of mandatory and widely applicable QoS and security guaranties
makes the contemporary VoIP systems vulnerable to attacks and network
disturbances. In this paper we are facing these issues and propose the SecMon
system, which simultaneously provides a lightweight security mechanism and
improves quality parameters of the call. SecMon is intended specially for VoIP
service over P2P networks and its main advantage is that it provides
authentication, data integrity services, adaptive QoS and (D)DoS attack
detection. Moreover, the SecMon approach represents a low-bandwidth consumption
solution that is transparent to the users and possesses a self-organizing
capability. The above-mentioned features are accomplished mainly by utilizing
two information hiding techniques: digital audio watermarking and network
steganography. These techniques are used to create covert channels that serve
as transport channels for lightweight QoS measurement's results. Furthermore,
these metrics are aggregated in a reputation system that enables best route
path selection in the P2P network. The reputation system helps also to mitigate
(D)DoS attacks, maximize performance and increase transmission efficiency in
the network."
"The paper investigates the possible coherent and effective alternatives to
solve the problems related to the communication needs of any multimedia
product. In essence, the presentation will focus on identifying the issues and
principles governing three types of the design - in fact, the multimedia design
in a broader sense - namely the information design - precisely aiming at ways
of organization and presentation of information in a useful and significant
form, the graphical user interface design, whose sub-domain consists of the
information displayed on the monitor screen and of interactivity between user,
computer and electronic devices, meaning, in fact, everything the user sees,
touches, hears and all the elements with which he interacts, the graphic
design, whose main concern is to create an aesthetic layout arrangement (from
the visual and perceptive) information."
"This paper reviews and refers to the latest telematics technology that has
turned the open system learning and helped it to become an institutional
alternative to the face-to-face traditional one. Most technologies, briefly
presented here, will be implemented in the ""ARTeFACt"" project - telematic
system for vocational education system of open system learning, system which
will be officially launched at the end of 2006, in the institutional offer of
the Faculty of Arts of the University West of Timisoara. The scientific
coordination of the doctoral project ""ARTeFACt"" is done by Mr. Prof. Dr. Eng.
Savi G. George, representing the Department of Mechatronics Faculty of
Mechanical Engineering from the University ""Politehnica"" of Timisoara, Romania"
"This article presents a new concept of a multimedia interactive product. It
is a multiuser versatile platform that can be used for different purposes. The
first implementation of the platform is a multiplayer game called Texas Hold
'em, which is a very popular community card game. The paper shows the product's
multimedia structure where Hardware and Software work together in creating a
realistic feeling for the users."
"The aim of this article is to familiarize the user with the Web publishing of
the files obtained by Flash. The article contains an overview of Macromedia
Flash 5, as well as the running of a Playing Flash movie, information on Flash
and Generator, the publishing of Flash movies, a HTLM publishing for Flash
Player files and publishing by Generator templates."
"This paper proposes the design and development of MPEG 2 Video Decoder to
offer flexible and effective utilization of bandwidth services. The decoder is
capable of decoding the MPEG 2 bit stream on a single host machine. The present
decoder is designed to be simple, but yet effectively reconstruct the video
from MPEG 2 bit stream."
"The objective of this paper is to implement the Trie based Tuple Space
Search(TTSS) packet classification algorithm for Network Processor(NP) based
router to enhance multimedia applications. The performance is evaluated using
Intel IXP2400 NP Simulator. The results demonstrate that, TTSS has better
performance than Tuple Space Search algorithm and is well suited to achieve
high speed packet classification to support multimedia applications."
"This paper is to create a practical steganographic implementation for 4-bit
images.The proposed technique converts 4 bit image into 4 shaded Gray Scale
image. This image will be act as reference image to hide the text. Using this
grey scale reference image any text can be hidden. Single character of a text
can be represented by 8-bit. The 8-bit character can be split into 4X2 bit
information. If the reference image and the data file are transmitted through
network separately, we can achieve the effect of Steganography. Here the image
is not at all distorted because said image is only used for referencing. Any
huge mount of text material can be hidden using a very small image. Decipher
the text is not possible intercepting the image or data file separately. So, it
is more secure."
"In this paper, an efficiently DWT-based watermarking technique is proposed to
embed signatures in images to attest the owner identification and discourage
the unauthorized copying. This paper deals with a fuzzy inference filter to
choose the larger entropy of coefficients to embed watermarks. Unlike most
previous watermarking frameworks which embedded watermarks in the larger
coefficients of inner coarser subbands, the proposed technique is based on
utilizing a context model and fuzzy inference filter by embedding watermarks in
the larger-entropy coefficients of coarser DWT subbands. The proposed
approaches allow us to embed adaptive casting degree of watermarks for
transparency and robustness to the general image-processing attacks such as
smoothing, sharpening, and JPEG compression. The approach has no need the
original host image to extract watermarks. Our schemes have been shown to
provide very good results in both image transparency and robustness."
"This paper presents a novel approach for web video categorization by
leveraging Wikipedia categories (WikiCs) and open resources describing the same
content as the video, i.e., content-duplicated open resources (CDORs). Note
that current approaches only col-lect CDORs within one or a few media forms and
ignore CDORs of other forms. We explore all these resources by utilizing WikiCs
and commercial search engines. Given a web video, its discrimin-ative Wikipedia
concepts are first identified and classified. Then a textual query is
constructed and from which CDORs are collected. Based on these CDORs, we
propose to categorize web videos in the space spanned by WikiCs rather than
that spanned by raw tags. Experimental results demonstrate the effectiveness of
both the proposed CDOR collection method and the WikiC voting catego-rization
algorithm. In addition, the categorization model built based on both WikiCs and
CDORs achieves better performance compared with the models built based on only
one of them as well as state-of-the-art approach."
"A modular method was suggested before to recover a band limited signal from
the sample and hold and linearly interpolated (or, in general, an
nth-order-hold) version of the regular samples. In this paper a novel approach
for compensating the distortion of any interpolation based on modular method
has been proposed. In this method the performance of the modular method is
optimized by adding only some simply calculated coefficients. This approach
causes drastic improvement in terms of SNRs with fewer modules compared to the
classical modular method. Simulation results clearly confirm the improvement of
the proposed method and also its superior robustness against additive noise."
"In this paper, we propose a new image inpainting method based on the property
that much of the image information in the transform domain is sparse. We add a
redundancy to the original image by mapping the transform coefficients with
small amplitudes to zero and the resultant sparsity pattern is used as the side
information in the recovery stage. If the side information is not available,
the receiver has to estimate the sparsity pattern. At the end, the recovery is
done by consecutive projecting between two spatial and transform sets.
Experimental results show that our method works well for both structural and
texture images and outperforms other techniques in objective and subjective
performance measures."
"We investigate the impact of cooperative relaying on uplink and downlink
multi-user (MU) wireless video transmissions. The objective is to maximize the
long-term sum of utilities across the video terminals in a decentralized
fashion, by jointly optimizing the packet scheduling, the resource allocation,
and the cooperation decisions, under the assumption that some nodes are willing
to act as cooperative relays. A pricing-based distributed resource allocation
framework is adopted, where the price reflects the expected future congestion
in the network. Specifically, we formulate the wireless video transmission
problem as an MU Markov decision process (MDP) that explicitly considers the
cooperation at the physical layer and the medium access control sublayer, the
video users' heterogeneous traffic characteristics, the dynamically varying
network conditions, and the coupling among the users' transmission strategies
across time due to the shared wireless resource. Although MDPs notoriously
suffer from the curse of dimensionality, our study shows that, with appropriate
simplications and approximations, the complexity of the MU-MDP can be
significantly mitigated. Our simulation results demonstrate that integrating
cooperative decisions into the MU-MDP optimization can increase the resource
price in networks that only support low transmission rates and can decrease the
price in networks that support high transmission rates. Additionally, our
results show that cooperation allows users with feeble direct signals to
achieve improvements in video quality on the order of 5-10 dB peak
signal-to-noise ratio (PSNR), with less than 0.8 dB quality loss by users with
strong direct signals, and with a moderate increase in total network energy
consumption that is significantly less than the energy that a distant node
would require to achieve an equivalent PSNR without exploiting cooperative
diversity."
"The abundance of multimedia data and information is challenging educators to
effectively search, browse, access, use, and store the data for their classroom
teaching. However, many educators could still be accustomed to teaching or
searching for information using conventional methods, but often the
conventional methods may not function well with multimedia data. Educators need
to efficiently interact and manage a variety of digital media files too. The
purpose of this study is to review current multimedia database applications in
teaching and learning, and further discuss some of the issues or concerns that
educators may have while incorporating multimedia data into their classrooms.
Some strategies and recommendations are also provided in order for educators to
be able to use multimedia data more effectively in their teaching environments."
"With the development of multimedia data types and available bandwidth there
is huge demand of video retrieval systems, as users shift from text based
retrieval systems to content based retrieval systems. Selection of extracted
features play an important role in content based video retrieval regardless of
video attributes being under consideration. These features are intended for
selecting, indexing and ranking according to their potential interest to the
user. Good features selection also allows the time and space costs of the
retrieval process to be reduced. This survey reviews the interesting features
that can be extracted from video data for indexing and retrieval along with
similarity measurement methods. We also identify present research issues in
area of content based video retrieval systems."
"The art of information hiding has been around nearly as long as the need for
covert communication. Steganography, the concealing of information, arose early
on as an extremely useful method for covert information transmission.
Steganography is the art of hiding secret message within a larger image or
message such that the hidden message or an image is undetectable; this is in
contrast to cryptography, where the existence of the message itself is not
disguised, but the content is obscure. The goal of a steganographic method is
to minimize the visually apparent and statistical differences between the cover
data and a steganogram while maximizing the size of the payload. Current
digital image steganography presents the challenge of hiding message in a
digital image in a way that is robust to image manipulation and attack. This
paper explains about how a secret message can be hidden into an image using
least significant bit insertion method along with chaos."
"The Internet as a whole does not use secure links, thus information in
transit may be vulnerable to interruption as well. The important of reducing a
chance of the information being detected during the transmission is being an
issue in the real world now days. The Digital watermarking method provides for
the quick and inexpensive distribution of digital information over the
Internet. This method provides new ways of ensuring the sufficient protection
of copyright holders in the intellectual property dispersion process. The
property of digital watermarking images allows insertion of additional data in
the image without altering the value of the image.In this paper investigate the
following relevant concepts and terminology, history of watermarks and the
properties of a watermarking system and applications. We are proposing edge
detection using Gabor Filters. In this paper we are proposed least significant
bit (LSB) substitution method to encrypt the message in the watermark image
file. The benefits of the LSB are its simplicity to embed the bits of the
message directly into the LSB plane of cover-image and many techniques using
these methods. The LSB does not result in a human perceptible difference
because the amplitude of the change is little therefore the human eye the
resulting stego image will look identical to the cover image and this allows
high perceptual transparency of the LSB. The spatial domain technique LSB
substitution it would be able to use a pseudo-random number generator to
determine the pixels to be used for embedding based on a given key. We are
using DCT transform watermark algorithms based on robustness. The watermarking
robustness have been calculated by the Peak Signal to Noise Ratio (PSNR) and
Normalized cross correlation (NC) is used to quantify by the similarity between
the real watermark and after extracting watermark."
"This paper develops a new video compression approach based on underdetermined
blind source separation. Underdetermined blind source separation, which can be
used to efficiently enhance the video compression ratio, is combined with
various off-the-shelf codecs in this paper. Combining with MPEG-2, video
compression ratio could be improved slightly more than 33%. As for combing with
H.264, 4X~12X more compression ratio could be achieved with acceptable PSNR,
according to different kinds of video sequences."
"This paper gives a new scheme of watermarking technique related to insert the
mark by adding edge in HH sub-band of the host image after wavelet
decomposition. Contrary to most of the watermarking algorithms in wavelet
domain, our method is blind and results show that it is robust against the JPEG
and GIF compression, histogram and spectrum spreading, noise adding and small
rotation. Its robustness against compression is better than others watermarking
algorithms reported in the literature. The algorithm is flexible because its
capacity or robustness can be improved by modifying some parameters."
"Emerging applications in multiview streaming look for providing interactive
navigation services to video players. The user can ask for information from any
viewpoint with a minimum transmission delay. The purpose is to provide user
with as much information as possible with least number of redundancies. The
recent concept of navigation segment representation consists of regrouping a
given number of viewpoints in one signal and transmitting them to the users
according to their navigation path. The question of the best description
strategy of these navigation segments is however still open. In this paper, we
propose to represent and code navigation segments by a method that extends the
recent layered depth image (LDI) format. It consists of describing the scene
from a viewpoint with multiple images organized in layers corresponding to the
different levels of occluded objects. The notion of extended LDI comes from the
fact that the size of this image is adapted to take into account the sides of
the scene also, in contrary to classical LDI. The obtained results show a
significant rate-distortion gain compared to classical multiview compression
approaches in navigation scenario."
"The Color to Gray and Back transformation watermarking with a secrete key is
considered. Color is embedded into the bit planes of the luminosity component
of the YUV color space with the help of a block algorithm that allows using not
only the least significant bits. An application of the problem of distributing
color digital images from a data base among legitimate users is discussed. The
proposed protocol can protect original images from unauthorized copying."
"When 60 de-interlaced fields per second digital uncompressed video is
streamed to a computer, some video fields are lost and not able to be stored on
a computer s hard drive successfully. Additionally, this problem amplifies once
multiple video sources are deployed. If it is possible to stream digital
uncompressed video without dropped video fields, then a sophisticated computer
analysis of the transmitted via IEEE 1394a connection video is possible. Such
process is used in biomechanics when it is important to analyze athletes
performance via streaming digital uncompressed video to a computer and then
analyzing it. If a loss of video fields occurs, then a quality analysis of
video is not possible."
"Recently, multi-screen cloud social TV is invented to transform TV into
social experience. People watching the same content on social TV may come from
different locations, while freely interact with each other through text, image,
audio and video. This crucial virtual living-room experience adds social
aspects into existing performance metrics. In this paper, we parse social TV
user experience into three elements (i.e., inter-user delay, video quality of
experience (QoE), and resource efficiency), and provide a joint analytical
framework to enhance user experience. Specifically, we propose a cloud-based
optimal playback rate allocation scheme to maximize the overall QoE while upper
bounding inter-user delay. Experiment results show that our algorithm achieves
near-optimal tradeoff between inter-user delay and video quality, and
demonstrates resilient performance even under very fast wireless channel
fading."
"Good quality video coding for low bit-rate applications is important for
transmission over narrow-bandwidth channels and for storage with limited memory
capacity. In this work, we develop a previous analysis for image compression at
low bit-rates to adapt it to video signals. Improving compression using
down-scaling in the spatial and temporal dimensions is examined. We show, both
theoretically and experimentally, that at low bit-rates, we benefit from
applying spatio-temporal scaling. The proposed method includes down-scaling
before the compression and a corresponding up-scaling afterwards, while the
codec itself is left unmodified. We propose analytic models for low bit-rate
compression and spatio-temporal scaling operations. Specifically, we use
theoretic models of motion-compensated prediction of available and absent
frames as in coding and frame-rate up-conversion (FRUC) applications,
respectively. The proposed models are designed for multi-resolution analysis.
In addition, we formulate a bit-allocation procedure and propose a method for
estimating good down-scaling factors of a given video based on its second-order
statistics and the given bit-budget. We validate our model with experimental
results of H.264 compression."
"Recent advances in computing, communication, and data storage have led to an
increasing number of large digital libraries publicly available on the
Internet. Main problem of content-based video retrieval is inferring semantics
from raw video data. Video data play an important role in these libraries.
Instead of words, a video retrieval system deals with collections of video
records. Therefore, the system is confronted with the problem of video
understanding. Because machine understanding of the video data is still an
unsolved research problem, text annotations are usually used to describe the
content of video data according to the annotator's understanding and the
purpose of that video data. Most of proposed systems for video annotation are
domain dependent. In addition, in many of these systems, an important feature
of video data, temporality, is disregarded. In this paper, we proposed a
framework for video temporal annotation. The proposed system uses domain
knowledge and a time ontology to perform temporal annotation of input video."
"Video Watermarking serves as a new technology mainly used to provide security
to the illegal distribution of digital video over the web. The purpose of any
video watermarking scheme is to embed extra information into video in such a
way that must be perceptually undetectable while still holding enough
information in order to extract the watermark beginning with the resultant
video. Information which is embedded within the original image is a Digital
Watermark, which could be visible or invisible. To improved more security,
embedding and extraction Watermark process should be complex against attackers.
Recent research indicates SVD (Singular Value Decomposition) algorithms are
employed owing to their simple scheme with mathematical function. In this
proposed work an advanced SVD transformation algorithm is used for embedding
and extraction process. Experimental results show proposed watermarking process
is more secured than existing SVD approach."
"Although techniques for separate image and audio steganography are widely
known, relatively little has been described concerning the hiding of
information within video streams (""video steganography""). In this paper we
review the current state of the art in this field, and describe the key issues
we have encountered in developing a practical video steganography system. A
supporting video is also available online at
http://www.youtube.com/watch?v=YhnlHmZolRM"
"We propose a novel scheme for watermarking of digital images based on
singular value decomposition (SVD), which makes use of the fact that the SVD
subspace preserves significant amount of information of an image, as compared
to its singular value matrix, Zhang and Li (2005). The principal components of
the watermark are embedded in the original image, leaving the detector with a
complimentary set of singular vectors for watermark extraction. The above step
invariably ensures that watermark extraction from the embedded watermark image,
using a modified matrix, is not possible, thereby removing a major drawback of
an earlier proposed algorithm by Liu and Tan (2002)."
"One of the current challenges of Information Systems is to ensure
semi-structured data transmission, such as multimedia data, in a distributed
and pervasive environment. Information Sytems must then guarantee users a
quality of service ensuring data accessibility whatever the hardware and
network conditions may be. They must also guarantee information coherence and
particularly intelligibility that imposes a personalization of the service.
Within this framework, we propose a design method based on original models of
multimedia applications and quality of service. We also define a supervision
platform Kalinahia using a user centered heuristic allowing us to define at any
moment which configuration of software components constitutes the best answers
to users' wishes in terms of service."
"Needs of multimedia systems evolved due to the evolution of their
architecture which is now distributed into heterogeneous contexts. A critical
issue lies in the fact that they handle, process, and transmit multimedia data.
This data integrates several properties which should be considered since it
holds a considerable part of its semantics, for instance the lips
synchronization in a video. In this paper, we focus on the definition of a
model as a basic abstraction for describing and modeling media in multimedia
systems by taking into account their properties. This model will be used in
software architecture in order to handle data in efficient way. The provided
model is an interesting solution for the integration of media into
applications; we propose to consider and to handle them in a uniform way. This
model is proposed with synchronization policies to ensure synchronous transport
of media. Therefore, we use it in a component model that we develop for the
design and deployment of distributed multimedia systems."
"Resource-constrained embedded and mobile devices are becoming increasingly
common. Since few years, some mobile and ubiquitous devices such as wireless
sensor, able to be aware of their physical environment, appeared. Such devices
enable proposing applications which adapt to user's need according the context
evolution. It implies the collaboration of sensors and software components
which differ on their nature and their communication mechanisms. This paper
proposes a unified component model in order to easily design applications based
on software components and sensors without taking care of their nature. Then it
presents a state of the art of communication problems linked to heterogeneous
components and proposes an interaction mechanism which ensures information
exchanges between wireless sensors and software components."
"In this paper, we formulate the collaborative multi-user wireless video
transmission problem as a multi-user Markov decision process (MUMDP) by
explicitly considering the users' heterogeneous video traffic characteristics,
time-varying network conditions and the resulting dynamic coupling between the
wireless users. These environment dynamics are often ignored in existing
multi-user video transmission solutions. To comply with the decentralized
nature of wireless networks, we propose to decompose the MUMDP into local MDPs
using Lagrangian relaxation. Unlike in conventional multi-user video
transmission solutions stemming from the network utility maximization
framework, the proposed decomposition enables each wireless user to
individually solve its own dynamic cross-layer optimization (i.e. the local
MDP) and the network coordinator to update the Lagrangian multipliers (i.e.
resource prices) based on not only current, but also future resource needs of
all users, such that the long-term video quality of all users is maximized.
However, solving the MUMDP requires statistical knowledge of the experienced
environment dynamics, which is often unavailable before transmission time. To
overcome this obstacle, we then propose a novel online learning algorithm,
which allows the wireless users to update their policies in multiple states
during one time slot. This is different from conventional learning solutions,
which often update one state per time slot. The proposed learning algorithm can
significantly improve the learning performance, thereby dramatically improving
the video quality experienced by the wireless users over time. Our simulation
results demonstrate the efficiency of the proposed MUMDP framework as compared
to conventional multi-user video transmission solutions."
"We propose a new approach for image compression in digital cameras, where the
goal is to achieve better quality at a given rate by using the characteristics
of a Bayer color filter array. Most digital cameras produce color images by
using a single CCD plate, so that each pixel in an image has only one color
component and therefore an interpolation method is needed to produce a full
color image. After the image processing stage, in order to reduce the memory
requirements of the camera, a lossless or lossy compression stage often
follows. But in this scheme, before decreasing redundancy through compression,
redundancy is increased in an interpolation stage. In order to avoid increasing
the redundancy before compression, we propose algorithms for image compression
in which the order of the compression and interpolation stages is reversed. We
introduce image transform algorithms, since non interpolated images cannot be
directly compressed with general image coders. The simulation results show that
our algorithm outperforms conventional methods with various color interpolation
methods in a wide range of compression ratios. Our proposed algorithm provides
not only better quality but also lower encoding complexity because the amount
of luminance data used is only half of that in conventional methods."
"This paper is focused on the presentation of Virtual Reality principles
together with the main implementation methods and techniques. An overview of
the main development directions is included."
"In this paper, we propose a novel approach to group users according to the
VoD user request pattern. We cluster the user requests based on ART1 neural
network algorithm. The knowledge extracted from the cluster is used to prefetch
the multimedia object from each cluster before the users request. We have
developed an algorithm to cluster users according to the users request patterns
based on ART1 neural network algorithm that offers an unsupervised clustering.
This approach adapts to changes in user request patterns over period without
losing previous information. Each cluster is represented as prototype vector by
generalizing the most frequently used URLs that are accessed by all the cluster
members. The simulation results of our proposed clustering and prefetching
algorithm, shows enormous increase in the performance of streaming server. Our
algorithm helps the servers agent to learn user preferences and discover the
information about the corresponding sources and other similar interested
individuals."
"Dance videos are interesting and semantics-intensive. At the same time, they
are the complex type of videos compared to all other types such as sports, news
and movie videos. In fact, dance video is the one which is less explored by the
researchers across the globe. Dance videos exhibit rich semantics such as macro
features and micro features and can be classified into several types. Hence,
the conceptual modeling of the expressive semantics of the dance videos is very
crucial and complex. This paper presents a generic Dance Video Semantics Model
(DVSM) in order to represent the semantics of the dance videos at different
granularity levels, identified by the components of the accompanying song. This
model incorporates both syntactic and semantic features of the videos and
introduces a new entity type called, Agent, to specify the micro features of
the dance videos. The instantiations of the model are expressed as graphs. The
model is implemented as a tool using J2SE and JMF to annotate the macro and
micro features of the dance videos. Finally examples and evaluation results are
provided to depict the effectiveness of the proposed dance video model.
Keywords: Agents, Dance videos, Macro features, Micro features, Video
annotation, Video semantics."
"Educational media mining is the process of converting raw media data from
educational systems to useful information that can be used to design learning
systems, answer research questions and allow personalized learning experiences.
Knowledge discovery encompasses a wide range of techniques ranging from
database queries to more recent developments in machine learning and language
technology. Educational media mining techniques are now being used in IT
Services research worldwide. Multi-modal Lecture Recordings is one of the
important types of educational media and this paper explores the research
challenges for mining lecture recordings for the efficient personalized
learning experiences. Keywords: Educational Media Mining; Lecture Recordings,
Multimodal Information System, Personalized Learning; Online Course Ware;
Skills and Competences;"
"Admission control is a key component in multimedia servers, which will allow
the resources to be used by the client only when they are available. A problem
faced by numerous content serving machines is overload, when there are too many
clients who need to be served, the server tends to slow down. An admission
control algorithm for a multimedia server is responsible for determining if a
new request can be accepted without violating the QoS requirements of the
existing requests in the system. By caching and streaming only the data in the
interval between two successive requests on the same object, the following
request can be serviced directly from the buffer cache without disk operations
and within the deadline of the request. An admission control strategy based on
Popularity-aware interval caching for Prefix [3] scheme extends the interval
caching by considering different popularity of multimedia objects. The method
of Prefix caching with multicast transmission of popular objects utilizes the
hard disk and network bandwidth efficiently and increases the number of
requests being served."
"- The aim of this paper is to propose a novel Voice On Demand (VoD)
architecture and implementation of an efficient load sharing algorithm to
achieve Quality of Service (QoS). This scheme reduces the transmission cost
from the Centralized Multimedia Se"
"In this paper we have proposed an adaptive dynamic cache replacement
algorithm for a multimedia servers cache system. The goal is to achieve an
effective utilization of the cache memory which stores the prefix of popular
videos. A replacement policy is usually evaluated using hit ratio, the
frequency with which any video is requested. Usually discarding the least
recently used page is the policy of choice in cache management. The adaptive
dynamic replacement approach for prefix cache is a self tuning, low overhead
algorithm that responds online to changing access patterns. It constantly
balances between lru and lfu to improve combined result. It automatically
adapts to evolving workloads. Since in our algorithm we have considered a
prefix caching with multicast transmission of popular objects it utilizes the
hard disk and network bandwidth efficiently and increases the number of
requests being served."
"As the multimedia and internet technologies are growing fast, the
transmission of digital media plays an important role in communication. The
various digital media like audio, video and images are being transferred
through internet. There are a lot of threats for the digital data that are
transferred through internet. Also, a number of security techniques have been
employed to protect the data that is transferred through internet. This paper
proposes a new technique for sending secret messages securely, using
steganographic technique. Since the proposed system uses multiple level of
security for data hiding, where the data is hidden in an image file and the
stego file is again concealed in another image. Previously, the secret message
is being encrypted with the encryption algorithm which ensures the achievement
of high security enabled data transfer through internet."
"...The steganography scheme makes it possible to hide the medical image in
different bit locations of host media without inviting suspicion. The Secret
file is embedded in a cover media with a key. At the receiving end the key can
be derived by all the classes which are higher in the hierarchy using symmetric
polynomial and the medical image file can be retrieved. The system is
implemented and found to be secure, fast and scalable. Simulation results show
that the system is dynamic in nature and allows any type of hierarchy. The
proposed approach performs better even during frequent member joins and leaves.
The computation cost is reduced as the same algorithm is used for key
computation and descendant key derivation. Steganographic technique used in
this paper does not use the conventional LSB's and uses two bit positions and
the hidden data occurs only from a frame which is dictated by the key that is
used. Hence the quality of stego data is improved."
"There has been a remarkable increase in the data exchange over web and the
widespread use of digital media. As a result, multimedia data transfers also
had a boost up. The mounting interest with reference to digital watermarking
throughout the last decade is certainly due to the increase in the need of
copyright protection of digital content. This is also enhanced due to
commercial prospective. Applications of video watermarking in copy control,
broadcast monitoring, fingerprinting, video authentication, copyright
protection etc is immensely rising. The main aspects of information hiding are
capacity, security and robustness. Capacity deals with the amount of
information that can be hidden. The skill of anyone detecting the information
is security and robustness refers to the resistance to modification of the
cover content before concealed information is destroyed. Video watermarking
algorithms normally prefers robustness. In a robust algorithm it is not
possible to eliminate the watermark without rigorous degradation of the cover
content. In this paper, we introduce the notion of Video Watermarking and the
features required to design a robust watermarked video for a valuable
application. We review several algorithms, and introduce frequently used key
techniques. The aim of this paper is to focus on the various domains of video
watermarking techniques. The majority of the reviewed methods based on video
watermarking emphasize on the notion of robustness of the algorithm."
"A method of lossless data hiding in images using integer wavelet transform
and histogram shifting for gray scale images is proposed. The method shifts
part of the histogram, to create space for embedding the watermark information
bits. The method embeds watermark while maintaining the visual quality well.
The method is completely reversible. The original image and the watermark data
can be recovered without any loss."
"Just about all the newest living room audio-video electronics and PC
multimedia products being designed today will incorporate some form of
compressed digitized-audio processing capability. Audio compression reduces the
bit rate required to represent an analog audio signal while maintaining the
perceived audio quality. Discarding inaudible data reduces the storage,
transmission and compute requirements of handling high-quality audio files.
This paper covers wave audio file format & algorithm of silence compression
method and companding method to compress and decompress wave audio file. Then
it compares the result of these two methods."
"There are different Standards of digital multimedia transmission, for example
DVB in Europe and ISDB in Japan and DMB in Korea, with different delivery
system (example MPEG-2, MPEG-4).This paper describe an overview of Digital
Multimedia Transmission (DMT) technologies. The economic aspects of digital
content & software solution industry as a strategic key in the future will be
discussed. The study then focuses on some important policy and technology
issues, such S-DMB, T-DMB, Digital Video Broadcasting Handheld (DVB-H) and
concludes DMT policies for convergence of telecommunications and broadcasting."
"We propose a novel error concealment algorithm to be used at the receiver
side of a lossy image transmission system. Our algorithm involves hiding the
edge map of the original image at the transmitter within itself using a robust
watermarking scheme. At the receiver, wherever a lost block is detected, the
extracted edge information is used as border constraint for the spatial
smoothing employing the intact neighboring blocks in order to conceal errors.
Simulation results show the superiority of our technique over existing methods
even in case of high packet loss ratios in the communication network."
"Speaker identification is the process of determining which registered speaker
provides a given utterance. Speaker identification required to make a claim on
the identity of speaker from the Ns trained speaker in its user database. In
this study, we propose the combination of clustering algorithm and the
classification technique - subtractive and Radial Basis Function (RBF). The
proposed technique is chosen because RBF is a simpler network structures and
faster learning algorithm. RBF finds the input to output map using the local
approximators which will combine the linear of the approximators and cause the
linear combiner have few weights. Besides that, RBF neural network model using
subtractive clustering algorithm for selecting the hidden node centers, which
can achieve faster training speed. In the meantime, the RBF network was trained
with a regularization term so as to minimize the variances of the nodes in the
hidden layer and perform more accu-rate prediction."
"Video fusion is a process that combines visual data from different sensors to
obtain a single composite video preserving the information of the sources. The
availability of a system, enhancing human ability to perceive the observed
scenario, is crucial to improve the performance of a surveillance system. The
infrared (IR) camera captures thermal image of object in night-time
environment, when only limited visual information can be captured by RGB
camera. The fusion of data recorded by an IR sensor and a visible RGB camera
can produce information otherwise not obtainable by viewing the sensor outputs
separately. In this paper we consider the problem of fusing two video streams
acquired by an RGB camera and an IR sensor. The pedestrians, distinctly
captured by IR video, are separated and fused with the RGB video. The
algorithms implemented involve estimation of the background, followed by
detection of object from the IR Video, after necessary denoising. Finally a
suitable fusion algorithm is employed to combine the extracted pedestrians with
the visual output. The obtained results clearly demonstrate the effectiveness
of the proposed video fusion scheme, for night vision."
"The goal of this paper is to design compact support basis spline functions
that best approximate a given filter (e.g., an ideal Lowpass filter). The
optimum function is found by minimizing the least square problem ($\ell$2 norm
of the difference between the desired and the approximated filters) by means of
the calculus of variation; more precisely, the introduced splines give optimal
filtering properties with respect to their time support interval. Both
mathematical analysis and simulation results confirm the superiority of these
splines."
"Information system designers face many challenges w.r.t. selecting
appropriate semantic technologies and deciding on a modelling approach for
their system. However, there is no clear methodology yet to evaluate
""semantically enriched"" information systems. In this paper we present a case
study on different modelling approaches for annotating medical images and
introduce a conceptual framework that can be used to analyse the fitness of
information systems and help designers to spot the strengths and weaknesses of
various modelling approaches as well as managing trade-offs between modelling
effort and their potential benefits."
"Most of the well known algorithms for watermarking of digital images involve
transformation of the image data to Fourier or singular vector space. In this
paper, we introduce watermarking in Hilbert transform domain for digital media.
Generally, if the image is a matrix of order $m$ by $n$, then the transformed
space is also an image of the same order. However, with Hilbert transforms, the
transformed space is of order $2m$ by $2n$. This allows for more latitude in
storing the watermark in the host image. Based on this idea, we propose an
algorithm for embedding and extracting watermark in a host image and
analytically obtain a parameter related to this procedure. Using extensive
simulations, we show that the algorithm performs well even if the host image is
corrupted by various attacks."
"Sterilization is a very popular word used in biomedical testing (like removal
of all microorganisms on surface of an article or in fluid using appropriate
chemical products). Motivated by this biological analogy, we, for the first
time, introduce the concept of sterilization of an image, i.e., removing any
steganographic information embedded in the image. Experimental results show
that our technique succeeded in sterilizing around 76% to 91% of stego pixels
in an image on average, where data is embedded using LSB-based steganography."
"Interdisciplinary collaboration is essential for the advance of research. As
domain subjects become more and more specialized, researchers need to cross
disciplines for insights from peers in other areas to have a broader and deeper
understand of a topic at micro- and macro-levels. We developed a 3D virtual
learning environment that served as a platform for faculty to plan curriculum,
share educational beliefs, and conduct cross-discipline research for effective
learning. Based upon the scripts designed by faculty from five disciplines,
virtual doctors, nurses, or patients interact in a 3D virtual hospital. The
teaching vignettes were then converted to video clips, allowing users to view,
pause, replay, or comment on the videos individually or in groups. Unlike many
existing platforms, we anticipated a value-added by adding a social networking
capacity to this virtual environment. The focus of this paper is on the
cost-efficiency and system design of the virtual learning environment."
"Copyright protection has become a need in today's world. To achieve a secure
copyright protection we embedded some information in images and videos and that
image or video is called copyright protected. The embedded information can't be
detected by human eye but some attacks and operations can tamper that
information to breach protection. So in order to find a secure technique of
copyright protection, we have analyzed image processing techniques i.e. Spatial
Domain (Least Significant Bit (LSB)), Transform Domain (Discrete Cosine
Transform (DCT)), Discrete Wavelet Transform (DWT) and there are numerous
algorithm for watermarking using them. After having a good understanding of the
same we have proposed a novel algorithm named as Stage Staffing Algorithm that
generates results with high effectiveness, additionally we can use self
extracted-watermark technique to increase the security and automate the process
of watermark image. The proposed algorithm provides protection in three stages.
We have implemented the algorithm and results of the simulations are shown. The
various factors affecting spatial domain watermarking are also discussed."
"Distributed Video Coding (DVC) is a new coding paradigm for video
compression, based on Slepian- Wolf (lossless coding) and Wyner-Ziv (lossy
coding) information theoretic results. DVC is useful for emerging applications
such as wireless video cameras, wireless low-power surveillance networks and
disposable video cameras for medical applications etc. The primary objective of
DVC is low-complexity video encoding, where bulk of computation is shifted to
the decoder, as opposed to low-complexity decoder in conventional video
compression standards such as H.264 and MPEG etc. There are couple of early
architectures and implementations of DVC from Stanford University[2][3] in
2002, Berkeley University PRISM (Power-efficient, Robust, hIgh-compression,
Syndrome-based Multimedia coding)[4][5] in 2002 and European project DISCOVER
(DIStributed COding for Video SERvices)[6] in 2007. Primarily there are two
types of DVC techniques namely pixel domain and transform domain based.
Transform domain design will have better rate-distortion (RD) performance as it
exploits spatial correlation between neighbouring samples and compacts the
block energy into as few transform coefficients as possible (aka energy
compaction). In this paper, architecture, implementation details and ""C"" model
results of our transform domain DVC are presented."
"In this paper, we investigate the problem of designing compact support
interpolation kernels for a given class of signals. By using calculus of
variations, we simplify the optimization problem from an infinite nonlinear
problem to a finite dimensional linear case, and then find the optimum compact
support function that best approximates a given filter in the least square
sense (l2 norm). The benefit of compact support interpolants is the low
computational complexity in the interpolation process while the optimum compact
support interpolant gaurantees the highest achivable Signal to Noise Ratio
(SNR). Our simulation results confirm the superior performance of the proposed
splines compared to other conventional compact support interpolants such as
cubic spline."
"In this report, I surveyed the cognitive radio technique in wireless
networks. Researched several kinds of cognitive techniques about their
advantages and disadvantages."
"Sign language recognition is a difficult task, yet required for many
applications in real-time speed. Using RGB cameras for recognition of sign
languages is not very successful in practical situations and accurate 3D
imaging requires expensive and complex instruments. With introduction of
Time-of-Flight (ToF) depth cameras in recent years, it has become easier to
scan the environment for accurate, yet fast depth images of the objects without
the need of any extra calibrating object. In this paper, a robust system for
sign language recognition using ToF depth cameras is presented for converting
the recorded signs to a standard and portable XML sign language named SiGML for
easy transferring and converting to real-time 3D virtual characters animations.
Feature extraction using moments and classification using nearest neighbor
classifier are used to track hand gestures and significant result of 100% is
achieved for the proposed approach."
"This paper presents a new paradigm for image transmission through analog
error correction codes. Conventional schemes rely on digitizing images through
quantization (which inevitably causes significant bandwidth expansion) and
transmitting binary bit-streams through digital error correction codes (which
do not automatically differentiate the different levels of significance among
the bits). To strike a better overall performance in terms of transmission
efficiency and quality, we propose to use a single analog error correction code
in lieu of digital quantization, digital code and digital modulation. The key
is to get analog coding right. We show that this can be achieved by cleverly
exploiting an elegant ""butterfly"" property of chaotic systems. Specifically, we
demonstrate a tail-biting triple-branch baker's map code and its
maximum-likelihood decoding algorithm. Simulations show that the proposed
analog code can actually outperform digital turbo code, one of the best codes
known to date. The results and findings discussed in this paper speak volume
for the promising potential of analog codes, in spite of their rather short
history."
"Due to the advances in hardware technology and increase in production of
multimedia data in many applications, during the last decades, multimedia
databases have become increasingly important. Contentbased multimedia retrieval
is one of an important research area in the field of multimedia databases. Lots
of research on this field has led to proposition of different kinds of index
structures to support fast and efficient similarity search to retrieve
multimedia data from these databases. Due to variety and plenty of proposed
index structures, we suggest a systematic framework based on partitioning
method used in these structures to classify multimedia index structures, and
then we evaluated these structures based on important functional measures. We
hope this proposed framework will lead to empirical and technical comparison of
multimedia index structures and development of more efficient structures at
future."
"Many tasks in music information retrieval, such as recommendation, and
playlist generation for online radio, fall naturally into the query-by-example
setting, wherein a user queries the system by providing a song, and the system
responds with a list of relevant or similar song recommendations. Such
applications ultimately depend on the notion of similarity between items to
produce high-quality results. Current state-of-the-art systems employ
collaborative filter methods to represent musical items, effectively comparing
items in terms of their constituent users. While collaborative filter
techniques perform well when historical data is available for each item, their
reliance on historical data impedes performance on novel or unpopular items. To
combat this problem, practitioners rely on content-based similarity, which
naturally extends to novel items, but is typically out-performed by
collaborative filter methods.
  In this article, we propose a method for optimizing contentbased similarity
by learning from a sample of collaborative filter data. The optimized
content-based similarity metric can then be applied to answer queries on novel
and unpopular items, while still maintaining high recommendation accuracy. The
proposed system yields accurate and efficient representations of audio content,
and experimental results show significant improvements in accuracy over
competing content-based recommendation techniques."
"In this paper, we suggest a general model for the fixed-valued impulse noise
and propose a two-stage method for high density noise suppression while
preserving the image details. In the first stage, we apply an iterative impulse
detector, exploiting the image entropy, to identify the corrupted pixels and
then employ an Adaptive Iterative Mean filter to restore them. The filter is
adaptive in terms of the number of iterations, which is different for each
noisy pixel, according to the Euclidean distance from the nearest uncorrupted
pixel. Experimental results show that the proposed filter is fast and
outperforms the best existing techniques in both objective and subjective
performance measures."
"In this paper, we attempt to revisit the problem of multi-party conferencing
from a practical perspective, and to rethink the design space involved in this
problem. We believe that an emphasis on low end-to-end delays between any two
parties in the conference is a must, and the source sending rate in a session
should adapt to bandwidth availability and congestion. We present Celerity, a
multi-party conferencing solution specifically designed to achieve our
objectives. It is entirely Peer-to-Peer (P2P), and as such eliminating the cost
of maintaining centrally administered servers. It is designed to deliver video
with low end-to-end delays, at quality levels commensurate with available
network resources over arbitrary network topologies where bottlenecks can be
anywhere in the network. This is in contrast to commonly assumed P2P scenarios
where bandwidth bottlenecks reside only at the edge of the network. The
highlight in our design is a distributed and adaptive rate control protocol,
that can discover and adapt to arbitrary topologies and network conditions
quickly, converging to efficient link rate allocations allowed by the
underlying network. In accordance with adaptive link rate control, source video
encoding rates are also dynamically controlled to optimize video quality in
arbitrary and unpredictable network conditions. We have implemented Celerity in
a prototype system, and demonstrate its superior performance over existing
solutions in a local experimental testbed and over the Internet."
"The paper describes in detail the recovery effort of one of the official
MediaWiki grammars. Over two hundred grammar transformation steps are reported
and annotated, leading to delivery of a level 2 grammar, semi-automatically
extracted from a community created semi-formal text using at least five
different syntactic notations, several non-enforced naming conventions,
multiple misspellings, obsolete parsing technology idiosyncrasies and other
problems commonly encountered in grammars that were not engineered properly.
Having a quality grammar will allow to test and validate it further, without
alienating the community with a separately developed grammar."
"This paper presents a method for indexing activities of daily living in
videos obtained from wearable cameras. In the context of dementia diagnosis by
doctors, the videos are recorded at patients' houses and later visualized by
the medical practitioners. The videos may last up to two hours, therefore a
tool for an efficient navigation in terms of activities of interest is crucial
for the doctors. The specific recording mode provides video data which are
really difficult, being a single sequence shot where strong motion and sharp
lighting changes often appear. Our work introduces an automatic motion based
segmentation of the video and a video structuring approach in terms of
activities by a hierarchical two-level Hidden Markov Model. We define our
description space over motion and visual characteristics of video and audio
channels. Experiments on real data obtained from the recording at home of
several patients show the difficulty of the task and the promising results of
our approach."
"Due to the increase of interest in Augmented Reality (AR), the potential uses
of AR are increasing also. It can benefit the user in various fields such as
education, business, medicine, and other. Augmented Reality supports the real
environment with synthetic environment to give more details and meaning to the
objects in the real word. AR refers to a situation in which the goal is to
supplement a user's perception of the real-world through the addition of
virtual objects. This paper is an attempt to make a survey of web-based
Augmented Reality applications and make a comparison among them."
"One important class of online videos is that of news broadcasts. Most news
organisations provide near-immediate access to topical news broadcasts over the
Internet, through RSS streams or podcasts. Until lately, technology has not
made it possible for a user to automatically go to the smaller parts, within a
longer broadcast, that might interest them. Recent advances in both speech
recognition systems and natural language processing have led to a number of
robust tools that allow us to provide users with quicker, more focussed access
to relevant segments of one or more news broadcast videos. Here we present our
new interface for browsing or searching news broadcasts (video/audio) that
exploits these new language processing tools to (i) provide immediate access to
topical passages within news broadcasts, (ii) browse news broadcasts by events
as well as by people, places and organisations, (iii) perform cross lingual
search of news broadcasts, (iv) search for news through a map interface, (v)
browse news by trending topics, and (vi) see automatically-generated textual
clues for news segments, before listening. Our publicly searchable demonstrator
currently indexes daily broadcast news content from 50 sources in English,
French, Chinese, Arabic, Spanish, Dutch and Russian."
"In this paper, we introduce a new digital watermarking algorithm using least
significant bit (LSB). LSB is used because of its little effect on the image.
This new algorithm is using LSB by inversing the binary values of the watermark
text and shifting the watermark according to the odd or even number of pixel
coordinates of image before embedding the watermark. The proposed algorithm is
flexible depending on the length of the watermark text. If the length of the
watermark text is more than ((MxN)/8)-2 the proposed algorithm will also embed
the extra of the watermark text in the second LSB. We compare our proposed
algorithm with the 1-LSB algorithm and Lee's algorithm using Peak
signal-to-noise ratio (PSNR). This new algorithm improved its quality of the
watermarked image. We also attack the watermarked image by using cropping and
adding noise and we got good results as well."
"Steganography is a general term referring to all methods for the embedding of
additional secret content into some form of carrier, with the aim of
concealment of the introduced alterations. The choice of the carrier is nearly
unlimited, it may be an ancient piece of parchment, as well as a network
protocol header. Inspired by biological phenomena, adopted by man in the
ancient times, it has been developed over the ages. Present day steganographic
methods are far more sophisticated than their ancient predecessors, but the
main principles have remained unchanged. They typically rely on the utilization
of digital media files or network protocols as a carrier, in which secret data
is embedded. This paper presents the evolution of the hidden data carrier from
the ancient times till the present day and pinpoints the observed development
trends, with special emphasis on network steganography."
"We propose two scheduling algorithms that seek to optimize the quality of
scalably coded videos that have been stored at a video server before
transmission.} The first scheduling algorithm is derived from a Markov Decision
Process (MDP) formulation developed here. We model the dynamics of the channel
as a Markov chain and reduce the problem of dynamic video scheduling to a
tractable Markov decision problem over a finite state space. Based on the MDP
formulation, a near-optimal scheduling policy is computed that minimize the
mean square error. Using insights taken from the development of the optimal
MDP-based scheduling policy, the second proposed scheduling algorithm is an
online scheduling method that only requires easily measurable knowledge of the
channel dynamics, and is thus viable in practice. Simulation results show that
the performance of both scheduling algorithms is close to a performance upper
bound also derived in this paper."
"The TV is dead motto of just a few years ago has been replaced by the
prospect of Internet Protocol (IP) television experiences over converged
networks to become one of the great technology opportunities in the next few
years. As an introduction to the Special Issue on Smart, Social and Converged
Television, this extended editorial intends to review the current IP television
landscape in its many realizations: operator-based, over-the-top, and user
generated. We will address new services like social TV and recommendation
engines, dissemination including new paradigms built on peer to peer and
content centric networks, as well as the all important quality of experience
that challenges services and networks alike. But we intend to go further than
just review the existing work by proposing areas for the future of television
research. These include strategies to provide services that are more efficient
in network and energy usage while being socially engaging, novel services that
will provide consumers with a broader choice of content and devices, and
metrics that will enable operators and users alike to define the level of
service they require or that they are ready to provide. These topics are
addressed in this survey paper that attempts to create a unifying framework to
link them all together. Not only is television not dead, it is well alive,
thriving and fostering innovation and this paper will hopefully prove it."
"Some state-of-art multimedia source encoders produce embedded source bit
streams that upon the reliable reception of only a fraction of the total bit
stream, the decoder is able reconstruct the source up to a basic quality.
Reliable reception of later source bits gradually improve the reconstruction
quality. Examples include scalable extensions of H.264/AVC and progressive
image coders such as JPEG2000. To provide an efficient protection for embedded
source bit streams, a concatenated block coding scheme using a minimum mean
distortion criterion was considered in the past. Although, the original design
was shown to achieve better mean distortion characteristics than previous
studies, the proposed coding structure was leading to dramatic quality
fluctuations. In this paper, a modification of the original design is first
presented and then the second order statistics of the distortion is taken into
account in the optimization. More specifically, an extension scheme is proposed
using a minimum distortion variance optimization criterion. This robust system
design is tested for an image transmission scenario. Numerical results show
that the proposed extension achieves significantly lower variance than the
original design, while showing similar mean distortion performance using both
convolutional codes and low density parity check codes."
"The paper introduces the idea of non-uniform quantization in the detail
components of wavelet transformed image. It argues that most of the
coefficients of horizontal, vertical and diagonal components lie near to zeros
and the coefficients representing large differences are few at the extreme ends
of histogram. Therefore, this paper advocates need for variable step size
quantization scheme which preserves the edge information at the edge of
histogram and removes redundancy with the minimal number of quantized values.
To support the idea, preliminary results are provided using a non-uniform
quantization algorithm. We believe that successful implementation of
non-uniform quantization in detail components in JPEG-2000 still image standard
will improve image quality and compression efficiency with lesser number of
quantized values."
"Digital watermarking is a technique of embedding pieces of information into
digital data such as text, audio, video, and still images that can be detected
or extracted later to show authentication about the data. Watermark is hidden
information in the image(s) and is so designed that it does not degrade/distort
the quality of the image and still keeps the information. Digital watermarking
is basically to protect ownership rights and to control of making illicit
copies of digital data. In this paper, we have discussed various watermarking
techniques and properties and have proposed a modified LSB technique. We have
implemented the proposed technique by following: 2-bits of 8-bit gray image is
replaced by luminance part, next 2-bits by red component, next 2-bits by green
component and next 2-bits by blue component of 32-bit image using secret key.
The advantage is that watermarking capacity has been increased and unaffected
by various attacks e.g. zero out LSB bits, cropping etc. Watermark image is
imperceptible in resultant image. We have tested this technique on several
images and found that it is quite satisfactory. This technique is secured as
unauthorized user can not extract the watermarked contents easily from the
original image and works well in adverse situations. We have implemented this
technique on platform java 1.5.0."
"This study proposes an audio copy detection system that is robust to various
attacks. These include the severe pitch shift and tempo change attacks which
existing systems fail to detect. First, we propose a novel two dimensional
representation for audio signals called the time-chroma image. This image is
based on a modification of the concept of chroma in the music literature and is
shown to achieve better performance in song identification. Then, we propose a
novel fingerprinting algorithm that extracts local fingerprints from the
time-chroma image. The proposed local fingerprinting algorithm is invariant to
time/frequency scale changes in audio signals. It also outperforms existing
methods like SIFT by a great extent. Finally, we introduce a song
identification algorithm that uses the proposed fingerprints. The resulting
copy detection system is shown to significantly outperform existing methods.
Besides being able to detect whether a song (or a part of it) has been copied,
the proposed system can accurately estimate the amount of pitch shift and/or
tempo change that might have been applied to a song."
"The matching of the soundtrack in a movie or a video can have an enormous
influence in the message being conveyed and its impact, in the sense of
involvement and engagement, and ultimately in their aesthetic and entertainment
qualities. Art is often associated with creativity, implying the presence of
inspiration, originality and appropriateness. Evolutionary systems provides us
with the novelty, showing us new and subtly different solutions in every
generation, possibly stimulating the creativity of the human using the system.
In this paper, we present Genetic Soundtracks, an evolutionary approach to the
creative matching of audio to a video. It analyzes both media to extract
features based on their content, and adopts genetic algorithms, with the
purpose of truncating, combining and adjusting audio clips, to align and match
them with the video scenes."
"Steganography is the science of invisible communication. The purpose of
Steganography is to maintain secret communication between two parties. The
secret information can be concealed in content such as image, audio, or video.
This paper provides a novel image steganography technique to hide multiple
secret images and keys in color cover image using Integer Wavelet Transform
(IWT). There is no visual difference between the stego image and the cover
image. The extracted secret images are also similar to the original secret
images. Very good PSNR (Peak Signal to Noise Ratio) values are obtained for
both stego and extracted secret images. The results are compared with the
results of other techniques, where single image is hidden and it is found that
the proposed technique is simple and gives better PSNR values than others."
"This paper introduces a redundancy adaptation algorithm for an on-the-fly
erasure network coding scheme called Tetrys in the context of real-time video
transmission. The algorithm exploits the relationship between the redundancy
ratio used by Tetrys and the gain or loss in encoding bit rate from changing a
video quality parameter called the Quantization Parameter (QP). Our evaluations
show that with equal or less bandwidth occupation, the video protected by
Tetrys with redundancy adaptation algorithm obtains a PSNR gain up to or more 4
dB compared to the video without Tetrys protection. We demonstrate that the
Tetrys redundancy adaptation algorithm performs well with the variations of
both loss pattern and delay induced by the networks. We also show that Tetrys
with the redundancy adaptation algorithm outperforms FEC with and without
redundancy adaptation."
"This paper presents an adaptable steganography (information hiding) method
for digital radio communication. Many radio steganography methods exist, but
most are defined at higher levels of the protocol stack and are thus protocol
dependent. In contrast, this method is defined at the physical layer, which
makes it widely applicable regardless of the protocols used at higher layers.
This approach is also adaptive; the covertness of the hidden channel is simple
to control via a single continuous parameter either manually or automatically.
Several variations are introduced, each with performance evaluated by
simulation. Results show this to be a feasible method with a reasonable
trade-off between performance and covertness."
"An optimal frame transmission scheme is presented for streaming scalable
video over a link with limited capacity. The objective is to select a
transmission sequence of frames and their transmission schedule such that the
overall video quality is maximized. The problem is solved for two general
classes of hierarchical prediction structures, which include as a special case
the popular dyadic structure. Based on a new characterization of the
interdependence among frames in terms of trees, structural properties of an
optimal transmission schedule are derived. These properties lead to the
development of a jointly optimal frame selection and scheduling algorithm,
which has computational complexity that is quadratic in the number of frames.
Simulation results show that the optimal scheme substantially outperforms three
existing alternatives."
"Free-viewpoint video conferencing allows a participant to observe the remote
3D scene from any freely chosen viewpoint. An intermediate virtual viewpoint
image is commonly synthesized using two pairs of transmitted texture and depth
maps from two neighboring captured viewpoints via depth-image-based rendering
(DIBR). To maintain high quality of synthesized images, it is imperative to
contain the adverse effects of network packet losses that may arise during
texture and depth video transmission. Towards this end, we develop an
integrated approach that exploits the representation redundancy inherent in the
multiple streamed videos a voxel in the 3D scene visible to two captured views
is sampled and coded twice in the two views. In particular, at the receiver we
first develop an error concealment strategy that adaptively blends
corresponding pixels in the two captured views during DIBR, so that pixels from
the more reliable transmitted view are weighted more heavily. We then couple it
with a sender-side optimization of reference picture selection (RPS) during
real-time video coding, so that blocks containing samples of voxels that are
visible in both views are more error-resiliently coded in one view only, given
adaptive blending will erase errors in the other view. Further, synthesized
view distortion sensitivities to texture versus depth errors are analyzed, so
that relative importance of texture and depth code blocks can be computed for
system-wide RPS optimization. Experimental results show that the proposed
scheme can outperform the use of a traditional feedback channel by up to 0.82
dB on average at 8% packet loss rate, and by as much as 3 dB for particular
frames."
"Video streaming via TCP networks has become a popular and highly demanded
service, but its quality assessment in both objective and subjective terms has
not been properly addressed. In this paper, based on statistical analysis a
full analytic model of a no-reference objective metric, namely Pause Intensity,
for video quality assessment is presented. The model characterizes the video
playout buffer behavior in connection with the network performance (throughput)
and the video playout rate. This allows for instant quality measurement and
control without requiring a reference video. Pause intensity specifically
addresses the need for assessing the quality issue in terms of the continuity
in the playout of TCP streaming videos, which cannot be properly measured by
other objective metrics such as PSNR, SSIM and buffer underrun or pause
frequency. The performance of the analytical model is rigidly verified by
simulation results and subjective tests using a range of video clips. It is
demonstrated that pause intensity is closely correlated with viewer opinion
scores regardless of the vastly different composition of individual elements,
such as pause duration and pause frequency which jointly constitute this new
quality metric. It is also shown that the correlation performance of pause
intensity is consistent and content independent."
"The aim of this paper is to introduce a new schema, based on a Compressive
Sampling technique, for the recovery of lost data in multimedia streaming. The
audio streaming data are encapsuled in different packets by using an
interleaving technique. The Compressive Sampling technique is used to recover
audio information in case of lost packets. Experimental results are presented
on speech and musical audio signals to illustrate the performances and the
capabilities of the proposed methodology."
"High frame video (HFV) is an important investigational tool in sciences,
engineering and military. In ultra-high speed imaging, the obtainable temporal,
spatial and spectral resolutions are limited by the sustainable throughput of
in-camera mass memory, the lower bound of exposure time, and illumination
conditions. In order to break these bottlenecks, we propose a new coded video
acquisition framework that employs K > 2 conventional cameras, each of which
makes random measurements of the 3D video signal in both temporal and spatial
domains. For each of the K cameras, this multi-camera strategy greatly relaxes
the stringent requirements in memory speed, shutter speed, and illumination
strength. The recovery of HFV from these random measurements is posed and
solved as a large scale l1 minimization problem by exploiting joint temporal
and spatial sparsities of the 3D signal. Three coded video acquisition
techniques of varied trade offs between performance and hardware complexity are
developed: frame-wise coded acquisition, pixel-wise coded acquisition, and
column-row-wise coded acquisition. The performances of these techniques are
analyzed in relation to the sparsity of the underlying video signal.
Simulations of these new HFV capture techniques are carried out and
experimental results are reported."
"With the development of embedded video acquisition nodes and wireless video
surveillance systems, traditional video coding methods could not meet the needs
of less computing complexity any more, as well as the urgent power consumption.
So, a low-complexity compressive sensing video encoder framework with
application-aware configurable mechanism is proposed in this paper, where novel
encoding methods are exploited based on the practical purposes of the real
applications to reduce the coding complexity effectively and improve the
compression ratio (CR). Moreover, the group of processing (GOP) size and the
measurement matrix size can be configured on the encoder side according to the
post-analysis requirements of an application example of object tracking to
increase the CR of encoder as best as possible. Simulations show the proposed
framework of encoder could achieve 60X of CR when the tracking successful rate
(SR) is still keeping above 90%."
"As communication channels are increasing in number, reliability of faithful
communication is reducing. Hacking and tempering of data are two major issues
for which security should be provided by channel. This raises the importance of
steganography. In this paper, a novel method to encode the message information
inside a carrier image has been described. It uses Karhunen-Lo\`eve Transform
for compression of data and Least Bit Substitution for data encryption.
Compression removes redundancy and thus also provides encoding to a level. It
is taken further by means of Least Bit Substitution. The algorithm used for
this purpose uses pixel matrix which serves as a best tool to work on. Three
different sets of images were used with three different numbers of bits to be
substituted by message information. The experimental results show that
algorithm is time efficient and provides high data capacity. Further, it can
decrypt the original data effectively. Parameters such as carrier error and
message error were calculated for each set and were compared for performance
analysis."
"Recent years have seen an explosion in wireless video communication systems.
Optimization in such systems is crucial - but most existing methods intended to
optimize the performance of multi-user wireless video transmission are
inefficient. Some works (e.g. Network Utility Maximization (NUM)) are myopic:
they choose actions to maximize instantaneous video quality while ignoring the
future impact of these actions. Such myopic solutions are known to be inferior
to foresighted solutions that optimize the long-term video quality.
Alternatively, foresighted solutions such as rate-distortion optimized packet
scheduling focus on single-user wireless video transmission, while ignoring the
resource allocation among the users.
  In this paper, we propose an optimal solution for performing joint
foresighted resource allocation and packet scheduling among multiple users
transmitting video over a shared wireless network. A key challenge in
developing foresighted solutions for multiple video users is that the users'
decisions are coupled. To decouple the users' decisions, we adopt a novel dual
decomposition approach, which differs from the conventional optimization
solutions such as NUM, and determines foresighted policies. Specifically, we
propose an informationally-decentralized algorithm in which the network manager
updates resource ""prices"" (i.e. the dual variables associated with the resource
constraints), and the users make individual video packet scheduling decisions
based on these prices. Because a priori knowledge of the system dynamics is
almost never available at run-time, the proposed solution can learn online,
concurrently with performing the foresighted optimization. Simulation results
show 7 dB and 3 dB improvements in Peak Signal-to-Noise Ratio (PSNR) over
myopic solutions and existing foresighted solutions, respectively."
"The network transport of 3D video, which contains two views of a video scene,
poses significant challenges due to the increased video data compared to
conventional single-view video. Addressing these challenges requires a thorough
understanding of the traffic and multiplexing characteristics of the different
representation formats of 3D video. We examine the average bitrate-distortion
(RD) and bitrate variability-distortion (VD) characteristics of three main
representation formats. Specifically, we compare multiview video (MV)
representation and encoding, frame sequential (FS) representation, and
side-by-side (SBS) representation, whereby conventional single-view encoding is
employed for the FS and SBS representations. Our results for long 3D videos in
full HD format indicate that the MV representation and encoding achieves the
highest RD efficiency, while exhibiting the highest bitrate variabilities. We
examine the impact of these bitrate variabilities on network transport through
extensive statistical multiplexing simulations. We find that when multiplexing
a small number of streams, the MV and FS representations require the same
bandwidth. However, when multiplexing a large number of streams or smoothing
traffic, the MV representation and encoding reduces the bandwidth requirement
relative to the FS representation."
"Newly developed HTTP-based video streaming technologies enable flexible
rate-adaptation under varying channel conditions. Accurately predicting the
users' Quality of Experience (QoE) for rate-adaptive HTTP video streams is thus
critical to achieve efficiency. An important aspect of understanding and
modeling QoE is predicting the up-to-the-moment subjective quality of a video
as it is played, which is difficult due to hysteresis effects and
nonlinearities in human behavioral responses. This paper presents a
Hammerstein-Wiener model for predicting the time-varying subjective quality
(TVSQ) of rate-adaptive videos. To collect data for model parameterization and
validation, a database of longer-duration videos with time-varying distortions
was built and the TVSQs of the videos were measured in a large-scale subjective
study. The proposed method is able to reliably predict the TVSQ of rate
adaptive videos. Since the Hammerstein-Wiener model has a very simple
structure, the proposed method is suitable for on-line TVSQ prediction in HTTP
based streaming."
"Adapting video data rate during streaming can effectively reduce the risk of
playback interruptions caused by channel throughput fluctuations. The
variations in rate, however, also introduce video quality fluctuations and thus
potentially affects viewers' Quality of Experience (QoE). We show how the QoE
of video users can be improved by rate adaptation and admission control. We
conducted a subjective study wherein we found that viewers' QoE was strongly
correlated with the empirical cumulative distribution function (eCDF) of the
predicted video quality. Based on this observation, we propose a
rate-adaptation algorithm that can incorporate QoE constraints on the empirical
cumulative quality distribution per user. We then propose a threshold-based
admission control policy to block users whose empirical cumulative quality
distribution is not likely to satisfy their QoE constraint. We further devise
an online adaptation algorithm to automatically optimize the threshold.
Extensive simulation results show that the proposed scheme can reduce network
resource consumption by $40\%$ over conventional average-quality maximized
rate-adaptation algorithms."
"In this paper, we propose a new representation for multiview image sets. Our
approach relies on graphs to describe geometry information in a compact and
controllable way. The links of the graph connect pixels in different images and
describe the proximity between pixels in the 3D space. These connections are
dependent on the geometry of the scene and provide the right amount of
information that is necessary for coding and reconstructing multiple views.
This multiview image representation is very compact and adapts the transmitted
geometry information as a function of the complexity of the prediction
performed at the decoder side. To achieve this, our GBR adapts the accuracy of
the geometry representation, in contrast with depth coding, which directly
compresses with losses the original geometry signal. We present the principles
of this graph-based representation (GBR) and we build a complete prototype
coding scheme for multiview images. Experimental results demonstrate the
potential of this new representation as compared to a depth-based approach. GBR
can achieve a gain of 2 dB in reconstructed quality over depth-based schemes
operating at similar rates."
"Progress in image sensors and computation power has fueled studies to improve
acquisition, processing, and analysis of 3D streams along with 3D
scenes/objects reconstruction. The role of motion compensation/motion
estimation (MCME) in 3D TV from end-to-end user is investigated in this
chapter. Motion vectors (MVs) are closely related to the concept of
disparities, and they can help improving dynamic scene acquisition, content
creation, 2D to 3D conversion, compression coding, decompression/decoding,
scene rendering, error concealment, virtual/augmented reality handling,
intelligent content retrieval, and displaying. Although there are different 3D
shape extraction methods, this chapter focuses mostly on shape-from-motion
(SfM) techniques due to their relevance to 3D TV. SfM extraction can restore 3D
shape information from a single camera data."
"The paper presents an overview of the main methods used to improve the
efficiency of DVB systems, based on multiplexing, through a study on the impact
of the multiplexing methods used in DVB, having as a final goal a better usage
of the data capacity and the possibility to insert new services into the
original DVB Transport Stream. This study revealed that not all DVB providers
are using statistical multiplexing. Based on this study, we were able to
propose a method to improve the original DVB stream, originated from DVB-S or
DVB-T providers. This method is proposing the detection of null packets,
removal and reinserting a new service, with a VBR content. The method developed
in this research can be implemented even in optimized statistical multiplexing
systems, due to a residual use of null packets for data rate adjustment. There
is no need to have access in the original stream multiplexer, since the method
allows the implementation on the fly, near to the end user. The proposed method
is proposed to be applied in DVB-S to DVB-C translation, using the computing
power of a PC or in a FPGA implementation."
"Video hashing finds a wide array of applications in content authentication,
robust retrieval and anti-piracy search. While much of the existing research
has focused on extracting robust and secure content descriptors, a significant
open challenge still remains: Most existing video hashing methods are fallible
to temporal desynchronization. That is, when the query video results by
deleting or inserting some frames from the reference video, most existing
methods assume the positions of the deleted (or inserted) frames are either
perfectly known or reliably estimated. This assumption may be okay under
typical transcoding and frame-rate changes but is highly inappropriate in
adversarial scenarios such as anti-piracy video search. For example, an illegal
uploader will try to bypass the 'piracy check' mechanism of YouTube/Dailymotion
etc by performing a cleverly designed non-uniform resampling of the video. We
present a new solution based on dynamic time warping (DTW), which can implement
automatic synchronization and can be used together with existing video hashing
methods. The second contribution of this paper is to propose a new robust
feature extraction method called flow hashing (FH), based on frame averaging
and optical flow descriptors. Finally, a fusion mechanism called distance
boosting is proposed to combine the information extracted by DTW and FH.
Experiments on real video collections show that such a hash extraction and
comparison enables unprecedented robustness under both spatial and temporal
attacks."
"MMS (Multimedia Messaging Service) is the next generation of messaging
services in multimedia mobile communications. MMS enables messaging with full
multimedia content including images, audios, videos, texts and data, from
client to client or e-mail. MMS is based on WAP technology, so it is technology
independent. This means that enabling messages from a GSM/GPRS network to be
sent to a TDMA or WCDMA network. In this paper a methodology for implementing
MMS client on embedded platforms especially on Wince OS is described."
"An adaptive visible/invisible watermarking scheme is done to prevent the
privacy and preserving copyright protection of digital data using Hadamard
transform based on the scaling factor of the image. The value of scaling factor
depends on the control parameter. The scaling factor is calculated to embedded
the watermark. Depend upon the control parameter the visible and invisible
watermarking is determined. The proposed Hadamard transform domain method is
more robust again image/signal processing attacks. Furthermore, it also shows
that the proposed method confirm the efficiency through various performance
analysis and experimental results."
"In this paper, we present an Android application which is able to evaluate
and analyze the perceived Quality of Experience (QoE) for YouTube service in
wireless terminals. To achieve this goal, the application carries out
measurements of objective Quality of Service (QoS) parameters, which are then
mapped onto subjective QoE (in terms of Mean Opinion Score, MOS) by means of a
utility function. Our application also informs the user about potential causes
that lead to a low MOS as well as provides some hints to improve it. After each
YouTube session, the users may optionally qualify the session through an online
opinion survey. This information has been used in a pilot experience to
correlate the theoretical QoE model with real user feedback. Results from such
an experience have shown that the theoretical model (taken from the literature)
provides slightly more pessimistic results compared to user feedback. Users
seem to be more indulgent with wireless connections, increasing the MOS from
the opinion survey in about 20% compared to the theoretical model, which was
obtained from wired scenarios."
"Motion estimation is one of the major problems in developing video coding
applications. Among all motion estimation approaches, Block matching (BM)
algorithms are the most popular methods due to their effectiveness and
simplicity for both software and hardware implementations. A BM approach
assumes that the movement of pixels within a defined region of the current
frame (Macro-Block, MB) can be modeled as a translation of pixels contained in
the previous frame. In this procedure, the motion vector is obtained by
minimizing the sum of absolute differences (SAD) produced by the MB of the
current frame over a determined search window from the previous frame. The SAD
evaluation is computationally expensive and represents the most consuming
operation in the BM process. The most straightforward BM method is the full
search algorithm (FSA) which finds the most accurate motion vector, calculating
exhaustively the SAD values for all elements of the search window. Over this
decade, several fast BM algorithms have been proposed to reduce the number of
SAD operations by calculating only a fixed subset of search locations at the
price of a poor accuracy. In this paper, a new algorithm based on Differential
Evolution (DE) is proposed to reduce the number of search locations in the BM
process. In order to avoid computing several search locations, the algorithm
estimates the SAD values (fitness) for some locations using the SAD values of
previously calculated neighboring positions. Since the proposed algorithm does
not consider any fixed search pattern or other different assumption, a high
probability for finding the true minimum (accurate motion vector) is expected.
In comparison to other fast BM algorithms, the proposed method deploys more
accurate motion vectors yet delivering competitive time rates."
"In last decade, ever growing internet technologies provided platform to share
the multimedia data among different communities. As the ultimate users are
human subjects who are concerned about quality of visual information, it is
often desired to have good resumed perceptual quality of videos, thus arises
the need of quality assessment. This paper presents a full reference hybrid
video quality metric which is capable to analyse the video quality for
spatially or temporally (frame drop) or spatio-temporally distorted video
sequences. Simulated results show that the metric efficiently analyses the
quality degradation and more closer to the developed human visual system"
"This paper presents a low complexity video coding method based on
Underdetermined Blind Signal Separation (UBSS). The detailed coding framework
is designed. Three key techniques are proposed to enhance the compression ratio
and the quality of the decoded frames. The experiments validate that the
proposed method costs 30ms encoding time less than DISCOVER. The simulation
shows that this new method can save 50% energy compared with H.264."
"Due to the increasing requirements for transmission of images in computer,
mobile environments, the research in the field of image compression has
increased significantly. Image compression plays a crucial role in digital
image processing, it is also very important for efficient transmission and
storage of images. When we compute the number of bits per image resulting from
typical sampling rates and quantization methods, we find that Image compression
is needed. Therefore development of efficient techniques for image compression
has become necessary .This paper is a survey for lossy image compression using
Discrete Cosine Transform, it covers JPEG compression algorithm which is used
for full-colour still image applications and describes all the components of
it."
"With the surge in modern research focus towards Pervasive Computing, lot of
techniques and challenges needs to be addressed so as to effectively create
smart spaces and achieve miniaturization. In the process of scaling down to
compact devices, the real things to ponder upon are the Information Retrieval
challenges. In this work, we discuss the aspects of multimedia which makes
information access challenging. An Example Pattern Recognition scenario is
presented and the mathematical techniques that can be used to model uncertainty
are also presented for developing a system that can sense, compute and
communicate in a way that can make human life easy with smart objects assisting
from around his surroundings."
"This paper focuses on the JPEG noises, which include the quantization noise
and the rounding noise, during a JPEG compression cycle. The JPEG noises in the
first compression cycle have been well studied; however, so far less attention
has been paid on the JPEG noises in higher compression cycles. In this work, we
present a statistical analysis on JPEG noises beyond the first compression
cycle. To our knowledge, this is the first work on this topic. We find that the
noise distributions in higher compression cycles are different from those in
the first compression cycle, and they are dependent on the quantization
parameters used between two successive cycles. To demonstrate the benefits from
the statistical analysis, we provide two applications that can employ the
derived noise distributions to uncover JPEG compression history with
state-of-the-art performance."
"Scalable video coding has drawn great interest in content delivery in many
multimedia services thanks to its capability to handle terminal heterogeneity
and network conditions variation. In our previous work, and under the umbrella
of ENVISION, we have proposed a playout smoothing mechanism to ensure the
uniform delivery of the layered stream, by reducing the quality changes that
the stream undergoes when adapting to changing network conditions. In this
paper we study the resulting video quality, from the final user perception
under different network conditions of loss and delays. For that we have adopted
the Double Stimulus Impairment Scale (DSIS) method. The results show that the
Mean Opinion Score for the smoothed video clips was higher under different
network configuration. This confirms the effectiveness of the proposed
smoothing mechanism."
"In networked video applications, the frame rate (FR) and quantization
stepsize (QS) of a compressed video are often adapted in response to the
changes of the available bandwidth. It is important to understand how do the
variation of FR and QS and their variation pattern affect the video quality. In
this paper, we investigate the impact of temporal variation of FR and QS on the
perceptual video quality. Among all possible variation patterns, we focus on
videos in which two FR's (or QS's) alternate over a fixed interval. We explore
the human responses to such variation by conducting subjective evaluation of
test videos with different variation magnitudes and frequencies. We further
analyze statistical significance of the impact of variation magnitude,
variation frequency, video content, and their interactions. By analyzing the
subjective ratings, we propose two models for predicting the quality of video
with alternating FR and QS, respectively, The proposed models have simple
mathematical forms with a few content-dependent parameters. The models fit the
measured data very well using parameters determined by least square fitting
with the measured data. We further propose some guidelines for adaptation of FR
and QS based on trends observed from subjective test results."
"At the last few years, multimedia communication has been developed and
improved rapidly in order to enable users to communicate between each other
over the internet. Generally, multimedia communication consists of audio and
video communication. However, this research concentrates on audio conferencing
only. The audio translation between protocols is a very critical issue, because
it solves the communication problems between any two protocols. So, it enables
people around the world to talk with each other even they use different
protocols. In this research, a real time audio translation module between two
protocols has been done. These two protocols are: InterAsterisk eXchange
Protocol (IAX) and Real-Time Switching Control Protocol (RSW), which they are
widely used to provide two ways audio transfer feature. The solution here is to
provide inter-working between the two protocols which they have different media
transports, audio codecs, header formats and different transport protocols for
the audio transmission. This translation will help bridging the gap between the
two protocols by providing inter-working capability between the two audio
streams of IAX and RSW. Some related works have been done to provide
translation between IAX and RSW control signalling messages. But, this research
paper concentrates on the translation that depends on the media transfer. The
proposed translation module was tested and evaluated in different scenarios in
order to examine its performance. The obtained results showed that the
Real-Time Audio Translation Module produces lower rates of packet delay and
jitter than the acceptance values for each of the mentioned performance
metrics."
"Adaptive streaming addresses the increasing and heterogenous demand of
multimedia content over the Internet by offering several encoded versions for
each video sequence. Each version (or representation) has a different
resolution and bit rate, aimed at a specific set of users, like TV or mobile
phone clients. While most existing works on adaptive streaming deal with
effective playout-control strategies at the client side, we take in this paper
a providers' perspective and propose solutions to improve user satisfaction by
optimizing the encoding rates of the video sequences. We formulate an integer
linear program that maximizes users' average satisfaction, taking into account
the network dynamics, the video content information, and the user population
characteristics. The solution of the optimization is a set of encoding
parameters that permit to create different streams to robustly satisfy users'
requests over time. We simulate multiple adaptive streaming sessions
characterized by realistic network connections models, where the proposed
solution outperforms commonly used vendor recommendations, in terms of user
satisfaction but also in terms of fairness and outage probability. The
simulation results further show that video content information as well as
network constraints and users' statistics play a crucial role in selecting
proper encoding parameters to provide fairness a mong users and to reduce
network resource usage. We finally propose a few practical guidelines that can
be used to choose the encoding parameters based on the user base
characteristics, the network capacity and the type of video content."
"Telemedicine is well known application where enormous amount of medical data
need to be transferred securely over network and manipulate effectively.
Security of digital data, especially medical images, becomes important for many
reasons such as confidentiality, authentication and integrity. Digital
watermarking has emerged as a advanced technology to enhance the security of
digital images. The insertion of watermark in medical images can authenticate
it and guarantee its integrity. The watermark must be generally hidden does not
affect the quality of the medical image. In this paper, we propose blind
watermarking based on Discrete Wavelet Transform (DWT), Discrete Cosine
Transform (DCT) and Singular Value Decomposition (SVD), we compare the
performance of this technique with watermarking based DWT and SVD. The proposed
method DWT, DCT and SVD comparatively better than DWT and SVD method."
"With the advancement of communication technology,data is exchanged digitally
over the network. At the other side the technology is also proven as a tool for
unauthorized access to attackers. Thus the security of data to be transmitted
digitally should get prime focus. Data hiding is the common approach to secure
data. In steganography technique, the existence of data is concealed. GA is an
emerging component of AI to provide suboptimal solutions. In this paper the use
of GA in Steganography is explored to find future scope of research."
"Digital media is the need of a people now a day as the alternate of paper
media.As the technology grown up digital media required protection while
transferring through internet or others mediums.Watermarking techniques have
been developed to fulfill this requirement.This paper aims to provide a
detailed survey of all watermarking techniques specially focuses on image
watermarking types and its applications in today world."
"Internet security finds it difficult to keep the information secure and to
maintain the integrity of the data. Sending messages over the internet secretly
is one of the major tasks as it is widely used for passing the message."
"Digitization of image was a revolutionary step for the fields of photography
and Image processing as this made the editing of images much effortless and
easier. Image editing was not an issue until it was limited to corrective
editing procedures used to enhance the quality of an image such as, contrast
stretching, noise filtering, sharpening etc. But, it became a headache for many
fields when image editing became manipulative. Digital images have become an
easier source of tampering and forgery during last few decades. Today users and
editing specialists, equipped with easily available image editing software,
manipulate digital images with varied goals. Photo journalists often tamper
photographs to give dramatic effect to their stories. Scientists and
researchers use this trick to get theirs works published. Patients' diagnoses
are misrepresented by manipulating medical imageries. Lawyers and Politicians
use tampered images to direct the opinion of people or court to their favor.
Terrorists, anti-social groups use manipulated Stego images for secret
communication. In this paper we present an effective method for detecting
spatial domain Steganography."
"During the recent years, tampering of digital images has become a general
habit among people and professionals. As a result, establishment of image
authenticity has become a key issue in fields those make use of digital images.
Authentication of an image involves separation of original camera outputs from
their tampered or Stego counterparts. Digital image cloning being a popular
type of image tampering, in this paper we have experimentally analyzed seven
different algorithms of cloning detection such as the simple overlapped block
matching with lexicographic sorting (SOBMwLS) algorithm, block matching with
discrete cosine transformation, principal component analysis, discrete wavelet
transformation and singular value decomposition performed on the blocks (DCT,
DWT, PCA, SVD), two combination models where, DCT and DWT are combined with
singular value decomposition (DCTSVD and DWTSVD. A comparative study of all
these techniques with respect to their time complexities and robustness of
detection against various post processing operations such as cropping,
brightness and contrast adjustments are presented in the paper."
"Outlier detection is an integral part of robust evaluation for
crowdsourceable Quality of Experience (QoE) and has attracted much attention in
recent years. In QoE for multimedia, outliers happen because of different test
conditions, human errors, abnormal variations in context, {etc}. In this paper,
we propose a simple yet effective algorithm for outlier detection and robust
QoE evaluation named iterative Least Trimmed Squares (iLTS). The algorithm
assigns binary weights to samples, i.e., 0 or 1 indicating if a sample is an
outlier, then the outlier-trimmed subset least squares solutions give robust
ranking scores. An iterative optimization is carried alternatively between
updating weights and ranking scores which converges to a local optimizer in
finite steps. In our test setting, iLTS is up to 190 times faster than
LASSO-based methods with a comparable performance. Moreover, a varied version
of this method shows adaptation in outlier detection, which provides an
automatic detection to determine whether a data sample is an outlier without
\emph{a priori} knowledge about the amount of the outliers. The effectiveness
and efficiency of iLTS are demonstrated on both simulated examples and
real-world applications. A Matlab package is provided to researchers exploiting
crowdsourcing paired comparison data for robust ranking."
"During the last years, many improvements were made to the hardware capability
of mobile devices. As mobile software also became more interactive and data
processing intensive, the increased power demand could not be compensated by
the improvements on battery technology. Adaptive systems can help to balance
the demand of applications with the limitations of battery resources. For
effective systems, the influence of multimedia quality on power consumption of
the components of mobile devices needs to be better understood. In this paper,
we analyze the impact of video quality and wireless network type on the energy
consumption of a mobile device. We have found that the additional power
consumption is up to 38% higher when a movie is played over a WiFi network
instead from internal memory and 64% higher in case of a mobile network (3G).
We have also discovered that a higher movie quality not only affects the power
consumption of the CPU but also the power consumption of the WiFi unit by up to
58% and up to 72% respectively on mobile networks."
"This paper presents an Immersive Telepresence system for Entertainment and
Meetings (ITEM). The system aims to provide a radically new video communication
experience by seamlessly merging participants into the same virtual space to
allow a natural interaction among them and shared collaborative contents. With
the goal to make a scalable, flexible system for various business solutions as
well as easily accessible by massive consumers, we address the challenges in
the whole pipeline of media processing, communication, and displaying in our
design and realization of such a system. Particularly, in this paper we focus
on the system aspects that maximize the end-user experience, optimize the
system and network resources, and enable various teleimmersive application
scenarios. In addition, we also present a few key technologies, i.e. fast
object-based video coding for real world data and spatialized audio capture and
3D sound localization for group teleconferencing. Our effort is to investigate
and optimize the key system components and provide an efficient end-to-end
optimization and integration by considering user needs and preferences.
Extensive experiments show the developed system runs reliably and comfortably
in real time with a minimal setup requirement (e.g. a webcam and/or a depth
camera, an optional microphone array, a laptop/desktop connected to the public
Internet) for teleimmersive communication. With such a really minimal
deployment requirement, we present a variety of interesting applications and
user experiences created by ITEM."
"With the advancements in the field of digital image processing during the
last decade, digital image data hiding techniques such as watermarking,
Steganography have gained wide popularity. Digital image watermarking
techniques hide a small amount of data into a digital image which, later can be
retrieved using some specific retrieval algorithms to prove the copyright of a
piece of digital information whereas, Steganographic techniques are used to
hide a large amount of data secretly into some innocuous looking digital
medium. In this paper we are providing an up-to-date review of these data
hiding techniques."
"Image width is important for image understanding. We propose a novel method
to estimate widths for JPEG images when their widths are not available. The key
idea is that the distance between two decoded MCUs (Minimum Coded Unit)
adjacent in the vertical direction is usually small, which is measured by the
average Euclidean distance between the pixels from the bottom row of the top
MCU and the top row of the bottom MCU. On PASCAL VOC 2010 challenge dataset and
USC-SIPI image database, experimental results show the high performance of the
proposed approach."
"The science of hiding secret information in another message is known as
Steganography; hence the presence of secret information is concealed. It is the
method of hiding cognitive content in same or another media to avoid
recognition by the intruders. This paper introduces new method wherein
irreversible steganography is used to hide an image in the same medium so that
the secret data is masked. The secret image is known as payload and the carrier
is known as cover image. X-OR operation is used amongst mid level bit planes of
carrier image and high level bit planes of data image to generate new low level
bit planes of the stego image. Recovery process includes the X-ORing of low
level bit planes and mid level bit planes of the stego image. Based on the
result of the recovery, subsequent data image is generated. A RGB color image
is used as carrier and the data image is a grayscale image of dimensions less
than or equal to the dimensions of the carrier image. The proposed method
greatly increases the embedding capacity without significantly decreasing the
PSNR value."
"To provide an added security level most of the existing reversible as well as
irreversible image steganography schemes emphasize on encrypting the secret
image (payload) before embedding it to the cover image. The complexity of
encryption for a large payload where the embedding algorithm itself is complex
may adversely affect the steganographic system. Schemes that can induce same
level of distortion, as any standard encryption technique with lower
computational complexity, can improve the performance of stego systems. In this
paper we propose a secure secret image sharing scheme, which bears minimal
computational complexity. The proposed scheme, as a replacement for encryption,
diversifies the payload into different matrices which are embedded into carrier
image (cover image) using bit X-OR operation. A payload is a grayscale image
which is divided into frequency matrix, error matrix, and sign matrix. The
frequency matrix is scaled down using a mapping algorithm to produce Down
Scaled Frequency (DSF) matrix. The DSF matrix, error matrix, and sign matrix
are then embedded in different cover images using bit X-OR operation between
the bit planes of the matrices and respective cover images. Analysis of the
proposed scheme shows that it effectively camouflages the payload with minimum
computation time."
"With the recent emergence of 3D-supported TVs, video service providers now
face an opportunity to provide high resolution multi-view 3D videos over IP
networks. One simple way to support efficient communications between a video
server and multiple clients is to deliver each desired view in a multicast
stream. Nevertheless, it is expected that significantly increased bandwidth
will be required to support the transmission of all views in multi-view 3D
videos. However, the recent emergence of a new video synthesis technique called
Depth-Image-Based Rendering (DIBR) suggests that multi-view 3D video does not
necessarily require the transmission of all views. Therefore, we formulate a
new problem, named Multi-view and Multicast Delivery Selection Problem (MMDS),
and design an algorithm, called MMDEA, to find the optimal solution. Simulation
results manifest that using DIBR can effectively reduce bandwidth consumption
by 35% compared to the original multicast delivery scheme."
"Human motion capture (mocap) is a widely used technique for digitalizing
human movements. With growing usage, compressing mocap data has received
increasing attention, since compact data size enables efficient storage and
transmission. Our analysis shows that mocap data have some unique
characteristics that distinguish themselves from images and videos. Therefore,
directly borrowing image or video compression techniques, such as discrete
cosine transform, does not work well. In this paper, we propose a novel
mocap-tailored transform coding algorithm that takes advantage of these
features. Our algorithm segments the input mocap sequences into clips, which
are represented in 2D matrices. Then it computes a set of data-dependent
orthogonal bases to transform the matrices to frequency domain, in which the
transform coefficients have significantly less dependency. Finally, the
compression is obtained by entropy coding of the quantized coefficients and the
bases. Our method has low computational cost and can be easily extended to
compress mocap databases. It also requires neither training nor complicated
parameter setting. Experimental results demonstrate that the proposed scheme
significantly outperforms state-of-the-art algorithms in terms of compression
performance and speed."
"Continuing our previous research on color image compression, we move towards
spectral image compression. This enormous amount of data needs more space to
store and more time to transmit. To manage this sheer amount of data,
researchers have investigated different techniques so that image quality can be
conserved and compressibility can be improved. The principle component analysis
(PCA) can be employed to reduce the dimensions of spectral images to achieve
high compressibility and performance. Due to processing complexity of PCA, a
simple interpolation technique called cubic spline interpolation (CSI) was
considered to reduce the dimensionality of spectral domain of spectral images.
The CSI and PCA were employed one by one in the spectral domain and were
amalgamated with the JPEG, which was employed in spatial domain. Three measures
including compression rate (CR), processing time (Tp) and color difference
CIEDE2000 were used for performance analysis. Test results showed that for a
fixed value of compression rate, CSI based algorithm performed poor in terms of
dE00, in comparison with PCA, but is still reliable because of small color
difference. On the other hand it has lower complexity and is computationally
much better as compared to PCA based algorithm, especially for spectral images
with large size."
"In this work, we propose a novel no-reference (NR) video quality metric that
evaluates the impact of frame freezing due to either packet loss or late
arrival. Our metric uses a trained neural network acting on features that are
chosen to capture the impact of frame freezing on the perceived quality. The
considered features include the number of freezes, freeze duration statistics,
inter-freeze distance statistics, frame difference before and after the freeze,
normal frame difference, and the ratio of them. We use the neural network to
find the mapping between features and subjective test scores. We optimize the
network structure and the feature selection through a cross validation
procedure, using training samples extracted from both VQEG and LIVE video
databases. The resulting feature set and network structure yields accurate
quality prediction for both the training data containing 54 test videos and a
separate testing dataset including 14 videos, with Pearson Correlation
Coefficients greater than 0.9 and 0.8 for the training set and the testing set,
respectively. Our proposed metric has low complexity and could be utilized in a
system with realtime processing constraint."
"The Discrete Cosine Transform (DCT) is widely used in lossy image and video
compression schemes, e.g., JPEG and MPEG. In this paper, we show that the
compression efficiency of the DCT is dependent on the edge directions within a
block. In particular, higher compression ratios are achieved when edges are
aligned with the image axes. To maximize compression for general images, we
propose a rotated block DCT method. It consists of rotating each block, before
applying the DCT, by an angle that aligns the edges, and rotating back the
block in the decompression stage. We show how to compute the rotation angle and
analyze two alternative block rotation approaches. Our experiments show that
our method enables both a perceptual improvement and a PSNR increase of up to
2dB, compared with the standard DCT, for low and medium bit rates."
"This paper proposes a tag identify approach based on fragile Watermark that
based on Least significant bit of the replacement that we first use a special
way to initialize the cover to ensure that we can use random positions to embed
the information of tag. Using this way enhance the security of other to get the
right information of this tag. Finally as long as the covered information can
be decoded, the completeness and accuracy of the tag information can be
guaranteed. the result of simulation experiment show that this approach has
high sensitivity and security ."
"In multiview video systems, multiple cameras generally acquire the same scene
from different perspectives, such that users have the possibility to select
their preferred viewpoint. This results in large amounts of highly redundant
data, which needs to be properly handled during encoding and transmission over
resource-constrained channels. In this work, we study coding and transmission
strategies in multicamera systems, where correlated sources send data through a
bottleneck channel to a central server, which eventually transmits views to
different interactive users. We propose a dynamic correlation-aware packet
scheduling optimization under delay, bandwidth, and interactivity constraints.
The optimization relies both on a novel rate-distortion model, which captures
the importance of each view in the 3D scene reconstruction, and on an objective
function that optimizes resources based on a client navigation model. The
latter takes into account the distortion experienced by interactive clients as
well as the distortion variations that might be observed by clients during
multiview navigation. We solve the scheduling problem with a novel
trellis-based solution, which permits to formally decompose the multivariate
optimization problem thereby significantly reducing the computation complexity.
Simulation results show the gain of the proposed algorithm compared to baseline
scheduling policies. More in details, we show the gain offered by our dynamic
scheduling policy compared to static camera allocation strategies and to
schemes with constant coding strategies. Finally, we show that the best
scheduling policy consistently adapts to the most likely user navigation path
and that it minimizes distortion variations that can be very disturbing for
users in traditional navigation systems."
"In this paper, we present a compressive sampling and Multi-Hypothesis (MH)
reconstruction strategy for video sequences which has a rather simple encoder,
while the decoding system is not that complex. We introduce a convex cost
function that incorporates the MH technique with the sparsity constraint and
the Tikhonov regularization. Consequently, we derive a new iterative algorithm
based on these criteria. This algorithm surpasses its counterparts (Elasticnet
and Tikhonov) in the recovery performance. Besides it is computationally much
faster than the Elasticnet and comparable to the Tikhonov. Our extensive
simulation results confirm these claims."
"With the rapid development of wireless networking and mobile devices, anytime
and anywhere data access becomes readily available nowadays. Given the
crowdsourced content capturing and sharing, the preferred content length
becomes shorter and shorter, even for such multimedia data as video. A
representative is Twitter's Vine service, which, mainly targeting mobile users,
enables them to create ultra-short video clips and instantly post and share
with their followers. In this paper, we present an initial study on this new
generation of instant video clip sharing service enabled by mobile platforms
and explore the potentials towards its further enhancement. We closely
investigate its unique mobile interface, revealing the key differences between
Vine-enabled anytime anywhere data access patterns and that of traditional
counterparts. We then examine the scheduling policy to maximize the user
watching experience as well as the efficiency on the monetary and energy costs.
We show that the generic scheduling problem involves two subproblems, namely,
pre-fetching scheduling and watch-time download scheduling, and develop
effective solutions towards both of them. The superiority of our solution is
demonstrated by extensive trace-driven simulations. To the best of our
knowledge, this is the first work on modeling and optimizing the instant video
clip sharing on mobile devices."
"Display-camera communication has become a promising direction in both
computer vision and wireless communication communities. However, the
consistency of the channel measurement is an open issue since precise
calibration of the experimental setting has not been fully studied in the
literatures. This paper focuses on establishing a scheme for precise
calibration of the display-camera channel performance. To guarantee high
consistency of the experiment, we propose an accurate measurement scheme for
the geometric parameters, and identify some unstable channel factors, e.g.,
Moire effect, rolling shutter effect, blocking artifacts, inconsistency in
auto-focus, trembling and vibration. In the experiment, we first define the
consistency criteria according to the error-prone region in bit error rate
(BER) plots of the channel measurements. It is demonstrated that the
consistency of the experimental result can be improved by the proposed precise
calibration scheme."
"This paper discusses the implementation of the pervasive game PacMap.
Openness and portability have been the main design objectives for PacMap. We
elaborate on programming techniques which may be applicable to a broad range of
location-based games that involve the movement of virtual characters over map
interfaces. In particular, we present techniques to execute shortest path
algorithms on spatial environments bypassing the restrictions imposed by
commercial mapping services. Last, we present ways to improve the movement and
enhance the intelligence of virtual characters taking into consideration the
actions and position of players in location-based games."
"This work provides to web users copyright protection of their Portable
Document Format (PDF) documents by proposing efficient and easily implementable
techniques for PDF watermarking; our techniques are based on the ideas of our
recently proposed watermarking techniques for software, image, and audio,
expanding thus the digital objects that can be efficiently watermarked through
the use of self-inverting permutations. In particular, we present various
representations of a self-inverting permutation $\pi^*$ namely
1D-representation, 2D-representation, and RPG-representation, and show that
theses representations can be efficiently applied to PDF watermarking. Indeed,
we first present an audio-based technique for marking a PDF document $T$ by
exploiting the 1D-representation of a permutation $\pi^*$, and then, since
pages of a PDF document $T$ are 2D objects, we present an image-based algorithm
for encoding $\pi^*$ into $T$ by first mapping the elements of $\pi^*$ into a
matrix $A^*$ and then using the information stored in $A^*$ to mark invisibly
specific areas of PDF document $T$. Finally, we describe a graph-based
watermarking algorithm for embedding a self-inverting permutation $\pi^*$ into
the document structure of a PDF file $T$ by exploiting the RPG-representation
of $\pi^*$ and the structure of a PDF document. We have evaluated the embedding
and extracting algorithms by testing them on various and different in
characteristics PDF documents."
"For watermarking of the digital grayscale image its Gray planes have been
used. With the help of the introduced representation over Gray planes the LSB
embedding method and detection have been discussed. It found that data, a
binary image, hidden in the Gray planes is more robust to JPEG lossy
compression than in the bit planes."
"Due to the wide distribution and usage of digital media, an important issue
is protection of the digital content. There is a number of algorithms and
techniques developed for the digital watermarking.In this paper, the invisible
image watermark procedure is considered. Watermark is created as a pseudo
random sequence, embedded in the certain region of the image, obtained using
Haar wavelet decomposition. Generally, the watermarking procedure should be
robust to the various attacks-filtering, noise etc. Here we assume the
Compressive sensing scenario as a new signal processing technique that may
influence the robustness. The focus of this paper was the possibility of the
watermark detection under Compressive Sensing attack with different number of
available image coefficients. The quality of the reconstructed images has been
evaluated using Peak Signal to Noise Ratio (PSNR).The theory is supported with
experimental results."
"With the rapid development of the multimedia,the secure of the multimedia is
get more concerned. as far as we know , Digital watermarking is an effective
way to protect copyright. The watermark must be generally hidden does not
affect the quality of the original image. In this paper,a novel way based on
discrete cosine transform(DCT) and singular value decomposition(SVD) .In the
proposed way,we decomposition the image into 8*8 blocks, next we use the DCT to
get the transformed block,then we choose the diagonal to embed the information,
after we do this, we recover the image and then we decomposition the image to
8*8 blocks,we use the SVD way to get the diagonal matrix and embed the
information in the matrix. next we extract the information use both inverse of
DCT and SVD, as we all know,after we embed the information seconded time , the
information we first information we embed must be changed, we choose a measure
way called Peak Signal to Noise Ratio(PSNR) to estimate the similarity of the
two image, and set a threshold to ensure whether the information is same or
not."
"In this work, we propose a two-stage video coding framework, as an extension
of our previous one-stage framework in [1]. The two-stage frameworks consists
two different dictionaries. Specifically, the first stage directly finds the
sparse representation of a block with a self-adaptive dictionary consisting of
all possible inter-prediction candidates by solving an L0-norm minimization
problem using an improved orthogonal matching pursuit with embedded
orthonormalization (eOMP) algorithm, and the second stage codes the residual
using DCT dictionary adaptively orthonormalized to the subspace spanned by the
first stage atoms. The transition of the first stage and the second stage is
determined based on both stages' quantization stepsizes and a threshold. We
further propose a complete context adaptive entropy coder to efficiently code
the locations and the coefficients of chosen first stage atoms. Simulation
results show that the proposed coder significantly improves the RD performance
over our previous one-stage coder. More importantly, the two-stage coder, using
a fixed block size and inter-prediction only, outperforms the H.264 coder
(x264) and is competitive with the HEVC reference coder (HM) over a large rate
range."
"Empowered by today's rich tools for media generation and collaborative
production, the multimedia service paradigm is shifting from the conventional
single source, to multi-source, to many sources, and now toward {\em
crowdsource}. Such crowdsourced live streaming platforms as Twitch.tv allow
general users to broadcast their content to massive viewers, thereby greatly
expanding the content and user bases. The resources available for these
non-professional broadcasters however are limited and unstable, which
potentially impair the streaming quality and viewers' experience. The diverse
live interactions among the broadcasters and viewers can further aggravate the
problem.
  In this paper, we present an initial investigation on the modern crowdsourced
live streaming systems. Taking Twitch as a representative, we outline their
inside architecture using both crawled data and captured traffic of local
broadcasters/viewers. Closely examining the access data collected in a
two-month period, we reveal that the view patterns are determined by both
events and broadcasters' sources. Our measurements explore the unique source-
and event-driven views, showing that the current delay strategy on the viewer's
side substantially impacts the viewers' interactive experience, and there is
significant disparity between the long broadcast latency and the short live
messaging latency. On the broadcaster's side, the dynamic uploading capacity is
a critical challenge, which noticeably affects the smoothness of live streaming
for viewers."
"The present work presents an architecture developed to evaluate the QoS
parameters for the IPTV heterogeneous network. At its very basic level lie two
software technologies: Video LAN and Windows Media Services with two operating
systems: Windows and Linux. Three types of streams are analyzed, which will be
transmitted to a Linux VLC client through means of the aggregation and access
servers. The first stream is generated in real time by a capture camera,
processed by the encapsulated VC-1 encoder and sent to the Media Server, while
the second one is of VoD(Video on Demand) type and the third one will be
handled by DVBViewer through the MPEG TS form. The first stream is transcoded
in H.264-AAC such that the Linux stations will recognize its format. Through
the simultaneous transmission of the three streams, we are analyzing their
performance from a QoS parameters point of view by means of an application
implemented in C programming language. The stream transporting the DVB-S
television content was proven to ensure the best performance regarding loss of
packets, delays and jitter."
"This paper considers the use of compressive sensing based algorithms for
velocity estimation of moving vehicles. The procedure is based on sparse
reconstruction algorithms combined with time-frequency analysis applied to
video data. This algorithm provides an accurate estimation of object's velocity
even in the case of a very reduced number of available video frames. The
influence of crucial parameters is analysed for different types of moving
vehicles."
"In this paper, a new Computation-Control Motion Estimation (CCME) method is
proposed which can perform Motion Estimation (ME) adaptively under different
computation or power budgets while keeping high coding performance. We first
propose a new class-based method to measure the Macroblock (MB) importance
where MBs are classified into different classes and their importance is
measured by combining their class information as well as their initial matching
cost information. Based on the new MB importance measure, a complete CCME
framework is then proposed to allocate computation for ME. The proposed method
performs ME in a one-pass flow. Experimental results demonstrate that the
proposed method can allocate computation more accurately than previous methods
and thus has better performance under the same computation budget."
"Motion Estimation (ME) is one of the most time-consuming parts in video
coding. The use of multiple partition sizes in H.264/AVC makes it even more
complicated when compared to ME in conventional video coding standards. It is
important to develop fast and effective sub-pixel ME algorithms since (a) The
computation overhead by sub-pixel ME has become relatively significant while
the complexity of integer-pixel search has been greatly reduced by fast
algorithms, and (b) Reducing sub-pixel search points can greatly save the
computation for sub-pixel interpolation. In this paper, a novel fast sub-pixel
ME algorithm is proposed which performs a 'rough' sub-pixel search before the
partition selection, and performs a 'precise' sub-pixel search for the best
partition. By reducing the searching load for the large number of non-best
partitions, the computation complexity for sub-pixel search can be greatly
decreased. Experimental results show that our method can reduce the sub-pixel
search points by more than 50% compared to existing fast sub-pixel ME methods
with negligible quality degradation."
"Region-of-Interest (ROI) location information in videos has many practical
usages in video coding field, such as video content analysis and user
experience improvement. Although ROI-based coding has been studied widely by
many researchers to improve coding efficiency for video contents, the ROI
location information itself is seldom coded in video bitstream. In this paper,
we will introduce our proposed ROI location coding tool which has been adopted
in surveillance profile of AVS2 video coding standard (surveillance profile).
Our tool includes three schemes: direct-coding scheme, differential- coding
scheme, and reconstructed-coding scheme. We will illustrate the details of
these schemes, and perform analysis of their advantages and disadvantages,
respectively."
"Rate-control plays an important role in video coding. However, in the
conventional rate-control algorithms, the number and position of Macroblocks
(MBs) inside one basic unit for rate-control is inflexible and predetermined.
The different characteristics of the MBs are not fully considered. Also, there
is no overall optimization of the coding of basic units. This paper proposes a
new region-based rate-control scheme for H.264/AVC to improve the coding
efficiency. The inter-frame information is explored to objectively divide one
frame into multiple regions based on their rate-distortion behaviors. The MBs
with the similar characteristics are classified into the same region, and the
entire region instead of a single MB or a group of contiguous MBs is treated as
a basic unit for rate-control. A linear rate-quantization stepsize model and a
linear distortion-quantization stepsize model are proposed to accurately
describe the rate-distortion characteristics for the region-based basic units.
Moreover, based on the above linear models, an overall optimization model is
proposed to obtain suitable Quantization Parameters (QPs) for the region-based
basic units. Experimental results demonstrate that the proposed region-based
rate-control approach can achieve both better subjective and objective quality
by performing the rate-control adaptively with the content, compared to the
conventional rate-control approaches."
"Image Steganography is the process of embedding text in images such that its
existence cannot be detected by Human Visual System (HVS) and is known only to
sender and receiver. This paper presents a novel approach for image
steganography using Hue-Saturation-Intensity (HSI) color space based on Least
Significant Bit (LSB). The proposed method transforms the image from RGB color
space to Hue-Saturation-Intensity (HSI) color space and then embeds secret data
inside the Intensity Plane (I-Plane) and transforms it back to RGB color model
after embedding. The said technique is evaluated by both subjective and
Objective Analysis. Experimentally it is found that the proposed method have
larger Peak Signal-to Noise Ratio (PSNR) values, good imperceptibility and
multiple security levels which shows its superiority as compared to several
existing methods"
"In this paper, a method for enhancing low contrast images is proposed. This
method, called Gaussian Mixture Model based Contrast Enhancement (GMMCE),
brings into play the Gaussian mixture modeling of histograms to model the
content of the images. Based on the fact that each homogeneous area in natural
images has a Gaussian-shaped histogram, it decomposes the narrow histogram of
low contrast images into a set of scaled and shifted Gaussians. The individual
histograms are then stretched by increasing their variance parameters, and are
diffused on the entire histogram by scattering their mean parameters, to build
a broad version of the histogram. The number of Gaussians as well as their
parameters are optimized to set up a GMM with lowest approximation error and
highest similarity to the original histogram. Compared to the existing
histogram-based methods, the experimental results show that the quality of
GMMCE enhanced pictures are mostly consistent and outperform other benchmark
methods. Additionally, the computational complexity analysis show that GMMCE is
a low complexity method."
"A semi-blind watermarking scheme is presented based on Singular Value
Decomposition (SVD), which makes essential use of the fact that, the SVD
subspace preserves significant amount of information of an image and is a one
way decomposition. The principal components are used, along with the
corresponding singular vectors of the watermark image to watermark the target
image. For further security, the semi-blind scheme is extended to an invisible
hash based watermarking scheme. The hash based scheme commits a watermark with
a key such that, it is incoherent with the actual watermark, and can only be
extracted using the key. Its security is analyzed in the random oracle model
and shown to be unforgeable, invisible and satisfying the property of
non-repudiation."
"This paper presents a novel 2-3-3 LSB insertion method. The image
steganography takes the advantage of human eye limitation. It uses color image
as cover media for embedding secret message.The important quality of a
steganographic system is to be less distortive while increasing the size of the
secret message. In this paper a method is proposed to embed a color secret
image into a color cover image. A 2-3-3 LSB insertion method has been used for
image steganography. Experimental results show an improvement in the Mean
squared error (MSE) and Peak Signal to Noise Ratio (PSNR) values of the
proposed technique over the base technique of hash based 3-3-2 LSB insertion."
"Coupled schemes between service-oriented architecture (SOA) and Web 2.0 have
recently been researched. Web-based content providers and telecommunications
company (Telecom) based Internet protocol television (IPTV) providers have
struggled against each other to accommodate more three-screen service
subscribers. Since the advent of Web 2.0, more abundant reproduced content can
be circulated. However, because according to increasing device's resolution and
content formats IPTV providers transcode content in advance, network bandwidth,
storage and operation costs for content management systems (CMSs) are wasted.
In this paper, we present a user centric CMS for open IPTV, which integrates
SOA and Web 2.0. Considering content popularity based on a Zipf-like
distribution to solve these problems, we analyze the performance between the
user centric CMS and the conventional Web syndication system for normalized
costs. Based on the user centric CMS, we implement a social Web TV with
device-aware function, which can aggregate, transcode, and deploy content over
social networking service (SNS) independently."
"Image forensics have attracted wide attention during the past decade. Though
many forensic methods have been proposed to identify image forgeries, most of
them are targeted ones, since their proposed features are highly dependent on
the image operation under investigation. The performance of the well-designed
features for detecting the targeted operation usually degrades significantly
for other operations. On the other hand, a wise attacker can perform
anti-forensics to fool the existing forensic methods, making countering
anti-forensics become an urgent need. In this paper, we try to find a universal
feature to detect various image processing and anti-forensic operations. Based
on our extensive experiments and analysis, we find that any image
processing/anti-forensic operations would inevitably modify many image pixels.
This would change some inherent statistics within original images, which is
similar to the case of steganography. Therefore, we model image
processing/anti-forensic operations as steganography problems, and propose a
detection strategy by applying steganalytic features. With some advanced
steganalytic features, we are able to detect various image operations and
further identify their types. In our experiments, we have tested several
steganalytic features on 11 different kinds of typical image processing
operations and 4 kinds of anti-forensic operations. The experimental results
have shown that the proposed strategy significantly outperforms the existing
forensic methods in both effectiveness and universality."
"To hide a binary pattern in the palette image a steganographic scheme with
blind detection is considered. The embedding algorithm uses the Lehmer code by
palette color permutations for which the cover image palette is generally
required. The found transformation between the palette and RGB images allows to
extract the hidden data without any cover work."
"With the emergence of naked-eye 3D mobile devices, mobile 3D video services
are becoming increasingly important for video service providers, such as
Youtube and Netflix, while multi-view 3D videos have the potential to inspire a
variety of innovative applications. However, enabling multi-view 3D video
services may overwhelm WiFi networks when every view of a video are
multicasted. In this paper, therefore, we propose to incorporate
depth-image-based rendering (DIBR), which allows each mobile client to
synthesize the desired view from nearby left and right views, in order to
effectively reduce the bandwidth consumption. Moreover, when each client
suffers from packet losses, retransmissions incur additional bandwidth
consumption and excess delay, which in turn undermines the quality of
experience in video applications. To address the above issue, we first discover
the merit of view protection via DIBR for multi-view video multicast using a
mathematical analysis and then design a new protocol, named Multi-View Group
Management Protocol (MVGMP), to support the dynamic join and leave of users and
the change of desired views. The simulation results demonstrate that our
protocol effectively reduces bandwidth consumption and increases the
probability for each client to successfully playback the desired views in a
multi-view 3D video."
"In this paper, we investigate an additive video watermarking method in H.264
standard in presence of the Laplacian noise. In some applications, due to the
loss of some pixels or a region of a frame, we resort to Laplacian noise rather
than Gaussian one. The embedding is performed in the transform domain; while an
optimum and a sub-optimum decoder are derived for the proposed Laplacian model.
Simulation results show that the proposed watermarking scheme has suitable
performance with enough transparency required for watermarking applications."
"Image Steganography is a thriving research area of information security where
secret data is embedded in images to hide its existence while getting the
minimum possible statistical detectability. This paper proposes a novel magic
least significant bit substitution method (M-LSB-SM) for RGB images. The
proposed method is based on the achromatic component (I-plane) of the
hue-saturation-intensity (HSI) color model and multi-level encryption (MLE) in
the spatial domain. The input image is transposed and converted into an HSI
color space. The I-plane is divided into four sub-images of equal size,
rotating each sub-image with a different angle using a secret key. The secret
information is divided into four blocks, which are then encrypted using an MLE
algorithm (MLEA). Each sub-block of the message is embedded into one of the
rotated sub-images based on a specific pattern using magic LSB substitution.
Experimental results validate that the proposed method not only enhances the
visual quality of stego images but also provides good imperceptibility and
multiple security levels as compared to several existing prominent methods."
"We consider an interactive multiview video streaming (IMVS) system where
clients select their preferred viewpoint in a given navigation window. To
provide high quality IMVS, many high quality views should be transmitted to the
clients. However, this is not always possible due to the limited and
heterogeneous capabilities of the clients. In this paper, we propose a novel
adaptive IMVS solution based on a layered multiview representation where camera
views are organized into layered subsets to match the different clients
constraints. We formulate an optimization problem for the joint selection of
the views subsets and their encoding rates. Then, we propose an optimal and a
reduced computational complexity greedy algorithms, both based on
dynamic-programming. Simulation results show the good performance of our novel
algorithms compared to a baseline algorithm, proving that an effective IMVS
adaptive solution should consider the scene content and the client capabilities
and their preferences in navigation."
"Due to the growing needs of human motion capture (mocap) in movie, video
games, sports, etc., it is highly desired to compress mocap data for efficient
storage and transmission. This paper presents two efficient frameworks for
compressing human mocap data with low latency. The first framework processes
the data in a frame-by-frame manner so that it is ideal for mocap data
streaming and time critical applications. The second one is clip-based and
provides a flexible tradeoff between latency and compression performance. Since
mocap data exhibits some unique spatial characteristics, we propose a very
effective transform, namely learned orthogonal transform (LOT), for reducing
the spatial redundancy. The LOT problem is formulated as minimizing square
error regularized by orthogonality and sparsity and solved via alternating
iteration. We also adopt a predictive coding and temporal DCT for temporal
decorrelation in the frame- and clip-based frameworks, respectively.
Experimental results show that the proposed frameworks can produce higher
compression performance at lower computational cost and latency than the
state-of-the-art methods."
"Low-rank matrix approximation (LRMA) is a powerful technique for signal
processing and pattern analysis. However, its potential for data compression
has not yet been fully investigated in the literature. In this paper, we
propose sparse low-rank matrix approximation (SLRMA), an effective
computational tool for data compression. SLRMA extends the conventional LRMA by
exploring both the intra- and inter-coherence of data samples simultaneously.
With the aid of prescribed orthogonal transforms (e.g., discrete cosine/wavelet
transform and graph transform), SLRMA decomposes a matrix into a product of two
smaller matrices, where one matrix is made of extremely sparse and orthogonal
column vectors, and the other consists of the transform coefficients.
Technically, we formulate SLRMA as a constrained optimization problem, i.e.,
minimizing the approximation error in the least-squares sense regularized by
$\ell_0$-norm and orthogonality, and solve it using the inexact augmented
Lagrangian multiplier method. Through extensive tests on real-world data, such
as 2D image sets and 3D dynamic meshes, we observe that (i) SLRMA empirically
converges well; (ii) SLRMA can produce approximation error comparable to LRMA
but in a much sparse form; (iii) SLRMA-based compression schemes significantly
outperform the state-of-the-art in terms of rate-distortion performance."
"The importance of data hiding in the field of Information Technology is a
widely accepted. The challenge is to be able to pass information in a manner
that the very existence of the message is unknown in order to repel attention
of the potential attacker. Steganography is a technique that has been widely
used to achieve this objective. However Steganography is often found to be
lacking when it comes to hiding bulk data. Attempting to hide data in a video
overcomes this problem because of the large sized cover object (video) as
compared to an image in the case of steganography. This paper attempts to
propose a scheme using which data can be hidden in a video. We focus on the
Triangularization method and make use of the Least Significant Bit (LSB)
technique in hiding messages in a video."
"An accurate predictor is crucial for histogram-shifting (HS) based reversible
data hiding methods. The embedding capacity is increased and the embedding
distortion is decreased simultaneously if the predictor can generate accurate
predictions. In this paper, we propose an accurate linear predictor based on
weighted least squares (WLS) estimation. The robustness of WLS helps the
proposed predictor generate accurate predictions, especially in complex texture
areas of an image, where other predictors usually fail. To further reduce the
embedding distortion, we propose a new embedding method called dynamic
histogram shifting with pixel selection (DHS-PS) that selects not only the
proper histogram bins but also the proper pixel locations to embed the given
data. As a result, the proposed method can obtain very high fidelity marked
images with low bit-rate data embedded. The experimental results show that the
proposed method outperforms the state-of-the-art low bit-rate reversible data
hiding method."
"The using of the internet with its technologies and applications have been
increased rapidly. So, protecting the text from illegal use is too needed .
Text watermarking is used for this purpose. Arabic text has many
characteristics such existing of diacritics , kashida (extension character) and
points above or under its letters .Each of Arabic letters can take different
shapes with different Unicode. These characteristics are utilized in the
watermarking process. In this paper, several methods are discussed in the area
of Arabic text watermarking with its advantages and disadvantages .Comparison
of these methods is done in term of capacity, robustness and Imperceptibility."
"An undesirable side effect of reversible color space transformation, which
consists of lifting steps, is that while removing correlation it contaminates
transformed components with noise from other components. To remove correlation
without increasing noise, we integrate denoising into the lifting steps and
obtain a reversible image component transformation. For JPEG-LS, JPEG 2000, and
JPEG XR algorithms in lossless mode, we find that the proposed method applied
to the RDgDb color space transformation with a simple denoising filter is
especially effective for images in the native optical resolutions of
acquisition devices, but may lead to increased bitrates for typical images. We
also present an efficient estimator of image component transformation effects."
"In this paper, we propose a new framework for compressive video sensing (CVS)
that exploits the inherent spatial and temporal redundancies of a video
sequence, effectively. The proposed method splits the video sequence into the
key and non-key frames followed by dividing each frame into small
non-overlapping blocks of equal sizes. At the decoder side, the key frames are
reconstructed using adaptively learned sparsifying (ALS) basis via $\ell_0$
minimization, in order to exploit the spatial redundancy. Also, the
effectiveness of three well-known dictionary learning algorithms is
investigated in our method. For recovery of the non-key frames, a prediction of
the current frame is initialized, by using the previous reconstructed frame, in
order to exploit the temporal redundancy. The prediction is employed in a
proper optimization problem to recover the current non-key frame. To compare
our experimental results with the results of some other methods, we employ peak
signal to noise ratio (PSNR) and structural similarity (SSIM) index as the
quality assessor. The numerical results show the adequacy of our proposed
method in CVS."
"To enable Interactive multiview video systems with a minimum view-switching
delay, multiple camera views are sent to the users, which are used as reference
images to synthesize additional virtual views via depth-image-based rendering.
In practice, bandwidth constraints may however restrict the number of reference
views sent to clients per time unit, which may in turn limit the quality of the
synthesized viewpoints. We argue that the reference view selection should
ideally be performed close to the users, and we study the problem of in-network
reference view synthesis such that the navigation quality is maximized at the
clients. We consider a distributed cloud network architecture where data stored
in a main cloud is delivered to end users with the help of cloudlets, i.e.,
resource-rich proxies close to the users. In order to satisfy last-hop
bandwidth constraints from the cloudlet to the users, a cloudlet re-samples
viewpoints of the 3D scene into a discrete set of views (combination of
received camera views and virtual views synthesized) to be used as reference
for the synthesis of additional virtual views at the client. This in-network
synthesis leads to better viewpoint sampling given a bandwidth constraint
compared to simple selection of camera views, but it may however carry a
distortion penalty in the cloudlet-synthesized reference views. We therefore
cast a new reference view selection problem where the best subset of views is
defined as the one minimizing the distortion over a view navigation window
defined by the user under some transmission bandwidth constraints. We show that
the view selection problem is NP-hard, and propose an effective polynomial time
algorithm using dynamic programming to solve the optimization problem.
Simulation results finally confirm the performance gain offered by virtual view
synthesis in the network."
"The demand for keeping the information secure and confidential simultaneously
has been progressively increasing. Among various techniques- Audio
Steganography, a technique of embedding information transparently in a digital
media thereby restricting the access to such information has been prominently
developed. Imperceptibility, robustness, and payload or hiding capacity are the
main character for it. In earlier, LSB techniques increased payload capacity
would hamper robustness as well as imperceptibility of the cover media and vice
versa. The proposed technique overcomes the problem. It provides relatively
good improvement in the payload capacity by dividing the bytes of cover media
into ranges to hide the bits of secret message appropriately. As well as due to
the use of ranges of bytes the robustness of cover media has maintained and
imperceptibility preserved by using a pyramid structure."
"The ability to efficiently switch from one pre-encoded video stream to
another (e.g., for bitrate adaptation or view switching) is important for many
interactive streaming applications. Recently, stream-switching mechanisms based
on distributed source coding (DSC) have been proposed. In order to reduce the
overall transmission rate, these approaches provide a ""merge"" mechanism, where
information is sent to the decoder such that the exact same frame can be
reconstructed given that any one of a known set of side information (SI) frames
is available at the decoder (e.g., each SI frame may correspond to a different
stream from which we are switching). However, the use of bit-plane coding and
channel coding in many DSC approaches leads to complex coding and decoding. In
this paper, we propose an alternative approach for merging multiple SI frames,
using a piecewise constant (PWC) function as the merge operator. In our
approach, for each block to be reconstructed, a series of parameters of these
PWC merge functions are transmitted in order to guarantee identical
reconstruction given the known side information blocks. We consider two
different scenarios. In the first case, a target frame is first given, and then
merge parameters are chosen so that this frame can be reconstructed exactly at
the decoder. In contrast, in the second scenario, the reconstructed frame and
merge parameters are jointly optimized to meet a rate-distortion criteria.
Experiments show that for both scenarios, our proposed merge techniques can
outperform both a recent approach based on DSC and the SP-frame approach in
H.264, in terms of compression efficiency and decoder complexity."
"In the digital watermarking with DCT method,the watermark is located within a
range of DCT coefficients of the cover image. In this paper to use the
low-frequency band, a new method is proposed by using a combination of the DCT
and PCA transform. The proposed method is compared to other DCT methods, our
method is robust and keeps the quality of cover image, also increases capacity
of the watermarking."
"This paper presents a memory efficient VLSI architecture of low complex video
encoder using three dimensional (3-D) wavelet and Compressed Sensing (CS) is
proposed for space and low power video applications. Majority of the
conventional video coding schemes are based on hybrid model, which requires
complex operations like transform coding (DCT), motion estimation and
deblocking filter at the encoder. Complexity of the proposed encoder is reduced
by replacing those complex operations by 3-D DWT and CS at the encoder. The
proposed architecture uses 3-D DWT to enable the scalability with levels of
wavelet decomposition and also to exploit the spatial and the temporal
redundancies. CS provides the good error resilience and coding efficiency. At
the first stage of the proposed architecture for encoder, 3-D DWT has been
applied (Lifting based 2-D DWT in spatial domain and Haar wavelet in temporal
domain) on each frame of the group of frames (GOF), and in the second stage CS
module exploits the sparsity of the wavelet coefficients. Small set of linear
measurements are extracted by projecting the sparse 3-D wavelet coefficients
onto random Bernoulli matrix at the encoder. Compared with the best existing
3-D DWT architectures, the proposed architecture for 3-D DWT requires less
memory and provide high throughput. For an N?N image, the proposed 3-D DWT
architecture consumes a total of only 2?(3N +40P) words of on-chip memory for
the one level of decomposition. The proposed architecture for an encoder is
first of its kind and to the best of my knowledge, no architecture is noted for
comparison. The proposed VLSI architecture of the encoder has been synthesized
on 90-nm CMOS process technology and results show that it consumes 90.08 mW
power and occupies an area equivalent to 416.799 K equivalent gate at frequency
of 158 MHz."
"Multimedia conferencing is the conversational exchange of multimedia content
between multiple parties. It has a wide range of applications (e.g. Massively
Multiplayer Online Games (MMOGs) and distance learning). Many multimedia
conferencing applications use video extensively, thus video mixing in
conferencing settings is of critical importance. Cloud computing is a
technology that can solve the scalability issue in multimedia conferencing,
while bringing other benefits, such as, elasticity, efficient use of resources,
rapid development, and introduction of new applications. However, proposed
cloud-based multimedia conferencing approaches so far have several deficiencies
when it comes to efficient resource usage while meeting Quality of Service
(QoS) requirements. We propose a solution to optimize resource allocation for
cloud-based video mixing service in multimedia conferencing applications, which
can support scalability in terms of number of users, while guaranteeing QoS. We
formulate the resource allocation problem mathematically as an Integer Linear
Programming (ILP) problem and design a heuristic for it. Simulation results
show that our resource allocation model can support more participants compared
to the state-of-the-art, while honoring QoS, with respect to end-to-end delay."
"Nowadays, real-time video communication over the internet through video
conferencing applications has become an invaluable tool in everyone's
professional and personal life. This trend underlines the need for video coding
algorithms that provide acceptable quality on low bitrates and can support
various resolutions inside the same stream in order to cope with limitations on
computational resources and network bandwidth. In this work, a novel scalable
video coding algorithm based on the contourlet transform is presented. The
algorithm utilizes both lossy and lossless methods in order to achieve
compression. One of its most notable features is that due to the transform
utilised, it does not suffer from blocking artifacts that occur with many
widely adopted compression algorithms. The proposed algorithm takes advantage
of the vast computational capabilities of modern GPUs, in order to achieve
real-time performance and provide satisfactory encoding and decoding times at
relatively low cost, making it suitable for applications like video
conferencing. Experiments show that the proposed algorithm performs
satisfactorily in terms of compression ratio and speed, while it outperforms
standard methods in terms of perceptual quality on lower bitrates."
"Low delay video transmission is becoming increasingly important. Delay
critical, video enabled applications range from teleoperation scenarios such as
controlling drones or telesurgery to autonomous control through computer vision
algorithms applied on real-time video. To judge the quality of the video
transmission in such a system, it is important to be able to precisely measure
the end-to-end (E2E) delay of the transmitted video. We present a
low-complexity system that automatically takes pairwise independent
measurements of E2E delay. The precision can be far below the millisecond
order, mainly limited by the sampling rate of the measurement system. In our
implementation, we achieve a precision of 0.5 milliseconds with a sampling rate
of 2kHz."
"Creating web applications for the multiscreen environment is still a
challenge. One approach is to transform existing single-screen applications but
this has not been done yet automatically or generically. This paper proposes a
refactor-ing system. It consists of a generic and extensible mapping phase that
automatically analyzes the application content based on a semantic or a visual
criterion determined by the author or the user, and prepares it for the
splitting process. The system then splits the application and as a result
delivers two instrumented applications ready for distribution across devices.
During runtime, the system uses a mirroring phase to maintain the functionality
of the distributed application and to support a dynamic splitting process.
Developed as a Chrome extension, our approach is validated on several web
applications, including a YouTube page and a video application from Mozilla."
"This paper introduces a new watermarking algorithm based on discrete chaotic
iterations. After defining some coefficients deduced from the description of
the carrier medium, chaotic discrete iterations are used to mix the watermark
and to embed it in the carrier medium. It can be proved that this procedure
generates topological chaos, which ensures that desired properties of a
watermarking algorithm are satisfied."
"Recently, HTTP streaming has become very popular for delivering video over
the Internet. For adaptivity, a provider should generate multiple versions of a
video as well as the related metadata. Various adaptation methods have been
proposed to support a streaming client in coping with strong bandwidth
variations. However, most of existing methods target at constant bitrate (CBR)
videos only. In this paper, we present a new method for quality adaptation in
on-demand streaming of variable bitrate (VBR) videos. To cope with strong
variations of VBR bitrate, we use a local average bitrate as the representative
bitrate of a version. A buffer-based algorithm is then proposed to
conservatively adapt video quality. Through experiments, we show that our
method can provide quality stability as well as buffer stability even under
very strong variations of bandwidth and video bitrates."
"In video coding, it is expected that the encoder could adaptively select the
encoding parameters (e.g., quantization parameter) to optimize the bit
allocation to different sources under the given constraint. However, in hybrid
video coding, the dependency between sources brings high complexity for the bit
allocation optimization, especially in the block-level, and existing
optimization methods mostly focus on frame-level bit allocation. In this paper,
we propose a macroblock (MB) level bit allocation method based on the minimum
maximum (MINMAX) criterion, which has acceptable encoding complexity for
offline applications. An iterative-based algorithm, namely maximum distortion
descend (MDD), is developed to reduce quality fluctuation among MBs within a
frame, where the Structure SIMilarity (SSIM) index is used to measure the
perceptual distortion of MBs. Our extensive experimental results on benchmark
video sequences show that the proposed method can greatly enhance the encoding
performance in terms of both bits saving and perceptual quality improvement."
"Addressing the security concerns in wireless sensor networks (WSN) is a
challenging task, which has attracted the attention of many researchers from
the last few decades. Researchers have presented various schemes in WSN,
addressing the problems of processing, bandwidth, load balancing, and efficient
routing. However, little work has been done on security aspects of WSN. In a
typical WSN network, the tiny nodes installed on different locations sense the
surrounding environment, send the collected data to their neighbors, which in
turn is forwarded to a sink node. The sink node aggregate the data received
from different sensors and send it to the base station for further processing
and necessary actions. In highly critical sensor networks such as military and
law enforcement agencies networks, the transmission of such aggregated data via
the public network Internet is very sensitive and vulnerable to various attacks
and risks. Therefore, this paper provides a solution for addressing these
security issues based on steganography, where the aggregated data can be
embedded as a secret message inside an innocent-looking cover image. The stego
image containing the embedded data can be then sent to fusion center using
Internet. At the fusion center, the hidden data is extracted from the image,
the required processing is performed and decision is taken accordingly.
Experimentally, the proposed method is evaluated by objective analysis using
peak signal-to-noise ratio (PSNR), mean square error (MSE), normalized cross
correlation (NCC), and structural similarity index metric (SSIM), providing
promising results in terms of security and image quality, thus validating its
superiority."
"3D video coding is one of the most popular research area in multimedia. This
paper reviews the recent progress of the coding technologies for multiview
video (MVV) and free view-point video (FVV) which is represented by MVV and
depth maps. We first discuss the traditional multiview video coding (MVC)
framework with different prediction structures. The rate-distortion performance
and the view switching delay of the three main coding prediction structures are
analyzed. We further introduce the joint coding technologies for MVV and depth
maps and evaluate the rate-distortion performance of them. The scalable 3D
video coding technologies are reviewed by the quality and view scalability,
respectively. Finally, we summarize the bit allocation work of 3D video coding.
This paper also points out some future research problems in high efficiency 3D
video coding such as the view switching latency optimization in coding
structure and bit allocation."
"In most steganographic methods, increasing in the capacity leads to decrease
in the quality of the stego-image, so in this paper, we propose to combine two
existing techniques, Pixel value differencing and Gray Level Modification, to
come up with a hybrid steganography scheme which can hide more information
without having to compromise much on the quality of the stego-image.
Experimental results demonstrate that the proposed approach has larger capacity
while its results are imperceptible. In comparison with original PVD method
criterion of the quality is declined by 2% dB averagely while the capacity is
increased around 25%."
"Image Steganography is a growing research area of information security where
secret information is embedded in innocent-looking public communication. This
paper proposes a novel crystographic technique for grayscale images in spatial
domain. The secret data is encrypted and shuffled using pattern based bits
shuffling algorithm (PBSA) and a secret key. The encrypted data is then
embedded in the cover image using magic least significant bit (M-LSB) method.
Experimentally, the proposed method is evaluated by qualitative and
quantitative analysis which validates the effectiveness of the proposed method
in contrast to several state-of-the-art methods."
"The file size and picture quality are factors to be considered for streaming,
storage and transmitting videos over networks. This work compares Cinepak,
Intel, Microsoft Video and Indeo Codec for video compression. The peak signal
to noise ratio is used to compare the quality of such video compressed using
AVI codecs. The most widely used objective measurement by developers of video
processing systems is Peak Signal-to-Noise Ratio (PSNR). Peak Signal to Noise
Ration is measured on a logarithmic scale and depends on the mean squared error
(MSE) between an original and an impaired image or video, relative to (2n-1)2.
  Previous research done regarding assessing of video quality has been mainly
by the use of subjective methods, and there is still no standard method for
objective assessments. Although it has been considered that compression might
not be significant in future as storage and transmission capabilities improve,
but at low bandwidths compression makes communication possible."
"This paper attempts to improve the quality and the modification rate of a
Stego Image. The input image provided for estimating the quality of an image
and the modified rate is a bitmap image. The threshold value is used as a
parameter for selecting the high frequency pixels from the Cover Image. The
data embedding process are performed on the pixels that are found with the help
of Threshold value by using LSBMR. The quality of an image is estimated by the
value of PSNR and the modification rate of an image is estimated by the value
of MSE. The proposed approach achieves about 0.2 to 0.6 % of improvement in the
quality of an image and about 4 to 10 % of improvement in the modification rate
of an image compared to the edge detection techniques such as Sobel and Canny."
"This paper presents a pixel-by-pixel spatial prediction method for lossless
intra coding within High Efficiency Video Coding (HEVC). A well-known previous
pixel-by-pixel spatial prediction method uses only two neighboring pixels for
prediction, based on the angular projection idea borrowed from block-based
intra prediction in lossy coding. This paper explores a method which uses three
neighboring pixels for prediction according to a two-dimensional correlation
model, and the used neighbor pixels and prediction weights change depending on
intra mode. To find the best prediction weights for each intra mode, a
two-stage offline optimization algorithm is used and a number of implementation
aspects are discussed to simplify the proposed prediction method. The proposed
method is implemented in the HEVC reference software and experimental results
show that the explored 3-tap filtering method can achieve an average 11.34%
bitrate reduction over the default lossless intra coding in HEVC. The proposed
method also decreases average decoding time by 12.7% while it increases average
encoding time by 9.7%"
"Multiple watermarking technique, embedding several watermarks in one carrier,
has enabled many interesting applications. In this study, a novel multiple
watermarking algorithm is proposed based on the spirit of spread transform
dither modulation (STDM). It can embed multiple watermarks into the same region
and the same transform domain of one image; meanwhile, the embedded watermarks
can be extracted independently and blindly in the detector without any
interference. Furthermore, to improve the fidelity of the watermarked image,
the properties of the dither modulation quantizer and the proposed multiple
watermarks embedding strategy are investigated, and two practical optimization
methods are proposed. Finally, to enhance the application flexibility, an
extension of the proposed algorithm is proposed which can sequentially embeds
different watermarks into one image during each stage of its circulation.
Compared with the pioneering multiple watermarking algorithms, the proposed one
owns more flexibility in practical application and is more robust against
distortion due to basic operations such as random noise, JPEG compression and
volumetric scaling."
"Many images, of natural or man-made scenes often contain Similar but Genuine
Objects (SGO). This poses a challenge to existing Copy-Move Forgery Detection
(CMFD) methods which match the key points / blocks, solely based on the pair
similarity in the scene. To address such issue, we propose a novel CMFD method
using Scaled Harris Feature Descriptors (SHFD) that preform consistently well
on forged images with SGO. It involves the following main steps: (i) Pyramid
scale space and orientation assignment are used to keep scaling and rotation
invariance; (ii) Combined features are applied for precise texture description;
(iii) Similar features of two points are matched and RANSAC is used to remove
the false matches. The experimental results indicate that the proposed
algorithm is effective in detecting SGO and copy-move forgery, which compares
favorably to existing methods. Our method exhibits high robustness even when an
image is operated by geometric transformation and post-processing"
"Commonsense knowledge representation and reasoning is key for tasks such as
artificial intelligence and natural language understanding. Since commonsense
consists of information that humans take for granted, gathering it is an
extremely difficult task. In this paper, we introduce a novel 3D game engine
for commonsense knowledge acquisition (GECKA3D) which aims to collect
commonsense from game designers through the development of serious games.
GECKA3D integrates the potential of serious games and games with a purpose.
This provides a platform for the acquisition of re-usable and multi-purpose
knowledge, and also enables the development of games that can provide
entertainment value and teach players something meaningful about the actual
world they live in."
"The increasing demand for video streaming services with high Quality of
Experience (QoE) has prompted a lot of research on client-side adaptation logic
approaches. However, most algorithms use the client's previous download
experience and do not use a crowd knowledge database generated by users of a
professional service. We propose a new crowd algorithm that maximizes the QoE.
Additionally, we show how crowd information can be integrated into existing
algorithms and illustrate this with two state-of-the-art algorithms. We
evaluate our algorithm and state-of-the-art algorithms (including our modified
algorithms) on a large, real-life crowdsourcing dataset that contains 336,551
samples on network performance. The dataset was provided by WeFi LTD. Our new
algorithm outperforms all other methods in terms of QoS (eMOS)."
"This paper applies energy conservation principles to the Daala video codec
using gain-shape vector quantization to encode a vector of AC coefficients as a
length (gain) and direction (shape). The technique originates from the CELT
mode of the Opus audio codec, where it is used to conserve the spectral
envelope of an audio signal. Conserving energy in video has the potential to
preserve textures rather than low-passing them. Explicitly quantizing a gain
allows a simple contrast masking model with no signaling cost. Vector
quantizing the shape keeps the number of degrees of freedom the same as scalar
quantization, avoiding redundancy in the representation. We demonstrate how to
predict the vector by transforming the space it is encoded in, rather than
subtracting off the predictor, which would make energy conservation impossible.
We also derive an encoding of the vector-quantized codewords that takes
advantage of their non-uniform distribution. We show that the resulting
technique outperforms scalar quantization by an average of 0.90 dB on still
images, equivalent to a 24.8% reduction in bitrate at equal quality, while for
videos, the improvement averages 0.83 dB, equivalent to a 13.7% reduction in
bitrate."
"This paper presents the deringing filter used in the Daala royalty-free video
codec. The filter is based on a non-linear conditional replacement filter and
is designed for vectorization efficiency. It takes into account the direction
of edges and patterns being filtered. The filter works by identifying the
direction of each block and then adaptively filtering along the identified
direction. In a second pass, the blocks are also filtered in a different
direction, with more conservative thresholds to avoid blurring edges. The
proposed deringing filter is shown to improve the quality of both Daala and the
Alliance for Open Media (AOM) AV1 video codec."
"The Daala project is a royalty-free video codec that attempts to compete with
the best patent-encumbered codecs. Part of our strategy is to replace core
tools of traditional video codecs with alternative approaches, many of them
designed to take perceptual aspects into account, rather than optimizing for
simple metrics like PSNR. This paper documents some of our experiences with
these tools, which ones worked and which did not, and what we've learned from
them. The result is a codec which compares favorably with HEVC on still images,
and is on a path to do so for video as well."
"3D tele-immersion improves the state of collaboration among geographically
distributed participants. Unlike the traditional 2D videos, a 3D tele-immersive
system employs multiple 3D cameras based in each physical site to cover a much
larger field of view, generating a very large amount of stream data. One of the
major challenges is how to efficiently transmit these bulky 3D streaming data
to bandwidth-constrained sites. In this paper, we study an adaptive Human
Visual System (HVS) -compliant bandwidth management framework for efficient
delivery of hundred-scale streams produced from distributed 3D tele-immersive
sites to a receiver site with limited bandwidth budget. Our adaptation
framework exploits the semantics link of HVS with multiple 3D streams in the 3D
tele-immersive environment. We developed TELEVIS, a visual simulation tool to
showcase a HVS-aware tele-immersive system for realistic cases. Our evaluation
results show that the proposed adaptation can improve the total quality per
unit of bandwidth used to deliver streams in 3D tele-immersive systems."
"In a typical video rate allocation problem, the objective is to optimally
distribute a source rate budget among a set of (in)dependently coded data units
to minimize the total distortion of all units. Conventional Lagrangian
approaches convert the lone rate constraint to a linear rate penalty scaled by
a multiplier in the objective, resulting in a simpler unconstrained
formulation. However, the search for the ""optimal"" multiplier, one that results
in a distortion-minimizing solution among all Lagrangian solutions that satisfy
the original rate constraint, remains an elusive open problem in the general
setting. To address this problem, we propose a computation-efficient search
strategy to identify this optimal multiplier numerically. Specifically, we
first formulate a general rate allocation problem where each data unit can be
dependently coded at different quantization parameters (QP) using a previous
unit as predictor, or left uncoded at the encoder and subsequently interpolated
at the decoder using neighboring coded units. After converting the original
rate constrained problem to the unconstrained Lagrangian counterpart, we design
an efficient dynamic programming (DP) algorithm that finds the optimal
Lagrangian solution for a fixed multiplier. Finally, within the DP framework,
we iteratively compute neighboring singular multiplier values, each resulting
in multiple simultaneously optimal Lagrangian solutions, to drive the rates of
the computed Lagrangian solutions towards the bit budget. We terminate when a
singular multiplier value results in two Lagrangian solutions with rates below
and above the bit budget. In extensive monoview and multiview video coding
experiments, we show that our DP algorithm and selection of optimal multipliers
on average outperform comparable rate control solutions used in video
compression standards such as HEVC that do not skip frames in Y-PSNR."
"This paper shows that characterizing co-occurrence between events is an
important but non-trivial and neglected aspect of discovering potential causal
relationships in multimedia event streams. First an introduction to the notion
of event co-occurrence and its relation to co-occurrence pattern detection is
given. Then a finite state automaton extended with a time model and event
parameterization is introduced to convert high level co-occurrence pattern
definition to its corresponding pattern matching automaton. Finally a
processing algorithm is applied to count the occurrence frequency of a
collection of patterns with only one pass through input event streams. The
method proposed in this paper can be used for detecting co-occurrences between
both events of one event stream (Auto co-occurrence), and events from multiple
event streams (Cross co-occurrence). Some fundamental results concerning the
characterization of event co-occurrence are presented in form of a visual co-
occurrence matrix. Reusable causality rules can be extracted easily from
co-occurrence matrix and fed into various analysis tools, such as
recommendation systems and complex event processing systems for further
analysis."
"With the growth of digital networks such as the Internet, digital media have
been explosively developed in e-commerce and online services. This causes
problems such as illegal copy and fake ownership. Watermarking is proposed as
one of the solutions to such cases. Among different watermarking techniques,
the wavelet transform has been used more because of its good ability in
modeling the human visual system. Recently, Shearlet transform as an extension
of Wavelet transform which is based on multi-resolution and multi-directional
analysis is introduced. The most important feature of this transform is the
appropriate representation of image edges. In this paper a hybrid scheme using
Discrete Wavelet Transform (DWT) and Discrete Shearlet Transform (DST) is
presented. In this way, the host image is decomposed using DWT, and then its
low frequency sub-band is decomposed by DST. After that, the bidiagonal
singular value decomposition (BSVD) is applied on the selected sub-band from
Shearlet transform and the gray-scale watermark image is embedded into its
bidiagonal singular values. The proposed method is examined on the images with
different textures and resistance is evaluated against various attacks like
image processing and geometric attacks. The results show good transparency and
high robustness in proposed method."
"Datasets representing the world around us are becoming ever more unwieldy as
data volumes grow. This is largely due to increased measurement and modelling
resolution, but the problem is often exacerbated when data are stored at
spuriously high precisions. In an effort to facilitate analysis of these
datasets, computationally intensive calculations are increasingly being
performed on specialised remote servers before the reduced data are transferred
to the consumer. Due to bandwidth limitations, this often means data are
displayed as simple 2D data visualisations, such as scatter plots or images. We
present here a novel way to efficiently encode and transmit 4D data fields
on-demand so that they can be locally visualised and interrogated. This nascent
""4D video"" format allows us to more flexibly move the boundary between data
server and consumer client. However, it has applications beyond purely
scientific visualisation, in the transmission of data to virtual and augmented
reality."
"This paper presents a novel reversible data hiding (RDH) algorithm for
gray-scaled images, in which the prediction-error of prediction error (PPE) of
a pixel is used to carry the secret data. In the proposed method, the pixels to
be embedded are firstly predicted with their neighboring pixels to obtain the
corresponding prediction errors (PEs). Then, by exploiting the PEs of the
neighboring pixels, the prediction of the PEs of the pixels can be determined.
And, a sorting technique based on the local complexity of a pixel is used to
collect the PPEs to generate an ordered PPE sequence so that, smaller PPEs will
be processed first for data embedding. By reversibly shifting the PPE histogram
(PPEH) with optimized parameters, the pixels corresponding to the altered PPEH
bins can be finally modified to carry the secret data. Experimental results
have implied that the proposed method can benefit from the prediction procedure
of the PEs, sorting technique as well as parameters selection, and therefore
outperform some state-of-the-art works in terms of payload-distortion
performance when applied to different images."
"In pixel-by-pixel spatial prediction methods for lossless intra coding, the
prediction is obtained by a weighted sum of neighbouring pixels. The proposed
prediction approach in this paper uses a weighted sum of three neighbor pixels
according to a two-dimensional correlation model. The weights are obtained
after a three step optimization procedure. The first two stages are offline
procedures where the computed prediction weights are obtained offline from
training sequences. The third stage is an online optimization procedure where
the offline obtained prediction weights are further fine-tuned and adapted to
each encoded block during encoding using a rate-distortion optimized method and
the modification in this third stage is transmitted to the decoder as side
information. The results of the simulations show average bit rate reductions of
12.02% and 3.28% over the default lossless intra coding in HEVC and the
well-known Sample-based Angular Prediction (SAP) method, respectively."
"Computational modeling of visual saliency has become an important research
problem in recent years, with applications in video quality estimation, video
compression, object tracking, retargeting, summarization, and so on. While most
visual saliency models for dynamic scenes operate on raw video, several models
have been developed for use with compressed-domain information such as motion
vectors and transform coefficients. This paper presents a comparative study of
eleven such models as well as two high-performing pixel-domain saliency models
on two eye-tracking datasets using several comparison metrics. The results
indicate that highly accurate saliency estimation is possible based only on a
partially decoded video bitstream. The strategies that have shown success in
compressed-domain saliency modeling are highlighted, and certain challenges are
identified as potential avenues for further improvement."
"The tremendous growth in 3D (stereo) imaging and display technologies has led
to stereoscopic content (video and image) becoming increasingly popular.
However, both the subjective and the objective evaluation of stereoscopic video
content has not kept pace with the rapid growth of the content. Further, the
availability of standard stereoscopic video databases is also quite limited. In
this work, we attempt to alleviate these shortcomings. We present a
stereoscopic video database and its subjective evaluation. We have created a
database containing a set of 144 distorted videos. We limit our attention to
H.264 compression artifacts. The distorted videos were generated using 6
uncompressed pristine videos of left and right views originally created by
Goldmann et al. at EPFL [1]. Further, 19 subjects participated in the
subjective assessment task. Based on the subjective study, we have formulated a
relation between the 2D and stereoscopic subjective scores as a function of
compression rate and depth range. We have also evaluated the performance of
popular 2D and 3D image/video quality assessment (I/VQA) algorithms on our
database."
"This paper presents the comparison of compression algorithms for voice
transferring method over SMS in satellite communication. Voice transferring
method over SMS is useful in situations when signal strength is low and due to
poor signal strength voice call connection is not possible to initiate or
signal dropped during voice call. This method has one serious flaw that it
produces large number of SMS while converting voice into SMS. Such issue is
catered to some extend by employing any compression algorithm. In this paper
our major aim is to find best compression scheme for said method, for that
purpose we compare 6 different types of compression algorithms which are; LZW
(Lempel-Ziv-Welch), Huffman coding, PPM (Prediction by partial matching),
Arithmetic Coding (AC), BWT (Burrows-Wheeler-Transform), LZMA
(Lempel-Ziv-Markov chain). This comparison shows that PPM compression method
offers better compression ratio and produce small number of SMS. For
experimentation we use Thuraya SG-2520 satellite phone. Moreover, we develop an
application using J2ME platform[Ref:a]. We tested that application more than
100 times and then we compare the result in terms of compression ratio of each
algorithm and number of connected SMS produce after each compression method.
The result of this study will help developers to choose better compression
scheme for their respective applications.
http://www.learnrnd.com/news.php?id=ISSUES_IN_MOLECULAR_COMMUNICATIONS"
"If object contours in images are coded efficiently as side information, then
they can facilitate advanced image / video coding techniques, such as graph
Fourier transform coding or motion prediction of arbitrarily shaped pixel
blocks. In this paper, we study the problem of lossless and lossy compression
of detected contours in images. Specifically, we first convert a detected
object contour composed of contiguous between-pixel edges to a sequence of
directional symbols drawn from a small alphabet. To encode the symbol sequence
using arithmetic coding, we compute an optimal variable-length context tree
(VCT) $\mathcal{T}$ via a maximum a posterior (MAP) formulation to estimate
symbols' conditional probabilities. MAP prevents us from overfitting given a
small training set $\mathcal{X}$ of past symbol sequences by identifying a VCT
$\mathcal{T}$ that achieves a high likelihood $P(\mathcal{X}|\mathcal{T})$ of
observing $\mathcal{X}$ given $\mathcal{T}$, and a large geometric prior
$P(\mathcal{T})$ stating that image contours are more often straight than
curvy. For the lossy case, we design efficient dynamic programming (DP)
algorithms that optimally trade off coding rate of an approximate contour
$\hat{\mathbf{x}}$ given a VCT $\mathcal{T}$ with two notions of distortion of
$\hat{\mathbf{x}}$ with respect to the original contour $\mathbf{x}$. To reduce
the size of the DP tables, a total suffix tree is derived from a given VCT
$\mathcal{T}$ for compact table entry indexing, reducing complexity.
Experimental results show that for lossless contour coding, our proposed
algorithm outperforms state-of-the-art context-based schemes consistently for
both small and large training datasets. For lossy contour coding, our
algorithms outperform comparable schemes in the literature in rate-distortion
performance."
"This paper presents a real time video based pointing method which allows
sketching and writing of English text over air in front of mobile camera.
Proposed method have two main tasks: first it track the colored finger tip in
the video frames and then apply English OCR over plotted images in order to
recognize the written characters. Moreover, proposed method provides a natural
human-system interaction in such way that it do not require keypad, stylus, pen
or glove etc for character input. For the experiments, we have developed an
application using OpenCv with JAVA language. We tested the proposed method on
Samsung Galaxy3 android mobile. Results show that proposed algorithm gains the
average accuracy of 92.083% when tested for different shaped alphabets. Here,
more than 3000 different Magnetic 3D shaped characters were used [Ref:
http://learnrnd.com/news.php?id=Magnetic_3D_Bio_Printing]. Our proposed system
is the software based approach and relevantly very simple, fast and easy. It
does not require sensors or any hardware rather than camera and red tape.
Moreover, proposed methodology can be applicable for all disconnected languages
but having one issue that it is color sensitive in such a way that existence of
any red color in the background before starting the character writing can lead
to false results."
"In this paper, a new reversible data hiding (RDH) algorithm that is based on
the concept of shifting of prediction error histograms is proposed. The
algorithm extends the efficient modification of prediction errors (MPE)
algorithm by incorporating two predictors and using one prediction error value
for data embedding. The motivation behind using two predictors is driven by the
fact that predictors have different prediction accuracy which is directly
related to the embedding capacity and quality of the stego image. The key
feature of the proposed algorithm lies in using two predictors without the need
to communicate additional overhead with the stego image. Basically, the
identification of the predictor that is used during embedding is done through a
set of rules. The proposed algorithm is further extended to use two and three
bins in the prediction errors histogram in order to increase the embedding
capacity. Performance evaluation of the proposed algorithm and its extensions
showed the advantage of using two predictors in boosting the embedding capacity
while providing competitive quality for the stego image."
"For mobile video codecs, the huge energy dissipation for external memory
traffic is a critical challenge under the battery power constraint. Lossy
embedded compression (EC), as a solution to this challenge, is considered in
this paper. While previous studies in EC mostly focused on compression
algorithms at the block level, this work, to the best of our knowledge, is the
first one that addresses the allocation of video quality and memory traffic at
the frame level. For lossy EC, a main difficulty of its application lies in the
error propagation from quality degradation of reference frames. Instinctively,
it is preferred to perform more lossy EC in non-reference frames to minimize
the quality loss. The analysis and experiments in this paper, however, will
show lossy EC should actually be distributed to more frames. Correspondingly,
for hierarchical-B GOPs, we developed an efficient allocation that outperforms
the non-reference-only allocation by up to 4.5 dB in PSNR. In comparison, the
proposed allocation also delivers more consistent quality between frames by
having lower PSNR fluctuation."
"By utilizing previously known areas in an image, intra-prediction techniques
can find a good estimate of the current block. This allows the encoder to store
only the error between the original block and the generated estimate, thus
leading to an improvement in coding efficiency. Standards such as AVC and HEVC
describe expert-designed prediction modes operating in certain angular
orientations alongside separate DC and planar prediction modes. Being designed
predictors, while these techniques have been demonstrated to perform well in
image and video coding applications, they do not necessarily fully utilize
natural image structures. In this paper, we describe a novel system for
developing predictors derived from natural image blocks. The proposed algorithm
is seeded with designed predictors (e.g. HEVC-style prediction) and allowed to
iteratively refine these predictors through regularized regression. The
resulting prediction models show significant improvements in estimation quality
over their designed counterparts across all conditions while maintaining
reasonable computational complexity. We also demonstrate how the proposed
algorithm handles the worst-case scenario of intra-prediction with no error
reporting."
"Although HTTP-based video streaming can easily penetrate firewalls and profit
from Web caches, the underlying TCP may introduce large delays in case of a
sudden capacity loss. To avoid an interruption of the video stream in such
cases we propose the Backward-Shifted Coding (BSC). Based on Scalable Video
Coding (SVC), BSC adds a time-shifted layer of redundancy to the video stream
such that future frames are downloaded at any instant. This pre-fetched content
maintains a fluent video stream even under highly variant network conditions
and leads to high Quality of Experience (QoE). We characterize this QoE gain by
analyzing initial buffering time, re-buffering time and content resolution
using the Ballot theorem. The probability generating functions of the playback
interruption and of the initial buffering latency are provided in closed form.
We further compute the quasi-stationary distribution of the video quality, in
order to compute the average quality, as well as temporal variability in video
quality. Employing these analytic results to optimize QoE shows interesting
trade-offs and video streaming at outstanding fluency."
"Daala is a new royalty-free video codec based on perceptually-driven coding
techniques. We explore using its keyframe format for still picture coding and
show how it has improved over the past year. We believe the technology used in
Daala could be the basis of an excellent, royalty-free image format."
"Many approaches have been proposed to support lossless coding within video
coding standards that are primarily designed for lossy coding. The simplest
approach is to just skip transform and quantization and directly entropy code
the prediction residual, which is used in HEVC version 1. However, this simple
approach is inefficient for compression. More efficient approaches include
processing the residual with DPCM prior to entropy coding. This paper explores
an alternative approach based on processing the residual with
integer-to-integer (i2i) transforms. I2i transforms map integers to integers,
however, unlike the integer transforms used in HEVC for lossy coding, they do
not increase the dynamic range at the output and can be used in lossless
coding. Experiments with the HEVC reference software show competitive results."
"It is desirable to support efficient lossless coding within video coding
standards, which are primarily designed for lossy coding, with as little
modification as possible. A simple approach is to skip transform and
quantization, and directly entropy code the prediction residual, but this is
inefficient for compression. A more efficient and popular approach is to
process the residual block with DPCM prior to entropy coding. This paper
explores an alternative approach based on processing the residual block with
integer-to-integer (i2i) transforms. I2i transforms map integers to integers,
however, unlike the integer transforms used in HEVC for lossy coding, they do
not increase the dynamic range at the output and can be used in lossless
coding. We use both an i2i DCT from the literature and a novel i2i
approximation of the DST. Experiments with the HEVC reference software show
competitive results."
"For the past few years, in the race between image steganography and
steganalysis, deep learning has emerged as a very promising alternative to
steganalyzer approaches based on rich image models combined with ensemble
classifiers. A key knowledge of image steganalyzer, which combines relevant
image features and innovative classification procedures, can be deduced by a
deep learning approach called Convolutional Neural Networks (CNN). These kind
of deep learning networks is so well-suited for classification tasks based on
the detection of variations in 2D shapes that it is the state-of-the-art in
many image recognition problems. In this article, we design a CNN-based
steganalyzer for images obtained by applying steganography with a unique
embedding key. This one is quite different from the previous study of {\em Qian
et al.} and its successor, namely {\em Pibre et al.} The proposed architecture
embeds less convolutions, with much larger filters in the final convolutional
layer, and is more general: it is able to deal with larger images and lower
payloads. For the ""same embedding key"" scenario, our proposal outperforms all
other steganalyzers, in particular the existing CNN-based ones, and defeats
many state-of-the-art image steganography schemes."
"Traditional intra prediction usually utilizes the nearest reference line to
generate the predicted block when considering strong spatial correlation.
However, this kind of single line-based method does not always work well due to
at least two issues. One is the incoherence caused by the signal noise or the
texture of other object, where this texture deviates from the inherent texture
of the current block. The other reason is that the nearest reference line
usually has worse reconstruction quality in block-based video coding. Due to
these two issues, this paper proposes an efficient multiple line-based intra
prediction scheme to improve coding efficiency. Besides the nearest reference
line, further reference lines are also utilized. The further reference lines
with relatively higher quality can provide potential better prediction. At the
same time, the residue compensation is introduced to calibrate the prediction
of boundary regions in a block when we utilize further reference lines. To
speed up the encoding process, this paper designs several fast algorithms.
Experimental results show that, compared with HM-16.9, the proposed fast search
method achieves 2.0% bit saving on average and up to 3.7%, with increasing the
encoding time by 112%."
"Recent years have witnessed a dramatic increase of user-generated video
services. In such user-generated video services, crowdsourced live streaming
(e.g., Periscope, Twitch) has significantly challenged today's edge network
infrastructure: today's edge networks (e.g., 4G, Wi-Fi) have limited uplink
capacity support, making high-bitrate live streaming over such links
fundamentally impossible. In this paper, we propose to let broadcasters (i.e.,
users who generate the video) upload crowdsourced video streams using
aggregated network resources from multiple edge networks. There are several
challenges in the proposal: First, how to design a framework that aggregates
bandwidth from multiple edge networks? Second, how to make this framework
transparent to today's crowdsourced live streaming services? Third, how to
maximize the streaming quality for the whole system? We design a
multi-objective and deployable bandwidth aggregation system BASS to address
these challenges: (1) We propose an aggregation framework transparent to
today's crowdsourced live streaming services, using an edge proxy box and
aggregation cloud paradigm; (2) We dynamically allocate geo-distributed cloud
aggregation servers to enable MPTCP (i.e., multi-path TCP), according to
location and network characteristics of both broadcasters and the original
streaming servers; (3) We maximize the overall performance gain for the whole
system, by matching streams with the best aggregation paths."
"To provide a live, active and high-quality virtual touring streaming
experience, we propose an unmanned drone stereoscopic streaming paradigm using
a control and streaming infrastructure of a 2.4GHz Wi-Fi grid. Our system
allows users to actively control the streaming captured by a drone, receive and
watch the streaming using a head mount display (HMD); a Wi-Fi grid is deployed
across the remote scene with multi-channel support to enable high-bitrate
stream- ing broadcast from the drones. The system adopt a joint view adaptation
and drone control scheme to enable fast viewer movement including both head
rotation and touring. We implement the prototype on Dji M100 quadcopter and HTC
Vive in a demo scene."
"Multimedia content delivery over the Internet is predominantly using the
Hypertext Transfer Protocol (HTTP) as its primary protocol and multiple
proprietary solutions exits. The MPEG standard Dynamic Adaptive Streaming over
HTTP (DASH) provides an interoperable solution and in recent years various
adaptation logics/algorithms have been proposed. However, to the best of our
knowledge, there is no comprehensive evaluation of the various
logics/algorithms. Therefore, this paper provides a comprehensive evaluation of
ten different adaptation logics/algorithms, which have been proposed in the
past years. The evaluation is done both objectively and subjectively. The
former is using a predefined bandwidth trajectory within a controlled
environment and the latter is done in a real-world environment adopting
crowdsourcing. The results shall provide insights about which strategy can be
adopted in actual deployment scenarios. Additionally, the evaluation
methodology described in this paper can be used to evaluate any other/new
adaptation logic and to compare it directly with the results reported here."
"In recent years, due to the powerful abilities to deal with highly complex
tasks, the artificial neural networks (ANNs) have been studied in the hope of
achieving human-like performance in many applications. Since the ANNs have the
ability to approximate complex functions from observations, it is
straightforward to consider the ANNs for steganography. In this paper, we aim
to implement the well-known LSB substitution and matrix coding steganography
with the feed-forward neural networks (FNNs). Our experimental results have
shown that, the used FNNs can achieve the data embedding operation of the LSB
substitution and matrix coding steganography. For steganography with the ANNs,
though there may be some challenges to us, it would be very promising and
valuable to pay attention to the ANNs for steganography, which may be a new
direction for steganography."
"Several existing and successful full reference image quality assessment (IQA)
models use linear color transformation and downsampling before measuring
similarity or quality of images. This paper indicates to the right order of
these two procedures and that the existing models have not chosen the more
efficient approach. In addition, efficiency of these metrics is not compared in
a fair basis in the literature."
"The massive growth of sports videos has resulted in a need for automatic
generation of sports highlights that are comparable in quality to the
hand-edited highlights produced by broadcasters such as ESPN. Unlike previous
works that mostly use audio-visual cues derived from the video, we propose an
approach that additionally leverages contextual cues derived from the
environment that the game is being played in. The contextual cues provide
information about the excitement levels in the game, which can be ranked and
selected to automatically produce high-quality basketball highlights. We
introduce a new dataset of 25 NCAA games along with their play-by-play stats
and the ground-truth excitement data for each basket. We explore the
informativeness of five different cues derived from the video and from the
environment through user studies. Our experiments show that for our study
participants, the highlights produced by our system are comparable to the ones
produced by ESPN for the same games."
"This technical report formally defines the QoE metrics which are introduced
and discussed in the article ""QoE Beyond the MOS: An In-Depth Look at QoE via
Better Metrics and their Relation to MOS"" by Tobias Ho{\ss}feld, Poul E.
Heegaard, Martin Varela, Sebastian M\""oller, accepted for publication in the
Springer journal ""Quality and User Experience"". Matlab scripts for computing
the QoE metrics for given data sets are available in GitHub."
"Multihoming for a video Content Delivery Network (CDN) allows edge peering
servers to deliver video chunks through different Internet Service Providers
(ISPs), to achieve an improved quality of service (QoS) for video streaming
users. However, since traditional strategies for a multihoming video CDN are
simply designed according to static rules, e.g., simply sending traffic via a
ISP which is the same as the ISP of client, they fail to dynamically allocate
resources among different ISPs over time. In this paper, we perform measurement
studies to demonstrate that such static allocation mechanism is inefficient to
make full utilization of multiple ISPs' resources. To address this problem, we
propose a dynamic flow scheduling strategy for multihoming video CDN. The
challenge is to find the control parameters that can guide the ISP selection
when performing flow scheduling. Using a data-driven approach, we find factors
that have a major impact on the performance improvement in the dynamic flow
scheduling. We further utilize an information gain approach to generate
parameter combinations that can be used to guide the flow scheduling, i.e., to
determine the ISP each request should be responded by. Our evaluation results
demonstrate that our design effectively performs the flow scheduling. In
particular, our design yields near optimal performance in a simulation of
real-world multihoming setup."
"Software watermarking has received considerable attention and was adopted by
the software development community as a technique to prevent or discourage
software piracy and copyright infringement. A wide range of software
watermarking techniques has been proposed among which the graph-based methods
that encode watermarks as graph structures. Following up on our recently
proposed methods for encoding watermark numbers $w$ as reducible permutation
flow-graphs $F[\pi^*]$ through the use of self-inverting permutations $\pi^*$,
in this paper, we extend the types of flow-graphs available for software
watermarking by proposing two different reducible permutation flow-graphs
$F_1[\pi^*]$ and $F_2[\pi^*]$ incorporating important properties which are
derived from the bitonic subsequences composing the self-inverting permutation
$\pi^*$. We show that a self-inverting permutation $\pi^*$ can be efficiently
encoded into either $F_1[\pi^*]$ or $F_2[\pi^*]$ and also efficiently decoded
from theses graph structures. The proposed flow-graphs $F_1[\pi^*]$ and
$F_2[\pi^*]$ enrich the repository of graphs which can encode the same
watermark number $w$ and, thus, enable us to embed multiple copies of the same
watermark $w$ into an application program $P$. Moreover, the enrichment of that
repository with new flow-graphs increases our ability to select a graph
structure more similar to the structure of a given application program $P$
thereby enhancing the resilience of our codec system to attacks."
"The growing needs for high-quality video applications have resulted in a lot
of studies and developments in video signal coding. This chapter presents some
advanced techniques in enhancing the rate-distortion performance of the
block-based hybrid video coding systems. Additionally, as can be seen from the
developments of H.264/AVC and HEVC, most of the current coding tools, such as
prediction, transformation and entropy coding, have less room to improve in the
compression performance. On the other hand, loop filer in the modern video
standards shows the promising results. Thus, we believe that loop filter can be
the candidate in contributing to higher video compression for the
next-generation video coding. Specifically, improvements on ALF and SAO are
also introduced, and the simulation results show that the proposed methods
outperform the existing method, which offer new degrees of freedom to improve
the overall rate-distortion performance. As a result, they can be the candidate
coding tools for the next-generation video codec."
"Preserving details in restoring images highly corrupted by impulse noise
remains a challenging problem. We proposed an algorithm based on radial basis
functions (RBF) interpolation which estimates the intensities of corrupted
pixels by their neighbors. In this algorithm, first intensity values of noisy
pixels in the corrupted image are estimated using RBFs. Next, the image is
smoothed. The proposed algorithm can effectively remove the highly dense
impulse noise. Experimental results show the superiority of the proposed
algorithm in comparison to the recent similar methods both in noise suppression
and detail preservation. Extensive simulations show better results in measure
of peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM),
especially when the image is corrupted by very highly dense impulse noise."
"In order to improve bitrates of lossless JPEG 2000, we propose to modify the
discrete wavelet transform (DWT) by skipping selected steps of its computation.
We employ a heuristic to construct the skipped steps DWT (SS-DWT) in an
image-adaptive way and define fixed SS-DWT variants. For a large and diverse
set of images, we find that SS-DWT significantly improves bitrates of
non-photographic images. From a practical standpoint, the most interesting
results are obtained by applying entropy estimation of coding effects for
selecting among the fixed SS-DWT variants. This way we get the compression
scheme that, as opposed to the general SS-DWT case, is compliant with the JPEG
2000 part 2 standard. It provides average bitrate improvement of roughly 5% for
the entire test-set, whereas the overall compression time becomes only 3%
greater than that of the unmodified JPEG 2000. Bitrates of photographic and
non-photographic images are improved by roughly 0.5% and 14%, respectively. At
a significantly increased cost of exploiting a heuristic, selecting the steps
to be skipped based on the actual bitrate instead of an estimated one, and by
applying reversible denoising and lifting steps to SS-DWT, we have attained
greater bitrate improvements of up to about 17.5% for non-photographic images."
"Daala is a new royalty-free video codec that attempts to compete with
state-of-the-art royalty-bearing codecs. To do so, it must achieve good
compression while avoiding all of their patented techniques. We use technology
that is as different as possible from traditional approaches to achieve this.
This paper describes the technology behind Daala and discusses where it fits in
the newly created AV1 codec from the Alliance for Open Media. We show that
Daala is approaching the performance level of more mature, state-of-the art
video codecs and can contribute to improving AV1."
"Reading text is one of the essential needs of the visually impaired people.
We developed a mobile system that can read Turkish scene and book text, using a
fast gradient-based multi-scale text detection algorithm for real-time
operation and Tesseract OCR engine for character recognition. We evaluated the
OCR accuracy and running time of our system on a new, publicly available mobile
Turkish scene text dataset we constructed and also compared with
state-of-the-art systems. Our system proved to be much faster, able to run on a
mobile device, with OCR accuracy comparable to the state-of-the-art."
"Lossy image and video compression algorithms yield visually annoying
artifacts including blocking, blurring, and ringing, especially at low
bit-rates. To reduce these artifacts, post-processing techniques have been
extensively studied. Recently, inspired by the great success of convolutional
neural network (CNN) in computer vision, some researches were performed on
adopting CNN in post-processing, mostly for JPEG compressed images. In this
paper, we present a CNN-based post-processing algorithm for High Efficiency
Video Coding (HEVC), the state-of-the-art video coding standard. We redesign a
Variable-filter-size Residue-learning CNN (VRCNN) to improve the performance
and to accelerate network training. Experimental results show that using our
VRCNN as post-processing leads to on average 4.6% bit-rate reduction compared
to HEVC baseline. The VRCNN outperforms previously studied networks in
achieving higher bit-rate reduction, lower memory cost, and multiplied
computational speedup."
"Crowdsourced Live Streaming (CLS), most notably Twitch.tv, has seen explosive
growth in its popularity in the past few years. In such systems, any user can
lively broadcast video content of interest to others, e.g., from a game player
to many online viewers. To fulfill the demands from both massive and
heterogeneous broadcasters and viewers, expensive server clusters have been
deployed to provide video ingesting and transcoding services. Despite the
existence of highly popular channels, a significant portion of the channels is
indeed unpopular. Yet as our measurement shows, these broadcasters are
consuming considerable system resources; in particular, 25% (resp. 30%) of
bandwidth (resp. computation) resources are used by the broadcasters who do not
have any viewers at all. In this paper, we closely examine the challenge of
handling unpopular live-broadcasting channels in CLS systems and present a
comprehensive solution for service partitioning on hybrid cloud. The
trace-driven evaluation shows that our hybrid cloud-assisted design can smartly
assign ingesting and transcoding tasks to the elastic cloud virtual machines,
providing flexible system deployment cost-effectively."
"Thumbnails play such an important role in online videos. As the most
representative snapshot, they capture the essence of a video and provide the
first impression to the viewers; ultimately, a great thumbnail makes a video
more attractive to click and watch. We present an automatic thumbnail selection
system that exploits two important characteristics commonly associated with
meaningful and attractive thumbnails: high relevance to video content and
superior visual aesthetic quality. Our system selects attractive thumbnails by
analyzing various visual quality and aesthetic metrics of video frames, and
performs a clustering analysis to determine the relevance to video content,
thus making the resulting thumbnails more representative of the video. On the
task of predicting thumbnails chosen by professional video editors, we
demonstrate the effectiveness of our system against six baseline methods, using
a real-world dataset of 1,118 videos collected from Yahoo Screen. In addition,
we study what makes a frame a good thumbnail by analyzing the statistical
relationship between thumbnail frames and non-thumbnail frames in terms of
various image quality features. Our study suggests that the selection of a good
thumbnail is highly correlated with objective visual quality metrics, such as
the frame texture and sharpness, implying the possibility of building an
automatic thumbnail selection system based on visual aesthetics."
"Interactive multi-view video streaming (IMVS) services permit to remotely
immerse within a 3D scene. This is possible by transmitting a set of reference
camera views (anchor views), which are used by the clients to freely navigate
in the scene and possibly synthesize additional viewpoints of interest. From a
networking perspective, the big challenge in IMVS systems is to deliver to each
client the best set of anchor views that maximizes the navigation quality,
minimizes the view-switching delay and yet satisfies the network constraints.
Integrating adaptive streaming solutions in free-viewpoint systems offers a
promising solution to deploy IMVS in large and heterogeneous scenarios, as long
as the multi-view video representations on the server are properly selected. We
therefore propose to optimize the multi-view data at the server by minimizing
the overall resource requirements, yet offering a good navigation quality to
the different users. We propose a video representation set optimization for
multiview adaptive streaming systems and we show that it is NP-hard. We
therefore introduce the concept of multi-view navigation segment that permits
to cast the video representation set selection as an integer linear programming
problem with a bounded computational complexity. We then show that the proposed
solution reduces the computational complexity while preserving optimality in
most of the 3D scenes. We then provide simulation results for different classes
of users and show the gain offered by an optimal multi-view video
representation selection compared to recommended representation sets (e.g.,
Netflix and Apple ones) or to a baseline representation selection algorithm
where the encoding parameters are decided a priori for all the views."
"HEVC HM 16 includes a Coding Unit (CU) level perceptual quantization
technique named AdaptiveQP. AdaptiveQP adjusts the Quantization Parameter (QP)
at the CU level based on the spatial activity of samples in the four
constituent NxN sub-blocks of the luma Coding Block (CB), which is contained
within a 2Nx2N CU. In this paper, we propose C-BAQ, which, in contrast to
AdaptiveQP, adjusts the CU level QP according to the spatial activity of
samples in the four constituent NxN sub-blocks of both the luma and chroma CBs.
By computing the sum of luma, chroma Cb and chroma Cr spatial activity in a CU,
a richer reflection of spatial activity in the CU is attained. Therefore, a
more appropriate CU level QP can be selected, thus leading to important
improvements in terms of coding efficiency. We evaluate the proposed technique
in HEVC HM 16.7 using 4:4:4, 4:2:2 and 4:2:0 YCbCr sequences. Both subjective
and objective evaluations are undertaken during which we compare C-BAQ with
AdaptiveQP. The objective evaluation reveals that C-BAQ attains a maximum
BD-Rate reduction of 15.9% (Y), 13.1% (Cr) and 16.1% (Cb) in addition to a
maximum decoding time reduction of 11.0%."
"Visual Display Units (VDUs), capable of displaying video data at High
Definition (HD) and Ultra HD (UHD) resolutions, are frequently employed in a
variety of technological domains. Quantization-induced video compression
artifacts, which are usually unnoticeable in low resolution environments, are
typically conspicuous on high resolution VDUs and video data. The default
quantization matrices (QMs) in HEVC do not take into account specific display
resolutions of VDUs or video data to determine the appropriate levels of
quantization required to reduce unwanted compression artifacts. Therefore, we
propose a novel, adaptive quantization matrix technique for the HEVC standard
including Scalable HEVC (SHVC). Our technique, which is based on a refinement
of the current QM technique in HEVC, takes into consideration specific display
resolutions of the target VDUs in order to minimize compression artifacts. We
undertake a thorough evaluation of the proposed technique by utilizing SHVC SHM
9.0 (two-layered bit-stream) and the BD-Rate and SSIM metrics. For the BD-Rate
evaluation, the proposed method achieves maximum BD-Rate reductions of 56.5% in
the enhancement layer. For the SSIM evaluation, our technique achieves a
maximum structural improvement of 0.8660 vs. 0.8538."
"We make an intensive use of multimedia frameworks in our research on modeling
the perceived quality estimation in streaming services and real-time
communications. In our preliminary work, we have used the VLC VOD software to
generate reference audiovisual files with various degree of coding and network
degradations. We have successfully built machine learning based models on the
subjective quality dataset we have generated using these files. However,
imperfections in the dataset introduced by the multimedia framework we have
used prevented us from achieving the full potential of these models.
  In order to develop better models, we have re-created our end-to-end
multimedia pipeline using the GStreamer framework for audio and video
streaming. A GStreamer based pipeline proved to be significantly more robust to
network degradations than the VLC VOD framework and allowed us to stream a
video flow at a loss rate up to 5\% packet very easily. GStreamer has also
enabled us to collect the relevant RTCP statistics that proved to be more
accurate than network-deduced information. This dataset is free to the public.
The accuracy of the statistics eventually helped us to generate better
performing perceived quality estimation models.
  In this paper, we present the implementation of these VLC and GStreamer-based
multimedia communication quality assessment testbeds with the references to
their publicly available code bases."
"While the daily user of digital, Internet-enabled devices has some explicit
control over what they read and see, the providers fulfilling searches,
offering options, and presenting material are using increasingly sophisticated
real-time algorithms that tune and target content for the particular user. They
redefine the historical relationships between tellers and users, providing a
responsiveness paralleled only by forms of live performance incorporating
elements of improvisation and audience interaction. The general accessibility
of algorithmically driven content delivery techniques suggests significant
untapped potential for new approaches to narrative beyond advertising and
commercially orientated customization."
"The delivery and display of 360-degree videos on Head-Mounted Displays (HMDs)
presents many technical challenges. 360-degree videos are ultra high resolution
spherical videos, which contain an omnidirectional view of the scene. However
only a portion of this scene is displayed on the HMD. Moreover, HMD need to
respond in 10 ms to head movements, which prevents the server to send only the
displayed video part based on client feedback. To reduce the bandwidth waste,
while still providing an immersive experience, a viewport-adaptive 360-degree
video streaming system is proposed. The server prepares multiple video
representations, which differ not only by their bit-rate, but also by the
qualities of different scene regions. The client chooses a representation for
the next segment such that its bit-rate fits the available throughput and a
full quality region matches its viewing. We investigate the impact of various
spherical-to-plane projections and quality arrangements on the video quality
displayed to the user, showing that the cube map layout offers the best quality
for the given bit-rate budget. An evaluation with a dataset of users navigating
360-degree videos demonstrates that segments need to be short enough to enable
frequent view switches."
"The main task of HTTP Adaptive Streaming is to adapt video quality
dynamically under variable network conditions. This is a key feature for
multimedia delivery especially when quality of service cannot be granted
network-wide and, e.g., throughput may suffer short term fluctuations.
  Hence, robust bitrate adaptation schemes become crucial in order to improve
video quality. The objective, in this context, is to control the filling level
of the playback buffer and maximize the quality of the video, while avoiding
unnecessary video quality variations.
  In this paper we study bitrate adaptation algorithms based on
Backward-Shifted Coding (BSC), a scalable video coding scheme able to greatly
improve video quality. We design bitrate adaptation algorithms that balance
video rate smoothness and high network capacity utilization, leveraging both on
throughput-based and buffer-based adaptation mechanisms.
  Extensive simulations using synthetic and real-world video traffic traces
show that the proposed scheme performs remarkably well even under challenging
network conditions."
"The advancement in digital technologies have made it possible to produce
perfect copies of digital content. In this environment, malicious users
reproduce the digital content and share it without compensation to the content
owner. Content owners are concerned about the potential loss of revenue and
reputation from piracy, especially when the content is available over the
Internet. Digital watermarking has emerged as a deterrent measure towards such
malicious activities. Several methods have been proposed for copyright
protection and fingerprinting of digital images. However, these methods are not
applicable to text documents as these documents lack rich texture information
which is abundantly available in digital images. In this paper, a framework
(mPDF) is proposed which facilitates the usage of digital image watermarking
algorithms on text documents. The proposed method divides a text document into
texture and non-texture blocks using an energy-based approach. After
classification, a watermark is embedded inside the texture blocks in a content
adaptive manner. The proposed method is integrated with five known image
watermarking methods and its performance is studied in terms of quality and
robustness. Experiments are conducted on documents in 11 different languages.
Experimental results clearly show that the proposed method facilitates the
usage of image watermarking algorithms on text documents and is robust against
attacks such as print & scan, print screen, and skew. Also, the proposed method
overcomes the drawbacks of existing text watermarking methods such as manual
inspection and language dependency."
"The Daala project is a royalty-free video codec that attempts to compete with
the best patent-encumbered codecs. Part of our strategy is to replace core
tools of traditional video codecs with alternative approaches, many of them
designed to take perceptual aspects into account, rather than optimizing for
simple metrics like PSNR. This paper documents some of our experiences with
these tools, which ones worked and which did not. We evaluate which tools are
easy to integrate into a more traditional codec design, and show results in the
context of the codec being developed by the Alliance for Open Media."
"The latest High Efficiency Video Coding (HEVC) standard significantly
improves coding efficiency over its previous video coding standards. The
expense of such improvement is enormous computational complexity, from both
encoding and decoding sides. Since computational capability and power capacity
are diverse across portable devices, it is necessary to reduce decoding
complexity to a target with tolerable quality loss, so called complexity
control. This paper proposes a Saliency-Guided Complexity Control (SGCC)
approach for HEVC decoding, which reduces the decoding complexity to the target
with minimal perceptual quality loss. First, saliency map of each video frame
is predicted in compressed domain from HEVC bitstreams, as the preliminary for
assessing perceptual quality. Based on detected saliency, we establish the SGCC
formulation to minimize perceptual quality loss at the constraint on reduced
decoding complexity, which is achieved via disabling Deblocking Filter (DF) and
simplifying Motion Compensation (MC) of some non-salient Coding Tree Units
(CTUs). One important component in this formulation is the modelled
relationship between decoding complexity reduction and DF disabling/MC
simplification, which determines the control accuracy of our approach. Another
component is the modelled relationship between quality loss and DF disabling/MC
simplification, responsible for optimizing perceptual quality. By solving the
SGCC formulation, we can obtain the DF and MC states of each CTU given a target
complexity, and then decoding complexity can be reduced to the target. Finally,
the experimental results validate the effectiveness of our SGCC approach, from
the aspects of control performance, complexity-distortion performance,
fluctuation of quality loss and subjective quality."
"Image Forensics has already achieved great results for the source camera
identification task on images. Standard approaches for data coming from Social
Network Platforms cannot be applied due to different processes involved (e.g.,
scaling, compression, etc.). Over 1 billion images are shared each day on the
Internet and obtaining information about their history from the moment they
were acquired could be exploited for investigation purposes. In this paper, a
classification engine for the reconstruction of the history of an image, is
presented. Specifically, exploiting K-NN and decision trees classifiers and
a-priori knowledge acquired through image analysis, we propose an automatic
approach that can understand which Social Network Platform has processed an
image and the software application used to perform the image upload. The engine
makes use of proper alterations introduced by each platform as features.
Results, in terms of global accuracy on a dataset of 2720 images, confirm the
effectiveness of the proposed strategy."
"Sending compressed video data in error-prone environments (like the Internet
and wireless networks) might cause data degradation. Error concealment
techniques try to conceal the received data in the decoder side. In this paper,
an adaptive boundary matching algorithm is presented for recovering the damaged
motion vectors (MVs). This algorithm uses an outer boundary matching or
directional temporal boundary matching method to compare every boundary of
candidate macroblocks (MBs), adaptively. It gives a specific weight according
to the accuracy of each boundary of the damaged MB. Moreover, if each of the
adjacent MBs is already concealed, different weights are given to the
boundaries. Finally, the MV with minimum adaptive boundary distortion is
selected as the MV of the damaged MB. Experimental results show that the
proposed algorithm can improve both objective and subjective quality of
reconstructed frames without any considerable computational complexity. The
average PSNR in some frames of test sequences increases about 5.20, 5.78, 5.88,
4.37, 4.41, and 3.50 dB compared to average MV, classic boundary matching,
directional boundary matching, directional temporal boundary matching, outer
boundary matching, and dynamical temporal error concealment algorithm,
respectively."
"Simple quality metrics such as PSNR are known to not correlate well with
subjective quality when tested across a wide spectrum of video content or
quality regime. Recently, efforts have been made in designing objective quality
metrics trained on subjective data (e.g. VMAF), demonstrating better
correlation with video quality perceived by human. Clearly, the accuracy of
such a metric heavily depends on the quality of the subjective data that it is
trained on. In this paper, we propose a new approach to recover subjective
quality scores from noisy raw measurements, using maximum likelihood
estimation, by jointly estimating the subjective quality of impaired videos,
the bias and consistency of test subjects, and the ambiguity of video contents
all together. We also derive closed-from expression for the confidence interval
of each estimate. Compared to previous methods which partially exploit the
subjective information, our approach is able to exploit the information in
full, yielding tighter confidence interval and better handling of outliers
without the need for z-scoring or subject rejection. It also handles missing
data more gracefully. Finally, as side information, it provides interesting
insights on the test subjects and video contents."
"Deep learning frameworks have recently achieved superior performance in many
pattern recognition problems. However, adoption of deep learning in image
steganalysis is still in its initial stage. In this paper we propose a hybrid
deep-learning framework for JPEG steganalysis incorporating the domain
knowledge behind rich steganalytic models. We prove that the convolution phase
and the quantization & truncation phase of the rich models are not learnable in
deep convolutional neural networks. Based on theoretical analysis, our proposed
framework involves two main stages. The first stage is hand-crafted,
corresponding to the convolution phase and the quantization & truncation phase
of the rich models. The second stage is a compound deep neural network
containing three deep subnets in which the model parameters are learned in the
training procedure. By doing so, we ably combine some merits of rich models
into our proposed deep-learning framework. We have conducted extensive
experiments on a large-scale dataset extracted from ImageNet. The primary
dataset used in our experiments contains 500,000 cover images, while our
largest dataset contains five million cover images. Our experiments show that
the proposed framework outperforms all other state-of-the-art steganalytic
models either hand-crafted or learned using deep networks in the literature.
Furthermore, we demonstrate that our framework is insensitive to JPEG blocking
artifact alterations and the learned model can be easily transferred to a
different attacking target. Both of these properties are of critical importance
in practical applications. According to our best knowledge, This is the first
report of deep learning in image steganalysis validated with large-scale test
data."
"The Multilingual Visual Sentiment Ontology (MVSO) consists of 15,600 concepts
in 12 different languages that are strongly related to emotions and sentiments
expressed in images. These concepts are defined in the form of Adjective-Noun
Pair (ANP), which are crawled and discovered from online image forum Flickr. In
this work, we used Amazon Mechanical Turk as a crowd-sourcing platform to
collect human judgments on sentiments expressed in images that are uniformly
sampled over 3,911 English ANPs extracted from a tag-restricted subset of MVSO.
Our goal is to use the dataset as a benchmark for the evaluation of systems
that automatically predict sentiments in images or ANPs."
"Steganography schemes are designed with the objective of minimizing a defined
distortion function. In most existing state of the art approaches, this
distortion function is based on image feature preservation. Since smooth
regions or clean edges define image core, even a small modification in these
areas largely modifies image features and is thus easily detectable. On the
contrary, textures, noisy or chaotic regions are so difficult to model that the
features having been modified inside these areas are similar to the initial
ones. These regions are characterized by disturbed level curves. This work
presents a new distortion function for steganography that is based on second
order derivatives, which are mathematical tools that usually evaluate level
curves. Two methods are explained to compute these partial derivatives and have
been completely implemented. The first experiments show that these approaches
are promising."
"Recently, multidimensional signal reconstruction using a low number of
measurements is of great interest. Therefore, an effective sampling scheme
which should acquire the most information of signal using a low number of
measurements is required. In this paper, we study a novel cube-based method for
sampling and reconstruction of multidimensional signals. First, inspired by the
block-based compressive sensing (BCS), we divide a group of pictures (GoP) in a
video sequence into cubes. By this way, we can easily store the measurement
matrix and also easily can generate the sparsifying basis. The reconstruction
process also can be done in parallel. Second, along with the Kronecker
structure of the sampling matrix, we design a weight matrix based on the human
visuality system, i.e. perceptually. We will also benefit from different
weighted $\ell_1$-minimization methods for reconstruction. Furthermore,
conventional methods for BCS consider an equal number of samples for all
blocks. However, the sparsity order of blocks in natural images could be
different and, therefore, a various number of samples could be required for
their reconstruction. Motivated by this point, we will adaptively allocate the
samples for each cube in a video sequence. Our aim is to show that our simple
linear sampling approach can be competitive with the other state-of-the-art
methods."
"In this paper, we present a novel pseudo sequence based 2-D hierarchical
reference structure for light-field image compression. In the proposed scheme,
we first decompose the light-field image into multiple views and organize them
into a 2-D coding structure according to the spatial coordinates of the
corresponding microlens. Then we mainly develop three technologies to optimize
the 2-D coding structure. First, we divide all the views into four quadrants,
and all the views are encoded one quadrant after another to reduce the
reference buffer size as much as possible. Inside each quadrant, all the views
are encoded hierarchically to fully exploit the correlations between different
views. Second, we propose to use the distance between the current view and its
reference views as the criteria for selecting better reference frames for each
inter view. Third, we propose to use the spatial relative positions between
different views to achieve more accurate motion vector scaling. The whole
scheme is implemented in the reference software of High Efficiency Video
Coding. The experimental results demonstrate that the proposed novel
pseudo-sequence based 2-D hierarchical structure can achieve maximum 14.2%
bit-rate savings compared with the state-of-the-art light-field image
compression method."
"A depth image provides partial geometric information of a 3D scene, namely
the shapes of physical objects as observed from a particular viewpoint. This
information is important when synthesizing images of different virtual camera
viewpoints via depth-image-based rendering (DIBR). It has been shown that depth
images can be efficiently coded using contour-adaptive codecs that preserve
edge sharpness, resulting in visually pleasing DIBR-synthesized images.
However, contours are typically losslessly coded as side information (SI),
which is expensive if the object shapes are complex.
  In this paper, we pursue a new paradigm in depth image coding for
color-plus-depth representation of a 3D scene: we pro-actively simplify object
shapes in a depth and color image pair to reduce depth coding cost, at a
penalty of a slight increase in synthesized view distortion. Specifically, we
first mathematically derive a distortion upper-bound proxy for 3DSwIM---a
quality metric tailored for DIBR-synthesized images. This proxy reduces
interdependency among pixel rows in a block to ease optimization. We then
approximate object contours via a dynamic programming (DP) algorithm to
optimally trade off coding cost of contours using arithmetic edge coding (AEC)
with our proposed view synthesis distortion proxy. We modify the depth and
color images according to the approximated object contours in an inter-view
consistent manner. These are then coded respectively using a contour-adaptive
image codec based on graph Fourier transform (GFT) for edge preservation and
HEVC intra. Experimental results show that by maintaining sharp but simplified
object contours during contour-adaptive coding, for the same visual quality of
DIBR-synthesized virtual views, our proposal can reduce depth image coding rate
by up to 22% compared to alternative coding strategies such as HEVC intra."
"HEVC includes a Coding Unit (CU) level luminance-based perceptual
quantization technique known as AdaptiveQP. AdaptiveQP perceptually adjusts the
Quantization Parameter (QP) at the CU level based on the spatial activity of
raw input video data in a luma Coding Block (CB). In this paper, we propose a
novel cross-color channel adaptive quantization scheme which perceptually
adjusts the CU level QP according to the spatial activity of raw input video
data in the constituent luma and chroma CBs; i.e., the combined spatial
activity across all three color channels (the Y, Cb and Cr channels). Our
technique is evaluated in HM 16 with 4:4:4, 4:2:2 and 4:2:0 YCbCr JCT-VC test
sequences. Both subjective and objective visual quality evaluations are
undertaken during which we compare our method with AdaptiveQP. Our technique
achieves considerable coding efficiency improvements, with maximum BD-Rate
reductions of 15.9% (Y), 13.1% (Cr) and 16.1% (Cb) in addition to a maximum
decoding time reduction of 11.0%."
"The recent rise of interest in Virtual Reality (VR) came with the
availability of commodity commercial VR prod- ucts, such as the Head Mounted
Displays (HMD) created by Oculus and other vendors. To accelerate the user
adoption of VR headsets, content providers should focus on producing high
quality immersive content for these devices. Similarly, multimedia streaming
service providers should enable the means to stream 360 VR content on their
platforms. In this study, we try to cover different aspects related to VR
content representation, streaming, and quality assessment that will help
establishing the basic knowledge of how to build a VR streaming system."
"Conventional state-of-the-art image steganalysis approaches usually consist
of a classifier trained with features provided by rich image models. As both
features extraction and classification steps are perfectly embodied in the deep
learning architecture called Convolutional Neural Network (CNN), different
studies have tried to design a CNN-based steganalyzer. The network designed by
Xu et al. is the first competitive CNN with the combination Spatial Rich Models
(SRM) and Ensemble Classifier (EC) providing detection performances of the same
order. In this work we propose a criterion to choose either the CNN or the
SRM+EC method for a given input image. Our approach is studied with three
different steganographic spatial domain algorithms: S-UNIWARD, MiPOD, and HILL,
using the Tensorflow computing platform, and exhibits detection capabilities
better than each method alone. Furthermore, as SRM+EC and the CNN are both only
trained with a single embedding algorithm, namely MiPOD, the proposed method
can be seen as an approach for blind steganalysis. In blind detection, error
rates are respectively of 16% for S-UNIWARD, 16% for MiPOD, and 17% for HILL on
the BOSSBase with a payload of 0.4 bpp. For 0.1 bpp, the respective
corresponding error rates are of 39%, 38%, and 41%, and are always better than
the ones provided by SRM+EC."
"Copy-move forgery is the most popular and simplest image manipulation method.
In this type of forgery, an area from the image copied, then after post
processing such as rotation and scaling, placed on the destination. The goal of
Copy-move forgery is to hide or duplicate one or more objects in the image.
Key-point based Copy-move forgery detection methods have five main steps:
preprocessing, feature extraction, matching, transform estimation and post
processing that matching and transform estimation have important effect on the
detection. More over the error could happens in some steps due to the noise.
The existing methods process these steps separately and in case of having an
error in a step, this error could be propagated to the following steps and
affects the detection. To solve the above mentioned problem, in this paper the
steps of the detection system interact with each other and if an error happens
in a step, following steps are trying to detect and solve it. We formulate this
interaction by defining and optimizing a cost function. This function includes
matching and transform estimation steps. Then in an iterative procedure the
steps are executed and in case of detecting error, the error will be corrected.
The efficiency of the proposed method analyzed in diverse cases such as pixel
image precision level on the simple forgery images, robustness to the rotation
and scaling, detecting professional forgery images and the precision of the
transformation matrix. The results indicate the better efficiency of the
proposed method."
"A new methodology to measure coded image/video quality using the
just-noticeable-difference (JND) idea was proposed. Several small JND-based
image/video quality datasets were released by the Media Communications Lab at
the University of Southern California. In this work, we present an effort to
build a large-scale JND-based coded video quality dataset. The dataset consists
of 220 5-second sequences in four resolutions (i.e., $1920 \times 1080$, $1280
\times 720$, $960 \times 540$ and $640 \times 360$). For each of the 880 video
clips, we encode it using the H.264 codec with $QP=1, \cdots, 51$ and measure
the first three JND points with 30+ subjects. The dataset is called the
""VideoSet"", which is an acronym for ""Video Subject Evaluation Test (SET)"". This
work describes the subjective test procedure, detection and removal of outlying
measured data, and the properties of collected JND data. Finally, the
significance and implications of the VideoSet to future video coding research
and standardization efforts are pointed out. All source/coded video clips as
well as measured JND data included in the VideoSet are available to the public
in the IEEE DataPort."
"Recently merging signal processing techniques with information security
services has found lots of attentions. Steganography and steganalysis are among
these emerging trends. Like their counterparts in cryptology, steganography and
steganalysis are in a constant battle- steganography methods try to hide the
presence of covert messages in innocuous-looking data, whereas steganalysis
methods try to reveal existence of such messages and to break steganography
methods. The stream nature of audio signals, their popularity, and their wide
spread usage makes them very suitable media for steganography. This has led to
a very rich literature on both steganography and steganalysis of audio signals.
This review intends to conduct a comprehensive survey of audio steganalysis
methods aggregated over near fifteen years. Furthermore, we implement some of
the most recent audio steganalysis methods and conduct a comparative analysis
on their performances. Finally, the paper provides some possible directions for
future researches on audio steganalysis."
"Calibration and higher order statistics (HOS) are standard components of many
image steganalysis systems. These techniques have not yet found adequate
attention in audio steganalysis context. Specifically, most of current works
are either non-calibrated or only based on noise removal approach. This paper
aims to fill these gaps by proposing a new set of calibrated features based on
re-embedding technique. Additionally, we show that least significant bit (LSB)
is the most sensitive bit-plane to data hiding algorithms and therefore it can
be employed as a universal embedding method. Furthermore, the proposed features
are based on a model that has the maximum deviation from human auditory system
(HAS), and therefore are more suitable for the purpose of steganalysis.
Performance of the proposed method is evaluated on a wide range of data hiding
algorithms in both targeted and universal paradigms. Simulation results show
that the proposed method can detect the finest traces of data hiding algorithms
and in very low embedding rates. The system detects steghide at capacity of
0.06 bit per symbol (BPS) with sensitivity of 98.6% (music) and 78.5% (speech).
These figures are respectively 7.1% and 27.5% higher than state-of-the-art
results based on RMFCC."
"We demonstrate an adaptive bandwidth-efficient 360 VR video streaming system
based on MPEG-DASH SRD. We extend MPEG-DASH SRD to the 3D space of 360 VR
videos, and showcase a dynamic view-aware adaptation technique to tackle the
high bandwidth demands of streaming 360 VR videos to wireless VR headsets. We
spatially partition the underlying 3D mesh into multiple 3D sub-meshes, and
construct an efficient 3D geometry mesh called hexaface sphere to optimally
represent tiled 360 VR videos in the 3D space. We then spatially divide the 360
videos into multiple tiles while encoding and packaging, use MPEG-DASH SRD to
describe the spatial relationship of tiles in the 3D space, and prioritize the
tiles in the Field of View (FoV) for view-aware adaptation. Our initial
evaluation results show that we can save up to 72% of the required bandwidth on
360 VR video streaming with minor negative quality impacts compared to the
baseline scenario when no adaptations is applied."
"Lately, World Wide Web came up with an evolution in the niche of
videoconference applications. Latest technologies give browsers a capacity to
initiate real-time communications. WebRTC is one of the free and open source
projects that aim at providing the users freedom to enjoy real-time
communications, and it does so by following and redefining the standards.
However, WebRTC is still a new project and it lacks some high-end
videoconferencing features such as media mixing, recording of a session and
different network conditions adaptation. This paper is an attempt at analyzing
the shortcomings and challenges faced by WebRTC and proposing a Multipoint
Control Unit or traditional communications entity based architecture as a
solution."
"One of the serious issues in communication between people is hiding
information from others, and the best way for this, is deceiving them. Since
nowadays face images are mostly used in three dimensional format, in this paper
we are going to steganography 3D face images, detecting which by curious people
will be impossible. As in detecting face only its texture is important, we
separate texture from shape matrices, for eliminating half of the extra
information, steganography is done only for face texture, and for
reconstructing 3D face, we can use any other shape. Moreover, we will indicate
that, by using two textures, how two 3D faces can be combined. For a complete
description of the process, first, 2D faces are used as an input for building
3D faces, and then 3D textures are hidden within other images."
"In this paper, we propose a novel CS approach in which the acquisition of
non-visible information is also avoided."
"In this paper, we study a simplified affine motion model based coding
framework to overcome the limitation of translational motion model and maintain
low computational complexity. The proposed framework mainly has three key
contributions. First, we propose to reduce the number of affine motion
parameters from 6 to 4. The proposed four-parameter affine motion model can not
only handle most of the complex motions in natural videos but also save the
bits for two parameters. Second, to efficiently encode the affine motion
parameters, we propose two motion prediction modes, i.e., advanced affine
motion vector prediction combined with a gradient-based fast affine motion
estimation algorithm and affine model merge, where the latter attempts to reuse
the affine motion parameters (instead of the motion vectors) of neighboring
blocks. Third, we propose two fast affine motion compensation algorithms. One
is the one-step sub-pixel interpolation, which reduces the computations of each
interpolation. The other is the interpolation-precision-based adaptive block
size motion compensation, which performs motion compensation at the block level
rather than the pixel level to reduce the interpolation times. Our proposed
techniques have been implemented based on the state-of-the-art high efficiency
video coding standard, and the experimental results show that the proposed
techniques altogether achieve on average 11.1% and 19.3% bits saving for random
access and low delay configurations, respectively, on typical video sequences
that have rich rotation or zooming motions. Meanwhile, the computational
complexity increases of both encoder and decoder are within an acceptable
range."
"Inspired by the recent advances of image super-resolution using convolutional
neural network (CNN), we propose a CNN-based block up-sampling scheme for intra
frame coding. A block can be down-sampled before being compressed by normal
intra coding, and then up-sampled to its original resolution. Different from
previous studies on down/up-sampling-based coding, the up-sampling methods in
our scheme have been designed by training CNN instead of hand-crafted. We
explore a new CNN structure for up-sampling, which features deconvolution of
feature maps, multi-scale fusion, and residue learning, making the network both
compact and efficient. We also design different networks for the up-sampling of
luma and chroma components, respectively, where the chroma up-sampling CNN
utilizes the luma information to boost its performance. In addition, we design
a two-stage up-sampling process, the first stage being within the
block-by-block coding loop, and the second stage being performed on the entire
frame, so as to refine block boundaries. We also empirically study how to set
the coding parameters of down-sampled blocks for pursuing the frame-level
rate-distortion optimization. Our proposed scheme is implemented into the High
Efficiency Video Coding (HEVC) reference software, and a comprehensive set of
experiments have been performed to evaluate our methods. Experimental results
show that our scheme achieves significant bits saving compared with HEVC anchor
especially at low bit rates, leading to on average 5.5% BD-rate reduction on
common test sequences and on average 9.0% BD-rate reduction on ultra high
definition (UHD) test sequences."
"The paper presents quantitative analysis of the video quality losses in the
homogenous HEVC video transcoder. With the use of HM15.0 reference software and
a set of test video sequences, cascaded pixel domain video transcoder (CPDT)
concept has been used to gather all the necessary data needed for the analysis.
This experiment was done for wide range of source and target bitrates. The
essential result of the work is extensive evaluation of CPDT, commonly used as
a reference in works on effective video transcoding. Until now no such
extensively performed study have been made available in the literature. Quality
degradation between transcoded video and the video that would be result of
direct compression of the original video at the same bitrate as the transcoded
one have been reported. The dependency between quality degradation caused by
transcoding and the bitrate changes of the transcoded data stream are clearly
presented on graphs."
"Today's Internet has witnessed an increase in the popularity of mobile video
streaming, which is expected to exceed 3/4 of the global mobile data traffic by
2019. To satisfy the considerable amount of mobile video requests, video
service providers have been pushing their content delivery infrastructure to
edge networks--from regional CDN servers to peer CDN servers (e.g.,
smartrouters in users' homes)--to cache content and serve users with storage
and network resources nearby. Among the edge network content caching paradigms,
Wi-Fi access point caching and cellular base station caching have become two
mainstream solutions. Thus, understanding the effectiveness and performance of
these solutions for large-scale mobile video delivery is important. However,
the characteristics and request patterns of mobile video streaming are unclear
in practical wireless network. In this paper, we use real-world datasets
containing 50 million trace items of nearly 2 million users viewing more than
0.3 million unique videos using mobile devices in a metropolis in China over 2
weeks, not only to understand the request patterns and user behaviors in mobile
video streaming, but also to evaluate the effectiveness of Wi-Fi and
cellular-based edge content caching solutions. To understand performance of
edge content caching for mobile video streaming, we first present temporal and
spatial video request patterns, and we analyze their impacts on caching
performance using frequency-domain and entropy analysis approaches. We then
study the behaviors of mobile video users, including their mobility and
geographical migration behaviors. Using trace-driven experiments, we compare
strategies for edge content caching including LRU and LFU, in terms of
supporting mobile video requests. Moreover, we design an efficient caching
strategy based on the measurement insights and experimentally evaluate its
performance."
"HEVC (MPEG-H Part 2 and H.265) is a new coding technology which is expected
to be deployed on the market along with new video services in the near future.
HEVC is a successor of currently widely used AVC (MPEG-4 Part 10 and H.264). In
this paper, the quality coding gains obtained for the Cascaded Pixel Domain
Transcoder of AVC-coded material to HEVC standard are reported. Extensive
experiments showed that transcoding with bitrate reduction allows the
achievement of better rate-distortion performance than by compressing an
original video sequence with the use of AVC at the same (reduced) bitrate."
"Mobile streaming video data accounts for a large and increasing percentage of
wireless network traffic. The available bandwidths of modern wireless networks
are often unstable, leading to difficulties in delivering smooth, high-quality
video. Streaming service providers such as Netflix and YouTube attempt to adapt
their systems to adjust in response to these bandwidth limitations by changing
the video bitrate or, failing that, allowing playback interruptions
(rebuffering). Being able to predict end user' quality of experience (QoE)
resulting from these adjustments could lead to perceptually-driven network
resource allocation strategies that would deliver streaming content of higher
quality to clients, while being cost effective for providers. Existing
objective QoE models only consider the effects on user QoE of video quality
changes or playback interruptions. For streaming applications, adaptive network
strategies may involve a combination of dynamic bitrate allocation along with
playback interruptions when the available bandwidth reaches a very low value.
Towards effectively predicting user QoE, we propose Video Assessment of
TemporaL Artifacts and Stalls (Video ATLAS): a machine learning framework where
we combine a number of QoE-related features, including objective quality
features, rebuffering-aware features and memory-driven features to make QoE
predictions. We evaluated our learning-based QoE prediction model on the
recently designed LIVE-Netflix Video QoE Database which consists of practical
playout patterns, where the videos are afflicted by both quality changes and
rebuffering events, and found that it provides improved performance over
state-of-the-art video quality metrics while generalizing well on different
datasets. The proposed algorithm is made publicly available at
http://live.ece.utexas.edu/research/Quality/VideoATLAS release_v2.rar."
"This paper presents a novel method for detection of LSB matching steganogra-
phy in grayscale images. This method is based on the analysis of the
differences between neighboring pixels before and after random data embedding.
In natu- ral images, there is a strong correlation between adjacent pixels.
This correla- tion is disturbed by LSB matching generating new types of
correlations. The pre- sented method generates patterns from these correlations
and analyzes their varia- tion when random data are hidden. The experiments
performed for two different image databases show that the method yields better
classification accuracy com- pared to prior art for both LSB matching and HUGO
steganography. In addition, although the method is designed for the spatial
domain, some experiments show its applicability also for detecting JPEG
steganography."
"The quality of experience (QoE) is known to be subjective and
context-dependent. Identifying and calculating the factors that affect QoE is
indeed a difficult task. Recently, a lot of effort has been devoted to estimate
the users QoE in order to improve video delivery. In the literature, most of
the QoE-driven optimization schemes that realize trade-offs among different
quality metrics have been addressed under the assumption of homogenous
populations. Nevertheless, people perceptions on a given video quality may not
be the same, which makes the QoE optimization harder. This paper aims at taking
a step further in order to address this limitation and meet users profiles. To
do so, we propose a closed-loop control framework based on the
users(subjective) feedbacks to learn the QoE function and optimize it at the
same time. Our simulation results show that our system converges to a steady
state, where the resulting QoE function noticeably improves the users
feedbacks."
"Motion compensation is a fundamental technology in video coding to remove the
temporal redundancy between video frames. To further improve the coding
efficiency, sub-pel motion compensation has been utilized, which requires
interpolation of fractional samples. The video coding standards usually adopt
fixed interpolation filters that are derived from the signal processing theory.
However, as video signal is not stationary, the fixed interpolation filters may
turn out less efficient. Inspired by the great success of convolutional neural
network (CNN) in computer vision, we propose to design a CNN-based
interpolation filter (CNNIF) for video coding. Different from previous studies,
one difficulty for training CNNIF is the lack of ground-truth since the
fractional samples are actually not available. Our solution for this problem is
to derive the ""ground-truth"" of fractional samples by smoothing high-resolution
images, which is verified to be effective by the conducted experiments.
Compared to the fixed half-pel interpolation filter for luma in High Efficiency
Video Coding (HEVC), our proposed CNNIF achieves up to 3.2% and on average 0.9%
BD-rate reduction under low-delay P configuration."
"The emergence of smart Wi-Fi APs (Access Point), which are equipped with huge
storage space, opens a new research area on how to utilize these resources at
the edge network to improve users' quality of experience (QoE) (e.g., a short
startup delay and smooth playback). One important research interest in this
area is content prefetching, which predicts and accurately fetches contents
ahead of users' requests to shift the traffic away during peak periods.
However, in practice, the different video watching patterns among users, and
the varying network connection status lead to the time-varying server load,
which eventually makes the content prefetching problem challenging. To
understand this challenge, this paper first performs a large-scale measurement
study on users' AP connection and TV series watching patterns using
real-traces. Then, based on the obtained insights, we formulate the content
prefetching problem as a Markov Decision Process (MDP). The objective is to
strike a balance between the increased prefetching&storage cost incurred by
incorrect prediction and the reduced content download delay because of
successful prediction. A learning-based approach is proposed to solve this
problem and another three algorithms are adopted as baselines. In particular,
first, we investigate the performance lower bound by using a random algorithm,
and the upper bound by using an ideal offline approach. Then, we present a
heuristic algorithm as another baseline. Finally, we design a reinforcement
learning algorithm that is more practical to work in the online manner. Through
extensive trace-based experiments, we demonstrate the performance gain of our
design. Remarkably, our learning-based algorithm achieves a better precision
and hit ratio (e.g., 80%) with about 70% (resp. 50%) cost saving compared to
the random (resp. heuristic) algorithm."
"The details of an image with noise may be restored by removing noise through
a suitable image de-noising method. In this research, a new method of image
de-noising based on using median filter (MF) in the wavelet domain is proposed
and tested. Various types of wavelet transform filters are used in conjunction
with median filter in experimenting with the proposed approach in order to
obtain better results for image de-noising process, and, consequently to select
the best suited filter. Wavelet transform working on the frequencies of
sub-bands split from an image is a powerful method for analysis of images.
According to this experimental work, the proposed method presents better
results than using only wavelet transform or median filter alone. The MSE and
PSNR values are used for measuring the improvement in de-noised images."
"This letter is about a principal weakness of the published article by Li et
al. in 2014. It seems that the mentioned work has a terrible conceptual mistake
while presenting its theoretical approach. In fact, the work has tried to
design a new attack and its effective solution for a basic watermarking
algorithm by Zhu et al. published in 2013, however in practice, we show the Li
et al.'s approach is not correct to obtain the aim. For disproof of the
incorrect approach, we only apply a numerical example as the counterexample of
the Li et al.'s approach."
"Copy-move forgery is one of the simple and effective operations to create
forged images. Recently, techniques based on singular value decomposition (SVD)
are widely used to detect copy-move forgery (CMF). Some approaches based on SVD
are most acceptable to detect copy-move forgery but some copy-move forgery
detection approaches can not produce satisfactory detection results. Sometimes
these approaches may even produce error results. According to our observation,
detection result produced using SVD depend highly on those parameters whose
values are often determined with experiences. These values are only applicable
to a few images, which limit their application. To solve this problem, a novel
approach named as copy-move forgery detection using Cuckoo search algorithm
(CMFD-CS) is proposed in this paper. CMFD-CS integrates the CS algorithm into
SVD. It utilizes the CS algorithm to generate customized parameter values for
images, which are used CMFD under block-based framework."
"Cross-modal retrieval has become a highlighted research topic for retrieval
across multimedia data such as image and text. A two-stage learning framework
is widely adopted by most existing methods based on Deep Neural Network (DNN):
The first learning stage is to generate separate representation for each
modality, and the second learning stage is to get the cross-modal common
representation. However, the existing methods have three limitations: (1) In
the first learning stage, they only model intra-modality correlation, but
ignore inter-modality correlation with rich complementary context. (2) In the
second learning stage, they only adopt shallow networks with single-loss
regularization, but ignore the intrinsic relevance of intra-modality and
inter-modality correlation. (3) Only original instances are considered while
the complementary fine-grained clues provided by their patches are ignored. For
addressing the above problems, this paper proposes a cross-modal correlation
learning (CCL) approach with multi-grained fusion by hierarchical network, and
the contributions are as follows: (1) In the first learning stage, CCL exploits
multi-level association with joint optimization to preserve the complementary
context from intra-modality and inter-modality correlation simultaneously. (2)
In the second learning stage, a multi-task learning strategy is designed to
adaptively balance the intra-modality semantic category constraints and
inter-modality pairwise similarity constraints. (3) CCL adopts multi-grained
modeling, which fuses the coarse-grained instances and fine-grained patches to
make cross-modal correlation more precise. Comparing with 13 state-of-the-art
methods on 6 widely-used cross-modal datasets, the experimental results show
our CCL approach achieves the best performance."
"Multimedia retrieval plays an indispensable role in big data utilization.
Past efforts mainly focused on single-media retrieval. However, the
requirements of users are highly flexible, such as retrieving the relevant
audio clips with one query of image. So challenges stemming from the ""media
gap"", which means that representations of different media types are
inconsistent, have attracted increasing attention. Cross-media retrieval is
designed for the scenarios where the queries and retrieval results are of
different media types. As a relatively new research topic, its concepts,
methodologies and benchmarks are still not clear in the literatures. To address
these issues, we review more than 100 references, give an overview including
the concepts, methodologies, major challenges and open issues, as well as build
up the benchmarks including datasets and experimental results. Researchers can
directly adopt the benchmarks to promptly evaluate their proposed methods. This
will help them to focus on algorithm design, rather than the time-consuming
compared methods and results. It is noted that we have constructed a new
dataset XMedia, which is the first publicly available dataset with up to five
media types (text, image, video, audio and 3D model). We believe this overview
will attract more researchers to focus on cross-media retrieval and be helpful
to them."
"A synchronization code scheme based on moving average is proposed for robust
audio watermarking in the paper. Two proper positive integers are chosen to
compute the moving average sequence by sliding one sample every time. The
synchronization bits are embedded at crosses of the two moving average
sequences with the quantization index modulation. The experimental results show
that the proposed watermarking scheme maintains high audio quality and is
robust to common attacks such as additive white Gaussian noise, re-sampling,
low-pass filtering, random cropping, MP3 compression, jitter attack and time
scale modification. Simultaneously, the algorithm has high search efficiency
and low false alarm rate."
"Noise is often brought to host audio by common signal processing operation,
and it usually changes the high-frequency component of an audio signal. So
embedding watermark by adjusting low-frequency coefficient can improve the
robustness of a watermark scheme. Moving Average sequence is a low-frequency
feature of an audio signal. This work proposed a method which embedding
watermark into the maximal coefficient in discrete cosine transform domain of a
moving average sequence. Subjective and objective tests reveal that the
proposed watermarking scheme maintains highly audio quality, and
simultaneously, the algorithm is highly robust to common digital signal
processing operations, including additive noise, sampling rate change, bit
resolution transformation, MP3 compression, and random cropping, especially
low-pass filtering."
"This paper introduces a blind watermarking based on a convolutional neural
network (CNN). We propose an iterative learning framework to secure robustness
of watermarking. One loop of learning process consists of the following three
stages: Watermark embedding, attack simulation, and weight update. We have
learned a network that can detect a 1-bit message from a image sub-block.
Experimental results show that this learned network is an extension of the
frequency domain that is widely used in existing watermarking scheme. The
proposed scheme achieved robustness against geometric and signal processing
attacks with a learning time of one day."
"The panoramic video is widely used to build virtual reality (VR) and is
expected to be one of the next generation Killer-Apps. Transmitting panoramic
VR videos is a challenging task because of two problems: 1) panoramic VR videos
are typically much larger than normal videos but they need to be transmitted
with limited bandwidth in mobile networks. 2) high-resolution and fluent views
should be provided to guarantee a superior user experience and avoid
side-effects such as dizziness and nausea. To address these two problems, we
propose a novel interactive streaming technology, namely Focus-based
Interactive Streaming Framework (FISF). FISF consists of three parts: 1) we use
the classic clustering algorithm DBSCAN to analyze real user data for Video
Focus Detection (VFD); 2) we propose a Focus-based Interactive Streaming
Technology (FIST), including a static version and a dynamic version; 3) we
propose two optimization methods: focus merging and prefetch strategy.
Experimental results show that FISF significantly outperforms the
state-of-the-art. The paper is submitted to Sigcomm 2017, VR/AR Network on 31
Mar 2017 at 10:44:04am EDT."
"This paper presents an empirical study on applying convolutional neural
networks (CNNs) to detecting J-UNIWARD, one of the most secure JPEG
steganographic method. Experiments guiding the architectural design of the CNNs
have been conducted on the JPEG compressed BOSSBase containing 10,000 covers of
size 512x512. Results have verified that both the pooling method and the depth
of the CNNs are critical for performance. Results have also proved that a
20-layer CNN, in general, outperforms the most sophisticated feature-based
methods, but its advantage gradually diminishes on hard-to-detect cases. To
show that the performance generalizes to large-scale databases and to different
cover sizes, one experiment has been conducted on the CLS-LOC dataset of
ImageNet containing more than one million covers cropped to unified size of
256x256. The proposed 20-layer CNN has cut the error achieved by a CNN recently
proposed for large-scale JPEG steganalysis by 35%. Source code is available via
GitHub: https://github.com/GuanshuoXu/deep_cnn_jpeg_steganalysis"
"The polyhedron projection for 360-degree video is becoming more and more
popular since it can lead to much less geometry distortion compared with the
equirectangular projection. However, in the polyhedron projection, we can
observe very obvious texture discontinuity in the area near the face boundary.
Such a texture discontinuity may lead to serious quality degradation when
motion compensation crosses the discontinuous face boundary. To solve this
problem, in this paper, we first propose to fill the corresponding neighboring
faces in the suitable positions as the extension of the current face to keep
approximated texture continuity. Then a co-projection-plane based 3-D padding
method is proposed to project the reference pixels in the neighboring face to
the current face to guarantee exact texture continuity. Under the proposed
scheme, the reference pixel is always projected to the same plane with the
current pixel when performing motion compensation so that the texture
discontinuity problem can be solved. The proposed scheme is implemented in the
reference software of High Efficiency Video Coding. Compared with the existing
method, the proposed algorithm can significantly improve the rate-distortion
performance. The experimental results obviously demonstrate that the texture
discontinuity in the face boundary can be well handled by the proposed
algorithm."
"This paper investigates the multiplicative spread spectrum watermarking
method for the image. The information bit is spreaded into middle-frequency
Discrete Cosine Transform (DCT) coefficients of each block of an image using a
generated pseudo-random sequence. Unlike the conventional signal modeling, we
suppose that both signal and noise are distributed with Laplacian distribution
because the sample loss of digital media can be better modeled with this
distribution than the Gaussian one. We derive the optimum decoder for the
proposed embedding method thanks to the maximum likelihood decoding scheme. We
also analyze our watermarking system in the presence of noise and provide
analytical evaluations and several simulations. The results show that it has
the suitable performance and transparency required for watermarking
applications."
"Augmented Reality (AR) devices are commonly head-worn to overlay
context-dependent information into the field of view of the device operators.
One particular scenario is the overlay of still images, either in a traditional
fashion, or as spherical, i.e., immersive, content. For both media types, we
evaluate the interplay of user ratings as Quality of Experience (QoE) with (i)
the non-referential BRISQUE objective image quality metric and (ii) human
subject dry electrode EEG signals gathered with a commercial device.
Additionally, we employ basic machine learning approaches to assess the
possibility of QoE predictions based on rudimentary subject data. Corroborating
prior research for the overall scenario, we find strong correlations for both
approaches with user ratings as Mean Opinion Scores, which we consider as QoE
metric. In prediction scenarios based on data subsets, we find good performance
for the objective metric as well as the EEG-based approach. While the objective
metric can yield high QoE prediction accuracies overall, it is limited i its
application for individual subjects. The subject-based EEG approach, on the
other hand, enables good predictability of the QoE for both media types, but
with better performance for regular content. Our results can be employed in
practical scenarios by content and network service providers to optimize the
user experience in augmented reality scenarios."
"HTTP Adaptive Streaming (HAS) techniques are now the dominant solution for
video delivery in mobile networks. Over the past few years, several HAS
algorithms have been introduced in order to improve user quality-of-experience
(QoE) by bit-rate adaptation. Their difference is mainly the required input
information, ranging from network characteristics to application-layer
parameters such as the playback buffer. Interestingly, despite the recent
outburst in scientific papers on the topic, a comprehensive comparative study
of the main algorithm classes is still missing. In this paper we provide such
comparison by evaluating the performance of the state-of-the-art HAS algorithms
per class, based on data from field measurements. We provide a systematic study
of the main QoE factors and the impact of the target buffer level. We conclude
that this target buffer level is a critical classifier for the studied HAS
algorithms. While buffer-based algorithms show superior QoE in most of the
cases, their performance may differ at the low target buffer levels of live
streaming services. Overall, we believe that our findings provide valuable
insight for the design and choice of HAS algorithms according to networks
conditions and service requirements."
"Multimedia Forensics allows to determine whether videos or images have been
captured with the same device, and thus, eventually, by the same person.
Currently, the most promising technology to achieve this task, exploits the
unique traces left by the camera sensor into the visual content. Anyway, image
and video source identification are still treated separately from one another.
This approach is limited and anachronistic if we consider that most of the
visual media are today acquired using smartphones, that capture both images and
videos. In this paper we overcome this limitation by exploring a new approach
that allows to synergistically exploit images and videos to study the device
from which they both come. Indeed, we prove it is possible to identify the
source of a digital video by exploiting a reference sensor pattern noise
generated from still images taken by the same device of the query video. The
proposed method provides comparable or even better performance, when compared
to the current video identification strategies, where a reference pattern is
estimated from video frames. We also show how this strategy can be effective
even in case of in-camera digitally stabilized videos, where a non-stabilized
reference is not available, by solving some state-of-the-art limitations. We
explore a possible direct application of this result, that is social media
profile linking, i.e. discovering relationships between two or more social
media profiles by comparing the visual contents - images or videos - shared
therein."
"Differently from traditional media streaming where a unique media content is
delivered to different users, interactive multiview navigation applications
enable users to choose their own viewpoints and freely navigate in a 3-D scene.
The interactivity brings new challenges in addition to the classical
rate-distortion trade-off, which considers only the compression performance and
viewing quality. On the one hand, it has to ensure sufficient viewpoints for
richer navigation; on the other hand, it requires to provide low bandwidth and
delay costs for smooth navigation during view transitions. In this paper, we
formally describe the novel trade-off posed by the navigation interactivity and
classical rate-distortion criterion, in the context of the proper data
representation method. Based on this original formulation, we look for the
optimal design of the data representation by introducing novel rate and
distortion models and practical solving algorithms. Experiments show that the
proposed data representation method outperforms the baseline solution by
providing lower resource consumptions and higher visual quality in all
navigation configurations, which certainly confirms the potential of the
proposed data representation in practical interactive navigation systems."
"The recent rise of interest in Virtual Reality (VR) came with the
availability of commodity commercial VR products, such as the Head Mounted
Displays (HMD) created by Oculus and other vendors. One of the main
applications of virtual reality that has been recently adopted is streaming
sports events. For instance, the last olympics held in Rio De Janeiro was
streamed over the Internet for users to view on VR headsets or using 360 video
players. A big challenge for streaming VR sports events is the users limited
bandwidth and the amount of data required to transmit 360 videos. While 360
video demands high bandwidth, at any time instant users are only viewing a
small portion of the video according to the HMD field of view (FOV). Many
approaches have been proposed in the literature such as proposing new
representations (e.g. pyramid and offset-cubemap) and tiling the video and
streaming the tiles currently being viewed. In this paper, we propose a tiled
streaming framework, where we provide a degrading quality model similar to the
state-of-the-art offset-cubemap while minimizing its storage requirements at
the server side. We conduct objective studies showing the effectiveness of our
approach providing smooth degradation of quality from the user FOV to the back
of the 360 space. In addition, we conduct subjective studies showing that users
tend to prefer our proposed scheme over offset-cubemap in low bandwidth
connections, and they don't feel difference for higher bandwidth connections.
That is, we achieve better perceived quality with huge storage savings up to
670%."
"This paper introduces the implementation of steganography method called
StegIbiza, which uses tempo modulation as hidden message carrier. With the use
of Python scripting language, a bit string was encoded and decoded using WAV
and MP3 files. Once the message was hidden into a music files, an internet
radio was created to evaluate broadcast possibilities. No dedicated music or
signal processing equipment was used in this StegIbiza implementation"
"This paper describes a novel system that provides key parameters of HTTP
Adaptive Streaming (HAS) sessions to the lower layers of the protocol stack. A
non-intrusive traffic profiling solution is proposed that observes packet flows
at the transmit queue of base stations, edge-routers, or gateways. By analyzing
IP flows in real time, the presented scheme identifies different phases of an
HAS session and estimates important application-layer parameters, such as
play-back buffer state and video encoding rate. The introduced estimators only
use IP-layer information, do not require standardization and work even with
traffic that is encrypted via Transport Layer Security (TLS). Experimental
results for a popular video streaming service clearly verify the high accuracy
of the proposed solution. Traffic profiling, thus, provides a valuable
alternative to cross-layer signaling and Deep Packet Inspection (DPI) in order
to perform efficient network optimization for video streaming."
"The compact descriptors for visual search (CDVS) standard from ISO/IEC moving
pictures experts group (MPEG) has succeeded in enabling the interoperability
for efficient and effective image retrieval by standardizing the bitstream
syntax of compact feature descriptors. However, the intensive computation of
CDVS encoder unfortunately hinders its widely deployment in industry for
large-scale visual search. In this paper, we revisit the merits of low
complexity design of CDVS core techniques and present a very fast CDVS encoder
by leveraging the massive parallel execution resources of GPU. We elegantly
shift the computation-intensive and parallel-friendly modules to the
state-of-the-arts GPU platforms, in which the thread block allocation and the
memory access are jointly optimized to eliminate performance loss. In addition,
those operations with heavy data dependence are allocated to CPU to resolve the
extra but non-necessary computation burden for GPU. Furthermore, we have
demonstrated the proposed fast CDVS encoder can work well with those
convolution neural network approaches which has harmoniously leveraged the
advantages of GPU platforms, and yielded significant performance improvements.
Comprehensive experimental results over benchmarks are evaluated, which has
shown that the fast CDVS encoder using GPU-CPU hybrid computing is promising
for scalable visual search."
"Good user experience with interactive cloud-based multimedia applications,
such as cloud gaming and cloud-based VR, requires low end-to-end latency and
large amounts of downstream network bandwidth at the same time. In this paper,
we present a foveated video streaming system for cloud gaming. The system
adapts video stream quality by adjusting the encoding parameters on the fly to
match the player's gaze position. We conduct measurements with a prototype that
we developed for a cloud gaming system in conjunction with eye tracker
hardware. Evaluation results suggest that such foveated streaming can reduce
bandwidth requirements by even more than 50% depending on parametrization of
the foveated video coding and that it is feasible from the latency perspective."
"The GFR (Gabor Filter Residual) features, built as histograms of quantized
residuals obtained with 2D Gabor filters, can achieve competitive detection
performance against adaptive JPEG steganography. In this paper, an improved
version of the GFR is proposed. First, a novel histogram merging method is
proposed according to the symmetries between different Gabor filters, thus
making the features more compact and robust. Second, a new weighted histogram
method is proposed by considering the position of the residual value in a
quantization interval, making the features more sensitive to the slight changes
in residual values. The experiments are given to demonstrate the effectiveness
of our proposed methods. Finally, we design a CNN to duplicate the detector
with the improved GFR features and the ensemble classifier, thus optimizing the
design of the filters used to form residuals in JPEG-phase-aware features."
"In this paper, a novel steganographic scheme based on chaotic iterations is
proposed. This research work takes place into the information hiding framework,
and focus more specifically on robust steganography. Steganographic algorithms
can participate in the development of a semantic web: medias being on the
Internet can be enriched by information related to their contents, authors,
etc., leading to better results for the search engines that can deal with such
tags. As media can be modified by users for various reasons, it is preferable
that these embedding tags can resist to changes resulting from some classical
transformations as for example cropping, rotation, image conversion, and so on.
This is why a new robust watermarking scheme for semantic search engines is
proposed in this document. For the sake of completeness, the robustness of this
scheme is finally compared to existing established algorithms."
"Many different parametric models for video quality assessment have been
proposed in the past few years. This paper presents a review of nine recent
models which cover a wide range of methodologies and have been validated for
estimating video quality due to different degradation factors. Each model is
briefly described with key algorithms and relevant parametric formulas. The
generalization capability of each model to estimate video quality in
real-application scenarios is evaluated and compared with other models, using a
dataset created with video sequences from practical applications. These video
sequences cover a wide range of possible realistic encoding parameters, labeled
with mean opinion scores (MOS) via subjective test. The weakness and strength
of each model are remarked. Finally, future work towards a more general
parametric model that could apply for a wider range of applications is
discussed."
"Web video is often used as a source of data in various fields of study. While
specialized subsets of web video, mainly earmarked for dedicated purposes, are
often analyzed in detail, there is little information available about the
properties of web video as a whole. In this paper we present insights gained
from the analysis of the metadata associated with more than 120 million videos
harvested from two popular web video platforms, vimeo and YouTube, in 2016 and
compare their properties with the ones found in commonly used video
collections. This comparison has revealed that existing collections do not (or
no longer) properly reflect the properties of web video ""in the wild""."
"Digital images can be copied without authorization and have to be protected.
Two schemes for watermarking images in PDF document were considered. Both
schemes include a converter to extract images from PDF pages and return the
protected images back. Frequency and spatial domain embedding were used for
hiding a message presented by a binary pattern. We considered visible and
invisible watermarking and found that spatial domain LSB technique can be more
preferable than frequency embedding using DWT."
"A novel diverse domain (DCT-SVD & DWT-SVD) watermarking scheme is proposed in
this paper. Here, the watermark is embedded simultaneously onto the two
domains. It is shown that an audio signal watermarked using this scheme has
better subjective and objective quality when compared with other watermarking
schemes. Also proposed are two novel watermark detection algorithms viz., AOT
(Adaptively Optimised Threshold) and AOTx (AOT eXtended). The fundamental idea
behind both is finding an optimum threshold for detecting a known character
embedded along with the actual watermarks in a known location, with the
constraint that the Bit Error Rate (BER) is minimum. This optimum threshold is
used for detecting the other characters in the watermarks. This approach is
shown to make the watermarking scheme less susceptible to various signal
processing attacks, thus making the watermarks more robust."
"HTTP-based video streaming technologies allow for flexible rate selection
strategies that account for time-varying network conditions. Such rate changes
may adversely affect the user's Quality of Experience; hence online prediction
of the time varying subjective quality can lead to perceptually optimised
bitrate allocation policies. Recent studies have proposed to use dynamic
network approaches for continuous-time prediction; yet they do not consider
multiple video quality models as inputs nor consider forecasting ensembles.
Here we address the problem of predicting continuous-time subjective quality
using multiple inputs fed to a non-linear autoregressive network. By
considering multiple network configurations and by applying simple averaging
forecasting techniques, we are able to considerably improve prediction
performance and decrease forecasting errors."
"Public speaking is an important aspect of human communication and
interaction. The majority of computational work on public speaking concentrates
on analyzing the spoken content, and the verbal behavior of the speakers. While
the success of public speaking largely depends on the content of the talk, and
the verbal behavior, non-verbal (visual) cues, such as gestures and physical
appearance also play a significant role. This paper investigates the importance
of visual cues by estimating their contribution towards predicting the
popularity of a public lecture. For this purpose, we constructed a large
database of more than $1800$ TED talk videos. As a measure of popularity of the
TED talks, we leverage the corresponding (online) viewers' ratings from
YouTube. Visual cues related to facial and physical appearance, facial
expressions, and pose variations are extracted from the video frames using
convolutional neural network (CNN) models. Thereafter, an attention-based long
short-term memory (LSTM) network is proposed to predict the video popularity
from the sequence of visual features. The proposed network achieves
state-of-the-art prediction accuracy indicating that visual cues alone contain
highly predictive information about the popularity of a talk. Furthermore, our
network learns a human-like attention mechanism, which is particularly useful
for interpretability, i.e. how attention varies with time, and across different
visual cues by indicating their relative importance."
"The fingerprint-copy attack aims to confuse camera identification based on
sensor pattern noise. However, the triangle test shows that the forged images
undergone fingerprint-copy attack would share a non-PRNU (Photo-response
nonuniformity) component with every stolen image, and thus can detect
fingerprint-copy attack. In this paper, we propose an improved fingerprint-copy
attack scheme. Our main idea is to superimpose the estimated fingerprint into
the target image dispersedly, via employing a block-wise method and using the
stolen images randomly and partly. We also develop a practical method to
determine the strength of the superimposed fingerprint based on objective image
quality. In such a way, the impact of non-PRNU component on the triangle test
is reduced, and our improved fingerprint-copy attack is difficultly detected.
The experiments evaluated on 2,900 images from 4 cameras show that our scheme
can effectively fool camera identification, and significantly degrade the
performance of the triangle test simultaneously."
"A few years after standardization of the High Efficiency Video Coding (HEVC),
now the Joint Video Exploration Team (JVET) group is exploring post-HEVC video
compression technologies. In the intra prediction domain, this effort has
resulted in an algorithm with 67 internal modes, new filters and tools which
significantly improve HEVC. However, the improved algorithm still suffers from
the long distance prediction inaccuracy problem. In this paper, we propose an
In-Loop Residual coding Intra Prediction (ILR-IP) algorithm which utilizes
inner-block reconstructed pixels as references to reduce the distance from
predicted pixels. This is done by using the ILR signal for partially
reconstructing each pixel, right after its prediction and before its
block-level out-loop residual calculation. The ILR signal is decided in the
rate-distortion sense, by a brute-force search on a QP-dependent finite
codebook that is known to the decoder. Experiments show that the proposed
ILR-IP algorithm improves the existing method in the Joint Exploration Model
(JEM) up to 0.45% in terms of bit rate saving, without complexity overhead at
the decoder side."
"Modern distributed systems include a class of applications in which
non-functional requirements are important. In particular, these applications
include multimedia facilities where real time constraints are crucial to their
correct functioning. In order to specify such systems it is necessary to
describe that events occur at times given by probability distributions and
stochastic automata have emerged as a useful technique by which such systems
can be specified and verified.
  However, stochastic descriptions are very general, in particular they allow
the use of general probability distribution functions, and therefore their
verification can be complex. In the last few years, model checking has emerged
as a useful verification tool for large systems.
  In this paper we describe two model checking algorithms for stochastic
automata. These algorithms consider how properties written in a simple
probabilistic real-time logic can be checked against a given stochastic
automaton."
"The ""event"" concept appears repeatedly when developing metadata models for
the description and management of multimedia content. During the typical life
cycle of multimedia content, events occur at many different levels - from the
events which happen during content creation (directing, acting, camera panning
and zooming) to the events which happen to the physical form (acquisition,
relocation, damage of film or video) to the digital conversion, reformatting,
editing and repackaging events, to the events which are depicted in the actual
content (political, news, sporting) to the usage, ownership and copyright
agreement events and even the metadata attribution events. Support is required
within both MPEG-7 and MPEG-21 for the clear and unambiguous description of all
of these event types which may occur at widely different levels of nesting and
granularity. In this paper we first describe an event-aware model (the ABC
model) which is capable of modeling and yet clearly differentiating between all
of these, often recursive and overlapping events. We then illustrate how this
model can be used as the foundation to facilitate semantic interoperability
between MPEG-7 and MPEG-21. By expressing the semantics of both MPEG-7 and
MPEG-21 metadata terms in RDF Schema (and some DAML+OIL extensions) and
attaching the MPEG-7 and MPEG-21 class and property hierarchies to the
appropriate top-level classes and properties of the ABC model, we are
essentially able to define a single distributed machine-understandable
ontology, which will enable interoperability of data and services across the
entire multimedia content delivery chain."
"The Virtual Rooms Videoconferencing Service (VRVS) provides a worldwide
videoconferencing service and collaborative environment to the research and
education communities. This system provides a low cost, bandwidth-efficient,
extensible means for videoconferencing and remote collaboration over networks
within the High Energy and Nuclear Physics communities (HENP). VRVS has become
a standard part of the toolset used daily by a large sector of HENP, and it is
used increasingly for other DoE/NSF-supported programs. The current features
included multi-protocol, multi-OS support for all significant video enabled
clients including: H.323, Mbone, QuickTime, MPEG2, Java Media Framework, and
other clients. The current architecture makes VRVS a distributed, highly
functional, and efficient software-only system for multipoint audio, video and
web conferencing and collaboration over global IP networks. VRVS has developed
the VRVS-AG Reflector and a specialized Web interface that enables end users to
connect to any Access Grid (AG) session, in any of the AG ""virtual venues"" from
anywhere worldwide. The VRVS system has now been running for the last five and
half years, offering to the HENP community a working and reliable tool for
collaboration within groups and among physicists dispersed world-wide. The goal
of this ongoing effort is to develop the next generation collaborative systems
running over next generation networks. The new developments area integrate
emerging standards, include all security aspects, and will extend the range of
VRVS video technologies supported to cover the latest high end standards
quality. We will focus the discussion on the new capability provides by the
latest version V3.0 and its future evolution."
"This paper provides a general technical overview of the Multimedia Home
Platform (MHP) specifications. MHP is a generic interface between digital
applications and user machines, whether they happen to be set top boxes,
digital TV sets or Multimedia PC's. MHP extends the DVB open standards.
Addressed are MHP architexture, System core and MHP Profiles."
"In this paper, we describe the Rich Representation Language (RRL) which is
used in the NECA system. The NECA system generates interactions between two or
more animated characters. The RRL is an XML compliant framework for
representing the information that is exchanged at the interfaces between the
various NECA system modules. The full XML Schemas for the RRL are available at
http://www.ai.univie.ac.at/NECA/RRL"
"Past experiences under the designation of ""Swarm Paintings"" conducted in
2001, not only confirmed the possibility of realizing an artificial art (thus
non-human), as introduced into the process the questioning of creative
migration, specifically from the computer monitors to the canvas via a robotic
harm. In more recent self-organized based research we seek to develop and
profound the initial ideas by using a swarm of autonomous robots (ARTsBOT
project 2002-03), that ""live"" avoiding the purpose of being merely a simple
perpetrator of order streams coming from an external computer, but instead,
that actually co-evolve within the canvas space, acting (that is, laying ink)
according to simple inner threshold stimulus response functions, reacting
simultaneously to the chromatic stimulus present in the canvas environment done
by the passage of their team-mates, as well as by the distributed feedback,
affecting their future collective behaviour. In parallel, and in what respects
to certain types of collective systems, we seek to confirm, in a physically
embedded way, that the emergence of order (even as a concept) seems to be found
at a lower level of complexity, based on simple and basic interchange of
information, and on the local dynamic of parts, who, by self-organizing
mechanisms tend to form an lived whole, innovative and adapting, allowing for
emergent open-ended creative and distributed production. KEYWORDS: ArtSBots
Project, Swarm Intelligence, Stigmergy, UnManned Art, Symbiotic Art, Swarm
Paintings, Robot Paintings, Non-Human Art, Painting Emergence and Cooperation,
Art and Complexity, ArtBots: The Robot Talent Show."
"In this paper, some existing perceptual encryption algorithms of MPEG videos
are reviewed and some problems, especially security defects of two recently
proposed MPEG-video perceptual encryption schemes, are pointed out. Then, a
simpler and more effective design is suggested, which selectively encrypts
fixed-length codewords (FLC) in MPEG-video bitstreams under the control of
three perceptibility factors. The proposed design is actually an encryption
configuration that can work with any stream cipher or block cipher. Compared
with the previously-proposed schemes, the new design provides more useful
features, such as strict size-preservation, on-the-fly encryption and multiple
perceptibility, which make it possible to support more applications with
different requirements. In addition, four different measures are suggested to
provide better security against known/chosen-plaintext attacks."
"We investigate methods of segmenting, visualizing, and indexing presentation
videos by separately considering audio and visual data. The audio track is
segmented by speaker, and augmented with key phrases which are extracted using
an Automatic Speech Recognizer (ASR). The video track is segmented by visual
dissimilarities and augmented by representative key frames. An interactive user
interface combines a visual representation of audio, video, text, and key
frames, and allows the user to navigate a presentation video. We also explore
clustering and labeling of speaker data and present preliminary results."
"In this paper we report on a multimedia communication system including a
VCoIP (Video Conferencing over IP) software with a distributed architecture and
its applications for teaching scenarios. It is a simple, ready-to-use scheme
for distributed presenting, recording and streaming multimedia content. We also
introduce and investigate concepts and experiments to IPv6 user and session
mobility, with the special focus on real-time video group communication."
"This paper studies the security of a recently-proposed MPEG-video encryption
scheme based on secret Huffman tables. Our cryptanalysis shows that: 1) the key
space of the encryption scheme is not sufficiently large against
divide-and-conquer (DAC) attack and known-plaintext attack; 2) it is possible
to decrypt a cipher-video with a partially-known key, thus dramatically
reducing the complexity of the DAC brute-force attack in some cases; 3) its
security against the chosen-plaintext attack is very weak. Some experimental
results are included to support the cryptanalytic results with a brief discuss
on how to improve this MPEG-video encryption scheme."
"Many classical encoding algorithms of Vector Quantization (VQ) of image
compression that can obtain global optimal solution have computational
complexity O(N). A pure quantum VQ encoding algorithm with probability of
success near 100% has been proposed, that performs operations 45sqrt(N) times
approximately. In this paper, a hybrid quantum VQ encoding algorithm between
classical method and quantum algorithm is presented. The number of its
operations is less than sqrt(N) for most images, and it is more efficient than
the pure quantum algorithm.
  Key Words: Vector Quantization, Grover's Algorithm, Image Compression,
Quantum Algorithm"
"In the watermark detection scenario, also known as zero-bit watermarking, a
watermark, carrying no hidden message, is inserted in content. The watermark
detector checks for the presence of this particular weak signal in content. The
article looks at this problem from a classical detection theory point of view,
but with side information enabled at the embedding side. This means that the
watermark signal is a function of the host content. Our study is twofold. The
first step is to design the best embedding function for a given detection
function, and the best detection function for a given embedding function. This
yields two conditions, which are mixed into one `fundamental' partial
differential equation. It appears that many famous watermarking schemes are
indeed solution to this `fundamental' equation. This study thus gives birth to
a constructive framework unifying solutions, so far perceived as very
different."
"The security of Fridrich Image Encryption Algorithm against brute-force
attack, statistical attack, known-plaintext attack and select-plaintext attack
is analyzed by investigating the properties of the involved chaotic maps and
diffusion functions. Based on the given analyses, some means are proposed to
strengthen the overall performance of the focused cryptosystem."
"In our previous work, we introduced a double-sided technique that utilizes
but not reject the host interference. Due to its nice property of utilizing but
not rejecting the host interference, it has a big advantage over the host
interference schemes in that the perceptual analysis can be easily implemented
for our scheme to achieve the locally bounded maximum embedding strength. Thus,
in this work, we detail how to implement the perceptual analysis in our
double-sided schemes since the perceptual analysis is very important for
improving the fidelity of watermarked contents. Through the extensive
performance comparisons, we can further validate the performance advantage of
our double-sided schemes."
"The embedder and the detector (or decoder) are the two most important
components of the digital watermarking systems. Thus in this work, we discuss
how to design a better embedder and detector (or decoder). I first give a
summary of the prospective applications of watermarking technology and major
watermarking schemes in the literature. My review on the literature closely
centers upon how the side information is exploited at both embedders and
detectors. In Chapter 3, I explore the optimum detector or decoder according to
a particular probability distribution of the host signals. We found that the
performance of both multiplicative and additive spread spectrum schemes depends
on the shape parameter of the host signals. For spread spectrum schemes, the
performance of the detector or the decoder is reduced by the host interference.
Thus I present a new host-interference rejection technique for the
multiplicative spread spectrum schemes. Its embedding rule is tailored to the
optimum detection or decoding rule. Though the host interference rejection
schemes enjoy a big performance gain over the traditional spread spectrum
schemes, their drawbacks that it is difficult for them to be implemented with
the perceptual analysis to achieve the maximum allowable embedding level
discourage their use in real scenarios. Thus, in the last chapters of this
work, I introduce a double-sided technique to tackle this drawback. It differs
from the host interference rejection schemes in that it utilizes but does not
reject the host interference at its embedder. The perceptual analysis can be
easily implemented in our scheme to achieve the maximum allowable level of
embedding strength."
"Fixed infrastructured networks naturally support centralized approaches for
group management and information provisioning. Contrary to infrastructured
networks, in multi-hop ad-hoc networks each node acts as a router as well as
sender and receiver. Some applications, however, requires hierarchical
arrangements that-for practical reasons-has to be done locally and
self-organized. An additional challenge is to deal with mobility that causes
permanent network partitioning and re-organizations. Technically, these
problems can be tackled by providing additional uplinks to a backbone network,
which can be used to access resources in the Internet as well as to inter-link
multiple ad-hoc network partitions, creating a hybrid wireless network. In this
paper, we present a prototypically implemented hybrid wireless network system
optimized for multimedia content distribution. To efficiently manage the ad-hoc
communicating devices a weighted clustering algorithm is introduced. The
proposed localized algorithm deals with mobility, but does not require
geographical information or distances."
"Till now, few work has been done to analyze the performances of joint
fingerprint embedding and decryption schemes. In this paper, the security of
the joint fingerprint embedding and decryption scheme proposed by Kundur et al.
is analyzed and improved. The analyses include the security against
unauthorized customer, the security against authorized customer, the
relationship between security and robustness, the relationship between
secu-rity and imperceptibility and the perceptual security. Based these
analyses, some means are proposed to strengthen the system, such as multi-key
encryp-tion and DC coefficient encryption. The method can be used to analyze
other JFD schemes. It is expected to provide valuable information to design JFD
schemes."
"Neural network has been attracting more and more researchers since the past
decades. The properties, such as parameter sensitivity, random similarity,
learning ability, etc., make it suitable for information protection, such as
data encryption, data authentication, intrusion detection, etc. In this paper,
by investigating neural networks' properties, the low-cost authentication
method based on neural networks is proposed and used to authenticate images or
videos. The authentication method can detect whether the images or videos are
modified maliciously. Firstly, this chapter introduces neural networks'
properties, such as parameter sensitivity, random similarity, diffusion
property, confusion property, one-way property, etc. Secondly, the chapter
gives an introduction to neural network based protection methods. Thirdly, an
image or video authentication scheme based on neural networks is presented, and
its performances, including security, robustness and efficiency, are analyzed.
Finally, conclusions are drawn, and some open issues in this field are
presented."
"This paper presents a new method for a quick similarity-based search through
long unlabeled audio streams to detect and locate audio clips provided by
users. The method involves feature-dimension reduction based on a piecewise
linear representation of a sequential feature trajectory extracted from a long
audio stream. Two techniques enable us to obtain a piecewise linear
representation: the dynamic segmentation of feature trajectories and the
segment-based Karhunen-L\'{o}eve (KL) transform. The proposed search method
guarantees the same search results as the search method without the proposed
feature-dimension reduction method in principle. Experiment results indicate
significant improvements in search speed. For example the proposed method
reduced the total search time to approximately 1/12 that of previous methods
and detected queries in approximately 0.3 seconds from a 200-hour audio
database."
"Recently a lot of multimedia applications are emerging on portable
appliances. They require both the flexibility of upgradeable devices
(traditionally software based) and a powerful computing engine (typically
hardware). In this context, programmable HW and dynamic reconfiguration allow
novel approaches to the migration of algorithms from SW to HW. Thus, in the
frame of the Symbad project, we propose an industrial design flow for
reconfigurable SoC's. The goal of Symbad consists of developing a system level
design platform for hardware and software SoC systems including formal and
semi-formal verification techniques."
"In recent work, various fractal image coding methods are reported, which
adopt the self-similarity of images to compress the size of images. However,
till now, no solutions for the security of fractal encoded images have been
provided. In this paper, a secure fractal image coding scheme is proposed and
evaluated, which encrypts some of the fractal parameters during fractal
encoding, and thus, produces the encrypted and encoded image. The encrypted
image can only be recovered by the correct key. To keep secure and efficient,
only the suitable parameters are selected and encrypted through in-vestigating
the properties of various fractal parameters, including parameter space,
parameter distribu-tion and parameter sensitivity. The encryption process does
not change the file format, keeps secure in perception, and costs little time
or computational resources. These properties make it suitable for secure image
encoding or transmission."
"The delay-based fingerprint embedding was recently proposed to support more
users in secure media distribution scenario. In this embedding scheme, some
users are assigned the same fingerprint code with only different embedding
delay. The algorithm's robustness against collusion attacks is investigated.
However, its robustness against common desynchronization attacks, e.g.,
cropping and time shifting, is not considered. In this paper, desynchronization
attacks are used to break the delay-based fingerprint embedding algorithm. To
improve the robustness, two means are proposed to keep the embedded fingerprint
codes synchronized, i.e., adding a synchronization fingerprint and adopting the
relative delay to detect users. Analyses and experiments are given to show the
improvements."
"We show an analysis of multi-dimensional time series via entropy and
statistical linguistic techniques. We define three markers encoding the
behavior of the series, after it has been translated into a multi-dimensional
symbolic sequence. The leading component and the trend of the series with
respect to a mobile window analysis result from the entropy analysis and label
the dynamical evolution of the series. The diversification formalizes the
differentiation in the use of recurrent patterns, from a Zipf law point of
view. These markers are the starting point of further analysis such as
classification or clustering of large database of multi-dimensional time
series, prediction of future behavior and attribution of new data. We also
present an application to economic data. We deal with measurements of money
investments of some business companies in advertising market for different
media sources."
"Documents early computer art in the Soviet bloc and describes Marxist art
theory."
"In this paper an extension of the sparse decomposition problem is considered
and an algorithm for solving it is presented. In this extension, it is known
that one of the shifted versions of a signal s (not necessarily the original
signal itself) has a sparse representation on an overcomplete dictionary, and
we are looking for the sparsest representation among the representations of all
the shifted versions of s. Then, the proposed algorithm finds simultaneously
the amount of the required shift, and the sparse representation. Experimental
results emphasize on the performance of our algorithm."
"Embedded real-time applications in communication systems require high
processing power. Manual scheduling devel-oped for single-processor
applications is not suited to multi-core architectures. The Algorithm
Architecture Matching (AAM) methodology optimizes static application
implementation on multi-core architectures. The Random Access Channel Preamble
Detection (RACH-PD) is an algorithm for non-synchronized access of Long Term
Evolu-tion (LTE) wireless networks. LTE aims to improve the spectral efficiency
of the next generation cellular system. This paper de-scribes a complete
methodology for implementing the RACH-PD. AAM prototyping is applied to the
RACH-PD which is modelled as a Synchronous DataFlow graph (SDF). An efficient
implemen-tation of the algorithm onto a multi-core DSP, the TI C6487, is then
explained. Benchmarks for the solution are given."
"Energy efficient watermarking preserves the watermark energy after linear
attack as much as possible. We consider in this letter non-stationary signal
models and derive conditions for energy efficient watermarking under random
vector model without WSS assumption. We find that the covariance matrix of the
energy efficient watermark should be proportional to host covariance matrix to
best resist the optimal linear removal attacks. In WSS process our result
reduces to the well known power spectrum condition. Intuitive geometric
interpretation of the results are also discussed which in turn also provide
more simpler proof of the main results."
"WiCoM enables remote management of web resources. Our application Mobile
reporter is aimed at Journalist, who will be able to capture the events in
real-time using their mobile phones and update their web server on the latest
event. WiCoM has been developed using J2ME technology on the client side and
PHP on the server side. The communication between the client and the server is
established through GPRS. Mobile reporter will be able to upload, edit and
remove both textual as well as multimedia contents in the server."
"The recent advent in the field of multimedia proposed a many facilities in
transport, transmission and manipulation of data. Along with this advancement
of facilities there are larger threats in authentication of data, its licensed
use and protection against illegal use of data. A lot of digital image
watermarking techniques have been designed and implemented to stop the illegal
use of the digital multimedia images. This paper compares the robustness of
three different watermarking schemes against brightness and rotation attacks.
The robustness of the watermarked images has been verified on the parameters of
PSNR (Peak Signal to Noise Ratio), RMSE (Root Mean Square Error) and MAE (Mean
Absolute Error)."
"In this paper we have investigated on the reliability of streams for a VoD
system. The objective of the paper is to maximize the availability of streams
for the peers in the VoD system. We have achieved this by using data
replication technique in the peers. Hence, we proposed a new data replication
technique to optimally store the videos in the peers. The new data replication
technique generates more number of replicas than the existing techniques such
as random, minimum request and maximize hit. We have also investigated by
applying the CTMC model for the reliability of replications during the peer
failures. Our result shows that the mean lifetime of replicas are more under
various circumstances. We have addressed the practical issues of efficient
utilization of overall bandwidth and buffer in the VoD system. We achieved
greater success playback probability of videos than the existing techniques."
"Out of the scope of the usual positions of computing in the field of music
and musicology, one notices the emergence of human-computer systems that do
exist by breaking off. Though these singular systems take effect in the usual
fields of expansion of music, they do not make any systematic reference to
known musicological categories. On the contrary, they make possible experiments
that open uses where listening, composition and musical transmission get merged
in a gesture sometimes named as ?music-ripping?. We will show in which way the
music-ripping practices provoke traditional musicology, whose canonical
categories happen to be ineffectual to explain here. To achieve that purpose,
we shall need: - to make explicit a minimal set of categories that is
sufficient to underlie the usual models of computer assisted music;- to do the
same for human-computer systems (anti-musicological?) that disturb us; - to
examine the possibility conditions of reduction of the second set to the first;
- to conclude on the nature of music-ripping."
"Motivated by the work of Uehara et al. [1], an improved method to recover DC
coefficients from AC coefficients of DCT-transformed images is investigated in
this work, which finds applications in cryptanalysis of selective multimedia
encryption. The proposed under/over-flow rate minimization (FRM) method employs
an optimization process to get a statistically more accurate estimation of
unknown DC coefficients, thus achieving a better recovery performance. It was
shown by experimental results based on 200 test images that the proposed DC
recovery method significantly improves the quality of most recovered images in
terms of the PSNR values and several state-of-the-art objective image quality
assessment (IQA) metrics such as SSIM and MS-SSIM."
"The purpose of this Paper is to describe our research on different feature
extraction and matching techniques in designing a Content Based Image Retrieval
(CBIR) system. Due to the enormous increase in image database sizes, as well as
its vast deployment in various applications, the need for CBIR development
arose. Firstly, this paper outlines a description of the primitive feature
extraction techniques like, texture, colour, and shape. Once these features are
extracted and used as the basis for a similarity check between images, the
various matching techniques are discussed. Furthermore, the results of its
performance are illustrated by a detailed example."
"A method for the design of Fast Haar wavelet for signal processing and image
processing has been proposed. In the proposed work, the analysis bank and
synthesis bank of Haar wavelet is modified by using polyphase structure.
Finally, the Fast Haar wavelet was designed and it satisfies alias free and
perfect reconstruction condition. Computational time and computational
complexity is reduced in Fast Haar wavelet transform."
"Although content-based image retrieval (CBIR) is not a new subject, it keeps
attracting more and more attention, as the amount of images grow tremendously
due to internet, inexpensive hardware and automation of image acquisition. One
of the applications of CBIR is fetching images from a database. This paper
presents a new method for automatic image retrieval using moment invariants and
image entropy, our technique could be used to find semi or perfect matches
based on query by example manner, experimental results demonstrate that the
purposed technique is scalable and efficient."
"Digital Watermarking is used for copyright protection and authentication. In
the proposed system, a Dual Watermarking Scheme based on DWT SVD with chaos
encryption algorithm, will be developed to improve the robustness and
protection along with security. DWT and SVD have been used as a mathematical
tool to embed watermark in the image. Two watermarks are embedded in the host
image. The secondary is embedded into primary watermark and the resultant
watermarked image is encrypted using chaos based logistic map. This provides an
efficient and secure way for image encryption and transmission. The watermarked
image is decrypted and a reliable watermark extraction scheme is developed for
the extraction of the primary as well as secondary watermark from the distorted
image."
"The rapid development of multimedia and internet allows for wide distribution
of digital media data. It becomes much easier to edit, modify and duplicate
digital information besides that, digital documents are also easy to copy and
distribute, therefore it will be faced by many threats. It is a big security
and privacy issue. Another problem with digital document and video is that
undetectable modifications can be made with very simple and widely available
equipment, which put the digital material for evidential purposes under
question With the large flood of information and the development of the digital
format, it become necessary to find appropriate protection because of the
significance, accuracy and sensitivity of the information, therefore multimedia
technology and popularity of internet communications they have great interest
in using digital watermarks for the purpose of copy protection and content
authentication. Digital watermarking is a technique used to embed a known piece
of digital data within another piece of digital data .A digital data may
represent a digital signature or digital watermark that is embedded in the host
media. The signature or watermark is hidden such that it's perceptually and
statistically undetectable. Then this signature or watermark can be extracted
from the host media and used to identify the owner of the media."
"Multimedia data is a form of data that can represent all types of data
(images, sound and text). The use of multimedia data for the online application
requires a more comprehensive database in the use of storage media, Sorting /
indexing, search and system / data searching. This is necessary in order to
help providers and users to access multimedia data online. Systems that use of
the index image as a reference requires storage media so that the rules and
require special expertise to obtain the desired file. Changes in multimedia
data into a series of stories / storyboard in the form of a text will help
reduce the consumption of media storage, system index / sorting and search
applications. Oriented Movement is one method that is being developed to change
the form of multimedia data into a storyboard."
"Institutions all over the world are continuously exploring ways to use ICT in
improving teaching and learning effectiveness. The use of course web pages,
discussion groups, bulletin boards, and e-mails have shown considerable impact
on teaching and learning in significant ways, across all disciplines. ELearning
has emerged as an alternative to traditional classroom-based education and
training and web lectures can be a powerful addition to traditional lectures.
They can even serve as a main content source for learning, provided users can
quickly navigate and locate relevant pages in a web lecture. A web lecture
consists of video and audio of the presenter and slides complemented with
screen capturing. In this paper, an automated approach for recording live
lectures and for browsing available web lectures for on-demand applications by
end users is presented."
"With the rapid development of various multimedia technologies, more and more
multimedia data are generated and transmitted in the medical, commercial, and
military fields, which may include some sensitive information which should not
be accessed by or can only be partially exposed to the general users.
Therefore, security and privacy has become an important, Another problem with
digital document and video is that undetectable modifications can be made with
very simple and widely available equipment, which put the digital material for
evidential purposes under question .With the large flood of information and the
development of the digital format Information hiding considers one of the
techniques which used to protect the important information. The main goals for
this paper, provides a general overview of the New Classification Methods for
Hiding Information into Two Parts: Multimedia Files and Non Multimedia Files."
"The digital image data is rapidly expanding in quantity and heterogeneity.
The traditional information retrieval techniques does not meet the user's
demand, so there is need to develop an efficient system for content based image
retrieval. Content based image retrieval means retrieval of images from
database on the basis of visual features of image like as color, texture etc.
In our proposed method feature are extracted after applying Phong shading on
input image. Phong shading, flattering out the dull surfaces of the image The
features are extracted using color, texture & edge density methods. Feature
extracted values are used to find the similarity between input query image and
the data base image. It can be measure by the Euclidean distance formula. The
experimental result shows that the proposed approach has a better retrieval
results with phong shading."
"This paper describes a framework and a method with which speech communication
can be analyzed. The framework consists of a set of low bit rate, short-range
acoustic communication systems, such as speech, but that are quite different
from speech. The method is to systematically compare these systems according to
different objective functions such as data rate, computational overhead,
psychoacoustic effects and semantics. One goal of this study is to better
understand the nature of human communication. Another goal is to identify
acoustic communication systems that are more efficient than human speech for
some specific purposes."
"Digital watermarking technique has been presented and widely researched to
solve some important issues in the digital world, such as copyright protection,
copy protection and content authentication. Several robust watermarking schemes
based on vector quantization (VQ) have been presented. In this paper, we
present a new digital image watermarking method based on SOFM vector quantizer
for color images. This method utilizes the codebook partition technique in
which the watermark bit is embedded into the selected VQ encoded block. The
main feature of this scheme is that the watermark exists both in VQ compressed
image and in the reconstructed image. The watermark extraction can be performed
without the original image. The watermark is hidden inside the compressed
image, so much transmission time and storage space can be saved when the
compressed data are transmitted over the Internet. Simulation results
demonstrate that the proposed method has robustness against various image
processing operations without sacrificing compression performance and the
computational speed."
"In this paper, we propose a new, scalable approach for the task of object
based image search or object recognition. Despite the very large literature
existing on the scalability issues in CBIR in the sense of retrieval
approaches, the scalability of media and scalability of features remain an
issue. In our work we tackle the problem of scalability and structural
organization of features. The proposed features are nested local graphs built
upon sets of SURF feature points with Delaunay triangulation. A
Bag-of-Visual-Words (BoVW) framework is applied on these graphs, giving birth
to a Bag-of-Graph-Words representation. The nested nature of the descriptors
consists in scaling from trivial Delaunay graphs - isolated feature points - by
increasing the number of nodes layer by layer up to graphs with maximal number
of nodes. For each layer of graphs its proper visual dictionary is built. The
experiments conducted on the SIVAL data set reveal that the graph features at
different layers exhibit complementary performances on the same content. The
nested approach, the combination of all existing layers, yields significant
improvement of the object recognition performance compared to single level
approaches."
"Being an integral part of the network traffic, nowadays it's vital to design
robust mechanisms to provide QoS for multimedia applications. The main goal of
this paper is to provide an efficient solution to support content-aware video
transmission mechanism with buffer underflow avoidance at the receiver in
multipath networks. Towards this, we introduce a content-aware time-varying
utility function, where the quality impacts of video content is incorporated
into its definition. Using the proposed utility function, we formulate a
multipath Dynamic Network Utility Maximization (DNUM) problem for the rate
allocation of video streams, where it takes into account QoS demand of video
streams in terms of buffer underflow avoidance. Finally, using primal-dual
method, we propose a distributed solution that optimally allocates the shared
bandwidth to video streams. The numerical examples demonstrate the efficacy of
the proposed content-aware rate allocation algorithm for video sources in both
single and multiple path network models."
"With the ever-growing digital libraries and video databases, it is
increasingly important to understand and mine the knowledge from video database
automatically. Discovering association rules between items in a large video
database plays a considerable role in the video data mining research areas.
Based on the research and development in the past years, application of
association rule mining is growing in different domains such as surveillance,
meetings, broadcast news, sports, archives, movies, medical data, as well as
personal and online media collections. The purpose of this paper is to provide
general framework of mining the association rules from video database. This
article is also represents the research issues in video association mining
followed by the recent trends."
"In this paper, the authors propose a new algorithm to hide data inside image
using steganography technique. The proposed algorithm uses binary codes and
pixels inside an image. The zipped file is used before it is converted to
binary codes to maximize the storage of data inside the image. By applying the
proposed algorithm, a system called Steganography Imaging System (SIS) is
developed. The system is then tested to see the viability of the proposed
algorithm. Various sizes of data are stored inside the images and the PSNR
(Peak signal-to-noise ratio) is also captured for each of the images tested.
Based on the PSNR value of each images, the stego image has a higher PSNR
value. Hence this new steganography algorithm is very efficient to hide the
data inside the image."
"Retention of secrecy is one of the significant features during communication
activity. Steganography is one of the popular methods to achieve secret
communication between sender and receiver by hiding message in any form of
cover media such as an audio, video, text, images etc. Least significant bit
encoding is the simplest encoding method used by many steganography programs to
hide secret message in 24bit, 8bit colour images and grayscale images.
Steganalysis is a method of detecting secret message hidden in a cover media
using steganography. RS steganalysis is one of the most reliable steganalysis
which performs statistical analysis of the pixels to successfully detect the
hidden message in an image. However, existing steganography method protects the
information against RS steganalysis in grey scale images. This paper presents a
steganography method using genetic algorithm to protect against the RS attack
in colour images. Stego image is divided into number of blocks. Subsequently,
with the implementation of natural evolution on the stego image using genetic
algorithm enables to achieve optimized security and image quality."
"The distributed representation of correlated multi-view images is an
important problem that arise in vision sensor networks. This paper concentrates
on the joint reconstruction problem where the distributively compressed
correlated images are jointly decoded in order to improve the reconstruction
quality of all the compressed images. We consider a scenario where the images
captured at different viewpoints are encoded independently using common coding
solutions (e.g., JPEG, H.264 intra) with a balanced rate distribution among
different cameras. A central decoder first estimates the underlying correlation
model from the independently compressed images which will be used for the joint
signal recovery. The joint reconstruction is then cast as a constrained convex
optimization problem that reconstructs total-variation (TV) smooth images that
comply with the estimated correlation model. At the same time, we add
constraints that force the reconstructed images to be consistent with their
compressed versions. We show by experiments that the proposed joint
reconstruction scheme outperforms independent reconstruction in terms of image
quality, for a given target bit rate. In addition, the decoding performance of
our proposed algorithm compares advantageously to state-of-the-art distributed
coding schemes based on disparity learning and on the DISCOVER."
"Digital watermarking is the process to hide digital pattern directly into a
digital content. Digital watermarking techniques are used to address digital
rights management, protect information and conceal secrets. An invisible
non-blind watermarking approach for gray scale images is proposed in this
paper. The host image is decomposed into 3-levels using Discrete Wavelet
Transform. Based on the parent-child relationship between the wavelet
coefficients the Set Partitioning in Hierarchical Trees (SPIHT) compression
algorithm is performed on the LH3, LH2, HL3 and HL2 subbands to find out the
significant coefficients. The most significant coefficients of LH2 and HL2
bands are selected to embed a binary watermark image. The selected significant
coefficients are modulated using Noise Visibility Function, which is considered
as the best strength to ensure better imperceptibility. The approach is tested
against various image processing attacks such as addition of noise, filtering,
cropping, JPEG compression, histogram equalization and contrast adjustment. The
experimental results reveal the high effectiveness of the method."
"While every network node only relays messages in a traditional communication
system, the recent network coding (NC) paradigm proposes to implement simple
in-network processing with packet combinations in the nodes. NC extends the
concept of ""encoding"" a message beyond source coding (for compression) and
channel coding (for protection against errors and losses). It has been shown to
increase network throughput compared to traditional networks implementation, to
reduce delay and to provide robustness to transmission errors and network
dynamics. These features are so appealing for multimedia applications that they
have spurred a large research effort towards the development of
multimedia-specific NC techniques. This paper reviews the recent work in NC for
multimedia applications and focuses on the techniques that fill the gap between
NC theory and practical applications. It outlines the benefits of NC and
presents the open challenges in this area. The paper initially focuses on
multimedia-specific aspects of network coding, in particular delay, in-network
error control, and media-specific error control. These aspects permit to handle
varying network conditions as well as client heterogeneity, which are critical
to the design and deployment of multimedia systems. After introducing these
general concepts, the paper reviews in detail two applications that lend
themselves naturally to NC via the cooperation and broadcast models, namely
peer-to-peer multimedia streaming and wireless networking."
"Content based video retrieval is an approach for facilitating the searching
and browsing of large image collections over World Wide Web. In this approach,
video analysis is conducted on low level visual properties extracted from video
frame. We believed that in order to create an effective video retrieval system,
visual perception must be taken into account. We conjectured that a technique
which employs multiple features for indexing and retrieval would be more
effective in the discrimination and search tasks of videos. In order to
validate this claim, content based indexing and retrieval systems were
implemented using color histogram, various texture features and other
approaches. Videos were stored in Oracle 9i Database and a user study measured
correctness of response."
"The needs for steganographic techniques for hiding secret message inside
images have been arise. This paper is to create a practical steganographic
implementation to hide text inside grey scale images. The secret message is
hidden inside the cover image using Five Modulus Method. The novel algorithm is
called (ST-FMM. FMM which consists of transforming all the pixels within the
5X5 window size into its corresponding multiples of 5. After that, the secret
message is hidden inside the 5X5 window as a non-multiples of 5. Since the
modulus of non-multiples of 5 are 1,2,3 and 4, therefore; if the reminder is
one of these, then this pixel represents a secret character. The secret key
that has to be sent is the window size. The main advantage of this novel
algorithm is to keep the size of the cover image constant while the secret
message increased in size. Peak signal-to-noise ratio is captured for each of
the images tested. Based on the PSNR value of each images, the stego image has
high PSNR value. Hence this new steganography algorithm is very efficient to
hide the data inside the image."
"We develop a multiexposure image fusion method based on texture features,
which exploits the edge preserving and intraregion smoothing property of
nonlinear diffusion filters based on partial differential equations (PDE). With
the captured multiexposure image series, we first decompose images into base
layers and detail layers to extract sharp details and fine details,
respectively. The magnitude of the gradient of the image intensity is utilized
to encourage smoothness at homogeneous regions in preference to inhomogeneous
regions. Then, we have considered texture features of the base layer to
generate a mask (i.e., decision mask) that guides the fusion of base layers in
multiresolution fashion. Finally, well-exposed fused image is obtained that
combines fused base layer and the detail layers at each scale across all the
input exposures. Proposed algorithm skipping complex High Dynamic Range Image
(HDRI) generation and tone mapping steps to produce detail preserving image for
display on standard dynamic range display devices. Moreover, our technique is
effective for blending flash/no-flash image pair and multifocus images, that
is, images focused on different targets."
"Steganography is one of the methods used for secret communication.
Steganography attempts to hide the existence of the information. The object
used to hide the secret information is called as cover object. Images are the
most popular cover objects used for steganography. Different techniques have to
be used for color image steganography and grey scale image steganography since
they are stored in different ways. Color image are normally stored with 24 bit
depth and grey scale images are stored with 8 bit depth. Color images can hold
large amount of secret information since they have three color components.
Different color spaces namely RGB (Red Green Blue), HSV (Hue, Saturation,
Value), YUV, YIQ, YCbCr (Luminance, Chrominance) etc. are used to represent
color images. Color image steganography can be done in any color space domain.
In this paper color image steganography in RGB and YCbCr domain are compared.
The secret information considered is grey scale image. Since RGB is the common
method of representation, hiding secret information in this format is not
secure."
"Bandwidth consumption is a significant concern for online video service
providers. Practical video streaming systems usually use some form of HTTP
streaming (progressive download) to let users download the video at a faster
rate than the video bitrate. Since users may quit before viewing the complete
video, however, much of the downloaded video will be ""wasted"". To the extent
that users' departure behavior can be predicted, we develop smart streaming
that can be used to improve user QoE with limited server bandwidth or save
bandwidth cost with unlimited server bandwidth. Through measurement, we extract
certain user behavior properties for implementing such smart streaming, and
demonstrate its advantage using prototype implementation as well as
simulations."
"A key problem in random network coding (NC) lies in the complexity and energy
consumption associated with the packet decoding processes, which hinder its
application in mobile environments. Controlling and hence limiting such factors
has always been an important but elusive research goal, since the packet degree
distribution, which is the main factor driving the complexity, is altered in a
non-deterministic way by the random recombinations at the network nodes. In
this paper we tackle this problem proposing Band Codes (BC), a novel class of
network codes specifically designed to preserve the packet degree distribution
during packet encoding, ecombination and decoding. BC are random codes over
GF(2) that exhibit low decoding complexity, feature limited and controlled
degree distribution by construction, and hence allow to effectively apply NC
even in energy-constrained scenarios. In particular, in this paper we motivate
and describe our new design and provide a thorough analysis of its performance.
We provide numerical simulations of the performance of BC in order to validate
the analysis and assess the overhead of BC with respect to a onventional NC
scheme. Moreover, peer-to-peer media streaming experiments with a random-push
protocol show that BC reduce the decoding complexity by a factor of two, to a
point where NC-based mobile streaming to mobile devices becomes practically
feasible."
"The goal of this paper is to investigate the speech signal enhancement using
Kernel Affine Projection Algorithm (KAPA) and Normalized KAPA. The removal of
background noise is very important in many applications like speech
recognition, telephone conversations, hearing aids, forensic, etc. Kernel
adaptive filters shown good performance for removal of noise. If the evaluation
of background noise is more slowly than the speech, i.e., noise signal is more
stationary than the speech, we can easily estimate the noise during the pauses
in speech. Otherwise it is more difficult to estimate the noise which results
in degradation of speech. In order to improve the quality and intelligibility
of speech, unlike time and frequency domains, we can process the signal in new
domain like Reproducing Kernel Hilbert Space (RKHS) for high dimensional to
yield more powerful nonlinear extensions. For experiments, we have used the
database of noisy speech corpus (NOIZEUS). From the results, we observed the
removal noise in RKHS has great performance in signal to noise ratio values in
comparison with conventional adaptive filters."
"Digital information revolution has brought about many advantages and new
issues. The protection of ownership and the prevention of unauthorized
manipulation of digital audio, image, and video materials has become an
important concern due to the ease of editing and perfect reproduction.
Watermarking is identified as a major means to achieve copyright protection. It
is a branch of information hiding which is used to hide proprietary information
in digital media like photographs, digital music, digital video etc. In this
paper, a new image watermarking algorithm that is robust against various
attacks is presented. DWT (Discrete Wavelet Transform) and SVD (Singular Value
Decomposition) have been used to embed two watermarks in the HL and LH bands of
the host image. Simulation evaluation demonstrates that the proposed technique
withstand various attacks."
"Many electronic content providers today like Flickr and Google, offer space
to users to publish their electronic media (e.g. photos and videos) in their
cloud infrastructures, so that they can be publicly accessed. Features like
including other information, such as keywords or owner information into the
digital material is already offered by existing providers. Despite the useful
features made available to users by such infrastructures, the authorship of the
published content is not protected against various attacks such as compression.
In this paper we propose a robust scheme that uses digital invisible
watermarking and hashing to protect the authorship of the digital content and
provide resistance against malicious manipulation of multimedia content. The
scheme is enhanced by an algorithm called MMBEC, that is an extension of an
established scheme MBEC, towards higher resistance."
"Affective multimedia documents such as images, sounds or videos elicit
emotional responses in exposed human subjects. These stimuli are stored in
affective multimedia databases and successfully used for a wide variety of
research in psychology and neuroscience in areas related to attention and
emotion processing. Although important all affective multimedia databases have
numerous deficiencies which impair their applicability. These problems, which
are brought forward in the paper, result in low recall and precision of
multimedia stimuli retrieval which makes creating emotion elicitation
procedures difficult and labor-intensive. To address these issues a new core
ontology STIMONT is introduced. The STIMONT is written in OWL-DL formalism and
extends W3C EmotionML format with an expressive and formal representation of
affective concepts, high-level semantics, stimuli document metadata and the
elicited physiology. The advantages of ontology in description of affective
multimedia stimuli are demonstrated in a document retrieval experiment and
compared against contemporary keyword-based querying methods. Also, a software
tool Intelligent Stimulus Generator for retrieval of affective multimedia and
construction of stimuli sequences is presented."
"In this paper, we propose a method for image block loss restoration based on
the notion of sparse representation. We use the sparsity pattern as side
information to efficiently restore block losses by iteratively imposing the
constraints of spatial and transform domains on the corrupted image. Two novel
features, including a pre-interpolation and a criterion for stopping the
iterations, are proposed to improve the performance. Also, to deal with
practical applications, we develop a technique to transmit the side information
along with the image. In this technique, we first compress the side information
and then embed its LDPC coded version in the least significant bits of the
image pixels. This technique ensures the error-free transmission of the side
information, while causing only a small perturbation on the transmitted image.
Mathematical analysis and extensive simulations are performed to validate the
method and investigate the efficiency of the proposed techniques. The results
verify that the proposed method outperforms its counterparts for image block
loss restoration."
"The paper concerns available steganographic techniques that can be used for
creating covert channels for VoIP (Voice over Internet Protocol) streams. Apart
from characterizing existing steganographic methods we provide new insights by
presenting two new techniques. The first one is network steganography solution
which exploits free/unused protocols' fields and is known for IP, UDP or TCP
protocols but has never been applied to RTP (Real-Time Transport Protocol) and
RTCP (Real-Time Control Protocol) which are characteristic for VoIP. The second
method, called LACK (Lost Audio Packets Steganography), provides hybrid
storage-timing covert channel by utilizing delayed audio packets. The results
of the experiment, that was performed to estimate a total amount of data that
can be covertly transferred during typical VoIP conversation phase, regardless
of steganalysis, are also included in this paper."
"In this paper, we evaluate available steganographic techniques for SIP
(Session Initiation Protocol) that can be used for creating covert channels
during signaling phase of VoIP (Voice over IP) call. Apart from characterizing
existing steganographic methods we provide new insights by introducing new
techniques. We also estimate amount of data that can be transferred in
signalling messages for typical IP telephony call."
"The scalability, as well as the effectiveness, of the different Content-based
Image Retrieval (CBIR) approaches proposed in literature, is today an important
research issue. Given the wealth of images on the Web, CBIR systems must in
fact leap towards Web-scale datasets. In this paper, we report on our
experience in building a test collection of 100 million images, with the
corresponding descriptive features, to be used in experimenting new scalable
techniques for similarity searching, and comparing their results. In the
context of the SAPIR (Search on Audio-visual content using Peer-to-peer
Information Retrieval) European project, we had to experiment our distributed
similarity searching technology on a realistic data set. Therefore, since no
large-scale collection was available for research purposes, we had to tackle
the non-trivial process of image crawling and descriptive feature extraction
(we used five MPEG-7 features) using the European EGEE computer GRID. The
result of this effort is CoPhIR, the first CBIR test collection of such scale.
CoPhIR is now open to the research community for experiments and comparisons,
and access to the collection was already granted to more than 50 research
groups worldwide."
"In this paper, the performance of the emerging MPEG-4 SVC CODEC is evaluated.
In the first part, a brief introduction on the subject of quality assessment
and the development of the MPEG-4 SVC CODEC is given. After that, the used test
methodologies are described in detail, followed by an explanation of the actual
test scenarios. The main part of this work concentrates on the performance
analysis of the MPEG-4 SVC CODEC - both objective and subjective."
"Dealing with network congestion is a criterion used to enhance quality of
service (QoS) in distributed multimedia systems. The existing solutions for the
problem of network congestion ignore scalability considerations because they
maintain a separate classification for each video stream. In this paper, we
propose a new method allowing to control QoS provided to clients according to
the network congestion, by discarding some frames when needed. The technique
proposed, called (m,k)-frame, is scalable with little degradation in
application performances. (m,k)-frame method is issued from the notion of
(m,k)-firm realtime constraints which means that among k invocations of a task,
m invocations must meet their deadline. Our simulation studies show the
usefulness of (m,k)-frame method to adapt the QoS to the real conditions in a
multimedia application, according to the current system load. Notably, the
system must adjust the QoS provided to active clients1 when their number
varies, i.e. dynamic arrival of clients."
"We propose a platform for distributed multimedia applications which
simplifies the development process and at the same time ensures application
portability, flexibility and performance. The platform is implemented using the
Netscape Portable Runtime (NSPR) and the Cross-Platform Component Object Model
(XPCOM)."
"Our research focuses on analysing human activities according to a known
behaviorist scenario, in case of noisy and high dimensional collected data. The
data come from the monitoring of patients with dementia diseases by wearable
cameras. We define a structural model of video recordings based on a Hidden
Markov Model. New spatio-temporal features, color features and localization
features are proposed as observations. First results in recognition of
activities are promising."
"This paper proposes Leader in Charge (LiC), a reliable multicast architecture
for device-to-device (D2D) radio underlaying cellular networks. The
multicast-requesting user equipments (UEs) in close proximity form a D2D
cluster to receive the multicast packets through cooperation. In addition to
receiving the multicast packets from the eNB, UEs share what they received from
the multicast on short-range links among UEs, namely the D2D links, to exploit
the wireless resources a more efficient way. Consequently, we show that
utilizing the D2D links in cellular networks increases the throughput of a
multicast session by means of simulation. We also discuss some practical issues
facing the integration of LiC into the current cellular networks. In
particular, we propose efficient delay control mechanism to reduce the average
and maximum delay experienced by LiC users, which is further confirmed by the
simulation results."
"Empirical data shows that in the absence of incentives, a peer participating
in a Peer-to-Peer (P2P) network wishes to free-riding. Most solutions for
providing incentives in P2P networks are based on direct reciprocity, which are
not appropriate for most P2P multimedia sharing networks due to the unique
features exhibited by such networks: large populations of anonymous agents
interacting infrequently, asymmetric interests of peers, network errors, and
multiple concurrent transactions. In this paper, we design and rigorously
analyze a new family of incentive protocols that utilizes indirect reciprocity
which is based on the design of efficient social norms. In the proposed P2P
protocols, the social norms consist of a social strategy, which represents the
rule prescribing to the peers when they should or should not provide content to
other peers, and a reputation scheme, which rewards or punishes peers depending
on whether they comply or not with the social strategy. We first define the
concept of a sustainable social norm, under which no peer has an incentive to
deviate. We then formulate the problem of designing optimal social norms, which
selects the social norm that maximizes the network performance among all
sustainable social norms. Hence, we prove that it becomes in the self-interest
of peers to contribute their content to the network rather than to free-ride.
We also investigate the impact of various punishment schemes on the social
welfare as well as how should the optimal social norms be designed if
altruistic and malicious peers are active in the network. Our results show that
optimal social norms are capable of providing significant improvements in the
sharing efficiency of multimedia P2P networks."
"This paper addresses rate control for transmission of scalable video streams
via Network Utility Maximization (NUM) formulation. Due to stringent QoS
requirements of video streams and specific characterization of utility
experienced by end-users, one has to solve nonconvex and even nonsmooth NUM
formulation for such streams, where dual methods often prove incompetent.
Convexification plays an important role in this work as it permits the use of
existing dual methods to solve an approximate to the NUM problem iteratively
and distributively. Hence, to tackle the nonsmoothness and nonconvexity, we aim
at reformulating the NUM problem through approximation and transformation of
the ideal discretely adaptive utility function for scalable video streams. The
reformulated problem is shown to be a D.C. (Difference of Convex) problem. We
leveraged Sequential Convex Programming (SCP) approach to replace the nonconvex
D.C. problem by a sequence of convex problems that aim to approximate the
original D.C. problem. We then solve each convex problem produced by SCP
approach using existing dual methods. This procedure is the essence of two
distributed iterative rate control algorithms proposed in this paper, for which
one can show the convergence to a locally optimal point of the nonconvex D.C.
problem and equivalently to a locally optimal point of an approximate to the
original nonconvex problem. Our experimental results show that the proposed
rate control algorithms converge with tractable convergence behavior."
"With the increase in the bandwidth & the transmission speed over the
internet, transmission of multimedia objects like video, audio, images has
become an easier work. In this paper we provide an approach that can be useful
for transmission of video objects over the internet without much fuzz. The
approach provides a ontology based framework that is used to establish an
automatic deployment of video transmission system. Further the video is
compressed using the structural flow mechanism that uses the wavelet principle
for compression of video frames. Finally the video transmission algorithm known
as RRDBFSF algorithm is provided that makes use of the concept of restrictive
flooding to avoid redundancy thereby increasing the efficiency."
"Contrast enhancement is an important area of research for the image analysis.
Over the decade, the researcher worked on this domain to develop an efficient
and adequate algorithm. The proposed method will enhance the contrast of image
using Binarization method with the help of Maximum Likelihood Estimation (MLE).
The paper aims to enhance the image contrast of bimodal and multi-modal images.
The proposed methodology use to collect mathematical information retrieves from
the image. In this paper, we are using binarization method that generates the
desired histogram by separating image nodes. It generates the enhanced image
using histogram specification with binarization method. The proposed method has
showed an improvement in the image contrast enhancement compare with the other
image."
"We present here the first work to propose different mechanisms for hiding
data in the Extensible Messaging and Presence Protocol (XMPP). This is a very
popular instant messaging protocol used by many messaging platforms such as
Google Talk, Cisco, LiveJournal and many others. Our paper describes how to
send a secret message from one XMPP client to another, without raising the
suspicion of any intermediaries. The methods described primarily focus on using
the underlying protocol as a means for steganography, unlike other related
works that try to hide data in the content of instant messages. In doing so, we
provide a more robust means of data hiding and additionally offer some
preliminary analysis of its general security, in particular against
entropic-based steganalysis."
"This paper presents a robust video watermarking scheme in Discrete Fourier
Transform (DFT) and Sequencyordered Complex Hadamard Transform (SCHT). The DFT
and SCHT coefficients are complex and consist of both magnitude and phase and
are well suited to adopt phase shift keying techniques to embed the watermark.
In the proposed schemes, the phases of DFT and SCHT coefficients are modified
to convey watermark information using binary phase shift keying in cover video.
Low amplitude block selection (LABS) is used to improve transparency, amplitude
boost to improve the resistance of watermark from signal processing and
compression attacks and spread spectrum technique is used for encrypting
watermark in order to protect it from third party. It is observed that both
algorithms showing more or less same robustness but SCHT offers high
transparency, simple implementation and less computational cost than DFT."
"Streaming of 60 de-interlaced fields per second digital uncompressed video
with 720x480 resolution without a loss of video fields is one of the desired
technologies by scientists in biomechanics. If it is possible to stream digital
uncompressed video without dropped video fields, then a sophisticated computer
analysis of the transmitted via IEEE 1394a connection video is possible. Such
process is used in biomechanics when it is important to analyze athletes
performance via streaming digital uncompressed video to a computer and then
analyzing it using specific software such as Arial Performance Analysis
Systems."
"Multimedia security has been the aim point of considerable research activity
because of its wide application area. The major technology to achieve copyright
protection, content authentication, access control and multimedia security is
watermarking which is the process of embedding data into a multimedia element
such as image or audio, this embedded data can later be extracted from, or
detected in the embedded element for different purposes. In this work, a blind
watermarking algorithm based on SVD and circulant matrices has been presented.
Every circulant matrix is associated with a matrix for which the SVD
decomposition coincides with the spectral decomposition. This leads to improve
the Chandra algorithm [1], our presentation will include a discussion on the
data hiding capacity, watermark transparency and robustness against a wide
range of common image processing attacks."
"Block-based motion estimation (ME) and compensation (MC) techniques are
widely used in modern video processing algorithms and compression systems. The
great variety of video applications and devices results in numerous compression
specifications. Specifically, there is a diversity of frame-rates and
bit-rates. In this paper, we study the effect of frame-rate and compression
bit-rate on block-based ME and MC as commonly utilized in inter-frame coding
and frame-rate up conversion (FRUC). This joint examination yields a
comprehensive foundation for comparing MC procedures in coding and FRUC. First,
the video signal is modeled as a noisy translational motion of an image. Then,
we theoretically model the motion-compensated prediction of an available and
absent frames as in coding and FRUC applications, respectively. The theoretic
MC-prediction error is further analyzed and its autocorrelation function is
calculated for coding and FRUC applications. We show a linear relation between
the variance of the MC-prediction error and temporal-distance. While the
affecting distance in MC-coding is between the predicted and reference frames,
MC-FRUC is affected by the distance between the available frames used for the
interpolation. Moreover, the dependency in temporal-distance implies an inverse
effect of the frame-rate. FRUC performance analysis considers the prediction
error variance, since it equals to the mean-squared-error of the interpolation.
However, MC-coding analysis requires the entire autocorrelation function of the
error; hence, analytic simplicity is beneficial. Therefore, we propose two
constructions of a separable autocorrelation function for prediction error in
MC-coding. We conclude by comparing our estimations with experimental results."
"We describe an intermediate language designed as a medium-level internal
representation of programs of the interactive music system Antescofo. This
representation is independent both of the Antescofo source language and of the
architecture of the execution platform. It is used in tasks such as
verification of timings, model-based conformance testing, static control-flow
analysis or simulation. This language is essentially a flat representation of
Antescofo's code, as a finite state machine extended with local and global
variables, with delays and with concurrent threads creation. It features a
small number of simple instructions which are either blocking (wait for
external event, signal or duration) or not (variable assignment, message
emission and control)."
"A text steganography method based on Markov chains is introduced, together
with a reference implementation. This method allows for information hiding in
texts that are automatically generated following a given Markov model. Other
Markov - based systems of this kind rely on big simplifications of the language
model to work, which produces less natural looking and more easily detectable
texts. The method described here is designed to generate texts within a good
approximation of the original language model provided."
"Mobile media has undoubtedly become the predominant source of traffic in
wireless networks. The result is not only congestion and poor
Quality-of-Experience, but also an unprecedented energy drain at both the
network and user devices. In order to sustain this continued growth, novel
disruptive paradigms of media delivery are urgently needed. We envision that
two key contemporary advancements can be leveraged to develop greener media
delivery platforms: 1) the proliferation of navigation hardware and software in
mobile devices has created an era of location-awareness, where both the current
and future user locations can be predicted; and 2) the rise of context-aware
network architectures and self-organizing functionalities is enabling context
signaling and in-network adaptation. With these developments in mind, this
article investigates the opportunities of exploiting location-awareness to
enable green end-to-end media delivery. In particular, we discuss and propose
approaches for location-based adaptive video quality planning, in-network
caching, content prefetching, and long-term radio resource management. To
provide insights on the energy savings, we then present a cross-layer framework
that jointly optimizes resource allocation and multi-user video quality using
location predictions. Finally, we highlight some of the future research
directions for location-aware media delivery in the conclusion."
"In recent years, characterized by the innovation of technology and the
digital revolution, the field of media has become important. The transfer and
exchange of multimedia data and duplication have become major concerns of
researchers. Consequently, protecting copyrights and ensuring service safety is
needed. Cryptography has a specific role, is to protect secret files against
unauthorized access. In this paper, a hierarchical cryptosystem algorithm based
on Logistic Map chaotic systems is proposed. The results show that the proposed
method improves the security of the image. Experimental results on a database
of 200 medical images show that the proposed method significantly gives better
results."
"In this paper, we propose a general cross-layer optimization framework in
which we explicitly consider both the heterogeneous and dynamically changing
characteristics of delay-sensitive applications and the underlying time-varying
network conditions. We consider both the independently decodable data units
(DUs, e.g. packets) and the interdependent DUs whose dependencies are captured
by a directed acyclic graph (DAG). We first formulate the cross-layer design as
a non-linear constrained optimization problem by assuming complete knowledge of
the application characteristics and the underlying network conditions. The
constrained cross-layer optimization is decomposed into several cross-layer
optimization subproblems for each DU and two master problems. The proposed
decomposition method determines the necessary message exchanges between layers
for achieving the optimal cross-layer solution. However, the attributes (e.g.
distortion impact, delay deadline etc) of future DUs as well as the network
conditions are often unknown in the considered real-time applications. The
impact of current cross-layer actions on the future DUs can be characterized by
a state-value function in the Markov decision process (MDP) framework. Based on
the dynamic programming solution to the MDP, we develop a low-complexity
cross-layer optimization algorithm using online learning for each DU
transmission. This online algorithm can be implemented in real-time in order to
cope with unknown source characteristics, network dynamics and resource
constraints. Our numerical results demonstrate the efficiency of the proposed
online algorithm."
"Recently, great attention was intended toward overcomplete dictionaries and
the sparse representations they can provide. In a wide variety of signal
processing problems, sparsity serves a crucial property leading to high
performance. Inpainting, the process of reconstructing lost or deteriorated
parts of images or videos, is an interesting application which can be handled
by suitably decomposition of an image through combination of overcomplete
dictionaries. This paper addresses a novel technique of such a decomposition
and investigate that through inpainting of images. Simulations are presented to
demonstrate the validation of our approach."
"In this paper, we discuss the issues in automatic recognition of vowels in
Persian language. The present work focuses on new statistical method of
recognition of vowels as a basic unit of syllables. First we describe a vowel
detection system then briefly discuss how the detected vowels can feed to
recognition unit. According to pattern recognition, Support Vector Machines
(SVM) as a discriminative classifier and Gaussian mixture model (GMM) as a
generative model classifier are two most popular techniques. Current
state-ofthe- art systems try to combine them together for achieving more power
of classification and improving the performance of the recognition systems. The
main idea of the study is to combine probabilistic SVM and traditional GMM
pattern classification with some characteristic of speech like band-pass energy
to achieve better classification rate. This idea has been analytically
formulated and tested on a FarsDat based vowel recognition system. The results
show inconceivable increases in recognition accuracy. The tests have been
carried out by various proposed vowel recognition algorithms and the results
have been compared."
"This paper presents a super-resolution method based on gradient-based
adaptive interpolation. In this method, in addition to considering the distance
between the interpolated pixel and the neighboring valid pixel, the
interpolation coefficients take the local gradient of the original image into
account. The smaller the local gradient of a pixel is, the more influence it
should have on the interpolated pixel. And the interpolated high resolution
image is finally deblurred by the application of wiener filter. Experimental
results show that our proposed method not only substantially improves the
subjective and objective quality of restored images, especially enhances edges,
but also is robust to the registration error and has low computational
complexity."
"QoS is a very important issue for multimedia communication systems. In this
paper, a new system that reinstalls the relation between the QoS elements
(RSVP, routing protocol, sender, and receiver) during the multimedia
transmission is proposed, then an alternative path is created in case of
original multimedia path failure. The suggested system considers the resulting
problems that may be faced within and after the creation of rerouting path.
Finally, the proposed system is simulated using OPNET 11.5 simulation package.
Simulation results show that our proposed system outperforms the old one in
terms of QoS parameters like packet loss and delay jitter."
"Streaming high quality videos consumes significantly large amount of network
resources. In this context request to service delay, network traffic,
congestion and server overloading are the main parameters to be considered in
video streaming over the internet that effect the quality of service (QoS). In
this paper, we propose an efficient architecture as a cluster of proxy servers
and clients that uses a peer to peer (P2P) approach to cooperatively stream the
video using chaining technique. We consider the following two key issues in the
proposed architecture (1) Prefix caching technique to accommodate more number
of videos close to client (2) Cooperative client and proxy chaining to achieve
the network efficiency. Our simulation results shows that the proposed approach
yields a prefix caching close to the optimal solution minimizing WAN bandwidth
usage on server-proxy path by utilizing the proxy-client and client-client path
bandwidth, which is much cheaper than the expensive server proxy path
bandwidth, server load, and client rejection ratio significantly using
chaining."
"Dance video is one of the important types of narrative videos with semantic
rich content. This paper proposes a new meta model, Dance Video Content Model
(DVCM) to represent the expressive semantics of the dance videos at multiple
granularity levels. The DVCM is designed based on the concepts such as video,
shot, segment, event and object, which are the components of MPEG-7 MDS. This
paper introduces a new relationship type called Temporal Semantic Relationship
to infer the semantic relationships between the dance video objects. Inverted
file based index is created to reduce the search time of the dance queries. The
effectiveness of containment queries using precision and recall is depicted.
Keywords: Dance Video Annotations, Effectiveness Metrics, Metamodeling,
Temporal Semantic Relationships."
"We consider the problem of rate allocation among multiple simultaneous video
streams sharing multiple heterogeneous access networks. We develop and evaluate
an analytical framework for optimal rate allocation based on observed available
bit rate (ABR) and round-trip time (RTT) over each access network and video
distortion-rate (DR) characteristics. The rate allocation is formulated as a
convex optimization problem that minimizes the total expected distortion of all
video streams. We present a distributed approximation of its solution and
compare its performance against H-infinity optimal control and two heuristic
schemes based on TCP-style additive-increase-multiplicative decrease (AIMD)
principles. The various rate allocation schemes are evaluated in simulations of
multiple high-definition (HD) video streams sharing multiple access networks.
Our results demonstrate that, in comparison with heuristic AIMD-based schemes,
both media-aware allocation and H-infinity optimal control benefit from
proactive congestion avoidance and reduce the average packet loss rate from 45%
to below 2%. Improvement in average received video quality ranges between 1.5
to 10.7 dB in PSNR for various background traffic loads and video playout
deadlines. Media-aware allocation further exploits its knowledge of the video
DR characteristics to achieve a more balanced video quality among all streams."
"Web conferencing tools have entered the mainstream of business applications.
Using web conferencing for IEEE conferences has a good potential of adding
value to both organizers and participants. Authors propose a concept of Truly
Integrated Conference (TIC) according to which a multi-point
worldwide-distributed network of conference online authors/participants will
enhance the standard (centralized) IEEE conference model, which requires
attendance of the participants in person at the main conference location. The
concept entails seamless integration of the onsite and online conference
systems, including data/presentation, video, audio channels. Benefits and
challenges of the TIC concept are analyzed. Requirements to the web
conferencing system capable of supporting the TIC conference are presented and
reviewed against commercial web conferencing tools. Case study of the IEEE
Toronto International Conference ? Science and Technology for Humanity, which
was the first realization of TIC, is presented which analyzes various aspects
(organizational, technological, and financial) of the integrated conference."
"We take an analytical approach to study Quality of user Experience (QoE) for
video streaming applications. First, we show that random linear network coding
applied to blocks of video frames can significantly simplify the packet
requests at the network layer and save resources by avoiding duplicate packet
reception. Network coding allows us to model the receiver's buffer as a queue
with Poisson arrivals and deterministic departures. We consider the probability
of interruption in video playback as well as the number of initially buffered
packets (initial waiting time) as the QoE metrics. We characterize the optimal
trade-off between these metrics by providing upper and lower bounds on the
minimum initial buffer size, required to achieve certain level of interruption
probability for different regimes of the system parameters. Our bounds are
asymptotically tight as the file size goes to infinity."
"In this paper, A new image steganography scheme is proposed which is a kind
of spatial domain technique. In order to hide secret data in cover-image, the
first component alteration technique is used. Techniques used so far focuses
only on the two or four bits of a pixel in a image (at the most five bits at
the edge of an image) which results in less peak to signal noise ratio and high
root mean square error. In this technique, 8 bits of blue components of pixels
are replaced with secret data bits. Proposed scheme can embed more data than
previous schemes and shows better image quality. To prove this scheme, several
experiments are performed, and are compared the experimental results with the
related previous works."
"For enhancing the protection level of dynamic graph software watermarks and
for the purpose of conducting the analysis which evaluates the effect of
integrating two software protection techniques such as software watermarking
and tamper proofing, constant encoding technique along with the enhancement
through the idea of constant splitting is proposed. In this paper Thomborson
technique has been implemented with the scheme of breaking constants which
enables to encode all constants without having any consideration about their
values with respect to the value of watermark tree. Experimental analysis which
have been conducted and provided in this paper concludes that the constant
encoding process significantly increases the code size, heap space usage, and
execution time, while making the tamper proofed code resilient to variety of
semantic preserving program transformation attacks."
"This paper introduces a novel indexing and access method, called Feature-
Based Adaptive Tolerance Tree (FATT), using wavelet transform is proposed to
organize large image data sets efficiently and to support popular image access
mechanisms like Content Based Image Retrieval (CBIR).Conventional database
systems are designed for managing textual and numerical data and retrieving
such data is often based on simple comparisons of text or numerical values.
However, this method is no longer adequate for images, since the digital
presentation of images does not convey the reality of images. Retrieval of
images become difficult when the database is very large. This paper addresses
such problems and presents a novel indexing technique, Feature Based Adaptive
Tolerance Tree (FATT), which is designed to bring an effective solution
especially for indexing large databases. The proposed indexing scheme is then
used along with a query by image content, in order to achieve the ultimate goal
from the user point of view that is retrieval of all relevant images. FATT
indexing technique, features of the image is extracted using 2-dimensional
discrete wavelet transform (2DDWT) and index code is generated from the
determinant value of the features. Multiresolution analysis technique using
2D-DWT can decompose the image into components at different scales, so that the
coarest scale components carry the global approximation information while the
finer scale components contain the detailed information. Experimental results
show that the FATT outperforms M-tree upto 200%, Slim-tree up to 120% and HCT
upto 89%. FATT indexing technique is adopted to increase the efficiently of
data storage and retrieval."
"The development and application of various remote sensing platforms result in
the production of huge amounts of satellite image data. Therefore, there is an
increasing need for effective querying and browsing in these image databases.
In order to take advantage and make good use of satellite images data, we must
be able to extract meaningful information from the imagery. Hence we proposed a
new algorithm for SAR image segmentation. In this paper we propose segmentation
using vector quantization technique on entropy image. Initially, we obtain
entropy image and in second step we use Kekre's Fast Codebook Generation (KFCG)
algorithm for segmentation of the entropy image. Thereafter, a codebook of size
128 was generated for the Entropy image. These code vectors were further
clustered in 8 clusters using same KFCG algorithm and converted into 8 images.
These 8 images were displayed as a result. This approach does not lead to over
segmentation or under segmentation. We compared these results with well known
Gray Level Co-occurrence Matrix. The proposed algorithm gives better
segmentation with less complexity."
"Establishing hidden communication is an important subject of discussion that
has gained increasing importance nowadays with the development of the internet.
One of the key methods for establishing hidden communication is steganography.
Modern day steganography mainly deals with hiding information within files like
image, text, html, binary files etc. These file contains small irrelevant
information that can be substituted for small secret data. To store a high
capacity secret data these carrier files are not very supportive. To overcome
the problem of storing the high capacity secret data with the utmost security
fence, we have proposed a novel methodology for concealing a voluminous data
with high levels of security wall by using movie clip as a carrier file."
"Offering of different attractive opportunities by different wireless
technologies trends the convergence of heterogeneous networks for the future
wireless communication system. To make a seamless handover among the
heterogeneous networks, the optimization of the power consumption, and optimal
selection of interface are the challenging issues for convergence networks. The
access of multi interfaces simultaneously reduces the handover latency and data
loss in heterogeneous handover. The mobile node (MN) maintains one interface
connection while other interface is used for handover process. However, it
causes much battery power consumption. In this paper we propose an efficient
interface selection scheme including interface selection algorithms, interface
selection procedures considering battery power consumption and user mobility
with other existing parameters for overlaying networks. We also propose a
priority based network selection scheme according to the service types. MN's
battery power level, provision of QoS/QoE in the target network and our
proposed priority parameters are considered as more important parameters for
our interface selection algorithm. The performances of the proposed scheme are
verified using numerical analysis."
"This paper presents a study of audio-video streaming using the additional
possibilities of a DVB-S card. The board used for experiments (Technisat
SkyStar 2) is one of the most frequently used cards for this purpose. Using the
main blocks of the board's software support it is possible the implement a
really useful and full functional system for audio-video streaming. The
streaming is possible to be implemented either for decoded MPEG stream or for
transport stream. In this last case it is possible to view not only a program,
but any program from the same multiplex. This allows us to implement"
"While mobile IPTV services are supported through the mobile WiMAX networks,
there must need some guaranteed bandwidth for the IPTV services especially if
IPTV and non-IPTV services are simultaneously supported by the mobile WiMAX
networks. The quality of an IPTV service definitely depends on the allocated
bandwidth for that channel. However, due to the high quality IPTV services and
to support of huge non-IPTV traffic over mobile WiMAX networks, it is not
possible to guarantee the sufficient amount of the limited mobile WiMAX
bandwidth for the mobile IPTV services every time. A Service Level Agreement
(SLA) between the mobile IPTV service provider and mobile WiMAX network
operator to reserve sufficient bandwidth for the IPTV calls can increase the
satisfaction level of the mobile IPTV users. In this paper, we propose a SLA
negotiation procedure for mobile IPTV users over mobile WiMAX networks. The
Bandwidth Broker controls the allocated bandwidth for IPTV and non-IPTV users.
The proposed dynamically reserved bandwidth for the IPTV services increases the
IPTV user's satisfaction level. The simulation results state that, our proposed
scheme is able to provide better user satisfaction level for the IPTV users."
"In this paper, we introduce a shape-based, time-scale invariant feature
descriptor for 1-D sensor signals. The time-scale invariance of the feature
allows us to use feature from one training event to describe events of the same
semantic class which may take place over varying time scales such as walking
slow and walking fast. Therefore it requires less training set. The descriptor
takes advantage of the invariant location detection in the scale space theory
and employs a high level shape encoding scheme to capture invariant local
features of events. Based on this descriptor, a scale-invariant classifier with
""R"" metric (SIC-R) is designed to recognize multi-scale events of human
activities. The R metric combines the number of matches of keypoint in scale
space with the Dynamic Time Warping score. SICR is tested on various types of
1-D sensors data from passive infrared, accelerometer and seismic sensors with
more than 90% classification accuracy."
"The paper presents an implementation and compatibility tests of a simple home
network implemented in a nonconventional manner using a CATV coaxial cable.
Reusing the cable, normally designated to supply RF modulated TV signals from
cable TV networks, makes possible to add data services as well. A short
presentation of the technology is given with an investigation of the main
performances obtained using this technique. The measurements revealed that this
simple solution makes possible to have both TV and data services with
performances close to traditional home data services: cable modems or ADSL,
with minimal investments. This technology keeps also open the possibility for
future improvements of the network: DVB-C or Data via Cable Modems."
"Recently many research efforts have been devoted to image annotation by
leveraging on the associated tags/keywords of web images as training labels. A
key issue to resolve is the relatively low accuracy of the tags. In this paper,
we propose a novel semi-automatic framework to construct a more accurate and
effective training set from these web media resources for each label that we
want to learn. Experiments conducted on a real-world dataset demonstrate that
the constructed training set can result in higher accuracy for image
annotation."
"In this paper, we show that we can apply probabilistic spatiotemporal
macroblock filtering (PSMF) and partial decoding processes to effectively
detect and track multiple objects in real time in H.264|AVC bitstreams with
stationary background. Our contribution is that our method cannot only show
fast processing time but also handle multiple moving objects that are
articulated, changing in size or internally have monotonous color, even though
they contain a chaotic set of non-homogeneous motion vectors inside. In
addition, our partial decoding process for H.264|AVC bitstreams enables to
improve the accuracy of object trajectories and overcome long occlusion by
using extracted color information."
"Lossy JPEG compression is a widely used compression technique. Normally the
JPEG standard technique uses three process mapping reduces interpixel
redundancy, quantization, which is lossy process and entropy encoding, which is
considered lossless process. In this paper, a new technique has been proposed
by combining the JPEG algorithm and Symbol Reduction Huffman technique for
achieving more compression ratio. The symbols reduction technique reduces the
number of symbols by combining together to form a new symbol. As a result of
this technique the number of Huffman code to be generated also reduced. It is
simple fast and easy to implement. The result shows that the performance of
standard JPEG method can be improved by proposed method. This hybrid approach
achieves about 20% more compression ratio than the Standard JPEG."
"Image compression helps in storing the transmitted data in proficient way by
decreasing its redundancy. This technique helps in transferring more digital or
multimedia data over internet as it increases the storage space. It is
important to maintain the image quality even if it is compressed to certain
extent. Depend upon this the image compression is classified into two
categories : lossy and lossless image compression. There are many lossy digital
image compression techniques exists. Among this Incremental Self Organizing Map
is a familiar one. The good pictures quality can be retrieved if image
denoising technique is used for compression and also provides better
compression ratio. Image denoising is an important pre-processing step for many
image analysis and computer vision system. It refers to the task of recovering
a good estimate of the true image from a degraded observation without altering
and changing useful structure in the image such as discontinuities and edges.
Many approaches have been proposed to remove the noise effectively while
preserving the original image details and features as much as possible. This
paper proposes a technique for image compression using Incremental Self
Organizing Map (ISOM) with Discret Wavelet Transform (DWT) by applying
filtering techniques which play a crucial role in enhancing the quality of a
reconstructed image. The experimental result shows that the proposed technique
obtained better compression ratio value."
"The dominant paradigm for video chat employs a single camera at each end of
the conversation, but some conversations can be greatly enhanced by using
multiple cameras at one or both ends. This paper provides the first rigorous
investigation of multi-camera video chat, concentrating especially on the
ability of users to switch between views at either end of the conversation. A
user study of 23 individuals analyzes the advantages and disadvantages of
permitting a user to switch between views at a remote location. Benchmark
experiments employing up to four webcams simultaneously demonstrate that
multi-camera video chat is feasible on consumer hardware. The paper also
presents the design of MultiCam, a software package permitting multi-camera
video chat. Some important trade-offs in the design of MultiCam are discussed,
and typical usage scenarios are analyzed."
"This paper gives a summary of the content-based Image Retrieval and
Content-based Audio Retrieval, which are two parts of the Content-based
Retrieval. Content-based Retrieval is the retrieval based on the features of
the content. Generally, it is a way to extract features of the media data and
find other data with the similar features from the database automatically.
Content-based Retrieval can not only work on discrete media like texts, but
also can be used on continuous media, such as video and audio."
"We report results from a measurement study of three video streaming services,
YouTube, Dailymotion and Vimeo on six different smartphones. We measure and
analyze the traffic and energy consumption when streaming different quality
videos over Wi-Fi and 3G. We identify five different techniques to deliver the
video and show that the use of a particular technique depends on the device,
player, quality, and service. The energy consumption varies dramatically
between devices, services, and video qualities depending on the streaming
technique used. As a consequence, we come up with suggestions on how to improve
the energy efficiency of mobile video streaming services."
"Speech activity detection (SAD) is an essential component for a variety of
speech processing applications. It has been observed that performances of
various speech based tasks are very much dependent on the efficiency of the
SAD. In this paper, we have systematically reviewed some popular SAD techniques
and their applications in speaker recognition. Speaker verification system
using different SAD technique are experimentally evaluated on NIST speech
corpora using Gaussian mixture model- universal background model (GMM-UBM)
based classifier for clean and noisy conditions. It has been found that two
Gaussian modeling based SAD is comparatively better than other SAD techniques
for different types of noises."
"Enabling users to interactively navigate through different viewpoints of a
static scene is a new interesting functionality in 3D streaming systems. While
it opens exciting perspectives towards rich multimedia applications, it
requires the design of novel representations and coding techniques in order to
solve the new challenges imposed by interactive navigation. Interactivity
clearly brings new design constraints: the encoder is unaware of the exact
decoding process, while the decoder has to reconstruct information from
incomplete subsets of data since the server can generally not transmit images
for all possible viewpoints due to resource constrains. In this paper, we
propose a novel multiview data representation that permits to satisfy bandwidth
and storage constraints in an interactive multiview streaming system. In
particular, we partition the multiview navigation domain into segments, each of
which is described by a reference image and some auxiliary information. The
auxiliary information enables the client to recreate any viewpoint in the
navigation segment via view synthesis. The decoder is then able to navigate
freely in the segment without further data request to the server; it requests
additional data only when it moves to a different segment. We discuss the
benefits of this novel representation in interactive navigation systems and
further propose a method to optimize the partitioning of the navigation domain
into independent segments, under bandwidth and storage constraints.
Experimental results confirm the potential of the proposed representation;
namely, our system leads to similar compression performance as classical
inter-view coding, while it provides the high level of flexibility that is
required for interactive streaming. Hence, our new framework represents a
promising solution for 3D data representation in novel interactive multimedia
services."
"In multiview applications, multiple cameras acquire the same scene from
different viewpoints and generally produce correlated video streams. This
results in large amounts of highly redundant data. In order to save resources,
it is critical to handle properly this correlation during encoding and
transmission of the multiview data. In this work, we propose a
correlation-aware packet scheduling algorithm for multi-camera networks, where
information from all cameras are transmitted over a bottleneck channel to
clients that reconstruct the multiview images. The scheduling algorithm relies
on a new rate-distortion model that captures the importance of each view in the
scene reconstruction. We propose a problem formulation for the optimization of
the packet scheduling policies, which adapt to variations in the scene content.
Then, we design a low complexity scheduling algorithm based on a trellis search
that selects the subset of candidate packets to be transmitted towards
effective multiview reconstruction at clients. Extensive simulation results
confirm the gain of our scheduling algorithm when inter-source correlation
information is used in the scheduler, compared to scheduling policies with no
information about the correlation or non-adaptive scheduling policies. We
finally show that increasing the optimization horizon in the packet scheduling
algorithm improves the transmission performance, especially in scenarios where
the level of correlation rapidly varies with time."
"In this paper, we propose a novel image interpolation algorithm, which is
formulated via combining both the local autoregressive (AR) model and the
nonlocal adaptive 3-D sparse model as regularized constraints under the
regularization framework. Estimating the high-resolution image by the local AR
regularization is different from these conventional AR models, which weighted
calculates the interpolation coefficients without considering the rough
structural similarity between the low-resolution (LR) and high-resolution (HR)
images. Then the nonlocal adaptive 3-D sparse model is formulated to regularize
the interpolated HR image, which provides a way to modify these pixels with the
problem of numerical stability caused by AR model. In addition, a new
Split-Bregman based iterative algorithm is developed to solve the above
optimization problem iteratively. Experiment results demonstrate that the
proposed algorithm achieves significant performance improvements over the
traditional algorithms in terms of both objective quality and visual perception"
"Multiview video has recently emerged as a means to improve user experience in
novel multimedia services. We propose a new stochastic model to characterize
the traffic generated by a Multiview Video Coding (MVC) variable bit rate
source. To this aim, we resort to a Poisson Hidden Markov Model (P-HMM), in
which the first (hidden) layer represents the evolution of the video activity
and the second layer represents the frame sizes of the multiple encoded views.
We propose a method for estimating the model parameters in long MVC sequences.
We then present extensive numerical simulations assessing the model's ability
to produce traffic with realistic characteristics for a general class of MVC
sequences. We then extend our framework to network applications where we show
that our model is able to accurately describe the sender and receiver buffers
behavior in MVC transmission. Finally, we derive a model of user behavior for
interactive view selection, which, in conjunction with our traffic model, is
able to accurately predict actual network load in interactive multiview
services."
"The amount of digital video data is increasing over the world. It highlights
the need for efficient algorithms that can index, retrieve and browse this data
by content. This can be achieved by identifying semantic description captured
automatically from video structure. Among these descriptions, text within video
is considered as rich features that enable a good way for video indexing and
browsing. Unlike most video text detection and extraction methods that treat
video sequences as collections of still images, we propose in this paper
spatiotemporal. video-text localization and identification approach which
proceeds in two main steps: text region localization and text region
classification. In the first step we detect the significant appearance of the
new objects in a frame by a split and merge processes applied on binarized edge
frame pair differences. Detected objects are, a priori, considered as text.
They are then filtered according to both local contrast variation and texture
criteria in order to get the effective ones. The resulted text regions are
classified based on a visual grammar descriptor containing a set of semantic
text class regions characterized by visual features. A visual table of content
is then generated based on extracted text regions occurring within video
sequence enriched by a semantic identification. The experimentation performed
on a variety of video sequences shows the efficiency of our approach."
"In this paper, we propose a spatial temporal video-text detection technique
which proceed in two principal steps:potential text region detection and a
filtering process. In the first step we divide dynamically each pair of
consecutive video frames into sub block in order to detect change. A
significant difference between homologous blocks implies the appearance of an
important object which may be a text region. The temporal redundancy is then
used to filter these regions and forms an effective text region. The
experimentation driven on a variety of video sequences shows the effectiveness
of our approach by obtaining a 89,39% as precision rate and 90,19 as recall."
"Automatic identification of TV programs within TV streams is an important
task for archive exploitation. This paper proposes a new spatial-temporal
approach to identify programs in TV streams in two main steps: First, a
reference catalogue for video grammars of visual jingles is constructed. We
exploit visual grammars characterizing instances of the same program type in
order to identify the various program types in the TV stream. The role of video
grammar is to represent the visual invariants for each visual jingle using a
set of descriptors appropriate for each TV program. Secondly, programs in TV
streams are identified by examining the similarity of the video signal to the
visual grammars in the catalogue. The main idea of identification process
consists in comparing the visual similarity of the video signal signature in TV
stream to the catalogue elements. After presenting the proposed approach, the
paper overviews the encouraging experimental results on several streams
extracted from different channels and composed of several programs."
"The availability of bandwidth for internet access is sufficient enough to
communicate digital assets. These digital assets are subjected to various types
of threats. [19] As a result of this, protection mechanism required for the
protection of digital assets is of priority in research. The threat of current
focus is unauthorized copying of digital assets which give boost to piracy.
This under the copyright act is illegal and a robust mechanism is required to
curb this kind of unauthorized copy. To safeguard the copyright digital assets,
a robust digital watermarking technique is needed. The existing digital
watermarking techniques protect digital assets by embedding a digital watermark
into a host digital image. This embedding does induce slight distortion in the
host image but the distortion is usually too small to be noticed. At the same
time the embedded watermark must be robust enough to with stand deliberate
attacks. There are various techniques of digital watermarking but researchers
are making constant efforts to increase the robustness of the watermark image.
The layered approach of watermarking based on Huffman coding [5] can soon
increase the robustness of digital watermark.[11] Ultimately, increasing the
security of copyright of protection. The proposed work is in similar direction
where in RMI (Random Matrix Image) is used in place of Huffman coding. This
innovative algorithm has considerably increased the robustness in digital
watermark while also enhancing security of production"
"This paper presents an extensible and reusable framework which addresses the
problem of video quality assessment over IP networks. The proposed tool
(referred to as Video-Tester) supports raw uncompressed video encoding and
decoding. It also includes different video over IP transmission methods (i.e.:
RTP over UDP unicast and multicast, as well as RTP over TCP). In addition, it
is furnished with a rich set of offline analysis capabilities. Video-Tester
analysis includes QoS and bitstream parameters estimation (i.e.: bandwidth,
packet inter-arrival time, jitter and loss rate, as well as GOP size and
I-frame loss rate). Our design facilitates the integration of virtually any
existing video quality metric thanks to the adopted Python-based modular
approach. Video-Tester currently provides PSNR, SSIM, ITU-T G.1070 video
quality metric, DIV and PSNR-based MOS estimations. In order to promote its use
and extension, Video-Tester is open and publicly available."
"In this paper a novel approach to embed watermark into the host image using
quantization with the help of Dynamic Fuzzy Inference System (DFIS) is
proposed. The cover image is decomposed up to 3- levels using quantization and
Discrete Wavelet Transform (DWT). A bitmap of size 64x64 pixels is embedded
into the host image using DFIS rule base. The DFIS is utilized to generate the
watermark weighting function to embed the imperceptible watermark. The
implemented watermarking algorithm is imperceptible and robust to some normal
attacks such as JPEG Compression, salt&pepper noise, median filtering, rotation
and cropping.
  Keywords: Watermark, Quantization, Dynamic Fuzzy Inference System,
Imperceptible, Robust, JPEG Compression, Cropping."
"Video sharing sites like YouTube, Metacafe, Dailymotion, Vimeo, etc. provide
a platform for media content sharing among its users. Some of these videos are
copyright protected and restricted from being downloaded and saved. But users
can use various download managers or application programs to download and save
these videos. This affects the incoming traffic on these websites reducing
their hit rate and consequently reducing their revenue. Adobe Flash Player is
the most commonly used player for watching online videos. It uses RTMP (Real
Time Messaging Protocol) to stream audio, video and data over the Internet,
between a Flash Player and Adobe Flash Media Server.Here, we propose a plug-in
that enables the site owner control over downloading of videos from such
website. The plug-in will be installed at the client side with the consent of
the user. When the video is being played this plug-in will send unique keys to
the media server. The server will continue streaming the video after verifying
the keys. Download managers or application programs will not be able to
download the videos as they wont be able to create the unique keys that need to
be sent to the server."
"In medical field, intravascular ultrasound (IVUS) is a tomographic imaging
modality, which can identify the boundaries of different layers of blood
vessels. IVUS can detect myocardial infarction (heart attack) that remains
ignored and unattended when only angioplasty is done. During the past decade,
it became easier for some individuals or groups to copy and transmits digital
information without the permission of the owner. For increasing authentication
and security of copyrights, digital watermarking, an information hiding
technique, was introduced. Achieving watermarking technique with lesser amount
of distortion in biomedical data is a challenging task. Watermark can be
embedded into an image or in a video. As video data is a huge amount of
information, therefore a large storage area is needed which is not feasible. In
this case motion vector based video compression is done to reduce size. In this
present paper, an Electronic Patient Record (EPR) is embedded as watermark
within an IVUS video and then motion vector is calculated. This proposed method
proves robustness as the extracted watermark has good PSNR value and less MSE."
"A large number of image forensics methods are available which are capable of
identifying image tampering. But these techniques are not capable of addressing
the anti-forensics method which is able to hide the trace of image tampering.
In this paper anti-forensics method for digital image compression has been
proposed. This anti-forensics method is capable of removing the traces of image
compression. Additionally, technique is also able to remove the traces of
blocking artifact that are left by image compression algorithms that divide an
image into segments during compression process. This method is targeted to
remove the compression fingerprints of JPEG compression."
"The paper proposes StegTorrent a new network steganographic method for the
popular P2P file transfer service-BitTorrent. It is based on modifying the
order of data packets in the peer-peer data exchange protocol. Unlike other
existing steganographic methods that modify the packets' order it does not
require any synchronization. Experimental results acquired from prototype
implementation proved that it provides high steganographic bandwidth of up to
270 b/s while introducing little transmission distortion and providing
difficult detectability."
"Multimedia documents such as images, sounds or videos can be used to elicit
emotional responses in exposed human subjects. These stimuli are stored in
affective multimedia databases and successfully used for a wide variety of
research in affective computing, human-computer interaction and cognitive
sciences. Affective multimedia databases are simple repositories of multimedia
documents with annotated high-level semantics and affective content. Although
important all affective multimedia databases have numerous deficiencies which
impair their applicability. To establish a better understanding of how experts
use affective multimedia databases an online survey was conducted into the
subject. The survey results are statistically significant and indicate that
contemporary databases lack stimuli with rich semantic and emotional content.
73.33% of survey participants find the databases lacking at least some
important semantic or emotion content. Most of the participants consider
stimuli descriptions to be inadequate. Overall, 1-2h or more than 24h are
generally needed to construct a single stimulation sequence. Almost 84% of the
survey participants would like to use real-life videos in their research.
Experts unequivocally recognize the need for an intelligent stimuli retrieval
application that would assist them in experimentation. Almost all experts agree
such applications could be useful in their work."
"Digital watermarking is a technique of information adding or information
hiding in order to identify the owner of the data in multimedia content. It
seems that a signal or digital image can permanently embed over another digital
data providing a good way to protect intellectual property from illegal
replication. The cover data that is transmitted through the internet hides the
watermark in a computer aided assertion method such that it becomes
undetectable. Finally it stands as a hindrance over many operations without
harming the embedded host document. Unfortunately, many owners of the digital
materials such as images, text, audio and video are reluctant to the spreading
of their documents on the web or other networked environment, because the ease
of duplicating digital materials facilitates copyright violation. Digital media
distribution occurs through various channels. The cover data may or may not
hold any relation with the watermark information. In the last two decades, a
considerable amount of research has been done on the digital watermarking of
multimedia files such as audio, video, images and text. Different type of
watermarking algorithms has been proposed by the researchers to achieve high
level of security and authenticity. In our proposed method, a modified
reversible watermarking technique is introduced, which employs a blueprint
generation of original image based on odd-even embedding methodology to yield
large data hiding capacity, security as well as high watermarked quality. The
experimental results demonstrate that, no matter how much secret data is
embedded, the watermarked quality is about 51dB in this proposed scheme."
"This paper is to create a practical steganographic implementation to hide
color image (stego) inside another color image (cover). The proposed technique
uses Five Modulus Method to convert the whole pixels within both the cover and
the stego images into multiples of five. Since each pixels inside the stego
image is divisible by five then the whole stego image could be divided by five
to get new range of pixels 0..51. Basically, the reminder of each number that
is not divisible by five is either 1,2,3 or 4 when divided by 5. Subsequently,
then a 4-by-4 window size has been implemented to accommodate the proposed
technique. For each 4-by-4 window inside the cover image, a number from 1 to 4
could be embedded secretly from the stego image. The previous discussion must
be applied separately for each of the R, G, and B arrays. Moreover, a stego-key
could be combined with the proposed algorithm to make it difficult for any
adversary to extract the secret image from the cover image. Based on the PSNR
value, the extracted stego image has high PSNR value. Hence this new
steganography algorithm is very efficient to hide color images."
"With exponential increase in the volumes of video traffic in cellular
net-works, there is an increasing need for optimizing the quality of video
delivery. 4G networks (Long Term Evolution Advanced or LTE A) are being
introduced in many countries worldwide, which allow a downlink speed of upto 1
Gbps and uplink of 100 Mbps over a single base station. This makes a strong
push towards video broadcasting over LTE networks, characterizing its
performance and developing metrics which can be deployed to provide user
feedback of video quality and feed-back them to network operators to fine tune
the network. In this paper, we characterize the performance of video
transmission over LTE A physical layer using popular video quality metrics such
as SSIM, Blocking, Blurring, NIQE and BRISQUE. We conduct experiments to find a
suitable no-reference metrics for mobile scenario and find that Blocking
Metrics is most promising in case of channel or modulation variations but it
does not perform well to quantize variations in compression ratios. The metrics
BRISQUE is very efficient in quantizing this distortion and performs well in
case of network variations also."
"The secure transmission of speech information is a significant issue faced by
many security professionals and individuals. By applying voice-encryption
technique any kind of encrypted sensitive speech data such as password can be
transmitted. But this has the serious disadvantage that by means of
cryptanalysis attack encrypted data can be compromised. Increasing the strength
of encryption/decryption results in an associated increased in the cost.
Additional techniques like stenography and digital watermarking can be used to
conceal information in an undetectable way in audio data. However this
watermarked audio data has to be send through unreliable media and an
eavesdropper might get hold of secret message and can also determine the
identity of a speaker who is sending the information since human voice contains
information based on its characteristics such as frequency, pitch, and energy.
This paper proposes Normalized Speech Watermarking technique. Speech signal is
normalized to hide the identity of the speaker who is sending the information
and then speech watermarking technique is applied on this normalized signal
that contains the message (password) so that what information is transmitted
should not be unauthorizedly revealed."
"Internet is one of the most valuable resources for information communication
and retrievals. Most multimedia signals today are in digital formats. The
digital data can be duplicated and edited with great ease which has led to a
need for data integrity and protection of digital data. The security
requirements such as integrity or data authentication can be met by
implementing security measures using digital watermarking techniques. In this
paper a blind speech watermarking algorithm that embeds the watermark signal
data in the musical (sequence) host signal by using frequency masking is used.
A different logarithmic approach is proposed. In this regard a logarithmic
function is first applied to watermark data. Then the transformed signal is
embedded to the converted version of host signal which is obtained by applying
Fast Fourier transform method. Finally using inverse Fast Fourier Transform and
antilogarithmic function watermark signal is retrieved."
"With exponential increase in the volumes of video traffic in cellular
net-works, there is an increasing need for optimizing the quality of video
delivery. 4G networks (Long Term Evolution Advanced or LTE A) are being
introduced in many countries worldwide, which allow a downlink speed of upto 1
Gbps and uplink of 100 Mbps over a single base station. In this paper, we
characterize the performance of LTE A physical layer in terms of transmitted
video quality when the channel condi-tions and LTE settings are varied. We test
the performance achieved as the channel quality is changed and HARQ features
are enabled in physical layer. Blocking and blurring metrics were used to model
image quality."
"The paper presents a non-uniform quantization method for the Detail
components in the JPEG2000 standard. Incorporating the fact that the
coefficients lying towards the ends of the histogram plot of each Detail
component represent the structural information of an image, the quantization
step sizes become smaller at they approach the ends of the histogram plot. The
variable quantization step sizes are determined by the actual statistics of the
wavelet coefficients. Mean and standard deviation are the two statistical
parameters used iteratively to obtain the variable step sizes. Moreover, the
mean of the coefficients lying within the step size is chosen as the quantized
value, contrary to the deadzone uniform quantizer which selects the midpoint of
the quantization step size as the quantized value. The experimental results of
the deadzone uniform quantizer and the proposed non-uniform quantizer are
objectively compared by using Mean-Squared Error (MSE) and Mean Structural
Similarity Index Measure (MSSIM), to evaluate the quantization error and
reconstructed image quality, respectively. Subjective analysis of the
reconstructed images is also carried out. Through the objective and subjective
assessments, it is shown that the non-uniform quantizer performs better than
the deadzone uniform quantizer in the perceptual quality of the reconstructed
image, especially at low bitrates. More importantly, unlike the deadzone
uniform quantizer, the non-uniform quantizer accomplishes better visual quality
with a few quantized values."
"We propose a novel method for image representation in quantum computers,
which uses the two-dimensional (2-D) quantum states to locate each pixel in an
image through row-location and column-location vectors for identifying each
pixel location. The quantum state of an image is the linear superposition of
the tensor product of the m-qubits row-location vector and the n-qubits
column-location vector of each pixel. It enables the natural quantum
representation of rectangular images that other methods lack. The
amplitude/intensity of each pixel is incorporated into the coefficient values
of the pixel's quantum state, without using any qubits. Due to the fact that
linear superposition, tensor product and qubits form the fundamental basis of
quantum computing, the proposed method presents the machine level
representation of images on quantum computers. Unlike other methods, this
method is a pure quantum representation without any classical components."
"Watermarking helps in ensuring originality, ownership and copyrights of a
digital image. This paper aims at embedding a Watermark in an image using Wave
Atom Transform. Preference of Wave Atoms on other transformations has been due
to its sparser expansion, adaptability to the direction of local pattern, and
sharp frequency localization. In this scheme, we had tried to spread the
watermark in an image so that the information at one place is very small and
undetectable. In order to extract the watermark and verify ownership of an
image, one would have the advantage of prior knowledge of embedded locations. A
noise of high amplitude will be needed to be added to the image for watermark
distortion. Furthermore, the information spread will ensure the robustness of
the watermark data. The proposed scheme has the ability to withstand malicious
operations and attacks."
"In this paper, we propose a reversible data hiding method in the spatial
domain for compressed grayscale images. The proposed method embeds secret bits
into a compressed thumbnail of the original image by using a novel
interpolation method and the Neighbour Mean Interpolation (NMI) technique as
scaling up to the original image occurs. Experimental results presented in this
paper show that the proposed method has significantly improved embedding
capacities over the approach proposed by Jung and Yoo."
"This paper presents a brief review of some existing correlation models which
attempt to map Quality of Service (QoS) to Quality of Experience (QoE) for
multimedia services. The term QoS refers to deterministic network behaviour, so
that data can be transported with a minimum of packet loss, delay and maximum
bandwidth. QoE is a subjective measure that involves human dimensions; it ties
together user perception, expectations, and experience of the application and
network performance. The Holy Grail of subjective measurement is to predict it
from the objective measurements; in other words predict QoE from a given set of
QoS parameters or vice versa. Whilst there are many quality models for
multimedia, most of them are only partial solutions to predicting QoE from a
given QoS. This contribution analyses a number of previous attempts and
optimisation techniquesthat can reliably compute the weighting coefficients for
the QoS/QoE mapping."
"As computer systems become more pervasive and complex, security is
increasingly important. Secure Transmission refers to the transfer of data such
as confidential or proprietary information over a secure channel. Many secure
transmission methods require a type of encryption. Secure transmissions are put
in place to prevent attacks such as ARP spoofing and general data loss. Hence,
in order to provide a better security mechanism, in this paper we propose
Enhanced Tiny Encryption Algorithm with Embedding (ETEA), a data hiding
technique called steganography along with the technique of encryption
(Cryptography). The advantage of ETEA is that it incorporates cryptography and
steganography. The advantage proposed algorithm is that it hides the messages."
"In computer vision, video segmentation and tracking is an important
challenging issue. In this paper, we describe a new video sequences
segmentation and tracking algorithm based on MAS ""multi-agent systems"" and SURF
""Speeded Up Robust Features"". Our approach consists in modelling a multi-agent
system for segmenting the first image from a video sequence and tracking
objects in the video sequences. The used agents are supervisor and explorator
agents, they are communicating between them and they inspire in their behavior
from active contours approaches. The tracking of objects is based on SURF
descriptors ""Speed Up Robust Features"". We used the DIMA platform and ""API
Ateji PX"" (an extension of the Java language to facilitate parallel programming
on heterogeneous architectures) to implement this algorithm. The experimental
results indicate that the proposed algorithm is more robust and faster than
previous approaches."
"Watermarking is a technique which consists in introducing a brand, the name
or the logo of the author, in an image in order to protect it against illegal
copy. The capacity of the existing watermark channel is often limited. We
propose in this paper a new robust method which consists in adding the
triangular matrix of the mark obtained after the Schur decomposition to the DCT
transform of the host image. The unitary matrix acts as secret key for the
extraction of the mark. Unlike most watermarking algorithms, the host image and
the mark have the same size. The results show that our method is robust against
attack techniques as : JPEG compression, colors reducing, adding noise,
filtering, cropping, low rotations, and histogram spreading."
"In this paper, we present an overview of a multimodal system to indexing and
searching video sequence by the content that has been developed within the
REGIMVid project. A large part of our system has been developed as part of
TRECVideo evaluation. The MAVSIR platform provides High-level feature
extraction from audio-visual content and concept/event-based video retrieval.
We illustrate the architecture of the system as well as provide an overview of
the descriptors supported to date. Then we demonstrate the usefulness of the
toolbox in the context of feature extraction, concepts/events learning and
retrieval in large collections of video surveillance dataset. The results are
encouraging as we are able to get good results on several event categories,
while for all events we have gained valuable insights and experience."
"A grid computing system is designed for solving complicated scientific and
commercial problems effectively,whereas mobile computing is a traditional
distributed system having computing capability with mobility and adopting
wireless communications. Media and Entertainment fields can take advantage from
both paradigms by applying its usage in gaming applications and multimedia data
management. Multimedia data has to be stored and retrieved in an efficient and
effective manner to put it in use. In this paper, we proposed an application
layer protocol for delivery of multimedia data in wireless girds i.e.
multimedia grid protocol (MMGP). To make streaming efficient a new video
compression algorithm called dWave is designed and embedded in the proposed
protocol. This protocol will provide faster, reliable access and render an
imperceptible QoS in delivering multimedia in wireless grid environment and
tackles the challenging issues such as i) intermittent connectivity, ii) device
heterogeneity, iii) weak security and iv) device mobility."
"In order to improve the data hiding in all types of multimedia data formats
such as image and audio and to make hidden message imperceptible, a novel
method for steganography is introduced in this paper. It is based on Least
Significant Bit (LSB) manipulation and inclusion of redundant noise as secret
key in the message. This method is applied to data hiding in images. For data
hiding in audio, Discrete Cosine Transform (DCT) and Discrete Wavelet Transform
(DWT) both are used. All the results displayed prove to be time-efficient and
effective. Also the algorithm is tested for various numbers of bits. For those
values of bits, Mean Square Error (MSE) and Peak-Signal-to-Noise-Ratio (PSNR)
are calculated and plotted. Experimental results show that the stego-image is
visually indistinguishable from the original cover-image when n<=4, because of
better PSNR which is achieved by this technique. The final results obtained
after steganography process does not reveal presence of any hidden message,
thus qualifying the criteria of imperceptible message."
"Multimedia streaming to mobile devices is challenging for two reasons. First,
the way content is delivered to a client must ensure that the user does not
experience a long initial playback delay or a distorted playback in the middle
of a streaming session. Second, multimedia streaming applications are among the
most energy hungry applications in smartphones. The energy consumption mostly
depends on the delivery techniques and on the power management techniques of
wireless access technologies (Wi-Fi, 3G, and 4G). In order to provide insights
on what kind of streaming techniques exist, how they work on different mobile
platforms, their efforts in providing smooth quality of experience, and their
impact on energy consumption of mobile phones, we did a large set of active
measurements with several smartphones having both Wi-Fi and cellular network
access. Our analysis reveals five different techniques to deliver the content
to the video players. The selection of a technique depends on the mobile
platform, device, player, quality, and service. The results from our traffic
and power measurements allow us to conclude that none of the identified
techniques is optimal because they take none of the following facts into
account: access technology used, user behavior, and user preferences concerning
data waste. We point out the technique with optimal playback buffer
configuration, which provides the most attractive trade-offs in particular
situations."
"Online video-on-demand(VoD) services invariably maintain a view count for
each video they serve, and it has become an important currency for various
stakeholders, from viewers, to content owners, advertizers, and the online
service providers themselves. There is often significant financial incentive to
use a robot (or a botnet) to artificially create fake views. How can we detect
the fake views? Can we detect them (and stop them) using online algorithms as
they occur? What is the extent of fake views with current VoD service
providers? These are the questions we study in the paper. We develop some
algorithms and show that they are quite effective for this problem."
"IEEE specifies different modulation techniques for WiMAX; namely, BPSK, QPSK,
16 QAM and 64 QAM. This paper studies the performance of Internet Protocol
Television (IPTV) over Fixed WiMAX system considering different combinations of
digital modulation. The performance is studied taking into account a number of
key system parameters which include the variation in the video coding,
path-loss, scheduling service classes different rated codes in FEC channel
coding. The performance study was conducted using OPNET simulation. The
performance is studied in terms of packet lost, packet jitter delay, end-to-end
delay, and network throughput. Simulation results show that higher order
modulation and coding schemes (namely, 16 QAM and 64 QAM) yield better
performance than that of QPSK."
"Although many of the information processing systems are text-based, much of
the information in the real life is generally multimedia objects, so there is a
need to define and standardize the frame works for multimedia-based information
processing systems. In this paper we consider the application of such a system
namely pervasive image computation system, in which the user uses the cellphone
for taking the picture of the objects, and he wants to get some information
about them. We have implemented two architectures, the first one, called online
architecture, which the user sends the picture to the server and server sends
the picture information directly back to him. In the second one, which is
called offline architecture, the user uploads the image in one public image
database such as Flickr and sends the ID of the image in this database to the
server. The server processes the image and adds the information of the image in
the database, and finally the user can connect to the database and download the
image information. The implementation results show that these architectures are
very flexible and could be easily extended to be used in more complicated
pervasive multimedia systems."
"Software watermarking involves embedding a unique identifier or,
equivalently, a watermark value within a software to prove owner's authenticity
and thus to prevent or discourage copyright infringement. Towards the embedding
process, several graph theoretic watermarking algorithmic techniques encode the
watermark values as graph structures and embed them in application programs.
Recently, we presented an efficient codec system for encoding a watermark
number $w$ as a reducible permutation graph $F[\pi^*]$ through the use of
self-inverting permutations $\pi^*$. In this paper, we propose a dynamic
watermarking model, which we call WaterRPG, for embedding the watermark graph
$F[\pi^*]$ into an application program $P$. The main idea behind the proposed
watermarking model is a systematic use of appropriate calls of specific
functions of the program $P$. More precisely, for a specific input $I_{key}$ of
the program $P$, our model takes the dynamic call-graph $G(P, I_{key})$ of $P$
and the watermark graph $F[\pi^*]$, and produces the watermarked program $P^*$
having the following key property: its dynamic call-graph $G(P^*, I_{key})$ is
isomorphic to the watermark graph $F[\pi^*]$. Within this idea the program
$P^*$ is produced by only altering appropriate calls of specific functions of
the input application program $P$. We have implemented our watermarking model
WaterRPG in real application programs and evaluated its functionality under
various and broadly used watermarking assessment criteria. The evaluation
results show that our model efficiently watermarks Java application programs
with respect to several watermarking metrics like data-rate, bytecode
instructions overhead, resiliency, time and space efficiency. Moreover, the
embedded watermarks withstand several software obfuscation and optimization
attacks."
"This paper presents a novel strategy for high-fidelity image restoration by
characterizing both local smoothness and nonlocal self-similarity of natural
images in a unified statistical manner. The main contributions are three-folds.
First, from the perspective of image statistics, a joint statistical modeling
(JSM) in an adaptive hybrid space-transform domain is established, which offers
a powerful mechanism of combining local smoothness and nonlocal self-similarity
simultaneously to ensure a more reliable and robust estimation. Second, a new
form of minimization functional for solving image inverse problem is formulated
using JSM under regularization-based framework. Finally, in order to make JSM
tractable and robust, a new Split-Bregman based algorithm is developed to
efficiently solve the above severely underdetermined inverse problem associated
with theoretical proof of convergence. Extensive experiments on image
inpainting, image deblurring and mixed Gaussian plus salt-and-pepper noise
removal applications verify the effectiveness of the proposed algorithm."
"Steganalysis means analysis of stego images. Like cryptanalysis, steganalysis
is used to detect messages often encrypted using secret key from stego images
produced by steganography techniques. Recently lots of new and improved
steganography techniques are developed and proposed by researchers which
require robust steganalysis techniques to detect the stego images having
minimum false alarm rate. This paper discusses about the different Steganalysis
techniques and help to understand how, where and when this techniques can be
used based on different situations."
"While low-level image features have proven to be effective representations
for visual recognition tasks such as object recognition and scene
classification, they are inadequate to capture complex semantic meaning
required to solve high-level visual tasks such as multimedia event detection
and recognition. Recognition or retrieval of events and activities can be
improved if specific discriminative objects are detected in a video sequence.
In this paper, we propose an image representation, called Detection Bank, based
on the detection images from a large number of windowed object detectors where
an image is represented by different statistics derived from these detections.
This representation is extended to video by aggregating the key frame level
image representations through mean and max pooling. We empirically show that it
captures complementary information to state-of-the-art representations such as
Spatial Pyramid Matching and Object Bank. These descriptors combined with our
Detection Bank representation significantly outperforms any of the
representations alone on TRECVID MED 2011 data."
"With the continuous growth in the consumer markets of mobile smartphones and
increasingly in augmented reality wearable devices, several avenues of research
investigate the relationships between the quality perceived by mobile users and
the delivery mechanisms at play to support a high quality of experience for
mobile users. In this paper, we present the first study that evaluates the
relationships of mobile movie quality and the viewer-perceived quality thereof
in an augmented reality setting with see-through devices. We find that
participants tend to overestimate the video quality and exhibit a significant
variation of accuracy that leans onto the movie content and its dynamics. Our
findings, thus, can broadly impact future media adaptation and delivery
mechanisms for this new display format of mobile multimedia."
"Network steganography encompasses the information hiding techniques that can
be applied in communication network environments and that utilize hidden data
carriers for this purpose. In this paper we introduce a characteristic called
steganographic cost which is an indicator for the degradation or distortion of
the carrier caused by the application of the steganographic method. Based on
exemplary cases for single- and multi-method steganographic cost analyses we
observe that it can be an important characteristic that allows to express
hidden data carrier degradation - similarly as MSE (Mean-Square Error) or PSNR
(Peak Signal-to-Noise Ratio) are utilized for digital media steganography.
Steganographic cost can moreover be helpful to analyse the relationships
between two or more steganographic methods applied to the same hidden data
carrier."
"In this contribution, we will discuss a prototype that allows a group of
users to design sound collaboratively in real time using a multi-touch
tabletop. We make use of a machine learning method to generate a mapping from
perceptual audio features to synthesis parameters. This mapping is then used
for visualization and interaction. Finally, we discuss the results of a
comparative evaluation study."
"Linear predictive coders form an important class of speech coders. This paper
describes the software level implementation of linear prediction based
vocoders, viz. Code Excited Linear Prediction (CELP), Low-Delay CELP (LD-CELP)
and Mixed Excitation Linear Prediction (MELP) at bit rates of 4.8 kb/s, 16 kb/s
and 2.4 kb/s respectively. The C programs of the vocoders have been compiled
and executed in Linux platform. Subjective testing with the help of Mean
Opinion Score test has been performed. Waveform analysis has been done using
Praat and Adobe Audition software. The results show that MELP and CELP produce
comparable quality while the quality of LD-CELP coder is much higher, at the
expense of higher bit rate."
"With the increasing demand for image-based applications, the efficient and
reliable evaluation of image quality has increased in importance. Measuring the
image quality is of fundamental importance for numerous image processing
applications, where the goal of image quality assessment (IQA) methods is to
automatically evaluate the quality of images in agreement with human quality
judgments. Numerous IQA methods have been proposed over the past years to
fulfill this goal. In this paper, a survey of the quality assessment methods
for conventional image signals, as well as the newly emerged ones, which
includes the high dynamic range (HDR) and 3-D images, is presented. A
comprehensive explanation of the subjective and objective IQA and their
classification is provided. Six widely used subjective quality datasets, and
performance measures are reviewed. Emphasis is given to the full-reference
image quality assessment (FR-IQA) methods, and 9 often-used quality measures
(including mean squared error (MSE), structural similarity index (SSIM),
multi-scale structural similarity index (MS-SSIM), visual information fidelity
(VIF), most apparent distortion (MAD), feature similarity measure (FSIM),
feature similarity measure for color images (FSIMC), dynamic range independent
measure (DRIM), and tone-mapped images quality index (TMQI)) are carefully
described, and their performance and computation time on four subjective
quality datasets are evaluated. Furthermore, a brief introduction to 3-D IQA is
provided and the issues related to this area of research are reviewed."
"This paper presents an immersive application where users receive sound and
visual feedbacks on their interactions with a virtual environment. In this
application, the users play the part of conductors of an orchestra of factory
machines since each of their actions on interaction devices triggers a pair of
visual and audio responses. Audio stimuli were spatialized around the listener.
The application was exhibited during the 2013 Science and Music day and
designed to be used in a large immersive system with head tracking, shutter
glasses and a 10.2 loudspeaker configuration."
"This paper proposes a robust watermarking approach based on Discrete Cosine
Transform domain that combines Quick Response Code and chaotic system."
"The availability of high definition video content on the web has brought
about a significant change in the characteristics of Internet video, but not
many studies on characterizing video have been done after this change. Video
characteristics such as video length, format, target bit rate, and resolution
provide valuable input to design Adaptive Bit Rate (ABR) algorithms, sizing
playout buffers in Dynamic Adaptive HTTP streaming (DASH) players, model the
variability in video frame sizes, etc. This paper presents datasets collected
in 2013 and 2014 that contains over 130,000 videos from YouTube's most viewed
(or most popular) video charts in 58 countries. We describe the basic
characteristics of the videos on YouTube for each category, format, video
length, file size, and data rate variation, observing that video length and
file size fit a log normal distribution. We show that three minutes of a video
suffice to represent its instant data rate fluctuation and that we can infer
data rate characteristics of different video resolutions from a single given
one. Based on our findings, we design active measurements for measuring the
performance of Internet video."
"The aim of Harmonic Broadcasting protocol is to reduce the bandwidth usage in
video-on-demand service where a video is divided into some equal sized segments
and every segment is repeatedly transmitted over a number of channels that
follows harmonic series for channel bandwidth assignment. As the bandwidth of
channels differs from each other and users can join at any time to these
multicast channels, they may experience a synchronization problem between
download and playback. To deal with this issue, some schemes have been
proposed, however, at the cost of additional or wastage of bandwidth or sudden
extreme bandwidth requirement. In this paper we present an adaptive quasi
harmonic broadcasting scheme (AQHB) which delivers all data segment on time
that is the download and playback synchronization problem is eliminated while
keeping the bandwidth consumption as same as traditional harmonic broadcasting
scheme without cost of any additional or wastage of bandwidth. It also ensures
the video server not to increase the channel bandwidth suddenly that is, also
eliminates the sudden buffer requirement at the client side. We present several
analytical results to exhibit the efficiency of our proposed broadcasting
scheme over the existing ones."
"Popular videos are often clicked by a mount of users in a short period. With
content recommendation, the popular contents could be broadcast to the
potential users in wireless network, to save huge transmitting resource. In
this paper, the contents propagation model is analyzed due to users' historical
behavior, location, and the converging properties in wireless data
transmission, with the users' communication log in the Chinese commercial
cellular network. And a recommendation scheme is proposed to achieve high
energy efficiency."
"In this paper a novel approach to hide sound files in a digital image is
proposed and implemented such that it becomes difficult to conclude about the
existence of the hidden data inside the image. In this approach, we utilize the
rightmost k-LSB of pixels in an image to embed MP3 sound bits into a pixel. The
pixels are so chosen that the distortion in image would be minimized due to
embedding. This requires comparing all the possible permutations of pixel
values, which may would lead to exponential time computation. To speed up this,
Cuckoo Search (CS) could be used to find the most optimal solution. The
advantage of using proposed CS is that it is easy to implement and is very
effective at converging in relatively less iterations/generations."
"Steganalysis tools play an important part in saving time and providing new
angles of attack for forensic analysts. StegExpose is a solution designed for
use in the real world, and is able to analyse images for LSB steganography in
bulk using proven attacks in a time efficient manner. When steganalytic methods
are combined intelligently, they are able generate even more accurate results.
This is the prime focus of StegExpose."
"By offering sophisticated services and centralizing a huge volume of personal
data, modern smartphones changed the way we socialize, entertain and work. To
this aim, they rely upon complex hardware/software frameworks leading to a
number of vulnerabilities, attacks and hazards to profile individuals or gather
sensitive information. However, the majority of works evaluating the security
degree of smartphones neglects steganography, which can be mainly used to: i)
exfiltrate confidential data via camouflage methods, and ii) conceal valuable
or personal information into innocent looking carriers.
  Therefore, this paper surveys the state of the art of steganographic
techniques for smartphones, with emphasis on methods developed over the period
2005 to the second quarter of 2014. The different approaches are grouped
according to the portion of the device used to hide information, leading to
three different covert channels, i.e., local, object and network. Also, it
reviews the relevant approaches used to detect and mitigate steganographic
attacks or threats. Lastly, it showcases the most popular software applications
to embed secret data into carriers, as well as possible future directions."
"Generic matrix multiplication (GEMM) and one-dimensional
convolution/cross-correlation (CONV) kernels often constitute the bulk of the
compute- and memory-intensive processing within image/audio recognition and
matching systems. We propose a novel method to scale the energy and processing
throughput of GEMM and CONV kernels for such error-tolerant multimedia
applications by adjusting the precision of computation. Our technique employs
linear projections to the input matrix or signal data during the top-level GEMM
and CONV blocking and reordering. The GEMM and CONV kernel processing then uses
the projected inputs and the results are accumulated to form the final outputs.
Throughput and energy scaling takes place by changing the number of projections
computed by each kernel, which in turn produces approximate results, i.e.
changes the precision of the performed computation. Results derived from a
voltage- and frequency-scaled ARM Cortex A15 processor running face recognition
and music matching algorithms demonstrate that the proposed approach allows for
280%~440% increase of processing throughput and 75%~80% decrease of energy
consumption against optimized GEMM and CONV kernels without any impact in the
obtained recognition or matching accuracy. Even higher gains can be obtained if
one is willing to tolerate some reduction in the accuracy of the recognition
and matching applications."
"Good quality video services always require higher bandwidth. Hence, to
provide the video services e.g., multicast/broadcast services (MBS) and unicast
services along with the existing voice, internet, and other background traffic
services over the wireless cellular networks, it is required to efficiently
manage the wireless resources in order to reduce the overall forced call
termination probability, to maximize the overall service quality, and to
maximize the revenue. Fixed bandwidth allocation for the MBS sessions either
reduces the quality of the MBS videos and bandwidth utilization or increases
the overall forced call termination probability and of course the handover call
dropping probability as well. Scalable Video Coding (SVC) technique allows the
variable bit rate allocation for the video services. In this paper, we propose
a bandwidth allocation scheme that efficiently allocates bandwidth among the
MBS sessions and the non-MBS traffic calls (e.g., voice, unicast, internet, and
other background traffic). The proposed scheme reduces the bandwidth allocation
for the MBS sessions during the congested traffic condition only to accommodate
more calls in the system. Instead of allocating fixed bandwidths for the BMS
sessions and the non-MBS traffic, our scheme allocates variable bandwidths for
them. However, the minimum quality of the videos is guaranteed by allocating
minimum bandwidth for them. Using the mathematical and numerical analyses, we
show that the proposed scheme maximizes the bandwidth utilization and
significantly reduces the overall forced call termination probability as well
as the handover call dropping probability."
"Provisioning of Quality of Service (QoS) is a key issue in any multi-media
system. However, in wireless systems, supporting QoS requirements of different
traffic types is more challenging due to the need to minimize two performance
metrics - the probability of dropping a handover call and the probability of
blocking a new call. Since QoS requirements are not as stringent for
non-real-time traffic types, as opposed to real-time traffic, more calls can be
accommodated by releasing some bandwidth from the already admitted
non-real-time traffic calls. If we require that such a released bandwidth to
accept a handover call ought to be larger than the bandwidth to accept a new
call, then the resulting probability of dropping a handover call will be
smaller than the probability of blocking a new call. In this paper we propose
an efficient Call Admission Control (CAC) that relies on adaptive multi-level
bandwidth-allocation scheme for non-real-time calls. The scheme allows
reduction of the call dropping probability along with increase of the bandwidth
utilization. The numerical results show that the proposed scheme is capable of
attaining negligible handover call dropping probability without sacrificing
bandwidth utilization."
"In this paper, we propose a novel fragile block based medical image
watermarking technique for embedding data of patient into medical image,
verifying the integrity of ROI (Region of Interest), detecting the tampered
blocks inside ROI and recovering original ROI with less size authentication and
recovery data and with simple mathematical calculations. In the proposed
method, the medical image is divided into three regions called ROI, RONI
(Region of Non Interest) and border pixels. Later, authentication data of ROI
and Electronic Patient Record (EPR) are compressed using Run Length Encoding
(RLE) technique and then embedded into ROI. Recovery information of ROI is
embedded inside RONI and information of ROI is embedded inside border pixels.
Results of experiments conducted on several medical images reveal that proposed
method produces high quality watermarked medical images, identifies tampered
areas inside ROI of watermarked medical images and recovers the original ROI."
"With the fast development of communication and multimedia technology, the
rights of the owners of multimedia products is vulnerable to the unauthorized
copies and watermarking is one of the best known methods for proving the
ownership of a product. In this paper we prosper the previous watermarking
method which was based on Tabu search by Chaos. The modification applied in the
permutation step of watermarking and the initial population generation of the
Tabu search. We analyze our method on some well known images and experimental
results shows the improvement in the quality and speed of the proposed
watermarking method."
"This paper investigates the use of Structural Similaritys (SSIM) index on the
minimized side effect to image watermarking. For fast implementation and more
compatibility with the standard DCT based codecs, watermark insertion is
carried out on the DCT coefficients and hence a SSIM model for DCT based
watermarking is developed. For faster implementation, the SSIM index is
maximized over independent 4x4 non-overlapped blocks but the disparity between
the adjacent blocks reduces the overall image quality. This problem is resolved
through optimization of overlapped blocks, but, the higher image quality is
achieved at a cost of high computational complexity. To reduce the
computational complexity while preserving the good quality, optimization of
semi-overlapped blocks is introduced. We show that while SSIM-based
optimization over overlapped blocks has as high as 64 times the complexity of
the 4x4 non-overlapped method, with semi-overlapped optimization the high
quality of overlapped method is preserved only at a cost of less than 8 times
the non-overlapped method."
"Vulnerability of watermarking schemes against intense signal processing
attacks is generally a major concern, particularly when there are techniques to
reproduce an acceptable copy of the original signal with no chance for
detecting the watermark. In this paper, we propose a two-layer, data
partitioning (DP) based, image in image watermarking method in the DCT domain
to improve the watermark detection performance. Truncated singular value
decomposition, binary wavelet decomposition and spatial scalability idea in
H.264/SVC are analyzed and employed as partitioning methods. It is shown that
the proposed scheme outperforms its two recent competitors in terms of both
data payload and robustness to intense attacks."
"Empowered by today's rich tools for media generation and distribution, and
the convenient Internet access, crowdsourced streaming generalizes the
single-source streaming paradigm by including massive contributors for a video
channel. It calls a joint optimization along the path from crowdsourcers,
through streaming servers, to the end-users to minimize the overall latency.
The dynamics of the video sources, together with the globalized request demands
and the high computation demand from each sourcer, make crowdsourced live
streaming challenging even with powerful support from modern cloud computing.
In this paper, we present a generic framework that facilitates a cost-effective
cloud service for crowdsourced live streaming. Through adaptively leasing, the
cloud servers can be provisioned in a fine granularity to accommodate
geo-distributed video crowdsourcers. We present an optimal solution to deal
with service migration among cloud instances of diverse lease prices. It also
addresses the location impact to the streaming quality. To understand the
performance of the proposed strategies in the realworld, we have built a
prototype system running over the planetlab and the Amazon/Microsoft Cloud. Our
extensive experiments demonstrate that the effectiveness of our solution in
terms of deployment cost and streaming quality."
"Information Security is a major concern in today's modern era. Almost all the
communicating bodies want the security, confidentiality and integrity of their
personal data. But this security goal cannot be achieved easily when we are
using an open network like Internet. Steganography provides one of the best
solutions to this problem. This paper represents a new Cyclic Steganographic T
echnique (CST) based on Least Significant Bit (LSB) for true color (RGB)
images. The proposed method hides the secret data in the LSBs of cover image
pixels in a randomized cyclic manner. The proposed technique is evaluated using
both subjective and objective analysis using histograms changeability, Peak
Signal-to-Noise Ratio (PSNR) and Mean Square Error (MSE). Experimentally it is
found that the proposed method gives promising results in terms of security,
imperceptibility and robustness as compared to some existent methods and
vindicates this new algorithm."
"Distributed visual analysis applications, such as mobile visual search or
Visual Sensor Networks (VSNs) require the transmission of visual content on a
bandwidth-limited network, from a peripheral node to a processing unit.
Traditionally, a Compress-Then-Analyze approach has been pursued, in which
sensing nodes acquire and encode the pixel-level representation of the visual
content, that is subsequently transmitted to a sink node in order to be
processed. This approach might not represent the most effective solution, since
several analysis applications leverage a compact representation of the content,
thus resulting in an inefficient usage of network resources. Furthermore,
coding artifacts might significantly impact the accuracy of the visual task at
hand. To tackle such limitations, an orthogonal approach named
Analyze-Then-Compress has been proposed. According to such a paradigm, sensing
nodes are responsible for the extraction of visual features, that are encoded
and transmitted to a sink node for further processing. In spite of improved
task efficiency, such paradigm implies the central processing node not being
able to reconstruct a pixel-level representation of the visual content. In this
paper we propose an effective compromise between the two paradigms, namely
Hybrid-Analyze-Then-Compress (HATC) that aims at jointly encoding visual
content and local image features. Furthermore, we show how a target tradeoff
between image quality and task accuracy might be achieved by accurately
allocating the bitrate to either visual content or local features."
"Binary local features represent an effective alternative to real-valued
descriptors, leading to comparable results for many visual analysis tasks,
while being characterized by significantly lower computational complexity and
memory requirements. When dealing with large collections, a more compact
representation based on global features is often preferred, which can be
obtained from local features by means of, e.g., the Bag-of-Visual-Word (BoVW)
model. Several applications, including for example visual sensor networks and
mobile augmented reality, require visual features to be transmitted over a
bandwidth-limited network, thus calling for coding techniques that aim at
reducing the required bit budget, while attaining a target level of efficiency.
In this paper we investigate a coding scheme tailored to both local and global
binary features, which aims at exploiting both spatial and temporal redundancy
by means of intra- and inter-frame coding. In this respect, the proposed coding
scheme can be conveniently adopted to support the Analyze-Then-Compress (ATC)
paradigm. That is, visual features are extracted from the acquired content,
encoded at remote nodes, and finally transmitted to a central controller that
performs visual analysis. This is in contrast with the traditional approach, in
which visual content is acquired at a node, compressed and then sent to a
central unit for further processing, according to the Compress-Then-Analyze
(CTA) paradigm. In this paper we experimentally compare ATC and CTA by means of
rate-efficiency curves in the context of two different visual analysis tasks:
homography estimation and content-based retrieval. Our results show that the
novel ATC paradigm based on the proposed coding primitives can be competitive
with CTA, especially in bandwidth limited scenarios."
"In this paper, a macroblock classification method is proposed for various
video processing applications involving motions. Based on the analysis of the
Motion Vector field in the compressed video, we propose to classify Macroblocks
of each video frame into different classes and use this class information to
describe the frame content. We demonstrate that this low-computation-complexity
method can efficiently catch the characteristics of the frame. Based on the
proposed macroblock classification, we further propose algorithms for different
video processing applications, including shot change detection, motion
discontinuity detection, and outlier rejection for global motion estimation.
Experimental results demonstrate that the methods based on the proposed
approach can work effectively on these applications."
"The Digital Forgeries though not visibly identifiable to human perception it
may alter or meddle with underlying natural statistics of digital content.
Tampering involves fiddling with video content in order to cause damage or make
unauthorized alteration/modification. Tampering detection in video is
cumbersome compared to image when considering the properties of the video.
Tampering impacts need to be studied and the applied technique/method is used
to establish the factual information for legal course in judiciary. In this
paper we give an overview of the prior literature and challenges involved in
video forgery detection where passive approach is found."
"We present the Yahoo Flickr Creative Commons 100 Million Dataset (YFCC100M),
the largest public multimedia collection that has ever been released. The
dataset contains a total of 100 million media objects, of which approximately
99.2 million are photos and 0.8 million are videos, all of which carry a
Creative Commons license. Each media object in the dataset is represented by
several pieces of metadata, e.g. Flickr identifier, owner name, camera, title,
tags, geo, media source. The collection provides a comprehensive snapshot of
how photos and videos were taken, described, and shared over the years, from
the inception of Flickr in 2004 until early 2014. In this article we explain
the rationale behind its creation, as well as the implications the dataset has
for science, research, engineering, and development. We further present several
new challenges in multimedia research that can now be expanded upon with our
dataset."
"The YLI Multimedia Event Detection corpus is a public-domain index of videos
with annotations and computed features, specialized for research in multimedia
event detection (MED), i.e., automatically identifying what's happening in a
video by analyzing the audio and visual content. The videos indexed in the
YLI-MED corpus are a subset of the larger YLI feature corpus, which is being
developed by the International Computer Science Institute and Lawrence
Livermore National Laboratory based on the Yahoo Flickr Creative Commons 100
Million (YFCC100M) dataset. The videos in YLI-MED are categorized as depicting
one of ten target events, or no target event, and are annotated for additional
attributes like language spoken and whether the video has a musical score. The
annotations also include degree of annotator agreement and average annotator
confidence scores for the event categorization of each video. Version 1.0 of
YLI-MED includes 1823 ""positive"" videos that depict the target events and
48,138 ""negative"" videos, as well as 177 supplementary videos that are similar
to event videos but are not positive examples. Our goal in producing YLI-MED is
to be as open about our data and procedures as possible. This report describes
the procedures used to collect the corpus; gives detailed descriptive
statistics about the corpus makeup (and how video attributes affected
annotators' judgments); discusses possible biases in the corpus introduced by
our procedural choices and compares it with the most similar existing dataset,
TRECVID MED's HAVIC corpus; and gives an overview of our future plans for
expanding the annotation effort."
"This paper presents the preliminary of a novel scheme of steganography, and
introduces the idea of combining two secret keys in the operation. The first
secret key encrypts the text using a standard cryptographic scheme (e.g. IDEA,
SAFER+, etc.) prior to the wavelet audio decomposition. The way in which the
cipher text is embedded in the file requires another key, namely a stego-key,
which is associated with features of the audio wavelet analysis."
"This paper analyzes a revised fragile watermarking scheme proposed by Botta
et al. which was developed as a revision of the watermarking scheme previously
proposed by Rawat et al. A new attack is presented that allows an attacker to
apply a valid watermark on tampered images, therefore circumventing the
protection that the watermarking scheme under study was supposed to offer.
Furthermore, the presented attack has very low computational and memory
requirements."
"The state-of-the-art pooling strategies for perceptual image quality
assessment (IQA) are based on the mean and the weighted mean. They are robust
pooling strategies which usually provide a moderate to high performance for
different IQAs. Recently, standard deviation (SD) pooling was also proposed.
Although, this deviation pooling provides a very high performance for a few
IQAs, its performance is lower than mean poolings for many other IQAs. In this
paper, we propose to use the mean absolute deviation (MAD) and show that it is
a more robust and accurate pooling strategy for a wider range of IQAs. In fact,
MAD pooling has the advantages of both mean pooling and SD pooling. The joint
computation and use of the MAD and SD pooling strategies is also considered in
this paper. Experimental results provide useful information on the choice of
the proper deviation pooling strategy for different IQA models."
"Network steganography conceals the transfer of sensitive information within
unobtrusive data in computer networks. So-called micro protocols are
communication protocols placed within the payload of a network steganographic
transfer. They enrich this transfer with features such as reliability, dynamic
overlay routing, or performance optimization --- just to mention a few. We
present different design approaches for the embedding of hidden channels with
micro protocols in digitized audio signals under consideration of different
requirements. On the basis of experimental results, our design approaches are
compared, and introduced into a protocol engineering approach for micro
protocols."
"Because Facebook is available on hundreds of millions of desktop and mobile
computing platforms around the world and because it is available on many
different kinds of platforms (from desktops and laptops running Windows, Unix,
or OS X to hand held devices running iOS, Android, or Windows Phone), it would
seem to be the perfect place to conduct steganography. On Facebook, information
hidden in image files will be further obscured within the millions of pictures
and other images posted and transmitted daily. Facebook is known to alter and
compress uploaded images so they use minimum space and bandwidth when displayed
on Facebook pages. The compression process generally disrupts attempts to use
Facebook for image steganography. This paper explores a method to minimize the
disruption so JPEG images can be used as steganography carriers on Facebook."
"The paper presents StegBlocks, which defines a new concept for performing
undetectable hidden communication. StegBlocks is a general approach for
constructing methods of network steganography. In StegBlocks, one has to
determine objects with defined properties which will be used to transfer hidden
messages. The objects are dependent on a specific network protocol (or
application) used as a carrier for a given network steganography method.
Moreover, the paper presents the approach to perfect undetectability of network
steganography, which was developed based on the rules of undetectability for
general steganography. The approach to undetectability of network steganography
was used to show the possibility of developing perfectly undetectable network
steganography methods using the StegBlocks concept."
"This paper presents a new approach for hiding information in digital image in
spatial domain. In this approach three bits of message is embedded in a pixel
using Lucas number system but only one bit plane is allowed for alternation.
The experimental results show that the proposed method has the larger capacity
of embedding data, high peak signal to noise ratio compared to existing methods
and is hardly detectable for steganolysis algorithm."
"Service Oriented Architecture (SOA) is commonly employed in the design and
implementation of web service systems. The key technology to enable media
communications in the context of SOA is the Service Oriented Communication. To
exploit the advantage of SOA, we design and implement a web-based multimedia
conferencing system that provides users with a hybrid orchestration of web and
communication services. As the current SOA lacks effective QoS guarantee
solutions for multimedia services, the user satisfaction is greatly challenged
with QoS violations, e.g., low video PSNR (Peak Signal-to-Noise Ratio) and long
playback delay. Motivated by addressing the critical problem, we firstly employ
the Business Process Execution Language (BPEL) service engine for the hybrid
services orchestration and execution. Secondly, we propose a novel
context-aware approach to quantify and leverage the causal relationships
between QoS metrics and available contexts based on Bayesian networks (CABIN).
This approach includes three phases: (1) information discretization, (2) causal
relationship profiling, and (3) optimal context tuning. We implement CABIN in a
real-life multimedia conferencing system and compare its performance with
existing delay and throughput oriented schemes. Experimental results show that
CABIN outperforms the competing approaches in improving the video quality in
terms of PSNR. It also provides a one-stop shop controls both the web and
communication services."
"The Internet has recently witnessed the convergence of online social network
services and online video services: users import videos from content sharing
sites, and propagate them along the social connections by re-sharing them. Such
social behaviors have dramatically reshaped how videos are disseminated, and
the users are now actively engaged to be part of the social ecosystem, rather
than being passively consumers. Despite the increasingly abundant bandwidth and
computation resources, the ever increasing data volume of user generated video
content and the boundless coverage of socialized sharing have presented
unprecedented challenges. In this paper, we first presents the challenges in
social-aware video delivery. Then, we present a principal framework for
data-driven social video delivery approaches. Moreover, we identify the unique
characteristics of social-aware video access and the social content
propagation, and closely reveal the design of individual modules and their
integration towards enhancing users' experience in the social network context."
"In this paper, we present methods for image compression on the basis of
eigenvalue decomposition of normal matrices. The proposed methods are
convenient and self-explanatory, requiring fewer and easier computations as
compared to some existing methods. Through the proposed techniques, the image
is transformed to the space of normal matrices. Then, the properties of
spectral decomposition are dealt with to obtain compressed images. Experimental
results are provided to illustrate the validity of the methods."
"This paper studies the problem of mobile video delivery in heterogenous
wireless networks from a server to multihomed device. Most existing works only
consider delivering video streaming on single path which bandwidth is limited
causing ultimate video transmission rate. To solve this live video streaming
transmission bottleneck problem, we propose a novel solution named Joint Data
Allocation and Fountain Coding (JDAFC) method that contain below characters:
(1) path selection, (2) dynamic data allocation, and (3) fountain coding. We
evaluate the performance of JDAFC by simulation experiments using Exata and
JVSM and compare it with some reference solutions. Experimental results
represent that JDAFC outperforms the competing solutions in improving the video
peak signal-to-noise ratio as well as reducing the end-to-end delay."
"High user interaction capability of mobile devices can help improve the
accuracy of mobile visual search systems. At query time, it is possible to
capture multiple views of an object from different viewing angles and at
different scales with the mobile device camera to obtain richer information
about the object compared to a single view and hence return more accurate
results. Motivated by this, we developed a mobile multi-view object image
search system, using a client-server architecture. Multi-view images of objects
acquired by the mobile clients are processed and local features are sent to the
server, which combines the query image representations with early/late fusion
methods based on bag-of-visual-words and sends back the query results. We
performed a comprehensive analysis of early and late fusion approaches using
various similarity functions, on an existing single view and a new multi-view
object image database. The experimental results show that multi-view search
provides significantly better retrieval accuracy compared to single view
search."
"In this paper we study the problem of estimating snow cover in mountainous
regions, that is, the spatial extent of the earth surface covered by snow. We
argue that publicly available visual content, in the form of user generated
photographs and image feeds from outdoor webcams, can both be leveraged as
additional measurement sources, complementing existing ground, satellite and
airborne sensor data. To this end, we describe two content acquisition and
processing pipelines that are tailored to such sources, addressing the specific
challenges posed by each of them, e.g., identifying the mountain peaks,
filtering out images taken in bad weather conditions, handling varying
illumination conditions. The final outcome is summarized in a snow cover index,
which indicates for a specific mountain and day of the year, the fraction of
visible area covered by snow, possibly at different elevations. We created a
manually labelled dataset to assess the accuracy of the image snow covered area
estimation, achieving 90.0% precision at 91.1% recall. In addition, we show
that seasonal trends related to air temperature are captured by the snow cover
index."
"In this paper we propose a new method for the evaluation of network
steganography algorithms based on the new concept of ""the moving observer"". We
considered three levels of undetectability named: ""good"", ""bad"", and ""ugly"". To
illustrate this method we chose Wi-Fi steganography as a solid family of
information hiding protocols. We present the state of the art in this area
covering well-known hiding techniques for 802.11 networks. ""The moving
observer"" approach could help not only in the evaluation of steganographic
algorithms, but also might be a starting point for a new detection system of
network steganography. The concept of a new detection system, called MoveSteg,
is explained in detail."
"Visual media are powerful means of expressing emotions and sentiments. The
constant generation of new content in social networks highlights the need of
automated visual sentiment analysis tools. While Convolutional Neural Networks
(CNNs) have established a new state-of-the-art in several vision problems,
their application to the task of sentiment analysis is mostly unexplored and
there are few studies regarding how to design CNNs for this purpose. In this
work, we study the suitability of fine-tuning a CNN for visual sentiment
prediction as well as explore performance boosting techniques within this deep
learning setting. Finally, we provide a deep-dive analysis into a benchmark,
state-of-the-art network architecture to gain insight about how to design
patterns for CNNs on the task of visual sentiment prediction."
"Most state-of-the-art image retrieval and recommendation systems
predominantly focus on individual images. In contrast, socially curated image
collections, condensing distinctive yet coherent images into one set, are
largely overlooked by the research communities. In this paper, we aim to design
a novel recommendation system that can provide users with image collections
relevant to individual personal preferences and interests. To this end, two key
issues need to be addressed, i.e., image collection modeling and similarity
measurement. For image collection modeling, we consider each image collection
as a whole in a group sparse reconstruction framework and extract concise
collection descriptors given the pretrained dictionaries. We then consider
image collection recommendation as a dynamic similarity measurement problem in
response to user's clicked image set, and employ a metric learner to measure
the similarity between the image collection and the clicked image set. As there
is no previous work directly comparable to this study, we implement several
competitive baselines and related methods for comparison. The evaluations on a
large scale Pinterest data set have validated the effectiveness of our proposed
methods for modeling and recommending image collections."
"Image classification is an enthusiastic research field where large amount of
image data is classified into various classes based on their visual contents.
Researchers have presented various low-level features-based techniques for
classifying images into different categories. However, efficient and effective
classification and retrieval is still a challenging problem due to complex
nature of visual contents. In addition, the traditional information retrieval
techniques are vulnerable to security risks, making it easy for attackers to
retrieve personal visual contents such as patients records and law enforcement
agencies databases. Therefore, we propose a novel ontology-based framework
using image steganography for secure image classification and information
retrieval. The proposed framework uses domain-specific ontology for mapping the
low-level image features to high-level concepts of ontologies which
consequently results in efficient classification. Furthermore, the proposed
method utilizes image steganography for hiding the image semantics as a secret
message inside them, making the information retrieval process secure from third
parties. The proposed framework minimizes the computational complexity of
traditional techniques, increasing its suitability for secure and real-time
visual contents retrieval from personalized image databases. Experimental
results confirm the efficiency, effectiveness, and security of the proposed
framework as compared with other state-of-the-art systems."
"In this work we explain the implementation of event-driven real-time
interpreters for the Concurrent Constraint Programming (CCP) and
Non-deterministic Timed Concurrent Constraint (NTCC) for- malisms. The CCP
interpreter was tested with a program to find, concurrently, paths in a graph
and it will be used in the future to find musical sequences in the music
improvisation software Omax, developed by the French Acoustics/Music Research
Institute (IRCAM). In the other hand, the NTCC interpreter was tested with a
music improvisation system based on NTCC (CCFOMI), developed by the AVISPA
research group and IRCAM. Additionally, we present GECOL 2, a wrapper for the
Generic Constraints Development Environment (GECODE) to Common LISP, de-
veloped to port the interpreters to Common LISP in the future. We concluded
that using GECODE for the concurrency control avoids the need of having threads
and synchronizing them, leading to a simple and efficient implementation of CCP
and NTCC. We also noticed that the time units in NTCC interpreter do not
represent discrete time units, because when we simulate the NTCC specifications
in the interpreter, the time units have different durations. In the future, we
propose forcing the duration of each time unit to a fix time, that way we would
be able to reason about NTCC time units as we do with discrete time units."
"Software to design multimedia scenarios is usually based either on a fixed
timeline or on cue lists, but both models are unrelated temporally. On the
contrary, the formalism of interactive scores can describe multimedia scenarios
with flexible and fixed temporal relations among the objects of the scenario,
but cannot express neither temporal relations for micro controls nor signal
processing. We extend interactive scores with such relations and with sound
processing. We show some applications and we describe how they can be
implemented in Pure Data. Our implementation has low average relative jitter
even under high cpu load."
"Information security is one of the most challenging problems in today's
technological world. In order to secure the transmission of secret data over
the public network (Internet), various schemes have been presented over the
last decade. Steganography combined with cryptography, can be one of the best
choices for solving this problem. This paper proposes a new steganographic
method based on gray-level modification for true colour images using image
transposition, secret key and cryptography. Both the secret key and secret
information are initially encrypted using multiple encryption algorithms
(bitxor operation, bits shuffling, and stego key-based encryption); these are,
subsequently, hidden in the host image pixels. In addition, the input image is
transposed before data hiding. Image transposition, bits shuffling, bitxoring,
stego key-based encryption, and gray-level modification introduce five
different security levels to the proposed scheme, making the data recovery
extremely difficult for attackers. The proposed technique is evaluated by
objective analysis using various image quality assessment metrics, producing
promising results in terms of imperceptibility and security. Moreover, the high
quality stego images and its minimal histogram changeability, also validate the
effectiveness of the proposed approach."
"Media sharing is an extremely popular paradigm of social interaction in
online social networks (OSNs) nowadays. The scalable media access control is
essential to perform information sharing among users with various access
privileges. In this paper, we present a multi-dimensional scalable media access
control (MD-SMAC) system based on the proposed scalable ciphertext policy
attribute-based encryption (SCP-ABE) algorithm. In the proposed MD-SMAC system,
fine-grained access control can be performed on the media contents encoded in a
multi-dimensional scalable manner based on data consumers' diverse attributes.
Through security analysis, we show that the proposed MC-SMAC system is able to
resist collusion attacks. Additionally, we conduct experiments to evaluate the
efficiency performance of the proposed system, especially on mobile devices."
"Based on the notion of just noticeable differences (JND), a stair quality
function (SQF) was recently proposed to model human perception on JPEG images.
Furthermore, a k-means clustering algorithm was adopted to aggregate JND data
collected from multiple subjects to generate a single SQF. In this work, we
propose a new method to derive the SQF using the Gaussian Mixture Model (GMM).
The newly derived SQF can be interpreted as a way to characterize the mean
viewer experience. Furthermore, it has a lower information criterion (BIC)
value than the previous one, indicating that it offers a better model. A
specific example is given to demonstrate the advantages of the new approach."
"As music streaming services dominate the music industry, the playlist is
becoming an increasingly crucial element of music consumption. Con- sequently,
the music recommendation problem is often casted as a playlist generation prob-
lem. Better understanding of the playlist is there- fore necessary for
developing better playlist gen- eration algorithms. In this work, we analyse
two playlist datasets to investigate some com- monly assumed hypotheses about
playlists. Our findings indicate that deeper understanding of playlists is
needed to provide better prior infor- mation and improve machine learning
algorithms in the design of recommendation systems."
"This short paper presents a perspective plan to build a null reference image
quality assessment. Its main goal is to deliver both the objective score and
the distortion map for a given distorted image without the knowledge of its
reference image."
"The knowledge of future throughput variation in wireless networks using
smartphone becomes more and more possible by exploiting the rich contextual
information from smartphone sensors through mobile applications and services.
Contextual information may include the traffic, mobility and radio conditions.
Inspired by the attractive features and potential advantages of this agile
resource management, several approaches have been proposed during the last
period. However, agile resource management also comes with its own challenges,
and there are significant technical issues that still need to be addressed for
successful rollout and operation of this technique. In this paper, we propose
an approach for anticipating throughput variation for mobile video streaming
services. The solution of the optimization problem realizes a fundamental
trade-offs among critical metrics that impact the user's perceptual quality of
the experience (QoE) and system utilization. Both simulated and real-world
traces are carried out to evaluate the performance of the proposed approach. It
is shown that our approach provides the accuracy, efficiency and robustness
that the new 5G architectures require."
"In this paper, we present a subclass-representation approach that predicts
the probability of a social image belonging to one particular class. We explore
the co-occurrence of user-contributed tags to find subclasses with a strong
connection to the top level class. We then project each image on to the
resulting subclass space to generate a subclass representation for the image.
The novelty of the approach is that subclass representations make use of not
only the content of the photos themselves, but also information on the
co-occurrence of their tags, which determines membership in both subclasses and
top-level classes. The novelty is also that the images are classified into
smaller classes, which have a chance of being more visually stable and easier
to model. These subclasses are used as a latent space and images are
represented in this space by their probability of relatedness to all of the
subclasses. In contrast to approaches directly modeling each top-level class
based on the image content, the proposed method can exploit more information
for visually diverse classes. The approach is evaluated on a set of $2$ million
photos with 10 classes, released by the Multimedia 2013 Yahoo! Large-scale
Flickr-tag Image Classification Grand Challenge. Experiments show that the
proposed system delivers sound performance for visually diverse classes
compared with methods that directly model top classes."
"With the increasing availability of wearable devices, research on egocentric
activity recognition has received much attention recently. In this paper, we
build a Multimodal Egocentric Activity dataset which includes egocentric videos
and sensor data of 20 fine-grained and diverse activity categories. We present
a novel strategy to extract temporal trajectory-like features from sensor data.
We propose to apply the Fisher Kernel framework to fuse video and temporal
enhanced sensor features. Experiment results show that with careful design of
feature extraction and fusion algorithm, sensor data can enhance
information-rich video data. We make publicly available the Multimodal
Egocentric Activity dataset to facilitate future research."
"An approach to watermarking digital images using non-regular wavelets is
advanced. Non-regular transforms spread the energy in the transform domain. The
proposed method leads at the same time to increased image quality and increased
robustness with respect to lossy compression. The approach provides robust
watermarking by suitably creating watermarked messages that have energy
compaction and frequency spreading. Our experimental results show that the
application of non-regular wavelets, instead of regular ones, can furnish a
superior robust watermarking scheme. The generated watermarked data is more
immune against non-intentional JPEG and JPEG2000 attacks."
"We propose an image representation and matching approach that substantially
improves visual-based location estimation for images. The main novelty of the
approach, called distinctive visual element matching (DVEM), is its use of
representations that are specific to the query image whose location is being
predicted. These representations are based on visual element clouds, which
robustly capture the connection between the query and visual evidence from
candidate locations. We then maximize the influence of visual elements that are
geo-distinctive because they do not occur in images taken at many other
locations. We carry out experiments and analysis for both geo-constrained and
geo-unconstrained location estimation cases using two large-scale,
publicly-available datasets: the San Francisco Landmark dataset with $1.06$
million street-view images and the MediaEval '15 Placing Task dataset with
$5.6$ million geo-tagged images from Flickr. We present examples that
illustrate the highly-transparent mechanics of the approach, which are based on
common sense observations about the visual patterns in image collections. Our
results show that the proposed method delivers a considerable performance
improvement compared to the state of the art."
"The IETF recently standardized the Opus codec as RFC6716. Opus targets a wide
range of real-time Internet applications by combining a linear prediction coder
with a transform coder. We describe the transform coder, with particular
attention to the psychoacoustic knowledge built into the format. The result
out-performs existing audio codecs that do not operate under real-time
constraints."
"We propose an audio codec that addresses the low-delay requirements of some
applications such as network music performance. The codec is based on the
modified discrete cosine transform (MDCT) with very short frames and uses
gain-shape quantization to preserve the spectral envelope. The short frame
sizes required for low delay typically hinder the performance of transform
codecs. However, at 96 kbit/s and with only 4 ms algorithmic delay, the
proposed codec out-performs the ULD codec operating at the same rate. The total
complexity of the codec is small, at only 17 WMOPS for real-time operation at
48 kHz."
"This paper describes a technique for performing intra prediction of the
chroma planes based on the reconstructed luma plane in the frequency domain.
This prediction exploits the fact that while RGB to YUV color conversion has
the property that it decorrelates the color planes globally across an image,
there is still some correlation locally at the block level. Previous proposals
compute a linear model of the spatial relationship between the luma plane (Y)
and the two chroma planes (U and V). In codecs that use lapped transforms this
is not possible since transform support extends across the block boundaries and
thus neighboring blocks are unavailable during intra-prediction. We design a
frequency domain intra predictor for chroma that exploits the same local
correlation with lower complexity than the spatial predictor and which works
with lapped transforms. We then describe a low-complexity algorithm that
directly uses luma coefficients as a chroma predictor based on gain-shape
quantization and band partitioning. An experiment is performed that compares
these two techniques inside the experimental Daala video codec and shows the
lower complexity algorithm to be a better chroma predictor."
"With the growth of communication over computer networks, how to maintain the
confidentiality and security of transmitted information have become some of the
important issues. In order to transfer data securely to the destination without
unwanted disclosure or damage, nature inspired hide and seek tricks such as,
cryptography and Steganography are heavily in use. Just like the Chameleon and
many other bio-species those change their body color and hide themselves in the
background in order to protect them from external attacks, Cryptography and
Steganography are techniques those are used to encrypt and hide the secret data
inside other media to ensure data security. This paper discusses the concept of
a simple spatial domain LSB Steganography that encrypts the secrets using
Fibonacci- Lucas transformation, before hiding, for better security."
"Network steganography has been a well-known covert data channeling method for
over three decades. The basic set of techniques and implementation tools have
not changed significantly since their introduction in the early 1980's. In this
paper, we review the predominant methods of classical network steganography,
describing the detailed operations and resultant challenges involved in
embedding data in the network transport domain. We also consider the various
cyber threat vectors of network steganography and point out the major
differences between classical network steganography and the widely known
end-point multimedia embedding techniques, which focus exclusively on static
data modification for data hiding. We then challenge the security community by
introducing an entirely new network dat hiding methodology, which we refer to
as real-time network data steganography. Finally we provide the groundwork for
this fundamental change of covert network data embedding by forming a basic
framework for real-time network data operations that will open the path for
even further advances in computer network security."
"The range of video annotation software currently available is set within
commercially specialized professions, distributed via outdated sources or
through online video hosting services. As video content becomes an increasingly
significant tool for analysis, there is a demand for appropriate digital
annotation techniques that offer equivalent functionality to tools used for
annotation of text based literature sources. This paper argues for the
importance of video annotating as an effective method for research that is as
accessible as literature annotation is. Video annotation has been shown to
trigger higher learning and engagement but research struggles to explain the
absence of video annotation in contemporary structures of education practice.
In both academic and informal settings the use of video playback as a
meaningful tool of analysis is apparent, yet the availability of supplementary
annotation software is not within obvious grasp or even prevalent in
standardized computer software. Practical software tools produced by the
researcher have demonstrated effective video annotation in a short development
time. With software design programs available for rapid application creation,
this paper also highlights the absence of a development community. This paper
argues that video annotation is an accessible tool, not just for academic
contexts, but also for wider practical video analysis applications, potentially
becoming a mainstream learning tool. This paper thus presents a practical
multimodal public approach to video research that potentially affords a deeper
analysis of media content. This is supported by an in-depth consideration of
the motivation for undertaking video annotation and a critical analysis of
currently available tools."
"We have developed reduced reference parametric models for estimating
perceived quality in audiovisual multimedia services. We have created 144
unique configurations for audiovisual content including various application and
network parameters such as bitrates and distortions in terms of bandwidth,
packet loss rate and jitter. To generate the data needed for model training and
validation we have tasked 24 subjects, in a controlled environment, to rate the
overall audiovisual quality on the absolute category rating (ACR) 5-level
quality scale. We have developed models using Random Forest and Neural Network
based machine learning methods in order to estimate Mean Opinion Scores (MOS)
values. We have used information retrieved from the packet headers and side
information provided as network parameters for model training. Random Forest
based models have performed better in terms of Root Mean Square Error (RMSE)
and Pearson correlation coefficient. The side information proved to be very
effective in developing the model. We have found that, while the model
performance might be improved by replacing the side information with more
accurate bit stream level measurements, they are performing well in estimating
perceived quality in audiovisual multimedia services."
"Among the various means to evaluate the quality of video streams,
No-Reference (NR) methods have low computation and may be executed on thin
clients. Thus, NR algorithms would be perfect candidates in cases of real-time
quality assessment, automated quality control and, particularly, in adaptive
mobile streaming. Yet, existing NR approaches are often inaccurate, in
comparison to Full-Reference (FR) algorithms, especially under lossy network
conditions. In this work, we present an NR method that combines machine
learning with simple NR metrics to achieve a quality index comparably as
accurate as the Video Quality Metric (VQM) Full-Reference algorithm. Our method
is tested in an extensive dataset (960 videos), under lossy network conditions
and considering nine different machine learning algorithms. Overall, we achieve
an over 97% correlation with VQM, while allowing real-time assessment of video
quality of experience in realistic streaming scenarios."
"This paper attacks the challenging problem of violence detection in videos.
Different from existing works focusing on combining multi-modal features, we go
one step further by adding and exploiting subclasses visually related to
violence. We enrich the MediaEval 2015 violence dataset by \emph{manually}
labeling violence videos with respect to the subclasses. Such fine-grained
annotations not only help understand what have impeded previous efforts on
learning to fuse the multi-modal features, but also enhance the generalization
ability of the learned fusion to novel test data. The new subclass based
solution, with AP of 0.303 and P100 of 0.55 on the MediaEval 2015 test set,
outperforms several state-of-the-art alternatives. Notice that our solution
does not require fine-grained annotations on the test set, so it can be
directly applied on novel and fully unlabeled videos. Interestingly, our study
shows that motion related features, though being essential part in previous
systems, are dispensable."
"This paper presents a novel method for efficient image retrieval, based on a
simple and effective hashing of CNN features and the use of an indexing
structure based on Bloom filters. These filters are used as gatekeepers for the
database of image features, allowing to avoid to perform a query if the query
features are not stored in the database and speeding up the query process,
without affecting retrieval performance. Thanks to the limited memory
requirements the system is suitable for mobile applications and distributed
databases, associating each filter to a distributed portion of the database.
Experimental validation has been performed on three standard image retrieval
datasets, outperforming state-of-the-art hashing methods in terms of precision,
while the proposed indexing method obtains a $2\times$ speedup."
"Live multimedia streaming from mobile devices is rapidly gaining popularity
but little is known about the QoE they provide. In this paper, we examine the
Periscope service. We first crawl the service in order to understand its usage
patterns. Then, we study the protocols used, the typical quality of experience
indicators, such as playback smoothness and latency, video quality, and the
energy consumption of the Android application."
"Adaptive bitrate streaming (ABR) has been widely adopted to support video
streaming services over heterogeneous devices and varying network conditions.
With ABR, each video content is transcoded into multiple representations in
different bitrates and resolutions. However, video transcoding is computing
intensive, which requires the transcoding service providers to deploy a large
number of servers for transcoding the video contents published by the content
producers. As such, a natural question for the transcoding service provider is
how to provision the computing resource for transcoding the video contents
while maximizing service profit. To address this problem, we design a cloud
video transcoding system by taking the advantage of cloud computing technology
to elastically allocate computing resource. We propose a method for jointly
considering the task scheduling and resource provisioning problem in two
timescales, and formulate the service profit maximization as a two-timescale
stochastic optimization problem. We derive some approximate policies for the
task scheduling and resource provisioning. Based on our proposed methods, we
implement our open source cloud video transcoding system Morph and evaluate its
performance in a real environment. The experiment results demonstrate that our
proposed method can reduce the resource consumption and achieve a higher profit
compared with the baseline schemes."
"Recent years have witnessed a new video delivery paradigm: smartrouter-based
video delivery network, which is enabled by smartrouters deployed at users'
homes, together with the conventional video servers deployed in the
datacenters. Recently, ChinaCache, a large content delivery network (CDN)
provider, and Youku, a video service provider using smartrouters to assist
video delivery, announced their cooperation to create a new paradigm of content
delivery based on householders' network resources. This new paradigm is
different from the conventional peer-to-peer (P2P) approach, because such
dedicated smartrouters are inherently operated by the centralized video service
providers in a coordinative manner. It is intriguing to study the strategies,
performance and potential impact on the content delivery ecosystem of such peer
CDN systems. In this paper, we study the Youku peer CDN, which has deployed
over 300K smartrouter devices for its video streaming. In our measurement, 78K
videos were investigated and 3TB traffic has been analyzed, over controlled
routers and players. Our contributions are the following measurement insights.
First, a global replication and caching strategy is essential for the peer CDN
systems, and proactively scheduling replication and caching on a daily basis
can guarantee their performance. Second, such peer CDN deployment can itself
form an effective Quality of Service (QoS) monitoring sub-system, which can be
used for fine-grained user request redirection. We also provide our analysis on
the performance issues and potential improvements to the peer CDN systems."
"Recent years have witnessed a new video delivery paradigm: smartrouter-based
peer video content delivery network, which is enabled by smartrouters deployed
at users' homes. ChinaCache (one of the largest CDN providers in China) and
Youku (a video provider using smartrouters to assist video delivery) announced
their cooperation in 2015, to create a new paradigm of content delivery based
on householders' network resources. This new paradigm is different from the
conventional peer-to-peer (P2P) approach, because millions of dedicated
smartrouters are operated by the centralized video service providers in a
coordinative manner. Thus it is intriguing to study the content placement
strategies used in a smartrouter-based content delivery system, as well as its
potential impact on the content delivery ecosystem. In this paper, we carry out
measurement studies of Youku's peer video CDN, who has deployed over 300K
smartrouter devices for its video delivery. In our measurement studies, 104K
videos were investigated and 4TB traffic has been analyzed, over controlled
smartrouter nodes and players. Our measurement insights are as follows. First,
a global content replication strategy is essential for the peer CDN systems.
Second, such peer CDN deployment itself can form an effective sub-system for
end-to-end QoS monitoring, which can be used for fine-grained request
redirection (e.g., user-level) and content replication. We also show our
analysis on the performance limitations and propose potential improvements to
the peer CDN systems."
"We introduce models and algorithmic foundations for graph watermarking. Our
frameworks include security definitions and proofs, as well as
characterizations when graph watermarking is algorithmically feasible, in spite
of the fact that the general problem is NP-complete by simple reductions from
the subgraph isomorphism or graph edit distance problems. In the digital
watermarking of many types of files, an implicit step in the recovery of a
watermark is the mapping of individual pieces of data, such as image pixels or
movie frames, from one object to another. In graphs, this step corresponds to
approximately matching vertices of one graph to another based on graph
invariants such as vertex degree. Our approach is based on characterizing the
feasibility of graph watermarking in terms of keygen, marking, and
identification functions defined over graph families with known distributions.
We demonstrate the strength of this approach with exemplary watermarking
schemes for two random graph models, the classic Erd\H{o}s-R\'{e}nyi model and
a random power-law graph model, both of which are used to model real-world
networks."
"Multimedia streaming over HTTP is no longer a niche research topic as it has
entered our daily live. The common assumption is that it is deployed on top of
the existing infrastructure utilizing application (HTTP) and transport (TCP)
layer protocols as is. Interestingly, standards like MPEG's Dynamic Adaptive
Streaming over HTTP (DASH) do not mandate the usage of any specific transport
protocol allowing for sufficient deployment flexibility which is further
supported by emerging developments within both protocol layers. This paper
investigates and evaluates the usage of advanced transport options for the
dynamic adaptive streaming over HTTP. We utilize a common test setup to
evaluate HTTP/2.0 and Google's Quick UDP Internet Connections (QUIC) protocol
in the context of DASH-based services."
"LSB steganography is a one of the most widely used methods for implementing
covert data channels in image file exchanges [1][2]. The low computational
complexity and implementation simplicity of the algorithm are significant
factors for its popularity with the primary reason being low image distortion.
Many attempts have been made to increase the embedding capacity of LSB
algorithms by expanding into the second or third binary layers of the image
while maintaining a low probability of detection with minimal distortive
effects [2][3][4]. In this paper, we introduce an advanced technique for
covertly embedding data within images using redundant number system
decomposition over non-standard digital bit planes. Both grayscale and
bit-mapped images are equally effective as cover files. It will be shown that
this unique steganography method has minimal visual distortive affects while
also preserving the cover file statistics, making it less susceptible to most
general steganography detection algorithms."
"Video captioning has been attracting broad research attention in multimedia
community. However, most existing approaches either ignore temporal information
among video frames or just employ local contextual temporal knowledge. In this
work, we propose a novel video captioning framework, termed as
\emph{Bidirectional Long-Short Term Memory} (BiLSTM), which deeply captures
bidirectional global temporal structure in video. Specifically, we first devise
a joint visual modelling approach to encode video data by combining a forward
LSTM pass, a backward LSTM pass, together with visual features from
Convolutional Neural Networks (CNNs). Then, we inject the derived video
representation into the subsequent language model for initialization. The
benefits are in two folds: 1) comprehensively preserving sequential and visual
information; and 2) adaptively learning dense visual features and sparse
semantic representations for videos and sentences, respectively. We verify the
effectiveness of our proposed video captioning framework on a commonly-used
benchmark, i.e., Microsoft Video Description (MSVD) corpus, and the
experimental results demonstrate that the superiority of the proposed approach
as compared to several state-of-the-art methods."
"Whilst affective responses to various forms and genres of multimedia content
have been well researched, precious few studies have investigated the combined
impact that multimedia system parameters and human factors have on affect.
Consequently, in this paper we explore the role that two primordial dimensions
of human factors - personality and culture - in conjunction with system factors
- frame rate, resolution, and bit rate - have on user affect and enjoyment of
multimedia presentations. To this end, a two-site, cross-cultural study was
undertaken, the results of which produced three predictve models. Personality
and Culture traits were shown statistically to represent 5.6% of the variance
in positive affect, 13.6% in negative affect and 9.3% in enjoyment. The
correlation between affect and enjoyment, was significant. Predictive modeling
incorporating human factors showed about 8%, 7% and 9% improvement in
predicting positive affect, negative affect and enjoyment respectively when
compared to models trained only on system factors. Results and analysis
indicate the significant role played by human factors in influencing affect
that users experience while watching multimedia."
"The main constraint of wireless sensor networks (WSN) in enabling wireless
image communication is the high energy requirement, which may exceed even the
future capabilities of battery technologies. In this paper we have shown that
this bottleneck can be overcome by developing local in-network image processing
algorithm that offers optimal energy consumption. Our algorithm is very
suitable for intruder detection applications. Each node is responsible for
processing the image captured by the video sensor, which consists of NxN
blocks. If an intruder is detected in the monitoring region, the node will
transmit the image for further processing. Otherwise, the node takes no action.
Results provided from our experiments show that our algorithm is better than
the traditional moving object detection techniques by a factor of (N/2) in
terms of energy savings."
"Due to the prevalence of mobile devices, mobile search becomes a more
convenient way than desktop search. Different from the traditional desktop
search, mobile visual search needs more consideration for the limited resources
on mobile devices (e.g., bandwidth, computing power, and memory consumption).
The state-of-the-art approaches show that bag-of-words (BoW) model is robust
for image and video retrieval; however, the large vocabulary tree might not be
able to be loaded on the mobile device. We observe that recent works mainly
focus on designing compact feature representations on mobile devices for
bandwidth-limited network (e.g., 3G) and directly adopt feature matching on
remote servers (cloud). However, the compact (binary) representation might fail
to retrieve target objects (images, videos). Based on the hashed binary codes,
we propose a de-hashing process that reconstructs BoW by leveraging the
computing power of remote servers. To mitigate the information loss from binary
codes, we further utilize contextual information (e.g., GPS) to reconstruct a
context-aware BoW for better retrieval results. Experiment results show that
the proposed method can achieve competitive retrieval accuracy as BoW while
only transmitting few bits from mobile devices."
"Popularly used to distribute a variety of multimedia content items in today
Internet, HTTP-based web content delivery still suffers from various content
delivery failures. Hindered by the expensive deployment cost, the conventional
CDN can not deploy as many edge servers as possible to successfully deliver
content items to all users under these delivery failures. In this paper, we
propose a joint CDN and peer-assisted web content delivery framework to address
the delivery failure problem. Different from conventional peer-assisted
approaches for web content delivery, which mainly focus on alleviating the CDN
servers bandwidth load, we study how to use a browser-based peer-assisted
scheme, namely WebRTC, to resolve content delivery failures. To this end, we
carry out large-scale measurement studies on how users access and view
webpages. Our measurement results demonstrate the challenges (e.g., peers stay
on a webpage extremely short) that can not be directly solved by conventional
P2P strategies, and some important webpage viewing patterns. Due to these
unique characteristics, WebRTC peers open up new possibilities for helping the
web content delivery, coming with the problem of how to utilize the dynamic
resources efficiently. We formulate the peer selection that is the critical
strategy in our framework, as an optimization problem, and design a heuristic
algorithm based on the measurement insights to solve it. Our simulation
experiments driven by the traces from Tencent QZone demonstrate the
effectiveness of our design: compared with non-peer-assisted strategy and
random peer selection strategy, our design significantly improves the
successful relay ratio of web content items under network failures, e.g., our
design improves the content download ratio up to 60% even when users located in
a particular region (e.g., city) where none can connect to the regional CDN
server."
"Dynamic Adaptive Streaming over HTTP (DASH) has emerged as an increasingly
popular paradigm for video streaming [13], in which a video is segmented into
many chunks delivered to users by HTTP request/response over Transmission
Control Protocol (TCP) con- nections. Therefore, it is intriguing to study the
performance of strategies implemented in conventional TCPs, which are not
dedicated for video streaming, e.g., whether chunks are efficiently delivered
when users per- form interactions with the video players. In this paper, we
conduct mea- surement studies on users chunk requesting traces in DASH from a
rep- resentative video streaming provider, to investigate users behaviors in
DASH, and TCP-connection-level traces from CDN servers, to investi- gate the
performance of TCP for DASH. By studying how video chunks are delivered in both
the slow start and congestion avoidance phases, our observations have revealed
the performance characteristics of TCP for DASH as follows: (1) Request
patterns in DASH have a great impact on the performance of TCP variations
including cubic; (2) Strategies in conventional TCPs may cause user perceived
quality degradation in DASH streaming; (3) Potential improvement to TCP
strategies for better delivery in DASH can be further explored."
"This paper proposes a new steganographic scheme relying on the principle of
cover-source switching, the key idea being that the embedding should switch
from one cover-source to another. The proposed implementation, called Natural
Steganography, considers the sensor noise naturally present in the raw images
and uses the principle that, by the addition of a specific noise the
steganographic embedding tries to mimic a change of ISO sensitivity. The
embedding methodology consists in 1) perturbing the image in the raw domain, 2)
modeling the perturbation in the processed domain, 3) embedding the payload in
the processed domain. We show that this methodology is easily tractable
whenever the processes are known and enables to embed large and undetectable
payloads. We also show that already used heuristics such as synchronization of
embedding changes or detectability after rescaling can be respectively
explained by operations such as color demosaicing and down-scaling kernels."
"First responders are increasingly using social media to identify and reduce
crime for well-being and safety of the society. Images shared on social media
hurting religious, political, communal and other sentiments of people, often
instigate violence and create law & order situations in society. This results
in the need for first responders to inspect the spread of such images and users
propagating them on social media. In this paper, we present a comparison
between different hand-crafted features and a Convolutional Neural Network
(CNN) model to retrieve similar images, which outperforms state-of-art
hand-crafted features. We propose an Open-Source-Intelligent (OSINT) real-time
image search system, robust to retrieve modified images that allows first
responders to analyze the current spread of images, sentiments floating and
details of users propagating such content. The system also aids officials to
save time of manually analyzing the content by reducing the search space on an
average by 67%."
"Audio/visual recognition and retrieval applications have recently garnered
significant attention within Internet-of-Things (IoT) oriented services, given
that video cameras and audio processing chipsets are now ubiquitous even in
low-end embedded systems. In the most typical scenario for such services, each
device extracts audio/visual features and compacts them into feature
descriptors, which comprise media queries. These queries are uploaded to a
remote cloud computing service that performs content matching for
classification or retrieval applications. Two of the most crucial aspects for
such services are: (i) controlling the device energy consumption when using the
service; (ii) reducing the billing cost incurred from the cloud infrastructure
provider. In this paper we derive analytic conditions for the optimal coupling
between the device energy consumption and the incurred cloud infrastructure
billing. Our framework encapsulates: the energy consumption to produce and
transmit audio/visual queries, the billing rates of the cloud infrastructure,
the number of devices concurrently connected to the same cloud server, {the
query volume constraint of each cluster of devices,} and the statistics of the
query data production volume per device. Our analytic results are validated via
a deployment with: (i) the device side comprising compact image descriptors
(queries) computed on Beaglebone Linux embedded platforms and transmitted to
Amazon Web Services (AWS) Simple Storage Service; (ii) the cloud side carrying
out image similarity detection via AWS Elastic Compute Cloud (EC2) instances,
with the AWS Auto Scaling being used to control the number of instances
according to the demand."
"Composing fashion outfits involves deep understanding of fashion standards
while incorporating creativity for choosing multiple fashion items (e.g.,
Jewelry, Bag, Pants, Dress). In fashion websites, popular or high-quality
fashion outfits are usually designed by fashion experts and followed by large
audiences. In this paper, we propose a machine learning system to compose
fashion outfits automatically. The core of the proposed automatic composition
system is to score fashion outfit candidates based on the appearances and
meta-data. We propose to leverage outfit popularity on fashion oriented
websites to supervise the scoring component. The scoring component is a
multi-modal multi-instance deep learning system that evaluates instance
aesthetics and set compatibility simultaneously. In order to train and evaluate
the proposed composition system, we have collected a large scale fashion outfit
dataset with 195K outfits and 368K fashion items from Polyvore. Although the
fashion outfit scoring and composition is rather challenging, we have achieved
an AUC of 85% for the scoring component, and an accuracy of 77% for a
constrained composition task."
"Steganography and steganalysis are two important branches of the information
hiding field of research. Steganography methods consist in hiding information
in such a way that the secret message is undetectable for the uninitiated.
Steganalyzis encompasses all the techniques that attempt to detect the presence
of such hidden information. This latter is usually designed by making
classifiers able to separate innocent images from steganographied ones
according to their differences on well-selected features. We wonder, in this
article whether it is possible to construct a kind of universal steganalyzer
without any knowledge regarding the steganographier side. The effects on the
classification score of a modification of either parameters or methods between
the learning and testing stages are then evaluated, while the possibility to
improve the separation score by merging many methods during learning stage is
deeper investigated."
"In this paper we address the issue of photo galleries synchronization, where
pictures related to the same event are collected by different users. Existing
solutions to address the problem are usually based on unrealistic assumptions,
like time consistency across photo galleries, and often heavily rely on
heuristics, limiting therefore the applicability to real-world scenarios. We
propose a solution that achieves better generalization performance for the
synchronization task compared to the available literature. The method is
characterized by three stages: at first, deep convolutional neural network
features are used to assess the visual similarity among the photos; then, pairs
of similar photos are detected across different galleries and used to construct
a graph; eventually, a probabilistic graphical model is used to estimate the
temporal offset of each pair of galleries, by traversing the minimum spanning
tree extracted from this graph. The experimental evaluation is conducted on
four publicly available datasets covering different types of events,
demonstrating the strength of our proposed method. A thorough discussion of the
obtained results is provided for a critical assessment of the quality in
synchronization."
"Vector quantization is an essential tool for tasks involving large scale
data, for example, large scale similarity search, which is crucial for
content-based information retrieval and analysis. In this paper, we propose a
novel vector quantization framework that iteratively minimizes quantization
error. First, we provide a detailed review on a relevant vector quantization
method named \textit{residual vector quantization} (RVQ). Next, we propose
\textit{generalized residual vector quantization} (GRVQ) to further improve
over RVQ. Many vector quantization methods can be viewed as the special cases
of our proposed framework. We evaluate GRVQ on several large scale benchmark
datasets for large scale search, classification and object retrieval. We
compared GRVQ with existing methods in detail. Extensive experiments
demonstrate our GRVQ framework substantially outperforms existing methods in
term of quantization accuracy and computation efficiency."
"Video resolutions used in variety of media are constantly rising. While
manufacturers struggle to perfect their screens it is also important to ensure
high quality of displayed image. Overall quality can be measured using Mean
Opinion Score (MOS). Video quality can be affected by miscellaneous artifacts,
appearing at every stage of video creation and transmission. In this paper, we
present a solution to calculate four distinct video quality metrics that can be
applied to a real time video quality assessment system. Our assessment module
is capable of processing 8K resolution in real time set at the level of 30
frames per second. Throughput of 2.19 GB/s surpasses performance of pure
software solutions. To concentrate on architectural optimization, the module
was created using high level language."
"Image quality assessment (IQA) continues to garner great interest in the
research community, particularly given the tremendous rise in consumer video
capture and streaming. Despite significant research effort in IQA in the past
few decades, the area of no-reference image quality assessment remains a great
challenge and is largely unsolved. In this paper, we propose a novel
no-reference image quality assessment system called Deep Quality, which
leverages the power of deep learning to model the complex relationship between
visual content and the perceived quality. Deep Quality consists of a novel
multi-scale deep convolutional neural network, trained to learn to assess image
quality based on training samples consisting of different distortions and
degradations such as blur, Gaussian noise, and compression artifacts.
Preliminary results using the CSIQ benchmark image quality dataset showed that
Deep Quality was able to achieve strong quality prediction performance (89%
patch-level and 98% image-level prediction accuracy), being able to achieve
similar performance as full-reference IQA methods."
"This article presents a new method for detecting a source point of time based
network steganography - MoveSteg. A steganography carrier could be an example
of multimedia stream made with packets. These packets are then delayed
intentionally to send hidden information using time based steganography
methods. The presented analysis describes a method that allows finding the
source of steganography stream in network that is under our management."
"Steganography is the art of hiding data, in such a way that it is
undetectable under traffic-pattern analysis and the data hidden is only known
to the receiver and the sender. In this paper new method of text steganography
over the silence interval of audio in a video file, is presented. In the
proposed method first the audio signal is extracted from the video. After doing
audio enhancement, the data on the audio signal is steganographed using new
technique and then audio signal is rewritten in video file again.
http://www.learnrnd.com/All_latest_research_findings.php
  To enhance the security level we apply chaotic maps on arbitrary text.
Furthermore, the algorithm in this paper, gives a technique which states that
undetectable stegotext and cover-text has same probability distribution and no
statistical test can detect the presence of the hidden message.
http://www.learnrnd.com/detail.php?id=Biohack_Eyes_through_Chlorin_e6_eye_drop_:Stanford_University_Research
  Moreover, hidden message does not affect the transmission rate of video file
at all."
"With the fast growth of communication networks, the video data transmission
from these networks is extremely vulnerable. Error concealment is a technique
to estimate the damaged data by employing the correctly received data at the
decoder. In this paper, an efficient boundary matching algorithm for estimating
damaged motion vectors (MVs) is proposed. The proposed algorithm performs error
concealment for each damaged macro block (MB) according to the list of
identified priority of each frame. It then uses a classic boundary matching
criterion or the proposed boundary matching criterion adaptively to identify
matching distortion in each boundary of candidate MB. Finally, the candidate MV
with minimum distortion is selected as an MV of damaged MB and the list of
priorities is updated. Experimental results show that the proposed algorithm
improves both objective and subjective qualities of reconstructed frames
without any significant increase in computational cost. The PSNR for test
sequences in some frames is increased about 4.7, 4.5, and 4.4 dB compared to
the classic boundary matching, directional boundary matching, and directional
temporal boundary matching algorithm, respectively."
"Numerous fake images spread on social media today and can severely jeopardize
the credibility of online content to public. In this paper, we employ deep
networks to learn distinct fake image related features. In contrast to
authentic images, fake images tend to be eye-catching and visually striking.
Compared with traditional visual recognition tasks, it is extremely challenging
to understand these psychologically triggered visual patterns in fake images.
Traditional general image classification datasets, such as ImageNet set, are
designed for feature learning at the object level but are not suitable for
learning the hyper-features that would be required by image credibility
analysis. In order to overcome the scarcity of training samples of fake images,
we first construct a large-scale auxiliary dataset indirectly related to this
task. This auxiliary dataset contains 0.6 million weakly-labeled fake and real
images collected automatically from social media. Through an AdaBoost-like
transfer learning algorithm, we train a CNN model with a few instances in the
target training set and 0.6 million images in the collected auxiliary set. This
learning algorithm is able to leverage knowledge from the auxiliary set and
gradually transfer it to the target task. Experiments on a real-world testing
set show that our proposed domain transferred CNN model outperforms several
competing baselines. It obtains superiror results over transfer learning
methods based on the general ImageNet set. Moreover, case studies show that our
proposed method reveals some interesting patterns for distinguishing fake and
authentic images."
"The query-by-image video retrieval (QBIVR) task has been attracting
considerable research attention recently. However, most existing methods
represent a video by either aggregating or projecting all its frames into a
single datum point, which may easily cause severe information loss. In this
paper, we propose an efficient QBIVR framework to enable an effective and
efficient video search with image query. We first define a
similarity-preserving distance metric between an image and its orthogonal
projection in the subspace of the video, which can be equivalently transformed
to a Maximum Inner Product Search (MIPS) problem.
  Besides, to boost the efficiency of solving the MIPS problem, we propose two
asymmetric hashing schemes, which bridge the domain gap of images and videos.
The first approach, termed Inner-product Binary Coding (IBC), preserves the
inner relationships of images and videos in a common Hamming space. To further
improve the retrieval efficiency, we devise a Bilinear Binary Coding (BBC)
approach, which employs compact bilinear projections instead of a single large
projection matrix. Extensive experiments have been conducted on four real-world
video datasets to verify the effectiveness of our proposed approaches as
compared to the state-of-the-arts."
"Video watermarking is extensively used in many media-oriented applications
for embedding watermarks, i.e. hidden digital data, in a video sequence to
protect the video from illegal copying and to identify manipulations made in
the video. In case of an invisible watermark, the human eye can not perceive
any difference in the video, but a watermark extraction application can read
the watermark and obtain the embedded information. Although numerous
methodologies exist for embedding watermarks, many of them have shortcomings
with respect to performance efficiency, especially over a distributed network.
This paper proposes and analyses a 2-bit Least Significant Bit (LSB) parallel
algorithmic approach for achieving performance efficiency to watermark and
distribute videos over a client-server framework."
"It has long been considered a significant problem to improve the visual
quality of lossy image and video compression. Recent advances in computing
power together with the availability of large training data sets has increased
interest in the application of deep learning cnns to address image recognition
and image processing tasks. Here, we present a powerful cnn tailored to the
specific task of semantic image understanding to achieve higher visual quality
in lossy compression. A modest increase in complexity is incorporated to the
encoder which allows a standard, off-the-shelf jpeg decoder to be used. While
jpeg encoding may be optimized for generic images, the process is ultimately
unaware of the specific content of the image to be compressed. Our technique
makes jpeg content-aware by designing and training a model to identify multiple
semantic regions in a given image. Unlike object detection techniques, our
model does not require labeling of object positions and is able to identify
objects in a single pass. We present a new cnn architecture directed
specifically to image compression, which generates a map that highlights
semantically-salient regions so that they can be encoded at higher quality as
compared to background regions. By adding a complete set of features for every
class, and then taking a threshold over the sum of all feature activations, we
generate a map that highlights semantically-salient regions so that they can be
encoded at a better quality compared to background regions. Experiments are
presented on the Kodak PhotoCD dataset and the MIT Saliency Benchmark dataset,
in which our algorithm achieves higher visual quality for the same compressed
size."
"We introduce a dataset for facilitating audio-visual analysis of musical
performances. The dataset comprises a number of simple multi-instrument musical
pieces assembled from coordinated but separately recorded performances of
individual tracks. For each piece, we provide the musical score in MIDI format,
the audio recordings of the individual tracks, the audio and video recording of
the assembled mixture, and ground-truth annotation files including frame-level
and note-level transcriptions. We anticipate that the dataset will be useful
for developing and evaluating multi-modal techniques for music source
separation, transcription, score following, and performance analysis. We
describe our methodology for the creation of this dataset, particularly
highlighting our approaches for addressing the challenges involved in
maintaining synchronization and naturalness. We briefly discuss the research
questions that can be investigated with this dataset."
"With the evolution of HDTV and Ultra HDTV, the bandwidth requirement for
IP-based TV content is rapidly increasing. Consumers demand uninterrupted
service with a high Quality of Experience (QoE). Service providers are
constantly trying to differentiate themselves by innovating new ways of
distributing content more efficiently with lower cost and higher penetration.
In this work, we propose a cost-efficient wireless framework (WiLiTV) for
delivering live TV services, consisting of a mix of wireless access
technologies (e.g. Satellite, WiFi and LTE overlay links). In the proposed
architecture, live TV content is injected into the network at a few residential
locations using satellite dishes. The content is then further distributed to
other homes using a house-to-house WiFi network or via an overlay LTE network.
Our problem is to construct an optimal TV distribution network with the minimum
number of satellite injection points, while preserving the highest QoE, for
different neighborhood densities. We evaluate the framework using realistic
time-varying demand patterns and a diverse set of home location data. Our study
demonstrates that the architecture requires 75 - 90% fewer satellite injection
points, compared to traditional architectures. Furthermore, we show that most
cost savings can be obtained using simple and practical relay routing
solutions."
"To stretch a music piece to a given length is a common demand in people's
daily lives, e.g., in audio-video synchronization and animation production.
However, it is not always guaranteed that the stretched music piece is
acceptable for general audience since music stretching suffers from people's
perceptual artefacts. Over-stretching a music piece will make it uncomfortable
for human psychoacoustic hearing. The research on music stretching resistance
attempts to estimate the maximum stretchability of music pieces to further
avoid over-stretch. It has been observed that musical genres can significantly
improve the accuracy of automatic estimation of music stretching resistance,
but how musical genres are related to music stretching resistance has never
been explained or studied in detail in the literature. In this paper, the
characteristics of music stretching resistance are compared across different
musical genres. It is found that music stretching resistance has strong
intra-genre cohesiveness and inter-genre discrepancies in the experiments.
Moreover, the ambiguity and the symmetry of music stretching resistance are
also observed in the experimental analysis. These findings lead to a new
measurement on the similarity between different musical genres based on their
music stretching resistance. In addition, the analysis of variance (ANOVA) also
supports the findings in this paper by verifying the significance of musical
genre in shaping music stretching resistance."
"In this study, a method to construct a full-colour volumetric display is
presented using a commercially available inkjet printer. Photoreactive
luminescence materials are minutely and automatically printed as the volume
elements, and volumetric displays are constructed with high resolution using
easy-to-fabricate means that exploit inkjet printing technologies. The results
experimentally demonstrate the first prototype of an inkjet printing-based
volumetric display composed of multiple layers of transparent films that yield
a full-colour three-dimensional (3D) image. Moreover, we propose a design
algorithm with 3D structures that provide multiple different 2D full-colour
patterns when viewed from different directions and experimentally demonstrates
prototypes. It is considered that these types of 3D volumetric structures and
their fabrication methods based on widely deployed existing printing
technologies can be utilised as novel information display devices and systems,
including digital signage, media art, entertainment and security."
"Photos are becoming spontaneous, objective, and universal sources of
information. This paper develops evolving situation recognition using photo
streams coming from disparate sources combined with the advances of deep
learning. Using visual concepts in photos together with space and time
information, we formulate the situation detection into a semi-supervised
learning framework and propose new graph-based models to solve the problem. To
extend the method for unknown situations, we introduce a soft label method
which enables the traditional semi-supervised learning framework to accurately
predict predefined labels as well as effectively form new clusters. To overcome
the noisy data which degrades graph quality, leading to poor recognition
results, we take advantage of two kinds of noise-robust norms which can
eliminate the adverse effects of outliers in visual concepts and improve the
accuracy of situation recognition. Finally, we demonstrate the idea and the
effectiveness of the proposed model on Yahoo Flickr Creative Commons 100
Million."
"This paper proposes a novel advanced motion model to handle the irregular
motion for the cubic map projection of 360-degree video. Since the irregular
motion is mainly caused by the projection from the sphere to the cube map, we
first try to project the pixels in both the current picture and reference
picture from unfolding cube back to the sphere. Then through utilizing the
characteristic that most of the motions in the sphere are uniform, we can
derive the relationship between the motion vectors of various pixels in the
unfold cube. The proposed advanced motion model is implemented in the High
Efficiency Video Coding reference software. Experimental results demonstrate
that quite obvious performance improvement can be achieved for the sequences
with obvious motions."
"Internet-native audio-visual services are witnessing rapid development. Among
these services, object-based audio-visual services are gaining importance. In
2014, we established the Software Defined Media (SDM) consortium to target new
research areas and markets involving object-based digital media and
Internet-by-design audio-visual environments. In this paper, we introduce the
SDM architecture that virtualizes networked audio-visual services along with
the development of smart buildings and smart cities using Internet of Things
(IoT) devices and smart building facilities. Moreover, we design the SDM
architecture as a layered architecture to promote the development of innovative
applications on the basis of rapid advancements in software-defined networking
(SDN). Then, we implement a prototype system based on the architecture, present
the system at an exhibition, and provide it as an SDM API to application
developers at hackathons. Various types of applications are developed using the
API at these events. An evaluation of SDM API access shows that the prototype
SDM platform effectively provides 3D audio reproducibility and interactiveness
for SDM applications."
"In the context of Social TV, the increasing popularity of first and second
screen users, interacting and posting content online, illustrates new business
opportunities and related technical challenges, in order to enrich user
experience on such environments. SAM (Socializing Around Media) project uses
Social Media-connected infrastructure to deal with the aforementioned
challenges, providing intelligent user context management models and mechanisms
capturing social patterns, to apply collaborative filtering techniques and
personalized recommendations towards this direction. This paper presents the
Context Management mechanism of SAM, running in a Social TV environment to
provide smart recommendations for first and second screen content. Work
presented is evaluated using real movie rating dataset found online, to
validate the SAM's approach in terms of effectiveness as well as efficiency."
"Although the protection of ownership and the prevention of unauthorized
manipulation of digital images becomes an important concern, there is also a
big issue of image source origin authentication. This paper proposes a
procedure for the identification of the image source and content by using the
Public Key Cryptography Signature (PKCS). The procedure is based on the PKCS
watermarking of the images captured with numerous automatic observing cameras
in the Trap View cloud system. Watermark is created based on 32-bit PKCS serial
number and embedded into the captured image. Watermark detection on the
receiver side extracts the serial number and indicates the camera which
captured the image by comparing the original and the extracted serial numbers.
The watermarking procedure is designed to provide robustness to image
optimization based on the Compressive Sensing approach. Also, the procedure is
tested under various attacks and shows successful identification of ownership."
"In this paper, an unsupervised steganalysis method that combines artificial
training setsand supervised classification is proposed. We provide a formal
framework for unsupervisedclassification of stego and cover images in the
typical situation of targeted steganalysis (i.e.,for a known algorithm and
approximate embedding bit rate). We also present a completeset of experiments
using 1) eight different image databases, 2) image features based on
RichModels, and 3) three different embedding algorithms: Least Significant Bit
(LSB) matching,Highly undetectable steganography (HUGO) and Wavelet Obtained
Weights (WOW). Weshow that the experimental results outperform previous methods
based on Rich Models inthe majority of the tested cases. At the same time, the
proposed approach bypasses theproblem of Cover Source Mismatch -when the
embedding algorithm and bit rate are known-, since it removes the need of a
training database when we have a large enough testing set.Furthermore, we
provide a generic proof of the proposed framework in the machine
learningcontext. Hence, the results of this paper could be extended to other
classification problemssimilar to steganalysis."
"The paper presents a novel approach to occlusion handling problem in depth
estimation using three views. A solution based on modification of similarity
cost function is proposed. During the depth estimation via optimization
algorithms like Graph Cut similarity metric is constantly updated so that only
non-occluded fragments in side views are considered. At each iteration of the
algorithm non-occluded fragments are detected based on side view virtual depth
maps synthesized from the best currently estimated depth map of the center
view. Then similarity metric is updated for correspondence search only in
non-occluded regions of the side views. The experimental results, conducted on
well-known 3D video test sequences, have proved that the depth maps estimated
with the proposed approach provide about 1.25 dB virtual view quality
improvement in comparison to the virtual view synthesized based on depth maps
generated by the state-of-the-art MPEG Depth Estimation Reference Software."
"Studies show that refining real-world categories into semantic subcategories
contributes to better image modeling and classification. Previous image
sub-categorization work relying on labeled images and WordNet's hierarchy is
not only labor-intensive, but also restricted to classify images into NOUN
subcategories. To tackle these problems, in this work, we exploit general
corpus information to automatically select and subsequently classify web images
into semantic rich (sub-)categories. The following two major challenges are
well studied: 1) noise in the labels of subcategories derived from the general
corpus; 2) noise in the labels of images retrieved from the web. Specifically,
we first obtain the semantic refinement subcategories from the text perspective
and remove the noise by the relevance-based approach. To suppress the search
error induced noisy images, we then formulate image selection and classifier
learning as a multi-class multi-instance learning problem and propose to solve
the employed problem by the cutting-plane algorithm. The experiments show
significant performance gains by using the generated data of our way on both
image categorization and sub-categorization tasks. The proposed approach also
consistently outperforms existing weakly supervised and web-supervised
approaches."
"Teleradiology enables medical images to be transferred over the computer
networks for many purposes including clinical interpretation, diagnosis,
archive, etc. In telemedicine, medical images can be manipulated while
transferring. In addition, medical information security requirements are
specified by the legislative rules, and concerned entities must adhere to them.
In this research, we propose a new scheme based on 2-dimensional Discrete
Wavelet Transform (2D DWT) to improve the robustness and authentication of
medical images. In addition, the current research improves security and
capacity of watermarking using encryption and compression in medical images.
The evaluation is performed on the personal dataset, which contains 194 CTI and
68 MRI cases."
"Virtual reality (VR) video provides an immersive 360 viewing experience to a
user wearing a head-mounted display: as the user rotates his head,
correspondingly different fields-of-view (FoV) of the 360 video are rendered
for observation. Transmitting the entire 360 video in high quality over
bandwidth-constrained networks from server to client for real-time playback is
challenging. In this paper we propose a multi-stream switching framework for VR
video streaming: the server pre-encodes a set of VR video streams covering
different view ranges that account for server-client round trip time (RTT)
delay, and during streaming the server transmits and switches streams according
to a user's detected head rotation angle. For a given RTT, we formulate an
optimization to seek multiple VR streams of different view ranges and the
head-angle-to-stream mapping function simultaneously, in order to minimize the
expected distortion subject to bandwidth and storage constraints. We propose an
alternating algorithm that, at each iteration, computes the optimal streams
while keeping the mapping function fixed and vice versa. Experiments show that
for the same bandwidth, our multi-stream switching scheme outperforms a
non-switching single-stream approach by up to 2.9dB in PSNR."
"With the headway of the advanced image handling software and altering tools,
a computerized picture can be effectively controlled. The identification of
image manipulation is vital in light of the fact that an image can be utilized
as legitimate confirmation, in crime scene investigation, and in numerous
different fields. The image forgery detection techniques intend to confirm the
credibility of computerized pictures with no prior information about the
original image. There are numerous routes for altering a picture, for example,
resampling, splicing, and copy-move. In this paper, we have examined different
type of image forgery and their detection techniques; mainly we focused on
pixel based image forgery detection techniques."
"Steganography involves hiding a secret message or image inside another cover
image. Changes are made in the cover image without affecting visual quality of
the image. In contrast to cryptography, Steganography provides complete secrecy
of the communication. Security of very sensitive data can be enhanced by
combining cryptography and steganography. A new technique that uses the concept
of Steganography to obtain the position values from an image is suggested. This
paper proposes a new method where no change is made to the cover image, only
the pixel position LSB (Least Significant Bit) values that match with the
secret message bit values are noted in a separate position file. At the sending
end the position file along with the cover image is sent. At the receiving end
the position file is opened only with a secret key. The bit positions are taken
from the position file and the LSB values from the positions are combined to
get ASCII values and then form characters of the secret message"
"Music emotion recognition (MER) is usually regarded as a multi-label tagging
task, and each segment of music can inspire specific emotion tags. Most
researchers extract acoustic features from music and explore the relations
between these features and their corresponding emotion tags. Considering the
inconsistency of emotions inspired by the same music segment for human beings,
seeking for the key acoustic features that really affect on emotions is really
a challenging task. In this paper, we propose a novel MER method by using deep
convolutional neural network (CNN) on the music spectrograms that contains both
the original time and frequency domain information. By the proposed method, no
additional effort on extracting specific features required, which is left to
the training procedure of the CNN model. Experiments are conducted on the
standard CAL500 and CAL500exp dataset. Results show that, for both datasets,
the proposed method outperforms state-of-the-art methods."
"In the 360-degree immersive video, a user only views a part of the entire raw
video frame based on her viewing direction. However, today's 360-degree video
players always fetch the entire panoramic view regardless of users' head
movement, leading to significant bandwidth waste that can be potentially
avoided. In this paper, we propose a novel adaptive streaming scheme for
360-degree videos. The basic idea is to fetch the invisible portion of a video
at the lowest quality based on users' head movement prediction and to
adaptively decide the video playback quality for the visible portion based on
bandwidth prediction. Doing both in a robust manner requires overcome a series
of challenges, such as jointly considering the spatial and temporal domains,
tolerating prediction errors, and achieving low complexity. To overcome these
challenges, we first define quality of experience (QoE) metrics for adaptive
360-degree video streaming. We then formulate an optimization problem and solve
it at a low complexity. The algorithm strategically leverages both future
bandwidth and the distribution of users' head positions to determine the
quality level of each tile (i.e., a sub-area of a raw frame). We further
provide theoretical proof showing that our algorithm achieves optimality under
practical assumptions. Numerical results show that our proposed algorithms
significantly boost the user QoE by at least 20\% compared to baseline
algorithms."
"Dynamic adaptive streaming over HTTP (DASH) has recently been widely deployed
in the Internet and adopted in the industry. It, however, does not impose any
adaptation logic for selecting the quality of video fragments requested by
clients and suffers from lackluster performance with respect to a number of
desirable properties: efficiency, stability, and fairness when multiple players
compete for a bottleneck link. In this paper, we propose a throughput-friendly
DASH (TFDASH) rate control scheme for video streaming with multiple clients
over DASH to well balance the trade-offs among efficiency, stability, and
fairness. The core idea behind guaranteeing fairness and high efficiency
(bandwidth utilization) is to avoid OFF periods during the downloading process
for all clients, i.e., the bandwidth is in perfect-subscription or
over-subscription with bandwidth utilization approach to 100\%. We also propose
a dual-threshold buffer model to solve the instability problem caused by the
above idea. As a result, by integrating these novel components, we also propose
a probability-driven rate adaption logic taking into account several key
factors that most influence visual quality, including buffer occupancy, video
playback quality, video bit-rate switching frequency and amplitude, to
guarantee high-quality video streaming. Our experiments evidently demonstrate
the superior performance of the proposed method."
"Open world games present players with more freedom than games with linear
progression structures. However, without clearly-defined objectives, they often
leave players without a sense of purpose. Most of the time, quests and
objectives are hand-authored and overlaid atop an open world's mechanics. But
what if they could be generated organically from the gameplay itself? The goal
of our project was to develop a model of the mechanics in Minecraft that could
be used to determine the ideal placement of objectives in an open world
setting. We formalized the game logic of Minecraft in terms of logical rules
that can be manipulated in two ways: they may be executed to generate graphs
representative of the player experience when playing an open world game with
little developer direction; and they may be statically analyzed to determine
dependency orderings, feedback loops, and bottlenecks. These analyses may then
be used to place achievements on gameplay actions algorithmically."
"The two-dimensional discrete cosine transform (DCT) can be found in the heart
of many image compression algorithms. Specifically, the JPEG format uses a
lossy form of compression based on that transform. Since the standardization of
the JPEG, many other transforms become practical in lossy data compression.
This article aims to analyze the use of these transforms as the DCT replacement
in the JPEG compression chain. Each transform is examined for different image
datasets and subsequently compared to other transforms using the peak
signal-to-noise ratio (PSNR). Our experiments show that an overlapping
variation of the DCT, the local cosine transform (LCT), overcame the original
block-wise transform at low bitrates. At high bitrates, the discrete wavelet
transform employing the Cohen-Daubechies-Feauveau 9/7 wavelet offers about the
same compression performance as the DCT."
"Continuous multimodal representations suitable for multimodal information
retrieval are usually obtained with methods that heavily rely on multimodal
autoencoders. In video hyperlinking, a task that aims at retrieving video
segments, the state of the art is a variation of two interlocked networks
working in opposing directions. These systems provide good multimodal
embeddings and are also capable of translating from one representation space to
the other. Operating on representation spaces, these networks lack the ability
to operate in the original spaces (text or image), which makes it difficult to
visualize the crossmodal function, and do not generalize well to unseen data.
Recently, generative adversarial networks have gained popularity and have been
used for generating realistic synthetic data and for obtaining high-level,
single-modal latent representation spaces. In this work, we evaluate the
feasibility of using GANs to obtain multimodal representations. We show that
GANs can be used for multimodal representation learning and that they provide
multimodal representations that are superior to representations obtained with
multimodal autoencoders. Additionally, we illustrate the ability of visualizing
crossmodal translations that can provide human-interpretable insights on
learned GAN-based video hyperlinking models."
"This paper presents two novel approaches to increase performance bounds of
image steganography under the criteria of minimizing distortion. First, in
order to efficiently use the images' capacities, we propose using parallel
images in the embedding stage. The result is then used to prove sub-optimality
of the message distribution technique used by all cost based algorithms
including HUGO, S-UNIWARD, and HILL. Second, a new distribution approach is
presented to further improve the security of these algorithms. Experiments show
that this distribution method avoids embedding in smooth regions and thus
achieves a better performance, measured by state-of-the-art steganalysis, when
compared with the current used distribution."
"In mobile networks, users may lose coverage when entering a building due to
the high signal attenuation at windows and walls. Under such conditions,
services with minimum bit-rate requirements, such as video streaming, often
show poor Quality-of-Experience (QoE). We will present a Bayesian detector that
combines measurements from two Smartphone sensors to decide if a user is inside
a building or not. Based on this coverage classification, we will propose an
HTTP adaptive streaming (HAS) algorithm to increase playback stability at a
high average bitrate. Measurements in a typical office building show high
accuracy for the presented detector and superior QoE for the proposed HAS
algorithm."
"Assessment of multimedia quality relies heavily on subjective assessment, and
is typically done by human subjects in the form of preferences or continuous
ratings. Such data is crucial for analysis of different multimedia processing
algorithms as well as validation of objective (computational) methods for the
said purpose. To that end, statistical testing provides a theoretical framework
towards drawing meaningful inferences, and making well grounded conclusions and
recommendations. While parametric tests (such as t test, ANOVA, and error
estimates like confidence intervals) are popular and widely used in the
community, there appears to be a certain degree of confusion in the application
of such tests. Specifically, the assumption of normality and homogeneity of
variance is often not well understood. Therefore, the main goal of this paper
is to revisit them from a theoretical perspective and in the process provide
useful insights into their practical implications. Experimental results on both
simulated and real data are presented to support the arguments made. A software
implementing the said recommendations is also made publicly available, in order
to achieve the goal of reproducible research."
"When an attacker wants to falsify an image, in most of cases she/he will
perform a JPEG recompression. Different techniques have been developed based on
diverse theoretical assumptions but very effective solutions have not been
developed yet. Recently, machine learning based approaches have been started to
appear in the field of image forensics to solve diverse tasks such as
acquisition source identification and forgery detection. In this last case, the
aim ahead would be to get a trained neural network able, given a to-be-checked
image, to reliably localize the forged areas. With this in mind, our paper
proposes a step forward in this direction by analyzing how a single or double
JPEG compression can be revealed and localized using convolutional neural
networks (CNNs). Different kinds of input to the CNN have been taken into
consideration, and various experiments have been carried out trying also to
evidence potential issues to be further investigated."
"Videos are inherently multimodal. This paper studies the problem of how to
fully exploit the abundant multimodal clues for improved video categorization.
We introduce a hybrid deep learning framework that integrates useful clues from
multiple modalities, including static spatial appearance information, motion
patterns within a short time window, audio information as well as long-range
temporal dynamics. More specifically, we utilize three Convolutional Neural
Networks (CNNs) operating on appearance, motion and audio signals to extract
their corresponding features. We then employ a feature fusion network to derive
a unified representation with an aim to capture the relationships among
features. Furthermore, to exploit the long-range temporal dynamics in videos,
we apply two Long Short Term Memory networks with extracted appearance and
motion features as inputs. Finally, we also propose to refine the prediction
scores by leveraging contextual relationships among video semantics. The hybrid
deep learning framework is able to exploit a comprehensive set of multimodal
features for video classification. Through an extensive set of experiments, we
demonstrate that (1) LSTM networks which model sequences in an explicitly
recurrent manner are highly complementary with CNN models; (2) the feature
fusion network which produces a fused representation through modeling feature
relationships outperforms alternative fusion strategies; (3) the semantic
context of video classes can help further refine the predictions for improved
performance. Experimental results on two challenging benchmarks, the UCF-101
and the Columbia Consumer Videos (CCV), provide strong quantitative evidence
that our framework achieves promising results: $93.1\%$ on the UCF-101 and
$84.5\%$ on the CCV, outperforming competing methods with clear margins."
"The explosive increase and ubiquitous accessibility of visual data on the Web
have led to the prosperity of research activity in image search or retrieval.
With the ignorance of visual content as a ranking clue, methods with text
search techniques for visual retrieval may suffer inconsistency between the
text words and visual content. Content-based image retrieval (CBIR), which
makes use of the representation of visual content to identify relevant images,
has attracted sustained attention in recent two decades. Such a problem is
challenging due to the intention gap and the semantic gap problems. Numerous
techniques have been developed for content-based image retrieval in the last
decade. The purpose of this paper is to categorize and evaluate those
algorithms proposed during the period of 2003 to 2016. We conclude with several
promising directions for future research."
"In this digital era, one thing that still holds the convention is a printed
archive. Printed documents find their use in many critical domains such as
contract papers, legal tenders and proof of identity documents. As more
advanced printing, scanning and image editing techniques are becoming
available, forgeries on these legal tenders pose a serious threat. Ability to
easily and reliably identify source printer of a printed document can help a
lot in reducing this menace. During printing procedure, printer hardware
introduces certain distortions in printed characters' locations and shapes
which are invisible to naked eyes. These distortions are referred as geometric
distortions, their profile (or signature) is generally unique for each printer
and can be used for printer classification purpose. This paper proposes a set
of features for characterizing text-line-level geometric distortions, referred
as geometric distortion signatures and presents a novel system to use them for
identification of the origin of a printed document. Detailed experiments
performed on a set of thirteen printers demonstrate that the proposed system
achieves state of the art performance and gives much higher accuracy under
small training size constraint. For four training and six test pages of three
different fonts, the proposed method gives 99\% classification accuracy."
"An important aspect of examining printed documents for potential forgeries
and copyright infringement is the identification of source printer as it can be
helpful for ascertaining the leak and detecting forged documents. This paper
proposes a system for classification of source printer from scanned images of
printed documents using all the printed letters simultaneously. This system
uses local texture patterns based features and a single classifier for
classifying all the printed letters. Letters are extracted from scanned images
using connected component analysis followed by morphological filtering without
the need of using an OCR. Each letter is sub-divided into a flat region and an
edge region, and local tetra patterns are estimated separately for these two
regions. A strategically constructed pooling technique is used to extract the
final feature vectors. The proposed method has been tested on both a publicly
available dataset of 10 printers and a new dataset of 18 printers scanned at a
resolution of 600 dpi as well as 300 dpi printed in four different fonts. The
results indicate shape independence property in the proposed method as using a
single classifier it outperforms existing handcrafted feature-based methods and
needs much smaller number of training pages by using all the printed letters."
"A wireless sensor network (WSN) typically consists of base stations and a
large number of wireless sensors. The sensory data gathered from the whole
network at a certain time snapshot can be visualized as an image. As a result,
information hiding techniques can be applied to this ""sensory data image"".
Steganography refers to the technology of hiding data into digital media
without drawing any suspicion, while steganalysis is the art of detecting the
presence of steganography. This article provides a brief review of
steganography and steganalysis applications for wireless sensor networks
(WSNs). Then we show that the steganographic techniques are both related to
sensed data authentication in wireless sensor networks, and when considering
the attacker point of view, which has not yet been investigated in the
literature. Our simulation results show that the sink level is unable to detect
an attack carried out by the nsF5 algorithm on sensed data."
"In this paper, we carry out a performance analysis from a probabilistic
perspective to introduce the EDHVW methods' expected performances and
limitations. Then, we propose a new general error diffusion based halftone
visual watermarking (EDHVW) method, Content aware Double-sided Embedding Error
Diffusion (CaDEED), via considering the expected watermark decoding performance
with specific content of the cover images and watermark, different noise
tolerance abilities of various cover image content and the different importance
levels of every pixel (when being perceived) in the secret pattern (watermark).
To demonstrate the effectiveness of CaDEED, we propose CaDEED with expectation
constraint (CaDEED-EC) and CaDEED-NVF&IF (CaDEED-N&I). Specifically, we build
CaDEED-EC by only considering the expected performances of specific cover
images and watermark. By adopting the noise visibility function (NVF) and
proposing the importance factor (IF) to assign weights to every embedding
location and watermark pixel, respectively, we build the specific method
CaDEED-N&I. In the experiments, we select the optimal parameters for NVF and IF
via extensive experiments. In both the numerical and visual comparisons, the
experimental results demonstrate the superiority of our proposed work."
"Multiview video supports observing a scene from different viewpoints. The
Joint Video Team (JVT) developed H.264/MVC to enhance the compression
efficiency for multiview video, however, MVC encoded multiview video (MVC
video) still requires the high bitrates for transmission. This paper
investigates live MVC video streaming over Peer-to-Peer (P2P) networks. The
goal is to minimize the server bandwidth costs whist ensuring high streaming
quality to peers. MVC employs intra-view and inter-view prediction structures,
which leads to a complicated layer dependency relationship. As the peers'
outbound bandwidth is shared while supplying all the MVC video layers, the
bandwidth allocation to one MVC layer affects the available outbound bandwidth
of the other layers. To optimise the utilisation of the peers' outbound
bandwidth for providing video layers, a maximum flow based model is proposed
which considers the MVC video layer dependency and the layer supplying
relationship between peers. Based on the model, a layer dependency aware live
MVC video streaming method over a BitTorrent-like P2P network is proposed,
named MVP2P. The key components of MVP2P include a chunk scheduling strategy
and a peer selection strategy for receiving peers, and a bandwidth scheduling
algorithm for supplying peers. To evaluate the efficiency of the proposed
solution, MVP2P is compared with existing methods considering the constraints
of peer bandwidth, peer numbers, view switching rates, and peer churns. The
test results show that MVP2P significantly outperforms the existing methods."
"We propose a framework for multimodal sentiment analysis and emotion
recognition using convolutional neural network-based feature extraction from
text and visual modalities. We obtain a performance improvement of 10% over the
state of the art by combining visual, text and audio features. We also discuss
some major issues frequently ignored in multimodal sentiment analysis research:
the role of speaker-independent models, importance of the modalities and
generalizability. The paper thus serve as a new benchmark for further research
in multimodal sentiment analysis and also demonstrates the different facets of
analysis to be considered while performing such tasks."
"Due to the lack of budget, competence, personnel and time, small museums are
often unable to develop compelling, educational and accessible web resources
for their permanent collections or temporary exhibitions. In an attempt to
prove that investing in these types of resources can be very fruitful even for
small institutions, we will illustrate the case of Accademia Carrara, a museum
in Bergamo, northern Italy, which, for a current temporary exhibition on
Cezanne and Renoir's masterpieces from the Paul Guillaume collection, developed
a series of multimedia applications, including an accessible website, rich in
content and educational material [www.cezannerenoir.it]."
"In this paper, we propose a method to address the problem of source
estimation for Sparse Component Analysis (SCA) in the presence of additive
noise. Our method is a generalization of a recently proposed method (SL0),
which has the advantage of directly minimizing the L0-norm instead of L1-norm,
while being very fast. SL0 is based on minimization of the smoothed L0-norm
subject to As=x. In order to better estimate the source vector for noisy
mixtures, we suggest then to remove the constraint As=x, by relaxing exact
equality to an approximation (we call our method Smoothed L0-norm Denoising or
SL0DN). The final result can then be obtained by minimization of a proper
linear combination of the smoothed L0-norm and a cost function for the
approximation. Experimental results emphasize on the significant enhancement of
the modified method in noisy cases."
"Nowadays, a popular method used for additive watermarking is wide spread
spectrum. It consists in adding a spread signal into the host document. This
signal is obtained by the sum of a set of carrier vectors, which are modulated
by the bits to be embedded. To extract these embedded bits, weighted
correlations between the watermarked document and the carriers are computed.
Unfortunately, even without any attack, the obtained set of bits can be
corrupted due to the interference with the host signal (host interference) and
also due to the interference with the others carriers (inter-symbols
interference (ISI) due to the non-orthogonality of the carriers). Some recent
watermarking algorithms deal with host interference using side informed
methods, but inter-symbols interference problem is still open. In this paper,
we deal with interference cancellation methods, and we propose to consider ISI
as side information and to integrate it into the host signal. This leads to a
great improvement of extraction performance in term of signal-to-noise ratio
and/or watermark robustness."
"This paper deals with public-key steganography in the presence of a passive
warden. The aim is to hide secret messages within cover-documents without
making the warden suspicious, and without any preliminar secret key sharing.
Whereas a practical attempt has been already done to provide a solution to this
problem, it suffers of poor flexibility (since embedding and decoding steps
highly depend on cover-signals statistics) and of little capacity compared to
recent data hiding techniques. Using the same framework, this paper explores
the use of trellis-coded quantization techniques (TCQ and turbo TCQ) to design
a more efficient public-key scheme. Experiments on audio signals show great
improvements considering Cachin's security criterion."
"Image filtering algorithms are applied on images to remove the different
types of noise that are either present in the image during capturing or
injected in to the image during transmission. Underwater images when captured
usually have Gaussian noise, speckle noise and salt and pepper noise. In this
work, five different image filtering algorithms are compared for the three
different noise types. The performances of the filters are compared using the
Peak Signal to Noise Ratio (PSNR) and Mean Square Error (MSE). The modified
spatial median filter gives desirable results in terms of the above two
parameters for the three different noise. Forty underwater images are taken for
study."