summary
"We present a new method for discovering a segmental discourse structure of a
document while categorizing segment function. We demonstrate how retrieval of
noun phrases and pronominal forms, along with a zero-sum weighting scheme,
determines topicalized segmentation. Futhermore, we use term distribution to
aid in identifying the role that the segment performs in the document. Finally,
we present results of evaluation in terms of precision and recall which surpass
earlier approaches."
"We outline how utterances in dialogs can be interpreted using a partial first
order logic. We exploit the capability of this logic to talk about the truth
status of formulae to define a notion of coherence between utterances and
explain how this coherence relation can serve for the construction of AND/OR
trees that represent the segmentation of the dialog. In a BDI model we
formalize basic assumptions about dialog and cooperative behaviour of
participants. These assumptions provide a basis for inferring speech acts from
coherence relations between utterances and attitudes of dialog participants.
Speech acts prove to be useful for determining dialog segments defined on the
notion of completing expectations of dialog participants. Finally, we sketch
how explicit segmentation signalled by cue phrases and performatives is covered
by our dialog model."
"This document describes a sizable grammar of English written in the TAG
formalism and implemented for use with the XTAG system. This report and the
grammar described herein supersedes the TAG grammar described in an earlier
1995 XTAG technical report. The English grammar described in this report is
based on the TAG formalism which has been extended to include lexicalization,
and unification-based feature structures. The range of syntactic phenomena that
can be handled is large and includes auxiliaries (including inversion), copula,
raising and small clause constructions, topicalization, relative clauses,
infinitives, gerunds, passives, adjuncts, it-clefts, wh-clefts, PRO
constructions, noun-noun modifications, extraposition, determiner sequences,
genitives, negation, noun-verb contractions, sentential adjuncts and
imperatives. This technical report corresponds to the XTAG Release 8/31/98. The
XTAG grammar is continuously updated with the addition of new analyses and
modification of old ones, and an online version of this report can be found at
the XTAG web page at http://www.cis.upenn.edu/~xtag/"
"Language models for speech recognition typically use a probability model of
the form Pr(a_n | a_1, a_2, ..., a_{n-1}). Stochastic grammars, on the other
hand, are typically used to assign structure to utterances. A language model of
the above form is constructed from such grammars by computing the prefix
probability Sum_{w in Sigma*} Pr(a_1 ... a_n w), where w represents all
possible terminations of the prefix a_1 ... a_n. The main result in this paper
is an algorithm to compute such prefix probabilities given a stochastic Tree
Adjoining Grammar (TAG). The algorithm achieves the required computation in
O(n^6) time. The probability of subderivations that do not derive any words in
the prefix, but contribute structurally to its derivation, are precomputed to
achieve termination. This algorithm enables existing corpus-based estimation
techniques for stochastic TAGs to be used for language modelling."
"Much of the power of probabilistic methods in modelling language comes from
their ability to compare several derivations for the same string in the
language. An important starting point for the study of such cross-derivational
properties is the notion of _consistency_. The probability model defined by a
probabilistic grammar is said to be _consistent_ if the probabilities assigned
to all the strings in the language sum to one. From the literature on
probabilistic context-free grammars (CFGs), we know precisely the conditions
which ensure that consistency is true for a given CFG. This paper derives the
conditions under which a given probabilistic Tree Adjoining Grammar (TAG) can
be shown to be consistent. It gives a simple algorithm for checking consistency
and gives the formal justification for its correctness. The conditions derived
here can be used to ensure that probability models that use TAGs can be checked
for _deficiency_ (i.e. whether any probability mass is assigned to strings that
cannot be generated)."
"In this paper we present a new tree-rewriting formalism called Link-Sharing
Tree Adjoining Grammar (LSTAG) which is a variant of synchronous TAGs. Using
LSTAG we define an approach towards coordination where linguistic dependency is
distinguished from the notion of constituency. Such an approach towards
coordination that explicitly distinguishes dependencies from constituency gives
a better formal understanding of its representation when compared to previous
approaches that use tree-rewriting systems which conflate the two issues."
"This paper describes the incremental generation of parse tables for the
LR-type parsing of Tree Adjoining Languages (TALs). The algorithm presented
handles modifications to the input grammar by updating the parser generated so
far. In this paper, a lazy generation of LR-type parsers for TALs is defined in
which parse tables are created by need while parsing. We then describe an
incremental parser generator for TALs which responds to modification of the
input grammar by updating parse tables built so far."
"In this paper we present Morphy, an integrated tool for German morphology,
part-of-speech tagging and context-sensitive lemmatization. Its large lexicon
of more than 320,000 word forms plus its ability to process German compound
nouns guarantee a wide morphological coverage. Syntactic ambiguities can be
resolved with a standard statistical part-of-speech tagger. By using the output
of the tagger, the lemmatizer can determine the correct root even for ambiguous
word forms. The complete package is freely available and can be downloaded from
the World Wide Web."
"The lexical acquisition system presented in this paper incrementally updates
linguistic properties of unknown words inferred from their surrounding context
by parsing sentences with an HPSG grammar for German. We employ a gradual,
information-based concept of ``unknownness'' providing a uniform treatment for
the range of completely known to maximally unknown lexical entries. ``Unknown''
information is viewed as revisable information, which is either generalizable
or specializable. Updating takes place after parsing, which only requires a
modified lexical lookup. Revisable pieces of information are identified by
grammar-specified declarations which provide access paths into the parse
feature structure. The updating mechanism revises the corresponding places in
the lexical feature structures iff the context actually provides new
information. For revising generalizable information, type union is required. A
worked-out example demonstrates the inferential capacity of our implemented
system."
"This paper describes a computational, declarative approach to prosodic
morphology that uses inviolable constraints to denote small finite candidate
sets which are filtered by a restrictive incremental optimization mechanism.
The new approach is illustrated with an implemented fragment of Modern Hebrew
verbs couched in MicroCUF, an expressive constraint logic formalism. For
generation and parsing of word forms, I propose a novel off-line technique to
eliminate run-time optimization. It produces a finite-state oracle that
efficiently restricts the constraint interpreter's search space. As a
byproduct, unknown words can be analyzed without special mechanisms. Unlike
pure finite-state transducer approaches, this hybrid setup allows for more
expressivity in constraints to specify e.g. token identity for reduplication or
arithmetic constraints for phonetics."
"This paper addresses the issue of {\sc pos} tagger evaluation. Such
evaluation is usually performed by comparing the tagger output with a reference
test corpus, which is assumed to be error-free. Currently used corpora contain
noise which causes the obtained performance to be a distortion of the real
value. We analyze to what extent this distortion may invalidate the comparison
between taggers or the measure of the improvement given by a new system. The
main conclusion is that a more rigorous testing experimentation
setting/designing is needed to reliably evaluate and compare tagger accuracies."
"We present a bootstrapping method to develop an annotated corpus, which is
specially useful for languages with few available resources. The method is
being applied to develop a corpus of Spanish of over 5Mw. The method consists
on taking advantage of the collaboration of two different POS taggers. The
cases in which both taggers agree present a higher accuracy and are used to
retrain the taggers."
"We report on two corpora to be used in the evaluation of component systems
for the tasks of (1) linear segmentation of text and (2) summary-directed
sentence extraction. We present characteristics of the corpora, methods used in
the collection of user judgments, and an overview of the application of the
corpora to evaluating the component system. Finally, we discuss the problems
and issues with construction of the test set which apply broadly to the
construction of evaluation resources for language technologies."
"Several methods are known for parsing languages generated by Tree Adjoining
Grammars (TAGs) in O(n^6) worst case running time. In this paper we investigate
which restrictions on TAGs and TAG derivations are needed in order to lower
this O(n^6) time complexity, without introducing large runtime constants, and
without losing any of the generative power needed to capture the syntactic
constructions in natural language that can be handled by unrestricted TAGs. In
particular, we describe an algorithm for parsing a strict subclass of TAG in
O(n^5), and attempt to show that this subclass retains enough generative power
to make it useful in the general case."
"This paper argues that an interlingual representation must explicitly
represent some parts of the meaning of a situation as possibilities (or
preferences), not as necessary or definite components of meaning (or
constraints). Possibilities enable the analysis and generation of nuance,
something required for faithful translation. Furthermore, the representation of
the meaning of words, especially of near-synonyms, is crucial, because it
specifies which nuances words can convey in which contexts."
"This paper presents a partial solution to a component of the problem of
lexical choice: choosing the synonym most typical, or expected, in context. We
apply a new statistical approach to representing the context of a word through
lexical co-occurrence networks. The implementation was trained and evaluated on
a large corpus, and results show that the inclusion of second-order
co-occurrence relations improves the performance of our implemented lexical
choice program."
"In this paper we present the results of comparing a statistical tagger for
German based on decision trees and a rule-based Brill-Tagger for German. We
used the same training corpus (and therefore the same tag-set) to train both
taggers. We then applied the taggers to the same test corpus and compared their
respective behavior and in particular their error rates. Both taggers perform
similarly with an error rate of around 5%. From the detailed error analysis it
can be seen that the rule-based tagger has more problems with unknown words
than the statistical tagger. But the results are opposite for tokens that are
many-ways ambiguous. If the unknown words are fed into the taggers with the
help of an external lexicon (such as the Gertwol system) the error rate of the
rule-based tagger drops to 4.7%, and the respective rate of the statistical
taggers drops to around 3.7%. Combining the taggers by using the output of one
tagger to help the other did not lead to any further improvement."
"The paper presents a language model that develops syntactic structure and
uses it to extract meaningful information from the word history, thus enabling
the use of long distance dependencies. The model assigns probability to every
joint sequence of words--binary-parse-structure with headword annotation and
operates in a left-to-right manner --- therefore usable for automatic speech
recognition. The model, its probabilistic parameterization, and a set of
experiments meant to evaluate its predictive power are presented; an
improvement over standard trigram modeling is achieved."
"The paper presents a language model that develops syntactic structure and
uses it to extract meaningful information from the word history, thus enabling
the use of long distance dependencies. The model assigns probability to every
joint sequence of words - binary-parse-structure with headword annotation. The
model, its probabilistic parametrization, and a set of experiments meant to
evaluate its predictive power are presented."
"In this thesis, I address the problem of automatically acquiring lexical
semantic knowledge, especially that of case frame patterns, from large corpus
data and using the acquired knowledge in structural disambiguation. The
approach I adopt has the following characteristics: (1) dividing the problem
into three subproblems: case slot generalization, case dependency learning, and
word clustering (thesaurus construction). (2) viewing each subproblem as that
of statistical estimation and defining probability models for each subproblem,
(3) adopting the Minimum Description Length (MDL) principle as learning
strategy, (4) employing efficient learning algorithms, and (5) viewing the
disambiguation problem as that of statistical prediction. Major contributions
of this thesis include: (1) formalization of the lexical knowledge acquisition
problem, (2) development of a number of learning methods for lexical knowledge
acquisition, and (3) development of a high-performance disambiguation method."
"There exist several methods of calculating a similarity curve, or a sequence
of similarity values, representing the lexical cohesion of successive text
constituents, e.g., paragraphs. Methods for deciding the locations of fragment
boundaries are, however, scarce. We propose a fragmentation method based on
dynamic programming. The method is theoretically sound and guaranteed to
provide an optimal splitting on the basis of a similarity curve, a preferred
fragment length, and a cost function defined. The method is especially useful
when control on fragment size is of importance."
"In order to support the efficient development of NL generation systems, two
orthogonal methods are currently pursued with emphasis: (1) reusable, general,
and linguistically motivated surface realization components, and (2) simple,
task-oriented template-based techniques. In this paper we argue that, from an
application-oriented perspective, the benefits of both are still limited. In
order to improve this situation, we suggest and evaluate shallow generation
methods associated with increased flexibility. We advise a close connection
between domain-motivated and linguistic ontologies that supports the quick
adaptation to new tasks and domains, rather than the reuse of general
resources. Our method is especially designed for generating reports with
limited linguistic variations."
"Scheduling dialogs, during which people negotiate the times of appointments,
are common in everyday life. This paper reports the results of an in-depth
empirical investigation of resolving explicit temporal references in scheduling
dialogs. There are four phases of this work: data annotation and evaluation,
model development, system implementation and evaluation, and model evaluation
and analysis. The system and model were developed primarily on one set of data,
and then applied later to a much more complex data set, to assess the
generalizability of the model for the task being performed. Many different
types of empirical methods are applied to pinpoint the strengths and weaknesses
of the approach. Detailed annotation instructions were developed and an
intercoder reliability study was performed, showing that naive annotators can
reliably perform the targeted annotations. A fully automatic system has been
developed and evaluated on unseen test data, with good results on both data
sets. We adopt a pure realization of a recency-based focus model to identify
precisely when it is and is not adequate for the task being addressed. In
addition to system results, an in-depth evaluation of the model itself is
presented, based on detailed manual annotations. The results are that few
errors occur specifically due to the model of focus being used, and the set of
anaphoric relations defined in the model are low in ambiguity for both data
sets."
"Treebanks, such as the Penn Treebank (PTB), offer a simple approach to
obtaining a broad coverage grammar: one can simply read the grammar off the
parse trees in the treebank. While such a grammar is easy to obtain, a
square-root rate of growth of the rule set with corpus size suggests that the
derived grammar is far from complete and that much more treebanked text would
be required to obtain a complete grammar, if one exists at some limit. However,
we offer an alternative explanation in terms of the underspecification of
structures within the treebank. This hypothesis is explored by applying an
algorithm to compact the derived grammar by eliminating redundant rules --
rules whose right hand sides can be parsed by other rules. The size of the
resulting compacted grammar, which is significantly less than that of the full
treebank grammar, is shown to approach a limit. However, such a compacted
grammar does not yield very good performance figures. A version of the
compaction algorithm taking rule probabilities into account is proposed, which
is argued to be more linguistically motivated. Combined with simple
thresholding, this method can be used to give a 58% reduction in grammar size
without significant change in parsing performance, and can produce a 69%
reduction with some gain in recall, but a loss in precision."
"The paper argues that Fodor and Lepore are misguided in their attack on
Pustejovsky's Generative Lexicon, largely because their argument rests on a
traditional, but implausible and discredited, view of the lexicon on which it
is effectively empty of content, a view that stands in the long line of
explaining word meaning (a) by ostension and then (b) explaining it by means of
a vacuous symbol in a lexicon, often the word itself after typographic
transmogrification. (a) and (b) both share the wrong belief that to a word must
correspond a simple entity that is its meaning. I then turn to the semantic
rules that Pustejovsky uses and argue first that, although they have novel
features, they are in a well-established Artificial Intelligence tradition of
explaining meaning by reference to structures that mention other structures
assigned to words that may occur in close proximity to the first. It is argued
that Fodor and Lepore's view that there cannot be such rules is without
foundation, and indeed systems using such rules have proved their practical
worth in computational systems. Their justification descends from line of
argument, whose high points were probably Wittgenstein and Quine that meaning
is not to be understood by simple links to the world, ostensive or otherwise,
but by the relationship of whole cultural representational structures to each
other and to the world as a whole."
"This paper compares the tasks of part-of-speech (POS) tagging and
word-sense-tagging or disambiguation (WSD), and argues that the tasks are not
related by fineness of grain or anything like that, but are quite different
kinds of task, particularly becuase there is nothing in POS corresponding to
sense novelty. The paper also argues for the reintegration of sub-tasks that
are being separated for evaluation"
"`Linguistic annotation' covers any descriptive or analytic notations applied
to raw language data. The basic data may be in the form of time functions --
audio, video and/or physiological recordings -- or it may be textual. The added
notations may include transcriptions of all sorts (from phonetic features to
discourse structures), part-of-speech and sense tagging, syntactic analysis,
`named entity' identification, co-reference annotation, and so on. While there
are several ongoing efforts to provide formats and tools for such annotations
and to publish annotated linguistic databases, the lack of widely accepted
standards is becoming a critical problem. Proposed standards, to the extent
they exist, have focussed on file formats. This paper focuses instead on the
logical structure of linguistic annotations. We survey a wide variety of
existing annotation formats and demonstrate a common conceptual core, the
annotation graph. This provides a formal framework for constructing,
maintaining and searching linguistic annotations, while remaining consistent
with many alternative data structures and file formats."
"Recent technological advances have made it possible to build real-time,
interactive spoken dialogue systems for a wide variety of applications.
However, when users do not respect the limitations of such systems, performance
typically degrades. Although users differ with respect to their knowledge of
system limitations, and although different dialogue strategies make system
limitations more apparent to users, most current systems do not try to improve
performance by adapting dialogue behavior to individual users. This paper
presents an empirical evaluation of TOOT, an adaptable spoken dialogue system
for retrieving train schedules on the web. We conduct an experiment in which 20
users carry out 4 tasks with both adaptable and non-adaptable versions of TOOT,
resulting in a corpus of 80 dialogues. The values for a wide range of
evaluation measures are then extracted from this corpus. Our results show that
adaptable TOOT generally outperforms non-adaptable TOOT, and that the utility
of adaptation depends on TOOT's initial dialogue strategies."
"Context sensitive rewrite rules have been widely used in several areas of
natural language processing, including syntax, morphology, phonology and speech
processing. Kaplan and Kay, Karttunen, and Mohri & Sproat have given various
algorithms to compile such rewrite rules into finite-state transducers. The
present paper extends this work by allowing a limited form of backreferencing
in such rules. The explicit use of backreferencing leads to more elegant and
general solutions."
"The two principal areas of natural language processing research in pragmatics
are belief modelling and speech act processing. Belief modelling is the
development of techniques to represent the mental attitudes of a dialogue
participant. The latter approach, speech act processing, based on speech act
theory, involves viewing dialogue in planning terms. Utterances in a dialogue
are modelled as steps in a plan where understanding an utterance involves
deriving the complete plan a speaker is attempting to achieve. However,
previous speech act based approaches have been limited by a reliance upon
relatively simplistic belief modelling techniques and their relationship to
planning and plan recognition. In particular, such techniques assume
precomputed nested belief structures. In this paper, we will present an
approach to speech act processing based on novel belief modelling techniques
where nested beliefs are propagated on demand."
"This paper links prosody to the information in a text and how it is processed
by the speaker. It describes the operation and output of LOQ, a text-to-speech
implementation that includes a model of limited attention and working memory.
Attentional limitations are key. Varying the attentional parameter in the
simulations varies in turn what counts as given and new in a text, and
therefore, the intonational contours with which it is uttered. Currently, the
system produces prosody in three different styles: child-like, adult
expressive, and knowledgeable. This prosody also exhibits differences within
each style -- no two simulations are alike. The limited resource approach
captures some of the stylistic and individual variety found in natural prosody."
"Corpus-based grammar induction generally relies on hand-parsed training data
to learn the structure of the language. Unfortunately, the cost of building
large annotated corpora is prohibitively expensive. This work aims to improve
the induction strategy when there are few labels in the training data. We show
that the most informative linguistic constituents are the higher nodes in the
parse trees, typically denoting complex noun phrases and sentential clauses.
They account for only 20% of all constituents. For inducing grammars from
sparsely labeled training data (e.g., only higher-level constituent labels), we
propose an adaptation strategy, which produces grammars that parse almost as
well as grammars induced from fully labeled corpora. Our results suggest that
for a partial parser to replace human annotators, it must be able to
automatically extract higher-level constituents rather than base noun phrases."
"Particles fullfill several distinct central roles in the Japanese language.
They can mark arguments as well as adjuncts, can be functional or have semantic
funtions. There is, however, no straightforward matching from particles to
functions, as, e.g., GA can mark the subject, the object or an adjunct of a
sentence. Particles can cooccur. Verbal arguments that could be identified by
particles can be eliminated in the Japanese sentence. And finally, in spoken
language particles are often omitted. A proper treatment of particles is thus
necessary to make an analysis of Japanese sentences possible. Our treatment is
based on an empirical investigation of 800 dialogues. We set up a type
hierarchy of particles motivated by their subcategorizational and
modificational behaviour. This type hierarchy is part of the Japanese syntax in
VERBMOBIL."
"This paper presents a new approach to partial parsing of context-free
structures. The approach is based on Markov Models. Each layer of the resulting
structure is represented by its own Markov Model, and output of a lower layer
is passed as input to the next higher layer. An empirical evaluation of the
method yields very good results for NP/PP chunking of German newspaper texts."
"The NWO Priority Programme Language and Speech Technology is a 5-year
research programme aiming at the development of spoken language information
systems. In the Programme, two alternative natural language processing (NLP)
modules are developed in parallel: a grammar-based (conventional, rule-based)
module and a data-oriented (memory-based, stochastic, DOP) module. In order to
compare the NLP modules, a formal evaluation has been carried out three years
after the start of the Programme. This paper describes the evaluation procedure
and the evaluation results. The grammar-based component performs much better
than the data-oriented one in this comparison."
"Grammatical relationships are an important level of natural language
processing. We present a trainable approach to find these relationships through
transformation sequences and error-driven learning. Our approach finds
grammatical relationships between core syntax groups and bypasses much of the
parsing phase. On our training and test set, our procedure achieves 63.6%
recall and 77.3% precision (f-score = 69.8)."
"Previous work in the context of natural language querying of temporal
databases has established a method to map automatically from a large subset of
English time-related questions to suitable expressions of a temporal logic-like
language, called TOP. An algorithm to translate from TOP to the TSQL2 temporal
database language has also been defined. This paper shows how TOP expressions
could be translated into a simpler logic-like language, called BOT. BOT is very
close to traditional first-order predicate logic (FOPL), and hence existing
methods to manipulate FOPL expressions can be exploited to interface to
time-sensitive applications other than TSQL2 databases, maintaining the
existing English-to-TOP mapping."
"This paper explores the automatic construction of a multilingual Lexical
Knowledge Base from pre-existing lexical resources. We present a new and robust
approach for linking already existing lexical/semantic hierarchies. We used a
constraint satisfaction algorithm (relaxation labeling) to select --among all
the candidate translations proposed by a bilingual dictionary-- the right
English WordNet synset for each sense in a taxonomy automatically derived from
a Spanish monolingual dictionary. Although on average, there are 15 possible
WordNet connections for each sense in the taxonomy, the method achieves an
accuracy over 80%. Finally, we also propose several ways in which this
technique could be applied to enrich and improve existing lexical databases."
"We argue that grammatical analysis is a viable alternative to concept
spotting for processing spoken input in a practical spoken dialogue system. We
discuss the structure of the grammar, and a model for robust parsing which
combines linguistic sources of information and statistical sources of
information. We discuss test results suggesting that grammatical processing
allows fast and accurate processing of spoken input."
"We present an approach to Machine Translation that combines the ideas and
methodologies of the Example-Based and Lexicalist theoretical frameworks. The
approach has been implemented in a multilingual Machine Translation system."
"In recent work we have presented a formal framework for linguistic annotation
based on labeled acyclic digraphs. These `annotation graphs' offer a simple yet
powerful method for representing complex annotation structures incorporating
hierarchy and overlap. Here, we motivate and illustrate our approach using
discourse-level annotations of text and speech data drawn from the CALLHOME,
COCONUT, MUC-7, DAMSL and TRAINS annotation schemes. With the help of domain
specialists, we have constructed a hybrid multi-level annotation for a fragment
of the Boston University Radio Speech Corpus which includes the following
levels: segment, word, breath, ToBI, Tilt, Treebank, coreference and named
entity. We show how annotation graphs can represent hybrid multi-level
structures which derive from a diverse set of file formats. We also show how
the approach facilitates substantive comparison of multiple annotations of a
single signal based on different theoretical models. The discussion shows how
annotation graphs open the door to wide-ranging integration of tools, formats
and corpora."
"Dividing sentences in chunks of words is a useful preprocessing step for
parsing, information extraction and information retrieval. (Ramshaw and Marcus,
1995) have introduced a ""convenient"" data representation for chunking by
converting it to a tagging task. In this paper we will examine seven different
data representations for the problem of recognizing noun phrase chunks. We will
show that the the data representation choice has a minor influence on chunking
performance. However, equipped with the most suitable data representation, our
memory-based learning chunker was able to improve the best published chunking
results for a standard data set."
"This paper proposes a Japanese/English cross-language information retrieval
(CLIR) system targeting technical documents. Our system first translates a
given query containing technical terms into the target language, and then
retrieves documents relevant to the translated query. The translation of
technical terms is still problematic in that technical terms are often compound
words, and thus new terms can be progressively created simply by combining
existing base words. In addition, Japanese often represents loanwords based on
its phonogram. Consequently, existing dictionaries find it difficult to achieve
sufficient coverage. To counter the first problem, we use a compound word
translation method, which uses a bilingual dictionary for base words and
collocational statistics to resolve translation ambiguity. For the second
problem, we propose a transliteration method, which identifies phonetic
equivalents in the target language. We also show the effectiveness of our
system using a test collection for CLIR."
"In this paper we present an application of explanation-based learning (EBL)
in the parsing module of a real-time English-Spanish machine translation system
designed to translate closed captions. We discuss the efficiency/coverage
trade-offs available in EBL and introduce the techniques we use to increase
coverage while maintaining a high level of space and time efficiency. Our
performance results indicate that this approach is effective."
"A statistical classification algorithm and its application to language
identification from noisy input are described. The main innovation is to
compute confidence limits on the classification, so that the algorithm
terminates when enough evidence to make a clear decision has been made, and so
avoiding problems with categories that have similar characteristics. A second
application, to genre identification, is briefly examined. The results show
that some of the problems of other language identification techniques can be
avoided, and illustrate a more important point: that a statistical language
process can be used to provide feedback about its own success rate."
"We propose a parser for constraint-logic grammars implementing HPSG that
combines the advantages of dynamic bottom-up and advanced top-down control. The
parser allows the user to apply magic compilation to specific constraints in a
grammar which as a result can be processed dynamically in a bottom-up and
goal-directed fashion. State of the art top-down processing techniques are used
to deal with the remaining constraints. We discuss various aspects concerning
the implementation of the parser as part of a grammar development system."
"We describe a recently developed corpus annotation scheme for evaluating
parsers that avoids shortcomings of current methods. The scheme encodes
grammatical relations between heads and dependents, and has been used to mark
up a new public-domain corpus of naturally occurring English text. We show how
the corpus can be used to evaluate the accuracy of a robust parser, and relate
the corpus to extant resources."
"We describe a method for automatically generating Lexical Transfer Rules
(LTRs) from word equivalences using transfer rule templates. Templates are
skeletal LTRs, unspecified for words. New LTRs are created by instantiating a
template with words, provided that the words belong to the appropriate lexical
categories required by the template. We define two methods for creating an
inventory of templates and using them to generate new LTRs. A simpler method
consists of extracting a finite set of templates from a sample of hand coded
LTRs and directly using them in the generation process. A further method
consists of abstracting over the initial finite set of templates to define
higher level templates, where bilingual equivalences are defined in terms of
correspondences involving phrasal categories. Phrasal templates are then mapped
onto sets of lexical templates with the aid of grammars. In this way an
infinite set of lexical templates is recursively defined. New LTRs are created
by parsing input words, matching a template at the phrasal level and using the
corresponding lexical categories to instantiate the lexical template. The
definition of an infinite set of templates enables the automatic creation of
LTRs for multi-word, non-compositional word equivalences of any cardinality."
"The paper describes the speech to speech translation system INTARC, developed
during the first phase of the Verbmobil project. The general design goals of
the INTARC system architecture were time synchronous processing as well as
incrementality and interactivity as a means to achieve a higher degree of
robustness and scalability. Interactivity means that in addition to the
bottom-up (in terms of processing levels) data flow the ability to process
top-down restrictions considering the same signal segment for all processing
levels. The construction of INTARC 2.0, which has been operational since fall
1996, followed an engineering approach focussing on the integration of symbolic
(linguistic) and stochastic (recognition) techniques which led to a
generalization of the concept of a ``one pass'' beam search."
"This paper addresses a novel task of detecting sub-topic correspondence in a
pair of text fragments, enhancing common notions of text similarity. This task
is addressed by coupling corresponding term subsets through bipartite
clustering. The paper presents a cost-based clustering scheme and compares it
with a bipartite version of the single-link method, providing illustrating
results."
"This paper describes how robust parsing techniques can be fruitful applied
for building a query generation module which is part of a pipelined NLP
architecture aimed at process natural language queries in a restricted domain.
We want to show that semantic robustness represents a key issue in those NLP
systems where it is more likely to have partial and ill-formed utterances due
to various factors (e.g. noisy environments, low quality of speech recognition
modules, etc...) and where it is necessary to succeed, even if partially, in
extracting some meaningful information."
"This paper proposes an efficient example sampling method for example-based
word sense disambiguation systems. To construct a database of practical size, a
considerable overhead for manual sense disambiguation (overhead for
supervision) is required. In addition, the time complexity of searching a
large-sized database poses a considerable problem (overhead for search). To
counter these problems, our method selectively samples a smaller-sized
effective subset from a given example set for use in word sense disambiguation.
Our method is characterized by the reliance on the notion of training utility:
the degree to which each example is informative for future example sampling
when used for the training of the system. The system progressively collects
examples by selecting those with greatest utility. The paper reports the
effectiveness of our method through experiments on about one thousand
sentences. Compared to experiments with other example sampling methods, our
method reduced both the overhead for supervision and the overhead for search,
without the degeneration of the performance of the system."
"Several methods are discussed that construct a finite automaton given a
context-free grammar, including both methods that lead to subsets and those
that lead to supersets of the original context-free language. Some of these
methods of regular approximation are new, and some others are presented here in
a more refined form with respect to existing literature. Practical experiments
with the different methods of regular approximation are performed for
spoken-language input: hypotheses from a speech recognizer are filtered through
a finite automaton."
"Question answering task is now being done in TREC8 using English documents.
We examined question answering task in Japanese sentences. Our method selects
the answer by matching the question sentence with knowledge-based data written
in natural language. We use syntactic information to obtain highly accurate
answers."
"Recent developments in theoretical linguistics have lead to a widespread
acceptance of constraint-based analyses of prosodic morphology phenomena such
as truncation, infixation, floating morphemes and reduplication. Of these,
reduplication is particularly challenging for state-of-the-art computational
morphology, since it involves copying of some part of a phonological string. In
this paper I argue for certain extensions to the one-level model of phonology
and morphology (Bird & Ellison 1994) to cover the computational aspects of
prosodic morphology using finite-state methods. In a nutshell, enriched lexical
representations provide additional automaton arcs to repeat or skip sounds and
also to allow insertion of additional material. A kind of resource
consciousness is introduced to control this additional freedom, distinguishing
between producer and consumer arcs. The non-finite-state copying aspect of
reduplication is mapped to automata intersection, itself a non-finite-state
operation. Bounded local optimization prunes certain automaton arcs that fail
to contribute to linguistic optimisation criteria. The paper then presents
implemented case studies of Ulwa construct state infixation, German
hypocoristic truncation and Tagalog over-applying reduplication that illustrate
the expressive power of this approach, before its merits and limitations are
discussed and possible extensions are sketched. I conclude that the one-level
approach to prosodic morphology presents an attractive way of extending
finite-state techniques to difficult phenomena that hitherto resisted elegant
computational analyses."
"A noun phrase can indirectly refer to an entity that has already been
mentioned. For example, ``I went into an old house last night. The roof was
leaking badly and ...'' indicates that ``the roof'' is associated with `` an
old house}'', which was mentioned in the previous sentence. This kind of
reference (indirect anaphora) has not been studied well in natural language
processing, but is important for coherence resolution, language understanding,
and machine translation. In order to analyze indirect anaphora, we need a case
frame dictionary for nouns that contains knowledge of the relationships between
two nouns but no such dictionary presently exists. Therefore, we are forced to
use examples of ``X no Y'' (Y of X) and a verb case frame dictionary instead.
We tried estimating indirect anaphora using this information and obtained a
recall rate of 63% and a precision rate of 68% on test sentences. This
indicates that the information of ``X no Y'' is useful to a certain extent when
we cannot make use of a noun case frame dictionary. We estimated the results
that would be given by a noun case frame dictionary, and obtained recall and
precision rates of 71% and 82% respectively. Finally, we proposed a way to
construct a noun case frame dictionary by using examples of ``X no Y.''"
"In this paper, we present a method of estimating referents of demonstrative
pronouns, personal pronouns, and zero pronouns in Japanese sentences using
examples, surface expressions, topics and foci. Unlike conventional work which
was semantic markers for semantic constraints, we used examples for semantic
constraints and showed in our experiments that examples are as useful as
semantic markers. We also propose many new methods for estimating referents of
pronouns. For example, we use the form ``X of Y'' for estimating referents of
demonstrative adjectives. In addition to our new methods, we used many
conventional methods. As a result, experiments using these methods obtained a
precision rate of 87% in estimating referents of demonstrative pronouns,
personal pronouns, and zero pronouns for training sentences, and obtained a
precision rate of 78% for test sentences."
"In machine translation and man-machine dialogue, it is important to clarify
referents of noun phrases. We present a method for determining the referents of
noun phrases in Japanese sentences by using the referential properties,
modifiers, and possessors of noun phrases. Since the Japanese language has no
articles, it is difficult to decide whether a noun phrase has an antecedent or
not. We had previously estimated the referential properties of noun phrases
that correspond to articles by using clue words in the sentences. By using
these referential properties, our system determined the referents of noun
phrases in Japanese sentences. Furthermore we used the modifiers and possessors
of noun phrases in determining the referents of noun phrases. As a result, on
training sentences we obtained a precision rate of 82% and a recall rate of 85%
in the determination of the referents of noun phrases that have antecedents. On
test sentences, we obtained a precision rate of 79% and a recall rate of 77%."
"Verbs are sometimes omitted in Japanese sentences. It is necessary to recover
omitted verbs for purposes of language understanding, machine translation, and
conversational processing. This paper describes a practical way to recover
omitted verbs by using surface expressions and examples. We experimented the
resolution of verb ellipses by using this information, and obtained a recall
rate of 73% and a precision rate of 66% on test sentences."
"We have developed a new method for Japanese-to-English translation of tense,
aspect, and modality that uses an example-based method. In this method the
similarity between input and example sentences is defined as the degree of
semantic matching between the expressions at the ends of the sentences. Our
method also uses the k-nearest neighbor method in order to exclude the effects
of noise; for example, wrongly tagged data in the bilingual corpora.
Experiments show that our method can translate tenses, aspects, and modalities
more accurately than the top-level MT software currently available on the
market can. Moreover, it does not require hand-craft rules."
"A system is described that uses a mixed-level representation of (part of)
meaning of natural language documents (based on standard Horn Clause Logic) and
a variable-depth search strategy that distinguishes between the different
levels of abstraction in the knowledge representation to locate specific
passages in the documents. Mixed-level representations as well as
variable-depth search strategies are applicable in fields outside that of NLP."
"A system is described that uses a mixed-level knowledge representation based
on standard Horn Clause Logic to represent (part of) the meaning of natural
language documents. A variable-depth search strategy is outlined that
distinguishes between the different levels of abstraction in the knowledge
representation to locate specific passages in the documents. A detailed
description of the linguistic aspects of the system is given. Mixed-level
representations as well as variable-depth search strategies are applicable in
fields outside that of NLP."
"In this paper we describe ExtrAns, an answer extraction system. Answer
extraction (AE) aims at retrieving those exact passages of a document that
directly answer a given user question. AE is more ambitious than information
retrieval and information extraction in that the retrieval results are phrases,
not entire documents, and in that the queries may be arbitrarily specific. It
is less ambitious than full-fledged question answering in that the answers are
not generated from a knowledge base but looked up in the text of documents. The
current version of ExtrAns is able to parse unedited Unix ""man pages"", and
derive the logical form of their sentences. User queries are also translated
into logical forms. A theorem prover then retrieves the relevant phrases, which
are presented through selective highlighting in their context."
"We study distributional similarity measures for the purpose of improving
probability estimation for unseen cooccurrences. Our contributions are
three-fold: an empirical comparison of a broad range of measures; a
classification of similarity functions based on the information that they
incorporate; and the introduction of a novel function that is superior at
evaluating potential proxy distributions."
"The thesis presents an attempt at using the syntactic structure in natural
language for improved language models for speech recognition. The structured
language model merges techniques in automatic parsing and language modeling
using an original probabilistic parameterization of a shift-reduce parser. A
maximum likelihood reestimation procedure belonging to the class of
expectation-maximization algorithms is employed for training the model.
Experiments on the Wall Street Journal, Switchboard and Broadcast News corpora
show improvement in both perplexity and word error rate - word lattice
rescoring - over the standard 3-gram language model. The significance of the
thesis lies in presenting an original approach to language modeling that uses
the hierarchical - syntactic - structure in natural language to improve on
current 3-gram modeling techniques for large vocabulary speech recognition."
"A new language model for speech recognition inspired by linguistic analysis
is presented. The model develops hidden hierarchical structure incrementally
and uses it to extract meaningful information from the word history - thus
enabling the use of extended distance dependencies - in an attempt to
complement the locality of currently used n-gram Markov models. The model, its
probabilistic parametrization, a reestimation algorithm for the model
parameters and a set of experiments meant to evaluate its potential for speech
recognition are presented."
"A new language model for speech recognition inspired by linguistic analysis
is presented. The model develops hidden hierarchical structure incrementally
and uses it to extract meaningful information from the word history - thus
enabling the use of extended distance dependencies - in an attempt to
complement the locality of currently used trigram models. The structured
language model, its probabilistic parameterization and performance in a
two-pass speech recognizer are presented. Experiments on the SWITCHBOARD corpus
show an improvement in both perplexity and word error rate over conventional
trigram models."
"A new language model for speech recognition is presented. The model develops
hidden hierarchical syntactic-like structure incrementally and uses it to
extract meaningful information from the word history, thus complementing the
locality of currently used trigram models. The structured language model (SLM)
and its performance in a two-pass speech recognizer --- lattice decoding ---
are presented. Experiments on the WSJ corpus show an improvement in both
perplexity (PPL) and word error rate (WER) over conventional trigram models."
"As text processing systems expand in scope, they will require ever larger
lexicons along with a parsing capability for discriminating among many senses
of a word. Existing systems do not incorporate such subtleties in meaning for
their lexicons. Ordinary dictionaries contain such information, but are largely
untapped. When the contents of dictionaries are scrutinized, they reveal many
requirements that must be satisfied in representing meaning and in developing
semantic parsers. These requirements were identified in research designed to
find primitive verb concepts. The requirements are outlined and general
procedures for satisfying them through the use of ordinary dictionaries are
described, illustrated by building frames for and examining the definitions of
""change"" and its uses as a hypernym in other definitions."
"A new word usage measure is proposed. It is based on psychophysical relations
and allows to reveal words by its degree of ""importance"" for making basic
dictionaries of sublanguages."
"Trigrams'n'Tags (TnT) is an efficient statistical part-of-speech tagger.
Contrary to claims found elsewhere in the literature, we argue that a tagger
based on Markov models performs at least as well as other current approaches,
including the Maximum Entropy framework. A recent comparison has even shown
that TnT performs significantly better for the tested corpora. We describe the
basic model of TnT, the techniques used for smoothing and for handling unknown
words. Furthermore, we present evaluations on two corpora."
"Customer care in technical domains is increasingly based on e-mail
communication, allowing for the reproduction of approved solutions. Identifying
the customer's problem is often time-consuming, as the problem space changes if
new products are launched. This paper describes a new approach to the
classification of e-mail requests based on shallow text processing and machine
learning techniques. It is implemented within an assistance system for call
center agents that is used in a commercial setting."
"A finite-state method, based on leftmost longest-match replacement, is
presented for segmenting words into graphemes, and for converting graphemes
into phonemes. A small set of hand-crafted conversion rules for Dutch achieves
a phoneme accuracy of over 93%. The accuracy of the system is further improved
by using transformation-based learning. The phoneme accuracy of the best system
(using a large set of rule templates and a `lazy' variant of Brill's algoritm),
trained on only 40K words, reaches 99% accuracy."
"The rate of occurrence of words is not uniform but varies from document to
document. Despite this observation, parameters for conventional n-gram language
models are usually derived using the assumption of a constant word rate. In
this paper we investigate the use of variable word rate assumption, modelled by
a Poisson distribution or a continuous mixture of Poissons. We present an
approach to estimating the relative frequencies of words or n-grams taking
prior information of their occurrences into account. Discounting and smoothing
schemes are also considered. Using the Broadcast News task, the approach
demonstrates a reduction of perplexity up to 10%."
"This paper describes a method for linear text segmentation which is twice as
accurate and over seven times as fast as the state-of-the-art (Reynar, 1998).
Inter-sentence similarity is replaced by rank in the local context. Boundary
locations are discovered by divisive clustering."
"This paper discusses the development of trainable statistical models for
extracting content from television and radio news broadcasts. In particular we
concentrate on statistical finite state models for identifying proper names and
other named entities in broadcast speech. Two models are presented: the first
represents name class information as a word attribute; the second represents
both word-word and class-class transitions explicitly. A common n-gram based
formulation is used for both models. The task of named entity identification is
characterized by relatively sparse training data and issues related to
smoothing are discussed. Experiments are reported using the DARPA/NIST Hub-4E
evaluation for North American Broadcast News."
"This paper is aimed at reporting on the development and application of a
computer model for discourse analysis through segmentation. Segmentation refers
to the principled division of texts into contiguous constituents. Other studies
have looked at the application of a number of models to the analysis of
discourse by computer. The segmentation procedure developed for the present
investigation is called LSM ('Link Set Median'). It was applied to three corpus
of 300 texts from three different genres. The results obtained by application
of the LSM procedure on the corpus were then compared to segmentation carried
out at random. Statistical analyses suggested that LSM significantly
outperformed random segmentation, thus indicating that the segmentation was
meaningful."
"This paper presents a corpus-based approach to word sense disambiguation that
builds an ensemble of Naive Bayesian classifiers, each of which is based on
lexical features that represent co--occurring words in varying sized windows of
context. Despite the simplicity of this approach, empirical results
disambiguating the widely studied nouns line and interest show that such an
ensemble achieves accuracy rivaling the best previously published results."
"The performance of machine learning algorithms can be improved by combining
the output of different systems. In this paper we apply this idea to the
recognition of noun phrases.We generate different classifiers by using
different representations of the data. By combining the results with voting
techniques described in (Van Halteren et.al. 1998) we manage to improve the
best reported performances on standard data sets for base noun phrases and
arbitrary noun phrases."
"This paper explores the usefulness of a technique from software engineering,
namely code instrumentation, for the development of large-scale natural
language grammars. Information about the usage of grammar rules in test
sentences is used to detect untested rules, redundant test sentences, and
likely causes of overgeneration. Results show that less than half of a
large-coverage grammar for German is actually tested by two large testsuites,
and that 10-30% of testing time is redundant. The methodology applied can be
seen as a re-use of grammar writing knowledge for testsuite compilation."
"This paper reports on the scalability of the answer extraction system
ExtrAns. An answer extraction system locates the exact phrases in the documents
that contain the explicit answers to the user queries. Answer extraction
systems are therefore more convenient than document retrieval systems in
situations where the user wants to find specific information in limited time.
  ExtrAns performs answer extraction over UNIX manpages. It has been
constructed by combining available linguistic resources and implementing only a
few modules from scratch. A resolution procedure between the minimal logical
form of the user query and the minimal logical forms of the manpage sentences
finds the answers to the queries. These answers are displayed to the user,
together with pointers to the respective manpages, and the exact phrases that
contribute to the answer are highlighted.
  This paper shows that the increase in response times is not a big issue when
scaling the system up from 30 to 500 documents, and that the response times for
500 documents are still acceptable for a real-time answer extraction system."
"Reduplication, a central instance of prosodic morphology, is particularly
challenging for state-of-the-art computational morphology, since it involves
copying of some part of a phonological string. In this paper I advocate a
finite-state method that combines enriched lexical representations via
intersection to implement the copying. The proposal includes a
resource-conscious variant of automata and can benefit from the existence of
lazy algorithms. Finally, the implementation of a complex case from Koasati is
presented."
"In this paper, we describe a system to rank suspected answers to natural
language questions. We process both corpus and query using a new technique,
predictive annotation, which augments phrases in texts with labels anticipating
their being targets of certain kinds of questions. Given a natural language
question, an IR system returns a set of matching passages, which are then
analyzed and ranked according to various criteria described in this paper. We
provide an evaluation of the techniques based on results from the TREC Q&A
evaluation in which our system participated."
"Three state-of-the-art statistical parsers are combined to produce more
accurate parses, as well as new bounds on achievable Treebank parsing accuracy.
Two general approaches are presented and two combination techniques are
described for each approach. Both parametric and non-parametric models are
explored. The resulting parsers surpass the best previously published
performance results for the Penn Treebank."
"Bagging and boosting, two effective machine learning techniques, are applied
to natural language parsing. Experiments using these techniques with a
trainable statistical parser are described. The best resulting system provides
roughly as large of a gain in F-measure as doubling the corpus size. Error
analysis of the result of the boosting technique reveals some inconsistent
annotations in the Penn Treebank, suggesting a semi-automatic method for
finding inconsistent treebank annotations."
"The popularity of applying machine learning methods to computational
linguistics problems has produced a large supply of trainable natural language
processing systems. Most problems of interest have an array of off-the-shelf
products or downloadable code implementing solutions using various techniques.
Where these solutions are developed independently, it is observed that their
errors tend to be independently distributed. This thesis is concerned with
approaches for capitalizing on this situation in a sample problem domain, Penn
Treebank-style parsing.
  The machine learning community provides techniques for combining outputs of
classifiers, but parser output is more structured and interdependent than
classifications. To address this discrepancy, two novel strategies for
combining parsers are used: learning to control a switch between parsers and
constructing a hybrid parse from multiple parsers' outputs.
  Off-the-shelf parsers are not developed with an intention to perform well in
a collaborative ensemble. Two techniques are presented for producing an
ensemble of parsers that collaborate. All of the ensemble members are created
using the same underlying parser induction algorithm, and the method for
producing complementary parsers is only loosely constrained by that chosen
algorithm."
"We describe an architecture for implementing spoken natural language dialogue
interfaces to semi-autonomous systems, in which the central idea is to
transform the input speech signal through successive levels of representation
corresponding roughly to linguistic knowledge, dialogue knowledge, and domain
knowledge. The final representation is an executable program in a simple
scripting language equivalent to a subset of Cshell. At each stage of the
translation process, an input is transformed into an output, producing as a
byproduct a ""meta-output"" which describes the nature of the transformation
performed. We show how consistent use of the output/meta-output distinction
permits a simple and perspicuous treatment of apparently diverse topics
including resolution of pronouns, correction of user misconceptions, and
optimization of scripts. The methods described have been concretely realized in
a prototype speech interface to a simulation of the Personal Satellite
Assistant."
"We describe an architecture for spoken dialogue interfaces to semi-autonomous
systems that transforms speech signals through successive representations of
linguistic, dialogue, and domain knowledge. Each step produces an output, and a
meta-output describing the transformation, with an executable program in a
simple scripting language as the final result. The output/meta-output
distinction permits perspicuous treatment of diverse tasks such as resolving
pronouns, correcting user misconceptions, and optimizing scripts."
"When people develop something intended as a large broad-coverage grammar,
they usually have a more specific goal in mind. Sometimes this goal is covering
a corpus; sometimes the developers have theoretical ideas they wish to
investigate; most often, work is driven by a combination of these two main
types of goal. What tends to happen after a while is that the community of
people working with the grammar starts thinking of some phenomena as
``central'', and makes serious efforts to deal with them; other phenomena are
labelled ``marginal'', and ignored. Before long, the distinction between
``central'' and ``marginal'' becomes so ingrained that it is automatic, and
people virtually stop thinking about the ``marginal'' phenomena. In practice,
the only way to bring the marginal things back into focus is to look at what
other people are doing and compare it with one's own work. In this paper, we
will take two large grammars, XTAG and the CLE, and examine each of them from
the other's point of view. We will find in both cases not only that important
things are missing, but that the perspective offered by the other grammar
suggests simple and practical ways of filling in the holes. It turns out that
there is a pleasing symmetry to the picture. XTAG has a very good treatment of
complement structure, which the CLE to some extent lacks; conversely, the CLE
offers a powerful and general account of adjuncts, which the XTAG grammar does
not fully duplicate. If we examine the way in which each grammar does the thing
it is good at, we find that the relevant methods are quite easy to port to the
other framework, and in fact only involve generalization and systematization of
existing mechanisms."
"Systems now exist which are able to compile unification grammars into
language models that can be included in a speech recognizer, but it is so far
unclear whether non-trivial linguistically principled grammars can be used for
this purpose. We describe a series of experiments which investigate the
question empirically, by incrementally constructing a grammar and discovering
what problems emerge when successively larger versions are compiled into finite
state graph representations and used as language models for a medium-vocabulary
recognition task."
"We describe a statistical approach for modeling dialogue acts in
conversational speech, i.e., speech-act-like units such as Statement, Question,
Backchannel, Agreement, Disagreement, and Apology. Our model detects and
predicts dialogue acts based on lexical, collocational, and prosodic cues, as
well as on the discourse coherence of the dialogue act sequence. The dialogue
model is based on treating the discourse structure of a conversation as a
hidden Markov model and the individual dialogue acts as observations emanating
from the model states. Constraints on the likely sequence of dialogue acts are
modeled via a dialogue act n-gram. The statistical dialogue grammar is combined
with word n-grams, decision trees, and neural networks modeling the
idiosyncratic lexical and prosodic manifestations of each dialogue act. We
develop a probabilistic integration of speech recognition with dialogue
modeling, to improve both speech recognition and dialogue act classification
accuracy. Models are trained and evaluated using a large hand-labeled database
of 1,155 conversations from the Switchboard corpus of spontaneous
human-to-human telephone speech. We achieved good dialogue act labeling
accuracy (65% based on errorful, automatically recognized words and prosody,
and 71% based on word transcripts, compared to a chance baseline accuracy of
35% and human accuracy of 84%) and a small reduction in word recognition error."
"Identifying whether an utterance is a statement, question, greeting, and so
forth is integral to effective automatic understanding of natural dialog.
Little is known, however, about how such dialog acts (DAs) can be automatically
classified in truly natural conversation. This study asks whether current
approaches, which use mainly word information, could be improved by adding
prosodic information. The study is based on more than 1000 conversations from
the Switchboard corpus. DAs were hand-annotated, and prosodic features
(duration, pause, F0, energy, and speaking rate) were automatically extracted
for each DA. In training, decision trees based on these features were inferred;
trees were then applied to unseen test data to evaluate performance.
Performance was evaluated for prosody models alone, and after combining the
prosody models with word information -- either from true words or from the
output of an automatic speech recognizer. For an overall classification task,
as well as three subtasks, prosody made significant contributions to
classification. Feature-specific analyses further revealed that although
canonical features (such as F0 for questions) were important, less obvious
features could compensate if canonical features were removed. Finally, in each
task, integrating the prosodic model with a DA-specific statistical language
model improved performance over that of the language model alone, especially
for the case of recognized words. Results suggest that DAs are redundantly
marked in natural conversation, and that a variety of automatically extractable
prosodic features could aid dialog processing in speech applications."
"A criterion for pruning parameters from N-gram backoff language models is
developed, based on the relative entropy between the original and the pruned
model. It is shown that the relative entropy resulting from pruning a single
N-gram can be computed exactly and efficiently for backoff models. The relative
entropy measure can be expressed as a relative change in training set
perplexity. This leads to a simple pruning criterion whereby all N-grams that
change perplexity by less than a threshold are removed from the model.
Experiments show that a production-quality Hub4 LM can be reduced to 26% its
original size without increasing recognition error. We also compare the
approach to a heuristic pruning criterion by Seymore and Rosenfeld (1996), and
show that their approach can be interpreted as an approximation to the relative
entropy criterion. Experimentally, both approaches select similar sets of
N-grams (about 85% overlap), with the exact relative entropy criterion giving
marginally better performance."
"We present three systems for surface natural language generation that are
trainable from annotated corpora. The first two systems, called NLG1 and NLG2,
require a corpus marked only with domain-specific semantic attributes, while
the last system, called NLG3, requires a corpus marked with both semantic
attributes and syntactic dependency information. All systems attempt to produce
a grammatical natural language phrase from a domain-specific semantic
representation. NLG1 serves a baseline system and uses phrase frequencies to
generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy
probability models to individually generate each word in the phrase. The
systems NLG2 and NLG3 learn to determine both the word choice and the word
order of the phrase. We present experiments in which we generate phrases to
describe flights in the air travel domain."
"A crucial step in processing speech audio data for information extraction,
topic detection, or browsing/playback is to segment the input into sentence and
topic units. Speech segmentation is challenging, since the cues typically
present for segmenting text (headers, paragraphs, punctuation) are absent in
spoken language. We investigate the use of prosody (information gleaned from
the timing and melody of speech) for these tasks. Using decision tree and
hidden Markov modeling techniques, we combine prosodic cues with word-based
approaches, and evaluate performance on two speech corpora, Broadcast News and
Switchboard. Results show that the prosodic model alone performs on par with,
or better than, word-based statistical language models -- for both true and
automatically recognized words in news speech. The prosodic model achieves
comparable performance with significantly less training data, and requires no
hand-labeling of prosodic events. Across tasks and corpora, we obtain a
significant improvement over word-only models using a probabilistic combination
of prosodic and lexical information. Inspection reveals that the prosodic
models capture language-independent boundary indicators described in the
literature. Finally, cue usage is task and corpus dependent. For example, pause
and pitch features are highly informative for segmenting news speech, whereas
pause, duration and word-based cues dominate for natural conversation."
"Previous work (Frank and Satta 1998; Karttunen, 1998) has shown that
Optimality Theory with gradient constraints generally is not finite state. A
new finite-state treatment of gradient constraints is presented which improves
upon the approximation of Karttunen (1998). The method turns out to be exact,
and very compact, for the syllabification analysis of Prince and Smolensky
(1993)."
"Finite-state morphology in the general tradition of the Two-Level and Xerox
implementations has proved very successful in the production of robust
morphological analyzer-generators, including many large-scale commercial
systems. However, it has long been recognized that these implementations have
serious limitations in handling non-concatenative phenomena. We describe a new
technique for constructing finite-state transducers that involves reapplying
the regular-expression compiler to its own output. Implemented in an algorithm
called compile-replace, this technique has proved useful for handling
non-concatenative phenomena; and we demonstrate it on Malay full-stem
reduplication and Arabic stem interdigitation."
"In this paper, we describe a new method for constructing minimal,
deterministic, acyclic finite-state automata from a set of strings. Traditional
methods consist of two phases: the first to construct a trie, the second one to
minimize it. Our approach is to construct a minimal automaton in a single phase
by adding new strings one by one and minimizing the resulting automaton
on-the-fly. We present a general algorithm as well as a specialization that
relies upon the lexicographical ordering of the input strings."
"This paper describes a new method, Combi-bootstrap, to exploit existing
taggers and lexical resources for the annotation of corpora with new tagsets.
Combi-bootstrap uses existing resources as features for a second level machine
learning module, that is trained to make the mapping to the new tagset on a
very small sample of annotated corpus material. Experiments show that
Combi-bootstrap: i) can integrate a wide variety of existing resources, and ii)
achieves much higher accuracy (up to 44.7 % error reduction) than both the best
single tagger and an ensemble tagger constructed out of the same small training
sample."
"We describe a formal model for annotating linguistic artifacts, from which we
derive an application programming interface (API) to a suite of tools for
manipulating these annotations. The abstract logical model provides for a range
of storage formats and promotes the reuse of tools that interact through this
API. We focus first on ``Annotation Graphs,'' a graph model for annotations on
linear signals (such as text and speech) indexed by intervals, for which
efficient database storage and querying techniques are applicable. We note how
a wide range of existing annotated corpora can be mapped to this annotation
graph model. This model is then generalized to encompass a wider variety of
linguistic ``signals,'' including both naturally occuring phenomena (as
recorded in images, video, multi-modal interactions, etc.), as well as the
derived resources that are increasingly important to the engineering of natural
language processing systems (such as word lists, dictionaries, aligned
bilingual corpora, etc.). We conclude with a review of the current efforts
towards implementing key pieces of this architecture."
"This paper discusses the challenges that arise when large speech corpora
receive an ever-broadening range of diverse and distinct annotations. Two case
studies of this process are presented: the Switchboard Corpus of telephone
conversations and the TDT2 corpus of broadcast news. Switchboard has undergone
two independent transcriptions and various types of additional annotation, all
carried out as separate projects that were dispersed both geographically and
chronologically. The TDT2 corpus has also received a variety of annotations,
but all directly created or managed by a core group. In both cases, issues
arise involving the propagation of repairs, consistency of references, and the
ability to integrate annotations having different formats and levels of detail.
We describe a general framework whereby these issues can be addressed
successfully."
"A model of rank polysemantic distribution with a minimal number of fitting
parameters is offered. In an ideal case a parameter-free description of the
dependence on the basis of one or several immediate features of the
distribution is possible."
"We present a robust approach for linking already existing lexical/semantic
hierarchies. We used a constraint satisfaction algorithm (relaxation labeling)
to select --among a set of candidates-- the node in a target taxonomy that
bests matches each node in a source taxonomy. In particular, we use it to map
the nominal part of WordNet 1.5 onto WordNet 1.6, with a very high precision
and a very low remaining ambiguity."
"Formal language techniques have been used in the past to study autonomous
dynamical systems. However, for controlled systems, new features are needed to
distinguish between information generated by the system and input control. We
show how the modelling framework for controlled dynamical systems leads
naturally to a formulation in terms of context-dependent grammars. A learning
algorithm is proposed for on-line generation of the grammar productions, this
formulation being then used for modelling, control and anomaly detection.
Practical applications are described for electromechanical drives. Grammatical
interpolation techniques yield accurate results and the pattern detection
capabilities of the language-based formulation makes it a promising technique
for the early detection of anomalies or faulty behaviour."
"Constraint-based grammars can, in principle, serve as the major linguistic
knowledge source for both parsing and generation. Surface generation starts
from input semantics representations that may vary across grammars. For many
declarative grammars, the concept of derivation implicitly built in is that of
parsing. They may thus not be interpretable by a generation algorithm. We show
that linguistically plausible semantic analyses can cause severe problems for
semantic-head-driven approaches for generation (SHDG). We use SeReal, a variant
of SHDG and the DISCO grammar of German as our source of examples. We propose a
new, general approach that explicitly accounts for the interface between the
grammar and the generation algorithm by adding a control-oriented layer to the
linguistic knowledge base that reorganizes the semantics in a way suitable for
generation."
"Grammatical relationships (GRs) form an important level of natural language
processing, but different sets of GRs are useful for different purposes.
Therefore, one may often only have time to obtain a small training corpus with
the desired GR annotations. On such a small training corpus, we compare two
systems. They use different learning techniques, but we find that this
difference by itself only has a minor effect. A larger factor is that in
English, a different GR length measure appears better suited for finding simple
argument GRs than for finding modifier GRs. We also find that partitioning the
data may help memory-based learning."
"Statistical significance testing of differences in values of metrics like
recall, precision and balanced F-score is a necessary part of empirical natural
language processing. Unfortunately, we find in a set of experiments that many
commonly used tests often underestimate the significance and so are less likely
to detect differences that exist between different techniques. This
underestimation comes from an independence assumption that is often violated.
We point out some useful tests that do not make this assumption, including
computationally-intensive randomization tests."
"We present methods for evaluating human and automatic taggers that extend
current practice in three ways. First, we show how to evaluate taggers that
assign multiple tags to each test instance, even if they do not assign
probabilities. Second, we show how to accommodate a common property of manually
constructed ``gold standards'' that are typically used for objective
evaluation, namely that there is often more than one correct answer. Third, we
show how to measure performance when the set of possible tags is
tree-structured in an IS-A hierarchy. To illustrate how our methods can be used
to measure inter-annotator agreement, we show how to compute the kappa
coefficient over hierarchical tag sets."
"We use seven machine learning algorithms for one task: identifying base noun
phrases. The results have been processed by different system combination
methods and all of these outperformed the best individual result. We have
applied the seven learners with the best combinator, a majority vote of the top
five systems, to a standard data set and managed to improve the best published
result for this data set."
"We apply rule induction, classifier combination and meta-learning (stacked
classifiers) to the problem of bootstrapping high accuracy automatic annotation
of corpora with pronunciation information. The task we address in this paper
consists of generating phonemic representations reflecting the Flemish and
Dutch pronunciations of a word on the basis of its orthographic representation
(which in turn is based on the actual speech recordings). We compare several
possible approaches to achieve the text-to-pronunciation mapping task:
memory-based learning, transformation-based learning, rule induction, maximum
entropy modeling, combination of classifiers in stacked learning, and stacking
of meta-learners. We are interested both in optimal accuracy and in obtaining
insight into the linguistic regularities involved. As far as accuracy is
concerned, an already high accuracy level (93% for Celex and 86% for Fonilex at
word level) for single classifiers is boosted significantly with additional
error reductions of 31% and 38% respectively using combination of classifiers,
and a further 5% using combination of meta-learners, bringing overall word
level accuracy to 96% for the Dutch variant and 92% for the Flemish variant. We
also show that the application of machine learning methods indeed leads to
increased insight into the linguistic regularities determining the variation
between the two pronunciation variants studied."
"Data-Oriented Parsing (dop) ranks among the best parsing schemes, pairing
state-of-the art parsing accuracy to the psycholinguistic insight that larger
chunks of syntactic structures are relevant grammatical and probabilistic
units. Parsing with the dop-model, however, seems to involve a lot of CPU
cycles and a considerable amount of double work, brought on by the concept of
multiple derivations, which is necessary for probabilistic processing, but
which is not convincingly related to a proper linguistic backbone. It is
however possible to re-interpret the dop-model as a pattern-matching model,
which tries to maximize the size of the substructures that construct the parse,
rather than the probability of the parse. By emphasizing this memory-based
aspect of the dop-model, it is possible to do away with multiple derivations,
opening up possibilities for efficient Viterbi-style optimizations, while still
retaining acceptable parsing accuracy through enhanced context-sensitivity."
"Temiar reduplication is a difficult piece of prosodic morphology. This paper
presents the first computational analysis of Temiar reduplication, using the
novel finite-state approach of One-Level Prosodic Morphology originally
developed by Walther (1999b, 2000). After reviewing both the data and the basic
tenets of One-level Prosodic Morphology, the analysis is laid out in some
detail, using the notation of the FSA Utilities finite-state toolkit (van Noord
1997). One important discovery is that in this approach one can easily define a
regular expression operator which ambiguously scans a string in the left- or
rightward direction for a certain prosodic property. This yields an elegant
account of base-length-dependent triggering of reduplication as found in
Temiar."
"This paper examines efficient predictive broad-coverage parsing without
dynamic programming. In contrast to bottom-up methods, depth-first top-down
parsing produces partial parses that are fully connected trees spanning the
entire left context, from which any kind of non-local dependency or partial
semantic interpretation can in principle be read. We contrast two predictive
parsing approaches, top-down and left-corner parsing, and find both to be
viable. In addition, we find that enhancement with non-local information not
only improves parser accuracy, but also substantially improves the search
efficiency."
"The left-corner transform removes left-recursion from (probabilistic)
context-free grammars and unification grammars, permitting simple top-down
parsing techniques to be used. Unfortunately the grammars produced by the
standard left-corner transform are usually much larger than the original. The
selective left-corner transform described in this paper produces a transformed
grammar which simulates left-corner recognition of a user-specified set of the
original productions, and top-down recognition of the others. Combined with two
factorizations, it produces non-left-recursive grammars that are not much
larger than the original."
"Selectional restrictions are semantic sortal constraints imposed on the
participants of linguistic constructions to capture contextually-dependent
constraints on interpretation. Despite their limitations, selectional
restrictions have proven very useful in natural language applications, where
they have been used frequently in word sense disambiguation, syntactic
disambiguation, and anaphora resolution. Given their practical value, we
explore two methods to incorporate selectional restrictions in the HPSG theory,
assuming that the reader is familiar with HPSG. The first method employs HPSG's
Background feature and a constraint-satisfaction component pipe-lined after the
parser. The second method uses subsorts of referential indices, and blocks
readings that violate selectional restrictions during parsing. While
theoretically less satisfactory, we have found the second method particularly
useful in the development of practical systems."
"We argue that some of the computational complexity associated with estimation
of stochastic attribute-value grammars can be reduced by training upon an
informative subset of the full training set. Results using the parsed Wall
Street Journal corpus show that in some circumstances, it is possible to obtain
better estimation results using an informative sample than when training upon
all the available material. Further experimentation demonstrates that with
unlexicalised models, a Gaussian Prior can reduce overfitting. However, when
models are lexicalised and contain overlapping features, overfitting does not
seem to be a problem, and a Gaussian Prior makes minimal difference to
performance. Our approach is applicable for situations when there are an
infeasibly large number of parses in the training set, or else for when
recovery of these parses from a packed representation is itself computationally
expensive."
"Generating semantic lexicons semi-automatically could be a great time saver,
relative to creating them by hand. In this paper, we present an algorithm for
extracting potential entries for a category from an on-line corpus, based upon
a small set of exemplars. Our algorithm finds more correct terms and fewer
incorrect ones than previous work in this area. Additionally, the entries that
are generated potentially provide broader coverage of the category than would
occur to an individual coding them by hand. Our algorithm finds many terms not
included within Wordnet (many more than previous algorithms), and could be
viewed as an ``enhancer'' of existing broad-coverage resources."
"Very little attention has been paid to the comparison of efficiency between
high accuracy statistical parsers. This paper proposes one machine-independent
metric that is general enough to allow comparisons across very different
parsing architectures. This metric, which we call ``events considered'',
measures the number of ``events'', however they are defined for a particular
parser, for which a probability must be calculated, in order to find the parse.
It is applicable to single-pass or multi-stage parsers. We discuss the
advantages of the metric, and demonstrate its usefulness by using it to compare
two parsers which differ in several fundamental ways."
"Log-linear models provide a statistically sound framework for Stochastic
``Unification-Based'' Grammars (SUBGs) and stochastic versions of other kinds
of grammars. We describe two computationally-tractable ways of estimating the
parameters of such grammars from a training corpus of syntactic analyses, and
apply these to estimate a stochastic version of Lexical-Functional Grammar."
"This paper describes a method for estimating conditional probability
distributions over the parses of ``unification-based'' grammars which can
utilize auxiliary distributions that are estimated by other means. We show how
this can be used to incorporate information about lexical selectional
preferences gathered from other sources into Stochastic ``Unification-based''
Grammars (SUBGs). While we apply this estimator to a Stochastic
Lexical-Functional Grammar, the method is general, and should be applicable to
stochastic versions of HPSGs, categorial grammars and transformational
grammars."
"We developed on example-based method of metonymy interpretation. One
advantages of this method is that a hand-built database of metonymy is not
necessary because it instead uses examples in the form ``Noun X no Noun Y (Noun
Y of Noun X).'' Another advantage is that we will be able to interpret
newly-coined metonymic sentences by using a new corpus. We experimented with
metonymy interpretation and obtained a precision rate of 66% when using this
method."
"This paper describes two new bunsetsu identification methods using supervised
learning. Since Japanese syntactic analysis is usually done after bunsetsu
identification, bunsetsu identification is important for analyzing Japanese
sentences. In experiments comparing the four previously available
machine-learning methods (decision tree, maximum-entropy method, example-based
approach and decision list) and two new methods using category-exclusive rules,
the new method using the category-exclusive rules with the highest similarity
performed best."
"Robertson's 2-poisson information retrieve model does not use location and
category information. We constructed a framework using location and category
information in a 2-poisson model. We submitted two systems based on this
framework to the IREX contest, Japanese language information retrieval contest
held in Japan in 1999. For precision in the A-judgement measure they scored
0.4926 and 0.4827, the highest values among the 15 teams and 22 systems that
participated in the IREX contest. We describe our systems and the comparative
experiments done when various parameters were changed. These experiments
confirmed the effectiveness of using location and category information."
"This paper describes in outline a method for translating Japanese temporal
expressions into English. We argue that temporal expressions form a special
subset of language that is best handled as a special module in machine
translation. The paper deals with problems of lexical idiosyncrasy as well as
the choice of articles and prepositions within temporal expressions. In
addition temporal expressions are considered as parts of larger structures, and
the question of whether to translate them as noun phrases or adverbials is
addressed."
"We present a new approach to stochastic modeling of constraint-based grammars
that is based on log-linear models and uses EM for estimation from unannotated
data. The techniques are applied to an LFG grammar for German. Evaluation on an
exact match task yields 86% precision for an ambiguity rate of 5.4, and 90%
precision on a subcat frame match for an ambiguity rate of 25. Experimental
comparison to training from a parsebank shows a 10% gain from EM training.
Also, a new class-based grammar lexicalization is presented, showing a 10% gain
over unlexicalized models."
"This paper presents the use of probabilistic class-based lexica for
disambiguation in target-word selection. Our method employs minimal but precise
contextual information for disambiguation. That is, only information provided
by the target-verb, enriched by the condensed information of a probabilistic
class-based lexicon, is used. Induction of classes and fine-tuning to verbal
arguments is done in an unsupervised manner by EM-based clustering techniques.
The method shows promising results in an evaluation on real-world translations."
"In this thesis, we present two approaches to a rigorous mathematical and
algorithmic foundation of quantitative and statistical inference in
constraint-based natural language processing. The first approach, called
quantitative constraint logic programming, is conceptualized in a clear logical
framework, and presents a sound and complete system of quantitative inference
for definite clauses annotated with subjective weights. This approach combines
a rigorous formal semantics for quantitative inference based on subjective
weights with efficient weight-based pruning for constraint-based systems. The
second approach, called probabilistic constraint logic programming, introduces
a log-linear probability distribution on the proof trees of a constraint logic
program and an algorithm for statistical inference of the parameters and
properties of such probability models from incomplete, i.e., unparsed data. The
possibility of defining arbitrary properties of proof trees as properties of
the log-linear probability model and efficiently estimating appropriate
parameter values for them permits the probabilistic modeling of arbitrary
context-dependencies in constraint logic programs. The usefulness of these
ideas is evaluated empirically in a small-scale experiment on finding the
correct parses of a constraint-based grammar. In addition, we address the
problem of computational intractability of the calculation of expectations in
the inference task and present various techniques to approximately solve this
task. Moreover, we present an approximate heuristic technique for searching for
the most probable analysis in probabilistic constraint logic programs."
"We present some novel machine learning techniques for the identification of
subcategorization information for verbs in Czech. We compare three different
statistical techniques applied to this problem. We show how the learning
algorithm can be used to discover previously unknown subcategorization frames
from the Czech Prague Dependency Treebank. The algorithm can then be used to
label dependents of a verb in the Czech treebank as either arguments or
adjuncts. Using our techniques, we ar able to achieve 88% precision on unseen
parsed text."
"We describe the CoNLL-2000 shared task: dividing text into syntactically
related non-overlapping groups of words, so-called text chunking. We give
background information on the data sets, present a general overview of the
systems that have taken part in the shared task and briefly discuss their
performance."
"Anaphora resolution is one of the major problems in natural language
processing. It is also one of the important tasks in machine translation and
man/machine dialogue. We solve the problem by using surface expressions and
examples. Surface expressions are the words in sentences which provide clues
for anaphora resolution. Examples are linguistic data which are actually used
in conversations and texts. The method using surface expressions and examples
is a practical method. This thesis handles almost all kinds of anaphora: i. The
referential property and number of a noun phrase ii. Noun phrase direct
anaphora iii. Noun phrase indirect anaphora iv. Pronoun anaphora v. Verb phrase
ellipsis"
"Coping with ambiguity has recently received a lot of attention in natural
language processing. Most work focuses on the semantic representation of
ambiguous expressions. In this paper we complement this work in two ways.
First, we provide an entailment relation for a language with ambiguous
expressions. Second, we give a sound and complete tableaux calculus for
reasoning with statements involving ambiguous quantification. The calculus
interleaves partial disambiguation steps with steps in a traditional deductive
process, so as to minimize and postpone branching in the proof process, and
thereby increases its efficiency."
"Common wisdom has it that the bias of stochastic grammars in favor of shorter
derivations of a sentence is harmful and should be redressed. We show that the
common wisdom is wrong for stochastic grammars that use elementary trees
instead of context-free rules, such as Stochastic Tree-Substitution Grammars
used by Data-Oriented Parsing models. For such grammars a non-probabilistic
metric based on the shortest derivation outperforms a probabilistic metric on
the ATIS and OVIS corpora, while it obtains very competitive results on the
Wall Street Journal corpus. This paper also contains the first published
experiments with DOP on the Wall Street Journal."
"We present an LFG-DOP parser which uses fragments from LFG-annotated
sentences to parse new sentences. Experiments with the Verbmobil and Homecentre
corpora show that (1) Viterbi n best search performs about 100 times faster
than Monte Carlo search while both achieve the same accuracy; (2) the DOP
hypothesis which states that parse accuracy increases with increasing fragment
size is confirmed for LFG-DOP; (3) LFG-DOP's relative frequency estimator
performs worse than a discounted frequency estimator; and (4) LFG-DOP
significantly outperforms Tree-DOP is evaluated on tree structures only."
"We describe a new framework for distilling information from word lattices to
improve the accuracy of speech recognition and obtain a more perspicuous
representation of a set of alternative hypotheses. In the standard MAP decoding
approach the recognizer outputs the string of words corresponding to the path
with the highest posterior probability given the acoustics and a language
model. However, even given optimal models, the MAP decoder does not necessarily
minimize the commonly used performance metric, word error rate (WER). We
describe a method for explicitly minimizing WER by extracting word hypotheses
with the highest posterior probabilities from word lattices. We change the
standard problem formulation by replacing global search over a large set of
sentence hypotheses with local search over a small set of word candidates. In
addition to improving the accuracy of the recognizer, our method produces a new
representation of the set of candidate hypotheses that specifies the sequence
of word-level confusions in a compact lattice format. We study the properties
of confusion networks and examine their use for other tasks, such as lattice
compression, word spotting, confidence annotation, and reevaluation of
recognition hypotheses using higher-level knowledge sources."
"Grammatical relationships (GRs) form an important level of natural language
processing, but different sets of GRs are useful for different purposes.
Therefore, one may often only have time to obtain a small training corpus with
the desired GR annotations. To boost the performance from using such a small
training corpus on a transformation rule learner, we use existing systems that
find related types of annotations."
"The most effective paradigm for word sense disambiguation, supervised
learning, seems to be stuck because of the knowledge acquisition bottleneck. In
this paper we take an in-depth study of the performance of decision lists on
two publicly available corpora and an additional corpus automatically acquired
from the Web, using the fine-grained highly polysemous senses in WordNet.
Decision lists are shown a versatile state-of-the-art technique. The
experiments reveal, among other facts, that SemCor can be an acceptable (0.7
precision for polysemous words) starting point for an all-words system. The
results on the DSO corpus show that for some highly polysemous words 0.7
precision seems to be the current state-of-the-art limit. On the other hand,
independently constructed hand-tagged corpora are not mutually useful, and a
corpus automatically acquired from the Web is shown to fail."
"This paper deals with the exploitation of dictionaries for the semi-automatic
construction of lexicons and lexical knowledge bases. The final goal of our
research is to enrich the Basque Lexical Database with semantic information
such as senses, definitions, semantic relations, etc., extracted from a Basque
monolingual dictionary. The work here presented focuses on the extraction of
the semantic relations that best characterise the headword, that is, those of
synonymy, antonymy, hypernymy, and other relations marked by specific relators
and derivation. All nominal, verbal and adjectival entries were treated. Basque
uses morphological inflection to mark case, and therefore semantic relations
have to be inferred from suffixes rather than from prepositions. Our approach
combines a morphological analyser and surface syntax parsing (based on
Constraint Grammar), and has proven very successful for highly inflected
languages such as Basque. Both the effort to write the rules and the actual
processing time of the dictionary have been very low. At present we have
extracted 42,533 relations, leaving only 2,943 (9%) definitions without any
extracted relation. The error rate is extremely low, as only 2.2% of the
extracted relations are wrong."
"This paper explores the possibility to exploit text on the world wide web in
order to enrich the concepts in existing ontologies. First, a method to
retrieve documents from the WWW related to a concept is described. These
document collections are used 1) to construct topic signatures (lists of
topically related words) for each concept in WordNet, and 2) to build
hierarchical clusters of the concepts (the word senses) that lexicalize a given
word. The overall goal is to overcome two shortcomings of WordNet: the lack of
topical links among concepts, and the proliferation of senses. Topic signatures
are validated on a word sense disambiguation task with good results, which are
improved when the hierarchical clusters are used."
"This paper revisits the one sense per collocation hypothesis using
fine-grained sense distinctions and two different corpora. We show that the
hypothesis is weaker for fine-grained sense distinctions (70% vs. 99% reported
earlier on 2-way ambiguities). We also show that one sense per collocation does
hold across corpora, but that collocations vary from one corpus to the other,
following genre and topic variations. This explains the low results when
performing word sense disambiguation across corpora. In fact, we demonstrate
that when two independent corpora share a related genre/topic, the word sense
disambiguation results would be better. Future work on word sense
disambiguation will have to take into account genre and topic as important
parameters on their models."
"This article describes an algorithm for reducing the intermediate alphabets
in cascades of finite-state transducers (FSTs). Although the method modifies
the component FSTs, there is no change in the overall relation described by the
whole cascade. No additional information or special algorithm, that could
decelerate the processing of input, is required at runtime. Two examples from
Natural Language Processing are used to illustrate the effect of the algorithm
on the sizes of the FSTs and their alphabets. With some FSTs the number of arcs
and symbols shrank considerably."
"In this paper, we propose a method to extract descriptions of technical terms
from Web pages in order to utilize the World Wide Web as an encyclopedia. We
use linguistic patterns and HTML text structures to extract text fragments
containing term descriptions. We also use a language model to discard
extraneous descriptions, and a clustering method to summarize resultant
descriptions. We show the effectiveness of our method by way of experiments."
"In information retrieval research, precision and recall have long been used
to evaluate IR systems. However, given that a number of retrieval systems
resembling one another are already available to the public, it is valuable to
retrieve novel relevant documents, i.e., documents that cannot be retrieved by
those existing systems. In view of this problem, we propose an evaluation
method that favors systems retrieving as many novel documents as possible. We
also used our method to evaluate systems that participated in the IREX
workshop."
"Cross-language information retrieval (CLIR), where queries and documents are
in different languages, needs a translation of queries and/or documents, so as
to standardize both of them into a common representation. For this purpose, the
use of machine translation is an effective approach. However, computational
cost is prohibitive in translating large-scale document collections. To resolve
this problem, we propose a two-stage CLIR method. First, we translate a given
query into the document language, and retrieve a limited number of foreign
documents. Second, we machine translate only those documents into the user
language, and re-rank them based on the translation result. We also show the
effectiveness of our method by way of experiments using Japanese queries and
English technical documents."
"This paper explores the usefulness of a technique from software engineering,
code instrumentation, for the development of large-scale natural language
grammars. Information about the usage of grammar rules in test and corpus
sentences is used to improve grammar and testsuite, as well as adapting a
grammar to a specific genre. Results show that less than half of a
large-coverage grammar for German is actually tested by two large testsuites,
and that 10--30% of testing time is redundant. This methodology applied can be
seen as a re-use of grammar writing knowledge for testsuite compilation."
"Besides temporal information explicitly available in verbs and adjuncts, the
temporal interpretation of a text also depends on general world knowledge and
default assumptions. We will present a theory for describing the relation
between, on the one hand, verbs, their tenses and adjuncts and, on the other,
the eventualities and periods of time they represent and their relative
temporal locations.
  The theory is formulated in logic and is a practical implementation of the
concepts described in Ness Schelkens et al. We will show how an abductive
resolution procedure can be used on this representation to extract temporal
information from texts."
"Texts in natural language contain a lot of temporal information, both
explicit and implicit. Verbs and temporal adjuncts carry most of the explicit
information, but for a full understanding general world knowledge and default
assumptions have to be taken into account. We will present a theory for
describing the relation between, on the one hand, verbs, their tenses and
adjuncts and, on the other, the eventualities and periods of time they
represent and their relative temporal locations, while allowing interaction
with general world knowledge.
  The theory is formulated in an extension of first order logic and is a
practical implementation of the concepts described in Van Eynde 2001 and
Schelkens et al. 2000. We will show how an abductive resolution procedure can
be used on this representation to extract temporal information from texts. The
theory presented here is an extension of that in Verdoolaege et al. 2000,
adapted to VanEynde 2001, with a simplified and extended analysis of adjuncts
and with more emphasis on how a model can be constructed."
"We aim at finding the minimal set of fragments which achieves maximal parse
accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street
Journal treebank show that counts of almost arbitrary fragments within parse
trees are important, leading to improved parse accuracy over previous models
tested on this treebank. We isolate a number of dependency relations which
previous models neglect but which contribute to higher parse accuracy."
"This paper describes a novel approach to constructing phonotactic models. The
underlying theoretical approach to phonological description is the
multisyllable approach in which multiple syllable classes are defined that
reflect phonotactically idiosyncratic syllable subcategories. A new
finite-state formalism, OFS Modelling, is used as a tool for encoding,
automatically constructing and generalising phonotactic descriptions.
Language-independent prototype models are constructed which are instantiated on
the basis of data sets of phonological strings, and generalised with a
clustering algorithm. The resulting approach enables the automatic construction
of phonotactic models that encode arbitrarily close approximations of a
language's set of attested phonological forms. The approach is applied to the
construction of multi-syllable word-level phonotactic models for German,
English and Dutch."
"Primitive Optimality Theory (OTP) (Eisner, 1997a; Albro, 1998), a
computational model of Optimality Theory (Prince and Smolensky, 1993), employs
a finite state machine to represent the set of active candidates at each stage
of an Optimality Theoretic derivation, as well as weighted finite state
machines to represent the constraints themselves. For some purposes, however,
it would be convenient if the set of candidates were limited by some set of
criteria capable of being described only in a higher-level grammar formalism,
such as a Context Free Grammar, a Context Sensitive Grammar, or a Multiple
Context Free Grammar (Seki et al., 1991). Examples include reduplication and
phrasal stress models. Here we introduce a mechanism for OTP-like Optimality
Theory in which the constraints remain weighted finite state machines, but sets
of candidates are represented by higher-level grammars. In particular, we use
multiple context-free grammars to model reduplication in the manner of
Correspondence Theory (McCarthy and Prince, 1995), and develop an extended
version of the Earley Algorithm (Earley, 1970) to apply the constraints to a
reduplicating candidate set."
"Home page of the workshop proceedings, with pointers to the individually
archived papers. Includes front matter from the printed version of the
proceedings."
"The data on 13 typologically different languages have been processed using a
two-parameter word length model, based on 1-displaced uniform Poisson
distribution. Statistical dependencies of the 2nd parameter on the 1st one are
revealed for the German texts and genre of letters."
"A two-parameter model of word length measured by the number of syllables
comprising it is proposed. The first parameter is dependent on language type,
the second one - on text genre and reflects the degree of completion of
synergetic processes of language optimization."
"George A. Miller said that human beings have only seven chunks in short-term
memory, plus or minus two. We counted the number of bunsetsus (phrases) whose
modifiees are undetermined in each step of an analysis of the dependency
structure of Japanese sentences, and which therefore must be stored in
short-term memory. The number was roughly less than nine, the upper bound of
seven plus or minus two. We also obtained similar results with English
sentences under the assumption that human beings recognize a series of words,
such as a noun phrase (NP), as a unit. This indicates that if we assume that
the human cognitive units in Japanese and English are bunsetsu and NP
respectively, analysis will support Miller's $7 \pm 2$ theory."
"The referential properties of noun phrases in the Japanese language, which
has no articles, are useful for article generation in Japanese-English machine
translation and for anaphora resolution in Japanese noun phrases. They are
generally classified as generic noun phrases, definite noun phrases, and
indefinite noun phrases. In the previous work, referential properties were
estimated by developing rules that used clue words. If two or more rules were
in conflict with each other, the category having the maximum total score given
by the rules was selected as the desired category. The score given by each rule
was established by hand, so the manpower cost was high. In this work, we
automatically adjusted these scores by using a machine-learning method and
succeeded in reducing the amount of manpower needed to adjust these scores."
"It is often useful to sort words into an order that reflects relations among
their meanings as obtained by using a thesaurus. In this paper, we introduce a
method of arranging words semantically by using several types of `{\sf is-a}'
thesauri and a multi-dimensional thesaurus. We also describe three major
applications where a meaning sort is useful and show the effectiveness of a
meaning sort. Since there is no doubt that a word list in meaning-order is
easier to use than a word list in some random order, a meaning sort, which can
easily produce a word list in meaning-order, must be useful and effective."
"We have developed systems of two types for NTCIR2. One is an enhenced version
of the system we developed for NTCIR1 and IREX. It submitted retrieval results
for JJ and CC tasks. A variety of parameters were tried with the system. It
used such characteristics of newspapers as locational information in the CC
tasks. The system got good results for both of the tasks. The other system is a
portable system which avoids free parameters as much as possible. The system
submitted retrieval results for JJ, JE, EE, EJ, and CC tasks. The system
automatically determined the number of top documents and the weight of the
original query used in automatic-feedback retrieval. It also determined
relevant terms quite robustly. For EJ and JE tasks, it used document expansion
to augment the initial queries. It achieved good results, except on the CC
tasks."
"This paper presents a corpus-based approach to word sense disambiguation
where a decision tree assigns a sense to an ambiguous word based on the bigrams
that occur nearby. This approach is evaluated using the sense-tagged corpora
from the 1998 SENSEVAL word sense disambiguation exercise. It is more accurate
than the average results reported for 30 of 36 words, and is more accurate than
the best results for 19 of 36 words."
"The present paper shows meta-programming turn programming, which is rich
enough to express arbitrary arithmetic computations. We demonstrate a type
system that implements Peano arithmetics, slightly generalized to negative
numbers. Certain types in this system denote numerals. Arithmetic operations on
such types-numerals - addition, subtraction, and even division - are expressed
as type reduction rules executed by a compiler. A remarkable trait is that
division by zero becomes a type error - and reported as such by a compiler."
"This paper presents a novel method of generating and applying hierarchical,
dynamic topic-based language models. It proposes and evaluates new cluster
generation, hierarchical smoothing and adaptive topic-probability estimation
techniques. These combined models help capture long-distance lexical
dependencies. Experiments on the Broadcast News corpus show significant
improvement in perplexity (10.5% overall and 33.5% on target vocabulary)."
"The process of microplanning encompasses a range of problems in Natural
Language Generation (NLG), such as referring expression generation, lexical
choice, and aggregation, problems in which a generator must bridge underlying
domain-specific representations and general linguistic representations. In this
paper, we describe a uniform approach to microplanning based on declarative
representations of a generator's communicative intent. These representations
describe the results of NLG: communicative intent associates the concrete
linguistic structure planned by the generator with inferences that show how the
meaning of that structure communicates needed information about some
application domain in the current discourse context. Our approach, implemented
in the SPUD (sentence planning using description) microplanner, uses the
lexicalized tree-adjoining grammar formalism (LTAG) to connect structure to
meaning and uses modal logic programming to connect meaning to context. At the
same time, communicative intent representations provide a resource for the
process of NLG. Using representations of communicative intent, a generator can
augment the syntax, semantics and pragmatics of an incomplete sentence
simultaneously, and can assess its progress on the various problems of
microplanning incrementally. The declarative formulation of communicative
intent translates into a well-defined methodology for designing grammatical and
conceptual resources which the generator can use to achieve desired
microplanning behavior in a specified domain."
"We performed corpus correction on a modality corpus for machine translation
by using such machine-learning methods as the maximum-entropy method. We thus
constructed a high-quality modality corpus based on corpus correction. We
compared several kinds of methods for corpus correction in our experiments and
developed a good method for corpus correction."
"A great deal of work has been done demonstrating the ability of machine
learning algorithms to automatically extract linguistic knowledge from
annotated corpora. Very little work has gone into quantifying the difference in
ability at this task between a person and a machine. This paper is a first step
in that direction."
"We describe a robust approach for linking already existing lexical/semantic
hierarchies. We use a constraint satisfaction algorithm (relaxation labelling)
to select --among a set of candidates-- the node in a target taxonomy that
bests matches each node in a source taxonomy. In this paper we present the
complete mapping of the nominal, verbal, adjectival and adverbial parts of
WordNet 1.5 onto WordNet 1.6."
"This paper compares two different ways of estimating statistical language
models. Many statistical NLP tagging and parsing models are estimated by
maximizing the (joint) likelihood of the fully-observed training data. However,
since these applications only require the conditional probability
distributions, these distributions can in principle be learnt by maximizing the
conditional likelihood of the training data. Perhaps somewhat surprisingly,
models estimated by maximizing the joint were superior to models estimated by
maximizing the conditional, even though some of the latter models intuitively
had access to ``more information''."
"This paper describes the functioning of a broad-coverage probabilistic
top-down parser, and its application to the problem of language modeling for
speech recognition. The paper first introduces key notions in language modeling
and probabilistic parsing, and briefly reviews some previous approaches to
using syntactic structure for language modeling. A lexicalized probabilistic
top-down parser is then presented, which performs very well, in terms of both
the accuracy of returned parses and the efficiency with which they are found,
relative to the best broad-coverage statistical parsers. A new language model
which utilizes probabilistic top-down parsing is then outlined, and empirical
results show that it improves upon previous work in test corpus perplexity.
Interpolation with a trigram model yields an exceptional improvement relative
to the improvement observed by other models, demonstrating the degree to which
the information captured by our parsing model is orthogonal to that captured by
a trigram model. A small recognition experiment also demonstrates the utility
of the model."
"This thesis presents a broad-coverage probabilistic top-down parser, and its
application to the problem of language modeling for speech recognition. The
parser builds fully connected derivations incrementally, in a single pass from
left-to-right across the string. We argue that the parsing approach that we
have adopted is well-motivated from a psycholinguistic perspective, as a model
that captures probabilistic dependencies between lexical items, as part of the
process of building connected syntactic structures. The basic parser and
conditional probability models are presented, and empirical results are
provided for its parsing accuracy on both newspaper text and spontaneous
telephone conversations. Modifications to the probability model are presented
that lead to improved performance. A new language model which uses the output
of the parser is then defined. Perplexity and word error rate reduction are
demonstrated over trigram models, even when the trigram is trained on
significantly more data. Interpolation on a word-by-word basis with a trigram
model yields additional improvements."
"This paper describes a prototype system to visualize and animate 3D scenes
from car accident reports, written in French. The problem of generating such a
3D simulation can be divided into two subtasks: the linguistic analysis and the
virtual scene generation. As a means of communication between these two
modules, we first designed a template formalism to represent a written accident
report. The CarSim system first processes written reports, gathers relevant
information, and converts it into a formal description. Then, it creates the
corresponding 3D scene and animates the vehicles."
"It is offered to consider word meanings changes in diachrony as
semicontinuous random walk with reflecting and swallowing screens. The basic
characteristics of word life cycle are defined. Verification of the model has
been realized on the data of Russian words distribution on various age periods."
"We present a probabilistic model that uses both prosodic and lexical cues for
the automatic segmentation of speech into topically coherent units. We propose
two methods for combining lexical and prosodic information using hidden Markov
models and decision trees. Lexical information is obtained from a speech
recognizer, and prosodic features are extracted automatically from speech
waveforms. We evaluate our approach on the Broadcast News corpus, using the
DARPA-TDT evaluation metrics. Results show that the prosodic model alone is
competitive with word-based segmentation methods. Furthermore, we achieve a
significant reduction in error by combining the prosodic and word-based
knowledge sources."
"We propose a method to generate large-scale encyclopedic knowledge, which is
valuable for much NLP research, based on the Web. We first search the Web for
pages containing a term in question. Then we use linguistic patterns and HTML
structures to extract text fragments describing the term. Finally, we organize
extracted term descriptions based on word senses and domains. In addition, we
apply an automatically generated encyclopedia to a question answering system
targeting the Japanese Information-Technology Engineers Examination."
"Statistical NLP systems are frequently evaluated and compared on the basis of
their performances on a single split of training and test data. Results
obtained using a single split are, however, subject to sampling noise. In this
paper we argue in favour of reporting a distribution of performance figures,
obtained by resampling the training data, rather than a single number. The
additional information from distributions can be used to make statistically
quantified statements about differences across parameter settings, systems, and
corpora."
"We present a hybrid statistical and grammar-based system for surface natural
language generation (NLG) that uses grammar rules, conditions on using those
grammar rules, and corpus statistics to determine the word order. We also
describe how this surface NLG module is implemented in a prototype
conversational system, and how it attempts to model informational novelty by
varying the word order. Using a combination of rules and statistical
information, the conversational system expresses the novel information
differently than the given information, based on the run-time dialog state. We
also discuss our plans for evaluating the generation strategy."
"We explore many ways of using conceptual distance measures in Word Sense
Disambiguation, starting with the Agirre-Rigau conceptual density measure. We
use a generalized form of this measure, introducing many (parameterized)
refinements and performing an exhaustive evaluation of all meaningful
combinations. We finally obtain a 42% improvement over the original algorithm,
and show that measures of conceptual distance are not worse indicators for
sense disambiguation than measures based on word-coocurrence (exemplified by
the Lesk algorithm). Our results, however, reinforce the idea that only a
combination of different sources of knowledge might eventually lead to accurate
word sense disambiguation."
"In this paper we analyze two question answering tasks : the TREC-8 question
answering task and a set of reading comprehension exams. First, we show that
Q/A systems perform better when there are multiple answer opportunities per
question. Next, we analyze common approaches to two subproblems: term overlap
for answer sentence identification, and answer typing for short answer
extraction. We present general tools for analyzing the strengths and
limitations of techniques for these subproblems. Our results quantify the
limitations of both term overlap and answer typing to distinguish between
competing answer candidates."
"We describe the CoNLL-2001 shared task: dividing text into clauses. We give
background information on the data sets, present a general overview of the
systems that have taken part in the shared task and briefly discuss their
performance."
"This paper reports on the ""Learning Computational Grammars"" (LCG) project, a
postdoc network devoted to studying the application of machine learning
techniques to grammars suitable for computational use. We were interested in a
more systematic survey to understand the relevance of many factors to the
success of learning, esp. the availability of annotated data, the kind of
dependencies in the data, and the availability of knowledge bases (grammars).
We focused on syntax, esp. noun phrase (NP) syntax."
"Memory-based learning (MBL) has enjoyed considerable success in corpus-based
natural language processing (NLP) tasks and is thus a reliable method of
getting a high-level of performance when building corpus-based NLP systems.
However there is a bottleneck in MBL whereby any novel testing item has to be
compared against all the training items in memory base. For this reason there
has been some interest in various forms of memory editing whereby some method
of selecting a subset of the memory base is employed to reduce the number of
comparisons. This paper investigates the use of a modified self-organising map
(SOM) to select a subset of the memory items for comparison. This method
involves reducing the number of comparisons to a value proportional to the
square root of the number of training items. The method is tested on the
identification of base noun-phrases in the Wall Street Journal corpus, using
sections 15 to 18 for training and section 20 for testing."
"The task of creating indicative summaries that help a searcher decide whether
to read a particular document is a difficult task. This paper examines the
indicative summarization task from a generation perspective, by first analyzing
its required content via published guidelines and corpus analysis. We show how
these summaries can be factored into a set of document features, and how an
implemented content planner uses the topicality document feature to create
indicative multidocument query-based summaries."
"Transformation-based learning has been successfully employed to solve many
natural language processing problems. It achieves state-of-the-art performance
on many natural language processing tasks and does not overtrain easily.
However, it does have a serious drawback: the training time is often
intorelably long, especially on the large corpora which are often used in NLP.
In this paper, we present a novel and realistic method for speeding up the
training time of a transformation-based learner without sacrificing
performance. The paper compares and contrasts the training time needed and
performance achieved by our modified learner with two other systems: a standard
transformation-based learner, and the ICA system \cite{hepple00:tbl}. The
results of these experiments show that our system is able to achieve a
significant improvement in training time while still achieving the same
performance as a standard transformation-based learner. This is a valuable
contribution to systems and algorithms which utilize transformation-based
learning at any part of the execution."
"This paper presents a novel method that allows a machine learning algorithm
following the transformation-based learning paradigm \cite{brill95:tagging} to
be applied to multiple classification tasks by training jointly and
simultaneously on all fields. The motivation for constructing such a system
stems from the observation that many tasks in natural language processing are
naturally composed of multiple subtasks which need to be resolved
simultaneously; also tasks usually learned in isolation can possibly benefit
from being learned in a joint framework, as the signals for the extra tasks
usually constitute inductive bias.
  The proposed algorithm is evaluated in two experiments: in one, the system is
used to jointly predict the part-of-speech and text chunks/baseNP chunks of an
English corpus; and in the second it is used to learn the joint prediction of
word segment boundaries and part-of-speech tagging for Chinese. The results
show that the simultaneous learning of multiple tasks does achieve an
improvement in each task upon training the same tasks sequentially. The
part-of-speech tagging result of 96.63% is state-of-the-art for individual
systems on the particular train/test split."
"In the past several years, a number of different language modeling
improvements over simple trigram models have been found, including caching,
higher-order n-grams, skipping, interpolated Kneser-Ney smoothing, and
clustering. We present explorations of variations on, or of the limits of, each
of these techniques, including showing that sentence mixture models may have
more potential. While all of these techniques have been studied separately,
they have rarely been studied in combination. We find some significant
interactions, especially with smoothing and clustering techniques. We compare a
combination of all techniques together to a Katz smoothed trigram model with no
count cutoffs. We achieve perplexity reductions between 38% and 50% (1 bit of
entropy), depending on training data size, as well as a word error rate
reduction of 8.9%. Our perplexity reductions are perhaps the highest reported
compared to a fair baseline. This is the extended version of the paper; it
contains additional details and proofs, and is designed to be a good
introduction to the state of the art in language modeling."
"Maximum entropy models are considered by many to be one of the most promising
avenues of language modeling research. Unfortunately, long training times make
maximum entropy research difficult. We present a novel speedup technique: we
change the form of the model to use classes. Our speedup works by creating two
maximum entropy models, the first of which predicts the class of each word, and
the second of which predicts the word itself. This factoring of the model leads
to fewer non-zero indicator functions, and faster normalization, achieving
speedups of up to a factor of 35 over one of the best previous techniques. It
also results in typically slightly lower perplexities. The same trick can be
used to speed training of other machine learning techniques, e.g. neural
networks, applied to any problem with a large number of outputs, such as
language modeling."
"The paper presents a study on the portability of statistical syntactic
knowledge in the framework of the structured language model (SLM). We
investigate the impact of porting SLM statistics from the Wall Street Journal
(WSJ) to the Air Travel Information System (ATIS) domain. We compare this
approach to applying the Microsoft rule-based parser (NLPwin) for the ATIS data
and to using a small amount of data manually parsed at UPenn for gathering the
intial SLM statistics. Surprisingly, despite the fact that it performs modestly
in perplexity (PPL), the model initialized on WSJ parses outperforms the other
initialization methods based on in-domain annotated data, achieving a
significant 0.4% absolute and 7% relative reduction in word error rate (WER)
over a baseline system whose word error rate is 5.8%; the improvement measured
relative to the minimum WER achievable on the N-best lists we worked with is
12%."
"We argue in this paper that many common adverbial phrases generally taken to
signal a discourse relation between syntactically connected units within
discourse structure, instead work anaphorically to contribute relational
meaning, with only indirect dependence on discourse structure. This allows a
simpler discourse structure to provide scaffolding for compositional semantics,
and reveals multiple ways in which the relational meaning conveyed by adverbial
connectives can interact with that associated with discourse structure. We
conclude by sketching out a lexicalised grammar for discourse that facilitates
discourse interpretation as a product of compositional rules, anaphor
resolution and inference."
"This paper describes a set of comparative experiments for the problem of
automatically filtering unwanted electronic mail messages. Several variants of
the AdaBoost algorithm with confidence-rated predictions [Schapire & Singer,
99] have been applied, which differ in the complexity of the base learners
considered. Two main conclusions can be drawn from our experiments: a) The
boosting-based methods clearly outperform the baseline learning algorithms
(Naive Bayes and Induction of Decision Trees) on the PU1 corpus, achieving very
high levels of the F1 measure; b) Increasing the complexity of the base
learners allows to obtain better ``high-precision'' classifiers, which is a
very important issue when misclassification costs are considered."
"Allowing users to interact through language borders is an interesting
challenge for information technology. For the purpose of a computer assisted
language learning system, we have chosen icons for representing meaning on the
input interface, since icons do not depend on a particular language. However, a
key limitation of this type of communication is the expression of articulated
ideas instead of isolated concepts. We propose a method to interpret sequences
of icons as complex messages by reconstructing the relations between concepts,
so as to build conceptual graphs able to represent meaning and to be used for
natural language sentence generation. This method is based on an electronic
dictionary containing semantic information."
"Selectional preference learning methods have usually focused on word-to-class
relations, e.g., a verb selects as its subject a given nominal class. This
papers extends previous statistical models to class-to-class preferences, and
presents a model that learns selectional preferences for classes of verbs. The
motivation is twofold: different senses of a verb may have different
preferences, and some classes of verbs can share preferences. The model is
tested on a word sense disambiguation task which uses subject-verb and
object-verb relationships extracted from a small sense-disambiguated corpus."
"Two kinds of systems have been defined during the long history of WSD:
principled systems that define which knowledge types are useful for WSD, and
robust systems that use the information sources at hand, such as, dictionaries,
light-weight ontologies or hand-tagged corpora. This paper tries to systematize
the relation between desired knowledge types and actual information sources. We
also compare the results for a wide range of algorithms that have been
evaluated on a common test setting in our research group. We hope that this
analysis will help change the shift from systems based on information sources
to systems based on knowledge sources. This study might also shed some light on
semi-automatic acquisition of desired knowledge types from existing resources."
"This paper explores the possibility of enriching the content of existing
ontologies. The overall goal is to overcome the lack of topical links among
concepts in WordNet. Each concept is to be associated to a topic signature,
i.e., a set of related words with associated weights. The signatures can be
automatically constructed from the WWW or from sense-tagged corpora. Both
approaches are compared and evaluated on a word sense disambiguation task. The
results show that it is possible to construct clean signatures from the WWW
using some filtering techniques."
"The mathematical distinction between prose and verse may be detected in
writings that are not apparently lineated, for example in T. S. Eliot's ""Burnt
Norton"", and Jim Crace's ""Quarantine"". In this paper we offer comments on
appropriate statistical methods for such work, and also on the nature of formal
innovation in these two texts. Additional remarks are made on the roots of
lineation as a metrical form, and on the prose-verse continuum."
"The paper investigates the use of richer syntactic dependencies in the
structured language model (SLM). We present two simple methods of enriching the
dependencies in the syntactic parse trees used for intializing the SLM. We
evaluate the impact of both methods on the perplexity (PPL) and
word-error-rate(WER, N-best rescoring) performance of the SLM. We show that the
new model achieves an improvement in PPL and WER over the baseline results
reported using the SLM on the UPenn Treebank and Wall Street Journal (WSJ)
corpora, respectively."
"We present a method of constructing and using a cascade consisting of a left-
and a right-sequential finite-state transducer (FST), T1 and T2, for
part-of-speech (POS) disambiguation. Compared to an HMM, this FST cascade has
the advantage of significantly higher processing speed, but at the cost of
slightly lower accuracy. Applications such as Information Retrieval, where the
speed can be more important than accuracy, could benefit from this approach.
  In the process of tagging, we first assign every word a unique ambiguity
class c_i that can be looked up in a lexicon encoded by a sequential FST. Every
c_i is denoted by a single symbol, e.g. [ADJ_NOUN], although it represents a
set of alternative tags that a given word can occur with. The sequence of the
c_i of all words of one sentence is the input to our FST cascade. It is mapped
by T1, from left to right, to a sequence of reduced ambiguity classes r_i.
Every r_i is denoted by a single symbol, although it represents a set of
alternative tags. Intuitively, T1 eliminates the less likely tags from c_i,
thus creating r_i. Finally, T2 maps the sequence of r_i, from right to left, to
a sequence of single POS tags t_i. Intuitively, T2 selects the most likely t_i
from every r_i.
  The probabilities of all t_i, r_i, and c_i are used only at compile time, not
at run time. They do not (directly) occur in the FSTs, but are ""implicitly
contained"" in their structure."
"We aim at finding the minimal set of fragments which achieves maximal parse
accuracy in Data Oriented Parsing. Experiments with the Penn Wall Street
Journal treebank show that counts of almost arbitrary fragments within parse
trees are important, leading to improved parse accuracy over previous models
tested on this treebank (a precision of 90.8% and a recall of 90.6%). We
isolate some dependency relations which previous models neglect but which
contribute to higher parse accuracy."
"Structured language models for speech recognition have been shown to remedy
the weaknesses of n-gram models. All current structured language models are,
however, limited in that they do not take into account dependencies between
non-headwords. We show that non-headword dependencies contribute to
significantly improved word error rate, and that a data-oriented parsing model
trained on semantically and syntactically annotated data can exploit these
dependencies. This paper also contains the first DOP model trained by means of
a maximum likelihood reestimation procedure, which solves some of the
theoretical shortcomings of previous DOP models."
"We describe an incremental unsupervised procedure to learn words from
transcribed continuous speech. The algorithm is based on a conservative and
traditional statistical model, and results of empirical tests show that it is
competitive with other algorithms that have been proposed recently for this
task."
"A statistical model for segmentation and word discovery in continuous speech
is presented. An incremental unsupervised learning algorithm to infer word
boundaries based on this model is described. Results of empirical tests showing
that the algorithm is competitive with other models that have been used for
similar tasks are also presented."
"This paper describes experiments carried out using a variety of
machine-learning methods, including the k-nearest neighborhood method that was
used in a previous study, for the translation of tense, aspect, and modality.
It was found that the support-vector machine method was the most precise of all
the methods tested."
"The elastic-input neuro tagger and hybrid tagger, combined with a neural
network and Brill's error-driven learning, have already been proposed for the
purpose of constructing a practical tagger using as little training data as
possible. When a small Thai corpus is used for training, these taggers have
tagging accuracies of 94.4% and 95.5% (accounting only for the ambiguous words
in terms of the part of speech), respectively. In this study, in order to
construct more accurate taggers we developed new tagging methods using three
machine learning methods: the decision-list, maximum entropy, and support
vector machine methods. We then performed tagging experiments by using these
methods. Our results showed that the support vector machine method has the best
precision (96.1%), and that it is capable of improving the accuracy of tagging
in the Thai language. Finally, we theoretically examined all these methods and
discussed how the improvements were achived."
"This paper describes a universal model for paraphrasing that transforms
according to defined criteria. We showed that by using different criteria we
could construct different kinds of paraphrasing systems including one for
answering questions, one for compressing sentences, one for polishing up, and
one for transforming written language to spoken language."
"This paper describes representations of time-dependent signals that are
invariant under any invertible time-independent transformation of the signal
time series. Such a representation is created by rescaling the signal in a
non-linear dynamic manner that is determined by recently encountered signal
levels. This technique may make it possible to normalize signals that are
related by channel-dependent and speaker-dependent transformations, without
having to characterize the form of the signal transformations, which remain
unknown. The technique is illustrated by applying it to the time-dependent
spectra of speech that has been filtered to simulate the effects of different
channels. The experimental results show that the rescaled speech
representations are largely normalized (i.e., channel-independent), despite the
channel-dependence of the raw (unrescaled) speech."
"Treebank formats and associated software tools are proliferating rapidly,
with little consideration for interoperability. We survey a wide variety of
treebank structures and operations, and show how they can be mapped onto the
annotation graph model, and leading to an integrated framework encompassing
tree and non-tree annotations alike. This development opens up new
possibilities for managing and exploiting multilayer annotations."
"Annotation graphs and annotation servers offer infrastructure to support the
analysis of human language resources in the form of time-series data such as
text, audio and video. This paper outlines areas of common need among empirical
linguists and computational linguists. After reviewing examples of data and
tools used or under development for each of several areas, it proposes a common
framework for future tool development, data annotation and resource sharing
based upon annotation graphs and servers."
"Phonology, as it is practiced, is deeply computational. Phonological analysis
is data-intensive and the resulting models are nothing other than specialized
data structures and algorithms. In the past, phonological computation -
managing data and developing analyses - was done manually with pencil and
paper. Increasingly, with the proliferation of affordable computers, IPA fonts
and drawing software, phonologists are seeking to move their computation work
online. Computational Phonology provides the theoretical and technological
framework for this migration, building on methodologies and tools from
computational linguistics. This piece consists of an apology for computational
phonology, a history, and an overview of current research."
"Phonology is the systematic study of the sounds used in language, their
internal structure, and their composition into syllables, words and phrases.
Computational phonology is the application of formal and computational
techniques to the representation and processing of phonological information.
This chapter will present the fundamentals of descriptive phonology along with
a brief overview of computational phonology."
"Selectional preference learning methods have usually focused on word-to-class
relations, e.g., a verb selects as its subject a given nominal class. This
paper extends previous statistical models to class-to-class preferences, and
presents a model that learns selectional preferences for classes of verbs,
together with an algorithm to integrate the learned preferences in WordNet. The
theoretical motivation is twofold: different senses of a verb may have
different preferences, and classes of verbs may share preferences. On the
practical side, class-to-class selectional preferences can be learned from
untagged corpora (the same as word-to-class), they provide selectional
preferences for less frequent word senses via inheritance, and more important,
they allow for easy integration in WordNet. The model is trained on
subject-verb and object-verb relationships extracted from a small corpus
disambiguated with WordNet senses. Examples are provided illustrating that the
theoretical motivations are well founded, and showing that the approach is
feasible. Experimental results on a word sense disambiguation task are also
provided."
"In this paper we describe the systems we developed for the English (lexical
and all-words) and Basque tasks. They were all supervised systems based on
Yarowsky's Decision Lists. We used Semcor for training in the English all-words
task. We defined different feature sets for each language. For Basque, in order
to extract all the information from the text, we defined features that have not
been used before in the literature, using a morphological analyzer. We also
implemented systems that selected automatically good features and were able to
obtain a prefixed precision (85%) at the cost of coverage. The systems that
used all the features were identified as BCU-ehu-dlist-all and the systems that
selected some features as BCU-ehu-dlist-best."
"In this paper we describe the Senseval 2 Basque lexical-sample task. The task
comprised 40 words (15 nouns, 15 verbs and 10 adjectives) selected from Euskal
Hiztegia, the main Basque dictionary. Most examples were taken from the
Egunkaria newspaper. The method used to hand-tag the examples produced low
inter-tagger agreement (75%) before arbitration. The four competing systems
attained results well above the most frequent baseline and the best system
scored 75% precision at 100% coverage. The paper includes an analysis of the
tagging procedure used, as well as the performance of the competing systems. In
particular, we argue that inter-tagger agreement is not a real upperbound for
the Basque WSD task."
"We present memory-based learning approaches to shallow parsing and apply
these to five tasks: base noun phrase identification, arbitrary base phrase
recognition, clause detection, noun phrase parsing and full parsing. We use
feature selection techniques and system combination methods for improving the
performance of the memory-based learner. Our approach is evaluated on standard
data sets and the results are compared with that of other systems. This reveals
that our approach works well for base phrase identification while its
application towards recognizing embedded structures leaves some room for
improvement."
"We present an algorithm that takes an unannotated corpus as its input, and
returns a ranked list of probable morphologically related pairs as its output.
The algorithm tries to discover morphologically related pairs by looking for
pairs that are both orthographically and semantically similar, where
orthographic similarity is measured in terms of minimum edit distance, and
semantic similarity is measured in terms of mutual information. The procedure
does not rely on a morpheme concatenation model, nor on distributional
properties of word substrings (such as affix frequency). Experiments with
German and English input give encouraging results, both in terms of precision
(proportion of good pairs found at various cutoff points of the ranked list),
and in terms of a qualitative analysis of the types of morphological patterns
discovered by the algorithm."
"Given the lack of word delimiters in written Japanese, word segmentation is
generally considered a crucial first step in processing Japanese texts. Typical
Japanese segmentation algorithms rely either on a lexicon and syntactic
analysis or on pre-segmented data; but these are labor-intensive, and the
lexico-syntactic techniques are vulnerable to the unknown word problem. In
contrast, we introduce a novel, more robust statistical method utilizing
unsegmented training data. Despite its simplicity, the algorithm yields
performance on long kanji sequences comparable to and sometimes surpassing that
of state-of-the-art morphological analyzers over a variety of error metrics.
The algorithm also outperforms another mostly-unsupervised statistical
algorithm previously proposed for Chinese.
  Additionally, we present a two-level annotation scheme for Japanese to
incorporate multiple segmentation granularities, and introduce two novel
evaluation metrics, both based on the notion of a compatible bracket, that can
account for multiple granularities simultaneously."
"This paper presents Ellogon, a multi-lingual, cross-platform, general-purpose
text engineering environment. Ellogon was designed in order to aid both
researchers in natural language processing, as well as companies that produce
language engineering systems for the end-user. Ellogon provides a powerful
TIPSTER-based infrastructure for managing, storing and exchanging textual data,
embedding and managing text processing components as well as visualising
textual data and their associated linguistic information. Among its key
features are full Unicode support, an extensive multi-lingual graphical user
interface, its modular architecture and the reduced hardware requirements."
"I propose a variable-free treatment of dynamic semantics. By ""dynamic
semantics"" I mean analyses of donkey sentences (""Every farmer who owns a donkey
beats it"") and other binding and anaphora phenomena in natural language where
meanings of constituents are updates to information states, for instance as
proposed by Groenendijk and Stokhof. By ""variable-free"" I mean denotational
semantics in which functional combinators replace variable indices and
assignment functions, for instance as advocated by Jacobson.
  The new theory presented here achieves a compositional treatment of dynamic
anaphora that does not involve assignment functions, and separates the
combinatorics of variable-free semantics from the particular linguistic
phenomena it treats. Integrating variable-free semantics and dynamic semantics
gives rise to interactions that make new empirical predictions, for example
""donkey weak crossover"" effects."
"NLTK, the Natural Language Toolkit, is a suite of open source program
modules, tutorials and problem sets, providing ready-to-use computational
linguistics courseware. NLTK covers symbolic and statistical natural language
processing, and is interfaced to annotated corpora. Students augment and
replace existing components, learn structured programming by example, and
manipulate sophisticated models from the outset."
"We present two methods for unsupervised segmentation of words into
morpheme-like units. The model utilized is especially suited for languages with
a rich morphology, such as Finnish. The first method is based on the Minimum
Description Length (MDL) principle and works online. In the second method,
Maximum Likelihood (ML) optimization is used. The quality of the segmentations
is measured using an evaluation method that compares the segmentations produced
to an existing morphological analysis. Experiments on both Finnish and English
corpora show that the presented methods perform well compared to a current
state-of-the-art system."
"An important component of any generation system is the mapping dictionary, a
lexicon of elementary semantic expressions and corresponding natural language
realizations. Typically, labor-intensive knowledge-based methods are used to
construct the dictionary. We instead propose to acquire it automatically via a
novel multiple-pass algorithm employing multiple-sequence alignment, a
technique commonly used in bioinformatics. Crucially, our method leverages
latent information contained in multi-parallel corpora -- datasets that supply
several verbalizations of the corresponding semantics rather than just one.
  We used our techniques to generate natural language versions of
computer-generated mathematical proofs, with good results on both a
per-component and overall-output basis. For example, in evaluations involving a
dozen human judges, our system produced output whose readability and
faithfulness to the semantic input rivaled that of a traditional generation
system."
"This paper presents an evaluation of an ensemble--based system that
participated in the English and Spanish lexical sample tasks of Senseval-2. The
system combines decision trees of unigrams, bigrams, and co--occurrences into a
single classifier. The analysis is extended to include the Senseval-1 data."
"This paper presents a comparative evaluation among the systems that
participated in the Spanish and English lexical sample tasks of Senseval-2. The
focus is on pairwise comparisons among systems to assess the degree to which
they agree, and on measuring the difficulty of the test instances included in
these tasks."
"This paper describes the sixteen Duluth entries in the Senseval-2 comparative
exercise among word sense disambiguation systems. There were eight pairs of
Duluth systems entered in the Spanish and English lexical sample tasks. These
are all based on standard machine learning algorithms that induce classifiers
from sense-tagged training text where the context in which ambiguous words
occur are represented by simple lexical features. These are highly portable,
robust methods that can serve as a foundation for more tailored approaches."
"While recent retrieval techniques do not limit the number of index terms,
out-of-vocabulary (OOV) words are crucial in speech recognition. Aiming at
retrieving information with spoken queries, we fill the gap between speech
recognition and text retrieval in terms of the vocabulary size. Given a spoken
query, we generate a transcription and detect OOV words through speech
recognition. We then correspond detected OOV words to terms indexed in a target
collection to complete the transcription, and search the collection for
documents relevant to the completed transcription. We show the effectiveness of
our method by way of experiments."
"Cross-language information retrieval (CLIR), where queries and documents are
in different languages, has of late become one of the major topics within the
information retrieval community. This paper proposes a Japanese/English CLIR
system, where we combine a query translation and retrieval modules. We
currently target the retrieval of technical documents, and therefore the
performance of our system is highly dependent on the quality of the translation
of technical terms. However, the technical term translation is still
problematic in that technical terms are often compound words, and thus new
terms are progressively created by combining existing base words. In addition,
Japanese often represents loanwords based on its special phonogram.
Consequently, existing dictionaries find it difficult to achieve sufficient
coverage. To counter the first problem, we produce a Japanese/English
dictionary for base words, and translate compound words on a word-by-word
basis. We also use a probabilistic method to resolve translation ambiguity. For
the second problem, we use a transliteration method, which corresponds words
unlisted in the base word dictionary to their phonetic equivalents in the
target language. We evaluate our system using a test collection for CLIR, and
show that both the compound word translation and transliteration methods
improve the system performance."
"This paper proposes a method to analyze Japanese anaphora, in which zero
pronouns (omitted obligatory cases) are used to refer to preceding entities
(antecedents). Unlike the case of general coreference resolution, zero pronouns
have to be detected prior to resolution because they are not expressed in
discourse. Our method integrates two probability parameters to perform zero
pronoun detection and resolution in a single framework. The first parameter
quantifies the degree to which a given case is a zero pronoun. The second
parameter quantifies the degree to which a given entity is the antecedent for a
detected zero pronoun. To compute these parameters efficiently, we use corpora
with/without annotations of anaphoric relations. We show the effectiveness of
our method by way of experiments."
"This paper applies an existing query translation method to cross-language
patent retrieval. In our method, multiple dictionaries are used to derive all
possible translations for an input query, and collocational statistics are used
to resolve translation ambiguity. We used Japanese/English parallel patent
abstracts to perform comparative experiments, where our method outperformed a
simple dictionary-based query translation method, and achieved 76% of
monolingual retrieval in terms of average precision."
"Given the growing number of patents filed in multiple countries, users are
interested in retrieving patents across languages. We propose a multi-lingual
patent retrieval system, which translates a user query into the target
language, searches a multilingual database for patents relevant to the query,
and improves the browsing efficiency by way of machine translation and
clustering. Our system also extracts new translations from patent families
consisting of comparable patents, to enhance the translation dictionary."
"We report experimental results associated with speech-driven text retrieval,
which facilitates retrieving information in multiple domains with spoken
queries. Since users speak contents related to a target collection, we produce
language models used for speech recognition based on the target collection, so
as to improve both the recognition and retrieval accuracy. Experiments using
existing test collections combined with dictated queries showed the
effectiveness of our method."
"Speech recognition has of late become a practical technology for real world
applications. Aiming at speech-driven text retrieval, which facilitates
retrieving information with spoken queries, we propose a method to integrate
speech recognition and retrieval methods. Since users speak contents related to
a target collection, we adapt statistical language models used for speech
recognition based on the target collection, so as to improve both the
recognition and retrieval accuracy. Experiments using existing test collections
combined with dictated queries showed the effectiveness of our method."
"This paper describes the results of some experiments exploring statistical
methods to infer syntactic behavior of words and morphemes from a raw corpus in
an unsupervised fashion. It shares certain points in common with Brown et al
(1992) and work that has grown out of that: it employs statistical techniques
to analyze syntactic behavior based on what words occur adjacent to a given
word. However, we use an eigenvector decomposition of a nearest-neighbor graph
to produce a two-dimensional rendering of the words of a corpus in which words
of the same syntactic category tend to form neighborhoods. We exploit this
technique for extending the value of automatic learning of morphology. In
particular, we look at the suffixes derived from a corpus by unsupervised
learning of morphology, and we ask which of these suffixes have a consistent
syntactic function (e.g., in English, -tion is primarily a mark of nouns, but
-s marks both noun plurals and 3rd person present on verbs), and we determine
that this method works well for this task."
"The title of a document has two roles, to give a compact summary and to lead
the reader to read the document. Conventional title generation focuses on
finding key expressions from the author's wording in the document to give a
compact summary and pays little attention to the reader's interest. To make the
title play its second role properly, it is indispensable to clarify the content
(``what to say'') and wording (``how to say'') of titles that are effective to
attract the target reader's interest. In this article, we first identify
typical content and wording of titles aimed at general readers in a comparative
study between titles of technical papers and headlines rewritten for
newspapers. Next, we describe the results of a questionnaire survey on the
effects of the content and wording of titles on the reader's interest. The
survey of general and knowledgeable readers shows both common and different
tendencies in interest."
"We present a broad coverage Japanese grammar written in the HPSG formalism
with MRS semantics. The grammar is created for use in real world applications,
such that robustness and performance issues play an important role. It is
connected to a POS tagging and word segmentation tool. This grammar is being
developed in a multilingual context, requiring MRS structures that are easily
comparable across languages."
"Diff is a software program that detects differences between two data sets and
is useful in natural language processing. This paper shows several examples of
the application of diff. They include the detection of differences between two
different datasets, extraction of rewriting rules, merging of two different
datasets, and the optimal matching of two different data sets. Since diff comes
with any standard UNIX system, it is readily available and very easy to use.
Our studies showed that diff is a practical tool for research into natural
language processing."
"This article studies the problem of assessing relevance to each of the rules
of a reference resolution system. The reference solver described here stems
from a formal model of reference and is integrated in a reference processing
workbench. Evaluation of the reference resolution is essential, as it enables
differential evaluation of individual rules. Numerical values of these measures
are given, and discussed, for simple selection rules and other processing
rules; such measures are then studied for numerical parameters."
"Reference resolution on extended texts (several thousand references) cannot
be evaluated manually. An evaluation algorithm has been proposed for the MUC
tests, using equivalence classes for the coreference relation. However, we show
here that this algorithm is too indulgent, yielding good scores even for poor
resolution strategies. We elaborate on the same formalism to propose two new
evaluation algorithms, comparing them first with the MUC algorithm and giving
then results on a variety of examples. A third algorithm using only
distributional comparison of equivalence classes is finally described; it
assesses the relative importance of the recall vs. precision errors."
"Anaphora resolution is envisaged in this paper as part of the reference
resolution process. A general open architecture is proposed, which can be
particularized and configured in order to simulate some classic anaphora
resolution methods. With the aim of improving pronoun resolution, the system
takes advantage of elementary cues about characters of the text, which are
represented through a particular data structure. In its most robust
configuration, the system uses only a general lexicon, a local morpho-syntactic
parser and a dictionary of synonyms. A short comparative corpus analysis shows
that narrative texts are the most suitable for testing such a system."
"A model for reference use in communication is proposed, from a
representationist point of view. Both the sender and the receiver of a message
handle representations of their common environment, including mental
representations of objects. Reference resolution by a computer is viewed as the
construction of object representations using referring expressions from the
discourse, whereas often only coreference links between such expressions are
looked for. Differences between these two approaches are discussed. The model
has been implemented with elementary rules, and tested on complex narrative
texts (hundreds to thousands of referring expressions). The results support the
mental representations paradigm."
"In some contexts, well-formed natural language cannot be expected as input to
information or communication systems. In these contexts, the use of
grammar-independent input (sequences of uninflected semantic units like e.g.
language-independent icons) can be an answer to the users' needs. A semantic
analysis can be performed, based on lexical semantic knowledge: it is
equivalent to a dependency analysis with no syntactic or morphological clues.
However, this requires that an intelligent system should be able to interpret
this input with reasonable accuracy and in reasonable time. Here we propose a
method allowing a purely semantic-based analysis of sequences of semantic
units. It uses an algorithm inspired by the idea of ``chart parsing'' known in
Natural Language Processing, which stores intermediate parsing results in order
to bring the calculation time down. In comparison with using declarative logic
programming - where the calculation time, left to a prolog engine, is
hyperexponential -, this method brings the calculation time down to a
polynomial time, where the order depends on the valency of the predicates."
"In this paper, we discuss the utility and deficiencies of existing ontology
resources for a number of language processing applications. We describe a
technique for increasing the semantic type coverage of a specific ontology, the
National Library of Medicine's UMLS, with the use of robust finite state
methods used in conjunction with large-scale corpus analytics of the domain
corpus. We call this technique ""semantic rerendering"" of the ontology. This
research has been done in the context of Medstract, a joint Brandeis-Tufts
effort aimed at developing tools for analyzing biomedical language (i.e.,
Medline), as well as creating targeted databases of bio-entities, biological
relations, and pathway data for biological researchers. Motivating the current
research is the need to have robust and reliable semantic typing of syntactic
elements in the Medline corpus, in order to improve the overall performance of
the information extraction applications mentioned above."
"We describe the CoNLL-2002 shared task: language-independent named entity
recognition. We give background information on the data sets and the evaluation
method, present a general overview of the systems that have taken part in the
task and discuss their performance."
"We present new results on the relation between purely symbolic context-free
parsing strategies and their probabilistic counter-parts. Such parsing
strategies are seen as constructions of push-down devices from grammars. We
show that preservation of probability distribution is possible under two
conditions, viz. the correct-prefix property and the property of strong
predictiveness. These results generalize existing results in the literature
that were obtained by considering parsing strategies in isolation. From our
general results we also derive negative results on so-called generalized LR
parsing."
"Robert French has argued that a disembodied computer is incapable of passing
a Turing Test that includes subcognitive questions. Subcognitive questions are
designed to probe the network of cultural and perceptual associations that
humans naturally develop as we live, embodied and embedded in the world. In
this paper, I show how it is possible for a disembodied computer to answer
subcognitive questions appropriately, contrary to French's claim. My approach
to answering subcognitive questions is to use statistical information extracted
from a very large collection of text. In particular, I show how it is possible
to answer a sample of subcognitive questions taken from French, by issuing
queries to a search engine that indexes about 350 million Web pages. This
simple algorithm may shed light on the nature of human (sub-) cognition, but
the scope of this paper is limited to demonstrating that French is mistaken: a
disembodied computer can answer subcognitive questions."
"In this paper we describe an algorithm for aligning sentences with their
translations in a bilingual corpus using lexical information of the languages.
Existing efficient algorithms ignore word identities and consider only the
sentence lengths (Brown, 1991; Gale and Church, 1993). For a sentence in the
source language text, the proposed algorithm picks the most likely translation
from the target language text using lexical information and certain heuristics.
It does not do statistical analysis using sentence lengths. The algorithm is
language independent. It also aids in detecting addition and deletion of text
in translations. The algorithm gives comparable results with the existing
algorithms in most of the cases while it does better in cases where statistical
algorithms do not give good results."
"Compounded words are a challenge for NLP applications such as machine
translation (MT). We introduce methods to learn splitting rules from
monolingual and parallel corpora. We evaluate them against a gold standard and
measure their impact on performance of statistical MT systems. Results show
accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a
German-English noun phrase translation task."
"The author uses the entropy of the ideal Bose-Einstein gas to minimize losses
in computer-oriented languages."
A method of languages genealogical trees construction is proposed.
"We address the text-to-text generation problem of sentence-level paraphrasing
-- a phenomenon distinct from and more difficult than word- or phrase-level
paraphrasing. Our approach applies multiple-sequence alignment to sentences
gathered from unannotated comparable corpora: it learns a set of paraphrasing
patterns represented by word lattice pairs and automatically determines how to
apply these patterns to rewrite new sentences. The results of our evaluation
experiments show that the system derives accurate paraphrases, outperforming
baseline systems."
"We show how to construct a channel-independent representation of speech that
has propagated through a noisy reverberant channel. This is done by blindly
rescaling the cepstral time series by a non-linear function, with the form of
this scale function being determined by previously encountered cepstra from
that channel. The rescaled form of the time series is an invariant property of
it in the following sense: it is unaffected if the time series is transformed
by any time-independent invertible distortion. Because a linear channel with
stationary noise and impulse response transforms cepstra in this way, the new
technique can be used to remove the channel dependence of a cepstral time
series. In experiments, the method achieved greater channel-independence than
cepstral mean normalization, and it was comparable to the combination of
cepstral mean normalization and spectral subtraction, despite the fact that no
measurements of channel noise or reverberations were required (unlike spectral
subtraction)."
A glottochronologic retrognostic of language system is proposed
"A brief, general-audience overview of the history of natural language
processing, focusing on data-driven approaches.Topics include ""Ambiguity and
language analysis"", ""Firth things first"", ""A 'C' change"", and ""The empiricists
strike back""."
"We report about the current state of development of a document suite and its
applications. This collection of tools for the flexible and robust processing
of documents in German is based on the use of XML as unifying formalism for
encoding input and output data as well as process information. It is organized
in modules with limited responsibilities that can easily be combined into
pipelines to solve complex tasks. Strong emphasis is laid on a number of
techniques to deal with lexical and conceptual gaps that are typical when
starting a new application."
"It is very costly to build up lexical resources and domain ontologies.
Especially when confronted with a new application domain lexical gaps and a
poor coverage of domain concepts are a problem for the successful exploitation
of natural language document analysis systems that need and exploit such
knowledge sources. In this paper we report about ongoing experiments with
`bootstrapping techniques' for lexicon and ontology creation."
"In this paper we describe an approach for the analysis of documents in German
and English with a shared pool of resources. For the analysis of German
documents we use a document suite, which supports the user in tasks like
information retrieval and information extraction. The core of the document
suite is based on our tool XDOC. Now we want to exploit these methods for the
analysis of English documents as well. For this aim we need a multilingual
presentation format of the resources. These resources must be transformed into
an unified format, in which we can set additional information about linguistic
characteristics of the language depending on the analyzed documents. In this
paper we describe our approach for such an exchange model for multilingual
resources based on XML."
"Factorization of statistical language models is the task that we resolve the
most discriminative model into factored models and determine a new model by
combining them so as to provide better estimate. Most of previous works mainly
focus on factorizing models of sequential events, each of which allows only one
factorization manner. To enable parallel factorization, which allows a model
event to be resolved in more than one ways at the same time, we propose a
general framework, where we adopt a backing-off lattice to reflect parallel
factorizations and to define the paths along which a model is resolved into
factored models, we use a mixture model to combine parallel paths in the
lattice, and generalize Katz's backing-off method to integrate all the mixture
models got by traversing the entire lattice. Based on this framework, we
formulate two types of model factorizations that are used in natural language
modeling."
"We describe the CoNLL-2003 shared task: language-independent named entity
recognition. We give background information on the data sets (English and
German) and the evaluation method, present a general overview of the systems
that have taken part in the task and discuss their performance."
"This paper presents a machine learning approach to discourse planning in
natural language generation. More specifically, we address the problem of
learning the most natural ordering of facts in discourse plans for a specific
domain. We discuss our methodology and how it was instantiated using two
different machine learning algorithms. A quantitative evaluation performed in
the domain of museum exhibit descriptions indicates that our approach performs
significantly better than manually constructed ordering rules. Being
retrainable, the resulting planners can be ported easily to other similar
domains, without requiring language technology expertise."
"k is the most important parameter in a text categorization system based on
k-Nearest Neighbor algorithm (kNN).In the classification process, k nearest
documents to the test one in the training set are determined firstly. Then, the
predication can be made according to the category distribution among these k
nearest neighbors. Generally speaking, the class distribution in the training
set is uneven. Some classes may have more samples than others. Therefore, the
system performance is very sensitive to the choice of the parameter k. And it
is very likely that a fixed k value will result in a bias on large categories.
To deal with these problems, we propose an improved kNN algorithm, which uses
different numbers of nearest neighbors for different categories, rather than a
fixed number across all categories. More samples (nearest neighbors) will be
used for deciding whether a test document should be classified to a category,
which has more samples in the training set. Preliminary experiments on Chinese
text categorization show that our method is less sensitive to the parameter k
than the traditional one, and it can properly classify documents belonging to
smaller classes with a large k. The method is promising for some cases, where
estimating the parameter k via cross-validation is not allowed."
"As interaction between autonomous agents, communication can be analyzed in
game-theoretic terms. Meaning game is proposed to formalize the core of
intended communication in which the sender sends a message and the receiver
attempts to infer its meaning intended by the sender. Basic issues involved in
the game of natural language communication are discussed, such as salience,
grammaticality, common sense, and common belief, together with some
demonstration of the feasibility of game-theoretic account of language."
"The standard tabulation techniques for logic programming presuppose fixed
order of computation. Some data-driven control should be introduced in order to
deal with diverse contexts. The present paper describes a data-driven method of
constraint transformation with a sort of compilation which subsumes
accessibility check and last-call optimization, which characterize standard
natural-language parsing techniques, semantic-head-driven generation, etc."
"MPEG-7 (Moving Picture Experts Group Phase 7) is an XML-based international
standard on semantic description of multimedia content. This document discusses
the Linguistic DS and related tools. The linguistic DS is a tool, based on the
GDA tag set (http://i-content.org/GDA/tagset.html), for semantic annotation of
linguistic data in or associated with multimedia content. The current document
text reflects `Study of FPDAM - MPEG-7 MDS Extensions' issued in March 2003,
and not most part of MPEG-7 MDS, for which the readers are referred to the
first version of MPEG-7 MDS document available from ISO (http://www.iso.org).
Without that reference, however, this document should be mostly intelligible to
those who are familiar with XML and linguistic theories. Comments are welcome
and will be considered in the standardization process."
"The world is passing through a major revolution called the information
revolution, in which information and knowledge is becoming available to people
in unprecedented amounts wherever and whenever they need it. Those societies
which fail to take advantage of the new technology will be left behind, just
like in the industrial revolution.
  The information revolution is based on two major technologies: computers and
communication. These technologies have to be delivered in a COST EFFECTIVE
manner, and in LANGUAGES accessible to people.
  One way to deliver them in cost effective manner is to make suitable
technology choices, and to allow people to access through shared resources.
This could be done throuch street corner shops (for computer usage, e-mail
etc.), schools, community centres and local library centres."
"The world is passing through a major revolution called the information
revolution, in which information and knowledge is becoming available to people
in unprecedented amounts wherever and whenever they need it. Those societies
which fail to take advantage of the new technology will be left behind, just
like in the industrial revolution.
  The information revolution is based on two major technologies: computers and
communication. These technologies have to be delivered in a COST EFFECTIVE
manner, and in LANGUAGES accessible to people.
  One way to deliver them in cost effective manner is to make suitable
technology choices (discussed later), and to allow people to access through
shared resources. This could be done throuch street corner shops (for computer
usage, e-mail etc.), schools, community centers and local library centres."
"The anusaaraka system makes text in one Indian language accessible in another
Indian language. In the anusaaraka approach, the load is so divided between man
and computer that the language load is taken by the machine, and the
interpretation of the text is left to the man. The machine presents an image of
the source text in a language close to the target language.In the image, some
constructions of the source language (which do not have equivalents) spill over
to the output. Some special notation is also devised. The user after some
training learns to read and understand the output. Because the Indian languages
are close, the learning time of the output language is short, and is expected
to be around 2 weeks.
  The output can also be post-edited by a trained user to make it grammatically
correct in the target language. Style can also be changed, if necessary. Thus,
in this scenario, it can function as a human assisted translation system.
  Currently, anusaarakas are being built from Telugu, Kannada, Marathi, Bengali
and Punjabi to Hindi. They can be built for all Indian languages in the near
future. Everybody must pitch in to build such systems connecting all Indian
languages, using the free software model."
"The anusaaraka system (a kind of machine translation system) makes text in
one Indian language accessible through another Indian language. The machine
presents an image of the source text in a language close to the target
language. In the image, some constructions of the source language (which do not
have equivalents in the target language) spill over to the output. Some special
notation is also devised.
  Anusaarakas have been built from five pairs of languages: Telugu,Kannada,
Marathi, Bengali and Punjabi to Hindi. They are available for use through Email
servers.
  Anusaarkas follows the principle of substitutibility and reversibility of
strings produced. This implies preservation of information while going from a
source language to a target language.
  For narrow subject areas, specialized modules can be built by putting subject
domain knowledge into the system, which produce good quality grammatical
output. However, it should be remembered, that such modules will work only in
narrow areas, and will sometimes go wrong. In such a situation, anusaaraka
output will still remain useful."
"The paper reports on efforts taken to create lexical resources pertaining to
Indian languages, using the collaborative model. The lexical resources being
developed are: (1) Transfer lexicon and grammar from English to several Indian
languages. (2) Dependencey tree bank of annotated corpora for several Indian
languages. The dependency trees are based on the Paninian model. (3) Bilingual
dictionary of 'core meanings'."
"This paper describes a test collection (benchmark data) for retrieval systems
driven by spoken queries. This collection was produced in the subtask of the
NTCIR-3 Web retrieval task, which was performed in a TREC-style evaluation
workshop. The search topics and document collection for the Web retrieval task
were used to produce spoken queries and language models for speech recognition,
respectively. We used this collection to evaluate the performance of our
retrieval system. Experimental results showed that (a) the use of target
documents for language modeling and (b) enhancement of the vocabulary size in
speech recognition were effective in improving the system performance."
"We propose a cross-media lecture-on-demand system, in which users can
selectively view specific segments of lecture videos by submitting text
queries. Users can easily formulate queries by using the textbook associated
with a target lecture, even if they cannot come up with effective keywords. Our
system extracts the audio track from a target lecture video, generates a
transcription by large vocabulary continuous speech recognition, and produces a
text index. Experimental results showed that by adapting speech recognition to
the topic of the lecture, the recognition accuracy increased and the retrieval
accuracy was comparable with that obtained by human transcription."
"Spoken Language can be used to provide insights into organisational
processes, unfortunately transcription and coding stages are very time
consuming and expensive. The concept of partial transcription and coding is
proposed in which spoken language is indexed prior to any subsequent
processing. The functional linguistic theory of texture is used to describe the
effects of partial transcription on observational records. The standard used to
encode transcript context and metadata is called CHAT, but a previous XML
schema developed to implement it contains design assumptions that make it
difficult to support partial transcription for example. This paper describes a
more effective XML schema that overcomes many of these problems and is intended
for use in applications that support the rapid development of spoken language
deliverables."
"Special technologies need to be used to take advantage of, and overcome, the
challenges associated with acquiring, transforming, storing, processing, and
distributing spoken language resources in organisations. This paper introduces
an application architecture consisting of tools and supporting utilities for
indexing and transcription, and describes how these tools, together with
downstream processing and distribution systems, can be integrated into a
workflow. Two sample applications for this architecture are outlined- the
analysis of decision-making processes in organisations and the deployment of
systems development methods by designers in the field."
"We use the rank-frequency analysis for the estimation of Kernel Vocabulary
size within specific corpora of Ukrainian. The extrapolation of high-rank
behaviour is utilized for estimation of the total vocabulary size."
"Frequency counts are a measure of how much use a language makes of a
linguistic unit, such as a phoneme or word. However, what is often important is
not the units themselves, but the contrasts between them. A measure is
therefore needed for how much use a language makes of a contrast, i.e. the
functional load (FL) of the contrast. We generalize previous work in
linguistics and speech recognition and propose a family of measures for the FL
of several phonological contrasts, including phonemic oppositions, distinctive
features, suprasegmentals, and phonological rules. We then test it for
robustness to changes of corpora. Finally, we provide examples in Cantonese,
Dutch, English, German and Mandarin, in the context of historical linguistics,
language acquisition and speech recognition. More information can be found at
http://dinoj.info/research/fload"
"In the article the fact is verified that the list of words selected by formal
statistical methods (frequency and functional genre unrestrictedness) is not a
conglomerate of non-related words. It creates a system of interrelated items
and it can be named ""lexical base of language"". This selected list of words
covers all the spheres of human activities. To verify this statement the
invariant synoptical scheme common for ideographic dictionaries of different
language was determined."
"We present a novel, type-logical analysis of_polarity sensitivity_: how
negative polarity items (like ""any"" and ""ever"") or positive ones (like ""some"")
are licensed or prohibited. It takes not just scopal relations but also linear
order into account, using the programming-language notions of delimited
continuations and evaluation order, respectively. It thus achieves greater
empirical coverage than previous proposals."
"This is a tutorial on tabular parsing, on the basis of tabulation of
nondeterministic push-down automata. Discussed are Earley's algorithm, the
Cocke-Kasami-Younger algorithm, tabular LR parsing, the construction of parse
trees, and further issues."
"This paper describes the Patent Retrieval Task in the Fourth NTCIR Workshop,
and the test collections produced in this task. We perform the invalidity
search task, in which each participant group searches a patent collection for
the patents that can invalidate the demand in an existing claim. We also
perform the automatic patent map generation task, in which the patents
associated with a specific topic are organized in a multi-dimensional matrix."
"A probabilistic model for computer-based generation of a machine translation
system on the basis of English-Russian parallel text corpora is suggested. The
model is trained using parallel text corpora with pre-aligned source and target
sentences. The training of the model results in a bilingual dictionary of words
and ""word blocks"" with relevant translation probability."
"We consider the problem of modeling the content structure of texts within a
specific domain, in terms of the topics the texts address and the order in
which these topics appear. We first present an effective knowledge-lean method
for learning content models from un-annotated documents, utilizing a novel
adaptation of algorithms for Hidden Markov Models. We then apply our method to
two complementary tasks: information ordering and extractive summarization. Our
experiments show that incorporating content models in these applications yields
substantial improvement over previously-proposed methods."
"This paper describes a standalone, publicly-available implementation of the
Resolution of Anaphora Procedure (RAP) given by Lappin and Leass (1994). The
RAP algorithm resolves third person pronouns, lexical anaphors, and identifies
pleonastic pronouns. Our implementation, JavaRAP, fills a current need in
anaphora resolution research by providing a reference implementation that can
be benchmarked against current algorithms. The implementation uses the
standard, publicly available Charniak (2000) parser as input, and generates a
list of anaphora-antecedent pairs as output. Alternately, an in-place
annotation or substitution of the anaphors with their antecedents can be
produced. Evaluation on the MUC-6 co-reference task shows that JavaRAP has an
accuracy of 57.9%, similar to the performance given previously in the
literature (e.g., Preiss 2002)."
"This paper discusses the problems and possibility of collecting bee dance
data in a linguistic \textit{corpus} and use linguistic instruments such as
Zipf's law and entropy statistics to decide on the question whether the dance
carries information of any kind. We describe this against the historical
background of attempts to analyse non-human communication systems."
"We report on a recently initiated project which aims at building a
multi-layered parallel treebank of English and German. Particular attention is
devoted to a dedicated predicate-argument layer which is used for aligning
translationally equivalent sentences of the two languages. We describe both our
conceptual decisions and aspects of their technical realisation. We discuss
some selected problems and conclude with a few remarks on how this project
relates to similar projects in the field."
"Designers of statistical machine translation (SMT) systems have begun to
employ tree-structured translation models. Systems involving tree-structured
translation models tend to be complex. This article aims to reduce the
conceptual complexity of such systems, in order to make them easier to design,
implement, debug, use, study, understand, explain, modify, and improve. In
service of this goal, the article extends the theory of semiring parsing to
arrive at a novel abstract parsing algorithm with five functional parameters: a
logic, a grammar, a semiring, a search strategy, and a termination condition.
The article then shows that all the common algorithms that revolve around
tree-structured translation models, including hierarchical alignment, inference
for parameter estimation, translation, and structured evaluation, can be
derived by generalizing two of these parameters -- the grammar and the logic.
The article culminates with a recipe for using such generalized parsers to
train, apply, and evaluate an SMT system that is driven by tree-structured
translation models."
"We are developing an automatic method to compile an encyclopedic corpus from
the Web. In our previous work, paragraph-style descriptions for a term are
extracted from Web pages and organized based on domains. However, these
descriptions are independent and do not comprise a condensed text as in
hand-crafted encyclopedias. To resolve this problem, we propose a summarization
method, which produces a single text from multiple descriptions. The resultant
summary concisely describes a term from different viewpoints. We also show the
effectiveness of our method by means of experiments."
"We are developing a cross-media information retrieval system, in which users
can view specific segments of lecture videos by submitting text queries. To
produce a text index, the audio track is extracted from a lecture video and a
transcription is generated by automatic speech recognition. In this paper, to
improve the quality of our retrieval system, we extensively investigate the
effects of adapting acoustic and language models on speech recognition. We
perform an MLLR-based method to adapt an acoustic model. To obtain a corpus for
language model adaptation, we use the textbook for a target lecture to search a
Web collection for the pages associated with the lecture topic. We show the
effectiveness of our method by means of experiments."
"We integrate automatic speech recognition (ASR) and question answering (QA)
to realize a speech-driven QA system, and evaluate its performance. We adapt an
N-gram language model to natural language questions, so that the input of our
system can be recognized with a high accuracy. We target WH-questions which
consist of the topic part and fixed phrase used to ask about something. We
first produce a general N-gram model intended to recognize the topic and
emphasize the counts of the N-grams that correspond to the fixed phrases. Given
a transcription by the ASR engine, the QA engine extracts the answer candidates
from target documents. We propose a passage retrieval method robust against
recognition errors in the transcription. We use the QA test collection produced
in NTCIR, which is a TREC-style evaluation workshop, and show the effectiveness
of our method by means of experiments."
"This paper describes a novel method of compiling ranked tagging rules into a
deterministic finite-state device called a bimachine. The rules are formulated
in the framework of regular rewrite operations and allow unrestricted regular
expressions in both left and right rule contexts. The compiler is illustrated
by an application within a speech synthesis system."
"The Metaphone algorithm applies the phonetic encoding of orthographic
sequences to simplify words prior to comparison. While Metaphone has been
highly successful for the English language, for which it was designed, it may
not be applied directly to Ethiopian languages. The paper details how the
principles of Metaphone can be applied to Ethiopic script and uses Amharic as a
case study. Match results improve as specific considerations are made for
Amharic writing practices. Results are shown to improve further when common
errors from Amharic input methods are considered."
"The aim of this paper is to present the R&D activities carried out at
Neurosoft S.A. regarding the development of proofing tools for Modern Greek.
Firstly, we focus on infrastructure issues that we faced during our initial
steps. Subsequently, we describe the most important insights of three proofing
tools developed by Neurosoft, i.e. the spelling checker, the hyphenator and the
thesaurus, outlining their efficiencies and inefficiencies. Finally, we discuss
some improvement ideas and give our future directions."
"A way of extracting French verbal chunks, inflected and infinitive, is
explored and tested on effective corpus. Declarative morphological and local
grammar rules specifying chunks and some simple contextual structures are used,
relying on limited lexical information and some simple heuristic/statistic
properties obtained from restricted corpora. The specific goals, the
architecture and the formalism of the system, the linguistic information on
which it relies and the obtained results on effective corpus are presented."
"The existence of a Dictionary in electronic form for Modern Greek (MG) is
mandatory if one is to process MG at the morphological and syntactic levels
since MG is a highly inflectional language with marked stress and a spelling
system with many characteristics carried over from Ancient Greek. Moreover,
such a tool becomes necessary if one is to create efficient and sophisticated
NLP applications with substantial linguistic backing and coverage. The present
paper will focus on the deployment of such an electronic dictionary for Modern
Greek, which was built in two phases: first it was constructed to be the basis
for a spelling correction schema and then it was reconstructed in order to
become the platform for the deployment of a wider spectrum of NLP tools."
"While alignment of texts on the sentential level is often seen as being too
coarse, and word alignment as being too fine-grained, bi- or multilingual texts
which are aligned on a level in-between are a useful resource for many
purposes. Starting from a number of examples of non-literal translations, which
tend to make alignment difficult, we describe an alignment model which copes
with these cases by explicitly coding them. The model is based on
predicate-argument structures and thus covers the middle ground between
sentence and word alignment. The model is currently used in a recently
initiated project of a parallel English-German treebank (FuSe), which can in
principle be extended with additional languages."
"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span;
an example application is classifying a movie review as ""thumbs up"" or ""thumbs
down"". To determine this sentiment polarity, we propose a novel
machine-learning method that applies text-categorization techniques to just the
subjective portions of the document. Extracting these portions can be
implemented using efficient techniques for finding minimum cuts in graphs; this
greatly facilitates incorporation of cross-sentence contextual constraints."
"The paper gives a brief review of the expectation-maximization algorithm
(Dempster 1977) in the comprehensible framework of discrete mathematics. In
Section 2, two prominent estimation methods, the relative-frequency estimation
and the maximum-likelihood estimation are presented. Section 3 is dedicated to
the expectation-maximization algorithm and a simpler variant, the generalized
expectation-maximization algorithm. In Section 4, two loaded dice are rolled. A
more interesting example is presented in Section 5: The estimation of
probabilistic context-free grammars."
"We briefly review the inside-outside and EM algorithm for probabilistic
context-free grammars. As a result, we formally prove that inside-outside
estimation is a dynamic-programming variant of EM. This is interesting in its
own right, but even more when considered in a theoretical context since the
well-known convergence behavior of inside-outside estimation has been confirmed
by many experiments but apparently has never been formally proved. However,
being a version of EM, inside-outside estimation also inherits the good
convergence behavior of EM. Therefore, the as yet imperfect line of
argumentation can be transformed into a coherent proof."
"Several Networks of Excellence have been set up in the framework of the
European FP5 research program. Among these Networks of Excellence, the NEMIS
project focuses on the field of Text Mining.
  Within this field, document processing and visualization was identified as
one of the key topics and the WG1 working group was created in the NEMIS
project, to carry out a detailed survey of techniques associated with the text
mining process and to identify the relevant research topics in related research
areas.
  In this document we present the results of this comprehensive survey. The
report includes a description of the current state-of-the-art and practice, a
roadmap for follow-up research in the identified areas, and recommendations for
anticipated technological development in the domain of text mining."
"Contrarily to standard approaches to topic annotation, the technique used in
this work does not centrally rely on some sort of -- possibly statistical --
keyword extraction. In fact, the proposed annotation algorithm uses a large
scale semantic database -- the EDR Electronic Dictionary -- that provides a
concept hierarchy based on hyponym and hypernym relations. This concept
hierarchy is used to generate a synthetic representation of the document by
aggregating the words present in topically homogeneous document segments into a
set of concepts best preserving the document's content.
  This new extraction technique uses an unexplored approach to topic selection.
Instead of using semantic similarity measures based on a semantic resource, the
later is processed to extract the part of the conceptual hierarchy relevant to
the document content. Then this conceptual hierarchy is searched to extract the
most relevant set of concepts to represent the topics discussed in the
document. Notice that this algorithm is able to extract generic concepts that
are not directly present in the document."
"In this paper we describe a biography summarization system using sentence
classification and ideas from information retrieval. Although the individual
techniques are not new, assembling and applying them to generate multi-document
biographies is new. Our system was evaluated in DUC2004. It is among the top
performers in task 5-short summaries focused by person questions."
"A general-audience introduction to the area of ""sentiment analysis"", the
computational treatment of subjective, opinion-oriented language (an example
application is determining whether a review is ""thumbs up"" or ""thumbs down"").
Some challenges, applications to business-intelligence tasks, and potential
future directions are described."
"This article describes the results of a systematic in-depth study of the
criteria used for word sense disambiguation. Our study is based on 60 target
words: 20 nouns, 20 adjectives and 20 verbs. Our results are not always in line
with some practices in the field. For example, we show that omitting
non-content words decreases performance and that bigrams yield better results
than unigrams."
"The goal of this work is to recover articulatory information from the speech
signal by acoustic-to-articulatory inversion. One of the main difficulties with
inversion is that the problem is underdetermined and inversion methods
generally offer no guarantee on the phonetical realism of the inverse
solutions. A way to adress this issue is to use additional phonetic
constraints. Knowledge of the phonetic caracteristics of French vowels enable
the derivation of reasonable articulatory domains in the space of Maeda
parameters: given the formants frequencies (F1,F2,F3) of a speech sample, and
thus the vowel identity, an ""ideal"" articulatory domain can be derived. The
space of formants frequencies is partitioned into vowels, using either
speaker-specific data or generic information on formants. Then, to each
articulatory vector can be associated a phonetic score varying with the
distance to the ""ideal domain"" associated with the corresponding vowel.
Inversion experiments were conducted on isolated vowels and vowel-to-vowel
transitions. Articulatory parameters were compared with those obtained without
using these constraints and those measured from X-ray data."
"This paper presents an ""elitist approach"" for extracting automatically
well-realized speech sounds with high confidence. The elitist approach uses a
speech recognition system based on Hidden Markov Models (HMM). The HMM are
trained on speech sounds which are systematically well-detected in an iterative
procedure. The results show that, by using the HMM models defined in the
training phase, the speech recognizer detects reliably specific speech sounds
with a small rate of errors."
"In the paper, a complex statistical characteristics of a Ukrainian novel is
given for the first time. The distribution of word-forms with respect to their
size is studied. The linguistic laws by Zipf-Mandelbrot and Altmann-Menzerath
are analyzed."
"In this paper we propose some new measures of language development using
network analyses, which is inspired by the recent surge of interests in network
studies of many real-world systems. Children's and care-takers' speech data
from a longitudinal study are represented as a series of networks, word forms
being taken as nodes and collocation of words as links. Measures on the
properties of the networks, such as size, connectivity, hub and authority
analyses, etc., allow us to make quantitative comparison so as to reveal
different paths of development. For example, the asynchrony of development in
network size and average degree suggests that children cannot be simply
classified as early talkers or late talkers by one or two measures. Children
follow different paths in a multi-dimensional space. They may develop faster in
one dimension but slower in another dimension. The network approach requires
little preprocessing of words and analyses on sentence structures, and the
characteristics of words and their usage emerge from the network and are
independent of any grammatical presumptions. We show that the change of the two
articles ""the"" and ""a"" in their roles as important nodes in the network
reflects the progress of children's syntactic development: the two articles
often start in children's networks as hubs and later shift to authorities,
while they are authorities constantly in the adult's networks. The network
analyses provide a new approach to study language development, and at the same
time language development also presents a rich area for network theories to
explore."
"This paper presents the TermSciences portal, which deals with the
implementation of a conceptual model that uses the recent ISO 16642 standard
(Terminological Markup Framework). This standard turns out to be suitable for
concept modeling since it allowed for organizing the original resources by
concepts and to associate the various terms for a given concept. Additional
structuring is produced by sharing conceptual relationships, that is,
cross-linking of resource results through the introduction of semantic
relations which may have initially be missing."
"A number of serious reasons will convince an increasing amount of researchers
to store their relevant material in centers which we will call ""language
resource archives"". They combine the duty of taking care of long-term
preservation as well as the task to give access to their material to different
user groups. Access here is meant in the sense that an active interaction with
the data will be made possible to support the integration of new data, new
versions or commentaries of all sort. Modern Language Resource Archives will
have to adhere to a number of basic principles to fulfill all requirements and
they will have to be involved in federations to create joint language resource
domains making it even more simple for the researchers to access the data. This
paper makes an attempt to formulate the essential pillars language resource
archives have to adhere to."
"This paper describes an interdisciplinary approach which brings together the
fields of corpus linguistics and translation studies. It presents ongoing work
on the creation of a corpus resource in which translation shifts are explicitly
annotated. Translation shifts denote departures from formal correspondence
between source and target text, i.e. deviations that have occurred during the
translation process. A resource in which such shifts are annotated in a
systematic way will make it possible to study those phenomena that need to be
addressed if machine translation output is to resemble human translation. The
resource described in this paper contains English source texts (parliamentary
proceedings) and their German translations. The shift annotation is based on
predicate-argument structures and proceeds in two steps: first, predicates and
their arguments are annotated monolingually in a straightforward manner. Then,
the corresponding English and German predicates and arguments are aligned with
each other. Whenever a shift - mainly grammatical or semantic -has occurred,
the alignment is tagged accordingly."
"Diagrammatic, analogical or iconic representations are often contrasted with
linguistic or logical representations, in which the shape of the symbols is
arbitrary. The aim of this paper is to make a case for the usefulness of
diagrams in inferential knowledge representation systems. Although commonly
used, diagrams have for a long time suffered from the reputation of being only
a heuristic tool or a mere support for intuition. The first part of this paper
is an historical background paying tribute to the logicians, psychologists and
computer scientists who put an end to this formal prejudice against diagrams.
The second part is a discussion of their characteristics as opposed to those of
linguistic forms. The last part is aimed at reviving the interest for
heterogeneous representation systems including both linguistic and diagrammatic
representations."
"Studies of different term extractors on a corpus of the biomedical domain
revealed decreasing performances when applied to highly technical texts. The
difficulty or impossibility of customising them to new domains is an additional
limitation. In this paper, we propose to use external terminologies to
influence generic linguistic data in order to augment the quality of the
extraction. The tool we implemented exploits testified terms at different steps
of the process: chunking, parsing and extraction of term candidates.
Experiments reported here show that, using this method, more term candidates
can be acquired with a higher level of reliability. We further describe the
extraction process involving endogenous disambiguation implemented in the term
extractor YaTeA."
"The paper aims at emphasizing that, even relaxed, the hypothesis of
compositionality has to face many problems when used for interpreting natural
language texts. Rather than fixing these problems within the compositional
framework, we believe that a more radical change is necessary, and propose
another approach."
"The paper concerns the understanding of plurals in the framework of
Artificial Intelligence and emphasizes the role of time. The construction of
collection(s) and their evolution across time is often crucial and has to be
accounted for. The paper contrasts a ""de dicto"" collection where the collection
can be considered as persisting over these situations even if its members
change with a ""de re"" collection whose composition does not vary through time.
It expresses different criteria of choice between the two interpretations (de
re and de dicto) depending on the context of enunciation."
"We present a new, unique and freely available parallel corpus containing
European Union (EU) documents of mostly legal nature. It is available in all 20
official EUanguages, with additional documents being available in the languages
of the EU candidate countries. The corpus consists of almost 8,000 documents
per language, with an average size of nearly 9 million words per language.
Pair-wise paragraph alignment information produced by two different aligners
(Vanilla and HunAlign) is available for all 190+ language pair combinations.
Most texts have been manually classified according to the EUROVOC subject
domains so that the collection can also be used to train and test multi-label
classification algorithms and keyword-assignment software. The corpus is
encoded in XML, according to the Text Encoding Initiative Guidelines. Due to
the large number of parallel texts in many languages, the JRC-Acquis is
particularly suitable to carry out all types of cross-language research, as
well as to test and benchmark text analysis software across different languages
(for instance for alignment, sentence splitting and term extraction)."
"DepAnn is an interactive annotation tool for dependency treebanks, providing
both graphical and text-based annotation interfaces. The tool is aimed for
semi-automatic creation of treebanks. It aids the manual inspection and
correction of automatically created parses, making the annotation process
faster and less error-prone. A novel feature of the tool is that it enables the
user to view outputs from several parsers as the basis for creating the final
tree to be saved to the treebank. DepAnn uses TIGER-XML, an XML-based general
encoding format for both, representing the parser outputs and saving the
annotated treebank. The tool includes an automatic consistency checker for
sentence structures. In addition, the tool enables users to build structures
manually, add comments on the annotations, modify the tagsets, and mark
sentences for further revision."
"In this paper, current dependencybased treebanks are introduced and analyzed.
The methods used for building the resources, the annotation schemes applied,
and the tools used (such as POS taggers, parsers and annotation software) are
discussed."
"The few available French resources for evaluating linguistic models or
algorithms on other linguistic levels than morpho-syntax are either
insufficient from quantitative as well as qualitative point of view or not
freely accessible. Based on this fact, the FREEBANK project intends to create
French corpora constructed using manually revised output from a hybrid
Constraint Grammar parser and annotated on several linguistic levels
(structure, morpho-syntax, syntax, coreference), with the objective to make
them available on-line for research purposes. Therefore, we will focus on using
standard annotation schemes, integration of existing resources and maintenance
allowing for continuous enrichment of the annotations. Prior to the actual
presentation of the prototype that has been implemented, this paper describes a
generic model for the organization and deployment of a linguistic resource
archive, in compliance with the various works currently conducted within
international standardization initiatives (TEI and ISO/TC 37/SC 4)."
"While a great effort has concerned the development of fully integrated
modular understanding systems, few researches have focused on the problem of
unifying existing linguistic formalisms with cognitive processing models. The
Situated Constructional Interpretation Model is one of these attempts. In this
model, the notion of ""construction"" has been adapted in order to be able to
mimic the behavior of Production Systems. The Construction Grammar approach
establishes a model of the relations between linguistic forms and meaning, by
the mean of constructions. The latter can be considered as pairings from a
topologically structured space to an unstructured space, in some way a special
kind of production rules."
"MontyLingua, an integral part of ConceptNet which is currently the largest
commonsense knowledge base, is an English text processor developed using Python
programming language in MIT Media Lab. The main feature of MontyLingua is the
coverage for all aspects of English text processing from raw input text to
semantic meanings and summary generation, yet each component in MontyLingua is
loosely-coupled to each other at the architectural and code level, which
enabled individual components to be used independently or substituted. However,
there has been no review exploring the role of MontyLingua in recent research
work utilizing it. This paper aims to review the use of and roles played by
MontyLingua and its components in research work published in 19 articles
between October 2004 and August 2006. We had observed a diversified use of
MontyLingua in many different areas, both generic and domain-specific. Although
the use of text summarizing component had not been observe, we are optimistic
that it will have a crucial role in managing the current trend of information
overload in future research."
"This paper introduces how human languages can be studied in light of recent
development of network theories. There are two directions of exploration. One
is to study networks existing in the language system. Various lexical networks
can be built based on different relationships between words, being semantic or
syntactic. Recent studies have shown that these lexical networks exhibit
small-world and scale-free features. The other direction of exploration is to
study networks of language users (i.e. social networks of people in the
linguistic community), and their role in language evolution. Social networks
also show small-world and scale-free features, which cannot be captured by
random or regular network models. In the past, computational models of language
change and language emergence often assume a population to have a random or
regular structure, and there has been little discussion how network structures
may affect the dynamics. In the second part of the paper, a series of
simulation models of diffusion of linguistic innovation are used to illustrate
the importance of choosing realistic conditions of population structure for
modeling language change. Four types of social networks are compared, which
exhibit two categories of diffusion dynamics. While the questions about which
type of networks are more appropriate for modeling still remains, we give some
preliminary suggestions for choosing the type of social networks for modeling."
"High dimensional, sparsely populated data spaces have been characterized in
terms of ultrametric topology. This implies that there are natural, not
necessarily unique, tree or hierarchy structures defined by the ultrametric
topology. In this note we study the extent of local ultrametric topology in
texts, with the aim of finding unique ``fingerprints'' for a text or corpus,
discriminating between texts from different domains, and opening up the
possibility of exploiting hierarchical structures in the data. We use coherent
and meaningful collections of over 1000 texts, comprising over 1.3 million
words."
"In the paper, the definition of clause suitable for an automated processing
of a Ukrainian text is proposed. The Menzerath-Altmann law is verified on the
sentence level and the parameters for the dependences of the clause length
counted in words and syllables on the sentence length counted in clauses are
calculated for ""Perekhresni Stezhky"" (""The Cross-Paths""), a novel by Ivan
Franko."
"In numerous domains in cognitive science it is often useful to have a source
for randomly generated corpora. These corpora may serve as a foundation for
artificial stimuli in a learning experiment (e.g., Ellefson & Christiansen,
2000), or as input into computational models (e.g., Christiansen & Dale, 2001).
The following compact and general C program interprets a phrase-structure
grammar specified in a text file. It follows parameters set at a Unix or
Unix-based command-line and generates a corpus of random sentences from that
grammar."
"This paper includes a reflection on the role of networks in the study of
English language acquisition, as well as a collection of practical criteria to
annotate free-speech corpora from children utterances. At the theoretical
level, the main claim of this paper is that syntactic networks should be
interpreted as the outcome of the use of the syntactic machinery. Thus, the
intrinsic features of such machinery are not accessible directly from (known)
network properties. Rather, what one can see are the global patterns of its use
and, thus, a global view of the power and organization of the underlying
grammar. Taking a look into more practical issues, the paper examines how to
build a net from the projection of syntactic relations. Recall that, as opposed
to adult grammars, early-child language has not a well-defined concept of
structure. To overcome such difficulty, we develop a set of systematic criteria
assuming constituency hierarchy and a grammar based on lexico-thematic
relations. At the end, what we obtain is a well defined corpora annotation that
enables us i) to perform statistics on the size of structures and ii) to build
a network from syntactic relations over which we can perform the standard
measures of complexity. We also provide a detailed example."
"This paper describes the Linguistic Annotation Framework under development
within ISO TC37 SC4 WG1. The Linguistic Annotation Framework is intended to
serve as a basis for harmonizing existing language resources as well as
developing new ones."
"We show that a general model of lexical information conforms to an abstract
model that reflects the hierarchy of information found in a typical dictionary
entry. We show that this model can be mapped into a well-formed XML document,
and how the XSL transformation language can be used to implement a semantics
defined over the abstract model to enable extraction and manipulation of the
information in any format."
"This paper describes experiments on learning Dutch phonotactic rules using
Inductive Logic Programming, a machine learning discipline based on inductive
logical operators. Two different ways of approaching the problem are
experimented with, and compared against each other as well as with related work
on the task. The results show a direct correspondence between the quality and
informedness of the background knowledge and the constructed theory,
demonstrating the ability of ILP to take good advantage of the prior domain
knowledge available. Further research is outlined."
"The task of finding a criterion allowing to distinguish a text from an
arbitrary set of words is rather relevant in itself, for instance, in the
aspect of development of means for internet-content indexing or separating
signals and noise in communication channels. The Zipf law is currently
considered to be the most reliable criterion of this kind [3]. At any rate,
conventional stochastic word sets do not meet this law. The present paper deals
with one of possible criteria based on the determination of the degree of data
compression."
"In the task of information retrieval the term relevance is taken to mean
formal conformity of a document given by the retrieval system to user's
information query. As a rule, the documents found by the retrieval system
should be submitted to the user in a certain order. Therefore, a retrieval
perceived as a selection of documents formally solving the user's query, should
be supplemented with a certain procedure of processing a relevant set. It would
be natural to introduce a quantitative measure of document conformity to query,
i.e. the relevance measure. Since no single rule exists for the determination
of the relevance measure, we shall consider two of them which are the simplest
in our opinion. The proposed approach does not suppose any restrictions and can
be applied to other relevance measures."
"We discuss the use of model building for temporal representations. We chose
Polish to illustrate our discussion because it has an interesting aspectual
system, but the points we wish to make are not language specific. Rather, our
goal is to develop theoretical and computational tools for temporal model
building tasks in computational semantics. To this end, we present a
first-order theory of time and events which is rich enough to capture
interesting semantic distinctions, and an algorithm which takes minimal models
for first-order theories and systematically attempts to ``perturb'' their
temporal component to provide non-minimal, but semantically significant,
models."
"The aim of this paper is to show how we can handle the Recognising Textual
Entailment (RTE) task by using Description Logics (DLs). To do this, we propose
a representation of natural language semantics in DLs inspired by existing
representations in first-order logic. But our most significant contribution is
the definition of two novel inference tasks: A-Box saturation and subgraph
detection which are crucial for our approach to RTE."
"Despite its importance, the task of summarizing evolving events has received
small attention by researchers in the field of multi-document summariztion. In
a previous paper (Afantenos et al. 2007) we have presented a methodology for
the automatic summarization of documents, emitted by multiple sources, which
describe the evolution of an event. At the heart of this methodology lies the
identification of similarities and differences between the various documents,
in two axes: the synchronic and the diachronic. This is achieved by the
introduction of the notion of Synchronic and Diachronic Relations. Those
relations connect the messages that are found in the documents, resulting thus
in a graph which we call grid. Although the creation of the grid completes the
Document Planning phase of a typical NLG architecture, it can be the case that
the number of messages contained in a grid is very large, exceeding thus the
required compression rate. In this paper we provide some initial thoughts on a
probabilistic model which can be applied at the Content Determination stage,
and which tries to alleviate this problem."
"In this paper we present an automated method for the classification of the
origin of non-native speakers. The origin of non-native speakers could be
identified by a human listener based on the detection of typical pronunciations
for each nationality. Thus we suppose the existence of several phoneme
sequences that might allow the classification of the origin of non-native
speakers. Our new method is based on the extraction of discriminative sequences
of phonemes from a non-native English speech database. These sequences are used
to construct a probabilistic classifier for the speakers' origin. The existence
of discriminative phone sequences in non-native speech is a significant result
of this work. The system that we have developed achieved a significant correct
classification rate of 96.3% and a significant error reduction compared to some
other tested techniques."
"In this paper, we present several adaptation methods for non-native speech
recognition. We have tested pronunciation modelling, MLLR and MAP non-native
pronunciation adaptation and HMM models retraining on the HIWIRE foreign
accented English speech database. The ``phonetic confusion'' scheme we have
developed consists in associating to each spoken phone several sequences of
confused phones. In our experiments, we have used different combinations of
acoustic models representing the canonical and the foreign pronunciations:
spoken and native models, models adapted to the non-native accent with MAP and
MLLR. The joint use of pronunciation modelling and acoustic adaptation led to
further improvements in recognition accuracy. The best combination of the above
mentioned techniques resulted in a relative word error reduction ranging from
46% to 71%."
"In this article, we present an approach for non native automatic speech
recognition (ASR). We propose two methods to adapt existing ASR systems to the
non-native accents. The first method is based on the modification of acoustic
models through integration of acoustic models from the mother tong. The
phonemes of the target language are pronounced in a similar manner to the
native language of speakers. We propose to combine the models of confused
phonemes so that the ASR system could recognize both concurrent
pronounciations. The second method we propose is a refinment of the
pronounciation error detection through the introduction of graphemic
constraints. Indeed, non native speakers may rely on the writing of words in
their uttering. Thus, the pronounctiation errors might depend on the characters
composing the words. The average error rate reduction that we observed is
(22.5%) relative for the sentence error rate, and 34.5% (relative) in word
error rate."
"This paper explores several extensions of proof nets for the Lambek calculus
in order to handle the different connectives of display logic in a natural way.
The new proof net calculus handles some recent additions to the Lambek
vocabulary such as Galois connections and Grishin interactions. It concludes
with an exploration of the generative capacity of the Lambek-Grishin calculus,
presenting an embedding of lexicalized tree adjoining grammars into the
Lambek-Grishin calculus."
"This article describes an exclusively resource-based method of morphological
annotation of written Korean text. Korean is an agglutinative language. Our
annotator is designed to process text before the operation of a syntactic
parser. In its present state, it annotates one-stem words only. The output is a
graph of morphemes annotated with accurate linguistic information. The
granularity of the tagset is 3 to 5 times higher than usual tagsets. A
comparison with a reference annotated corpus showed that it achieves 89% recall
without any corpus training. The language resources used by the system are
lexicons of stems, transducers of suffixes and transducers of generation of
allomorphs. All can be easily updated, which allows users to control the
evolution of the performances of the system. It has been claimed that
morphological annotation of Korean text could only be performed by a
morphological analysis module accessing a lexicon of morphemes. We show that it
can also be performed directly with a lexicon of words and without applying
morphological rules at annotation time, which speeds up annotation to 1,210
word/s. The lexicon of words is obtained from the maintainable language
resources through a fully automated compilation process."
"International standards for lexicon formats are in preparation. To a certain
extent, the proposed formats converge with prior results of standardization
projects. However, their adequacy for (i) lexicon management and (ii)
lexicon-driven applications have been little debated in the past, nor are they
as a part of the present standardization effort. We examine these issues. IGM
has developed XML formats compatible with the emerging international standards,
and we report experimental results on large-coverage lexica."
"Maurice Gross (1934-2001) was both a great linguist and a pioneer in natural
language processing. This article is written in homage to his memory"
"We describe a resource-based method of morphological annotation of written
Korean text. Korean is an agglutinative language. The output of our system is a
graph of morphemes annotated with accurate linguistic information. The language
resources used by the system can be easily updated, which allows us-ers to
control the evolution of the per-formances of the system. We show that
morphological annotation of Korean text can be performed directly with a
lexicon of words and without morpho-logical rules."
"Shifting to a lexicalized grammar reduces the number of parsing errors and
improves application results. However, such an operation affects a syntactic
parser in all its aspects. One of our research objectives is to design a
realistic model for grammar lexicalization. We carried out experiments for
which we used a grammar with a very simple content and formalism, and a very
informative syntactic lexicon, the lexicon-grammar of French elaborated by the
LADL. Lexicalization was performed by applying the parameterized-graph
approach. Our results tend to show that most information in the lexicon-grammar
can be transferred into a grammar and exploited successfully for the syntactic
parsing of sentences."
"Existing syntactic grammars of natural languages, even with a far from
complete coverage, are complex objects. Assessments of the quality of parts of
such grammars are useful for the validation of their construction. We evaluated
the quality of a grammar of French determiners that takes the form of a
recursive transition network. The result of the application of this local
grammar gives deeper syntactic information than chunking or information
available in treebanks. We performed the evaluation by comparison with a corpus
independently annotated with information on determiners. We obtained 86%
precision and 92% recall on text not tagged for parts of speech."
"We discuss the characteristics and behaviour of two parallel classes of verbs
in two Romance languages, French and Portuguese. Examples of these verbs are
Port. abater [gado] and Fr. abattre [b\'etail], both meaning ""slaughter
[cattle]"". In both languages, the definition of the class of verbs includes
several features: - They have only one essential complement, which is a direct
object. - The nominal distribution of the complement is very limited, i.e., few
nouns can be selected as head nouns of the complement. However, this selection
is not restricted to a single noun, as would be the case for verbal idioms such
as Fr. monter la garde ""mount guard"". - We excluded from the class
constructions which are reductions of more complex constructions, e.g. Port.
afinar [instrumento] com ""tune [instrument] with""."
"The Outilex software platform, which will be made available to research,
development and industry, comprises software components implementing all the
fundamental operations of written text processing: processing without lexicons,
exploitation of lexicons and grammars, language resource management. All data
are structured in XML formats, and also in more compact formats, either
readable or binary, whenever necessary; the required format converters are
included in the platform; the grammar formats allow for combining statistical
approaches with resource-based approaches. Manually constructed lexicons for
French and English, originating from the LADL, and of substantial coverage,
will be distributed with the platform under LGPL-LR license."
"Speaking a language and achieving proficiency in another one is a highly
complex process which requires the acquisition of various kinds of knowledge
and skills, like the learning of words, rules and patterns and their connection
to communicative goals (intentions), the usual starting point. To help the
learner to acquire these skills we propose an enhanced, electronic version of
an age old method: pattern drills (henceforth PDs). While being highly regarded
in the fifties, PDs have become unpopular since then, partially because of
their lack of grounding (natural context) and rigidity. Despite these
shortcomings we do believe in the virtues of this approach, at least with
regard to the acquisition of basic linguistic reflexes or skills (automatisms),
necessary to survive in the new language. Of course, the method needs
improvement, and we will show here how this can be achieved. Unlike tapes or
books, computers are open media, allowing for dynamic changes, taking users'
performances and preferences into account. Building an electronic version of
PDs amounts to building an open resource, accomodatable to the users' ever
changing needs."
"This paper discusses two new procedures for extracting verb valences from raw
texts, with an application to the Polish language. The first novel technique,
the EM selection algorithm, performs unsupervised disambiguation of valence
frame forests, obtained by applying a non-probabilistic deep grammar parser and
some post-processing to the text. The second new idea concerns filtering of
incorrect frames detected in the parsed text and is motivated by an observation
that verbs which take similar arguments tend to have similar frames. This
phenomenon is described in terms of newly introduced co-occurrence matrices.
Using co-occurrence matrices, we split filtering into two steps. The list of
valid arguments is first determined for each verb, whereas the pattern
according to which the arguments are combined into frames is computed in the
following stage. Our best extracted dictionary reaches an $F$-score of 45%,
compared to an $F$-score of 39% for the standard frame-based BHT filtering."
"Robustness in a parser refers to an ability to deal with exceptional
phenomena. A parser is robust if it deals with phenomena outside its normal
range of inputs. This paper reports on a series of robustness evaluations of
state-of-the-art parsers in which we concentrated on one aspect of robustness:
its ability to parse sentences containing misspelled words. We propose two
measures for robustness evaluation based on a comparison of a parser's output
for grammatical input sentences and their noisy counterparts. In this paper, we
use these measures to compare the overall robustness of the four evaluated
parsers, and we present an analysis of the decline in parser performance with
increasing error levels. Our results indicate that performance typically
declines tens of percentage units when parsers are presented with texts
containing misspellings. When it was tested on our purpose-built test set of
443 sentences, the best parser in the experiment (C&C parser) was able to
return exactly the same parse tree for the grammatical and ungrammatical
sentences for 60.8%, 34.0% and 14.9% of the sentences with one, two or three
misspelled words respectively."
"Most current word prediction systems make use of n-gram language models (LM)
to estimate the probability of the following word in a phrase. In the past
years there have been many attempts to enrich such language models with further
syntactic or semantic information. We want to explore the predictive powers of
Latent Semantic Analysis (LSA), a method that has been shown to provide
reliable information on long-distance semantic dependencies between words in a
context. We present and evaluate here several methods that integrate LSA-based
information with a standard language model: a semantic cache, partial
reranking, and different forms of interpolation. We found that all methods show
significant improvements, compared to the 4-gram baseline, and most of them to
a simple cache model as well."
"We investigate the grapheme-phoneme relation in Ukrainian and some properties
of the Ukrainian version of the Cyrillic alphabet."
"The algorithm of the creation texts parallel corpora was presented. The
algorithm is based on the use of ""key words"" in text documents, and on the
means of their automated translation. Key words were singled out by means of
using Russian and Ukrainian morphological dictionaries, as well as dictionaries
of the translation of nouns for the Russian and Ukrainianlanguages. Besides, to
calculate the weights of the terms in the documents, empiric-statistic rules
were used. The algorithm under consideration was realized in the form of a
program complex, integrated into the content-monitoring InfoStream system. As a
result, a parallel bilingual corpora of web-publications containing about 30
thousand documents, was created"
"In this paper, we present an open-source parsing environment (Tuebingen
Linguistic Parsing Architecture, TuLiPA) which uses Range Concatenation Grammar
(RCG) as a pivot formalism, thus opening the way to the parsing of several
mildly context-sensitive formalisms. This environment currently supports
tree-based grammars (namely Tree-Adjoining Grammars, TAG) and Multi-Component
Tree-Adjoining Grammars with Tree Tuples (TT-MCTAG)) and allows computation not
only of syntactic structures, but also of the corresponding semantic
representations. It is used for the development of a tree-based grammar for
German."
"The paper deals with using descriptive mark-up to emphasize translation
mistakes. The author postulates the necessity to develop a standard and formal
XML-based way of describing translation mistakes. It is considered to be
important for achieving impersonal translation quality assessment. Marked-up
translations can be used in corpus translation studies; moreover, automatic
translation assessment based on marked-up mistakes is possible. The paper
concludes with setting up guidelines for further activity within the described
field."
"In the paper, we analyze the distribution of complexities in the Vai script,
an indigenous syllabic writing system from Liberia. It is found that the
uniformity hypothesis for complexities fails for this script. The models using
Poisson distribution for the number of components and hyper-Poisson
distribution for connections provide good fits in the case of the Vai script."
"In this article, some first elements of a computational modelling of the
grammar of the Martiniquese French Creole dialect are presented. The sources of
inspiration for the modelling is the functional description given by Damoiseau
(1984), and Pinalie's & Bernabe's (1999) grammar manual. Based on earlier works
in text generation (Vaillant, 1997), a unification grammar formalism, namely
Tree Adjoining Grammars (TAG), and a modelling of lexical functional categories
based on syntactic and semantic properties, are used to implement a grammar of
Martiniquese Creole which is used in a prototype of text generation system. One
of the main applications of the system could be its use as a tool software
supporting the task of learning Creole as a second language. -- Nous
pr\'esenterons dans cette communication les premiers travaux de mod\'elisation
informatique d'une grammaire de la langue cr\'eole martiniquaise, en nous
inspirant des descriptions fonctionnelles de Damoiseau (1984) ainsi que du
manuel de Pinalie & Bernab\'e (1999). Prenant appui sur des travaux
ant\'erieurs en g\'en\'eration de texte (Vaillant, 1997), nous utilisons un
formalisme de grammaires d'unification, les grammaires d'adjonction d'arbres
(TAG d'apr\`es l'acronyme anglais), ainsi qu'une mod\'elisation de cat\'egories
lexicales fonctionnelles \`a base syntaxico-s\'emantique, pour mettre en oeuvre
une grammaire du cr\'eole martiniquais utilisable dans une maquette de
syst\`eme de g\'en\'eration automatique. L'un des int\'er\^ets principaux de ce
syst\`eme pourrait \^etre son utilisation comme logiciel outil pour l'aide \`a
l'apprentissage du cr\'eole en tant que langue seconde."
"This article describes the design of a common syntactic description for the
core grammar of a group of related dialects. The common description does not
rely on an abstract sub-linguistic structure like a metagrammar: it consists in
a single FS-LTAG where the actual specific language is included as one of the
attributes in the set of attribute types defined for the features. When the
lang attribute is instantiated, the selected subset of the grammar is
equivalent to the grammar of one dialect. When it is not, we have a model of a
hybrid multidialectal linguistic system. This principle is used for a group of
creole languages of the West-Atlantic area, namely the French-based Creoles of
Haiti, Guadeloupe, Martinique and French Guiana."
"We present the architecture of the UNL-French deconverter, which ""generates""
from the UNL interlingua by first""localizing"" the UNL form for French, within
UNL, and then applying slightly adapted but classical transfer and generation
techniques, implemented in GETA's Ariane-G5 environment, supplemented by some
UNL-specific tools. Online interaction can be used during deconversion to
enhance output quality and is now used for development purposes. We show how
interaction could be delayed and embedded in the postedition phase, which would
then interact not directly with the output text, but indirectly with several
components of the deconverter. Interacting online or offline can improve the
quality not only of the utterance at hand, but also of the utterances processed
later, as various preferences may be automatically changed to let the
deconverter ""learn""."
"Collocations are important for many tasks of Natural language processing such
as information retrieval, machine translation, computational lexicography etc.
So far many statistical methods have been used for collocation extraction.
Almost all the methods form a classical crisp set of collocation. We propose a
fuzzy logic approach of collocation extraction to form a fuzzy set of
collocations in which each word combination has a certain grade of membership
for being collocation. Fuzzy logic provides an easy way to express natural
language into fuzzy logic rules. Two existing methods; Mutual information and
t-test have been utilized for the input of the fuzzy inference system. The
resulting membership function could be easily seen and demonstrated. To show
the utility of the fuzzy logic some word pairs have been examined as an
example. The working data has been based on a corpus of about one million words
contained in different novels constituting project Gutenberg available on
www.gutenberg.org. The proposed method has all the advantages of the two
methods, while overcoming their drawbacks. Hence it provides a better result
than the two methods."
"The Indus script is one of the major undeciphered scripts of the ancient
world. The small size of the corpus, the absence of bilingual texts, and the
lack of definite knowledge of the underlying language has frustrated efforts at
decipherment since the discovery of the remains of the Indus civilisation.
Recently, some researchers have questioned the premise that the Indus script
encodes spoken language. Building on previous statistical approaches, we apply
the tools of statistical language processing, specifically $n$-gram Markov
chains, to analyse the Indus script for syntax. Our main results are that the
script has well-defined signs which begin and end texts, that there is
directionality and strong correlations in the sign order, and that there are
groups of signs which appear to have identical syntactic function. All these
require no {\it a priori} suppositions regarding the syntactic or semantic
content of the signs, but follow directly from the statistical analysis. Using
information theoretic measures, we find the information in the script to be
intermediate between that of a completely random and a completely fixed
ordering of signs. Our study reveals that the Indus script is a structured sign
system showing features of a formal language, but, at present, cannot
conclusively establish that it encodes {\it natural} language. Our $n$-gram
Markov model is useful for predicting signs which are missing or illegible in a
corpus of Indus texts. This work forms the basis for the development of a
stochastic grammar which can be used to explore the syntax of the Indus script
in greater detail."
"Cilibrasi and Vitanyi have demonstrated that it is possible to extract the
meaning of words from the world-wide web. To achieve this, they rely on the
number of webpages that are found through a Google search containing a given
word and they associate the page count to the probability that the word appears
on a webpage. Thus, conditional probabilities allow them to correlate one word
with another word's meaning. Furthermore, they have developed a similarity
distance function that gauges how closely related a pair of words is. We
present a specific counterexample to the triangle inequality for this
similarity distance function."
"A confidence measure is able to estimate the reliability of an hypothesis
provided by a machine translation system. The problem of confidence measure can
be seen as a process of testing : we want to decide whether the most probable
sequence of words provided by the machine translation system is correct or not.
In the following we describe several original word-level confidence measures
for machine translation, based on mutual information, n-gram language model and
lexical features language model. We evaluate how well they perform individually
or together, and show that using a combination of confidence measures based on
mutual information yields a classification error rate as low as 25.1% with an
F-measure of 0.708."
"In this paper we present the first step in a larger series of experiments for
the induction of predicate/argument structures. The structures that we are
inducing are very similar to the conceptual structures that are used in Frame
Semantics (such as FrameNet). Those structures are called messages and they
were previously used in the context of a multi-document summarization system of
evolving events. The series of experiments that we are proposing are
essentially composed from two stages. In the first stage we are trying to
extract a representative vocabulary of words. This vocabulary is later used in
the second stage, during which we apply to it various clustering approaches in
order to identify the clusters of predicates and arguments--or frames and
semantic roles, to use the jargon of Frame Semantics. This paper presents in
detail and evaluates the first stage."
"We report experiments about the syntactic variations of support verb
constructions, a special type of multiword expressions (MWEs) containing
predicative nouns. In these expressions, the noun can occur with or without the
verb, with no clear-cut semantic difference. We extracted from a large French
corpus a set of examples of the two situations and derived statistical results
from these data. The extraction involved large-coverage language resources and
finite-state techniques. The results show that, most frequently, predicative
nouns occur without a support verb. This fact has consequences on methods of
extracting or recognising MWEs."
"The paper reviews the hurdles while trying to implement the OLAC extension
for Dravidian / Indian languages. The paper further explores the possibilities
which could minimise or solve these problems. In this context, the Chinese
system of text processing and the anusaaraka system are scrutinised."
"Sandhi means to join two or more words to coin new word. Sandhi literally
means `putting together' or combining (of sounds), It denotes all combinatory
sound-changes effected (spontaneously) for ease of pronunciation.
Sandhi-vicheda describes [5] the process by which one letter (whether single or
cojoined) is broken to form two words. Part of the broken letter remains as the
last letter of the first word and part of the letter forms the first letter of
the next letter. Sandhi- Vicheda is an easy and interesting way that can give
entirely new dimension that add new way to traditional approach to Hindi
Teaching. In this paper using the Rule based algorithm we have reported an
accuracy of 60-80% depending upon the number of rules to be implemented."
"Following the principles of Cognitive Grammar, we concentrate on a model for
reference resolution that attempts to overcome the difficulties previous
approaches, based on the fundamental assumption that all reference (independent
on the type of the referring expression) is accomplished via access to and
restructuring of domains of reference rather than by direct linkage to the
entities themselves. The model accounts for entities not explicitly mentioned
but understood in a discourse, and enables exploitation of discursive and
perceptual context to limit the set of potential referents for a given
referring expression. As the most important feature, we note that a single
mechanism is required to handle what are typically treated as diverse
phenomena. Our approach, then, provides a fresh perspective on the relations
between Cognitive Grammar and the problem of reference."
"We describe an encoding scheme for discourse structure and reference, based
on the TEI Guidelines and the recommendations of the Corpus Encoding
Specification (CES). A central feature of the scheme is a CES-based data
architecture enabling the encoding of and access to multiple views of a
marked-up document. We describe a tool architecture that supports the encoding
scheme, and then show how we have used the encoding scheme and the tools to
perform a discourse analytic task in support of a model of global discourse
cohesion called Veins Theory (Cristea & Ide, 1998)."
"It is widely recognized that the proliferation of annotation schemes runs
counter to the need to re-use language resources, and that standards for
linguistic annotation are becoming increasingly mandatory. To answer this need,
we have developed a framework comprised of an abstract model for a variety of
different annotation types (e.g., morpho-syntactic tagging, syntactic
annotation, co-reference annotation, etc.), which can be instantiated in
different ways depending on the annotator's approach and goals. In this paper
we provide an overview of the framework, demonstrate its applicability to
syntactic annotation, and show how it can contribute to comparative evaluation
of parser output and diverse syntactic annotation schemes."
"This paper presents an abstract data model for linguistic annotations and its
implementation using XML, RDF and related standards; and to outline the work of
a newly formed committee of the International Standards Organization (ISO),
ISO/TC 37/SC 4 Language Resource Management, which will use this work as its
starting point. The primary motive for presenting the latter is to solicit the
participation of members of the research community to contribute to the work of
the committee."
"Handwriting is an alternative method for entering texts composing Short
Message Services. However, a whole new language features the texts which are
produced. They include for instance abbreviations and other consonantal writing
which sprung up for time saving and fashion. We have collected and processed a
significant number of such handwriting SMS, and used various strategies to
tackle this challenging area of handwriting recognition. We proposed to study
more specifically three different phenomena: consonant skeleton, rebus, and
phonetic writing. For each of them, we compare the rough results produced by a
standard recognition system with those obtained when using a specific language
model."
"Handwriting is an alternative method for entering texts which composed Short
Message Services. However, a whole new language features the texts which are
produced. They include for instance abbreviations and other consonantal writing
which sprung up for time saving and fashion. We have collected and processed a
significant number of such handwritten SMS, and used various strategies to
tackle this challenging area of handwriting recognition. We proposed to study
more specifically three different phenomena: consonant skeleton, rebus, and
phonetic writing. For each of them, we compare the rough results produced by a
standard recognition system with those obtained when using a specific language
model to take care of them."
"This article proposes a method to extract dependency structures from
phrase-structure level parsing with Interaction Grammars. Interaction Grammars
are a formalism which expresses interactions among words using a polarity
system. Syntactical composition is led by the saturation of polarities.
Interactions take place between constituents, but as grammars are lexicalized,
these interactions can be translated at the level of words. Dependency
relations are extracted from the parsing process: every dependency is the
consequence of a polarity saturation. The dependency relations we obtain can be
seen as a refinement of the usual dependency tree. Generally speaking, this
work sheds new light on links between phrase structure and dependency parsing."
"We present a method for grouping the synonyms of a lemma according to its
dictionary senses. The senses are defined by a large machine readable
dictionary for French, the TLFi (Tr\'esor de la langue fran\c{c}aise
informatis\'e) and the synonyms are given by 5 synonym dictionaries (also for
French). To evaluate the proposed method, we manually constructed a gold
standard where for each (word, definition) pair and given the set of synonyms
defined for that word by the 5 synonym dictionaries, 4 lexicographers specified
the set of synonyms they judge adequate. While inter-annotator agreement ranges
on that task from 67% to at best 88% depending on the annotator pair and on the
synonym dictionary being considered, the automatic procedure we propose scores
a precision of 67% and a recall of 71%. The proposed method is compared with
related work namely, word sense disambiguation, synonym lexicon acquisition and
WordNet construction."
"There are many scientific problems generated by the multiple and conflicting
alternative definitions of linguistic recursion and human recursive processing
that exist in the literature. The purpose of this article is to make available
to the linguistic community the standard mathematical definition of recursion
and to apply it to discuss linguistic recursion. As a byproduct, we obtain an
insight into certain ""soft universals"" of human languages, which are related to
cognitive constructs necessary to implement mathematical reasoning, i.e.
mathematical model theory."
"Multimodal interfaces, combining the use of speech, graphics, gestures, and
facial expressions in input and output, promise to provide new possibilities to
deal with information in more effective and efficient ways, supporting for
instance: - the understanding of possibly imprecise, partial or ambiguous
multimodal input; - the generation of coordinated, cohesive, and coherent
multimodal presentations; - the management of multimodal interaction (e.g.,
task completion, adapting the interface, error prevention) by representing and
exploiting models of the user, the domain, the task, the interactive context,
and the media (e.g. text, audio, video). The present document is intended to
support the discussion on multimodal content representation, its possible
objectives and basic constraints, and how the definition of a generic
representation framework for multimodal content representation may be
approached. It takes into account the results of the Dagstuhl workshop, in
particular those of the informal working group on multimodal meaning
representation that was active during the workshop (see
http://www.dfki.de/~wahlster/Dagstuhl_Multi_Modality, Working Group 4)."
"Automated language processing is central to the drive to enable facilitated
referencing of increasingly available Sanskrit E texts. The first step towards
processing Sanskrit text involves the handling of Sanskrit compound words that
are an integral part of Sanskrit texts. This firstly necessitates the
processing of euphonic conjunctions or sandhis, which are points in words or
between words, at which adjacent letters coalesce and transform. The ancient
Sanskrit grammarian Panini's codification of the Sanskrit grammar is the
accepted authority in the subject. His famed sutras or aphorisms, numbering
approximately four thousand, tersely, precisely and comprehensively codify the
rules of the grammar, including all the rules pertaining to sandhis. This work
presents a fresh new approach to processing sandhis in terms of a computational
schema. This new computational model is based on Panini's complex codification
of the rules of grammar. The model has simple beginnings and is yet powerful,
comprehensive and computationally lean."
"Artificial Neural Network (ANN) s has widely been used for recognition of
optically scanned character, which partially emulates human thinking in the
domain of the Artificial Intelligence. But prior to recognition, it is
necessary to segment the character from the text to sentences, words etc.
Segmentation of words into individual letters has been one of the major
problems in handwriting recognition. Despite several successful works all over
the work, development of such tools in specific languages is still an ongoing
process especially in the Indian context. This work explores the application of
ANN as an aid to segmentation of handwritten characters in Assamese- an
important language in the North Eastern part of India. The work explores the
performance difference obtained in applying an ANN-based dynamic segmentation
algorithm compared to projection- based static segmentation. The algorithm
involves, first training of an ANN with individual handwritten characters
recorded from different individuals. Handwritten sentences are separated out
from text using a static segmentation method. From the segmented line,
individual characters are separated out by first over segmenting the entire
line. Each of the segments thus obtained, next, is fed to the trained ANN. The
point of segmentation at which the ANN recognizes a segment or a combination of
several segments to be similar to a handwritten character, a segmentation
boundary for the character is assumed to exist and segmentation performed. The
segmented character is next compared to the best available match and the
segmentation boundary confirmed."
"This paper presents a theoretical research based approach to ellipsis
resolution in machine translation. The formula of discourse is applied in order
to resolve ellipses. The validity of the discourse formula is analyzed by
applying it to the real world text, i.e., newspaper fragments. The source text
is converted into mono-sentential discourses where complex discourses require
further dissection either directly into primitive discourses or first into
compound discourses and later into primitive ones. The procedure of dissection
needs further improvement, i.e., discovering as many primitive discourse forms
as possible. An attempt has been made to investigate new primitive discourses
or patterns from the given text."
"This paper presents a mechanism of resolving unidentified lexical units in
Text-based Machine Translation (TBMT). In a Machine Translation (MT) system it
is unlikely to have a complete lexicon and hence there is intense need of a new
mechanism to handle the problem of unidentified words. These unknown words
could be abbreviations, names, acronyms and newly introduced terms. We have
proposed an algorithm for the resolution of the unidentified words. This
algorithm takes discourse unit (primitive discourse) as a unit of analysis and
provides real time updates to the lexicon. We have manually applied the
algorithm to news paper fragments. Along with anaphora and cataphora
resolution, many unknown words especially names and abbreviations were updated
to the lexicon."
"The goal of this paper is two-fold: to present an abstract data model for
linguistic annotations and its implementation using XML, RDF and related
standards; and to outline the work of a newly formed committee of the
International Standards Organization (ISO), ISO/TC 37/SC 4 Language Resource
Management, which will use this work as its starting point."
"A simple method for finding the entropy and redundancy of a reasonable long
sample of English text by direct computer processing and from first principles
according to Shannon theory is presented. As an example, results on the entropy
of the English language have been obtained based on a total of 20.3 million
characters of written English, considering symbols from one to five hundred
characters in length. Besides a more realistic value of the entropy of English,
a new perspective on some classic entropy-related concepts is presented. This
method can also be extended to other Latin languages. Some implications for
practical applications such as plagiarism-detection software, and the minimum
number of words that should be used in social Internet network messaging, are
discussed."
"A survey of dictionary models and formats is presented as well as a
presentation of corresponding recent standardisation activities."
"A natural language (or ordinary language) is a language that is spoken,
written, or signed by humans for general-purpose communication, as
distinguished from formal languages (such as computer-programming languages or
the ""languages"" used in the study of formal logic). The computational
activities required for enabling a computer to carry out information processing
using natural language is called natural language processing. We have taken
Assamese language to check the grammars of the input sentence. Our aim is to
produce a technique to check the grammatical structures of the sentences in
Assamese text. We have made grammar rules by analyzing the structures of
Assamese sentences. Our parsing program finds the grammatical errors, if any,
in the Assamese sentence. If there is no error, the program will generate the
parse tree for the Assamese sentence"
"In this chapter we present the main issues in representing machine readable
dictionaries in XML, and in particular according to the Text Encoding
Dictionary (TEI) guidelines."
"In this article, we record the main linguistic differences or singularities
of 17th century English, analyse them morphologically and syntactically and
propose equivalent forms in contemporary English. We show how 17th century
texts may be transcribed into modern English, combining the use of electronic
dictionaries with rules of transcription implemented as transducers. Apr\`es
avoir expos\'e la constitution du corpus, nous recensons les principales
diff\'erences ou particularit\'es linguistiques de la langue anglaise du XVIIe
si\`ecle, les analysons du point de vue morphologique et syntaxique et
proposons des \'equivalents en anglais contemporain (AC). Nous montrons comment
nous pouvons effectuer une transcription automatique de textes anglais du XVIIe
si\`ecle en anglais moderne, en combinant l'utilisation de dictionnaires
\'electroniques avec des r\`egles de transcriptions impl\'ement\'ees sous forme
de transducteurs."
"If the use of the apostrophe in contemporary English often marks the Saxon
genitive, it may also indicate the omission of one or more let-ters. Some
writers (wrongly?) use it to mark the plural in symbols or abbreviations,
visual-ised thanks to the isolation of the morpheme ""s"". This punctuation mark
was imported from the Continent in the 16th century. During the 19th century
its use was standardised. However the rules of its usage still seem problematic
to many, including literate speakers of English. ""All too often, the apostrophe
is misplaced"", or ""errant apostrophes are springing up every-where"" is a
complaint that Internet users fre-quently come across when visiting grammar
websites. Many of them detail its various uses and misuses, and attempt to
correct the most common mistakes about it, especially its mis-use in the
plural, called greengrocers' apostro-phes and humorously misspelled
""greengro-cers apostrophe's"". While studying English travel accounts published
in the seventeenth century, we noticed that the different uses of this symbol
may accompany various models of metaplasms. We were able to highlight the
linguistic variations of some lexemes, and trace the origin of modern grammar
rules gov-erning its usage."
"The recognition of Arabic Named Entities (NE) is a problem in different
domains of Natural Language Processing (NLP) like automatic translation.
Indeed, NE translation allows the access to multilingual in-formation. This
translation doesn't always lead to expected result especially when NE contains
a person name. For this reason and in order to ameliorate translation, we can
transliterate some part of NE. In this context, we propose a method that
integrates translation and transliteration together. We used the linguis-tic
NooJ platform that is based on local grammars and transducers. In this paper,
we focus on sport domain. We will firstly suggest a refinement of the
typological model presented at the MUC Conferences we will describe the
integration of an Arabic transliteration module into translation system.
Finally, we will detail our method and give the results of the evaluation."
"We are developing electronic dictionaries and transducers for the automatic
processing of the Albanian Language. We will analyze the words inside a linear
segment of text. We will also study the relationship between units of sense and
units of form. The composition of words takes different forms in Albanian. We
have found that morphemes are frequently concatenated or simply juxtaposed or
contracted. The inflected grammar of NooJ allows constructing the dictionaries
of flexed forms (declensions or conjugations). The diversity of word structures
requires tools to identify words created by simple concatenation, or to treat
contractions. The morphological tools of NooJ allow us to create grammatical
tools to represent and treat these phenomena. But certain problems exceed the
morphological analysis and must be represented by syntactical grammars."
"Maximum mutual information (MMI) is a model selection criterion used for
hidden Markov model (HMM) parameter estimation that was developed more than
twenty years ago as a discriminative alternative to the maximum likelihood
criterion for HMM-based speech recognition. It has been shown in the speech
recognition literature that parameter estimation using the current MMI
paradigm, lattice-based MMI, consistently outperforms maximum likelihood
estimation, but this is at the expense of undesirable convergence properties.
In particular, recognition performance is sensitive to the number of times that
the iterative MMI estimation algorithm, extended Baum-Welch, is performed. In
fact, too many iterations of extended Baum-Welch will lead to degraded
performance, despite the fact that the MMI criterion improves at each
iteration. This phenomenon is at variance with the analogous behavior of
maximum likelihood estimation -- at least for the HMMs used in speech
recognition -- and it has previously been attributed to `over fitting'. In this
paper, we present an analysis of lattice-based MMI that demonstrates, first of
all, that the asymptotic behavior of lattice-based MMI is much worse than was
previously understood, i.e. it does not appear to converge at all, and, second
of all, that this is not due to `over fitting'. Instead, we demonstrate that
the `over fitting' phenomenon is the result of standard methodology that
exacerbates the poor behavior of two key approximations in the lattice-based
MMI machinery. We also demonstrate that if we modify the standard methodology
to improve the validity of these approximations, then the convergence
properties of lattice-based MMI become benign without sacrificing improvements
to recognition accuracy."
"Using Pustejovsky's ""The Syntax of Event Structure"" and Fong's ""On Mending a
Torn Dress"" we give a glimpse of a Pustejovsky-like analysis to some example
sentences in Fong. We attempt to give a framework for semantics to the noun
phrases and adverbs as appropriate as well as the lexical entries for all words
in the examples and critique both papers in light of our findings and
difficulties."
"This document discusses an approach and its rudimentary realization towards
automatic classification of PPs; the topic, that has not received as much
attention in NLP as NPs and VPs. The approach is a rule-based heuristics
outlined in several levels of our research. There are 7 semantic categories of
PPs considered in this document that we are able to classify from an annotated
corpus."
"Rhetorical structure analysis (RSA) explores discourse relations among
elementary discourse units (EDUs) in a text. It is very useful in many text
processing tasks employing relationships among EDUs such as text understanding,
summarization, and question-answering. Thai language with its distinctive
linguistic characteristics requires a unique technique. This article proposes
an approach for Thai rhetorical structure analysis. First, EDUs are segmented
by two hidden Markov models derived from syntactic rules. A rhetorical
structure tree is constructed from a clustering technique with its similarity
measure derived from Thai semantic rules. Then, a decision tree whose features
derived from the semantic rules is used to determine discourse relations."
"Combined with space-time coding, the orthogonal frequency division
multiplexing (OFDM) system explores space diversity. It is a potential scheme
to offer spectral efficiency and robust high data rate transmissions over
frequency-selective fading channel. However, space-time coding impairs the
system ability to suppress interferences as the signals transmitted from two
transmit antennas are superposed and interfered at the receiver antennas. In
this paper, we developed an adaptive beamforming based on least mean squared
error algorithm and null deepening to combat co-channel interference (CCI) for
the space-time coded OFDM (STC-OFDM) system. To illustrate the performance of
the presented approach, it is compared to the null steering beamformer which
requires a prior knowledge of directions of arrival (DOAs). The structure of
space-time decoders are preserved although there is the use of beamformers
before decoding. By incorporating the proposed beamformer as a CCI canceller in
the STC-OFDM systems, the performance improvement is achieved as shown in the
simulation results."
"This article presents SLAM, an Automatic Solver for Lexical Metaphors like
?d\'eshabiller* une pomme? (to undress* an apple). SLAM calculates a
conventional solution for these productions. To carry on it, SLAM has to
intersect the paradigmatic axis of the metaphorical verb ?d\'eshabiller*?,
where ?peler? (?to peel?) comes closer, with a syntagmatic axis that comes from
a corpus where ?peler une pomme? (to peel an apple) is semantically and
syntactically regular. We test this model on DicoSyn, which is a ?small world?
network of synonyms, to compute the paradigmatic axis and on Frantext.20, a
French corpus, to compute the syntagmatic axis. Further, we evaluate the model
with a sample of an experimental corpus of the database of Flexsem"
"Hidden Markov models (HMMs) have been successfully applied to automatic
speech recognition for more than 35 years in spite of the fact that a key HMM
assumption -- the statistical independence of frames -- is obviously violated
by speech data. In fact, this data/model mismatch has inspired many attempts to
modify or replace HMMs with alternative models that are better able to take
into account the statistical dependence of frames. However it is fair to say
that in 2010 the HMM is the consensus model of choice for speech recognition
and that HMMs are at the heart of both commercially available products and
contemporary research systems. In this paper we present a preliminary
exploration aimed at understanding how speech data depart from HMMs and what
effect this departure has on the accuracy of HMM-based speech recognition. Our
analysis uses standard diagnostic tools from the field of statistics --
hypothesis testing, simulation and resampling -- which are rarely used in the
field of speech recognition. Our main result, obtained by novel manipulations
of real and resampled data, demonstrates that real data have statistical
dependency and that this dependency is responsible for significant numbers of
recognition errors. We also demonstrate, using simulation and resampling, that
if we `remove' the statistical dependency from data, then the resulting
recognition error rates become negligible. Taken together, these results
suggest that a better understanding of the structure of the statistical
dependency in speech data is a crucial first step towards improving HMM-based
speech recognition."
"The article provides lexical statistical analysis of K. Vonnegut's two novels
and their Russian translations. It is found out that there happen some changes
between the speed of word types and word tokens ratio change in the source and
target texts. The author hypothesizes that these changes are typical for
English-Russian translations, and moreover, they represent an example of
Baker's translation feature of levelling out."
"Text documents are complex high dimensional objects. To effectively visualize
such data it is important to reduce its dimensionality and visualize the low
dimensional embedding as a 2-D or 3-D scatter plot. In this paper we explore
dimensionality reduction methods that draw upon domain knowledge in order to
achieve a better low dimensional embedding and visualization of documents. We
consider the use of geometries specified manually by an expert, geometries
derived automatically from corpus statistics, and geometries computed from
linguistic resources."
"Developers express the meaning of the domain ideas in specifically selected
identifiers and comments that form the target implemented code. Software
maintenance requires knowledge and understanding of the encoded ideas. This
paper presents a way how to create automatically domain vocabulary. Knowledge
of domain vocabulary supports the comprehension of a specific domain for later
code maintenance or evolution. We present experiments conducted in two selected
domains: application servers and web frameworks. Knowledge of domain terms
enables easy localization of chunks of code that belong to a certain term. We
consider these chunks of code as ""concepts"" and their placement in the code as
""concept location"". Application developers may also benefit from the obtained
domain terms. These terms are parts of speech that characterize a certain
concept. Concepts are encoded in ""classes"" (OO paradigm) and the obtained
vocabulary of terms supports the selection and the comprehension of the class'
appropriate identifiers. We measured the following software products with our
tool: JBoss, JOnAS, GlassFish, Tapestry, Google Web Toolkit and Echo2."
"Poetry-writing in Sanskrit is riddled with problems for even those who know
the language well. This is so because the rules that govern Sanskrit prosody
are numerous and stringent. We propose a computational algorithm that converts
prose given as E-text into poetry in accordance with the metrical rules of
Sanskrit prosody, simultaneously taking care to ensure that sandhi or euphonic
conjunction, which is compulsory in verse, is handled. The algorithm is
considerably speeded up by a novel method of reducing the target search
database. The algorithm further gives suggestions to the poet in case what
he/she has given as the input prose is impossible to fit into any allowed
metrical format. There is also an interactive component of the algorithm by
which the algorithm interacts with the poet to resolve ambiguities. In
addition, this unique work, which provides a solution to a problem that has
never been addressed before, provides a simple yet effective speech recognition
interface that would help the visually impaired dictate words in E-text, which
is in turn versified by our Poetry Composer Engine."
"The recognition and classification of Named Entities (NER) are regarded as an
important component for many Natural Language Processing (NLP) applications.
The classification is usually made by taking into account the immediate context
in which the NE appears. In some cases, this immediate context does not allow
getting the right classification. We show in this paper that the use of an
extended syntactic context and large-scale resources could be very useful in
the NER task."
"In this chapter, we assume that systematically studying spatial markers
semantics in language provides a means to reveal fundamental properties and
concepts characterizing conceptual representations of space. We propose a
formal system accounting for the properties highlighted by the linguistic
analysis, and we use these tools for representing the semantic content of
several spatial relations of French. The first part presents a semantic
analysis of the expression of space in French aiming at describing the
constraints that formal representations have to take into account. In the
second part, after presenting the structure of our formal system, we set out
its components. A commonsense geometry is sketched out and several functional
and pragmatic spatial concepts are formalized. We take a special attention in
showing that these concepts are well suited to representing the semantic
content of several prepositions of French ('sur' (on), 'dans' (in), 'devant'
(in front of), 'au-dessus' (above)), and in illustrating the inferential
adequacy of these representations."
"While previous linguistic and psycholinguistic research on space has mainly
analyzed spatial relations, the studies reported in this paper focus on how
language distinguishes among spatial entities. Descriptive and experimental
studies first propose a classification of entities, which accounts for both
static and dynamic space, has some cross-linguistic validity, and underlies
adults' cognitive processing. Formal and computational analyses then introduce
theoretical elements aiming at modelling these categories, while fulfilling
various properties of formal ontologies (generality, parsimony, coherence...).
This formal framework accounts, in particular, for functional dependences among
entities underlying some part-whole descriptions. Finally, developmental
research shows that language-specific properties have a clear impact on how
children talk about space. The results suggest some cross-linguistic
variability in children's spatial representations from an early age onwards,
bringing into question models in which general cognitive capacities are the
only determinants of spatial cognition during the course of development."
"Automatically detecting discourse segments is an important preliminary step
towards full discourse parsing. Previous research on discourse segmentation
have relied on the assumption that elementary discourse units (EDUs) in a
document always form a linear sequence (i.e., they can never be nested).
Unfortunately, this assumption turns out to be too strong, for some theories of
discourse like SDRT allows for nested discourse units. In this paper, we
present a simple approach to discourse segmentation that is able to produce
nested EDUs. Our approach builds on standard multi-class classification
techniques combined with a simple repairing heuristic that enforces global
coherence. Our system was developed and evaluated on the first round of
annotations provided by the French Annodis project (an ongoing effort to create
a discourse bank for French). Cross-validated on only 47 documents (1,445
EDUs), our system achieves encouraging performance results with an F-score of
73% for finding EDUs."
"This paper describes in details the first version of Morphonette, a new
French morphological resource and a new radically lexeme-based method of
morphological analysis. This research is grounded in a paradigmatic conception
of derivational morphology where the morphological structure is a structure of
the entire lexicon and not one of the individual words it contains. The
discovery of this structure relies on a measure of morphological similarity
between words, on formal analogy and on the properties of two morphological
paradigms:"
"The Lambek-Grishin calculus LG is the symmetric extension of the
non-associative Lambek calculus NL. In this paper we prove that the
derivability problem for LG is NP-complete."
"In the article, the project of quantitative parametrization of all texts by
Ivan Franko is manifested. It can be made only by using modern computer
techniques after the frequency dictionaries for all Franko's works are
compiled. The paper describes the application spheres, methodology, stages,
principles and peculiarities in the compilation of the frequency dictionary of
the second half of the 19th century - the beginning of the 20th century. The
relation between the Ivan Franko frequency dictionary, explanatory dictionary
of writer's language and text corpus is discussed."
"Lexicon-Grammar tables constitute a large-coverage syntactic lexicon but they
cannot be directly used in Natural Language Processing (NLP) applications
because they sometimes rely on implicit information. In this paper, we
introduce LGExtract, a generic tool for generating a syntactic lexicon for NLP
from the Lexicon-Grammar tables. It is based on a global table that contains
undefined information and on a unique extraction script including all
operations to be performed for all tables. We also present an experiment that
has been conducted to generate a new lexicon of French verbs and predicative
nouns."
"In the article, the methodology and the principles of the compilation of the
Frequency dictionary for Ivan Franko's novel Dlja domashnjoho ohnyshcha (For
the Hearth) are described. The following statistical parameters of the novel
vocabulary are obtained: variety, exclusiveness, concentration indexes,
correlation between word rank and text coverage, etc. The main quantitative
characteristics of Franko's novels Perekhresni stezhky (The Cross-Paths) and
Dlja domashnjoho ohnyshcha are compared on the basis of their frequency
dictionaries."
"The ambition of a character recognition system is to transform a text
document typed on paper into a digital format that can be manipulated by word
processor software Unlike other languages, Arabic has unique features, while
other language doesn't have, from this language these are seven or eight
language such as ordo, jewie and Persian writing, Arabic has twenty eight
letters, each of which can be linked in three different ways or separated
depending on the case. The difficulty of the Arabic handwriting recognition is
that, the accuracy of the character recognition which affects on the accuracy
of the word recognition, in additional there is also two or three from for each
character, the suggested solution by using artificial neural network can solve
the problem and overcome the difficulty of Arabic handwriting recognition."
"Indian languages have long history in World Natural languages. Panini was the
first to define Grammar for Sanskrit language with about 4000 rules in fifth
century. These rules contain uncertainty information. It is not possible to
Computer processing of Sanskrit language with uncertain information. In this
paper, fuzzy logic and fuzzy reasoning are proposed to deal to eliminate
uncertain information for reasoning with Sanskrit grammar. The Sanskrit
language processing is also discussed in this paper."
"This companion paper complements the main DEFT'10 article describing the MARF
approach (arXiv:0905.1235) to the DEFT'10 NLP challenge (described at
http://www.groupes.polymtl.ca/taln2010/deft.php in French). This paper is aimed
to present the complete result sets of all the conducted experiments and their
settings in the resulting tables highlighting the approach and the best
results, but also showing the worse and the worst and their subsequent
analysis. This particular work focuses on application of the MARF's classical
and NLP pipelines to identification tasks within various francophone corpora to
identify decades when certain articles were published for the first track
(Piste 1) and place of origin of a publication (Piste 2), such as the journal
and location (France vs. Quebec). This is the sixth iteration of the release of
the results."
"The Right Frontier Constraint (RFC), as a constraint on the attachment of new
constituents to an existing discourse structure, has important implications for
the interpretation of anaphoric elements in discourse and for Machine Learning
(ML) approaches to learning discourse structures. In this paper we provide
strong empirical support for SDRT's version of RFC. The analysis of about 100
doubly annotated documents by five different naive annotators shows that SDRT's
RFC is respected about 95% of the time. The qualitative analysis of presumed
violations that we have performed shows that they are either click-errors or
structural misconceptions."
"Lexicon-Grammar tables are a very rich syntactic lexicon for the French
language. This linguistic database is nevertheless not directly suitable for
use by computer programs, as it is incomplete and lacks consistency. Tables are
defined on the basis of features which are not explicitly recorded in the
lexicon. These features are only described in literature. Our aim is to define
for each tables these essential properties to make them usable in various
Natural Language Processing (NLP) applications, such as parsing."
"Categorial type logics, pioneered by Lambek, seek a proof-theoretic
understanding of natural language syntax by identifying categories with
formulas and derivations with proofs. We typically observe an intuitionistic
bias: a structural configuration of hypotheses (a constituent) derives a single
conclusion (the category assigned to it). Acting upon suggestions of Grishin to
dualize the logical vocabulary, Moortgat proposed the Lambek-Grishin calculus
(LG) with the aim of restoring symmetry between hypotheses and conclusions. We
develop a theory of labeled modal tableaux for LG, inspired by the
interpretation of its connectives as binary modal operators in the relational
semantics of Kurtonina and Moortgat. As a linguistic application of our method,
we show that grammars based on LG are context-free through use of an
interpolation lemma. This result complements that of Melissen, who proved that
LG augmented by mixed associativity and -commutativity was exceeds LTAG in
expressive power."
"This paper describes a probabilistic top-down parser for minimalist grammars.
Top-down parsers have the great advantage of having a certain predictive power
during the parsing, which takes place in a left-to-right reading of the
sentence. Such parsers have already been well-implemented and studied in the
case of Context-Free Grammars, which are already top-down, but these are
difficult to adapt to Minimalist Grammars, which generate sentences bottom-up.
I propose here a way of rewriting Minimalist Grammars as Linear Context-Free
Rewriting Systems, allowing to easily create a top-down parser. This rewriting
allows also to put a probabilistic field on these grammars, which can be used
to accelerate the parser. Finally, I propose a method of refining the
probabilistic field by using algorithms used in data compression."
"In this paper the problems of deriving a taxonomy from a text and
concept-oriented text segmentation are approached. Formal Concept Analysis
(FCA) method is applied to solve both of these linguistic problems. The
proposed segmentation method offers a conceptual view for text segmentation,
using a context-driven clustering of sentences. The Concept-oriented Clustering
Segmentation algorithm (COCS) is based on k-means linear clustering of the
sentences. Experimental results obtained using COCS algorithm are presented."
"We reformulate minimalist grammars as partial functions on term algebras for
strings and trees. Using filler/role bindings and tensor product
representations, we construct homomorphisms for these data structures into
geometric vector spaces. We prove that the structure-building functions as well
as simple processors for minimalist languages can be realized by piecewise
linear operators in representation space. We also propose harmony, i.e. the
distance of an intermediate processing step from the final well-formed state in
representation space, as a measure of processing complexity. Finally, we
illustrate our findings by means of two particular arithmetic and fractal
representations."
"Arabic morphological analysis is one of the essential stages in Arabic
Natural Language Processing. In this paper we present an approach for Arabic
morphological analysis. This approach is based on Arabic morphological
automaton (AMAUT). The proposed technique uses a morphological database
realized using XMODEL language. Arabic morphology represents a special type of
morphological systems because it is based on the concept of scheme to represent
Arabic words. We use this concept to develop the Arabic morphological automata.
The proposed approach has development standardization aspect. It can be
exploited by NLP applications such as syntactic and semantic analysis,
information retrieval, machine translation and orthographical correction. The
proposed approach is compared with Xerox Arabic Analyzer and Smrz Arabic
Analyzer."
"Grishin proposed enriching the Lambek calculus with multiplicative
disjunction (par) and coresiduals. Applications to linguistics were discussed
by Moortgat, who spoke of the Lambek-Grishin calculus (LG). In this paper, we
adapt Girard's polarity-sensitive double negation embedding for classical logic
to extract a compositional Montagovian semantics from a display calculus for
focused proof search in LG. We seize the opportunity to illustrate our approach
alongside an analysis of extraction, providing linguistic motivation for linear
distributivity of tensor over par, thus answering a question of
Kurtonina&Moortgat. We conclude by comparing our proposal to the continuation
semantics of Bernardi&Moortgat, corresponding to call-by- name and
call-by-value evaluation strategies."
"This article presents a fragment of a new comparative dictionary ""A
comparative dictionary of names of expansive action in Russian and Bulgarian
languages"". Main features of the new web-based comparative dictionary are
placed, the principles of its formation are shown, primary links between the
word-matches are classified. The principal difference between translation
dictionaries and the model of double comparison is also shown. The
classification scheme of the pages is proposed. New concepts and keywords have
been introduced. The real prototype of the dictionary with a few key pages is
published. The broad debate about the possibility of this prototype to become a
version of Russian-Bulgarian comparative dictionary of a new generation is
available."
"To facilitate future research in unsupervised induction of syntactic
structure and to standardize best-practices, we propose a tagset that consists
of twelve universal part-of-speech categories. In addition to the tagset, we
develop a mapping from 25 different treebank tagsets to this universal set. As
a result, when combined with the original treebank data, this universal tagset
and mapping produce a dataset consisting of common parts-of-speech for 22
different languages. We highlight the use of this resource via two experiments,
including one that reports competitive accuracies for unsupervised grammar
induction without gold standard part-of-speech tags."
"Chinese characters can be compared to a molecular structure: a character is
analogous to a molecule, radicals are like atoms, calligraphic strokes
correspond to elementary particles, and when characters form compounds, they
are like molecular structures. In chemistry the conjunction of all of these
structural levels produces what we perceive as matter. In language, the
conjunction of strokes, radicals, characters, and compounds produces meaning.
But when does meaning arise? We all know that radicals are, in some sense, the
basic semantic components of Chinese script, but what about strokes?
Considering the fact that many characters are made by adding individual strokes
to (combinations of) radicals, we can legitimately ask the question whether
strokes carry meaning, or not. In this talk I will present my project of
extending traditional NLP techniques to radicals and strokes, aiming to obtain
a deeper understanding of the way ideographic languages model the world."
"This paper introduces the performance evaluation of statistical approaches
for TextIndependent speaker recognition system using source feature. Linear
prediction LP residual is used as a representation of excitation information in
speech. The speaker-specific information in the excitation of voiced speech is
captured using statistical approaches such as Gaussian Mixture Models GMMs and
Hidden Markov Models HMMs. The decrease in the error during training and
recognizing speakers during testing phase close to 100 percent accuracy
demonstrates that the excitation component of speech contains speaker-specific
information and is indeed being effectively captured by continuous Ergodic HMM
than GMM. The performance of the speaker recognition system is evaluated on GMM
and 2 state ergodic HMM with different mixture components and test speech
duration. We demonstrate the speaker recognition studies on TIMIT database for
both GMM and Ergodic HMM."
"This paper presents an algorithm for identifying noun-phrase antecedents of
pronouns and adjectival anaphors in Spanish dialogues. We believe that anaphora
resolution requires numerous sources of information in order to find the
correct antecedent of the anaphor. These sources can be of different kinds,
e.g., linguistic information, discourse/dialogue structure information, or
topic information. For this reason, our algorithm uses various different kinds
of information (hybrid information). The algorithm is based on linguistic
constraints and preferences and uses an anaphoric accessibility space within
which the algorithm finds the noun phrase. We present some experiments related
to this algorithm and this space using a corpus of 204 dialogues. The algorithm
is implemented in Prolog. According to this study, 95.9% of antecedents were
located in the proposed space, a precision of 81.3% was obtained for pronominal
anaphora resolution, and 81.5% for adjectival anaphora."
"Natural language generation (NLG) systems are computer software systems that
produce texts in English and other human languages, often from non-linguistic
input data. NLG systems, like most AI systems, need substantial amounts of
knowledge. However, our experience in two NLG projects suggests that it is
difficult to acquire correct knowledge for NLG systems; indeed, every knowledge
acquisition (KA) technique we tried had significant problems. In general terms,
these problems were due to the complexity, novelty, and poorly understood
nature of the tasks our systems attempted, and were worsened by the fact that
people write so differently. This meant in particular that corpus-based KA
approaches suffered because it was impossible to assemble a sizable corpus of
high-quality consistent manually written texts in our domains; and structured
expert-oriented KA techniques suffered because experts disagreed and because we
could not get enough information about special and unusual cases to build
robust systems. We believe that such problems are likely to affect many other
NLG systems as well. In the long term, we hope that new KA techniques may
emerge to help NLG system builders. In the shorter term, we believe that
understanding how individual KA techniques can fail, and using a mixture of
different KA techniques with different strengths and weaknesses, can help
developers acquire NLG knowledge that is mostly correct."
"This paper presents an investigation of the entropy of the Telugu script.
Since this script is syllabic, and not alphabetic, the computation of entropy
is somewhat complicated."
"In this paper we consider the problem of efficient computation of
cross-moments of a vector random variable represented by a stochastic
context-free grammar. Two types of cross-moments are discussed. The sample
space for the first one is the set of all derivations of the context-free
grammar, and the sample space for the second one is the set of all derivations
which generate a string belonging to the language of the grammar. In the past,
this problem was widely studied, but mainly for the cross-moments of scalar
variables and up to the second order. This paper presents new algorithms for
computing the cross-moments of an arbitrary order, and the previously developed
ones are derived as special cases."
"This paper introduces, an XML format developed to serialise the object model
defined by the ISO Syntactic Annotation Framework SynAF. Based on widespread
best practices we adapt a popular XML format for syntactic annotation,
TigerXML, with additional features to support a variety of syntactic phenomena
including constituent and dependency structures, binding, and different node
types such as compounds or empty elements. We also define interfaces to other
formats and standards including the Morpho-syntactic Annotation Framework MAF
and the ISOCat Data Category Registry. Finally a case study of the German
Treebank TueBa-D/Z is presented, showcasing the handling of constituent
structures, topological fields and coreference annotation in tandem."
"The usefulness of annotated corpora is greatly increased if there is an
associated tool that can allow various kinds of operations to be performed in a
simple way. Different kinds of annotation frameworks and many query languages
for them have been proposed, including some to deal with multiple layers of
annotation. We present here an easy to learn query language for a particular
kind of annotation framework based on 'threaded trees', which are somewhere
between the complete order of a tree and the anarchy of a graph. Through
'typed' threads, they can allow multiple levels of annotation in the same
document. Our language has a simple, intuitive and concise syntax and high
expressive power. It allows not only to search for complicated patterns with
short queries but also allows data manipulation and specification of arbitrary
return values. Many of the commonly used tasks that otherwise require writing
programs, can be performed with one or more queries. We compare the language
with some others and try to evaluate it."
"We present a system to translate natural language sentences to formulas in a
formal or a knowledge representation language. Our system uses two inverse
lambda-calculus operators and using them can take as input the semantic
representation of some words, phrases and sentences and from that derive the
semantic representation of other words and phrases. Our inverse lambda operator
works on many formal languages including first order logic, database query
languages and answer set programming. Our system uses a syntactic combinatorial
categorial parser to parse natural language sentences and also to construct the
semantic meaning of the sentences as directed by their parsing. The same parser
is used for both. In addition to the inverse lambda-calculus operators, our
system uses a notion of generalization to learn semantic representation of
words from the semantic representation of other words that are of the same
category. Together with this, we use an existing statistical learning approach
to assign weights to deal with multiple meanings of words. Our system produces
improved results on standard corpora on natural language interfaces for robot
command and control and database queries."
"For a system to understand natural language, it needs to be able to take
natural language text and answer questions given in natural language with
respect to that text; it also needs to be able to follow instructions given in
natural language. To achieve this, a system must be able to process natural
language and be able to capture the knowledge within that text. Thus it needs
to be able to translate natural language text into a formal language. We
discuss our approach to do this, where the translation is achieved by composing
the meaning of words in a sentence. Our initial approach uses an inverse lambda
method that we developed (and other methods) to learn meaning of words from
meaning of sentences and an initial lexicon. We then present an improved method
where the initial lexicon is also learned by analyzing the training sentence
and meaning pairs. We evaluate our methods and compare them with other existing
methods on a corpora of database querying and robot command and control."
"This paper investigates the efficiency of the EWC semantic relatedness
measure in an ad-hoc retrieval task. This measure combines the Wikipedia-based
Explicit Semantic Analysis measure, the WordNet path measure and the mixed
collocation index. In the experiments, the open source search engine Terrier
was utilised as a tool to index and retrieve data. The proposed technique was
tested on the NTCIR data collection. The experiments demonstrated promising
results."
"Stabler proposes an implementation of the Chomskyan Minimalist Program,
Chomsky 95 with Minimalist Grammars - MG, Stabler 97. This framework inherits a
long linguistic tradition. But the semantic calculus is more easily added if
one uses the Curry-Howard isomorphism. Minimalist Categorial Grammars - MCG,
based on an extension of the Lambek calculus, the mixed logic, were introduced
to provide a theoretically-motivated syntax-semantics interface, Amblard 07. In
this article, we give full definitions of MG with algebraic tree descriptions
and of MCG, and take the first steps towards giving a proof of inclusion of
their generated languages."
"Formality is one of the most important dimensions of writing style variation.
In this study we conducted an inter-rater reliability experiment for assessing
sentence formality on a five-point Likert scale, and obtained good agreement
results as well as different rating distributions for different sentence
categories. We also performed a difficulty analysis to identify the bottlenecks
of our rating procedure. Our main objective is to design an automatic scoring
mechanism for sentence-level formality, and this study is important for that
purpose."
"This paper presents a method to understand spoken Tunisian dialect based on
lexical semantic. This method takes into account the specificity of the
Tunisian dialect which has no linguistic processing tools. This method is
ontology-based which allows exploiting the ontological concepts for semantic
annotation and ontological relations for speech interpretation. This
combination increases the rate of comprehension and limits the dependence on
linguistic resources. This paper also details the process of building the
ontology used for annotation and interpretation of Tunisian dialect in the
context of speech understanding in dialogue systems for restricted domain."
"We introduce a stochastic graph-based method for computing relative
importance of textual units for Natural Language Processing. We test the
technique on the problem of Text Summarization (TS). Extractive TS relies on
the concept of sentence salience to identify the most important sentences in a
document or set of documents. Salience is typically defined in terms of the
presence of particular important words or in terms of similarity to a centroid
pseudo-sentence. We consider a new approach, LexRank, for computing sentence
importance based on the concept of eigenvector centrality in a graph
representation of sentences. In this model, a connectivity matrix based on
intra-sentence cosine similarity is used as the adjacency matrix of the graph
representation of sentences. Our system, based on LexRank ranked in first place
in more than one task in the recent DUC 2004 evaluation. In this paper we
present a detailed analysis of our approach and apply it to a larger data set
including data from earlier DUC evaluations. We discuss several methods to
compute centrality using the similarity graph. The results show that
degree-based methods (including LexRank) outperform both centroid-based methods
and other systems participating in DUC in most of the cases. Furthermore, the
LexRank with threshold method outperforms the other degree-based techniques
including continuous LexRank. We also show that our approach is quite
insensitive to the noise in the data that may result from an imperfect topical
clustering of documents."
"In this paper we concentrate on the resolution of the lexical ambiguity that
arises when a given word has several different meanings. This specific task is
commonly referred to as word sense disambiguation (WSD). The task of WSD
consists of assigning the correct sense to words using an electronic dictionary
as the source of word definitions. We present two WSD methods based on two main
methodological approaches in this research area: a knowledge-based method and a
corpus-based method. Our hypothesis is that word-sense disambiguation requires
several knowledge sources in order to solve the semantic ambiguity of the
words. These sources can be of different kinds--- for example, syntagmatic,
paradigmatic or statistical information. Our approach combines various sources
of knowledge, through combinations of the two WSD methods mentioned above.
Mainly, the paper concentrates on how to combine these methods and sources of
information in order to achieve good results in the disambiguation. Finally,
this paper presents a comprehensive study and experimental work on evaluation
of the methods and their combinations."
"A fundamental requirement of any task-oriented dialogue system is the ability
to generate object descriptions that refer to objects in the task domain. The
subproblem of content selection for object descriptions in task-oriented
dialogue has been the focus of much previous work and a large number of models
have been proposed. In this paper, we use the annotated COCONUT corpus of
task-oriented design dialogues to develop feature sets based on Dale and
Reiters (1995) incremental model, Brennan and Clarks (1996) conceptual pact
model, and Jordans (2000b) intentional influences model, and use these feature
sets in a machine learning experiment to automatically learn a model of content
selection for object descriptions. Since Dale and Reiters model requires a
representation of discourse structure, the corpus annotations are used to
derive a representation based on Grosz and Sidners (1986) theory of the
intentional structure of discourse, as well as two very simple representations
of discourse structure based purely on recency. We then apply the
rule-induction program RIPPER to train and test the content selection component
of an object description generator on a set of 393 object descriptions from the
corpus. To our knowledge, this is the first reported experiment of a trainable
content selection component for object description generation in dialogue.
Three separate content selection models that are based on the three theoretical
models, all independently achieve accuracies significantly above the majority
class baseline (17%) on unseen test data, with the intentional influences model
(42.4%) performing significantly better than either the incremental model
(30.4%) or the conceptual pact model (28.9%). But the best performing models
combine all the feature sets, achieving accuracies near 60%. Surprisingly, a
simple recency-based representation of discourse structure does as well as one
based on intentional structure. To our knowledge, this is also the first
empirical comparison of a representation of Grosz and Sidners model of
discourse structure with a simpler model for any generation task."
"The relationship between written and spoken words is convoluted in languages
with a deep orthography such as English and therefore it is difficult to devise
explicit rules for generating the pronunciations for unseen words.
Pronunciation by analogy (PbA) is a data-driven method of constructing
pronunciations for novel words from concatenated segments of known words and
their pronunciations. PbA performs relatively well with English and outperforms
several other proposed methods. However, the best published word accuracy of
65.5% (for the 20,000 word NETtalk corpus) suggests there is much room for
improvement in it.
  Previous PbA algorithms have used several different scoring strategies such
as the product of the frequencies of the component pronunciations of the
segments, or the number of different segmentations that yield the same
pronunciation, and different combinations of these methods, to evaluate the
candidate pronunciations. In this article, we instead propose to use a
probabilistically justified scoring rule. We show that this principled approach
alone yields better accuracy (66.21% for the NETtalk corpus) than any
previously published PbA algorithm. Furthermore, combined with certain ad hoc
modifications motivated by earlier algorithms, the performance climbs up to
66.6%, and further improvements are possible by combining this method with
other methods."
"Since 2006 we have undertaken to describe the differences between 17th
century English and contemporary English thanks to NLP software. Studying a
corpus spanning the whole century (tales of English travellers in the Ottoman
Empire in the 17th century, Mary Astell's essay A Serious Proposal to the
Ladies and other literary texts) has enabled us to highlight various lexical,
morphological or grammatical singularities. Thanks to the NooJ linguistic
platform, we created dictionaries indexing the lexical variants and their
transcription in CE. The latter is often the result of the validation of forms
recognized dynamically by morphological graphs. We also built syntactical
graphs aimed at transcribing certain archaic forms in contemporary English. Our
previous research implied a succession of elementary steps alternating textual
analysis and result validation. We managed to provide examples of
transcriptions, but we have not created a global tool for automatic
transcription. Therefore we need to focus on the results we have obtained so
far, study the conditions for creating such a tool, and analyze possible
difficulties. In this paper, we will be discussing the technical and linguistic
aspects we have not yet covered in our previous work. We are using the results
of previous research and proposing a transcription method for words or
sequences identified as archaic."
"A new approach to the problem of natural language understanding is proposed.
The knowledge domain under consideration is the social behavior of people.
English sentences are translated into set of predicates of a semantic database,
which describe persons, occupations, organizations, projects, actions, events,
messages, machines, things, animals, location and time of actions, relations
between objects, thoughts, cause-and-effect relations, abstract objects. There
is a knowledge base containing the description of semantics of objects
(functions and structure), actions (motives and causes), and operations."
"Traditional language processing tools constrain language designers to
specific kinds of grammars. In contrast, model-based language specification
decouples language design from language processing. As a consequence,
model-based language specification tools need general parsers able to parse
unrestricted context-free grammars. As languages specified following this
approach may be ambiguous, parsers must deal with ambiguities. Model-based
language specification also allows the definition of associativity, precedence,
and custom constraints. Therefore parsers generated by model-driven language
specification tools need to enforce constraints. In this paper, we propose
Fence, an efficient bottom-up chart parser with lexical and syntactic ambiguity
support that allows the specification of constraints and, therefore, enables
the use of model-based language specification in practice."
"The goal of the present chapter is to explore the possibility of providing
the research (but also the industrial) community that commonly uses spoken
corpora with a stable portfolio of well-documented standardised formats that
allow a high re-use rate of annotated spoken resources and, as a consequence,
better interoperability across tools used to produce or exploit such resources."
"In anaphora resolution for English, animacy identification can play an
integral role in the application of agreement restrictions between pronouns and
candidates, and as a result, can improve the accuracy of anaphora resolution
systems. In this paper, two methods for animacy identification are proposed and
evaluated using intrinsic and extrinsic measures. The first method is a
rule-based one which uses information about the unique beginners in WordNet to
classify NPs on the basis of their animacy. The second method relies on a
machine learning algorithm which exploits a WordNet enriched with animacy
information for each sense. The effect of word sense disambiguation on the two
methods is also assessed. The intrinsic evaluation reveals that the machine
learning method reaches human levels of performance. The extrinsic evaluation
demonstrates that animacy identification can be beneficial in anaphora
resolution, especially in the cases where animate entities are identified with
high precision."
"This paper presents a novel algorithm to compute sentiment orientation of
Chinese sentiment word. The algorithm uses ideograms which are a distinguishing
feature of Chinese language. The proposed algorithm can be applied to any
sentiment classification scheme. To compute a word's sentiment orientation
using the proposed algorithm, only the word itself and a precomputed character
ontology is required, rather than a corpus. The influence of three parameters
over the algorithm performance is analyzed and verified by experiment.
Experiment also shows that proposed algorithm achieves an F Measure of 85.02%
outperforming existing ideogram based algorithm."
"This works aims to design a statistical machine translation from English text
to American Sign Language (ASL). The system is based on Moses tool with some
modifications and the results are synthesized through a 3D avatar for
interpretation. First, we translate the input text to gloss, a written form of
ASL. Second, we pass the output to the WebSign Plug-in to play the sign.
Contributions of this work are the use of a new couple of language English/ASL
and an improvement of statistical machine translation based on string matching
thanks to Jaro-distance."
"In this paper we describe function tagging using Transformation Based
Learning (TBL) for Myanmar that is a method of extensions to the previous
statistics-based function tagger. Contextual and lexical rules (developed using
TBL) were critical in achieving good results. First, we describe a method for
expressing lexical relations in function tagging that statistical function
tagging are currently unable to express. Function tagging is the preprocessing
step to show grammatical relations of the sentences. Then we use the context
free grammar technique to clarify the grammatical relations in Myanmar
sentences or to output the parse trees. The grammatical relations are the
functional structure of a language. They rely very much on the function tag of
the tokens. We augment the grammatical relations of Myanmar sentences with
transformation-based learning of function tagging."
"Short Message Service (SMS) messages are largely sent directly from one
person to another from their mobile phones. They represent a means of personal
communication that is an important communicative artifact in our current
digital era. As most existing studies have used private access to SMS corpora,
comparative studies using the same raw SMS data has not been possible up to
now. We describe our efforts to collect a public SMS corpus to address this
problem. We use a battery of methodologies to collect the corpus, paying
particular attention to privacy issues to address contributors' concerns. Our
live project collects new SMS message submissions, checks their quality and
adds the valid messages, releasing the resultant corpus as XML and as SQL
dumps, along with corpus statistics, every month. We opportunistically collect
as much metadata about the messages and their sender as possible, so as to
enable different types of analyses. To date, we have collected about 60,000
messages, focusing on English and Mandarin Chinese."
"A step-to-step introduction is provided on how to generate a semantic map
from a collection of messages (full texts, paragraphs or statements) using
freely available software and/or SPSS for the relevant statistics and the
visualization. The techniques are discussed in the various theoretical contexts
of (i) linguistics (e.g., Latent Semantic Analysis), (ii) sociocybernetics and
social systems theory (e.g., the communication of meaning), and (iii)
communication studies (e.g., framing and agenda-setting). We distinguish
between the communication of information in the network space (social network
analysis) and the communication of meaning in the vector space. The vector
space can be considered a generated as an architecture by the network of
relations in the network space; words are then not only related, but also
positioned. These positions are expected rather than observed and therefore one
can communicate meaning. Knowledge can be generated when these meanings can
recursively be communicated and therefore also further codified."
"Grishin's generalization of Lambek's Syntactic Calculus combines a
non-commutative multiplicative conjunction and its residuals (product, left and
right division) with a dual family: multiplicative disjunction, right and left
difference. Interaction between these two families takes the form of linear
distributivity principles. We study proof nets for the Lambek-Grishin calculus
and the correspondence between these nets and unfocused and focused versions of
its sequent calculus."
"A formal theory based on a binary operator of directional associative
relation is constructed in the article and an understanding of an associative
normal form of image constructions is introduced. A model of a commutative
semigroup, which provides a presentation of a sentence as three components of
an interrogative linguistic image construction, is considered."
"We describe a Context Free Grammar (CFG) for Bangla language and hence we
propose a Bangla parser based on the grammar. Our approach is very much general
to apply in Bangla Sentences and the method is well accepted for parsing a
language of a grammar. The proposed parser is a predictive parser and we
construct the parse table for recognizing Bangla grammar. Using the parse table
we recognize syntactical mistakes of Bangla sentences when there is no entry
for a terminal in the parse table. If a natural language can be successfully
parsed then grammar checking from this language becomes possible. The proposed
scheme is based on Top down parsing method and we have avoided the left
recursion of the CFG using the idea of left factoring."
"[This is the translation of paper ""Arborification de Wikip\'edia et analyse
s\'emantique explicite stratifi\'ee"" submitted to TALN 2012.]
  We present an extension of the Explicit Semantic Analysis method by
Gabrilovich and Markovitch. Using their semantic relatedness measure, we weight
the Wikipedia categories graph. Then, we extract a minimal spanning tree, using
Chu-Liu & Edmonds' algorithm. We define a notion of stratified tfidf where the
stratas, for a given Wikipedia page and a given term, are the classical tfidf
and categorical tfidfs of the term in the ancestor categories of the page
(ancestors in the sense of the minimal spanning tree). Our method is based on
this stratified tfidf, which adds extra weight to terms that ""survive"" when
climbing up the category tree. We evaluate our method by a text classification
on the WikiNews corpus: it increases precision by 18%. Finally, we provide
hints for future research"
"I pinpoint an interesting similarity between a recent account to rational
parsing and the treatment of sequential decisions problems in a dynamical
systems approach. I argue that expectation-driven search heuristics aiming at
fast computation resembles a high-risk decision strategy in favor of large
transition velocities. Hale's rational parser, combining generalized
left-corner parsing with informed $\mathrm{A}^*$ search to resolve processing
conflicts, explains gardenpath effects in natural sentence processing by
misleading estimates of future processing costs that are to be minimized. On
the other hand, minimizing the duration of cognitive computations in
time-continuous dynamical systems can be described by combining vector space
representations of cognitive states by means of filler/role decompositions and
subsequent tensor product representations with the paradigm of stable
heteroclinic sequences. Maximizing transition velocities according to a
high-risk decision strategy could account for a fast race even between states
that are apparently remote in representation space."
"This paper describes a context free grammar (CFG) based grammatical relations
for Myanmar sentences which combine corpus-based function tagging system. Part
of the challenge of statistical function tagging for Myanmar sentences comes
from the fact that Myanmar has free-phrase-order and a complex morphological
system. Function tagging is a pre-processing step to show grammatical relations
of Myanmar sentences. In the task of function tagging, which tags the function
of Myanmar sentences with correct segmentation, POS (part-of-speech) tagging
and chunking information, we use Naive Bayesian theory to disambiguate the
possible function tags of a word. We apply context free grammar (CFG) to find
out the grammatical relations of the function tags. We also create a functional
annotated tagged corpus for Myanmar and propose the grammar rules for Myanmar
sentences. Experiments show that our analysis achieves a good result with
simple sentences and complex sentences."
"The ability to mimic human notions of semantic distance has widespread
applications. Some measures rely only on raw text (distributional measures) and
some rely on knowledge sources such as WordNet. Although extensive studies have
been performed to compare WordNet-based measures with human judgment, the use
of distributional measures as proxies to estimate semantic distance has
received little attention. Even though they have traditionally performed poorly
when compared to WordNet-based measures, they lay claim to certain uniquely
attractive features, such as their applicability in resource-poor languages and
their ability to mimic both semantic similarity and semantic relatedness.
Therefore, this paper presents a detailed study of distributional measures.
Particular attention is paid to flesh out the strengths and limitations of both
WordNet-based and distributional measures, and how distributional measures of
distance can be brought more in line with human notions of semantic distance.
We conclude with a brief discussion of recent work on hybrid measures."
"The automatic ranking of word pairs as per their semantic relatedness and
ability to mimic human notions of semantic relatedness has widespread
applications. Measures that rely on raw data (distributional measures) and
those that use knowledge-rich ontologies both exist. Although extensive studies
have been performed to compare ontological measures with human judgment, the
distributional measures have primarily been evaluated by indirect means. This
paper is a detailed study of some of the major distributional measures; it
lists their respective merits and limitations. New measures that overcome these
drawbacks, that are more in line with the human notions of semantic
relatedness, are suggested. The paper concludes with an exhaustive comparison
of the distributional and ontology-based measures. Along the way, significant
research problems are identified. Work on these problems may lead to a better
understanding of how semantic relatedness is to be measured."
"The study of natural language, especially Arabic, and mechanisms for the
implementation of automatic processing is a fascinating field of study, with
various potential applications. The importance of tools for natural language
processing is materialized by the need to have applications that can
effectively treat the vast mass of information available nowadays on electronic
forms. Among these tools, mainly driven by the necessity of a fast writing in
alignment to the actual daily life speed, our interest is on the writing
auditors. The morphological and syntactic properties of Arabic make it a
difficult language to master, and explain the lack in the processing tools for
that language. Among these properties, we can mention: the complex structure of
the Arabic word, the agglutinative nature, lack of vocalization, the
segmentation of the text, the linguistic richness, etc."
"Modern computational linguistic software cannot produce important aspects of
sign language translation. Using some researches we deduce that the majority of
automatic sign language translation systems ignore many aspects when they
generate animation; therefore the interpretation lost the truth information
meaning. Our goals are: to translate written text from any language to ASL
animation; to model maximum raw information using machine learning and
computational techniques; and to produce a more adapted and expressive form to
natural looking and understandable ASL animations. Our methods include
linguistic annotation of initial text and semantic orientation to generate the
facial expression. We use the genetic algorithms coupled to learning/recognized
systems to produce the most natural form. To detect emotion we are based on
fuzzy logic to produce the degree of interpolation between facial expressions.
Roughly, we present a new expressive language Text Adapted Sign Modeling
Language TASML that describes all maximum aspects related to a natural sign
language interpretation. This paper is organized as follow: the next section is
devoted to present the comprehension effect of using Space/Time/SVO form in ASL
animation based on experimentation. In section 3, we describe our technical
considerations. We present the general approach we adopted to develop our tool
in section 4. Finally, we give some perspectives and future works."
"In spite of its robust syntax, semantic cohesion, and less ambiguity, lemma
level analysis and generation does not yet focused in Arabic NLP literatures.
In the current research, we propose the first non-statistical accurate Arabic
lemmatizer algorithm that is suitable for information retrieval (IR) systems.
The proposed lemmatizer makes use of different Arabic language knowledge
resources to generate accurate lemma form and its relevant features that
support IR purposes. As a POS tagger, the experimental results show that, the
proposed algorithm achieves a maximum accuracy of 94.8%. For first seen
documents, an accuracy of 89.15% is achieved, compared to 76.7% of up to date
Stanford accurate Arabic model, for the same, dataset."
"In this paper, a supervised learning technique for extracting keyphrases of
Arabic documents is presented. The extractor is supplied with linguistic
knowledge to enhance its efficiency instead of relying only on statistical
information such as term frequency and distance. During analysis, an annotated
Arabic corpus is used to extract the required lexical features of the document
words. The knowledge also includes syntactic rules based on part of speech tags
and allowed word sequences to extract the candidate keyphrases. In this work,
the abstract form of Arabic words is used instead of its stem form to represent
the candidate terms. The Abstract form hides most of the inflections found in
Arabic words. The paper introduces new features of keyphrases based on
linguistic knowledge, to capture titles and subtitles of a document. A simple
ANOVA test is used to evaluate the validity of selected features. Then, the
learning model is built using the LDA - Linear Discriminant Analysis - and
training documents. Although, the presented system is trained using documents
in the IT domain, experiments carried out show that it has a significantly
better performance than the existing Arabic extractor systems, where precision
and recall values reach double their corresponding values in the other systems
especially for lengthy and non-scientific articles."
"This paper gives a detail overview about the modified features selection in
CRF (Conditional Random Field) based Manipuri POS (Part of Speech) tagging.
Selection of features is so important in CRF that the better are the features
then the better are the outputs. This work is an attempt or an experiment to
make the previous work more efficient. Multiple new features are tried to run
the CRF and again tried with the Reduplicated Multiword Expression (RMWE) as
another feature. The CRF run with RMWE because Manipuri is rich of RMWE and
identification of RMWE becomes one of the necessities to bring up the result of
POS tagging. The new CRF system shows a Recall of 78.22%, Precision of 73.15%
and F-measure of 75.60%. With the identification of RMWE and considering it as
a feature makes an improvement to a Recall of 80.20%, Precision of 74.31% and
F-measure of 77.14%."
"We present CAVaT, a tool that performs Corpus Analysis and Validation for
TimeML. CAVaT is an open source, modular checking utility for statistical
analysis of features specific to temporally-annotated natural language corpora.
It provides reporting, highlights salient links between a variety of general
and time-specific linguistic features, and also validates a temporal annotation
to ensure that it is logically consistent and sufficiently annotated. Uniquely,
CAVaT provides analysis specific to TimeML-annotated temporal information.
TimeML is a standard for annotating temporal information in natural language
text. In this paper, we present the reporting part of CAVaT, and then its
error-checking ability, including the workings of several novel TimeML document
verification methods. This is followed by the execution of some example tasks
using the tool to show relations between times, events, signals and links. We
also demonstrate inconsistencies in a TimeML corpus (TimeBank) that have been
detected with CAVaT."
"Temporal information conveyed by language describes how the world around us
changes through time. Events, durations and times are all temporal elements
that can be viewed as intervals. These intervals are sometimes temporally
related in text. Automatically determining the nature of such relations is a
complex and unsolved problem. Some words can act as ""signals"" which suggest a
temporal ordering between intervals. In this paper, we use these signal words
to improve the accuracy of a recent approach to classification of temporal
links."
"We describe the University of Sheffield system used in the TempEval-2
challenge, USFD2. The challenge requires the automatic identification of
temporal entities and relations in text. USFD2 identifies and anchors temporal
expressions, and also attempts two of the four temporal relation assignment
tasks. A rule-based system picks out and anchors temporal expressions, and a
maximum entropy classifier assigns temporal link labels, based on features that
include descriptions of associated temporal signal words. USFD2 identified
temporal expressions successfully, and correctly classified their type in 90%
of cases. Determining the relation between an event and time expression in the
same sentence was performed at 63% accuracy, the second highest score in this
part of the challenge."
"In this paper we present RTMML, a markup language for the tenses of verbs and
temporal relations between verbs. There is a richness to tense in language that
is not fully captured by existing temporal annotation schemata. Following
Reichenbach we present an analysis of tense in terms of abstract time points,
with the aim of supporting automated processing of tense and temporal relations
in language. This allows for precise reasoning about tense in documents, and
the deduction of temporal relations between the times and verbal events in a
discourse. We define the syntax of RTMML, and demonstrate the markup in a range
of situations."
"Automatic temporal ordering of events described in discourse has been of
great interest in recent years. Event orderings are conveyed in text via va
rious linguistic mechanisms including the use of expressions such as ""before"",
""after"" or ""during"" that explicitly assert a temporal relation -- temporal
signals. In this paper, we investigate the role of temporal signals in temporal
relation extraction and provide a quantitative analysis of these expres sions
in the TimeBank annotated corpus."
"This paper describes the University of Sheffield's entry in the 2011 TAC KBP
entity linking and slot filling tasks. We chose to participate in the
monolingual entity linking task, the monolingual slot filling task and the
temporal slot filling tasks. We set out to build a framework for
experimentation with knowledge base population. This framework was created, and
applied to multiple KBP tasks. We demonstrated that our proposed framework is
effective and suitable for collaborative development efforts, as well as useful
in a teaching environment. Finally we present results that, while very modest,
provide improvements an order of magnitude greater than our 2010 attempt."
"Automatic annotation of temporal expressions is a research challenge of great
interest in the field of information extraction. Gold standard
temporally-annotated resources are limited in size, which makes research using
them difficult. Standards have also evolved over the past decade, so not all
temporally annotated data is in the same format. We vastly increase available
human-annotated temporal expression resources by converting older format
resources to TimeML/TIMEX3. This task is difficult due to differing annotation
methods. We present a robust conversion tool and a new, large temporal
expression resource. Using this, we evaluate our conversion process by using it
as training data for an existing TimeML annotation tool, achieving a 0.87 F1
measure -- better than any system in the TempEval-2 timex recognition exercise."
"ASR short for Automatic Speech Recognition is the process of converting a
spoken speech into text that can be manipulated by a computer. Although ASR has
several applications, it is still erroneous and imprecise especially if used in
a harsh surrounding wherein the input speech is of low quality. This paper
proposes a post-editing ASR error correction method and algorithm based on
Bing's online spelling suggestion. In this approach, the ASR recognized output
text is spell-checked using Bing's spelling suggestion technology to detect and
correct misrecognized words. More specifically, the proposed algorithm breaks
down the ASR output text into several word-tokens that are submitted as search
queries to Bing search engine. A returned spelling suggestion implies that a
query is misspelled; and thus it is replaced by the suggested correction;
otherwise, no correction is performed and the algorithm continues with the next
token until all tokens get validated. Experiments carried out on various
speeches in different languages indicated a successful decrease in the number
of ASR errors and an improvement in the overall error correction rate. Future
research can improve upon the proposed algorithm so much so that it can be
parallelized to take advantage of multiprocessor computers."
"At the present time, computers are employed to solve complex tasks and
problems ranging from simple calculations to intensive digital image processing
and intricate algorithmic optimization problems to computationally-demanding
weather forecasting problems. ASR short for Automatic Speech Recognition is yet
another type of computational problem whose purpose is to recognize human
spoken speech and convert it into text that can be processed by a computer.
Despite that ASR has many versatile and pervasive real-world applications,it is
still relatively erroneous and not perfectly solved as it is prone to produce
spelling errors in the recognized text, especially if the ASR system is
operating in a noisy environment, its vocabulary size is limited, and its input
speech is of bad or low quality. This paper proposes a post-editing ASR error
correction method based on MicrosoftN-Gram dataset for detecting and correcting
spelling errors generated by ASR systems. The proposed method comprises an
error detection algorithm for detecting word errors; a candidate corrections
generation algorithm for generating correction suggestions for the detected
word errors; and a context-sensitive error correction algorithm for selecting
the best candidate for correction. The virtue of using the Microsoft N-Gram
dataset is that it contains real-world data and word sequences extracted from
the web which canmimica comprehensive dictionary of words having a large and
all-inclusive vocabulary. Experiments conducted on numerous speeches, performed
by different speakers, showed a remarkable reduction in ASR errors. Future
research can improve upon the proposed algorithm so much so that it can be
parallelized to take advantage of multiprocessor and distributed systems."
"Tree transducers are formal automata that transform trees into other trees.
Many varieties of tree transducers have been explored in the automata theory
literature, and more recently, in the machine translation literature. In this
paper I review T and xT transducers, situate them among related formalisms, and
show how they can be used to implement rules for machine translation systems
that cover all of the cross-language structural divergences described in Bonnie
Dorr's influential article on the topic. I also present an implementation of xT
transduction, suitable and convenient for experimenting with translation rules."
"WordNet proved that it is possible to construct a large-scale electronic
lexical database on the principles of lexical semantics. It has been accepted
and used extensively by computational linguists ever since it was released.
Inspired by WordNet's success, we propose as an alternative a similar resource,
based on the 1987 Penguin edition of Roget's Thesaurus of English Words and
Phrases.
  Peter Mark Roget published his first Thesaurus over 150 years ago. Countless
writers, orators and students of the English language have used it.
Computational linguists have employed Roget's for almost 50 years in Natural
Language Processing, however hesitated in accepting Roget's Thesaurus because a
proper machine tractable version was not available.
  This dissertation presents an implementation of a machine-tractable version
of the 1987 Penguin edition of Roget's Thesaurus - the first implementation of
its kind to use an entire current edition. It explains the steps necessary for
taking a machine-readable file and transforming it into a tractable system.
This involves converting the lexical material into a format that can be more
easily exploited, identifying data structures and designing classes to
computerize the Thesaurus. Roget's organization is studied in detail and
contrasted with WordNet's.
  We show two applications of the computerized Thesaurus: computing semantic
similarity between words and phrases, and building lexical chains in a text.
The experiments are performed using well-known benchmarks and the results are
compared to those of other systems that use Roget's, WordNet and statistical
techniques. Roget's has turned out to be an excellent resource for measuring
semantic similarity; lexical chains are easily built but more difficult to
evaluate. We also explain ways in which Roget's Thesaurus and WordNet can be
combined."
"Spell-checking is the process of detecting and sometimes providing
suggestions for incorrectly spelled words in a text. Basically, the larger the
dictionary of a spell-checker is, the higher is the error detection rate;
otherwise, misspellings would pass undetected. Unfortunately, traditional
dictionaries suffer from out-of-vocabulary and data sparseness problems as they
do not encompass large vocabulary of words indispensable to cover proper names,
domain-specific terms, technical jargons, special acronyms, and terminologies.
As a result, spell-checkers will incur low error detection and correction rate
and will fail to flag all errors in the text. This paper proposes a new
parallel shared-memory spell-checking algorithm that uses rich real-world word
statistics from Yahoo! N-Grams Dataset to correct non-word and real-word errors
in computer text. Essentially, the proposed algorithm can be divided into three
sub-algorithms that run in a parallel fashion: The error detection algorithm
that detects misspellings, the candidates generation algorithm that generates
correction suggestions, and the error correction algorithm that performs
contextual error correction. Experiments conducted on a set of text articles
containing misspellings, showed a remarkable spelling error correction rate
that resulted in a radical reduction of both non-word and real-word errors in
electronic text. In a further study, the proposed algorithm is to be optimized
for message-passing systems so as to become more flexible and less costly to
scale over distributed machines."
"With the advent of digital optical scanners, a lot of paper-based books,
textbooks, magazines, articles, and documents are being transformed into an
electronic version that can be manipulated by a computer. For this purpose,
OCR, short for Optical Character Recognition was developed to translate scanned
graphical text into editable computer text. Unfortunately, OCR is still
imperfect as it occasionally mis-recognizes letters and falsely identifies
scanned text, leading to misspellings and linguistics errors in the OCR output
text. This paper proposes a post-processing context-based error correction
algorithm for detecting and correcting OCR non-word and real-word errors. The
proposed algorithm is based on Google's online spelling suggestion which
harnesses an internal database containing a huge collection of terms and word
sequences gathered from all over the web, convenient to suggest possible
replacements for words that have been misspelled during the OCR process.
Experiments carried out revealed a significant improvement in OCR error
correction rate. Future research can improve upon the proposed algorithm so
much so that it can be parallelized and executed over multiprocessing
platforms."
"We have implemented a system that measures semantic similarity using a
computerized 1987 Roget's Thesaurus, and evaluated it by performing a few
typical tests. We compare the results of these tests with those produced by
WordNet-based similarity measures. One of the benchmarks is Miller and Charles'
list of 30 noun pairs to which human judges had assigned similarity measures.
We correlate these measures with those computed by several NLP systems. The 30
pairs can be traced back to Rubenstein and Goodenough's 65 pairs, which we have
also studied. Our Roget's-based system gets correlations of .878 for the
smaller and .818 for the larger list of noun pairs; this is quite close to the
.885 that Resnik obtained when he employed humans to replicate the Miller and
Charles experiment. We further evaluate our measure by using Roget's and
WordNet to answer 80 TOEFL, 50 ESL and 300 Reader's Digest questions: the
correct synonym must be selected amongst a group of four words. Our system gets
78.75%, 82.00% and 74.33% of the questions respectively."
"Morris and Hirst present a method of linking significant words that are about
the same topic. The resulting lexical chains are a means of identifying
cohesive regions in a text, with applications in many natural language
processing tasks, including text summarization. The first lexical chains were
constructed manually using Roget's International Thesaurus. Morris and Hirst
wrote that automation would be straightforward given an electronic thesaurus.
All applications so far have used WordNet to produce lexical chains, perhaps
because adequate electronic versions of Roget's were not available until
recently. We discuss the building of lexical chains using an electronic version
of Roget's Thesaurus. We implement a variant of the original algorithm, and
explain the necessary design decisions. We include a comparison with other
implementations."
"This paper presents the steps involved in creating an electronic lexical
knowledge base from the 1987 Penguin edition of Roget's Thesaurus. Semantic
relations are labelled with the help of WordNet. The two resources are compared
in a qualitative and quantitative manner. Differences in the organization of
the lexical material are discussed, as well as the possibility of merging both
resources."
"We propose a new segmentation evaluation metric, called segmentation
similarity (S), that quantifies the similarity between two segmentations as the
proportion of boundaries that are not transformed when comparing them using
edit distance, essentially using edit distance as a penalty function and
scaling penalties by segmentation size. We propose several adapted
inter-annotator agreement coefficients which use S that are suitable for
segmentation. We show that S is configurable enough to suit a wide variety of
segmentation evaluations, and is an improvement upon the state of the art. We
also propose using inter-annotator agreement coefficients to evaluate automatic
segmenters in terms of human performance."
"Jules Bloch's work on formation of the Marathi language has to be expanded
further to provide for a study of evolution and formation of Indian languages
in the Indian language union (sprachbund). The paper analyses the stages in the
evolution of early writing systems which began with the evolution of counting
in the ancient Near East. A stage anterior to the stage of syllabic
representation of sounds of a language, is identified. Unique geometric shapes
required for tokens to categorize objects became too large to handle to
abstract hundreds of categories of goods and metallurgical processes during the
production of bronze-age goods. About 3500 BCE, Indus script as a writing
system was developed to use hieroglyphs to represent the 'spoken words'
identifying each of the goods and processes. A rebus method of representing
similar sounding words of the lingua franca of the artisans was used in Indus
script. This method is recognized and consistently applied for the lingua
franca of the Indian sprachbund. That the ancient languages of India,
constituted a sprachbund (or language union) is now recognized by many
linguists. The sprachbund area is proximate to the area where most of the Indus
script inscriptions were discovered, as documented in the corpora. That
hundreds of Indian hieroglyphs continued to be used in metallurgy is evidenced
by their use on early punch-marked coins. This explains the combined use of
syllabic scripts such as Brahmi and Kharoshti together with the hieroglyphs on
Rampurva copper bolt, and Sohgaura copper plate from about 6th century
BCE.Indian hieroglyphs constitute a writing system for meluhha language and are
rebus representations of archaeo-metallurgy lexemes. The rebus principle was
employed by the early scripts and can legitimately be used to decipher the
Indus script, after secure pictorial identification."
"In computing, spell checking is the process of detecting and sometimes
providing spelling suggestions for incorrectly spelled words in a text.
Basically, a spell checker is a computer program that uses a dictionary of
words to perform spell checking. The bigger the dictionary is, the higher is
the error detection rate. The fact that spell checkers are based on regular
dictionaries, they suffer from data sparseness problem as they cannot capture
large vocabulary of words including proper names, domain-specific terms,
technical jargons, special acronyms, and terminologies. As a result, they
exhibit low error detection rate and often fail to catch major errors in the
text. This paper proposes a new context-sensitive spelling correction method
for detecting and correcting non-word and real-word errors in digital text
documents. The approach hinges around data statistics from Google Web 1T 5-gram
data set which consists of a big volume of n-gram word sequences, extracted
from the World Wide Web. Fundamentally, the proposed method comprises an error
detector that detects misspellings, a candidate spellings generator based on a
character 2-gram model that generates correction suggestions, and an error
corrector that performs contextual error correction. Experiments conducted on a
set of text documents from different domains and containing misspellings,
showed an outstanding spelling error correction rate and a drastic reduction of
both non-word and real-word errors. In a further study, the proposed algorithm
is to be parallelized so as to lower the computational cost of the error
detection and correction processes."
"The aim of this paper is to evaluate a Text to Knowledge Mapping (TKM)
Prototype. The prototype is domain-specific, the purpose of which is to map
instructional text onto a knowledge domain. The context of the knowledge domain
is DC electrical circuit. During development, the prototype has been tested
with a limited data set from the domain. The prototype reached a stage where it
needs to be evaluated with a representative linguistic data set called corpus.
A corpus is a collection of text drawn from typical sources which can be used
as a test data set to evaluate NLP systems. As there is no available corpus for
the domain, we developed and annotated a representative corpus. The evaluation
of the prototype considers two of its major components- lexical components and
knowledge model. Evaluation on lexical components enriches the lexical
resources of the prototype like vocabulary and grammar structures. This leads
the prototype to parse a reasonable amount of sentences in the corpus. While
dealing with the lexicon was straight forward, the identification and
extraction of appropriate semantic relations was much more involved. It was
necessary, therefore, to manually develop a conceptual structure for the domain
to formulate a domain-specific framework of semantic relations. The framework
of semantic relationsthat has resulted from this study consisted of 55
relations, out of which 42 have inverse relations. We also conducted rhetorical
analysis on the corpus to prove its representativeness in conveying semantic.
Finally, we conducted a topical and discourse analysis on the corpus to analyze
the coverage of discourse by the prototype."
"This project explores the nature of language acquisition in computers, guided
by techniques similar to those used in children. While existing natural
language processing methods are limited in scope and understanding, our system
aims to gain an understanding of language from first principles and hence
minimal initial input. The first portion of our system was implemented in Java
and is focused on understanding the morphology of language using bigrams. We
use frequency distributions and differences between them to define and
distinguish languages. English and French texts were analyzed to determine a
difference threshold of 55 before the texts are considered to be in different
languages, and this threshold was verified using Spanish texts. The second
portion of our system focuses on gaining an understanding of the syntax of a
language using a recursive method. The program uses one of two possible methods
to analyze given sentences based on either sentence patterns or surrounding
words. Both methods have been implemented in C++. The program is able to
understand the structure of simple sentences and learn new words. In addition,
we have provided some suggestions regarding future work and potential
extensions of the existing program."
"Universal Networking Language (UNL) is a declarative formal language that is
used to represent semantic data extracted from natural language texts. This
paper presents a novel approach to converting Bangla natural language text into
UNL using a method known as Predicate Preserving Parser (PPP) technique. PPP
performs morphological, syntactic and semantic, and lexical analysis of text
synchronously. This analysis produces a semantic-net like structure represented
using UNL. We demonstrate how Bangla texts are analyzed following the PPP
technique to produce UNL documents which can then be translated into any other
suitable natural language facilitating the opportunity to develop a universal
language translation method via UNL."
"Understanding the ways in which participants in public discussions frame
their arguments is important in understanding how public opinion is formed. In
this paper, we adopt the position that it is time for more
computationally-oriented research on problems involving framing. In the
interests of furthering that goal, we propose the following specific,
interesting and, we believe, relatively accessible question: In the controversy
regarding the use of genetically-modified organisms (GMOs) in agriculture, do
pro- and anti-GMO articles differ in whether they choose to adopt a
""scientific"" tone?
  Prior work on the rhetoric and sociology of science suggests that hedging may
distinguish popular-science text from text written by professional scientists
for their colleagues. We propose a detailed approach to studying whether hedge
detection can be used to understanding scientific framing in the GMO debates,
and provide corpora to facilitate this study. Some of our preliminary analyses
suggest that hedges occur less frequently in scientific discourse than in
popular text, a finding that contradicts prior assertions in the literature. We
hope that our initial work and data will encourage others to pursue this
promising line of inquiry."
"In this memory we made the design of an indexing model for Arabic language
and adapting standards for describing learning resources used (the LOM and
their application profiles) with learning conditions such as levels education
of students, their levels of understanding...the pedagogical context with
taking into account the repre-sentative elements of the text, text's
length,...in particular, we highlight the specificity of the Arabic language
which is a complex language, characterized by its flexion, its voyellation and
its agglutination."
"BADREX uses dynamically generated regular expressions to annotate term
definition-term abbreviation pairs, and corefers unpaired acronyms and
abbreviations back to their initial definition in the text. Against the
Medstract corpus BADREX achieves precision and recall of 98% and 97%, and
against a much larger corpus, 90% and 85%, respectively. BADREX yields improved
performance over previous approaches, requires no training data and allows
runtime customisation of its input parameters. BADREX is freely available from
https://github.com/philgooch/BADREX-Biomedical-Abbreviation-Expander as a
plugin for the General Architecture for Text Engineering (GATE) framework and
is licensed under the GPLv3."
"We describe the TempEval-3 task which is currently in preparation for the
SemEval-2013 evaluation exercise. The aim of TempEval is to advance research on
temporal information processing. TempEval-3 follows on from previous TempEval
events, incorporating: a three-part task structure covering event, temporal
expression and temporal relation extraction; a larger dataset; and single
overall task quality scores."
"In this article we focus firstly on the principle of pedagogical indexing and
characteristics of Arabic language and secondly on the possibility of adapting
the standard for describing learning resources used (the LOM and its
Application Profiles) with learning conditions such as the educational levels
of students and their levels of understanding,... the educational context with
taking into account the representative elements of text, text length, ... in
particular, we put in relief the specificity of the Arabic language which is a
complex language, characterized by its flexion, its voyellation and
agglutination."
"The sense analysis is still critical problem in machine translation system,
especially such as English-Korean translation which the syntactical different
between source and target languages is very great. We suggest a method for
selecting the noun sense using contextual feature in English-Korean
Translation."
"With such increasing popularity and availability of digital text data,
authorships of digital texts can not be taken for granted due to the ease of
copying and parsing. This paper presents a new text style analysis called
natural frequency zoned word distribution analysis (NFZ-WDA), and then a basic
authorship attribution scheme and an open authorship attribution scheme for
digital texts based on the analysis. NFZ-WDA is based on the observation that
all authors leave distinct intrinsic word usage traces on texts written by them
and these intrinsic styles can be identified and employed to analyze the
authorship. The intrinsic word usage styles can be estimated through the
analysis of word distribution within a text, which is more than normal word
frequency analysis and can be expressed as: which groups of words are used in
the text; how frequently does each group of words occur; how are the
occurrences of each group of words distributed in the text. Next, the basic
authorship attribution scheme and the open authorship attribution scheme
provide solutions for both closed and open authorship attribution problems.
Through analysis and extensive experimental studies, this paper demonstrates
the efficiency of the proposed method for authorship attribution."
"A recent advance in computer technology has permitted scientists to implement
and test algorithms that were known from quite some time (or not) but which
were computationally expensive. Two such projects are IBM's Jeopardy as a part
of its DeepQA project [1] and Wolfram's Wolframalpha[2]. Both these methods
implement natural language processing (another goal of AI scientists) and try
to answer questions as asked by the user. Though the goal of the two projects
is similar, both of them have a different procedure at it's core. In the
following sections, the mechanism and history of IBM's Jeopardy and Wolfram
alpha has been explained followed by the implications of these projects in
realizing Ray Kurzweil's [3] dream of passing the Turing test by 2029. A recipe
of taking the above projects to a new level is also explained."
"In this paper, we present a new approach dedicated to correcting the spelling
errors of the Arabic language. This approach corrects typographical errors like
inserting, deleting, and permutation. Our method is inspired from the
Levenshtein algorithm, and allows a finer and better scheduling than
Levenshtein. The results obtained are very satisfactory and encouraging, which
shows the interest of our new approach."
"Dynamics of average length of words in Russian and English is analysed in the
article. Words belonging to the diachronic text corpus Google Books Ngram and
dated back to the last two centuries are studied. It was found out that average
word length slightly increased in the 19th century, and then it was growing
rapidly most of the 20th century and started decreasing over the period from
the end of the 20th - to the beginning of the 21th century. Words which
contributed mostly to increase or decrease of word average length were
identified. At that, content words and functional words are analysed
separately. Long content words contribute mostly to word average length of
word. As it was shown, these words reflect the main tendencies of social
development and thus, are used frequently. Change of frequency of personal
pronouns also contributes significantly to change of average word length. The
other parameters connected with average length of word were also analysed."
"In principle, the design of transition-based dependency parsers makes it
possible to experiment with any general-purpose classifier without other
changes to the parsing algorithm. In practice, however, it often takes
substantial software engineering to bridge between the different
representations used by two software packages. Here we present extensions to
MaltParser that allow the drop-in use of any classifier conforming to the
interface of the Weka machine learning package, a wrapper for the TiMBL
memory-based learner to this interface, and experiments on multilingual
dependency parsing with a variety of classifiers. While earlier work had
suggested that memory-based learners might be a good choice for low-resource
parsing scenarios, we cannot support that hypothesis in this work. We observed
that support-vector machines give better parsing performance than the
memory-based learner, regardless of the size of the training set."
"Analyzing writing styles of non-native speakers is a challenging task. In
this paper, we analyze the comments written in the discussion pages of the
English Wikipedia. Using learning algorithms, we are able to detect native
speakers' writing style with an accuracy of 74%. Given the diversity of the
English Wikipedia users and the large number of languages they speak, we
measure the similarities among their native languages by comparing the
influence they have on their English writing style. Our results show that
languages known to have the same origin and development path have similar
footprint on their speakers' English writing style. To enable further studies,
the dataset we extracted from Wikipedia will be made available publicly."
"Controlled natural languages (CNL) with a direct mapping to formal logic have
been proposed to improve the usability of knowledge representation systems,
query interfaces, and formal specifications. Predictive editors are a popular
approach to solve the problem that CNLs are easy to read but hard to write.
Such predictive editors need to be able to ""look ahead"" in order to show all
possible continuations of a given unfinished sentence. Such lookahead features,
however, are difficult to implement in a satisfying way with existing grammar
frameworks, especially if the CNL supports complex nonlocal structures such as
anaphoric references. Here, methods and algorithms are presented for a new
grammar notation called Codeco, which is specifically designed for controlled
natural languages and predictive editors. A parsing approach for Codeco based
on an extended chart parsing algorithm is presented. A large subset of Attempto
Controlled English (ACE) has been represented in Codeco. Evaluation of this
grammar and the parser implementation shows that the approach is practical,
adequate and efficient."
"Web users produce more and more documents expressing opinions. Because these
have become important resources for customers and manufacturers, many have
focused on them. Opinions are often expressed through adjectives with positive
or negative semantic values. In extracting information from users' opinion in
online reviews, exact recognition of the semantic polarity of adjectives is one
of the most important requirements. Since adjectives have different semantic
orientations according to contexts, it is not satisfying to extract opinion
information without considering the semantic and lexical relations between the
adjectives and the feature nouns appropriate to a given domain. In this paper,
we present a classification of adjectives by polarity, and we analyze
adjectives that are undetermined in the absence of contexts. Our research
should be useful for accurately predicting semantic orientations of opinion
sentences, and should be taken into account before relying on an automatic
methods."
"We design a new co-occurrence based word association measure by incorporating
the concept of significant cooccurrence in the popular word association measure
Pointwise Mutual Information (PMI). By extensive experiments with a large
number of publicly available datasets we show that the newly introduced measure
performs better than other co-occurrence based measures and despite being
resource-light, compares well with the best known resource-heavy distributional
similarity and knowledge based word association measures. We investigate the
source of this performance improvement and find that of the two types of
significant co-occurrence - corpus-level and document-level, the concept of
corpus level significance combined with the use of document counts in place of
word counts is responsible for all the performance gains observed. The concept
of document level significance is not helpful for PMI adaptation."
"Inferring evaluation scores based on human judgments is invaluable compared
to using current evaluation metrics which are not suitable for real-time
applications e.g. post-editing. However, these judgments are much more
expensive to collect especially from expert translators, compared to evaluation
based on indicators contrasting source and translation texts. This work
introduces a novel approach for quality estimation by combining learnt
confidence scores from a probabilistic inference model based on human
judgments, with selective linguistic features-based scores, where the proposed
inference model infers the credibility of given human ranks to solve the
scarcity and inconsistency issues of human judgments. Experimental results,
using challenging language-pairs, demonstrate improvement in correlation with
human judgments over traditional evaluation metrics."
"Machine Translation for Indian languages is an emerging research area.
Transliteration is one such module that we design while designing a translation
system. Transliteration means mapping of source language text into the target
language. Simple mapping decreases the efficiency of overall translation
system. We propose the use of stemming and part-of-speech tagging for
transliteration. The effectiveness of translation can be improved if we use
part-of-speech tagging and stemming assisted transliteration.We have shown that
much of the content in Gujarati gets transliterated while being processed for
translation to Hindi language."
"In this paper we present a Marathi part of speech tagger. It is a
morphologically rich language. It is spoken by the native people of
Maharashtra. The general approach used for development of tagger is statistical
using trigram Method. The main concept of trigram is to explore the most likely
POS for a token based on given information of previous two tags by calculating
probabilities to determine which is the best sequence of a tag. In this paper
we show the development of the tagger. Moreover we have also shown the
evaluation done."
"Machine Transliteration has come out to be an emerging and a very important
research area in the field of machine translation. Transliteration basically
aims to preserve the phonological structure of words. Proper transliteration of
name entities plays a very significant role in improving the quality of machine
translation. In this paper we are doing machine transliteration for
English-Punjabi language pair using rule based approach. We have constructed
some rules for syllabification. Syllabification is the process to extract or
separate the syllable from the words. In this we are calculating the
probabilities for name entities (Proper names and location). For those words
which do not come under the category of name entities, separate probabilities
are being calculated by using relative frequency through a statistical machine
translation toolkit known as MOSES. Using these probabilities we are
transliterating our input text from English to Punjabi."
"Natural language processing area is still under research. But now a day it is
on platform for worldwide researchers. Natural language processing includes
analyzing the language based on its structure and then tagging of each word
appropriately with its grammar base. Here we have 50,000 tagged words set and
we try to cluster those Gujarati words based on proposed algorithm, we have
defined our own algorithm for processing. Many clustering techniques are
available Ex. Single linkage, complete, linkage,average linkage, Hear no of
clusters to be formed are not known, so it is all depends on the type of data
set provided . Clustering is preprocess for stemming . Stemming is the process
where root is extracted from its word. Ex. cats= cat+S, meaning. Cat: Noun and
plural form."
"For the past 60 years, Research in machine translation is going on. For the
development in this field, a lot of new techniques are being developed each
day. As a result, we have witnessed development of many automatic machine
translators. A manager of machine translation development project needs to know
the performance increase/decrease, after changes have been done in his system.
Due to this reason, a need for evaluation of machine translation systems was
felt. In this article, we shall present the evaluation of some machine
translators. This evaluation will be done by a human evaluator and by some
automatic evaluation metrics, which will be done at sentence, document and
system level. In the end we shall also discuss the comparison between the
evaluations."
"We develop a probabilistic latent-variable model to discover semantic
frames---types of events and their participants---from corpora. We present a
Dirichlet-multinomial model in which frames are latent categories that explain
the linking of verb-subject-object triples, given document-level sparsity. We
analyze what the model learns, and compare it to FrameNet, noting it learns
some novel and interesting frames. This document also contains a discussion of
inference issues, including concentration parameter learning; and a small-scale
error analysis of syntactic parsing accuracy."
"The integration of lexical semantics and pragmatics in the analysis of the
meaning of natural lan- guage has prompted changes to the global framework
derived from Montague. In those works, the original lexicon, in which words
were assigned an atomic type of a single-sorted logic, has been re- placed by a
set of many-facetted lexical items that can compose their meaning with salient
contextual properties using a rich typing system as a guide. Having related our
proposal for such an expanded framework \LambdaTYn, we present some recent
advances in the logical formalisms associated, including constraints on lexical
transformations and polymorphic quantifiers, and ongoing discussions and
research on the granularity of the type system and the limits of transitivity."
"We present an open-domain Question-Answering system that learns to answer
questions based on successful past interactions. We follow a pattern-based
approach to Answer-Extraction, where (lexico-syntactic) patterns that relate a
question to its answer are automatically learned and used to answer future
questions. Results show that our approach contributes to the system's best
performance when it is conjugated with typical Answer-Extraction strategies.
Moreover, it allows the system to learn with the answered questions and to
rectify wrong or unsolved past questions."
"This paper considers the problem for estimating the quality of machine
translation outputs which are independent of human intervention and are
generally addressed using machine learning techniques.There are various
measures through which a machine learns translations quality. Automatic
Evaluation metrics produce good co-relation at corpus level but cannot produce
the same results at the same segment or sentence level. In this paper 16
features are extracted from the input sentences and their translations and a
quality score is obtained based on Bayesian inference produced from training
data."
"This document gives a brief description of Korean data prepared for the SPMRL
2013 shared task. A total of 27,363 sentences with 350,090 tokens are used for
the shared task. All constituent trees are collected from the KAIST Treebank
and transformed to the Penn Treebank style. All dependency trees are converted
from the transformed constituent trees using heuristics and labeling rules de-
signed specifically for the KAIST Treebank. In addition to the gold-standard
morphological analysis provided by the KAIST Treebank, two sets of automatic
morphological analysis are provided for the shared task, one is generated by
the HanNanum morphological analyzer, and the other is generated by the Sejong
morphological analyzer."
"UNL system is designed and implemented by a nonprofit organization, UNDL
Foundation at Geneva in 1999. UNL applications are application softwares that
allow end users to accomplish natural language tasks, such as translating,
summarizing, retrieving or extracting information, etc. Two major web based
application softwares are Interactive ANalyzer (IAN), which is a natural
language analysis system. It represents natural language sentences as semantic
networks in the UNL format. Other application software is dEep-to-sUrface
GENErator (EUGENE), which is an open-source interactive NLizer. It generates
natural language sentences out of semantic networks represented in the UNL
format. In this paper, NLization framework with EUGENE is focused, while using
UNL system for accomplishing the task of machine translation. In whole
NLization process, EUGENE takes a UNL input and delivers an output in natural
language without any human intervention. It is language-independent and has to
be parametrized to the natural language input through a dictionary and a
grammar, provided as separate interpretable files. In this paper, it is
explained that how UNL input is syntactically and semantically analyzed with
the UNL-NL T-Grammar for NLization of UNL sentences involving verbs, pronouns
and determiners for Punjabi natural language."
"Textual sentiment analysis and emotion detection consists in retrieving the
sentiment or emotion carried by a text or document. This task can be useful in
many domains: opinion mining, prediction, feedbacks, etc. However, building a
general purpose tool for doing sentiment analysis and emotion detection raises
a number of issues, theoretical issues like the dependence to the domain or to
the language but also pratical issues like the emotion representation for
interoperability. In this paper we present our sentiment/emotion analysis
tools, the way we propose to circumvent the di culties and the applications
they are used for."
"Dictionaries and phrase tables are the basis of modern statistical machine
translation systems. This paper develops a method that can automate the process
of generating and extending dictionaries and phrase tables. Our method can
translate missing word and phrase entries by learning language structures based
on large monolingual data and mapping between languages from small bilingual
data. It uses distributed representation of words and learns a linear mapping
between vector spaces of languages. Despite its simplicity, our method is
surprisingly effective: we can achieve almost 90% precision@5 for translation
of words between English and Spanish. This method makes little assumption about
the languages, so it can be used to extend and refine dictionaries and
translation tables for any language pairs."
"Learning word representations has recently seen much success in computational
linguistics. However, assuming sequences of word tokens as input to linguistic
analysis is often unjustified. For many languages word segmentation is a
non-trivial task and naturally occurring text is sometimes a mixture of natural
language strings and other character data. We propose to learn text
representations directly from raw character sequences by training a Simple
recurrent Network to predict the next character in text. The network uses its
hidden layer to evolve abstract representations of the character sequences it
sees. To demonstrate the usefulness of the learned text embeddings, we use them
as features in a supervised character level text segmentation and labeling
task: recognizing spans of text containing programming language code. By using
the embeddings as features we are able to substantially improve over a baseline
which uses only surface character n-grams."
"EuroVoc (2012) is a highly multilingual thesaurus consisting of over 6,700
hierarchically organised subject domains used by European Institutions and many
authorities in Member States of the European Union (EU) for the classification
and retrieval of official documents. JEX is JRC-developed multi-label
classification software that learns from manually labelled data to
automatically assign EuroVoc descriptors to new documents in a profile-based
category-ranking task. The JEX release consists of trained classifiers for 22
official EU languages, of parallel training data in the same languages, of an
interface that allows viewing and amending the assignment results, and of a
module that allows users to re-train the tool on their own document
collections. JEX allows advanced users to change the document representation so
as to possibly improve the categorisation result through linguistic
pre-processing. JEX can be used as a tool for interactive EuroVoc descriptor
assignment to increase speed and consistency of the human categorisation
process, or it can be used fully automatically. The output of JEX is a
language-independent EuroVoc feature vector lending itself also as input to
various other Language Technology tasks, including cross-lingual clustering and
classification, cross-lingual plagiarism detection, sentence selection and
ranking, and more."
"The European Commission's (EC) Directorate General for Translation, together
with the EC's Joint Research Centre, is making available a large translation
memory (TM; i.e. sentences and their professionally produced translations)
covering twenty-two official European Union (EU) languages and their 231
language pairs. Such a resource is typically used by translation professionals
in combination with TM software to improve speed and consistency of their
translations. However, this resource has also many uses for translation studies
and for language technology applications, including Statistical Machine
Translation (SMT), terminology extraction, Named Entity Recognition (NER),
multilingual classification and clustering, and many more. In this reference
paper for DGT-TM, we introduce this new resource, provide statistics regarding
its size, and explain how it was produced and how to use it."
"Most large organizations have dedicated departments that monitor the media to
keep up-to-date with relevant developments and to keep an eye on how they are
represented in the news. Part of this media monitoring work can be automated.
In the European Union with its 23 official languages, it is particularly
important to cover media reports in many languages in order to capture the
complementary news content published in the different countries. It is also
important to be able to access the news content across languages and to merge
the extracted information. We present here the four publicly accessible systems
of the Europe Media Monitor (EMM) family of applications, which cover between
19 and 50 languages (see http://press.jrc.it/overview.html). We give an
overview of their functionality and discuss some of the implications of the
fact that they cover quite so many languages. We discuss design issues
necessary to be able to achieve this high multilinguality, as well as the
benefits of this multilinguality."
"Colour is a key component in the successful dissemination of information.
Since many real-world concepts are associated with colour, for example danger
with red, linguistic information is often complemented with the use of
appropriate colours in information visualization and product marketing. Yet,
there is no comprehensive resource that captures concept-colour associations.
We present a method to create a large word-colour association lexicon by
crowdsourcing. A word-choice question was used to obtain sense-level
annotations and to ensure data quality. We focus especially on abstract
concepts and emotions to show that even they tend to have strong colour
associations. Thus, using the right colours can not only improve semantic
coherence, but also inspire the desired emotional response."
"The Linguistic Data Consortium (LDC) has developed hundreds of data corpora
for natural language processing (NLP) research. Among these are a number of
annotated treebank corpora for Arabic. Typically, these corpora consist of a
single collection of annotated documents. NLP research, however, usually
requires multiple data sets for the purposes of training models, developing
techniques, and final evaluation. Therefore it becomes necessary to divide the
corpora used into the required data sets (divisions). This document details a
set of rules that have been defined to enable consistent divisions for old and
new Arabic treebanks (ATB) and related corpora."
"In this paper, a new hybrid algorithm which combines both of token-based and
character-based approaches is presented. The basic Levenshtein approach has
been extended to token-based distance metric. The distance metric is enhanced
to set the proper granularity level behavior of the algorithm. It smoothly maps
a threshold of misspellings differences at the character level, and the
importance of token level errors in terms of token's position and frequency.
Using a large Arabic dataset, the experimental results show that the proposed
algorithm overcomes successfully many types of errors such as: typographical
errors, omission or insertion of middle name components, omission of
non-significant popular name components, and different writing styles character
variations. When compared the results with other classical algorithms, using
the same dataset, the proposed algorithm was found to increase the minimum
success level of best tested algorithms, while achieving higher upper limits ."
"Assigning a positive or negative score to a word out of context (i.e. a
word's prior polarity) is a challenging task for sentiment analysis. In the
literature, various approaches based on SentiWordNet have been proposed. In
this paper, we compare the most often used techniques together with newly
proposed ones and incorporate all of them in a learning framework to see
whether blending them can further improve the estimation of prior polarity
scores. Using two different versions of SentiWordNet and testing regression and
classification models across tasks and datasets, our learning approach
consistently outperforms the single metrics, providing a new state-of-the-art
approach in computing words' prior polarity for sentiment analysis. We conclude
our investigation showing interesting biases in calculated prior polarity
scores when word Part of Speech and annotator gender are considered."
"Today we have access to unprecedented amounts of literary texts. However,
search still relies heavily on key words. In this paper, we show how sentiment
analysis can be used in tandem with effective visualizations to quantify and
track emotions in both individual books and across very large collections. We
introduce the concept of emotion word density, and using the Brothers Grimm
fairy tales as example, we show how collections of text can be organized for
better search. Using the Google Books Corpus we show how to determine an
entity's emotion associations from co-occurring words. Finally, we compare
emotion words in fairy tales and novels, to show that fairy tales have a much
wider range of emotion word densities than novels."
"Since many real-world concepts are associated with colour, for example danger
with red, linguistic information is often complimented with the use of
appropriate colours in information visualization and product marketing. Yet,
there is no comprehensive resource that captures concept-colour associations.
We present a method to create a large word-colour association lexicon by
crowdsourcing. We focus especially on abstract concepts and emotions to show
that even though they cannot be physically visualized, they too tend to have
strong colour associations. Finally, we show how word-colour associations
manifest themselves in language, and quantify usefulness of co-occurrence and
polarity cues in automatically detecting colour associations."
"This paper describes a new, freely available, highly multilingual named
entity resource for person and organisation names that has been compiled over
seven years of large-scale multilingual news analysis combined with Wikipedia
mining, resulting in 205,000 per-son and organisation names plus about the same
number of spelling variants written in over 20 different scripts and in many
more languages. This resource, produced as part of the Europe Media Monitor
activity (EMM, http://emm.newsbrief.eu/overview.html), can be used for a number
of purposes. These include improving name search in databases or on the
internet, seeding machine learning systems to learn named entity recognition
rules, improve machine translation results, and more. We describe here how this
resource was created; we give statistics on its current size; we address the
issue of morphological inflection; and we give details regarding its
functionality. Updates to this resource will be made available daily."
"We are presenting work on recognising acronyms of the form Long-Form
(Short-Form) such as ""International Monetary Fund (IMF)"" in millions of news
articles in twenty-two languages, as part of our more general effort to
recognise entities and their variants in news text and to use them for the
automatic analysis of the news, including the linking of related news across
languages. We show how the acronym recognition patterns, initially developed
for medical terms, needed to be adapted to the more general news domain and we
present evaluation results. We describe our effort to automatically merge the
numerous long-form variants referring to the same short-form, while keeping
non-related long-forms separate. Finally, we provide extensive statistics on
the frequency and the distribution of short-form/long-form pairs across
languages."
"Recent years have brought a significant growth in the volume of research in
sentiment analysis, mostly on highly subjective text types (movie or product
reviews). The main difference these texts have with news articles is that their
target is clearly defined and unique across the text. Following different
annotation efforts and the analysis of the issues encountered, we realised that
news opinion mining is different from that of other text types. We identified
three subtasks that need to be addressed: definition of the target; separation
of the good and bad news content from the good and bad sentiment expressed on
the target; and analysis of clearly marked opinion that is expressed
explicitly, not needing interpretation or the use of world knowledge.
Furthermore, we distinguish three different possible views on newspaper
articles - author, reader and text, which have to be addressed differently at
the time of analysing sentiment. Given these definitions, we present work on
mining opinions about entities in English language news, in which (a) we test
the relative suitability of various sentiment dictionaries and (b) we attempt
to separate positive or negative opinion from good or bad news. In the
experiments described here, we tested whether or not subject domain-defining
vocabulary should be ignored. Results showed that this idea is more appropriate
in the context of news opinion mining and that the approaches taking this into
consideration produce a better performance."
"With the widespread use of email, we now have access to unprecedented amounts
of text that we ourselves have written. In this paper, we show how sentiment
analysis can be used in tandem with effective visualizations to quantify and
track emotions in many types of mail. We create a large word--emotion
association lexicon by crowdsourcing, and use it to compare emotions in love
letters, hate mail, and suicide notes. We show that there are marked
differences across genders in how they use emotion words in work-place email.
For example, women use many words from the joy--sadness axis, whereas men
prefer terms from the fear--trust axis. Finally, we show visualizations that
can help people track emotions in their emails."
"Past work on personality detection has shown that frequency of lexical
categories such as first person pronouns, past tense verbs, and sentiment words
have significant correlations with personality traits. In this paper, for the
first time, we show that fine affect (emotion) categories such as that of
excitement, guilt, yearning, and admiration are significant indicators of
personality. Additionally, we perform experiments to show that the gains
provided by the fine affect categories are not obtained by using coarse affect
categories alone or with specificity features alone. We employ these features
in five SVM classifiers for detecting five personality traits through essays.
We find that the use of fine emotion features leads to statistically
significant improvement over a competitive baseline, whereas the use of coarse
affect and specificity features does not."
"This paper focuses on the automatic extraction of domain-specific sentiment
word (DSSW), which is a fundamental subtask of sentiment analysis. Most
previous work utilizes manual patterns for this task. However, the performance
of those methods highly relies on the labelled patterns or selected seeds. In
order to overcome the above problem, this paper presents an automatic framework
to detect large-scale domain-specific patterns for DSSW extraction. To this
end, sentiment seeds are extracted from massive dataset of user comments.
Subsequently, these sentiment seeds are expanded by synonyms using a
bootstrapping mechanism. Simultaneously, a synonymy graph is built and the
graph propagation algorithm is applied on the built synonymy graph. Afterwards,
syntactic and sequential relations between target words and high-ranked
sentiment words are extracted automatically to construct large-scale patterns,
which are further used to extracte DSSWs. The experimental results in three
domains reveal the effectiveness of our method."
"A balanced speech corpus is the basic need for any speech processing task. In
this report we describe our effort on development of Assamese speech corpus. We
mainly focused on some issues and challenges faced during development of the
corpus. Being a less computationally aware language, this is the first effort
to develop speech corpus for Assamese. As corpus development is an ongoing
process, in this paper we report only the initial task."
"In modern electronic medical records (EMR) much of the clinically important
data - signs and symptoms, symptom severity, disease status, etc. - are not
provided in structured data fields, but rather are encoded in clinician
generated narrative text. Natural language processing (NLP) provides a means of
""unlocking"" this important data source for applications in clinical decision
support, quality assurance, and public health. This chapter provides an
overview of representative NLP systems in biomedicine based on a unified
architectural view. A general architecture in an NLP system consists of two
main components: background knowledge that includes biomedical knowledge
resources and a framework that integrates NLP tools to process text. Systems
differ in both components, which we will review briefly. Additionally,
challenges facing current research efforts in biomedical NLP include the
paucity of large, publicly available annotated corpora, although initiatives
that facilitate data sharing, system evaluation, and collaborative work between
researchers in clinical NLP are starting to emerge."
"Current multi-document summarization systems can successfully extract summary
sentences, however with many limitations including: low coverage, inaccurate
extraction to important sentences, redundancy and poor coherence among the
selected sentences. The present study introduces a new concept of centroid
approach and reports new techniques for extracting summary sentences for
multi-document. In both techniques keyphrases are used to weigh sentences and
documents. The first summarization technique (Sen-Rich) prefers maximum
richness sentences. While the second (Doc-Rich), prefers sentences from
centroid document. To demonstrate the new summarization system application to
extract summaries of Arabic documents we performed two experiments. First, we
applied Rouge measure to compare the new techniques among systems presented at
TAC2011. The results show that Sen-Rich outperformed all systems in ROUGE-S.
Second, the system was applied to summarize multi-topic documents. Using human
evaluators, the results show that Doc-Rich is the superior, where summary
sentences characterized by extra coverage and more cohesion."
"We developed a type-theoretical framework for natural lan- guage semantics
that, in addition to the usual Montagovian treatment of compositional
semantics, includes a treatment of some phenomena of lex- ical semantic:
coercions, meaning, transfers, (in)felicitous co-predication. In this setting
we see how the various readings of plurals (collective, dis- tributive,
coverings,...) can be modelled."
"Spoken Language Systems at Saarland University (LSV) participated this year
with 5 runs at the TAC KBP English slot filling track. Effective algorithms for
all parts of the pipeline, from document retrieval to relation prediction and
response post-processing, are bundled in a modular end-to-end relation
extraction system called RelationFactory. The main run solely focuses on
shallow techniques and achieved significant improvements over LSV's last year's
system, while using the same training data and patterns. Improvements mainly
have been obtained by a feature representation focusing on surface skip n-grams
and improved scoring for extracted distant supervision patterns. Important
factors for effective extraction are the training and tuning scheme for distant
supervision classifiers, and the query expansion by a translation model based
on Wikipedia links. In the TAC KBP 2013 English Slotfilling evaluation, the
submitted main run of the LSV RelationFactory system achieved the top-ranked
F1-score of 37.3%."
"Computational measures of semantic similarity between geographic terms
provide valuable support across geographic information retrieval, data mining,
and information integration. To date, a wide variety of approaches to
geo-semantic similarity have been devised. A judgment of similarity is not
intrinsically right or wrong, but obtains a certain degree of cognitive
plausibility, depending on how closely it mimics human behavior. Thus selecting
the most appropriate measure for a specific task is a significant challenge. To
address this issue, we make an analogy between computational similarity
measures and soliciting domain expert opinions, which incorporate a subjective
set of beliefs, perceptions, hypotheses, and epistemic biases. Following this
analogy, we define the semantic similarity ensemble (SSE) as a composition of
different similarity measures, acting as a panel of experts having to reach a
decision on the semantic similarity of a set of geographic terms. The approach
is evaluated in comparison to human judgments, and results indicate that an SSE
performs better than the average of its parts. Although the best member tends
to outperform the ensemble, all ensembles outperform the average performance of
each ensemble's member. Hence, in contexts where the best measure is unknown,
the ensemble provides a more cognitively plausible approach."
"Dictionaries are essence of any language providing vital linguistic recourse
for the language learners, researchers and scholars. This paper focuses on the
methodology and techniques used in developing software architecture for a
UBSESD (Unicode Based Sindhi to English and English to Sindhi Dictionary). The
proposed system provides an accurate solution for construction and
representation of Unicode based Sindhi characters in a dictionary implementing
Hash Structure algorithm and a custom java Object as its internal data
structure saved in a file. The System provides facilities for Insertion,
Deletion and Editing of new records of Sindhi. Through this framework any type
of Sindhi to English and English to Sindhi Dictionary (belonging to different
domains of knowledge, e.g. engineering, medicine, computer, biology etc.) could
be developed easily with accurate representation of Unicode Characters in font
independent manner."
"In this study, a dictionary-based method is used to extract expressive
concepts from documents. So far, there have been many studies concerning
concept mining in English, but this area of study for Turkish, an agglutinative
language, is still immature. We used dictionary instead of WordNet, a lexical
database grouping words into synsets that is widely used for concept
extraction. The dictionaries are rarely used in the domain of concept mining,
but taking into account that dictionary entries have synonyms, hypernyms,
hyponyms and other relationships in their meaning texts, the success rate has
been high for determining concepts. This concept extraction method is
implemented on documents, that are collected from different corpora."
"Multilingual text processing is useful because the information content found
in different languages is complementary, both regarding facts and opinions.
While Information Extraction and other text mining software can, in principle,
be developed for many languages, most text analysis tools have only been
applied to small sets of languages because the development effort per language
is large. Self-training tools obviously alleviate the problem, but even the
effort of providing training data and of manually tuning the results is usually
considerable. In this paper, we gather insights by various multilingual system
developers on how to minimise the effort of developing natural language
processing applications for many languages. We also explain the main guidelines
underlying our own effort to develop complex text mining software for tens of
languages. While these guidelines - most of all: extreme simplicity - can be
very restrictive and limiting, we believe to have shown the feasibility of the
approach through the development of the Europe Media Monitor (EMM) family of
applications (http://emm.newsbrief.eu/overview.html). EMM is a set of complex
media monitoring tools that process and analyse up to 100,000 online news
articles per day in between twenty and fifty languages. We will also touch upon
the kind of language resources that would make it easier for all to develop
highly multilingual text mining applications. We will argue that - to achieve
this - the most needed resources would be freely available, simple, parallel
and uniform multilingual dictionaries, corpora and software tools."
"We propose a real-time machine translation system that allows users to select
a news category and to translate the related live news articles from Arabic,
Czech, Danish, Farsi, French, German, Italian, Polish, Portuguese, Spanish and
Turkish into English. The Moses-based system was optimised for the news domain
and differs from other available systems in four ways: (1) News items are
automatically categorised on the source side, before translation; (2) Named
entity translation is optimised by recognising and extracting them on the
source side and by re-inserting their translation in the target language,
making use of a separate entity repository; (3) News titles are translated with
a separate translation system which is optimised for the specific style of news
titles; (4) The system was optimised for speed in order to cope with the large
volume of daily news articles."
"In this study it is proven that the Hrebs used in Denotation analysis of
texts and Cohesion Chains (defined as a fusion between Lexical Chains and
Coreference Chains) represent similar linguistic tools. This result gives us
the possibility to extend to Cohesion Chains (CCs) some important indicators
as, for example the Kernel of CCs, the topicality of a CC, text concentration,
CC-diffuseness and mean diffuseness of the text. Let us mention that nowhere in
the Lexical Chains or Coreference Chains literature these kinds of indicators
are introduced and used since now. Similarly, some applications of CCs in the
study of a text (as for example segmentation or summarization of a text) could
be realized starting from hrebs. As an illustration of the similarity between
Hrebs and CCs a detailed analyze of the poem ""Lacul"" by Mihai Eminescu is
given."
"Large bilingual parallel texts (also known as bitexts) are usually stored in
a compressed form, and previous work has shown that they can be more
efficiently compressed if the fact that the two texts are mutual translations
is exploited. For example, a bitext can be seen as a sequence of biwords
---pairs of parallel words with a high probability of co-occurrence--- that can
be used as an intermediate representation in the compression process. However,
the simple biword approach described in the literature can only exploit
one-to-one word alignments and cannot tackle the reordering of words. We
therefore introduce a generalization of biwords which can describe multi-word
expressions and reorderings. We also describe some methods for the binary
compression of generalized biword sequences, and compare their performance when
different schemes are applied to the extraction of the biword sequence. In
addition, we show that this generalization of biwords allows for the
implementation of an efficient algorithm to look on the compressed bitext for
words or text segments in one of the texts and retrieve their counterpart
translations in the other text ---an application usually referred to as
translation spotting--- with only some minor modifications in the compression
algorithm."
"This paper presents a tree-to-tree transduction method for sentence
compression. Our model is based on synchronous tree substitution grammar, a
formalism that allows local distortion of the tree topology and can thus
naturally capture structural mismatches. We describe an algorithm for decoding
in this framework and show how the model can be trained discriminatively within
a large margin framework. Experimental results on sentence compression bring
significant improvements over a state-of-the-art model."
"This article considers the task of automatically inducing role-semantic
annotations in the FrameNet paradigm for new languages. We propose a general
framework that is based on annotation projection, phrased as a graph
optimization problem. It is relatively inexpensive and has the potential to
reduce the human effort involved in creating role-semantic resources. Within
this framework, we present projection models that exploit lexical and syntactic
information. We provide an experimental evaluation on an English-German
parallel corpus which demonstrates the feasibility of inducing high-precision
German semantic role annotation both for manually and automatically annotated
English data."
"We demonstrate the effectiveness of multilingual learning for unsupervised
part-of-speech tagging. The central assumption of our work is that by combining
cues from multiple languages, the structure of each becomes more apparent. We
consider two ways of applying this intuition to the problem of unsupervised
part-of-speech tagging: a model that directly merges tag structures for a pair
of languages into a single sequence and a second model which instead
incorporates multilingual context using latent variables. Both approaches are
formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo
sampling techniques for inference. Our results demonstrate that by
incorporating multilingual evidence we can achieve impressive performance gains
across a range of scenarios. We also found that performance improves steadily
as the number of available languages increases."
"The task of identifying synonymous relations and objects, or synonym
resolution, is critical for high-quality information extraction. This paper
investigates synonym resolution in the context of unsupervised information
extraction, where neither hand-tagged training examples nor domain knowledge is
available. The paper presents a scalable, fully-implemented system that runs in
O(KN log N) time in the number of extractions, N, and the maximum number of
synonyms per word, K. The system, called Resolver, introduces a probabilistic
relational model for predicting whether two strings are co-referential based on
the similarity of the assertions containing them. On a set of two million
assertions extracted from the Web, Resolver resolves objects with 78% precision
and 68% recall, and resolves relations with 90% precision and 35% recall.
Several variations of resolvers probabilistic model are explored, and
experiments demonstrate that under appropriate conditions these variations can
improve F1 by 5%. An extension to the basic Resolver system allows it to handle
polysemous names with 97% precision and 95% recall on a data set from the TREC
corpus."
"Adequate representation of natural language semantics requires access to vast
amounts of common sense and domain-specific world knowledge. Prior work in the
field was based on purely statistical techniques that did not make use of
background knowledge, on limited lexicographic knowledge bases such as WordNet,
or on huge manual efforts such as the CYC project. Here we propose a novel
method, called Explicit Semantic Analysis (ESA), for fine-grained semantic
interpretation of unrestricted natural language texts. Our method represents
meaning in a high-dimensional space of concepts derived from Wikipedia, the
largest encyclopedia in existence. We explicitly represent the meaning of any
text in terms of Wikipedia-based concepts. We evaluate the effectiveness of our
method on text categorization and on computing the degree of semantic
relatedness between fragments of natural language text. Using ESA results in
significant improvements over the previous state of the art in both tasks.
Importantly, due to the use of natural concepts, the ESA model is easy to
explain to human users."
"In a significant minority of cases, certain pronouns, especially the pronoun
it, can be used without referring to any specific entity. This phenomenon of
pleonastic pronoun usage poses serious problems for systems aiming at even a
shallow understanding of natural language texts. In this paper, a novel
approach is proposed to identify such uses of it: the extrapositional cases are
identified using a series of queries against the web, and the cleft cases are
identified using a simple set of syntactic rules. The system is evaluated with
four sets of news articles containing 679 extrapositional cases as well as 78
cleft constructs. The identification results are comparable to those obtained
by human efforts."
"The computation of relatedness between two fragments of text in an automated
manner requires taking into account a wide range of factors pertaining to the
meaning the two fragments convey, and the pairwise relations between their
words. Without doubt, a measure of relatedness between text segments must take
into account both the lexical and the semantic relatedness between words. Such
a measure that captures well both aspects of text relatedness may help in many
tasks, such as text retrieval, classification and clustering. In this paper we
present a new approach for measuring the semantic relatedness between words
based on their implicit semantic links. The approach exploits only a word
thesaurus in order to devise implicit semantic links between words. Based on
this approach, we introduce Omiotis, a new measure of semantic relatedness
between texts which capitalizes on the word-to-word semantic relatedness
measure (SR) and extends it to measure the relatedness between texts. We
gradually validate our method: we first evaluate the performance of the
semantic relatedness measure between individual words, covering word-to-word
similarity and relatedness, synonym identification and word analogy; then, we
proceed with evaluating the performance of our method in measuring text-to-text
semantic relatedness in two tasks, namely sentence-to-sentence similarity and
paraphrase recognition. Experimental evaluation shows that the proposed method
outperforms every lexicon-based method of semantic relatedness in the selected
tasks and the used data sets, and competes well against corpus-based and hybrid
approaches."
"This paper describes a method for the automatic inference of structural
transfer rules to be used in a shallow-transfer machine translation (MT) system
from small parallel corpora. The structural transfer rules are based on
alignment templates, like those used in statistical MT. Alignment templates are
extracted from sentence-aligned parallel corpora and extended with a set of
restrictions which are derived from the bilingual dictionary of the MT system
and control their application as transfer rules. The experiments conducted
using three different language pairs in the free/open-source MT platform
Apertium show that translation quality is improved as compared to word-for-word
translation (when no transfer rules are used), and that the resulting
translation quality is close to that obtained using hand-coded transfer rules.
The method we present is entirely unsupervised and benefits from information in
the rest of modules of the MT system in which the inferred rules are applied."
"Semantic parsing, i.e., the automatic derivation of meaning representation
such as an instantiated predicate-argument structure for a sentence, plays a
critical role in deep processing of natural language. Unlike all other top
systems of semantic dependency parsing that have to rely on a pipeline
framework to chain up a series of submodels each specialized for a specific
subtask, the one presented in this article integrates everything into one
model, in hopes of achieving desirable integrity and practicality for real
applications while maintaining a competitive performance. This integrative
approach tackles semantic parsing as a word pair classification problem using a
maximum entropy classifier. We leverage adaptive pruning of argument candidates
and large-scale feature selection engineering to allow the largest feature
space ever in use so far in this field, it achieves a state-of-the-art
performance on the evaluation data set for CoNLL-2008 shared task, on top of
all but one top pipeline system, confirming its feasibility and effectiveness."
"One of the key issues in both natural language understanding and generation
is the appropriate processing of Multiword Expressions (MWEs). MWEs pose a huge
problem to the precise language processing due to their idiosyncratic nature
and diversity in lexical, syntactical and semantic properties. The semantics of
a MWE cannot be expressed after combining the semantics of its constituents.
Therefore, the formalism of semantic clustering is often viewed as an
instrument for extracting MWEs especially for resource constraint languages
like Bengali. The present semantic clustering approach contributes to locate
clusters of the synonymous noun tokens present in the document. These clusters
in turn help measure the similarity between the constituent words of a
potentially candidate phrase using a vector space model and judge the
suitability of this phrase to be a MWE. In this experiment, we apply the
semantic clustering approach for noun-noun bigram MWEs, though it can be
extended to any types of MWEs. In parallel, the well known statistical models,
namely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR),
Significance function are also employed to extract MWEs from the Bengali
corpus. The comparative evaluation shows that the semantic clustering approach
outperforms all other competing statistical models. As a by-product of this
experiment, we have started developing a standard lexicon in Bengali that
serves as a productive Bengali linguistic thesaurus."
"We present a statistical parsing framework for sentence-level sentiment
classification in this article. Unlike previous works that employ syntactic
parsing results for sentiment analysis, we develop a statistical parser to
directly analyze the sentiment structure of a sentence. We show that
complicated phenomena in sentiment analysis (e.g., negation, intensification,
and contrast) can be handled the same as simple and straightforward sentiment
expressions in a unified and probabilistic way. We formulate the sentiment
grammar upon Context-Free Grammars (CFGs), and provide a formal description of
the sentiment parsing framework. We develop the parsing model to obtain
possible sentiment parse trees for a sentence, from which the polarity model is
proposed to derive the sentiment strength and polarity, and the ranking model
is dedicated to selecting the best sentiment tree. We train the parser directly
from examples of sentences annotated only with sentiment polarity labels but
without any syntactic annotations or polarity annotations of constituents
within sentences. Therefore we can obtain training data easily. In particular,
we train a sentiment parser, s.parser, from a large amount of review sentences
with users' ratings as rough sentiment polarity labels. Extensive experiments
on existing benchmark datasets show significant improvements over baseline
sentiment classification approaches."
"We present a model for aggregation of product review snippets by joint aspect
identification and sentiment analysis. Our model simultaneously identifies an
underlying set of ratable aspects presented in the reviews of a product (e.g.,
sushi and miso for a Japanese restaurant) and determines the corresponding
sentiment of each aspect. This approach directly enables discovery of
highly-rated or inconsistent aspects of a product. Our generative model admits
an efficient variational mean-field inference algorithm. It is also easily
extensible, and we describe several modifications and their effects on model
structure and inference. We test our model on two tasks, joint aspect
identification and sentiment analysis on a set of Yelp reviews and aspect
identification alone on a set of medical summaries. We evaluate the performance
of the model on aspect identification, sentiment analysis, and per-word
labeling accuracy. We demonstrate that our model outperforms applicable
baselines by a considerable margin, yielding up to 32% relative error reduction
on aspect identification and up to 20% relative error reduction on sentiment
analysis."
"To tackle the vocabulary problem in conversational systems, previous work has
applied unsupervised learning approaches on co-occurring speech and eye gaze
during interaction to automatically acquire new words. Although these
approaches have shown promise, several issues related to human language
behavior and human-machine conversation have not been addressed. First,
psycholinguistic studies have shown certain temporal regularities between human
eye movement and language production. While these regularities can potentially
guide the acquisition process, they have not been incorporated in the previous
unsupervised approaches. Second, conversational systems generally have an
existing knowledge base about the domain and vocabulary. While the existing
knowledge can potentially help bootstrap and constrain the acquired new words,
it has not been incorporated in the previous models. Third, eye gaze could
serve different functions in human-machine conversation. Some gaze streams may
not be closely coupled with speech stream, and thus are potentially detrimental
to word acquisition. Automated recognition of closely-coupled speech-gaze
streams based on conversation context is important. To address these issues, we
developed new approaches that incorporate user language behavior, domain
knowledge, and conversation context in word acquisition. We evaluated these
approaches in the context of situated dialogue in a virtual world. Our
experimental results have shown that incorporating the above three types of
contextual information significantly improves word acquisition performance."
"We propose a novel language-independent approach for improving machine
translation for resource-poor languages by exploiting their similarity to
resource-rich ones. More precisely, we improve the translation from a
resource-poor source language X_1 into a resource-rich language Y given a
bi-text containing a limited number of parallel sentences for X_1-Y and a
larger bi-text for X_2-Y for some resource-rich language X_2 that is closely
related to X_1. This is achieved by taking advantage of the opportunities that
vocabulary overlap and similarities between the languages X_1 and X_2 in
spelling, word order, and syntax offer: (1) we improve the word alignments for
the resource-poor language, (2) we further augment it with additional
translation options, and (3) we take care of potential spelling differences
through appropriate transliteration. The evaluation for Indonesian- >English
using Malay and for Spanish -> English using Portuguese and pretending Spanish
is resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points,
respectively, which is an improvement over the best rivaling approaches, while
using much less additional data. Overall, our method cuts the amount of
necessary ""real training data by a factor of 2--5."
"We measured entropy and symbolic diversity for English and Spanish texts
including literature Nobel laureates and other famous authors. Entropy, symbol
diversity and symbol frequency profiles were compared for these four groups. We
also built a scale sensitive to the quality of writing and evaluated its
relationship with the Flesch's readability index for English and the
Szigriszt's perspicuity index for Spanish. Results suggest a correlation
between entropy and word diversity with quality of writing. Text genre also
influences the resulting entropy and diversity of the text. Results suggest the
plausibility of automated quality assessment of texts."
"We propose a range of deep lexical acquisition methods which make use of
morphological, syntactic and ontological language resources to model word
similarity and bootstrap from a seed lexicon. The different methods are
deployed in learning lexical items for a precision grammar, and shown to each
have strengths and weaknesses over different word classes. A particular focus
of this paper is the relative accessibility of different language resource
types, and predicted ``bang for the buck'' associated with each in deep lexical
acquisition applications."
"A computational model of the construction of word meaning through exposure to
texts is built in order to simulate the effects of co-occurrence values on word
semantic similarities, paragraph by paragraph. Semantic similarity is here
viewed as association. It turns out that the similarity between two words W1
and W2 strongly increases with a co-occurrence, decreases with the occurrence
of W1 without W2 or W2 without W1, and slightly increases with high-order
co-occurrences. Therefore, operationalizing similarity as a frequency of
co-occurrence probably introduces a bias: first, there are cases in which there
is similarity without co-occurrence and, second, the frequency of co-occurrence
overestimates similarity."
"The derivation trees of a tree adjoining grammar provide a first insight into
the sentence semantics, and are thus prime targets for generation systems. We
define a formalism, feature-based regular tree grammars, and a translation from
feature based tree adjoining grammars into this new formalism. The translation
preserves the derivation structures of the original grammar, and accounts for
feature unification."
"Proof nets are a graph theoretical representation of proofs in various
fragments of type-logical grammar. In spite of this basis in graph theory,
there has been relatively little attention to the use of graph theoretic
algorithms for type-logical proof search. In this paper we will look at several
ways in which standard graph theoretic algorithms can be used to restrict the
search space. In particular, we will provide an O(n4) algorithm for selecting
an optimal axiom link at any stage in the proof search as well as a O(kn3)
algorithm for selecting the k best proof candidates."
"In this paper we describe the conception of a software toolkit designed for
the construction, maintenance and collaborative use of a Generative Lexicon. In
order to ease its portability and spreading use, this tool was built with free
and open source products. We eventually tested the toolkit and showed it
filters the adequate form of anaphoric reference to the modifier in endocentric
compounds."
"We describe a modular system for generating sentences from formal definitions
of underlying linguistic structures using domain-specific languages. The system
uses Java in general, Prolog for lexical entries and custom domain-specific
languages based on Functional Grammar and Functional Discourse Grammar
notation, implemented using the ANTLR parser generator. We show how linguistic
and technological parts can be brought together in a natural language
processing system and how domain-specific languages can be used as a tool for
consistent formal notation in linguistic description."
"The type-theoretic modelling of DRT that [degroote06] proposed features
continuations for the management of the context in which a clause has to be
interpreted. This approach, while keeping the standard definitions of
quantifier scope, translates the rules of the accessibility constraints of
discourse referents inside the semantic recipes. In this paper, we deal with
additional rules for these accessibility constraints. In particular in the case
of discourse referents introduced by proper nouns, that negation does not
block, and in the case of rhetorical relations that structure discourses. We
show how this continuation-based approach applies to those accessibility
constraints and how we can consider the parallel management of various
principles."
"The goal of this paper is to present a model of children's semantic memory,
which is based on a corpus reproducing the kinds of texts children are exposed
to. After presenting the literature in the development of the semantic memory,
a preliminary French corpus of 3.2 million words is described. Similarities in
the resulting semantic space are compared to human data on four tests:
association norms, vocabulary test, semantic judgments and memory tasks. A
second corpus is described, which is composed of subcorpora corresponding to
various ages. This stratified corpus is intended as a basis for developmental
studies. Finally, two applications of these models of semantic memory are
presented: the first one aims at tracing the development of semantic
similarities paragraph by paragraph; the second one describes an implementation
of a model of text comprehension derived from the Construction-integration
model (Kintsch, 1988, 1998) and based on such models of semantic memory."
"In this paper we present two original methods for recognizing textual
inference. First one is a modified resolution method such that some linguistic
considerations are introduced in the unification of two atoms. The approach is
possible due to the recent methods of transforming texts in logic formulas.
Second one is based on semantic relations in text, as presented in WordNet.
Some similarities between these two methods are remarked."
"A large class of unsupervised algorithms for Word Sense Disambiguation (WSD)
is that of dictionary-based methods. Various algorithms have as the root Lesk's
algorithm, which exploits the sense definitions in the dictionary directly. Our
approach uses the lexical base WordNet for a new algorithm originated in
Lesk's, namely ""chain algorithm for disambiguation of all words"", CHAD. We show
how translation from a language into another one and also text entailment
verification could be accomplished by this disambiguation."
"Measuring the similarity of short written contexts is a fundamental problem
in Natural Language Processing. This article provides a unifying framework by
which short context problems can be categorized both by their intended
application and proposed solution. The goal is to show that various problems
and methodologies that appear quite different on the surface are in fact very
closely related. The axes by which these categorizations are made include the
format of the contexts (headed versus headless), the way in which the contexts
are to be measured (first-order versus second-order similarity), and the
information used to represent the features in the contexts (micro versus macro
views). The unifying thread that binds together many short context applications
and methods is the fact that similarity decisions must be made between contexts
that share few (if any) words in common."
"The paper presents a linguistic and computational model aiming at making the
morphological structure of the lexicon emerge from the formal and semantic
regularities of the words it contains. The model is word-based. The proposed
morphological structure consists of (1) binary relations that connect each
headword with words that are morphologically related, and especially with the
members of its morphological family and its derivational series, and of (2) the
analogies that hold between the words. The model has been tested on the lexicon
of French using the TLFi machine readable dictionary."
"We examine the issue of digital formats for document encoding, archiving and
publishing, through the specific example of ""born-digital"" scholarly journal
articles. We will begin by looking at the traditional workflow of journal
editing and publication, and how these practices have made the transition into
the online domain. We will examine the range of different file formats in which
electronic articles are currently stored and published. We will argue strongly
that, despite the prevalence of binary and proprietary formats such as PDF and
MS Word, XML is a far superior encoding choice for journal articles. Next, we
look at the range of XML document structures (DTDs, Schemas) which are in
common use for encoding journal articles, and consider some of their strengths
and weaknesses. We will suggest that, despite the existence of specialized
schemas intended specifically for journal articles (such as NLM), and more
broadly-used publication-oriented schemas such as DocBook, there are strong
arguments in favour of developing a subset or customization of the Text
Encoding Initiative (TEI) schema for the purpose of journal-article encoding;
TEI is already in use in a number of journal publication projects, and the
scale and precision of the TEI tagset makes it particularly appropriate for
encoding scholarly articles. We will outline the document structure of a
TEI-encoded journal article, and look in detail at suggested markup patterns
for specific features of journal articles."
"An important part of textual inference is making deductions involving
monotonicity, that is, determining whether a given assertion entails
restrictions or relaxations of that assertion. For instance, the statement 'We
know the epidemic spread quickly' does not entail 'We know the epidemic spread
quickly via fleas', but 'We doubt the epidemic spread quickly' entails 'We
doubt the epidemic spread quickly via fleas'. Here, we present the first
algorithm for the challenging lexical-semantics problem of learning linguistic
constructions that, like 'doubt', are downward entailing (DE). Our algorithm is
unsupervised, resource-lean, and effective, accurately recovering many DE
operators that are missing from the hand-constructed lists that
textual-inference systems currently use."
"We describe a statistical model over linguistic areas and phylogeny.
  Our model recovers known areas and identifies a plausible hierarchy of areal
features. The use of areas improves genetic reconstruction of languages both
qualitatively and quantitatively according to a variety of metrics. We model
linguistic areas by a Pitman-Yor process and linguistic phylogeny by Kingman's
coalescent."
"This paper presents the system called PATATRAS (PATent and Article Tracking,
Retrieval and AnalysiS) realized for the IP track of CLEF 2009. Our approach
presents three main characteristics: 1. The usage of multiple retrieval models
(KL, Okapi) and term index definitions (lemma, phrase, concept) for the three
languages considered in the present track (English, French, German) producing
ten different sets of ranked results. 2. The merging of the different results
based on multiple regression models using an additional validation set created
from the patent collection. 3. The exploitation of patent metadata and of the
citation structures for creating restricted initial working sets of patents and
for producing a final re-ranking regression model. As we exploit specific
metadata of the patent documents and the citation relations only at the
creation of initial working sets and during the final post ranking step, our
architecture remains generic and easy to extend."
"OLAC was founded in 2000 for creating online databases of language resources.
This paper intends to review the bottom-up distributed character of the project
and proposes an extension of the architecture for Dravidian languages. An
ontological structure is considered for effective natural language processing
(NLP) and its advantages over statistical methods are reviewed"
"The Lambek-Grishin calculus is a symmetric extension of the Lambek calculus:
in addition to the residuated family of product, left and right division
operations of Lambek's original calculus, one also considers a family of
coproduct, right and left difference operations, related to the former by an
arrow-reversing duality. Communication between the two families is implemented
in terms of linear distributivity principles. The aim of this paper is to
complement the symmetry between (dual) residuated type-forming operations with
an orthogonal opposition that contrasts residuated and Galois connected
operations. Whereas the (dual) residuated operations are monotone, the Galois
connected operations (and their duals) are antitone. We discuss the algebraic
properties of the (dual) Galois connected operations, and generalize the
(co)product distributivity principles to include the negative operations. We
give a continuation-passing-style translation for the new type-forming
operations, and discuss some linguistic applications."
"We report on work in progress on extracting lexical simplifications (e.g.,
""collaborate"" -> ""work together""), focusing on utilizing edit histories in
Simple English Wikipedia for this task. We consider two main approaches: (1)
deriving simplification probabilities via an edit model that accounts for a
mixture of different operations, and (2) using metadata to focus on edits that
are more likely to be simplification operations. We find our methods to
outperform a reasonable baseline and yield many high-quality lexical
simplifications not included in an independently-created manually prepared
list."
"Researchers in textual entailment have begun to consider inferences involving
'downward-entailing operators', an interesting and important class of lexical
items that change the way inferences are made. Recent work proposed a method
for learning English downward-entailing operators that requires access to a
high-quality collection of 'negative polarity items' (NPIs). However, English
is one of the very few languages for which such a list exists. We propose the
first approach that can be applied to the many languages for which there is no
pre-existing high-precision database of NPIs. As a case study, we apply our
method to Romanian and show that our method yields good results. Also, we
perform a cross-linguistic analysis that suggests interesting connections to
some findings in linguistic typology."
"It is usual to consider that standards generate mixed feelings among
scientists. They are often seen as not really reflecting the state of the art
in a given domain and a hindrance to scientific creativity. Still, scientists
should theoretically be at the best place to bring their expertise into
standard developments, being even more neutral on issues that may typically be
related to competing industrial interests. Even if it could be thought of as
even more complex to think about developping standards in the humanities, we
will show how this can be made feasible through the experience gained both
within the Text Encoding Initiative consortium and the International
Organisation for Standardisation. By taking the specific case of lexical
resources, we will try to show how this brings about new ideas for designing
future research infrastructures in the human and social sciences."
"We have developed a full discourse parser in the Penn Discourse Treebank
(PDTB) style. Our trained parser first identifies all discourse and
non-discourse relations, locates and labels their arguments, and then
classifies their relation types. When appropriate, the attribution spans to
these relations are also determined. We present a comprehensive evaluation from
both component-wise and error-cascading perspectives."
"A temporal analysis of emoticon use in Swedish, Italian, German and English
asynchronous electronic communication is reported. Emoticons are classified as
positive, negative and neutral. Postings to newsgroups over a 66 week period
are considered. The aggregate analysis of emoticon use in newsgroups for
science and politics tend on the whole to be consistent over the entire time
period. Where possible, events that coincide with divergences from trends in
language-subject pairs are noted. Political discourse in Italian over the
period shows marked use of negative emoticons, and in Swedish, positive
emoticons."
"This article describes a method to build syntactical dependencies starting
from the phrase structure parsing process. The goal is to obtain all the
information needed for a detailled semantical analysis. Interaction Grammars
are used for parsing; the saturation of polarities which is the core of this
formalism can be mapped to dependency relation. Formally, graph patterns are
used to express the set of constraints which control dependency creations."
"""What other people think"" has always been an important piece of information
during various decision-making processes. Today people frequently make their
opinions available via the Internet, and as a result, the Web has become an
excellent source for gathering consumer opinions. There are now numerous Web
resources containing such opinions, e.g., product reviews forums, discussion
groups, and Blogs. But, due to the large amount of information and the wide
range of sources, it is essentially impossible for a customer to read all of
the reviews and make an informed decision on whether to purchase the product.
It is also difficult for the manufacturer or seller of a product to accurately
monitor customer opinions. For this reason, mining customer reviews, or opinion
mining, has become an important issue for research in Web information
extraction. One of the important topics in this research area is the
identification of opinion polarity. The opinion polarity of a review is usually
expressed with values 'positive', 'negative' or 'neutral'. We propose a
technique for identifying polarity of reviews by identifying the polarity of
the adjectives that appear in them. Our evaluation shows the technique can
provide accuracy in the area of 73%, which is well above the 58%-64% provided
by naive Bayesian classifiers."
"Our study applies statistical methods to French and Italian corpora to
examine the phenomenon of multi-word term reduction in specialty languages.
There are two kinds of reduction: anaphoric and lexical. We show that anaphoric
reduction depends on the discourse type (vulgarization, pedagogical,
specialized) but is independent of both domain and language; that lexical
reduction depends on domain and is more frequent in technical, rapidly evolving
domains; and that anaphoric reductions tend to follow full terms rather than
precede them. We define the notion of the anaphoric tree of the term and study
its properties. Concerning lexical reduction, we attempt to prove statistically
that there is a notion of term lifecycle, where the full form is progressively
replaced by a lexical reduction. ----- Nous \'etudions par des m\'ethodes
statistiques sur des corpus fran\c{c}ais et italiens, le ph\'enom\`ene de
r\'eduction des termes complexes dans les langues de sp\'ecialit\'e. Il existe
deux types de r\'eductions : anaphorique et lexicale. Nous montrons que la
r\'eduction anaphorique d\'epend du type de discours (de vulgarisation,
p\'edagogique, sp\'ecialis\'e) mais ne d\'epend ni du domaine, ni de la langue,
alors que la r\'eduction lexicale d\'epend du domaine et est plus fr\'equente
dans les domaines techniques \`a \'evolution rapide. D'autre part, nous
montrons que la r\'eduction anaphorique a tendance \`a suivre la forme pleine
du terme, nous d\'efinissons une notion d'arbre anaphorique de terme et nous
\'etudions ses propri\'et\'es. Concernant la r\'eduction lexicale, nous tentons
de d\'emontrer statistiquement qu'il existe une notion de cycle de vie de
terme, o\`u la forme pleine est progressivement remplac\'ee par une r\'eduction
lexicale."
"Two formalisms, both based on context-free grammars, have recently been
proposed as a basis for a non-uniform random generation of combinatorial
objects. The former, introduced by Denise et al, associates weights with
letters, while the latter, recently explored by Weinberg et al in the context
of random generation, associates weights to transitions. In this short note, we
use a simple modification of the Greibach Normal Form transformation algorithm,
due to Blum and Koch, to show the equivalent expressivities, in term of their
induced distributions, of these two formalisms."
"This paper describes the use of Naive Bayes to address the task of assigning
function tags and context free grammar (CFG) to parse Myanmar sentences. Part
of the challenge of statistical function tagging for Myanmar sentences comes
from the fact that Myanmar has free-phrase-order and a complex morphological
system. Function tagging is a pre-processing step for parsing. In the task of
function tagging, we use the functional annotated corpus and tag Myanmar
sentences with correct segmentation, POS (part-of-speech) tagging and chunking
information. We propose Myanmar grammar rules and apply context free grammar
(CFG) to find out the parse tree of function tagged Myanmar sentences.
Experiments show that our analysis achieves a good result with parsing of
simple sentences and three types of complex sentences."
"Existing probabilistic scanners and parsers impose hard constraints on the
way lexical and syntactic ambiguities can be resolved. Furthermore, traditional
grammar-based parsing tools are limited in the mechanisms they allow for taking
context into account. In this paper, we propose a model-driven tool that allows
for statistical language models with arbitrary probability estimators. Our work
on model-driven probabilistic parsing is built on top of ModelCC, a model-based
parser generator, and enables the probabilistic interpretation and resolution
of anaphoric, cataphoric, and recursive references in the disambiguation of
abstract syntax graphs. In order to prove the expression power of ModelCC, we
describe the design of a general-purpose natural language parser."
"This work consists of creating a system of the Computer Assisted Language
Learning (CALL) based on a system of Automatic Speech Recognition (ASR) for the
Arabic language using the tool CMU Sphinx3 [1], based on the approach of HMM.
To this work, we have constructed a corpus of six hours of speech recordings
with a number of nine speakers. we find in the robustness to noise a grounds
for the choice of the HMM approach [2]. the results achieved are encouraging
since our corpus is made by only nine speakers, but they are always reasons
that open the door for other improvement works."
"While the use of cluster features became ubiquitous in core NLP tasks, most
cluster features in NLP are based on distributional similarity. We propose a
new type of clustering criteria, specific to the task of part-of-speech
tagging. Instead of distributional similarity, these clusters are based on the
beha vior of a baseline tagger when applied to a large corpus. These cluster
features provide similar gains in accuracy to those achieved by
distributional-similarity derived clusters. Using both types of cluster
features together further improve tagging accuracies. We show that the method
is effective for both the in-domain and out-of-domain scenarios for English,
and for French, German and Italian. The effect is larger for out-of-domain
text."
"We introduce precision-biased parsing: a parsing task which favors precision
over recall by allowing the parser to abstain from decisions deemed uncertain.
We focus on dependency-parsing and present an ensemble method which is capable
of assigning parents to 84% of the text tokens while being over 96% accurate on
these tokens. We use the precision-biased parsing task to solve the related
high-quality parse-selection task: finding a subset of high-quality (accurate)
trees in a large collection of parsed text. We present a method for choosing
over a third of the input trees while keeping unlabeled dependency parsing
accuracy of 97% on these trees. We also present a method which is not based on
an ensemble but rather on directly predicting the risk associated with
individual parser decisions. In addition to its efficiency, this method
demonstrates that a parsing system can provide reasonable estimates of
confidence in its predictions without relying on ensembles or aggregate corpus
counts."
"Lexical substitutes have found use in areas such as paraphrasing, text
simplification, machine translation, word sense disambiguation, and part of
speech induction. However the computational complexity of accurately
identifying the most likely substitutes for a word has made large scale
experiments difficult. In this paper I introduce a new search algorithm,
FASTSUBS, that is guaranteed to find the K most likely lexical substitutes for
a given word in a sentence based on an n-gram language model. The computation
is sub-linear in both K and the vocabulary size V. An implementation of the
algorithm and a dataset with the top 100 substitutes of each token in the WSJ
section of the Penn Treebank are available at http://goo.gl/jzKH0."
"The study of the Tip of the Tongue phenomenon (TOT) provides valuable clues
and insights concerning the organisation of the mental lexicon (meaning, number
of syllables, relation with other words, etc.). This paper describes a tool
based on psycho-linguistic observations concerning the TOT phenomenon. We've
built it to enable a speaker/writer to find the word he is looking for, word he
may know, but which he is unable to access in time. We try to simulate the TOT
phenomenon by creating a situation where the system knows the target word, yet
is unable to access it. In order to find the target word we make use of the
paradigmatic and syntagmatic associations stored in the linguistic databases.
Our experiment allows the following conclusion: a tool like SVETLAN, capable to
structure (automatically) a dictionary by domains can be used sucessfully to
help the speaker/writer to find the word he is looking for, if it is combined
with a database rich in terms of paradigmatic links like EuroWordNet."
"This paper presents a novel approach to machine translation by combining the
state of art name entity translation scheme. Improper translation of name
entities lapse the quality of machine translated output. In this work, name
entities are transliterated by using statistical rule based approach. This
paper describes the translation and transliteration of name entities from
English to Punjabi. We have experimented on four types of name entities which
are: Proper names, Location names, Organization names and miscellaneous.
Various rules for the purpose of syllabification have been constructed.
Transliteration of name entities is accomplished with the help of Probability
calculation. N-Gram probabilities for the extracted syllables have been
calculated using statistical machine translation toolkit MOSES."
"Part-of-speech (POS) tagging is a process of assigning the words in a text
corresponding to a particular part of speech. A fundamental version of POS
tagging is the identification of words as nouns, verbs, adjectives etc. For
processing natural languages, Part of Speech tagging is a prominent tool. It is
one of the simplest as well as most constant and statistical model for many NLP
applications. POS Tagging is an initial stage of linguistics, text analysis
like information retrieval, machine translator, text to speech synthesis,
information extraction etc. In POS Tagging we assign a Part of Speech tag to
each word in a sentence and literature. Various approaches have been proposed
to implement POS taggers. In this paper we present a Marathi part of speech
tagger. It is morphologically rich language. Marathi is spoken by the native
people of Maharashtra. The general approach used for development of tagger is
statistical using Unigram, Bigram, Trigram and HMM Methods. It presents a clear
idea about all the algorithms with suitable examples. It also introduces a tag
set for Marathi which can be used for tagging Marathi text. In this paper we
have shown the development of the tagger as well as compared to check the
accuracy of taggers output. The three Marathi POS taggers viz. Unigram, Bigram,
Trigram and HMM gives the accuracy of 77.38%, 90.30%, 91.46% and 93.82%
respectively."
"Machine translation is research based area where evaluation is very important
phenomenon for checking the quality of MT output. The work is based on the
evaluation of English to Urdu Machine translation. In this research work we
have evaluated the translation quality of Urdu language which has been
translated by using different Machine Translation systems like Google, Babylon
and Ijunoon. The evaluation process is done by using two approaches - Human
evaluation and Automatic evaluation. We have worked for both the approaches
where in human evaluation emphasis is given to scales and parameters while in
automatic evaluation emphasis is given to some automatic metric such as BLEU,
GTM, METEOR and ATEC."
"Urdu is a combination of several languages like Arabic, Hindi, English,
Turkish, Sanskrit etc. It has a complex and rich morphology. This is the reason
why not much work has been done in Urdu language processing. Stemming is used
to convert a word into its respective root form. In stemming, we separate the
suffix and prefix from the word. It is useful in search engines, natural
language processing and word processing, spell checkers, word parsing, word
frequency and count studies. This paper presents a rule based stemmer for Urdu.
The stemmer that we have discussed here is used in information retrieval. We
have also evaluated our results by verifying it with a human expert."
"Stemming is the process of extracting root word from the given inflection
word and also plays significant role in numerous application of Natural
Language Processing (NLP). Tamil Language raises several challenges to NLP,
since it has rich morphological patterns than other languages. The rule based
approach light-stemmer is proposed in this paper, to find stem word for given
inflection Tamil word. The performance of proposed approach is compared to a
rule based suffix removal stemmer based on correctly and incorrectly predicted.
The experimental result clearly show that the proposed approach light stemmer
for Tamil language perform better than suffix removal stemmer and also more
effective in Information Retrieval System (IRS)."
"Semantic measures are widely used today to estimate the strength of the
semantic relationship between elements of various types: units of language
(e.g., words, sentences, documents), concepts or even instances semantically
characterized (e.g., diseases, genes, geographical locations). Semantic
measures play an important role to compare such elements according to semantic
proxies: texts and knowledge representations, which support their meaning or
describe their nature. Semantic measures are therefore essential for designing
intelligent agents which will for example take advantage of semantic analysis
to mimic human ability to compare abstract or concrete objects. This paper
proposes a comprehensive survey of the broad notion of semantic measure for the
comparison of units of language, concepts or instances based on semantic proxy
analyses. Semantic measures generalize the well-known notions of semantic
similarity, semantic relatedness and semantic distance, which have been
extensively studied by various communities over the last decades (e.g.,
Cognitive Sciences, Linguistics, and Artificial Intelligence to mention a few)."
"Word Sense Disambiguation (WSD), the process of automatically identifying the
meaning of a polysemous word in a sentence, is a fundamental task in Natural
Language Processing (NLP). Progress in this approach to WSD opens up many
promising developments in the field of NLP and its applications. Indeed,
improvement over current performance levels could allow us to take a first step
towards natural language understanding. Due to the lack of lexical resources it
is sometimes difficult to perform WSD for under-resourced languages. This paper
is an investigation on how to initiate research in WSD for under-resourced
languages by applying Word Sense Induction (WSI) and suggests some interesting
topics to focus on."
"This paper discusses the dominancy of local features (LFs), as input to the
multilayer neural network (MLN), extracted from a Bangla input speech over mel
frequency cepstral coefficients (MFCCs). Here, LF-based method comprises three
stages: (i) LF extraction from input speech, (ii) phoneme probabilities
extraction using MLN from LF and (iii) the hidden Markov model (HMM) based
classifier to obtain more accurate phoneme strings. In the experiments on
Bangla speech corpus prepared by us, it is observed that the LFbased automatic
speech recognition (ASR) system provides higher phoneme correct rate than the
MFCC-based system. Moreover, the proposed system requires fewer mixture
components in the HMMs."
"Active languages such as Bangla (or Bengali) evolve over time due to a
variety of social, cultural, economic, and political issues. In this paper, we
analyze the change in the written form of the modern phase of Bangla
quantitatively in terms of character-level, syllable-level, morpheme-level and
word-level features. We collect three different types of corpora---classical,
newspapers and blogs---and test whether the differences in their features are
statistically significant. Results suggest that there are significant changes
in the length of a word when measured in terms of characters, but there is not
much difference in usage of different characters, syllables and morphemes in a
word or of different words in a sentence. To the best of our knowledge, this is
the first work on Bangla of this kind."
"We begin by introducing the Computer Science branch of Natural Language
Processing, then narrowing the attention on its subbranch of Information
Extraction and particularly on Named Entity Recognition, discussing briefly its
main methodological approaches. It follows an introduction to state-of-the-art
Conditional Random Fields under the form of linear chains. Subsequently, the
idea of constrained inference as a way to model long-distance relationships in
a text is presented, based on an Integer Linear Programming representation of
the problem. Adding such relationships to the problem as automatically inferred
logical formulas, translatable into linear conditions, we propose to solve the
resulting more complex problem with the aid of Lagrangian relaxation, of which
some technical details are explained. Lastly, we give some experimental
results."
"ARKref is a tool for noun phrase coreference. It is a deterministic,
rule-based system that uses syntactic information from a constituent parser,
and semantic information from an entity recognition component. Its architecture
is based on the work of Haghighi and Klein (2009). ARKref was originally
written in 2009. At the time of writing, the last released version was in March
2011. This document describes that version, which is open-source and publicly
available at: http://www.ark.cs.cmu.edu/ARKref"
"In recent years, semantic similarity measure has a great interest in Semantic
Web and Natural Language Processing (NLP). Several similarity measures have
been developed, being given the existence of a structured knowledge
representation offered by ontologies and corpus which enable semantic
interpretation of terms. Semantic similarity measures compute the similarity
between concepts/terms included in knowledge sources in order to perform
estimations. This paper discusses the existing semantic similarity methods
based on structure, information content and feature approaches. Additionally,
we present a critical evaluation of several categories of semantic similarity
approaches based on two standard benchmarks. The aim of this paper is to give
an efficient evaluation of all these measures which help researcher and
practitioners to select the measure that best fit for their requirements."
"Evaluation plays a crucial role in development of Machine translation
systems. In order to judge the quality of an existing MT system i.e. if the
translated output is of human translation quality or not, various automatic
metrics exist. We here present the implementation results of different metrics
when used on Hindi language along with their comparisons, illustrating how
effective are these metrics on languages like Hindi (free word order language)."
"This article reports the evaluation of the integration of data from a
syntactic-semantic lexicon, the Lexicon-Grammar of French, into a syntactic
parser. We show that by changing the set of labels for verbs and predicational
nouns, we can improve the performance on French of a non-lexicalized
probabilistic parser."
"We present the creation of an English-Swedish FrameNet-based grammar in
Grammatical Framework. The aim of this research is to make existing framenets
computationally accessible for multilingual natural language applications via a
common semantic grammar API, and to facilitate the porting of such grammar to
other languages. In this paper, we describe the abstract syntax of the semantic
grammar while focusing on its automatic extraction possibilities. We have
extracted a shared abstract syntax from ~58,500 annotated sentences in Berkeley
FrameNet (BFN) and ~3,500 annotated sentences in Swedish FrameNet (SweFN). The
abstract syntax defines 769 frame-specific valence patterns that cover 77.8%
examples in BFN and 74.9% in SweFN belonging to the shared set of 471 frames.
As a side result, we provide a unified method for comparing semantic and
syntactic valence patterns across framenets."
"The ability to accurately represent sentences is central to language
understanding. We describe a convolutional architecture dubbed the Dynamic
Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of
sentences. The network uses Dynamic k-Max Pooling, a global pooling operation
over linear sequences. The network handles input sentences of varying length
and induces a feature graph over the sentence that is capable of explicitly
capturing short and long-range relations. The network does not rely on a parse
tree and is easily applicable to any language. We test the DCNN in four
experiments: small scale binary and multi-class sentiment prediction, six-way
question classification and Twitter sentiment prediction by distant
supervision. The network achieves excellent performance in the first three
tasks and a greater than 25% error reduction in the last task with respect to
the strongest baseline."
"Stemming is a pre-processing step in Text Mining applications as well as a
very common requirement of Natural Language processing functions. Stemming is
the process for reducing inflected words to their stem. The main purpose of
stemming is to reduce different grammatical forms / word forms of a word like
its noun, adjective, verb, adverb etc. to its root form. Stemming is widely
uses in Information Retrieval system and reduces the size of index files. We
can say that the goal of stemming is to reduce inflectional forms and sometimes
derivationally related forms of a word to a common base form. In this paper we
have discussed different stemming algorithm for non-Indian and Indian language,
methods of stemming, accuracy and errors."
"We introduce a novel approach for building language models based on a
systematic, recursive exploration of skip n-gram models which are interpolated
using modified Kneser-Ney smoothing. Our approach generalizes language models
as it contains the classical interpolation with lower order models as a special
case. In this paper we motivate, formalize and present our approach. In an
extensive empirical experiment over English text corpora we demonstrate that
our generalized language models lead to a substantial reduction of perplexity
between 3.1% and 12.7% in comparison to traditional language models using
modified Kneser-Ney smoothing. Furthermore, we investigate the behaviour over
three other languages and a domain specific corpus where we observed consistent
improvements. Finally, we also show that the strength of our approach lies in
its ability to cope in particular with sparse training data. Using a very small
training data set of only 736 KB text we yield improvements of even 25.7%
reduction of perplexity."
"Metrics for measuring the comparability of corpora or texts need to be
developed and evaluated systematically. Applications based on a corpus, such as
training Statistical MT systems in specialised narrow domains, require finding
a reasonable balance between the size of the corpus and its consistency, with
controlled and benchmarked levels of comparability for any newly added
sections. In this article we propose a method that can meta-evaluate
comparability metrics by calculating monolingual comparability scores
separately on the 'source' and 'target' sides of parallel corpora. The range of
scores on the source side is then correlated (using Pearson's r coefficient)
with the range of 'target' scores; the higher the correlation - the more
reliable is the metric. The intuition is that a good metric should yield the
same distance between different domains in different languages. Our method
gives consistent results for the same metrics on different data sets, which
indicates that it is reliable and can be used for metric comparison or for
optimising settings of parametrised metrics."
"Evaluation plays a vital role in checking the quality of MT output. It is
done either manually or automatically. Manual evaluation is very time consuming
and subjective, hence use of automatic metrics is done most of the times. This
paper evaluates the translation quality of different MT Engines for
Hindi-English (Hindi data is provided as input and English is obtained as
output) using various automatic metrics like BLEU, METEOR etc. Further the
comparison automatic evaluation results with Human ranking have also been
given."
"Stanford typed dependencies are a widely desired representation of natural
language sentences, but parsing is one of the major computational bottlenecks
in text analysis systems. In light of the evolving definition of the Stanford
dependencies and developments in statistical dependency parsing algorithms,
this paper revisits the question of Cer et al. (2010): what is the tradeoff
between accuracy and speed in obtaining Stanford dependencies in particular? We
also explore the effects of input representations on this tradeoff:
part-of-speech tags, the novel use of an alternative dependency representation
as input, and distributional representaions of words. We find that direct
dependency parsing is a more viable solution than it was found to be in the
past. An accompanying software release can be found at:
http://www.ark.cs.cmu.edu/TBSD"
"In this article, we have introduced the first parallel corpus of Persian with
more than 10 other European languages. This article describes primary steps
toward preparing a Basic Language Resources Kit (BLARK) for Persian. Up to now,
we have proposed morphosyntactic specification of Persian based on
EAGLE/MULTEXT guidelines and specific resources of MULTEXT-East. The article
introduces Persian Language, with emphasis on its orthography and
morphosyntactic features, then a new Part-of-Speech categorization and
orthography for Persian in digital environments is proposed. Finally, the
corpus and related statistic will be analyzed."
"We present a novel technique for learning semantic representations, which
extends the distributional hypothesis to multilingual data and joint-space
embeddings. Our models leverage parallel data and learn to strongly align the
embeddings of semantically equivalent sentences, while maintaining sufficient
distance between those of dissimilar sentences. The models do not rely on word
alignments or any syntactic information and are successfully applied to a
number of diverse languages. We extend our approach to learn semantic
representations at the document level, too. We evaluate these models on two
cross-lingual document classification tasks, outperforming the prior state of
the art. Through qualitative analysis and the study of pivoting effects we
demonstrate that our representations are semantically plausible and can capture
semantic relationships across languages without parallel data."
"We present a method to leverage radical for learning Chinese character
embedding. Radical is a semantic and phonetic component of Chinese character.
It plays an important role as characters with the same radical usually have
similar semantic meaning and grammatical usage. However, existing Chinese
processing algorithms typically regard word or character as the basic unit but
ignore the crucial radical information. In this paper, we fill this gap by
leveraging radical for learning continuous representation of Chinese character.
We develop a dedicated neural architecture to effectively learn character
embedding and apply it on Chinese character similarity judgement and Chinese
word segmentation. Experiment results show that our radical-enhanced method
outperforms existing embedding learning algorithms on both tasks."
"Farsi, also known as Persian, is the official language of Iran and Tajikistan
and one of the two main languages spoken in Afghanistan. Farsi enjoys a unified
Arabic script as its writing system. In this paper we briefly introduce the
writing standards of Farsi and highlight problems one would face when analyzing
Farsi electronic texts, especially during development of Farsi corpora
regarding to transcription and encoding of Farsi e-texts. The pointes mentioned
may sounds easy but they are crucial when developing and processing written
corpora of Farsi."
"This paper develops a compositional vector-based semantics of subject and
object relative pronouns within a categorical framework. Frobenius algebras are
used to formalise the operations required to model the semantics of relative
pronouns, including passing information between the relative clause and the
modified noun phrase, as well as copying, combining, and discarding parts of
the relative clause. We develop two instantiations of the abstract semantics,
one based on a truth-theoretic approach and one based on corpus statistics."
"In this work we present a morphological analysis of Bishnupriya Manipuri
language, an Indo-Aryan language spoken in the north eastern India. As of now,
there is no computational work available for the language. Finite state
morphology is one of the successful approaches applied in a wide variety of
languages over the year. Therefore we adapted the finite state approach to
analyse morphology of the Bishnupriya Manipuri language."
"Most state-of-the-art approaches for named-entity recognition (NER) use semi
supervised information in the form of word clusters and lexicons. Recently
neural network-based language models have been explored, as they as a byproduct
generate highly informative vector representations for words, known as word
embeddings. In this paper we present two contributions: a new form of learning
word embeddings that can leverage information from relevant lexicons to improve
the representations, and the first system to use neural word embeddings to
achieve state-of-the-art results on named-entity recognition in both CoNLL and
Ontonotes NER. Our system achieves an F1 score of 90.90 on the test set for
CoNLL 2003---significantly better than any previous system trained on public
data, and matching a system employing massive private industrial query-log
data."
"Linguists and psychologists have long been studying cross-linguistic
transfer, the influence of native language properties on linguistic performance
in a foreign language. In this work we provide empirical evidence for this
process in the form of a strong correlation between language similarities
derived from structural features in English as Second Language (ESL) texts and
equivalent similarities obtained from the typological features of the native
languages. We leverage this finding to recover native language typological
similarity structure directly from ESL text, and perform prediction of
typological features in an unsupervised fashion with respect to the target
languages. Our method achieves 72.2% accuracy on the typology prediction task,
a result that is highly competitive with equivalent methods that rely on
typological resources."
"Many successful approaches to semantic parsing build on top of the syntactic
analysis of text, and make use of distributional representations or statistical
models to match parses to ontology-specific queries. This paper presents a
novel deep learning architecture which provides a semantic parsing system
through the union of two neural models of language semantics. It allows for the
generation of ontology-specific queries from natural language statements and
questions without the need for parsing, which makes it especially suitable to
grammatically malformed or syntactically atypical text, such as tweets, as well
as permitting the development of semantic parsers for resource-poor languages."
"In this empirical study, I compare various tree distance measures --
originally developed in computational biology for the purpose of tree
comparison -- for the purpose of parser evaluation. I will control for the
parser setting by comparing the automatically generated parse trees from the
state-of-the-art parser Charniak, 2000) with the gold-standard parse trees. The
article describes two different tree distance measures (RF and QD) along with
its variants (GRF and GQD) for the purpose of parser evaluation. The article
will argue that RF measure captures similar information as the standard EvalB
metric (Sekine and Collins, 1997) and the tree edit distance (Zhang and Shasha,
1989) applied by Tsarfaty et al. (2011). Finally, the article also provides
empirical evidence by reporting high correlations between the different tree
distances and EvalB metric's scores."
"This report describes an NLP assistant for the collaborative development
environment Clide, that supports the development of NLP applications by
providing easy access to some common NLP data structures. The assistant
visualizes text fragments and their dependencies by displaying the semantic
graph of a sentence, the coreference chain of a paragraph and mined triples
that are extracted from a paragraph's semantic graphs and linked using its
coreference chain. Using this information and a logic programming library, we
create an NLP database which is used by a series of queries to mine the
triples. The algorithm is tested by translating a natural language text
describing a graph to an actual graph that is shown as an annotation in the
text editor."
"Automatic Multi-Word Term (MWT) extraction is a very important issue to many
applications, such as information retrieval, question answering, and text
categorization. Although many methods have been used for MWT extraction in
English and other European languages, few studies have been applied to Arabic.
In this paper, we propose a novel, hybrid method which combines linguistic and
statistical approaches for Arabic Multi-Word Term extraction. The main
contribution of our method is to consider contextual information and both
termhood and unithood for association measures at the statistical filtering
step. In addition, our technique takes into account the problem of MWT
variation in the linguistic filtering step. The performance of the proposed
statistical measure (NLC-value) is evaluated using an Arabic environment corpus
by comparing it with some existing competitors. Experimental results show that
our NLC-value measure outperforms the other ones in term of precision for both
bi-grams and tri-grams."
"This paper presents a new model of WordNet that is used to disambiguate the
correct sense of polysemy word based on the clue words. The related words for
each sense of a polysemy word as well as single sense word are referred to as
the clue words. The conventional WordNet organizes nouns, verbs, adjectives and
adverbs together into sets of synonyms called synsets each expressing a
different concept. In contrast to the structure of WordNet, we developed a new
model of WordNet that organizes the different senses of polysemy words as well
as the single sense words based on the clue words. These clue words for each
sense of a polysemy word as well as for single sense word are used to
disambiguate the correct meaning of the polysemy word in the given context
using knowledge based Word Sense Disambiguation (WSD) algorithms. The clue word
can be a noun, verb, adjective or adverb."
"This paper describes adaptations for EaFi, a parser for easy-first parsing of
discontinuous constituents, to adapt it to multiple languages as well as make
use of the unlabeled data that was provided as part of the SPMRL shared task
2014."
"The rendering of Sanskrit poetry from text to speech is a problem that has
not been solved before. One reason may be the complications in the language
itself. We present unique algorithms based on extensive empirical analysis, to
synthesize speech from a given text input of Sanskrit verses. Using a
pre-recorded audio units database which is itself tremendously reduced in size
compared to the colossal size that would otherwise be required, the algorithms
work on producing the best possible, tunefully rendered chanting of the given
verse. His would enable the visually impaired and those with reading
disabilities to easily access the contents of Sanskrit verses otherwise
available only in writing."
"Comprehensively searching for words in Sanskrit E-text is a non-trivial
problem because words could change their forms in different contexts. One such
context is sandhi or euphonic conjunctions, which cause a word to change owing
to the presence of adjacent letters or words. The change wrought by these
possible conjunctions can be so significant in Sanskrit that a simple search
for the word in its given form alone can significantly reduce the success level
of the search. This work presents a representational schema that represents
letters in a binary format and reduces Paninian rules of euphonic conjunctions
to simple bit set-unset operations. The work presents an efficient algorithm to
process vowel-based sandhis using this schema. It further presents another
algorithm that uses the sandhi processor to generate the possible transformed
word forms of a given word to use in a comprehensive word search."
"Searching for words in Sanskrit E-text is a problem that is accompanied by
complexities introduced by features of Sanskrit such as euphonic conjunctions
or sandhis. A word could occur in an E-text in a transformed form owing to the
operation of rules of sandhi. Simple word search would not yield these
transformed forms of the word. Further, there is no search engine in the
literature that can comprehensively search for words in Sanskrit E-texts taking
euphonic conjunctions into account. This work presents an optimal binary
representational schema for letters of the Sanskrit alphabet along with
algorithms to efficiently process the sandhi rules of Sanskrit grammar. The
work further presents an algorithm that uses the sandhi processing algorithm to
perform a comprehensive word search on E-text."
"Twitter with over 500 million users globally, generates over 100,000 tweets
per minute . The 140 character limit per tweet, perhaps unintentionally,
encourages users to use shorthand notations and to strip spellings to their
bare minimum ""syllables"" or elisions e.g. ""srsly"". The analysis of twitter
messages which typically contain misspellings, elisions, and grammatical
errors, poses a challenge to established Natural Language Processing (NLP)
tools which are generally designed with the assumption that the data conforms
to the basic grammatical structure commonly used in English language. In order
to make sense of Twitter messages it is necessary to first transform them into
a canonical form, consistent with the dictionary or grammar. This process,
performed at the level of individual tokens (""words""), is called lexical
normalisation. This paper investigates various techniques for lexical
normalisation of Twitter data and presents the findings as the techniques are
applied to process raw data from Twitter."
"A crowdsourcing translation approach is an effective tool for globalization
of site content, but it is also an important source of parallel linguistic
data. For the given site, processed with a crowdsourcing system, a
sentence-aligned corpus can be fetched, which covers a very narrow domain of
terminology and language patterns - a site-specific domain. These data can be
used for training and estimation of site-specific statistical machine
translation engine"
"In this paper, the performance of two dependency parsers, namely Stanford and
Minipar, on biomedical texts has been reported. The performance of te parsers
to assignm dependencies between two biomedical concepts that are already proved
to be connected is not satisfying. Both Stanford and Minipar, being statistical
parsers, fail to assign dependency relation between two connected concepts if
they are distant by at least one clause. Minipar's performance, in terms of
precision, recall and the F-score of the attachment score (e.g., correctly
identified head in a dependency), to parse biomedical text is also measured
taking the Stanford's as a gold standard. The results suggest that Minipar is
not suitable yet to parse biomedical texts. In addition, a qualitative
investigation reveals that the difference between working principles of the
parsers also play a vital role for Minipar's degraded performance."
"Contemporary research on computational processing of linguistic metaphors is
divided into two main branches: metaphor recognition and metaphor
interpretation. We take a different line of research and present an automated
method for generating conceptual metaphors from linguistic data. Given the
generated conceptual metaphors, we find corresponding linguistic metaphors in
corpora. In this paper, we describe our approach and its evaluation using
English and Russian data."
"This paper describes performance of CRF based systems for Named Entity
Recognition (NER) in Indian language as a part of ICON 2013 shared task. In
this task we have considered a set of language independent features for all the
languages. Only for English a language specific feature, i.e. capitalization,
has been added. Next the use of gazetteer is explored for Bengali, Hindi and
English. The gazetteers are built from Wikipedia and other sources. Test
results show that the system achieves the highest F measure of 88% for English
and the lowest F measure of 69% for both Tamil and Telugu. Note that for the
least performing two languages no gazetteer was used. NER in Bengali and Hindi
finds accuracy (F measure) of 87% and 79%, respectively."
"Machine Translation is one of the major oldest and the most active research
area in Natural Language Processing. Currently, Statistical Machine Translation
(SMT) dominates the Machine Translation research. Statistical Machine
Translation is an approach to Machine Translation which uses models to learn
translation patterns directly from data, and generalize them to translate a new
unseen text. The SMT approach is largely language independent, i.e. the models
can be applied to any language pair. Statistical Machine Translation (SMT)
attempts to generate translations using statistical methods based on bilingual
text corpora. Where such corpora are available, excellent results can be
attained translating similar texts, but such corpora are still not available
for many language pairs. Statistical Machine Translation systems, in general,
have difficulty in handling the morphology on the source or the target side
especially for morphologically rich languages. Errors in morphology or syntax
in the target language can have severe consequences on meaning of the sentence.
They change the grammatical function of words or the understanding of the
sentence through the incorrect tense information in verb. Baseline SMT also
known as Phrase Based Statistical Machine Translation (PBSMT) system does not
use any linguistic information and it only operates on surface word form.
Recent researches shown that adding linguistic information helps to improve the
accuracy of the translation with less amount of bilingual corpora. Adding
linguistic information can be done using the Factored Statistical Machine
Translation system through pre-processing steps. This paper investigates about
how English side pre-processing is used to improve the accuracy of
English-Tamil SMT system."
"Because of the wide variety of contemporary practices used in the automatic
syntactic parsing of natural languages, it has become necessary to analyze and
evaluate the strengths and weaknesses of different approaches. This research is
all the more necessary because there are currently no genre- and
domain-independent parsers that are able to analyze unrestricted text with 100%
preciseness (I use this term to refer to the correctness of analyses assigned
by a parser). All these factors create a need for methods and resources that
can be used to evaluate and compare parsing systems. This research describes:
(1) A theoretical analysis of current achievements in parsing and parser
evaluation. (2) A framework (called FEPa) that can be used to carry out
practical parser evaluations and comparisons. (3) A set of new evaluation
resources: FiEval is a Finnish treebank under construction, and MGTS and RobSet
are parser evaluation resources in English. (4) The results of experiments in
which the developed evaluation framework and the two resources for English were
used for evaluating a set of selected parsers."
"The ancient and extinct language Meroitic is investigated using Zipf's Law.
In particular, since Meroitic is still undeciphered, the Zipf law analysis
allows us to assess the quality of current texts and possible avenues for
future investigation using statistical techniques."
"Julian Jaynes's profound humanitarian convictions not only prevented him from
going to war, but would have prevented him from ever kicking a dog. Yet
according to his theory, not only are language-less dogs unconscious, but so
too were the speaking/hearing Greeks in the Bicameral Era, when they heard
gods' voices telling them what to do rather than thinking for themselves. I
argue that to be conscious is to be able to feel, and that all mammals (and
probably lower vertebrates and invertebrates too) feel, hence are conscious.
Julian Jaynes's brilliant analysis of our concepts of consciousness
nevertheless keeps inspiring ever more inquiry and insights into the age-old
mind/body problem and its relation to cognition and language."
"Meroitic is the still undeciphered language of the ancient civilization of
Kush. Over the years, various techniques for decipherment such as finding a
bilingual text or cognates from modern or other ancient languages in the Sudan
and surrounding areas has not been successful. Using techniques borrowed from
information theory and natural language statistics, similar words are paired
and attempts are made to use currently defined words to extract at least
partial meaning from unknown words."
"Multilingual parallel texts (abbreviated to parallel texts) are linguistic
versions of the same content (""translations""); e.g., the Maastricht Treaty in
English and Spanish are parallel texts. This document is about creating an open
architecture for the whole Authoring, Translation and Publishing Chain
(ATP-chain) for the processing of parallel texts."
"Formal work in linguistics has both produced and used important mathematical
tools. Motivated by a survey of models for context and word meaning, syntactic
categories, phrase structure rules and trees, an attempt is being made in the
present paper to present a mathematical model for structuring of sentences from
active voice to passive voice, which is is the form of a transitive verb whose
grammatical subject serves as the patient, receiving the action of the verb.
  For this purpose we have parsed all sentences of a corpus and have generated
Boolean groups for each of them. It has been observed that when we take
constituents of the sentences as subgroups, the sequences of phrases form
permutation roups. Application of isomorphism property yields permutation
mapping between the important subgroups. It has resulted in a model for
transformation of sentences from active voice to passive voice. A computer
program has been written to enable the software developers to evolve grammar
software for sentence transformations."
"A standard form of analysis for linguistic typology is the universal
implication. These implications state facts about the range of extant
languages, such as ``if objects come after verbs, then adjectives come after
nouns.'' Such implications are typically discovered by painstaking hand
analysis over a small sample of languages. We propose a computational model for
assisting at this process. Our model is able to discover both well-known
implications as well as some novel implications that deserve further study.
Moreover, through a careful application of hierarchical analysis, we are able
to cope with the well-known sampling problem: languages are not independent."
"Current research in automatic single document summarization is dominated by
two effective, yet naive approaches: summarization by sentence extraction, and
headline generation via bag-of-words models. While successful in some tasks,
neither of these models is able to adequately capture the large set of
linguistic devices utilized by humans when they produce summaries. One possible
explanation for the widespread use of these models is that good techniques have
been developed to extract appropriate training data for them from existing
document/abstract and document/headline corpora. We believe that future
progress in automatic summarization will be driven both by the development of
more sophisticated, linguistically informed models, as well as a more effective
leveraging of document/abstract corpora. In order to open the doors to
simultaneously achieving both of these goals, we have developed techniques for
automatically producing word-to-word and phrase-to-phrase alignments between
documents and their human-written abstracts. These alignments make explicit the
correspondences that exist in such document/abstract pairs, and create a
potentially rich data source from which complex summarization algorithms may
learn. This paper describes experiments we have carried out to analyze the
ability of humans to perform such alignments, and based on these analyses, we
describe experiments for creating them automatically. Our model for the
alignment task is based on an extension of the standard hidden Markov model,
and learns to create alignments in a completely unsupervised fashion. We
describe our model in detail and present experimental results that show that
our model is able to learn to reliably identify word- and phrase-level
alignments in a corpus of <document,abstract> pairs."
"We present a document compression system that uses a hierarchical
noisy-channel model of text production. Our compression system first
automatically derives the syntactic structure of each sentence and the overall
discourse structure of the text given as input. The system then uses a
statistical hierarchical model of text production in order to drop
non-important syntactic and discourse constituents so as to generate coherent,
grammatical document compressions of arbitrary length. The system outperforms
both a baseline and a sentence-based compression system that operates by
simplifying sequentially all sentences in a text. Our results support the claim
that discourse knowledge plays an important role in document summarization."
"Entity detection and tracking (EDT) is the task of identifying textual
mentions of real-world entities in documents, extending the named entity
detection and coreference resolution task by considering mentions other than
names (pronouns, definite descriptions, etc.). Like NE tagging and coreference
resolution, most solutions to the EDT task separate out the mention detection
aspect from the coreference aspect. By doing so, these solutions are limited to
using only local features for learning. In contrast, by modeling both aspects
of the EDT task simultaneously, we are able to learn using highly complex,
non-local features. We develop a new joint EDT model and explore the utility of
many features, demonstrating their effectiveness on this task."
"In this paper, we propose a pattern-based term extraction approach for
Japanese, applying ACABIT system originally developed for French. The proposed
approach evaluates termhood using morphological patterns of basic terms and
term variants. After extracting term candidates, ACABIT system filters out
non-terms from the candidates based on log-likelihood. This approach is
suitable for Japanese term extraction because most of Japanese terms are
compound nouns or simple phrasal patterns."
"We present a method of automatic translation (French/English) of Complex
Lexical Units (CLU) for aiming at extracting a bilingual lexicon. Our modular
system is based on linguistic properties (compositionality, polysemy, etc.).
Different aspects of the multilingual Web are used to validate candidate
translations and collect new terms. We first build a French corpus of Web pages
to collect CLU. Three adapted processing stages are applied for each linguistic
property : compositional and non polysemous translations, compositional
polysemous translations and non compositional translations. Our evaluation on a
sample of CLU shows that our technique based on the Web can reach a very high
precision."
"Both syntax-phonology and syntax-semantics interfaces in Higher Order Grammar
(HOG) are expressed as axiomatic theories in higher-order logic (HOL), i.e. a
language is defined entirely in terms of provability in the single logical
system. An important implication of this elegant architecture is that the
meaning of a valid expression turns out to be represented not by a single, nor
even by a few ""discrete"" terms (in case of ambiguity), but by a ""continuous""
set of logically equivalent terms. The note is devoted to precise formulation
and proof of this observation."
"Proofs, in Ludics, have an interpretation provided by their counter-proofs,
that is the objects they interact with. We follow the same idea by proposing
that sentence meanings are given by the counter-meanings they are opposed to in
a dialectical interaction. The conception is at the intersection of a
proof-theoretic and a game-theoretic accounts of semantics, but it enlarges
them by allowing to deal with possibly infinite processes."
"Machine Translation in India is relatively young. The earliest efforts date
from the late 80s and early 90s. The success of every system is judged from its
evaluation experimental results. Number of machine translation systems has been
started for development but to the best of author knowledge, no high quality
system has been completed which can be used in real applications. Recently,
Punjabi University, Patiala, India has developed Punjabi to Hindi Machine
translation system with high accuracy of about 92%. Both the systems i.e.
system under question and developed system are between same closely related
languages. Thus, this paper presents the evaluation results of Hindi to Punjabi
machine translation system. It makes sense to use same evaluation criteria as
that of Punjabi to Hindi Punjabi Machine Translation System. After evaluation,
the accuracy of the system is found to be about 95%."
"This paper presents a brief survey on Automatic Speech Recognition and
discusses the major themes and advances made in the past 60 years of research,
so as to provide a technological perspective and an appreciation of the
fundamental progress that has been accomplished in this important area of
speech communication. After years of research and development the accuracy of
automatic speech recognition remains one of the important research challenges
(e.g., variations of the context, speakers, and environment).The design of
Speech Recognition system requires careful attentions to the following issues:
Definition of various types of speech classes, speech representation, feature
extraction techniques, speech classifiers, database and performance evaluation.
The problems that are existing in ASR and the various techniques to solve these
problems constructed by various research workers have been presented in a
chronological order. Hence authors hope that this work shall be a contribution
in the area of speech recognition. The objective of this review paper is to
summarize and compare some of the well known methods used in various stages of
speech recognition system and identify research topic and applications which
are at the forefront of this exciting and challenging field."
"Accurate systems for extracting Protein-Protein Interactions (PPIs)
automatically from biomedical articles can help accelerate biomedical research.
Biomedical Informatics researchers are collaborating to provide metaservices
and advance the state-of-art in PPI extraction. One problem often neglected by
current Natural Language Processing systems is the characteristic complexity of
the sentences in biomedical literature. In this paper, we report on the impact
that automatic simplification of sentences has on the performance of a
state-of-art PPI extraction system, showing a substantial improvement in recall
(8%) when the sentence simplification method is applied, without significant
impact to precision."
"The complexity of sentences characteristic to biomedical articles poses a
challenge to natural language parsers, which are typically trained on
large-scale corpora of non-technical text. We propose a text simplification
process, bioSimplify, that seeks to reduce the complexity of sentences in
biomedical abstracts in order to improve the performance of syntactic parsers
on the processed sentences. Syntactic parsing is typically one of the first
steps in a text mining pipeline. Thus, any improvement in performance would
have a ripple effect over all processing steps. We evaluated our method using a
corpus of biomedical sentences annotated with syntactic links. Our empirical
results show an improvement of 2.90% for the Charniak-McClosky parser and of
4.23% for the Link Grammar parser when processing simplified sentences rather
than the original sentences in the corpus."
"The Lambek calculus provides a foundation for categorial grammar in the form
of a logic of concatenation. But natural language is characterized by
dependencies which may also be discontinuous. In this paper we introduce the
displacement calculus, a generalization of Lambek calculus, which preserves its
good proof-theoretic properties while embracing discontinuiity and subsuming
it. We illustrate linguistic applications and prove Cut-elimination, the
subformula property, and decidability"
"This document presents Annotated English, a system of diacritical symbols
which turns English pronunciation into a precise and unambiguous process. The
annotations are defined and located in such a way that the original English
text is not altered (not even a letter), thus allowing for a consistent reading
and learning of the English language with and without annotations. The
annotations are based on a set of general rules that make the frequency of
annotations not dramatically high. This makes the reader easily associate
annotations with exceptions, and makes it possible to shape, internalise and
consolidate some rules for the English language which otherwise are weakened by
the enormous amount of exceptions in English pronunciation. The advantages of
this annotation system are manifold. Any existing text can be annotated without
a significant increase in size. This means that we can get an annotated version
of any document or book with the same number of pages and fontsize. Since no
letter is affected, the text can be perfectly read by a person who does not
know the annotation rules, since annotations can be simply ignored. The
annotations are based on a set of rules which can be progressively learned and
recognised, even in cases where the reader has no access or time to read the
rules. This means that a reader can understand most of the annotations after
reading a few pages of Annotated English, and can take advantage from that
knowledge for any other annotated document she may read in the future."
"We address the problem of inferring a speaker's level of certainty based on
prosodic information in the speech signal, which has application in
speech-based dialogue systems. We show that using phrase-level prosodic
features centered around the phrases causing uncertainty, in addition to
utterance-level prosodic features, improves our model's level of certainty
classification. In addition, our models can be used to predict which phrase a
person is uncertain about. These results rely on a novel method for eliciting
utterances of varying levels of certainty that allows us to compare the utility
of contextually-based feature sets. We elicit level of certainty ratings from
both the speakers themselves and a panel of listeners, finding that there is
often a mismatch between speakers' internal states and their perceived states,
and highlighting the importance of this distinction."
"The limited range in its abscissa of ranked letter frequency distributions
causes multiple functions to fit the observed distribution reasonably well. In
order to critically compare various functions, we apply the statistical model
selections on ten functions, using the texts of U.S. and Mexican presidential
speeches in the last 1-2 centuries. Dispite minor switching of ranking order of
certain letters during the temporal evolution for both datasets, the letter
usage is generally stable. The best fitting function, judged by either
least-square-error or by AIC/BIC model selection, is the Cocho/Beta function.
We also use a novel method to discover clusters of letters by their
observed-over-expected frequency ratios."
"Existing grammar frameworks do not work out particularly well for controlled
natural languages (CNL), especially if they are to be used in predictive
editors. I introduce in this paper a new grammar notation, called Codeco, which
is designed specifically for CNLs and predictive editors. Two different parsers
have been implemented and a large subset of Attempto Controlled English (ACE)
has been represented in Codeco. The results show that Codeco is practical,
adequate and efficient."
"This article overviews the current state of the English-Lithuanian-English
machine translation system. The first part of the article describes the
problems that system poses today and what actions will be taken to solve them
in the future. The second part of the article tackles the main issue of the
translation process. Article briefly overviews the word sense disambiguation
for MT technique using Google."
"The paper presents the design and development of English-Lithuanian-English
dictionarylexicon tool and lexicon database management system for MT. The
system is oriented to support two main requirements: to be open to the user and
to describe much more attributes of speech parts as a regular dictionary that
are required for the MT. Programming language Java and database management
system MySql is used to implement the designing tool and lexicon database
respectively. This solution allows easily deploying this system in the
Internet. The system is able to run on various OS such as: Windows, Linux, Mac
and other OS where Java Virtual Machine is supported. Since the modern lexicon
database managing system is used, it is not a problem accessing the same
database for several users."
"In natural speech, the speaker does not pause between words, yet a human
listener somehow perceives this continuous stream of phonemes as a series of
distinct words. The detection of boundaries between spoken words is an instance
of a general capability of the human neocortex to remember and to recognize
recurring sequences. This paper describes a computer algorithm that is designed
to solve the problem of locating word boundaries in blocks of English text from
which the spaces have been removed. This problem avoids the complexities of
speech processing but requires similar capabilities for detecting recurring
sequences. The algorithm relies entirely on statistical relationships between
letters in the input stream to infer the locations of word boundaries. A
Viterbi trellis is used to simultaneously evaluate a set of hypothetical
segmentations of a block of adjacent words. This technique improves accuracy
but incurs a small latency between the arrival of letters in the input stream
and the sending of words to the output stream. The source code for a C++
version of this algorithm is presented in an appendix."
"This article studies the emergence of ambiguity in communication through the
concept of logical irreversibility and within the framework of Shannon's
information theory. This leads us to a precise and general expression of the
intuition behind Zipf's vocabulary balance in terms of a symmetry equation
between the complexities of the coding and the decoding processes that imposes
an unavoidable amount of logical uncertainty in natural communication.
Accordingly, the emergence of irreversible computations is required if the
complexities of the coding and the decoding processes are balanced in a
symmetric scenario, which means that the emergence of ambiguous codes is a
necessary condition for natural communication to succeed."
"These notes are a continuation of topics covered by V. Selegej in his article
""Electronic Dictionaries and Computational lexicography"". How can an electronic
dictionary have as its object the description of closely related languages?
Obviously, such a question allows multiple answers."
"Model-based language specification has applications in the implementation of
language processors, the design of domain-specific languages, model-driven
software development, data integration, text mining, natural language
processing, and corpus-based induction of models. Model-based language
specification decouples language design from language processing and, unlike
traditional grammar-driven approaches, which constrain language designers to
specific kinds of grammars, it needs general parser generators able to deal
with ambiguities. In this paper, we propose Fence, an efficient bottom-up
parsing algorithm with lexical and syntactic ambiguity support that enables the
use of model-based language specification in practice."
"We describe a new semantic relatedness measure combining the Wikipedia-based
Explicit Semantic Analysis measure, the WordNet path measure and the mixed
collocation index. Our measure achieves the currently highest results on the
WS-353 test: a Spearman rho coefficient of 0.79 (vs. 0.75 in (Gabrilovich and
Markovitch, 2007)) when applying the measure directly, and a value of 0.87 (vs.
0.78 in (Agirre et al., 2009)) when using the prediction of a polynomial SVM
classifier trained on our measure.
  In the appendix we discuss the adaptation of ESA to 2011 Wikipedia data, as
well as various unsuccessful attempts to enhance ESA by filtering at word,
sentence, and section level."
"Diacritical marks play a crucial role in meeting the criteria of usability of
typographic text, such as: homogeneity, clarity and legibility. To change the
diacritic of a letter in a word could completely change its semantic. The
situation is very complicated with multilingual text. Indeed, the problem of
design becomes more difficult by the presence of diacritics that come from
various scripts; they are used for different purposes, and are controlled by
various typographic rules. It is quite challenging to adapt rules from one
script to another. This paper aims to study the placement and sizing of
diacritical marks in Arabic script, with a comparison with the Latin's case.
The Arabic script is cursive and runs from right-to-left; its criteria and
rules are quite distinct from those of the Latin script. In the beginning, we
compare the difficulty of processing diacritics in both scripts. After, we will
study the limits of Latin resolution strategies when applied to Arabic. At the
end, we propose an approach to resolve the problem for positioning and resizing
diacritics. This strategy includes creating an Arabic font, designed in
OpenType format, along with suitable justification in TEX."
"The interest in text to speech synthesis increased in the world .text to
speech have been developed formany popular languages such as English, Spanish
and French and many researches and developmentshave been applied to those
languages. Persian on the other hand, has been given little attentioncompared
to other languages of similar importance and the research in Persian is still
in its infancy.Persian language possess many difficulty and exceptions that
increase complexity of text to speechsystems. For example: short vowels is
absent in written text or existence of homograph words. in thispaper we propose
a new method for persian text to phonetic that base on pronunciations by
analogy inwords, semantic relations and grammatical rules for finding proper
phonetic. Keywords:PbA, text to speech, Persian language, FPbA"
"We propose NEMO, a system for extracting organization names in the
affiliation and normalizing them to a canonical organization name. Our parsing
process involves multi-layered rule matching with multiple dictionaries. The
system achieves more than 98% f-score in extracting organization names. Our
process of normalization that involves clustering based on local sequence
alignment metrics and local learning based on finding connected components. A
high precision was also observed in normalization. NEMO is the missing link in
associating each biomedical paper and its authors to an organization name in
its canonical form and the Geopolitical location of the organization. This
research could potentially help in analyzing large social networks of
organizations for landscaping a particular topic, improving performance of
author disambiguation, adding weak links in the co-author network of authors,
augmenting NLM's MARS system for correcting errors in OCR output of affiliation
field, and automatically indexing the PubMed citations with the normalized
organization name and country. Our system is available as a graphical user
interface available for download along with this paper."
"BioSimplify is an open source tool written in Java that introduces and
facilitates the use of a novel model for sentence simplification tuned for
automatic discourse analysis and information extraction (as opposed to sentence
simplification for improving human readability). The model is based on a
""shot-gun"" approach that produces many different (simpler) versions of the
original sentence by combining variants of its constituent elements. This tool
is optimized for processing biomedical scientific literature such as the
abstracts indexed in PubMed. We tested our tool on its impact to the task of
PPI extraction and it improved the f-score of the PPI tool by around 7%, with
an improvement in recall of around 20%. The BioSimplify tool and test corpus
can be downloaded from https://biosimplify.sourceforge.net."
"Overall, the two main contributions of this work include the application of
sentence simplification to association extraction as described above, and the
use of distributional semantics for concept extraction. The proposed work on
concept extraction amalgamates for the first time two diverse research areas
-distributional semantics and information extraction. This approach renders all
the advantages offered in other semi-supervised machine learning systems, and,
unlike other proposed semi-supervised approaches, it can be used on top of
different basic frameworks and algorithms.
http://gradworks.umi.com/34/49/3449837.html"
"One of the biggest challenges in the development and deployment of spoken
dialogue systems is the design of the spoken language generation module. This
challenge arises from the need for the generator to adapt to many features of
the dialogue domain, user population, and dialogue context. A promising
approach is trainable generation, which uses general-purpose linguistic
knowledge that is automatically adapted to the features of interest, such as
the application domain, individual user, or user group. In this paper we
present and evaluate a trainable sentence planner for providing restaurant
information in the MATCH dialogue system. We show that trainable sentence
planning can produce complex information presentations whose quality is
comparable to the output of a template-based generator tuned to this domain. We
also show that our method easily supports adapting the sentence planner to
individuals, and that the individualized sentence planners generally perform
better than models trained and tested on a population of individuals. Previous
work has documented and utilized individual preferences for content selection,
but to our knowledge, these results provide the first demonstration of
individual preferences for sentence planning operations, affecting the content
order, discourse structure and sentence structure of system responses. Finally,
we evaluate the contribution of different feature sets, and show that, in our
application, n-gram features often do as well as features based on higher-level
linguistic representations."
"This paper presents the preliminary works to put online a French oral corpus
and its transcription. This corpus is the Socio-Linguistic Survey in Orleans,
realized in 1968. First, we numerized the corpus, then we handwritten
transcribed it with the Transcriber software adding different tags about
speakers, time, noise, etc. Each document (audio file and XML file of the
transcription) was described by a set of metadata stored in an XML format to
allow an easy consultation. Second, we added different levels of annotations,
recognition of named entities and annotation of personal information about
speakers. This two annotation tasks used the CasSys system of transducer
cascades. We used and modified a first cascade to recognize named entities.
Then we built a second cascade to annote the designating entities, i.e.
information about the speaker. These second cascade parsed the named entity
annotated corpus. The objective is to locate information about the speaker and,
also, what kind of information can designate him/her. These two cascades was
evaluated with precision and recall measures."
"In this paper, we evaluate various French lexica with the parser FRMG: the
Lefff, LGLex, the lexicon built from the tables of the French Lexicon-Grammar,
the lexicon DICOVALENCE and a new version of the verbal entries of the Lefff,
obtained by merging with DICOVALENCE and partial manual validation. For this,
all these lexica have been converted to the format of the Lefff, Alexina
format. The evaluation was made on the part of the EASy corpus used in the
first evaluation campaign Passage."
"In this paper, we summerize the work done on the resources of Modern Greek on
the Lexicon-Grammar of verbs. We detail the definitional features of each
table, and all changes made to the names of features to make them consistent.
Through the development of the table of classes, including all the features, we
have considered the conversion of tables in a syntactic lexicon: LGLex. The
lexicon, in plain text format or XML, is generated by the LGExtract tool
(Constant & Tolone, 2010). This format is directly usable in applications of
Natural Language Processing (NLP)."
"This paper presents a work on extending the adverbial entries of LGLex: a NLP
oriented syntactic resource for French. Adverbs were extracted from the
Lexicon-Grammar tables of both simple adverbs ending in -ment '-ly' (Molinier
and Levrier, 2000) and compound adverbs (Gross, 1986; 1990). This work relies
on the exploitation of fine-grained linguistic information provided in existing
resources. Various features are encoded in both LG tables and they haven't been
exploited yet. They describe the relations of deleting, permuting, intensifying
and paraphrasing that associate, on the one hand, the simple and compound
adverbs and, on the other hand, different types of compound adverbs. The
resulting syntactic resource is manually evaluated and freely available under
the LGPL-LR license."
"Algorithms of question answering in a computer system oriented on input and
logical processing of text information are presented. A knowledge domain under
consideration is social behavior of a person. A database of the system includes
an internal representation of natural language sentences and supplemental
information. The answer {\it Yes} or {\it No} is formed for a general question.
A special question containing an interrogative word or group of interrogative
words permits to find a subject, object, place, time, cause, purpose and way of
action or event. Answer generation is based on identification algorithms of
persons, organizations, machines, things, places, and times. Proposed
algorithms of question answering can be realized in information systems closely
connected with text processing (criminology, operation of business, medicine,
document systems)."
"A tagger is a mandatory segment of most text scrutiny systems, as it
consigned a s yntax class (e.g., noun, verb, adjective, and adverb) to every
word in a sentence. In this paper, we present a simple part of speech tagger
for homoeopathy clinical language. This paper reports about the anticipated
part of speech tagger for homoeopathy clinical language. It exploit standard
pattern for evaluating sentences, untagged clinical corpus of 20085 words is
used, from which we had selected 125 sentences (2322 tokens). The problem of
tagging in natural language processing is to find a way to tag every word in a
text as a meticulous part of speech. The basic idea is to apply a set of rules
on clinical sentences and on each word, Accuracy is the leading factor in
evaluating any POS tagger so the accuracy of proposed tagger is also conversed."
"Twitter messages often contain so-called hashtags to denote keywords related
to them. Using a dataset of 29 million messages, I explore relations among
these hashtags with respect to co-occurrences. Furthermore, I present an
attempt to classify hashtags into five intuitive classes, using a
machine-learning approach. The overall outcome is an interactive Web
application to explore Twitter hashtags."
"Algorithms of inference in a computer system oriented to input and semantic
processing of text information are presented. Such inference is necessary for
logical questions when the direct comparison of objects from a question and
database can not give a result. The following classes of problems are
considered: a check of hypotheses for persons and non-typical actions, the
determination of persons and circumstances for non-typical actions, planning
actions, the determination of event cause and state of persons. To form an
answer both deduction and plausible reasoning are used. As a knowledge domain
under consideration is social behavior of persons, plausible reasoning is based
on laws of social psychology. Proposed algorithms of inference and plausible
reasoning can be realized in computer systems closely connected with text
processing (criminology, operation of business, medicine, document systems)."
"Here we describe work on learning the subcategories of verbs in a
morphologically rich language using only minimal linguistic resources. Our goal
is to learn verb subcategorizations for Quechua, an under-resourced
morphologically rich language, from an unannotated corpus. We compare results
from applying this approach to an unannotated Arabic corpus with those achieved
by processing the same text in treebank form. The original plan was to use only
a morphological analyzer and an unannotated corpus, but experiments suggest
that this approach by itself will not be effective for learning the
combinatorial potential of Arabic verbs in general. The lower bound on
resources for acquiring this information is somewhat higher, apparently
requiring a a part-of-speech tagger and chunker for most languages, and a
morphological disambiguater for Arabic."
"Sentiment analysis predicts the presence of positive or negative emotions in
a text document. In this paper we consider higher dimensional extensions of the
sentiment concept, which represent a richer set of human emotions. Our approach
goes beyond previous work in that our model contains a continuous manifold
rather than a finite set of human emotions. We investigate the resulting model,
compare it to psychological observations, and explore its predictive
capabilities. Besides obtaining significant improvements over a baseline
without manifold, we are also able to visualize different notions of positive
sentiment in different domains."
"This paper presents the continuation of the work completed by Satori and all.
[SCH07] by the realization of an automatic speech recognition system (ASR) for
Arabic language based SPHINX 4 system. The previous work was limited to the
recognition of the first ten digits, whereas the present work is a remarkable
projection consisting in continuous Arabic speech recognition with a rate of
recognition of surroundings 96%."
"We now have a rich and growing set of modeling tools and algorithms for
inducing linguistic structure from text that is less than fully annotated. In
this paper, we discuss some of the weaknesses of our current methodology. We
present a new abstract framework for evaluating natural language processing
(NLP) models in general and unsupervised NLP models in particular. The central
idea is to make explicit certain adversarial roles among researchers, so that
the different roles in an evaluation are more clearly defined and performers of
all roles are offered ways to make measurable contributions to the larger goal.
Adopting this approach may help to characterize model successes and failures by
encouraging earlier consideration of error analysis. The framework can be
instantiated in a variety of ways, simulating some familiar intrinsic and
extrinsic evaluations as well as some new evaluations."
"This paper addresses the problem of mapping natural language sentences to
lambda-calculus encodings of their meaning. We describe a learning algorithm
that takes as input a training set of sentences labeled with expressions in the
lambda calculus. The algorithm induces a grammar for the problem, along with a
log-linear model that represents a distribution over syntactic and semantic
analyses conditioned on the input sentence. We apply the method to the task of
learning natural language interfaces to databases and show that the learned
parsers outperform previous methods in two benchmark database domains."
"The following study presents a collocation extraction approach based on
clustering technique. This study uses a combination of several classical
measures which cover all aspects of a given corpus then it suggests separating
bigrams found in the corpus in several disjoint groups according to the
probability of presence of collocations. This will allow excluding groups where
the presence of collocations is very unlikely and thus reducing in a meaningful
way the search space."
"The work of automatic segmentation of a Manipuri language (or Meiteilon) word
into syllabic units is demonstrated in this paper. This language is a scheduled
Indian language of Tibeto-Burman origin, which is also a very highly
agglutinative language. This language usages two script: a Bengali script and
Meitei Mayek (Script). The present work is based on the second script. An
algorithm is designed so as to identify mainly the syllables of Manipuri origin
word. The result of the algorithm shows a Recall of 74.77, Precision of 91.21
and F-Score of 82.18 which is a reasonable score with the first attempt of such
kind for this language."
"The notion of appropriate sequence as introduced by Z. Harris provides a
powerful syntactic way of analysing the detailed meaning of various sentences,
including ambiguous ones. In an adjectival sentence like 'The leather was
yellow', the introduction of an appropriate noun, here 'colour', specifies
which quality the adjective describes. In some other adjectival sentences with
an appropriate noun, that noun plays the same part as 'colour' and seems to be
relevant to the description of the adjective. These appropriate nouns can
usually be used in elementary sentences like 'The leather had some colour', but
in many cases they have a more or less obligatory modifier. For example, you
can hardly mention that an object has a colour without qualifying that colour
at all. About 300 French nouns are appropriate in at least one adjectival
sentence and have an obligatory modifier. They enter in a number of sentence
structures related by several syntactic transformations. The appropriateness of
the noun and the fact that the modifier is obligatory are reflected in these
transformations. The description of these syntactic phenomena provides a basis
for a classification of these nouns. It also concerns the lexical properties of
thousands of predicative adjectives, and in particular the relations between
the sentence without the noun : 'The leather was yellow' and the adjectival
sentence with the noun : 'The colour of the leather was yellow'."
"The comparative evaluation of Arabic HPSG grammar lexica requires a deep
study of their linguistic coverage. The complexity of this task results mainly
from the heterogeneity of the descriptive components within those lexica
(underlying linguistic resources and different data categories, for example).
It is therefore essential to define more homogeneous representations, which in
turn will enable us to compare them and eventually merge them. In this context,
we present a method for comparing HPSG lexica based on a rule system. This
method is implemented within a prototype for the projection from Arabic HPSG to
a normalised pivot language compliant with LMF (ISO 24613 - Lexical Markup
Framework) and serialised using a TEI (Text Encoding Initiative) based
representation. The design of this system is based on an initial study of the
HPSG formalism looking at its adequacy for the representation of Arabic, and
from this, we identify the appropriate feature structures corresponding to each
Arabic lexical category and their possible LMF counterparts."
"Written Communication on Computers requires knowledge of writing text for the
desired language using Computer. Mostly people do not use any other language
besides English. This creates a barrier. To resolve this issue we have
developed a scheme to input text in Hindi using phonetic mapping scheme. Using
this scheme we generate intermediate code strings and match them with
pronunciations of input text. Our system show significant success over other
input systems available."
"Natural Language Parsing has been the most prominent research area since the
genesis of Natural Language Processing. Probabilistic Parsers are being
developed to make the process of parser development much easier, accurate and
fast. In Indian context, identification of which Computational Grammar
Formalism is to be used is still a question which needs to be answered. In this
paper we focus on this problem and try to analyze different formalisms for
Indian languages."
"This paper defines a method for lexicon in the biomedical domain from
comparable corpora. The method is based on compositional translation and
exploits morpheme-level translation equivalences. It can generate translations
for a large variety of morphologically constructed words and can also generate
'fertile' translations. We show that fertile translations increase the overall
quality of the extracted lexicon for English to French translation."
"The utility and power of Natural Language Processing (NLP) seems destined to
change our technological society in profound and fundamental ways. However
there are, to date, few accessible descriptions of the science of NLP that have
been written for a popular audience, or even for an audience of intelligent,
but uninitiated scientists. This paper aims to provide just such an overview.
In short, the objective of this article is to describe the purpose, procedures
and practical applications of NLP in a clear, balanced, and readable way. We
will examine the most recent literature describing the methods and processes of
NLP, analyze some of the challenges that researchers are faced with, and
briefly survey some of the current and future applications of this science to
IT research in general."
"We present a study of the relationship between gender, linguistic style, and
social networks, using a novel corpus of 14,000 Twitter users. Prior
quantitative work on gender often treats this social variable as a female/male
binary; we argue for a more nuanced approach. By clustering Twitter users, we
find a natural decomposition of the dataset into various styles and topical
interests. Many clusters have strong gender orientations, but their use of
linguistic resources sometimes directly conflicts with the population-level
language statistics. We view these clusters as a more accurate reflection of
the multifaceted nature of gendered language styles. Previous corpus-based work
has also had little to say about individuals whose linguistic styles defy
population-level gender patterns. To identify such individuals, we train a
statistical classifier, and measure the classifier confidence for each
individual in the dataset. Examining individuals whose language does not match
the classifier's model for their gender, we find that they have social networks
that include significantly fewer same-gender social connections and that, in
general, social network homophily is correlated with the use of same-gender
language markers. Pairing computational methods and social theory thus offers a
new perspective on how gender emerges as individuals position themselves
relative to audiences, topics, and mainstream gender norms."
"Gujarati is a resource poor language with almost no language processing tools
being available. In this paper we have shown an implementation of a rule based
stemmer of Gujarati. We have shown the creation of rules for stemming and the
richness in morphology that Gujarati possesses. We have also evaluated our
results by verifying it with a human expert."
"Developing parallel corpora is an important and a difficult activity for
Machine Translation. This requires manual annotation by Human Translators.
Translating same text again is a useless activity. There are tools available to
implement this for European Languages, but no such tool is available for Indian
Languages. In this paper we present a tool for Indian Languages which not only
provides automatic translations of the previously available translation but
also provides multiple translations, in cases where a sentence has multiple
translations, in ranked list of suggestive translations for a sentence.
Moreover this tool also lets translators have global and local saving options
of their work, so that they may share it with others, which further lightens
the task."
"This paper proposes a method for extracting translations of morphologically
constructed terms from comparable corpora. The method is based on compositional
translation and exploits translation equivalences at the morpheme-level, which
allows for the generation of ""fertile"" translations (translation pairs in which
the target term has more words than the source term). Ranking methods relying
on corpus-based and translation-based features are used to select the best
candidate translation. We obtain an average precision of 91% on the Top1
candidate translation. The method was tested on two language pairs
(English-French and English-German) and with a small specialized comparable
corpora (400k words per language)."
"The use of naive Bayesian classifier (NB) and the classifier by the k nearest
neighbors (kNN) in classification semantic analysis of authors' texts of
English fiction has been analysed. The authors' works are considered in the
vector space the basis of which is formed by the frequency characteristics of
semantic fields of nouns and verbs. Highly precise classification of authors'
texts in the vector space of semantic fields indicates about the presence of
particular spheres of author's idiolect in this space which characterizes the
individual author's style."
"This paper describes the Hangulphabet, a new writing system that should prove
useful in a number of contexts. Using the Hangulphabet, a user can instantly
see voicing, manner and place of articulation of any phoneme found in human
language. The Hangulphabet places consonant graphemes on a grid with the x-axis
representing the place of articulation and the y-axis representing manner of
articulation. Each individual grapheme contains radicals from both axes where
the points intersect. The top radical represents manner of articulation where
the bottom represents place of articulation. A horizontal line running through
the middle of the bottom radical represents voicing. For vowels, place of
articulation is located on a grid that represents the position of the tongue in
the mouth. This grid is similar to that of the IPA vowel chart (International
Phonetic Association, 1999). The difference with the Hangulphabet being the
trapezoid representing the vocal apparatus is on a slight tilt. Place of
articulation for a vowel is represented by a breakout figure from the grid.
This system can be used as an alternative to the International Phonetic
Alphabet (IPA) or as a complement to it. Beginning students of linguistics may
find it particularly useful. A Hangulphabet font has been created to facilitate
switching between the Hangulphabet and the IPA."
"Large language models have been proven quite beneficial for a variety of
automatic speech recognition tasks in Google. We summarize results on Voice
Search and a few YouTube speech transcription tasks to highlight the impact
that one can expect from increasing both the amount of training data, and the
size of the language model estimated from such data. Depending on the task,
availability and amount of training data used, language model size and amount
of work and care put into integrating them in the lattice rescoring step we
observe reductions in word error rate between 6% and 10% relative, for systems
on a wide range of operating points between 17% and 52% word error rate."
"In this paper we present a new and simple language-independent method for
word-alignment based on the use of external sources of bilingual information
such as machine translation systems. We show that the few parameters of the
aligner can be trained on a very small corpus, which leads to results
comparable to those obtained by the state-of-the-art tool GIZA++ in terms of
precision. Regarding other metrics, such as alignment error rate or F-measure,
the parametric aligner, when trained on a very small gold-standard (450 pairs
of sentences), provides results comparable to those produced by GIZA++ when
trained on an in-domain corpus of around 10,000 pairs of sentences.
Furthermore, the results obtained indicate that the training is
domain-independent, which enables the use of the trained aligner 'on the fly'
on any new pair of sentences."
"Using a corpus of over 17,000 financial news reports (involving over 10M
words), we perform an analysis of the argument-distributions of the UP- and
DOWN-verbs used to describe movements of indices, stocks, and shares. Using
measures of the overlap in the argument distributions of these verbs and
k-means clustering of their distributions, we advance evidence for the proposal
that the metaphors referred to by these verbs are organised into hierarchical
structures of superordinate and subordinate groups."
"Using a corpus of 17,000+ financial news reports (involving over 10M words),
we perform an analysis of the argument-distributions of the UP and DOWN verbs
used to describe movements of indices, stocks and shares. In Study 1
participants identified antonyms of these verbs in a free-response task and a
matching task from which the most commonly identified antonyms were compiled.
In Study 2, we determined whether the argument-distributions for the verbs in
these antonym-pairs were sufficiently similar to predict the most
frequently-identified antonym. Cosine similarity correlates moderately with the
proportions of antonym-pairs identified by people (r = 0.31). More
impressively, 87% of the time the most frequently-identified antonym is either
the first- or second-most similar pair in the set of alternatives. The
implications of these results for distributional approaches to determining
metaphoric knowledge are discussed."
"We present a method of finding and analyzing shifts in grammatical relations
found in diachronic corpora. Inspired by the econometric technique of measuring
return and volatility instead of relative frequencies, we propose them as a way
to better characterize changes in grammatical patterns like nominalization,
modification and comparison. To exemplify the use of these techniques, we
examine a corpus of NIPS papers and report trends which manifest at the token,
part-of-speech and grammatical levels. Building up from frequency observations
to a second-order analysis, we show that shifts in frequencies overlook deeper
trends in language, even when part-of-speech information is included. Examining
token, POS and grammatical levels of variation enables a summary view of
diachronic text as a whole. We conclude with a discussion about how these
methods can inform intuitions about specialist domains as well as changes in
language use as a whole."
"Many approaches to sentiment analysis rely on lexica where words are tagged
with their prior polarity - i.e. if a word out of context evokes something
positive or something negative. In particular, broad-coverage resources like
SentiWordNet provide polarities for (almost) every word. Since words can have
multiple senses, we address the problem of how to compute the prior polarity of
a word starting from the polarity of each sense and returning its polarity
strength as an index between -1 and 1. We compare 14 such formulae that appear
in the literature, and assess which one best approximates the human judgement
of prior polarities, with both regression and classification models."
"In this paper, we define event expression over sentences of natural language
and semantic relations between events. Based on this definition, we formally
consider text understanding process having events as basic unit."
"The present paper explores various arguments in favour of making the Text
Encoding Initia-tive (TEI) guidelines an appropriate serialisation for ISO
standard 24613:2008 (LMF, Lexi-cal Mark-up Framework) . It also identifies the
issues that would have to be resolved in order to reach an appropriate
implementation of these ideas, in particular in terms of infor-mational
coverage. We show how the customisation facilities offered by the TEI
guidelines can provide an adequate background, not only to cover missing
components within the current Dictionary chapter of the TEI guidelines, but
also to allow specific lexical projects to deal with local constraints. We
expect this proposal to be a basis for a future ISO project in the context of
the on going revision of LMF."
"Online content analysis employs algorithmic methods to identify entities in
unstructured text. Both machine learning and knowledge-base approaches lie at
the foundation of contemporary named entities extraction systems. However, the
progress in deploying these approaches on web-scale has been been hampered by
the computational cost of NLP over massive text corpora. We present SpeedRead
(SR), a named entity recognition pipeline that runs at least 10 times faster
than Stanford NLP pipeline. This pipeline consists of a high performance Penn
Treebank- compliant tokenizer, close to state-of-art part-of-speech (POS)
tagger and knowledge-based named entity recognizer."
"Sentiment analysis predicts the presence of positive or negative emotions in
a text document. In this paper, we consider higher dimensional extensions of
the sentiment concept, which represent a richer set of human emotions. Our
approach goes beyond previous work in that our model contains a continuous
manifold rather than a finite set of human emotions. We investigate the
resulting model, compare it to psychological observations, and explore its
predictive capabilities."
"A neural probabilistic language model (NPLM) provides an idea to achieve the
better perplexity than n-gram language model and their smoothed language
models. This paper investigates application area in bilingual NLP, specifically
Statistical Machine Translation (SMT). We focus on the perspectives that NPLM
has potential to open the possibility to complement potentially `huge'
monolingual resources into the `resource-constraint' bilingual resources. We
introduce an ngram-HMM language model as NPLM using the non-parametric Bayesian
construction. In order to facilitate the application to various tasks, we
propose the joint space model of ngram-HMM language model. We show an
experiment of system combination in the area of SMT. One discovery was that our
treatment of noise improved the results 0.20 BLEU points if NPLM is trained in
relatively small corpus, in our case 500,000 sentence pairs, which is often the
case due to the long training time of NPLM."
"We propose two novel model architectures for computing continuous vector
representations of words from very large data sets. The quality of these
representations is measured in a word similarity task, and the results are
compared to the previously best performing techniques based on different types
of neural networks. We observe large improvements in accuracy at much lower
computational cost, i.e. it takes less than a day to learn high quality word
vectors from a 1.6 billion words data set. Furthermore, we show that these
vectors provide state-of-the-art performance on our test set for measuring
syntactic and semantic word similarities."
"Children learn their native language by exposure to their linguistic and
communicative environment, but apparently without requiring that their mistakes
are corrected. Such learning from positive evidence has been viewed as raising
logical problems for language acquisition. In particular, without correction,
how is the child to recover from conjecturing an over-general grammar, which
will be consistent with any sentence that the child hears? There have been many
proposals concerning how this logical problem can be dissolved. Here, we review
recent formal results showing that the learner has sufficient data to learn
successfully from positive evidence, if it favours the simplest encoding of the
linguistic input. Results include the ability to learn a linguistic prediction,
grammaticality judgements, language production, and form-meaning mappings. The
simplicity approach can also be scaled-down to analyse the ability to learn a
specific linguistic constructions, and is amenable to empirical test as a
framework for describing human language acquisition."
"The paper revives an older approach to acoustic modeling that borrows from
n-gram language modeling in an attempt to scale up both the amount of training
data and model size (as measured by the number of parameters in the model), to
approximately 100 times larger than current sizes used in automatic speech
recognition. In such a data-rich setting, we can expand the phonetic context
significantly beyond triphones, as well as increase the number of Gaussian
mixture components for the context-dependent states that allow it. We have
experimented with contexts that span seven or more context-independent phones,
and up to 620 mixture components per state. Dealing with unseen phonetic
contexts is accomplished using the familiar back-off technique used in language
modeling due to implementation simplicity. The back-off acoustic model is
estimated, stored and served using MapReduce distributed computing
infrastructure.
  Speech recognition experiments are carried out in an N-best list rescoring
framework for Google Voice Search. Training big models on large amounts of data
proves to be an effective way to increase the accuracy of a state-of-the-art
automatic speech recognition system. We use 87,000 hours of training data
(speech along with transcription) obtained by filtering utterances in Voice
Search logs on automatic speech recognition confidence. Models ranging in size
between 20--40 million Gaussians are estimated using maximum likelihood
training. They achieve relative reductions in word-error-rate of 11% and 6%
when combined with first-pass models trained using maximum likelihood, and
boosted maximum mutual information, respectively. Increasing the context size
beyond five phones (quinphones) does not help."
"When developing a conversational agent, there is often an urgent need to have
a prototype available in order to test the application with real users. A
Wizard of Oz is a possibility, but sometimes the agent should be simply
deployed in the environment where it will be used. Here, the agent should be
able to capture as many interactions as possible and to understand how people
react to failure. In this paper, we focus on the rapid development of a natural
language understanding module by non experts. Our approach follows the learning
paradigm and sees the process of understanding natural language as a
classification problem. We test our module with a conversational agent that
answers questions in the art domain. Moreover, we show how our approach can be
used by a natural language interface to a cinema database."
"The variation of word meaning according to the context leads us to enrich the
type system of our syntactical and semantic analyser of French based on
categorial grammars and Montague semantics (or lambda-DRT). The main advantage
of a deep semantic analyse is too represent meaning by logical formulae that
can be easily used e.g. for inferences. Determiners and quantifiers play a
fundamental role in the construction of those formulae. But in our rich type
system the usual semantic terms do not work. We propose a solution ins- pired
by the tau and epsilon operators of Hilbert, kinds of generic elements and
choice functions. This approach unifies the treatment of the different determi-
ners and quantifiers as well as the dynamic binding of pronouns. Above all,
this fully computational view fits in well within the wide coverage parser
Grail, both from a theoretical and a practical viewpoint."
"The Lexical Access Problem consists of determining the intended sequence of
words corresponding to an input sequence of phonemes (basic speech sounds) that
come from a low-level phoneme recognizer. In this paper we present an
information-theoretic approach based on the Minimum Message Length Criterion
for solving the Lexical Access Problem. We model sentences using phoneme
realizations seen in training, and word and part-of-speech information obtained
from text corpora. We show results on multiple-speaker, continuous, read speech
and discuss a heuristic using equivalence classes of similar sounding words
which speeds up the recognition process without significant deterioration in
recognition accuracy."
"This paper describes our submission to the First Workshop on Reordering for
Statistical Machine Translation. We have decided to build a reordering system
based on tree-to-string model, using only publicly available tools to
accomplish this task. With the provided training data we have built a
translation model using Moses toolkit, and then we applied a chart decoder,
implemented in Moses, to reorder the sentences. Even though our submission only
covered English-Farsi language pair, we believe that the approach itself should
work regardless of the choice of the languages, so we have also carried out the
experiments for English-Italian and English-Urdu. For these language pairs we
have noticed a significant improvement over the baseline in BLEU, Kendall-Tau
and Hamming metrics. A detailed description is given, so that everyone can
reproduce our results. Also, some possible directions for further improvements
are discussed."
"Cross-Language Information Retrieval (CLIR) and machine translation (MT)
resources, such as dictionaries and parallel corpora, are scarce and hard to
come by for special domains. Besides, these resources are just limited to a few
languages, such as English, French, and Spanish and so on. So, obtaining
comparable corpora automatically for such domains could be an answer to this
problem effectively. Comparable corpora, that the subcorpora are not
translations of each other, can be easily obtained from web. Therefore,
building and using comparable corpora is often a more feasible option in
multilingual information processing. Comparability metrics is one of key issues
in the field of building and using comparable corpus. Currently, there is no
widely accepted definition or metrics method of corpus comparability. In fact,
Different definitions or metrics methods of comparability might be given to
suit various tasks about natural language processing. A new comparability,
namely, termhood-based metrics, oriented to the task of bilingual terminology
extraction, is proposed in this paper. In this method, words are ranked by
termhood not frequency, and then the cosine similarities, calculated based on
the ranking lists of word termhood, is used as comparability. Experiments
results show that termhood-based metrics performs better than traditional
frequency-based metrics."
"Purpose: Terminology is the set of technical words or expressions used in
specific contexts, which denotes the core concept in a formal discipline and is
usually applied in the fields of machine translation, information retrieval,
information extraction and text categorization, etc. Bilingual terminology
extraction plays an important role in the application of bilingual dictionary
compilation, bilingual Ontology construction, machine translation and
cross-language information retrieval etc. This paper addresses the issues of
monolingual terminology extraction and bilingual term alignment based on
multi-level termhood.
  Design/methodology/approach: A method based on multi-level termhood is
proposed. The new method computes the termhood of the terminology candidate as
well as the sentence that includes the terminology by the comparison of the
corpus. Since terminologies and general words usually have differently
distribution in the corpus, termhood can also be used to constrain and enhance
the performance of term alignment when aligning bilingual terms on the parallel
corpus. In this paper, bilingual term alignment based on termhood constraints
is presented.
  Findings: Experiment results show multi-level termhood can get better
performance than existing method for terminology extraction. If termhood is
used as constrain factor, the performance of bilingual term alignment can be
improved."
"Regulations in the Building Industry are becoming increasingly complex and
involve more than one technical area. They cover products, components and
project implementation. They also play an important role to ensure the quality
of a building, and to minimize its environmental impact. In this paper, we are
particularly interested in the modeling of the regulatory constraints derived
from the Technical Guides issued by CSTB and used to validate Technical
Assessments. We first describe our approach for modeling regulatory constraints
in the SBVR language, and formalizing them in the SPARQL language. Second, we
describe how we model the processes of compliance checking described in the
CSTB Technical Guides. Third, we show how we implement these processes to
assist industrials in drafting Technical Documents in order to acquire a
Technical Assessment; a compliance report is automatically generated to explain
the compliance or noncompliance of this Technical Documents."
"In natural-language discourse, related events tend to appear near each other
to describe a larger scenario. Such structures can be formalized by the notion
of a frame (a.k.a. template), which comprises a set of related events and
prototypical participants and event transitions. Identifying frames is a
prerequisite for information extraction and natural language generation, and is
usually done manually. Methods for inducing frames have been proposed recently,
but they typically use ad hoc procedures and are difficult to diagnose or
extend. In this paper, we propose the first probabilistic approach to frame
induction, which incorporates frames, events, participants as latent topics and
learns those frame and event transitions that best explain the text. The number
of frames is inferred by a novel application of a split-merge method from
syntactic parsing. In end-to-end evaluations from text to induced frames and
extracted facts, our method produced state-of-the-art results while
substantially reducing engineering effort."
"In the first part of this article, we explore the background of
computer-assisted learning from its beginnings in the early XIXth century and
the first teaching machines, founded on theories of learning, at the start of
the XXth century. With the arrival of the computer, it became possible to offer
language learners different types of language activities such as comprehension
tasks, simulations, etc. However, these have limits that cannot be overcome
without some contribution from the field of natural language processing (NLP).
In what follows, we examine the challenges faced and the issues raised by
integrating NLP into CALL. We hope to demonstrate that the key to success in
integrating NLP into CALL is to be found in multidisciplinary work between
computer experts, linguists, language teachers, didacticians and NLP
specialists."
"This project is a part of nature language processing and its aims to develop
a system of recognition inference text-appointed TIMINF. This type of system
can detect, given two portions of text, if a text is semantically deducted from
the other. We focused on making the inference time in this type of system. For
that we have built and analyzed a body built from questions collected through
the web. This study has enabled us to classify different types of times
inferences and for designing the architecture of TIMINF which seeks to
integrate a module inference time in a detection system inference text. We also
assess the performance of sorties TIMINF system on a test corpus with the same
strategy adopted in the challenge RTE."
"Probabilistic approaches to part-of-speech tagging rely primarily on
whole-word statistics about word/tag combinations as well as contextual
information. But experience shows about 4 per cent of tokens encountered in
test sets are unknown even when the training set is as large as a million
words. Unseen words are tagged using secondary strategies that exploit word
features such as endings, capitalizations and punctuation marks. In this work,
word-ending statistics are primary and whole-word statistics are secondary.
First, a tagger was trained and tested on word endings only. Subsequent
experiments added back whole-word statistics for the words occurring most
frequently in the training set. As grew larger, performance was expected to
improve, in the limit performing the same as word-based taggers. Surprisingly,
the ending-based tagger initially performed nearly as well as the word-based
tagger; in the best case, its performance significantly exceeded that of the
word-based tagger. Lastly, and unexpectedly, an effect of negative returns was
observed - as grew larger, performance generally improved and then declined. By
varying factors such as ending length and tag-list strategy, we achieved a
success rate of 97.5 percent."
"The classification of opinion texts in positive and negative is becoming a
subject of great interest in sentiment analysis. The existence of many labeled
opinions motivates the use of statistical and machine-learning methods.
First-order statistics have proven to be very limited in this field. The Opinum
approach is based on the order of the words without using any syntactic and
semantic information. It consists of building one probabilistic model for the
positive and another one for the negative opinions. Then the test opinions are
compared to both models and a decision and confidence measure are calculated.
In order to reduce the complexity of the training corpus we first lemmatize the
texts and we replace most named-entities with wildcards. Opinum presents an
accuracy above 81% for Spanish opinions in the financial products domain. In
this work we discuss which are the most important factors that have impact on
the classification performance."
"This article reports on the results of the research done towards the fully
automatically merging of lexical resources. Our main goal is to show the
generality of the proposed approach, which have been previously applied to
merge Spanish Subcategorization Frames lexica. In this work we extend and apply
the same technique to perform the merging of morphosyntactic lexica encoded in
LMF. The experiments showed that the technique is general enough to obtain good
results in these two different tasks which is an important step towards
performing the merging of lexical resources fully automatically."
"The work we present here addresses cue-based noun classification in English
and Spanish. Its main objective is to automatically acquire lexical semantic
information by classifying nouns into previously known noun lexical classes.
This is achieved by using particular aspects of linguistic contexts as cues
that identify a specific lexical class. Here we concentrate on the task of
identifying such cues and the theoretical background that allows for an
assessment of the complexity of the task. The results show that, despite of the
a-priori complexity of the task, cue-based classification is a useful tool in
the automatic acquisition of lexical semantic classes."
"Subjective language detection is one of the most important challenges in
Sentiment Analysis. Because of the weight and frequency in opinionated texts,
adjectives are considered a key piece in the opinion extraction process. These
subjective units are more and more frequently collected in polarity lexicons in
which they appear annotated with their prior polarity. However, at the moment,
any polarity lexicon takes into account prior polarity variations across
domains. This paper proves that a majority of adjectives change their prior
polarity value depending on the domain. We propose a distinction between domain
dependent and domain independent adjectives. Moreover, our analysis led us to
propose a further classification related to subjectivity degree: constant,
mixed and highly subjective adjectives. Following this classification, polarity
values will be a better support for Sentiment Analysis."
"The objective of the PANACEA ICT-2007.2.2 EU project is to build a platform
that automates the stages involved in the acquisition, production, updating and
maintenance of the large language resources required by, among others, MT
systems. The development of a Corpus Acquisition Component (CAC) for extracting
monolingual and bilingual data from the web is one of the most innovative
building blocks of PANACEA. The CAC, which is the first stage in the PANACEA
pipeline for building Language Resources, adopts an efficient and distributed
methodology to crawl for web documents with rich textual content in specific
languages and predefined domains. The CAC includes modules that can acquire
parallel data from sites with in-domain content available in more than one
language. In order to extrinsically evaluate the CAC methodology, we have
conducted several experiments that used crawled parallel corpora for the
identification and extraction of parallel sentences using sentence alignment.
The corpora were then successfully used for domain adaptation of Machine
Translation Systems."
"In this work we present the results of our experimental work on the
develop-ment of lexical class-based lexica by automatic means. The objective is
to as-sess the use of linguistic lexical-class based information as a feature
selection methodology for the use of classifiers in quick lexical development.
The results show that the approach can help in re-ducing the human effort
required in the development of language resources sig-nificantly."
"Acquiring lexical information is a complex problem, typically approached by
relying on a number of contexts to contribute information for classification.
One of the first issues to address in this domain is the determination of such
contexts. The work presented here proposes the use of automatically obtained
FORMAL role descriptors as features used to draw nouns from the same lexical
semantic class together in an unsupervised clustering task. We have dealt with
three lexical semantic classes (HUMAN, LOCATION and EVENT) in English. The
results obtained show that it is possible to discriminate between elements from
different lexical semantic classes using only FORMAL role information, hence
validating our initial hypothesis. Also, iterating our method accurately
accounts for fine-grained distinctions within lexical classes, namely
distinctions involving ambiguous expressions. Moreover, a filtering and
bootstrapping strategy employed in extracting FORMAL role descriptors proved to
minimize effects of sparse data and noise in our task."
"This article presents a probabilistic generative model for text based on
semantic topics and syntactic classes called Part-of-Speech LDA (POSLDA).
POSLDA simultaneously uncovers short-range syntactic patterns (syntax) and
long-range semantic patterns (topics) that exist in document collections. This
results in word distributions that are specific to both topics (sports,
education, ...) and parts-of-speech (nouns, verbs, ...). For example,
multinomial distributions over words are uncovered that can be understood as
""nouns about weather"" or ""verbs about law"". We describe the model and an
approximate inference algorithm and then demonstrate the quality of the learned
topics both qualitatively and quantitatively. Then, we discuss an NLP
application where the output of POSLDA can lead to strong improvements in
quality: unsupervised part-of-speech tagging. We describe algorithms for this
task that make use of POSLDA-learned distributions that result in improved
performance beyond the state of the art."
"SYNTAGMA is a rule-based parsing system, structured on two levels: a general
parsing engine and a language specific grammar. The parsing engine is a
language independent program, while grammar and language specific rules and
resources are given as text files, consisting in a list of constituent
structuresand a lexical database with word sense related features and
constraints. Since its theoretical background is principally Tesniere's
Elements de syntaxe, SYNTAGMA's grammar emphasizes the role of argument
structure (valency) in constraint satisfaction, and allows also horizontal
bounds, for instance treating coordination. Notions such as Pro, traces, empty
categories are derived from Generative Grammar and some solutions are close to
Government&Binding Theory, although they are the result of an autonomous
research. These properties allow SYNTAGMA to manage complex syntactic
configurations and well known weak points in parsing engineering. An important
resource is the semantic network, which is used in disambiguation tasks.
Parsing process follows a bottom-up, rule driven strategy. Its behavior can be
controlled and fine-tuned."
"Computers still have a long way to go before they can interact with users in
a truly natural fashion. From a users perspective, the most natural way to
interact with a computer would be through a speech and gesture interface.
Although speech recognition has made significant advances in the past ten
years, gesture recognition has been lagging behind. Sign Languages (SL) are the
most accomplished forms of gestural communication. Therefore, their automatic
analysis is a real challenge, which is interestingly implied to their lexical
and syntactic organization levels. Statements dealing with sign language occupy
a significant interest in the Automatic Natural Language Processing (ANLP)
domain. In this work, we are dealing with sign language recognition, in
particular of French Sign Language (FSL). FSL has its own specificities, such
as the simultaneity of several parameters, the important role of the facial
expression or movement and the use of space for the proper utterance
organization. Unlike speech recognition, Frensh sign language (FSL) events
occur both sequentially and simultaneously. Thus, the computational processing
of FSL is too complex than the spoken languages. We present a novel approach
based on HMM to reduce the recognition complexity."
"Our day-to-day life has always been influenced by what people think. Ideas
and opinions of others have always affected our own opinions. The explosion of
Web 2.0 has led to increased activity in Podcasting, Blogging, Tagging,
Contributing to RSS, Social Bookmarking, and Social Networking. As a result
there has been an eruption of interest in people to mine these vast resources
of data for opinions. Sentiment Analysis or Opinion Mining is the computational
treatment of opinions, sentiments and subjectivity of text. In this report, we
take a look at the various challenges and applications of Sentiment Analysis.
We will discuss in details various approaches to perform a computational
treatment of sentiments and opinions. Various supervised or data-driven
techniques to SA like Na\""ive Byes, Maximum Entropy, SVM, and Voted Perceptrons
will be discussed and their strengths and drawbacks will be touched upon. We
will also see a new dimension of analyzing sentiments by Cognitive Psychology
mainly through the work of Janyce Wiebe, where we will see ways to detect
subjectivity, perspective in narrative and understanding the discourse
structure. We will also study some specific topics in Sentiment Analysis and
the contemporary works in those areas."
"In the geolocation field where high-level programs and low-level devices
coexist, it is often difficult to find a friendly user inter- face to configure
all the parameters. The challenge addressed in this paper is to propose
intuitive and simple, thus natural lan- guage interfaces to interact with
low-level devices. Such inter- faces contain natural language processing and
fuzzy represen- tations of words that facilitate the elicitation of
business-level objectives in our context."
"Word ambiguity removal is a task of removing ambiguity from a word, i.e.
correct sense of word is identified from ambiguous sentences. This paper
describes a model that uses Part of Speech tagger and three categories for word
sense disambiguation (WSD). Human Computer Interaction is very needful to
improve interactions between users and computers. For this, the Supervised and
Unsupervised methods are combined. The WSD algorithm is used to find the
efficient and accurate sense of a word based on domain information. The
accuracy of this work is evaluated with the aim of finding best suitable domain
of word."
"TimeML is an XML-based schema for annotating temporal information over
discourse. The standard has been used to annotate a variety of resources and is
followed by a number of tools, the creation of which constitute hundreds of
thousands of man-hours of research work. However, the current state of
resources is such that many are not valid, or do not produce valid output, or
contain ambiguous or custom additions and removals. Difficulties arising from
these variances were highlighted in the TempEval-3 exercise, which included its
own extra stipulations over conventional TimeML as a response.
  To unify the state of current resources, and to make progress toward easy
adoption of its current incarnation ISO-TimeML, this paper introduces
TimeML-strict: a valid, unambiguous, and easy-to-process subset of TimeML. We
also introduce three resources -- a schema for TimeML-strict; a validator tool
for TimeML-strict, so that one may ensure documents are in the correct form;
and a repair tool that corrects common invalidating errors and adds
disambiguating markup in order to convert documents from the laxer TimeML
standard to TimeML-strict."
"This paper describes a temporal expression identification and normalization
system, ManTIME, developed for the TempEval-3 challenge. The identification
phase combines the use of conditional random fields along with a
post-processing identification pipeline, whereas the normalization phase is
carried out using NorMA, an open-source rule-based temporal normalizer. We
investigate the performance variation with respect to different feature types.
Specifically, we show that the use of WordNet-based features in the
identification task negatively affects the overall performance, and that there
is no statistically significant difference in using gazetteers, shallow parsing
and propositional noun phrases labels on top of the morphological features. On
the test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in the
identification phase. Normalization accuracies are 0.84 (type attribute) and
0.77 (value attribute). Surprisingly, the use of the silver data (alone or in
addition to the gold annotated ones) does not improve the performance."
"We consider the unsupervised alignment of the full text of a book with a
human-written summary. This presents challenges not seen in other text
alignment problems, including a disparity in length and, consequent to this, a
violation of the expectation that individual words and phrases should align,
since large passages and chapters can be distilled into a single summary
phrase. We present two new methods, based on hidden Markov models, specifically
targeted to this problem, and demonstrate gains on an extractive book
summarization task. While there is still much room for improvement,
unsupervised alignment holds intrinsic value in offering insight into what
features of a book are deemed worthy of summarization."
"The project presented in this article aims to formalize criteria and
procedures in order to extract semantic information from parsed dictionary
glosses. The actual purpose of the project is the generation of a semantic
network (nearly an ontology) issued from a monolingual Italian dictionary,
through unsupervised procedures. Since the project involves rule-based Parsing,
Semantic Tagging and Word Sense Disambiguation techniques, its outcomes may
find an interest also beyond this immediate intent. The cooperation of both
syntactic and semantic features in meaning construction are investigated, and
procedures which allows a translation of syntactic dependencies in semantic
relations are discussed. The procedures that rise from this project can be
applied also to other text types than dictionary glosses, as they convert the
output of a parsing process into a semantic representation. In addition some
mechanism are sketched that may lead to a kind of procedural semantics, through
which multiple paraphrases of an given expression can be generated. Which means
that these techniques may find an application also in 'query expansion'
strategies, interesting Information Retrieval, Search Engines and Question
Answering Systems."
"Chinese word segmentation is a fundamental task for Chinese language
processing. The granularity mismatch problem is the main cause of the errors.
This paper showed that the binary tree representation can store outputs with
different granularity. A binary tree based framework is also designed to
overcome the granularity mismatch problem. There are two steps in this
framework, namely tree building and tree pruning. The tree pruning step is
specially designed to focus on the granularity problem. Previous work for
Chinese word segmentation such as the sequence tagging can be easily employed
in this framework. This framework can also provide quantitative error analysis
methods. The experiments showed that after using a more sophisticated tree
pruning function for a state-of-the-art conditional random field based
baseline, the error reduction can be up to 20%."
"Conceptual combination performs a fundamental role in creating the broad
range of compound phrases utilized in everyday language. This article provides
a novel probabilistic framework for assessing whether the semantics of
conceptual combinations are compositional, and so can be considered as a
function of the semantics of the constituent concepts, or not. While the
systematicity and productivity of language provide a strong argument in favor
of assuming compositionality, this very assumption is still regularly
questioned in both cognitive science and philosophy. Additionally, the
principle of semantic compositionality is underspecified, which means that
notions of both ""strong"" and ""weak"" compositionality appear in the literature.
Rather than adjudicating between different grades of compositionality, the
framework presented here contributes formal methods for determining a clear
dividing line between compositional and non-compositional semantics. In
addition, we suggest that the distinction between these is contextually
sensitive. Utilizing formal frameworks developed for analyzing composite
systems in quantum theory, we present two methods that allow the semantics of
conceptual combinations to be classified as ""compositional"" or
""non-compositional"". Compositionality is first formalised by factorising the
joint probability distribution modeling the combination, where the terms in the
factorisation correspond to individual concepts. This leads to the necessary
and sufficient condition for the joint probability distribution to exist. A
failure to meet this condition implies that the underlying concepts cannot be
modeled in a single probability space when considering their combination, and
the combination is thus deemed ""non-compositional"". The formal analysis methods
are demonstrated by applying them to an empirical study of twenty-four
non-lexicalised conceptual combinations."
"We describe an inventory of semantic relations that are expressed by
prepositions. We define these relations by building on the word sense
disambiguation task for prepositions and propose a mapping from preposition
senses to the relation labels by collapsing semantically related senses across
prepositions."
"Conventional statistics-based methods for joint Chinese word segmentation and
part-of-speech tagging (S&T) have generalization ability to recognize new words
that do not appear in the training data. An undesirable side effect is that a
number of meaningless words will be incorrectly created. We propose an
effective and efficient framework for S&T that introduces features to
significantly reduce meaningless words generation. A general lexicon, Wikepedia
and a large-scale raw corpus of 200 billion characters are used to generate
word-based features for the wordhood. The word-lattice based framework consists
of a character-based model and a word-based model in order to employ our
word-based features. Experiments on Penn Chinese treebank 5 show that this
method has a 62.9% reduction of meaningless word generation in comparison with
the baseline. As a result, the F1 measure for segmentation is increased to
0.984."
"We live in a translingual society, in order to communicate with people from
different parts of the world we need to have an expertise in their respective
languages. Learning all these languages is not at all possible; therefore we
need a mechanism which can do this task for us. Machine translators have
emerged as a tool which can perform this task. In order to develop a machine
translator we need to develop several different rules. The very first module
that comes in machine translation pipeline is morphological analysis. Stemming
and lemmatization comes under morphological analysis. In this paper we have
created a lemmatizer which generates rules for removing the affixes along with
the addition of rules for creating a proper root word."
"We introduce a framework for lightweight dependency syntax annotation. Our
formalism builds upon the typical representation for unlabeled dependencies,
permitting a simple notation and annotation workflow. Moreover, the formalism
encourages annotators to underspecify parts of the syntax if doing so would
streamline the annotation process. We demonstrate the efficacy of this
annotation on three languages and develop algorithms to evaluate and compare
underspecified annotations."
"With the increasing empirical success of distributional models of
compositional semantics, it is timely to consider the types of textual logic
that such models are capable of capturing. In this paper, we address
shortcomings in the ability of current models to capture logical operations
such as negation. As a solution we propose a tripartite formulation for a
continuous vector space representation of semantics and subsequently use this
representation to develop a formal compositional notion of negation within such
models."
"The compositionality of meaning extends beyond the single sentence. Just as
words combine to form the meaning of sentences, so do sentences combine to form
the meaning of paragraphs, dialogues and general discourse. We introduce both a
sentence model and a discourse model corresponding to the two levels of
compositionality. The sentence model adopts convolution as the central
operation for composing semantic vectors and is based on a novel hierarchical
convolutional neural network. The discourse model extends the sentence model
and is based on a recurrent neural network that is conditioned in a novel way
both on the current sentence and on the current speaker. The discourse model is
able to capture both the sequentiality of sentences and the interaction between
different speakers. Without feature engineering or pretraining and with simple
greedy decoding, the discourse model coupled to the sentence model obtains
state of the art performance on a dialogue act classification experiment."
"A Dialogue System is a system which interacts with human in natural language.
At present many universities are developing the dialogue system in their
regional language. This paper will discuss about dialogue system, its
components, challenges and its evaluation. This paper helps the researchers for
getting info regarding dialogues system."
"My system utilizes the outcomes feature found in Moodle and other learning
content management systems (LCMSs) to keep track of where students are in terms
of what language competencies they have mastered and the competencies they need
to get where they want to go. These competencies are based on the Common
European Framework for (English) Language Learning. This data can be available
for everyone involved with a given student's progress (e.g. educators, parents,
supervisors and the students themselves). A given student's record of past
accomplishments can also be meshed with those of his classmates. Not only are a
student's competencies easily seen and tracked, educators can view competencies
of a group of students that were achieved prior to enrollment in the class.
This should make curriculum decision making easier and more efficient for
educators."
"The problem of named entity recognition in the medical/clinical domain has
gained increasing attention do to its vital role in a wide range of clinical
decision support applications. The identification of complete and correct term
span is vital for further knowledge synthesis (e.g., coding/mapping concepts
thesauruses and classification standards). This paper investigates boundary
adjustment by sequence labeling representations models and post-processing
techniques in the problem of clinical named entity recognition (recognition of
clinical events). Using current state-of-the-art sequence labeling algorithm
(conditional random fields), we show experimentally that sequence labeling
representation and post-processing can be significantly helpful in strict
boundary identification of clinical events."
"An object--oriented approach to create a natural language understanding
system is considered. The understanding program is a formal system built on the
base of predicative calculus. Horn's clauses are used as well--formed formulas.
An inference is based on the principle of resolution. Sentences of natural
language are represented in the view of typical predicate set. These predicates
describe physical objects and processes, abstract objects, categories and
semantic relations between objects. Predicates for concrete assertions are
saved in a database. To describe the semantics of classes for physical objects,
abstract concepts and processes, a knowledge base is applied. The proposed
representation of natural language sentences is a semantic net. Nodes of such
net are typical predicates. This approach is perspective as, firstly, such
typification of nodes facilitates essentially forming of processing algorithms
and object descriptions, secondly, the effectiveness of algorithms is increased
(particularly for the great number of nodes), thirdly, to describe the
semantics of words, encyclopedic knowledge is used, and this permits
essentially to extend the class of solved problems."
"How many words are needed to define all the words in a dictionary?
Graph-theoretic analysis reveals that about 10% of a dictionary is a unique
Kernel of words that define one another and all the rest, but this is not the
smallest such subset. The Kernel consists of one huge strongly connected
component (SCC), about half its size, the Core, surrounded by many small SCCs,
the Satellites. Core words can define one another but not the rest of the
dictionary. The Kernel also contains many overlapping Minimal Grounding Sets
(MGSs), each about the same size as the Core, each part-Core, part-Satellite.
MGS words can define all the rest of the dictionary. They are learned earlier,
more concrete and more frequent than the rest of the dictionary. Satellite
words, not correlated with age or frequency, are less concrete (more abstract)
words that are also needed for full lexical power."
"Discourse analysis may seek to characterize not only the overall composition
of a given text but also the dynamic patterns within the data. This technical
report introduces a data format intended to facilitate multi-level
investigations, which we call the by-word long-form or B(eo)W(u)LF. Inspired by
the long-form data format required for mixed-effects modeling, B(eo)W(u)LF
structures linguistic data into an expanded matrix encoding any number of
researchers-specified markers, making it ideal for recurrence-based analyses.
While we do not necessarily claim to be the first to use methods along these
lines, we have created a series of tools utilizing Python and MATLAB to enable
such discourse analyses and demonstrate them using 319 lines of the Old English
epic poem, Beowulf, translated into modern English."
"In this paper we introduce a method to detect words or phrases in a given
sequence of alphabets without knowing the lexicon. Our linear time unsupervised
algorithm relies entirely on statistical relationships among alphabets in the
input sequence to detect location of word boundaries. We compare our algorithm
to previous approaches from unsupervised sequence segmentation literature and
provide superior segmentation over number of benchmarks."
"By investigating the distribution of phrase pairs in phrase translation
tables, the work in this paper describes an approach to increase the number of
n-gram alignments in phrase translation tables output by a sampling-based
alignment method. This approach consists in enforcing the alignment of n-grams
in distinct translation subtables so as to increase the number of n-grams.
Standard normal distribution is used to allot alignment time among translation
subtables, which results in adjustment of the distribution of n- grams. This
leads to better evaluation results on statistical machine translation tasks
than the original sampling-based alignment approach. Furthermore, the
translation quality obtained by merging phrase translation tables computed from
the sampling-based alignment method and from MGIZA++ is examined."
"Stemming is the process of extracting root word from the given inflection
word. It also plays significant role in numerous application of Natural
Language Processing (NLP). The stemming problem has addressed in many contexts
and by researchers in many disciplines. This expository paper presents survey
of some of the latest developments on stemming algorithms in data mining and
also presents with some of the solutions for various Indian language stemming
algorithms along with the results."
"This text is a conceptual introduction to mixed effects modeling with
linguistic applications, using the R programming environment. The reader is
introduced to linear modeling and assumptions, as well as to mixed
effects/multilevel modeling, including a discussion of random intercepts,
random slopes and likelihood ratio tests. The example used throughout the text
focuses on the phonetic analysis of voice pitch data."
"In this paper, we describe how we created two state-of-the-art SVM
classifiers, one to detect the sentiment of messages such as tweets and SMS
(message-level task) and one to detect the sentiment of a term within a
submissions stood first in both tasks on tweets, obtaining an F-score of 69.02
in the message-level task and 88.93 in the term-level task. We implemented a
variety of surface-form, semantic, and sentiment features. with sentiment-word
hashtags, and one from tweets with emoticons. In the message-level task, the
lexicon-based features provided a gain of 5 F-score points over all others.
Both of our systems can be replicated us available resources."
"Even though considerable attention has been given to the polarity of words
(positive and negative) and the creation of large polarity lexicons, research
in emotion analysis has had to rely on limited and small emotion lexicons. In
this paper we show how the combined strength and wisdom of the crowds can be
used to generate a large, high-quality, word-emotion and word-polarity
association lexicon quickly and inexpensively. We enumerate the challenges in
emotion annotation in a crowdsourcing scenario and propose solutions to address
them. Most notably, in addition to questions about emotions associated with
terms, we show how the inclusion of a word choice question can discourage
malicious data entry, help identify instances where the annotator may not be
familiar with the target term (allowing us to reject such annotations), and
help obtain annotations at sense level (rather than at word level). We
conducted experiments on how to formulate the emotion-annotation questions, and
show that asking if a term is associated with an emotion leads to markedly
higher inter-annotator agreement than that obtained by asking if a term evokes
an emotion."
"Knowing the degree of semantic contrast between words has widespread
application in natural language processing, including machine translation,
information retrieval, and dialogue systems. Manually-created lexicons focus on
opposites, such as {\rm hot} and {\rm cold}. Opposites are of many kinds such
as antipodals, complementaries, and gradable. However, existing lexicons often
do not classify opposites into the different kinds. They also do not explicitly
list word pairs that are not opposites but yet have some degree of contrast in
meaning, such as {\rm warm} and {\rm cold} or {\rm tropical} and {\rm
freezing}. We propose an automatic method to identify contrasting word pairs
that is based on the hypothesis that if a pair of words, $A$ and $B$, are
contrasting, then there is a pair of opposites, $C$ and $D$, such that $A$ and
$C$ are strongly related and $B$ and $D$ are strongly related. (For example,
there exists the pair of opposites {\rm hot} and {\rm cold} such that {\rm
tropical} is related to {\rm hot,} and {\rm freezing} is related to {\rm
cold}.) We will call this the contrast hypothesis. We begin with a large
crowdsourcing experiment to determine the amount of human agreement on the
concept of oppositeness and its different kinds. In the process, we flesh out
key features of different kinds of opposites. We then present an automatic and
empirical measure of lexical contrast that relies on the contrast hypothesis,
corpus statistics, and the structure of a {\it Roget}-like thesaurus. We show
that the proposed measure of lexical contrast obtains high precision and large
coverage, outperforming existing methods."
"Sentiment polarity classification is perhaps the most widely studied topic.
It classifies an opinionated document as expressing a positive or negative
opinion. In this paper, using movie review dataset, we perform a comparative
study with different single kind linguistic features and the combinations of
these features. We find that the classic topic-based classifier(Naive Bayes and
Support Vector Machine) do not perform as well on sentiment polarity
classification. And we find that with some combination of different linguistic
features, the classification accuracy can be boosted a lot. We give some
reasonable explanations about these boosting outcomes."
"Principal component analysis (PCA) and related techniques have been
successfully employed in natural language processing. Text mining applications
in the age of the online social media (OSM) face new challenges due to
properties specific to these use cases (e.g. spelling issues specific to texts
posted by users, the presence of spammers and bots, service announcements,
etc.). In this paper, we employ a Robust PCA technique to separate typical
outliers and highly localized topics from the low-dimensional structure present
in language use in online social networks. Our focus is on identifying
geospatial features among the messages posted by the users of the Twitter
microblogging service. Using a dataset which consists of over 200 million
geolocated tweets collected over the course of a year, we investigate whether
the information present in word usage frequencies can be used to identify
regional features of language use and topics of interest. Using the PCA pursuit
method, we are able to identify important low-dimensional features, which
constitute smoothly varying functions of the geographic location."
"Tweets pertaining to a single event, such as a national election, can number
in the hundreds of millions. Automatically analyzing them is beneficial in many
downstream natural language applications such as question answering and
summarization. In this paper, we propose a new task: identifying the purpose
behind electoral tweets--why do people post election-oriented tweets? We show
that identifying purpose is correlated with the related phenomenon of sentiment
and emotion detection, but yet significantly different. Detecting purpose has a
number of applications including detecting the mood of the electorate,
estimating the popularity of policies, identifying key issues of contention,
and predicting the course of events. We create a large dataset of electoral
tweets and annotate a few thousand tweets for purpose. We develop a system that
automatically classifies electoral tweets as per their purpose, obtaining an
accuracy of 43.56% on an 11-class task and an accuracy of 73.91% on a 3-class
task (both accuracies well above the most-frequent-class baseline). Finally, we
show that resources developed for emotion detection are also helpful for
detecting purpose."
"In this paper, we explore a set of novel features for authorship attribution
of documents. These features are derived from a word network representation of
natural language text. As has been noted in previous studies, natural language
tends to show complex network structure at word level, with low degrees of
separation and scale-free (power law) degree distribution. There has also been
work on authorship attribution that incorporates ideas from complex networks.
The goal of our paper is to explore properties of these complex networks that
are suitable as features for machine-learning-based authorship attribution of
documents. We performed experiments on three different datasets, and obtained
promising results."
"The Cornell Semantic Parsing Framework (SPF) is a learning and inference
framework for mapping natural language to formal representation of its meaning."
"Machine translation evaluation is a very important activity in machine
translation development. Automatic evaluation metrics proposed in literature
are inadequate as they require one or more human reference translations to
compare them with output produced by machine translation. This does not always
give accurate results as a text can have several different translations. Human
evaluation metrics, on the other hand, lacks inter-annotator agreement and
repeatability. In this paper we have proposed a new human evaluation metric
which addresses these issues. Moreover this metric also provides solid grounds
for making sound assumptions on the quality of the text produced by a machine
translation."
"Since long, research on machine translation has been ongoing. Still, we do
not get good translations from MT engines so developed. Manual ranking of these
outputs tends to be very time consuming and expensive. Identifying which one is
better or worse than the others is a very taxing task. In this paper, we show
an approach which can provide automatic ranks to MT outputs (translations)
taken from different MT Engines and which is based on N-gram approximations. We
provide a solution where no human intervention is required for ranking systems.
Further we also show the evaluations of our results which show equivalent
results as that of human ranking."
"There are many known Arabic lexicons organized on different ways, each of
them has a different number of Arabic words according to its organization way.
This paper has used mathematical relations to count a number of Arabic words,
which proofs the number of Arabic words presented by Al Farahidy. The paper
also presents new way to build an electronic Arabic lexicon by using a hash
function that converts each word (as input) to correspond a unique integer
number (as output), these integer numbers will be used as an index to a lexicon
entry."
"In this paper, we briefly introduce the Narrative Information Linear
Extraction (NILE) system, a natural language processing library for clinical
narratives. NILE is an experiment of our ideas on efficient and effective
medical language processing. We introduce the overall design of NILE and its
major components, and show the performance of it in real projects."
"This paper presents a novel semantic-based phrase translation model. A pair
of source and target phrases are projected into continuous-valued vector
representations in a low-dimensional latent semantic space, where their
translation score is computed by the distance between the pair in this new
space. The projection is performed by a multi-layer neural network whose
weights are learned on parallel training data. The learning is aimed to
directly optimize the quality of end-to-end machine translation results.
Experimental evaluation has been performed on two Europarl translation tasks,
English-French and German-English. The results show that the new semantic-based
phrase translation model significantly improves the performance of a
state-of-the-art phrase-based statistical machine translation sys-tem, leading
to a gain of 0.7-1.0 BLEU points."
"The author describes a conceptual study towards mapping grounded natural
language discourse representation structures to instances of controlled
language statements. This can be achieved via a pipeline of preexisting state
of the art technologies, namely natural language syntax to semantic discourse
mapping, and a reduction of the latter to controlled language discourse, given
a set of previously learnt reduction rules. Concludingly a description on
evaluation, potential and limitations for ontology-based reasoning is
presented."
"We propose a new benchmark corpus to be used for measuring progress in
statistical language modeling. With almost one billion words of training data,
we hope this benchmark will be useful to quickly evaluate novel language
modeling techniques, and to compare their contribution when combined with other
advanced techniques. We show performance of several well-known types of
language models, with the best results achieved with a recurrent neural network
based language model. The baseline unpruned Kneser-Ney 5-gram model achieves
perplexity 67.6; a combination of techniques leads to 35% reduction in
perplexity, or 10% reduction in cross-entropy (bits), over that baseline.
  The benchmark is available as a code.google.com project; besides the scripts
needed to rebuild the training/held-out data, it also makes available
log-probability values for each word in each of ten held-out data sets, for
each of the baseline n-gram models."
"We propose a cognitively and linguistically motivated set of sorts for
lexical semantics in a compositional setting: the classifiers in languages that
do have such pronouns. These sorts are needed to include lexical considerations
in a semantical analyser such as Boxer or Grail. Indeed, all proposed lexical
extensions of usual Montague semantics to model restriction of selection,
felicitous and infelicitous copredication require a rich and refined type
system whose base types are the lexical sorts, the basis of the many-sorted
logic in which semantical representations of sentences are stated. However,
none of those approaches define precisely the actual base types or sorts to be
used in the lexicon. In this article, we shall discuss some of the options
commonly adopted by researchers in formal lexical semantics, and defend the
view that classifiers in the languages which have such pronouns are an
appealing solution, both linguistically and cognitively motivated."
"For any deep computational processing of language we need evidences, and one
such set of evidences is corpus. This paper describes the development of a
text-based corpus for the Bishnupriya Manipuri language. A Corpus is considered
as a building block for any language processing tasks. Due to the lack of
awareness like other Indian languages, it is also studied less frequently. As a
result the language still lacks a good corpus and basic language processing
tools. As per our knowledge this is the first effort to develop a corpus for
Bishnupriya Manipuri language."
"So far and trying to reach human capabilities, research in automatic
summarization has been based on hypothesis that are both enabling and limiting.
Some of these limitations are: how to take into account and reflect (in the
generated summary) the implicit information conveyed in the text, the author
intention, the reader intention, the context influence, the general world
knowledge. Thus, if we want machines to mimic human abilities, then they will
need access to this same large variety of knowledge. The implicit is affecting
the orientation and the argumentation of the text and consequently its summary.
Most of Text Summarizers (TS) are processing as compressing the initial data
and they necessarily suffer from information loss. TS are focusing on features
of the text only, not on what the author intended or why the reader is reading
the text. In this paper, we address this problem and we present a system
focusing on acquiring knowledge that is implicit. We principally spotlight the
implicit information conveyed by the argumentative connectives such as: but,
even, yet and their effect on the summary."
"Deep learning embeddings have been successfully used for many natural
language processing problems. Embeddings are mostly computed for word forms
although a number of recent papers have extended this to other linguistic units
like morphemes and phrases. In this paper, we argue that learning embeddings
for discontinuous linguistic units should also be considered. In an
experimental evaluation on coreference resolution, we show that such embeddings
perform better than word form embeddings."
"There are two main approaches to the distributed representation of words:
low-dimensional deep learning embeddings and high-dimensional distributional
models, in which each dimension corresponds to a context word. In this paper,
we combine these two approaches by learning embeddings based on
distributional-model vectors - as opposed to one-hot vectors as is standardly
done in deep learning. We show that the combined approach has better
performance on a word relatedness judgment task."
"Distributed representations of meaning are a natural way to encode covariance
relationships between words and phrases in NLP. By overcoming data sparsity
problems, as well as providing information about semantic relatedness which is
not available in discrete representations, distributed representations have
proven useful in many NLP tasks. Recent work has shown how compositional
semantic representations can successfully be applied to a number of monolingual
applications such as sentiment analysis. At the same time, there has been some
initial success in work on learning shared word-level representations across
languages. We combine these two approaches by proposing a method for learning
distributed representations in a multilingual setup. Our model learns to assign
similar embeddings to aligned sentences and dissimilar ones to sentence which
are not aligned while not requiring word alignments. We show that our
representations are semantically informative and apply them to a cross-lingual
document classification task where we outperform the previous state of the art.
Further, by employing parallel corpora of multiple language pairs we find that
our model learns representations that capture semantic relationships across
languages for which no parallel data was used."
"In this paper we present an approach for estimating the quality of machine
translation system. There are various methods for estimating the quality of
output sentences, but in this paper we focus on Na\""ive Bayes classifier to
build model using features which are extracted from the input sentences. These
features are used for finding the likelihood of each of the sentences of the
training data which are then further used for determining the scores of the
test data. On the basis of these scores we determine the class labels of the
test data."
"Although, Chinese and Spanish are two of the most spoken languages in the
world, not much research has been done in machine translation for this language
pair. This paper focuses on investigating the state-of-the-art of
Chinese-to-Spanish statistical machine translation (SMT), which nowadays is one
of the most popular approaches to machine translation. For this purpose, we
report details of the available parallel corpus which are Basic Traveller
Expressions Corpus (BTEC), Holy Bible and United Nations (UN). Additionally, we
conduct experimental work with the largest of these three corpora to explore
alternative SMT strategies by means of using a pivot language. Three
alternatives are considered for pivoting: cascading, pseudo-corpus and
triangulation. As pivot language, we use either English, Arabic or French.
Results show that, for a phrase-based SMT system, English is the best pivot
language between Chinese and Spanish. We propose a system output combination
using the pivot strategies which is capable of outperforming the direct
translation strategy. The main objective of this work is motivating and
involving the research community to work in this important pair of languages
given their demographic impact."
"Many natural language processing (NLP) applications require the computation
of similarities between pairs of syntactic or semantic trees. Many researchers
have used tree edit distance for this task, but this technique suffers from the
drawback that it deals with single node operations only. We have extended the
standard tree edit distance algorithm to deal with subtree transformation
operations as well as single nodes. The extended algorithm with subtree
operations, TED+ST, is more effective and flexible than the standard algorithm,
especially for applications that pay attention to relations among nodes (e.g.
in linguistic trees, deleting a modifier subtree should be cheaper than the sum
of deleting its components individually). We describe the use of TED+ST for
checking entailment between two Arabic text snippets. The preliminary results
of using TED+ST were encouraging when compared with two string-based approaches
and with the standard algorithm."
"Topic segmentation and labeling is often considered a prerequisite for
higher-level conversation analysis and has been shown to be useful in many
Natural Language Processing (NLP) applications. We present two new corpora of
email and blog conversations annotated with topics, and evaluate annotator
reliability for the segmentation and labeling tasks in these asynchronous
conversations. We propose a complete computational framework for topic
segmentation and labeling in asynchronous conversations. Our approach extends
state-of-the-art methods by considering a fine-grained structure of an
asynchronous conversation, along with other conversational features by applying
recent graph-based methods for NLP. For topic segmentation, we propose two
novel unsupervised models that exploit the fine-grained conversational
structure, and a novel graph-theoretic supervised model that combines lexical,
conversational and topic features. For topic labeling, we propose two novel
(unsupervised) random walk models that respectively capture conversation
specific clues from two different sources: the leading sentences and the
fine-grained conversational structure. Empirical evaluation shows that the
segmentation and the labeling performed by our best models beat the
state-of-the-art, and are highly correlated with human annotations."
"Bilingual machine-readable dictionaries are knowledge resources useful in
many automatic tasks. However, compared to monolingual computational lexicons
like WordNet, bilingual dictionaries typically provide a lower amount of
structured information, such as lexical and semantic relations, and often do
not cover the entire range of possible translations for a word of interest. In
this paper we present Cycles and Quasi-Cycles (CQC), a novel algorithm for the
automated disambiguation of ambiguous translations in the lexical entries of a
bilingual machine-readable dictionary. The dictionary is represented as a
graph, and cyclic patterns are sought in the graph to assign an appropriate
sense tag to each translation in a lexical entry. Further, we use the
algorithms output to improve the quality of the dictionary itself, by
suggesting accurate solutions to structural problems such as misalignments,
partial alignments and missing entries. Finally, we successfully apply CQC to
the task of synonym extraction."
"We present PR2, a personality recognition system available online, that
performs instance-based classification of Big5 personality types from
unstructured text, using language-independent features. It has been tested on
English and Italian, achieving performances up to f=.68."
"Module-Attribute Representation of Verbal Semantics (MARVS) is a theory of
the representation of verbal semantics that is based on Mandarin Chinese data
(Huang et al. 2000). In the MARVS theory, there are two different types of
modules: Event Structure Modules and Role Modules. There are also two sets of
attributes: Event-Internal Attributes and Role-Internal Attributes, which are
linked to the Event Structure Module and the Role Module, respectively. In this
study, we focus on four transitive verbs as chi1(eat), wan2(play),
huan4(change) and shao1(burn) and explore their event structures by the MARVS
theory."
"In geographic information science and semantics, the computation of semantic
similarity is widely recognised as key to supporting a vast number of tasks in
information integration and retrieval. By contrast, the role of geo-semantic
relatedness has been largely ignored. In natural language processing, semantic
relatedness is often confused with the more specific semantic similarity. In
this article, we discuss a notion of geo-semantic relatedness based on Lehrer's
semantic fields, and we compare it with geo-semantic similarity. We then
describe and validate the Geo Relatedness and Similarity Dataset (GeReSiD), a
new open dataset designed to evaluate computational measures of geo-semantic
relatedness and similarity. This dataset is larger than existing datasets of
this kind, and includes 97 geographic terms combined into 50 term pairs rated
by 203 human subjects. GeReSiD is available online and can be used as an
evaluation baseline to determine empirically to what degree a given
computational model approximates geo-semantic relatedness and similarity."
"This paper presents machine learning solutions to a practical problem of
Natural Language Generation (NLG), particularly the word formation in
agglutinative languages like Tamil, in a supervised manner. The morphological
generator is an important component of Natural Language Processing in
Artificial Intelligence. It generates word forms given a root and affixes. The
morphophonemic changes like addition, deletion, alternation etc., occur when
two or more morphemes or words joined together. The Sandhi rules should be
explicitly specified in the rule based morphological analyzers and generators.
In machine learning framework, these rules can be learned automatically by the
system from the training samples and subsequently be applied for new inputs. In
this paper we proposed the machine learning models which learn the
morphophonemic rules for noun declensions from the given training data. These
models are trained to learn sandhi rules using various learning algorithms and
the performance of those algorithms are presented. From this we conclude that
machine learning of morphological processing such as word form generation can
be successfully learned in a supervised manner, without explicit description of
rules. The performance of Decision trees and Bayesian machine learning
algorithms on noun declensions are discussed."
"A Verbal Autopsy is the record of an interview about the circumstances of an
uncertified death. In developing countries, if a death occurs away from health
facilities, a field-worker interviews a relative of the deceased about the
circumstances of the death; this Verbal Autopsy can be reviewed off-site. We
report on a comparative study of the processes involved in Text Classification
applied to classifying Cause of Death: feature value representation; machine
learning classification algorithms; and feature reduction strategies in order
to identify the suitable approaches applicable to the classification of Verbal
Autopsy text. We demonstrate that normalised term frequency and the standard
TFiDF achieve comparable performance across a number of classifiers. The
results also show Support Vector Machine is superior to other classification
algorithms employed in this research. Finally, we demonstrate the effectiveness
of employing a ""locally-semi-supervised"" feature reduction strategy in order to
increase performance accuracy."
"We present a new algorithm to model and investigate the learning process of a
learner mastering a set of grammatical rules from an inconsistent source. The
compelling interest of human language acquisition is that the learning succeeds
in virtually every case, despite the fact that the input data are formally
inadequate to explain the success of learning. Our model explains how a learner
can successfully learn from or even surpass its imperfect source without
possessing any additional biases or constraints about the types of patterns
that exist in the language. We use the data collected by Singleton and Newport
(2004) on the performance of a 7-year boy Simon, who mastered the American Sign
Language (ASL) by learning it from his parents, both of whom were imperfect
speakers of ASL. We show that the algorithm possesses a frequency-boosting
property, whereby the frequency of the most common form of the source is
increased by the learner. We also explain several key features of Simon's ASL."
"Automatically inducing the syntactic part-of-speech categories for words in
text is a fundamental task in Computational Linguistics. While the performance
of unsupervised tagging models has been slowly improving, current
state-of-the-art systems make the obviously incorrect assumption that all
tokens of a given word type must share a single part-of-speech tag. This
one-tag-per-type heuristic counters the tendency of Hidden Markov Model based
taggers to over generate tags for a given word type. However, it is clearly
incompatible with basic syntactic theory. In this paper we extend a
state-of-the-art Pitman-Yor Hidden Markov Model tagger with an explicit model
of the lexicon. In doing so we are able to incorporate a soft bias towards
inducing few tags per type. We develop a particle filter for drawing samples
from the posterior of our model and present empirical results that show that
our model is competitive with and faster than the state-of-the-art without
making any unrealistic restrictions."
"The textual, big-data literature misses Bentley, OBrien, & Brocks (Bentley et
als) message on distributions; it largely examines the first-order effects of
how a single, signature distribution can predict population behaviour,
neglecting second-order effects involving distributional shifts, either between
signature distributions or within a given signature distribution. Indeed,
Bentley et al. themselves under-emphasise the potential richness of the latter,
within-distribution effects."
"Over the past 50 years many have debated what representation should be used
to capture the meaning of natural language utterances. Recently new needs of
such representations have been raised in research. Here I survey some of the
interesting representations suggested to answer for these new needs."
"This paper presents an attempt to customise the TEI (Text Encoding
Initiative) guidelines in order to offer the possibility to incorporate TBX
(TermBase eXchange) based terminological entries within any kind of TEI
documents. After presenting the general historical, conceptual and technical
contexts, we describe the various design choices we had to take while creating
this customisation, which in turn have led to make various changes in the
actual TBX serialisation. Keeping in mind the objective to provide the TEI
guidelines with, again, an onomasiological model, we try to identify the best
comprise in maintaining both the isomorphism with the existing TBX Basic
standard and the characteristics of the TEI framework."
"Modalities of communication for human beings are gradually increasing in
number with the advent of new forms of technology. Many human beings can
readily transition between these different forms of communication with little
or no effort, which brings about the question: How similar are these different
communication modalities? To understand technology$\text{'}$s influence on
English communication, four different corpora were analyzed and compared:
Writing from Books using the 1-grams database from the Google Books project,
Twitter, IRC Chat, and transcribed Talking. Multi-word confusion matrices
revealed that Talking has the most similarity when compared to the other modes
of communication, while 1-grams were the least similar form of communication
analyzed. Based on the analysis of word usage, word usage frequency
distributions, and word class usage, among other things, Talking is also the
most similar to Twitter and IRC Chat. This suggests that communicating using
Twitter and IRC Chat evolved from Talking rather than Writing. When we
communicate online, even though we are writing, we do not Tweet or Chat how we
write books; we Tweet and Chat how we Speak. Nonfiction and Fiction writing
were clearly differentiable from our analysis with Twitter and Chat being much
more similar to Fiction than Nonfiction writing. These hypotheses were then
tested using author and journalists Cory Doctorow. Mr. Doctorow$\text{'}$s
Writing, Twitter usage, and Talking were all found to have very similar
vocabulary usage patterns as the amalgamized populations, as long as the
writing was Fiction. However, Mr. Doctorow$\text{'}$s Nonfiction writing is
different from 1-grams and other collected Nonfiction writings. This data could
perhaps be used to create more entertaining works of Nonfiction."
"Developments in the educational landscape have spurred greater interest in
the problem of automatically scoring short answer questions. A recent shared
task on this topic revealed a fundamental divide in the modeling approaches
that have been applied to this problem, with the best-performing systems split
between those that employ a knowledge engineering approach and those that
almost solely leverage lexical information (as opposed to higher-level
syntactic information) in assigning a score to a given response. This paper
aims to introduce the NLP community to the largest corpus currently available
for short-answer scoring, provide an overview of methods used in the shared
task using this data, and explore the extent to which more
syntactically-informed features can contribute to the short answer scoring task
in a way that avoids the question-specific manual effort of the knowledge
engineering approach."
"Specificity is important for extracting collocations, keyphrases, multi-word
and index terms [Newman et al. 2012]. It is also useful for tagging, ontology
construction [Ryu and Choi 2006], and automatic summarization of documents
[Louis and Nenkova 2011, Chali and Hassan 2012]. Term frequency and
inverse-document frequency (TF-IDF) are typically used to do this, but fail to
take advantage of the semantic relationships between terms [Church and Gale
1995]. The result is that general idiomatic terms are mistaken for specific
terms. We demonstrate use of relational data for estimation of term
specificity. The specificity of a term can be learned from its distribution of
relations with other terms. This technique is useful for identifying relevant
words or terms for other natural language processing tasks."
"We present a system, TransProse, that automatically generates musical pieces
from text. TransProse uses known relations between elements of music such as
tempo and scale, and the emotions they evoke. Further, it uses a novel
mechanism to determine sequences of notes that capture the emotional activity
in the text. The work has applications in information visualization, in
creating audio-visual e-books, and in developing music apps."
"In this paper, a novel hierarchical Persian stemming approach based on the
Part-Of-Speech of the word in a sentence is presented. The implemented stemmer
includes hash tables and several deterministic finite automata in its different
levels of hierarchy for removing the prefixes and suffixes of the words. We had
two intentions in using hash tables in our method. The first one is that the
DFA don't support some special words, so hash table can partly solve the
addressed problem. the second goal is to speed up the implemented stemmer with
omitting the time that deterministic finite automata need. Because of the
hierarchical organization, this method is fast and flexible enough. Our
experiments on test sets from Hamshahri collection and security news (istna.ir)
show that our method has the average accuracy of 95.37% which is even improved
in using the method on a test set with common topics."
"Language is contextual and sheaf theory provides a high level mathematical
framework to model contextuality. We show how sheaf theory can model the
contextual nature of natural language and how gluing can be used to provide a
global semantics for a discourse by putting together the local logical
semantics of each sentence within the discourse. We introduce a presheaf
structure corresponding to a basic form of Discourse Representation Structures.
Within this setting, we formulate a notion of semantic unification --- gluing
meanings of parts of a discourse into a coherent whole --- as a form of
sheaf-theoretic gluing. We illustrate this idea with a number of examples where
it can used to represent resolutions of anaphoric references. We also discuss
multivalued gluing, described using a distributions functor, which can be used
to represent situations where multiple gluings are possible, and where we may
need to rank them using quantitative measures.
  Dedicated to Jim Lambek on the occasion of his 90th birthday."
"Much of philosophical logic and all of philosophy of language make empirical
claims about the vernacular natural language. They presume semantics under
which `and' and `or' are related by the dually paired distributive and
absorption laws. However, at least one of each pair of laws fails in the
vernacular. `Implicature'-based auxiliary theories associated with the
programme of H.P. Grice do not prove remedial. Conceivable alternatives that
might replace the familiar logics as descriptive instruments are briefly noted:
(i) substructural logics and (ii) meaning composition in linear algebras over
the reals, occasionally constrained by norms of classical logic. Alternative
(ii) locates the problem in violations of one of the idempotent laws. Reasons
for a lack of curiosity about elementary and easily testable implications of
the received theory are considered. The concept of `reflective equilibrium' is
critically examined for its role in reconciling normative desiderata and
descriptive commitments."
"We propose a new similarity measure between texts which, contrary to the
current state-of-the-art approaches, takes a global view of the texts to be
compared. We have implemented a tool to compute our textual distance and
conducted experiments on several corpuses of texts. The experiments show that
our methods can reliably identify different global types of texts."
"Sign Language (SL) linguistic is dependent on the expensive task of
annotating. Some automation is already available for low-level information (eg.
body part tracking) and the lexical level has shown significant progresses. The
syntactic level lacks annotated corpora as well as complete and consistent
models. This article presents a solution for the automatic annotation of SL
syntactic elements. It exposes a formalism able to represent both
constituency-based and dependency-based models. The first enable the
representation the structures one may want to annotate, the second aims at
fulfilling the holes of the first. A parser is presented and used to conduct
two experiments on the solution. One experiment is on a real corpus, the other
is on a synthetic corpus."
"Sign Language (SL) automatic processing slowly progresses bottom-up. The
field has seen proposition to handle the video signal, to recognize and
synthesize sublexical and lexical units. It starts to see the development of
supra-lexical processing. But the recognition, at this level, lacks data. The
syntax of SL appears very specific as it uses massively the multiplicity of
articulators and its access to the spatial dimensions. Therefore new parsing
techniques are developed. However these need to be evaluated. The shortage on
real data restrains the corpus-based models to small sizes. We propose here a
solution to produce data-sets for the evaluation of parsers on the specific
properties of SL. The article first describes the general model used to
generates dependency grammars and the phrase generation from these lasts. It
then discusses the limits of approach. The solution shows to be of particular
interest to evaluate the scalability of the techniques on big models."
"Statistical error Correction technique is the most accurate and widely used
approach today, but for a language like Sindhi which is a low resourced
language the trained corpora's are not available, so the statistical techniques
are not possible at all. Instead a useful alternative would be to exploit
various spelling error trends in Sindhi by using a Rule based approach. For
designing such technique an essential prerequisite would be to study the
various error patterns in a language. This pa per presents various studies of
spelling error trends and their types in Sindhi Language. The research shows
that the error trends common to all languages are also encountered in Sindhi
but their do exist some error patters that are catered specifically to a Sindhi
language."
"Motivation: Entropy measurements on hierarchical structures have been used in
methods for information retrieval and natural language modeling. Here we
explore its application to semantic similarity. By finding shared ontology
terms, semantic similarity can be established between annotated genes. A common
procedure for establishing semantic similarity is to calculate the
descriptiveness (information content) of ontology terms and use these values to
determine the similarity of annotations. Most often information content is
calculated for an ontology term by analyzing its frequency in an annotation
corpus. The inherent problems in using these values to model functional
similarity motivates our work. Summary: We present a novel calculation for
establishing the entropy of a DAG-based ontology, which can be used in an
alternative method for establishing the information content of its terms. We
also compare our IC metric to two others using semantic and sequence
similarity."
"We describe the Clinical TempEval task which is currently in preparation for
the SemEval-2015 evaluation exercise. This task involves identifying and
describing events, times and the relations between them in clinical text. Six
discrete subtasks are included, focusing on recognising mentions of times and
events, describing those mentions for both entity types, identifying the
relation between an event and the document creation time, and identifying
narrative container relations."
"Natural language processing is a prompt research area across the country.
Parsing is one of the very crucial tool in language analysis system which aims
to forecast the structural relationship among the words in a given sentence.
Many researchers have already developed so many language tools but the accuracy
is not meet out the human expectation level, thus the research is still exists.
Machine translation is one of the major application area under Natural Language
Processing. While translation between one language to another language, the
structure identification of a sentence play a key role. This paper introduces
the hybrid way to solve the identification of relationship among the given
words in a sentence. In existing system is implemented using rule based
approach, which is not suited in huge amount of data. The machine learning
approaches is suitable for handle larger amount of data and also to get better
accuracy via learning and training the system. The proposed approach takes a
Tamil sentence as an input and produce the result of a dependency relation as a
tree like structure using hybrid approach. This proposed tool is very helpful
for researchers and act as an odd-on improve the quality of existing
approaches."
"In this paper, we present the implementation of an automatic Sign Language
(SL) sign annotation framework based on a formal logic, the Propositional
Dynamic Logic (PDL). Our system relies heavily on the use of a specific variant
of PDL, the Propositional Dynamic Logic for Sign Language (PDLSL), which lets
us describe SL signs as formulae and corpora videos as labeled transition
systems (LTSs). Here, we intend to show how a generic annotation system can be
constructed upon these underlying theoretical principles, regardless of the
tracking technologies available or the input format of corpora. With this in
mind, we generated a development framework that adapts the system to specific
use cases. Furthermore, we present some results obtained by our application
when adapted to one distinct case, 2D corpora analysis with pre-processed
tracking information. We also present some insights on how such a technology
can be used to analyze 3D real-time data, captured with a depth device."
"This paper explores the use of Propositional Dynamic Logic (PDL) as a
suitable formal framework for describing Sign Language (SL), the language of
deaf people, in the context of natural language processing. SLs are visual,
complete, standalone languages which are just as expressive as oral languages.
Signs in SL usually correspond to sequences of highly specific body postures
interleaved with movements, which make reference to real world objects,
characters or situations. Here we propose a formal representation of SL signs,
that will help us with the analysis of automatically-collected hand tracking
data from French Sign Language (FSL) video corpora. We further show how such a
representation could help us with the design of computer aided SL verification
tools, which in turn would bring us closer to the development of an automatic
recognition system for these languages."
"Machine translation (MT) research in Indian languages is still in its
infancy. Not much work has been done in proper transliteration of name entities
in this domain. In this paper we address this issue. We have used English-Hindi
language pair for our experiments and have used a hybrid approach. At first we
have processed English words using a rule based approach which extracts
individual phonemes from the words and then we have applied statistical
approach which converts the English into its equivalent Hindi phoneme and in
turn the corresponding Hindi word. Through this approach we have attained
83.40% accuracy."
"We describe a contextual parser for the Robot Commands Treebank, a new
crowdsourced resource. In contrast to previous semantic parsers that select the
most-probable parse, we consider the different problem of parsing using
additional situational context to disambiguate between different readings of a
sentence. We show that multiple semantic analyses can be searched using dynamic
programming via interaction with a spatial planner, to guide the parsing
process. We are able to parse sentences in near linear-time by ruling out
analyses early on that are incompatible with spatial context. We report a 34%
upper bound on accuracy, as our planner correctly processes spatial context for
3,394 out of 10,000 sentences. However, our parser achieves a 96.53%
exact-match score for parsing within the subset of sentences recognized by the
planner, compared to 82.14% for a non-contextual parser."
"We present an approach to the extraction of family relations from literary
narrative, which incorporates a technique for utterance attribution proposed
recently by Elson and McKeown (2010). In our work this technique is used in
combination with the detection of vocatives - the explicit forms of address
used by the characters in a novel. We take advantage of the fact that certain
vocatives indicate family relations between speakers. The extracted relations
are then propagated using a set of rules. We report the results of the
application of our method to Jane Austen's Pride and Prejudice."
"Named Entities (NEs) are often written with no orthographic changes across
different languages that share a common alphabet. We show that this can be
leveraged so as to improve named entity recognition (NER) by using unsupervised
word clusters from secondary languages as features in state-of-the-art
discriminative NER systems. We observe significant increases in performance,
finding that person and location identification is particularly improved, and
that phylogenetically close languages provide more valuable features than more
distant languages."
"We present a probabilistic model that simultaneously learns alignments and
distributed representations for bilingual data. By marginalizing over word
alignments the model captures a larger semantic context than prior work relying
on hard alignments. The advantage of this approach is demonstrated in a
cross-lingual classification task, where we outperform the prior published
state of the art."
"It is now widely recognized that ontologies, are one of the fundamental
cornerstones of knowledge-based systems. What is lacking, however, is a
currently accepted strategy of how to build ontology; what kinds of the
resources and techniques are indispensables to optimize the expenses and the
time on the one hand and the amplitude, the completeness, the robustness of en
ontology on the other hand. The paper offers a semi-automatic ontology
construction method from text corpora in the domain of radiological protection.
This method is composed from next steps: 1) text annotation with part-of-speech
tags; 2) revelation of the significant linguistic structures and forming the
templates; 3) search of text fragments corresponding to these templates; 4)
basic ontology instantiation process"
"Conjuring up our thoughts, language reflects statistical patterns of word
co-occurrences which in turn come to describe how we perceive the world.
Whether counting how frequently nouns and verbs combine in Google search
queries, or extracting eigenvectors from term document matrices made up of
Wikipedia lines and Shakespeare plots, the resulting latent semantics capture
not only the associative links which form concepts, but also spatial dimensions
embedded within the surface structure of language. As both the shape and
movements of objects have been found to be associated with phonetic contrasts
already in toddlers, this study explores whether articulatory and acoustic
parameters may likewise differentiate the latent semantics of action verbs.
Selecting 3 x 20 emotion, face, and hand related verbs known to activate
premotor areas in the brain, their mutual cosine similarities were computed
using latent semantic analysis LSA, and the resulting adjacency matrices were
compared based on two different large scale text corpora; HAWIK and TASA.
Applying hierarchical clustering to identify common structures across the two
text corpora, the verbs largely divide into combined mouth and hand movements
versus emotional expressions. Transforming the verbs into their constituent
phonemes, the clustered small and large size movements appear differentiated by
front versus back vowels corresponding to increasing levels of arousal. Whereas
the clustered emotional verbs seem characterized by sequences of close versus
open jaw produced phonemes, generating up- or downwards shifts in formant
frequencies that may influence their perceived valence. Suggesting, that the
latent semantics of action verbs reflect parameters of intensity and emotional
polarity that appear correlated with the articulatory contrasts and acoustic
characteristics of phonemes"
"Word sense disambiguation (WSD) is a problem in the field of computational
linguistics given as finding the intended sense of a word (or a set of words)
when it is activated within a certain context. WSD was recently addressed as a
combinatorial optimization problem in which the goal is to find a sequence of
senses that maximize the semantic relatedness among the target words. In this
article, a novel algorithm for solving the WSD problem called D-Bees is
proposed which is inspired by bee colony optimization (BCO)where artificial bee
agents collaborate to solve the problem. The D-Bees algorithm is evaluated on a
standard dataset (SemEval 2007 coarse-grained English all-words task corpus)and
is compared to simulated annealing, genetic algorithms, and two ant colony
optimization techniques (ACO). It will be observed that the BCO and ACO
approaches are on par."
"The strength with which a statement is made can have a significant impact on
the audience. For example, international relations can be strained by how the
media in one country describes an event in another; and papers can be rejected
because they overstate or understate their findings. It is thus important to
understand the effects of statement strength. A first step is to be able to
distinguish between strong and weak statements. However, even this problem is
understudied, partly due to a lack of data. Since strength is inherently
relative, revisions of texts that make claims are a natural source of data on
strength differences. In this paper, we introduce a corpus of sentence-level
revisions from academic writing. We also describe insights gained from our
annotation efforts for this task."
"In this work we present our expert system of Automatic reading or speech
synthesis based on a text written in Standard Arabic, our work is carried out
in two great stages: the creation of the sound data base, and the
transformation of the written text into speech (Text To Speech TTS). This
transformation is done firstly by a Phonetic Orthographical Transcription (POT)
of any written Standard Arabic text with the aim of transforming it into his
corresponding phonetics sequence, and secondly by the generation of the voice
signal which corresponds to the chain transcribed. We spread out the different
of conception of the system, as well as the results obtained compared to others
works studied to realize TTS based on Standard Arabic."
"Minimum error rate training (MERT) is a widely used training procedure for
statistical machine translation. A general problem of this approach is that the
search space is easy to converge to a local optimum and the acquired weight set
is not in accord with the real distribution of feature functions. This paper
introduces coordinate system selection (RSS) into the search algorithm for
MERT. Contrary to previous approaches in which every dimension only corresponds
to one independent feature function, we create several coordinate systems by
moving one of the dimensions to a new direction. The basic idea is quite simple
but critical that the training procedure of MERT should be based on a
coordinate system formed by search directions but not directly on feature
functions. Experiments show that by selecting coordinate systems with tuning
set results, better results can be obtained without any other language
knowledge."
"This paper presents a novel combinational phonetic algorithm for Sindhi
Language, to be used in developing Sindhi Spell Checker which has yet not been
developed prior to this work. The compound textual forms and glyphs of Sindhi
language presents a substantial challenge for developing Sindhi spell checker
system and generating similar suggestion list for misspelled words. In order to
implement such a system, phonetic based Sindhi language rules and patterns must
be considered into account for increasing the accuracy and efficiency. The
proposed system is developed with a blend between Phonetic based SoundEx
algorithm and ShapeEx algorithm for pattern or glyph matching, generating
accurate and efficient suggestion list for incorrect or misspelled Sindhi
words. A table of phonetically similar sounding Sindhi characters for SoundEx
algorithm is also generated along with another table containing similar glyph
or shape based character groups for ShapeEx algorithm. Both these are first
ever attempt of any such type of categorization and representation for Sindhi
Language."
"We provide a method for automatically detecting change in language across
time through a chronologically trained neural language model. We train the
model on the Google Books Ngram corpus to obtain word vector representations
specific to each year, and identify words that have changed significantly from
1900 to 2009. The model identifies words such as ""cell"" and ""gay"" as having
changed during that time period. The model simultaneously identifies the
specific years during which such words underwent change."
"We describe INAUT, a controlled natural language dedicated to collaborative
update of a knowledge base on maritime navigation and to automatic generation
of coast pilot books (Instructions nautiques) of the French National
Hydrographic and Oceanographic Service SHOM. INAUT is based on French language
and abundantly uses georeferenced entities. After describing the structure of
the overall system, giving details on the language and on its generation, and
discussing the three major applications of INAUT (document production,
interaction with ENCs and collaborative updates of the knowledge base), we
conclude with future extensions and open problems."
"In recent years, new developments in the area of lexicography have altered
not only the management, processing and publishing of lexicographical data, but
also created new types of products such as electronic dictionaries and
thesauri. These expand the range of possible uses of lexical data and support
users with more flexibility, for instance in assisting human translation. In
this article, we give a short and easy-to-understand introduction to the
problematic nature of the storage, display and interpretation of lexical data.
We then describe the main methods and specifications used to build and
represent lexical data. This paper is targeted for the following groups of
people: linguists, lexicographers, IT specialists, computer linguists and all
others who wish to learn more about the modelling, representation and
visualization of lexical knowledge. This paper is written in two languages:
French and German."
"This paper presents preliminary results of Croatian syllable networks
analysis. Syllable network is a network in which nodes are syllables and links
between them are constructed according to their connections within words. In
this paper we analyze networks of syllables generated from texts collected from
the Croatian Wikipedia and Blogs. As a main tool we use complex network
analysis methods which provide mechanisms that can reveal new patterns in a
language structure. We aim to show that syllable networks have much higher
clustering coefficient in comparison to Erd\""os-Renyi random networks. The
results indicate that Croatian syllable networks exhibit certain properties of
a small world networks. Furthermore, we compared Croatian syllable networks
with Portuguese and Chinese syllable networks and we showed that they have
similar properties."
"We present a natural language modelization method which is strongely relying
on mathematics. This method, called ""Formal Semantics,"" has been initiated by
the American linguist Richard M. Montague in the 1970's. It uses mathematical
tools such as formal languages and grammars, first-order logic, type theory and
$\lambda$-calculus. Our goal is to have the reader discover both Montagovian
formal semantics and the mathematical tools that he used in his method.
  -----
  Nous pr\'esentons une m\'ethode de mod\'elisation de la langue naturelle qui
est fortement bas\'ee sur les math\'ematiques. Cette m\'ethode, appel\'ee
{\guillemotleft}s\'emantique formelle{\guillemotright}, a \'et\'e initi\'ee par
le linguiste am\'ericain Richard M. Montague dans les ann\'ees 1970. Elle
utilise des outils math\'ematiques tels que les langages et grammaires formels,
la logique du 1er ordre, la th\'eorie de types et le $\lambda$-calcul. Nous
nous proposons de faire d\'ecouvrir au lecteur tant la s\'emantique formelle de
Montague que les outils math\'ematiques dont il s'est servi."
"This paper presents a scalable method for integrating compositional
morphological representations into a vector-based probabilistic language model.
Our approach is evaluated in the context of log-bilinear language models,
rendered suitably efficient for implementation inside a machine translation
decoder by factoring the vocabulary. We perform both intrinsic and extrinsic
evaluations, presenting results on a range of languages which demonstrate that
our model learns morphological representations that both perform well on word
similarity tasks and lead to substantial reductions in perplexity. When used
for translation into morphologically rich languages with large vocabularies,
our models obtain improvements of up to 1.2 BLEU points relative to a baseline
system using back-off n-gram models."
"We present an extended, thematically reinforced version of Gabrilovich and
Markovitch's Explicit Semantic Analysis (ESA), where we obtain thematic
information through the category structure of Wikipedia. For this we first
define a notion of categorical tfidf which measures the relevance of terms in
categories. Using this measure as a weight we calculate a maximal spanning tree
of the Wikipedia corpus considered as a directed graph of pages and categories.
This tree provides us with a unique path of ""most related categories"" between
each page and the top of the hierarchy. We reinforce tfidf of words in a page
by aggregating it with categorical tfidfs of the nodes of these paths, and
define a thematically reinforced ESA semantic relatedness measure which is more
robust than standard ESA and less sensitive to noise caused by out-of-context
words. We apply our method to the French Wikipedia corpus, evaluate it through
a text classification on a 37.5 MB corpus of 20 French newsgroups and obtain a
precision increase of 9-10% compared with standard ESA."
"Traditional learning-based coreference resolvers operate by training the
mention-pair model for determining whether two mentions are coreferent or not.
Though conceptually simple and easy to understand, the mention-pair model is
linguistically rather unappealing and lags far behind the heuristic-based
coreference models proposed in the pre-statistical NLP era in terms of
sophistication. Two independent lines of recent research have attempted to
improve the mention-pair model, one by acquiring the mention-ranking model to
rank preceding mentions for a given anaphor, and the other by training the
entity-mention model to determine whether a preceding cluster is coreferent
with a given mention. We propose a cluster-ranking approach to coreference
resolution, which combines the strengths of the mention-ranking model and the
entity-mention model, and is therefore theoretically more appealing than both
of these models. In addition, we seek to improve cluster rankers via two
extensions: (1) lexicalization and (2) incorporating knowledge of anaphoricity
by jointly modeling anaphoricity determination and coreference resolution.
Experimental results on the ACE data sets demonstrate the superior performance
of cluster rankers to competing approaches as well as the effectiveness of our
two extensions."
"Chinese characters have a complex and hierarchical graphical structure
carrying both semantic and phonetic information. We use this structure to
enhance the text model and obtain better results in standard NLP operations.
First of all, to tackle the problem of graphical variation we define
allographic classes of characters. Next, the relation of inclusion of a
subcharacter in a characters, provides us with a directed graph of allographic
classes. We provide this graph with two weights: semanticity (semantic relation
between subcharacter and character) and phoneticity (phonetic relation) and
calculate ""most semantic subcharacter paths"" for each character. Finally,
adding the information contained in these paths to unigrams we claim to
increase the efficiency of text mining methods. We evaluate our method on a
text classification task on two corpora (Chinese and Japanese) of a total of 18
million characters and get an improvement of 3% on an already high baseline of
89.6% precision, obtained by a linear SVM classifier. Other possible
applications and perspectives of the system are discussed."
"Although the parallel corpus has an irreplaceable role in machine
translation, its scale and coverage is still beyond the actual needs.
Non-parallel corpus resources on the web have an inestimable potential value in
machine translation and other natural language processing tasks. This article
proposes a semi-supervised transductive learning method for expanding the
training corpus in statistical machine translation system by extracting
parallel sentences from the non-parallel corpus. This method only requires a
small amount of labeled corpus and a large unlabeled corpus to build a
high-performance classifier, especially for when there is short of labeled
corpus. The experimental results show that by combining the non-parallel corpus
alignment and the semi-supervised transductive learning method, we can more
effectively use their respective strengths to improve the performance of
machine translation system."
"Economic issues related to the information processing techniques are very
important. The development of such technologies is a major asset for developing
countries like Cambodia and Laos, and emerging ones like Vietnam, Malaysia and
Thailand. The MotAMot project aims to computerize an under-resourced language:
Khmer, spoken mainly in Cambodia. The main goal of the project is the
development of a multilingual lexical system targeted for Khmer. The
macrostructure is a pivot one with each word sense of each language linked to a
pivot axi. The microstructure comes from a simplification of the explanatory
and combinatory dictionary. The lexical system has been initialized with data
coming mainly from the conversion of the French-Khmer bilingual dictionary of
Denis Richer from Word to XML format. The French part was completed with
pronunciation and parts-of-speech coming from the FeM French-english-Malay
dictionary. The Khmer headwords noted in IPA in the Richer dictionary were
converted to Khmer writing with OpenFST, a finite state transducer tool. The
resulting resource is available online for lookup, editing, download and remote
programming via a REST API on a Jibiki platform."
"This paper relates work done during the DiLAF project. It consists in
converting 5 bilingual African language-French dictionaries originally in Word
format into XML following the LMF model. The languages processed are Bambara,
Hausa, Kanuri, Tamajaq and Songhai-zarma, still considered as under-resourced
languages concerning Natural Language Processing tools. Once converted, the
dictionaries are available online on the Jibiki platform for lookup and
modification. The DiLAF project is first presented. A description of each
dictionary follows. Then, the conversion methodology from .doc format to XML
files is presented. A specific point on the usage of Unicode follows. Then,
each step of the conversion into XML and LMF is detailed. The last part
presents the Jibiki lexical resources management platform used for the project."
"The technique of building of networks of hierarchies of terms based on the
analysis of chosen text corpora is offered. The technique is based on the
methodology of horizontal visibility graphs. Constructed and investigated
language network, formed on the basis of electronic preprints arXiv on topics
of information retrieval."
"The Swiss avalanche bulletin is produced twice a day in four languages. Due
to the lack of time available for manual translation, a fully automated
translation system is employed, based on a catalogue of predefined phrases and
predetermined rules of how these phrases can be combined to produce sentences.
The system is able to automatically translate such sentences from German into
the target languages French, Italian and English without subsequent
proofreading or correction. Our catalogue of phrases is limited to a small
sublanguage. The reduction of daily translation costs is expected to offset the
initial development costs within a few years. After being operational for two
winter seasons, we assess here the quality of the produced texts based on an
evaluation where participants rate real danger descriptions from both origins,
the catalogue of phrases versus the manually written and translated texts. With
a mean recognition rate of 55%, users can hardly distinguish between the two
types of texts, and give similar ratings with respect to their language
quality. Overall, the output from the catalogue system can be considered
virtually equivalent to a text written by avalanche forecasters and then
manually translated by professional translators. Furthermore, forecasters
declared that all relevant situations were captured by the system with
sufficient accuracy and within the limited time available."
"Name matching between multiple natural languages is an important step in
cross-enterprise integration applications and data mining. It is difficult to
decide whether or not two syntactic values (names) from two heterogeneous data
sources are alternative designation of the same semantic entity (person), this
process becomes more difficult with Arabic language due to several factors
including spelling and pronunciation variation, dialects and special vowel and
consonant distinction and other linguistic characteristics. This paper proposes
a new framework for name matching between the Arabic language and other
languages. The framework uses a dictionary based on a new proposed version of
the Soundex algorithm to encapsulate the recognition of special features of
Arabic names. The framework proposes a new proximity matching algorithm to suit
the high importance of order sensitivity in Arabic name matching. New
performance evaluation metrics are proposed as well. The framework is
implemented and verified empirically in several case studies demonstrating
substantial improvements compared to other well-known techniques found in
literature."
"This paper re-investigates a lexical acquisition system initially developed
for French.We show that, interestingly, the architecture of the system
reproduces and implements the main components of Optimality Theory. However, we
formulate the hypothesis that some of its limitations are mainly due to a poor
representation of the constraints used. Finally, we show how a better
representation of the constraints used would yield better results."
"This paper reports about our work in the ICON 2013 NLP TOOLS CONTEST on Named
Entity Recognition. We submitted runs for Bengali, English, Hindi, Marathi,
Punjabi, Tamil and Telugu. A statistical HMM (Hidden Markov Models) based model
has been used to implement our system. The system has been trained and tested
on the NLP TOOLS CONTEST: ICON 2013 datasets. Our system obtains F-measures of
0.8599, 0.7704, 0.7520, 0.4289, 0.5455, 0.4466, and 0.4003 for Bengali,
English, Hindi, Marathi, Punjabi, Tamil and Telugu respectively."
"We present a novel framework for learning to interpret and generate language
using only perceptual context as supervision. We demonstrate its capabilities
by developing a system that learns to sportscast simulated robot soccer games
in both English and Korean without any language-specific prior knowledge.
Training employs only ambiguous supervision consisting of a stream of
descriptive textual comments and a sequence of events extracted from the
simulation trace. The system simultaneously establishes correspondences between
individual comments and the events that they describe while building a
translation model that supports both parsing and generation. We also present a
novel algorithm for learning which events are worth describing. Human
evaluations of the generated commentaries indicate they are of reasonable
quality and in some cases even on par with those produced by humans for our
limited domain."
"Several messages express opinions about events, products, and services,
political views or even their author's emotional state and mood. Sentiment
analysis has been used in several applications including analysis of the
repercussions of events in social networks, analysis of opinions about products
and services, and simply to better understand aspects of social communication
in Online Social Networks (OSNs). There are multiple methods for measuring
sentiments, including lexical-based approaches and supervised machine learning
methods. Despite the wide use and popularity of some methods, it is unclear
which method is better for identifying the polarity (i.e., positive or
negative) of a message as the current literature does not provide a method of
comparison among existing methods. Such a comparison is crucial for
understanding the potential limitations, advantages, and disadvantages of
popular methods in analyzing the content of OSNs messages. Our study aims at
filling this gap by presenting comparisons of eight popular sentiment analysis
methods in terms of coverage (i.e., the fraction of messages whose sentiment is
identified) and agreement (i.e., the fraction of identified sentiments that are
in tune with ground truth). We develop a new method that combines existing
approaches, providing the best coverage results and competitive agreement. We
also present a free Web service called iFeel, which provides an open API for
accessing and comparing results across different sentiment methods for a given
text."
"Sentence extraction based summarization methods has some limitations as it
doesn't go into the semantics of the document. Also, it lacks the capability of
sentence generation which is intuitive to humans. Here we present a novel
method to summarize text documents taking the process to semantic levels with
the use of WordNet and other resources, and using a technique for sentence
generation. We involve semantic role labeling to get the semantic
representation of text and use of segmentation to form clusters of the related
pieces of text. Picking out the centroids and sentence generation completes the
task. We evaluate our system against human composed summaries and also present
an evaluation done by humans to measure the quality attributes of our
summaries."
"It has been proved that large scale realistic Knowledge Based Machine
Translation applications require acquisition of huge knowledge about language
and about the world. This knowledge is encoded in computational grammars,
lexicons and domain models. Another approach which avoids the need for
collecting and analyzing massive knowledge, is the Example Based approach,
which is the topic of this paper. We show through the paper that using Example
Based in its native form is not suitable for translating into Arabic. Therefore
a modification to the basic approach is presented to improve the accuracy of
the translation process. The basic idea of the new approach is to improve the
technique by which template-based approaches select the appropriate templates."
"Development of a proper names pronunciation lexicon is usually a manual
effort which can not be avoided. Grapheme to phoneme (G2P) conversion modules,
in literature, are usually rule based and work best for non-proper names in a
particular language. Proper names are foreign to a G2P module. We follow an
optimization approach to enable automatic construction of proper names
pronunciation lexicon. The idea is to construct a small orthogonal set of words
(basis) which can span the set of names in a given database. We propose two
algorithms for the construction of this basis. The transcription lexicon of all
the proper names in a database can be produced by the manual transcription of
only the small set of basis words. We first construct a cost function and show
that the minimization of the cost function results in a basis. We derive
conditions for convergence of this cost function and validate them
experimentally on a very large proper name database. Experiments show the
transcription can be achieved by transcribing a set of small number of basis
words. The algorithms proposed are generic and independent of language; however
performance is better if the proper names have same origin, namely, same
language or geographical region."
"IsiZulu is one of the eleven official languages of South Africa and roughly
half the population can speak it. It is the first (home) language for over 10
million people in South Africa. Only a few computational resources exist for
isiZulu and its related Nguni languages, yet the imperative for tool
development exists. We focus on natural language generation, and the grammar
options and preferences in particular, which will inform verbalization of
knowledge representation languages and could contribute to machine translation.
The verbalization pattern specification shows that the grammar rules are
elaborate and there are several options of which one may have preference. We
devised verbalization patterns for subsumption, basic disjointness, existential
and universal quantification, and conjunction. This was evaluated in a survey
among linguists and non-linguists. Some differences between linguists and
non-linguists can be observed, with the former much more in agreement, and
preferences depend on the overall structure of the sentence, such as singular
for subsumption and plural in other cases."
"Recent developments in controlled natural language editors for knowledge
engineering (KE) have given rise to expectations that they will make KE tasks
more accessible and perhaps even enable non-engineers to build knowledge bases.
This exploratory research focussed on novices and experts in knowledge
engineering during their attempts to learn a controlled natural language (CNL)
known as OWL Simplified English and use it to build a small knowledge base.
Participants' behaviours during the task were observed through eye-tracking and
screen recordings. This was an attempt at a more ambitious user study than in
previous research because we used a naturally occurring text as the source of
domain knowledge, and left them without guidance on which information to
select, or how to encode it. We have identified a number of skills
(competencies) required for this difficult task and key problems that authors
face."
"This paper presents a currently bilingual but potentially multilingual
FrameNet-based grammar library implemented in Grammatical Framework. The
contribution of this paper is two-fold. First, it offers a methodological
approach to automatically generate the grammar based on semantico-syntactic
valence patterns extracted from FrameNet-annotated corpora. Second, it provides
a proof of concept for two use cases illustrating how the acquired multilingual
grammar can be exploited in different CNL applications in the domains of arts
and tourism."
"One of the main challenges for building the Semantic web is Ontology
Authoring. Controlled Natural Languages CNLs offer a user friendly means for
non-experts to author ontologies. This paper provides a snapshot of the
state-of-the-art for the core CNLs for ontology authoring and reviews their
respective evaluations."
"Controlled natural languages for industrial application are often regarded as
a response to the challenges of translation and multilingual communication.
This paper presents a quite different approach taken by Koenig & Bauer AG,
where the main goal was the improvement of the authoring process for technical
documentation. Most importantly, this paper explores the notion of a controlled
language and demonstrates how style guides can emerge from non-linguistic
considerations. Moreover, it shows the transition from loose language
recommendations into precise and prescriptive rules and investigates whether
such rules can be regarded as a full-fledged controlled language."
"This paper presents a system which learns to answer questions on a broad
range of topics from a knowledge base using few hand-crafted features. Our
model learns low-dimensional embeddings of words and knowledge base
constituents; these representations are used to score natural language
questions against candidate answers. Training our system using pairs of
questions and structured representations of their answers, and pairs of
question paraphrases, yields competitive results on a competitive benchmark of
the literature."
"In todays digital world automated Machine Translation of one language to
another has covered a long way to achieve different kinds of success stories.
Whereas Babel Fish supports a good number of foreign languages and only Hindi
from Indian languages, the Google Translator takes care of about 10 Indian
languages. Though most of the Automated Machine Translation Systems are doing
well but handling Indian languages needs a major care while handling the local
proverbs/ idioms. Most of the Machine Translation system follows the direct
translation approach while translating one Indian language to other. Our
research at KMIT R&D Lab found that handling the local proverbs/idioms is not
given enough attention by the earlier research work. This paper focuses on two
of the majorly spoken Indian languages Marathi and Telugu, and translation
between them. Handling proverbs and idioms of both the languages have been
given a special care, and the research outcome shows a significant achievement
in this direction."
"In this paper, we describe methods for handling multilingual
non-compositional constructions in the framework of GF. We specifically look at
methods to detect and extract non-compositional phrases from parallel texts and
propose methods to handle such constructions in GF grammars. We expect that the
methods to handle non-compositional constructions will enrich CNLs by providing
more flexibility in the design of controlled languages. We look at two specific
use cases of non-compositional constructions: a general-purpose method to
detect and extract multilingual multiword expressions and a procedure to
identify nominal compounds in German. We evaluate our procedure for multiword
expressions by performing a qualitative analysis of the results. For the
experiments on nominal compounds, we incorporate the detected compounds in a
full SMT pipeline and evaluate the impact of our method in machine translation
process."
"In this paper, we investigate and experiment the notion of error correction
memory applied to error correction in technical texts. The main purpose is to
induce relatively generic correction patterns associated with more contextual
correction recommendations, based on previously memorized and analyzed
corrections. The notion of error correction memory is developed within the
framework of the LELIE project and illustrated on the case of fuzzy lexical
items, which is a major problem in technical texts."
"Inspired by embedded programming languages, an embedded CNL (controlled
natural language) is a proper fragment of an entire natural language (its host
language), but it has a parser that recognizes the entire host language. This
makes it possible to process out-of-CNL input and give useful feedback to
users, instead of just reporting syntax errors. This extended abstract explains
the main concepts of embedded CNL implementation in GF (Grammatical Framework),
with examples from machine translation and some other ongoing work."
"In this paper we describe our contribution to the PoliInformatics 2014
Challenge on the 2007-2008 financial crisis. We propose a state of the art
technique to extract information from texts and provide different
representations, giving first a static overview of the domain and then a
dynamic representation of its main evolutions. We show that this strategy
provides a practical solution to some recent theories in social sciences that
are facing a lack of methods and tools to automatically extract information
from natural language texts."
"Text classification is a task of automatic classification of text into one of
the predefined categories. The problem of text classification has been widely
studied in different communities like natural language processing, data mining
and information retrieval. Text classification is an important constituent in
many information management tasks like topic identification, spam filtering,
email routing, language identification, genre classification, readability
assessment etc. The performance of text classification improves notably when
phrase patterns are used. The use of phrase patterns helps in capturing
non-local behaviours and thus helps in the improvement of text classification
task. Phrase structure extraction is the first step to continue with the phrase
pattern identification. In this survey, detailed study of phrase structure
learning methods have been carried out. This will enable future work in several
NLP tasks, which uses syntactic information from phrase structure like grammar
checkers, question answering, information extraction, machine translation, text
classification. The paper also provides different levels of classification and
detailed comparison of the phrase structure learning methods."
"In this paper we present an ongoing research investigating the possibility
and potential of integrating frame semantics, particularly FrameNet, in the
Grammatical Framework (GF) application grammar development. An important
component of GF is its Resource Grammar Library (RGL) that encapsulates the
low-level linguistic knowledge about morphology and syntax of currently more
than 20 languages facilitating rapid development of multilingual applications.
In the ideal case, porting a GF application grammar to a new language would
only require introducing the domain lexicon - translation equivalents that are
interlinked via common abstract terms. While it is possible for a highly
restricted CNL, developing and porting a less restricted CNL requires above
average linguistic knowledge about the particular language, and above average
GF experience. Specifying a lexicon is mostly straightforward in the case of
nouns (incl. multi-word units), however, verbs are the most complex category
(in terms of both inflectional paradigms and argument structure), and adding
them to a GF application grammar is not a straightforward task. In this paper
we are focusing on verbs, investigating the possibility of creating a
multilingual FrameNet-based GF library. We propose an extension to the current
RGL, allowing GF application developers to define clauses on the semantic
level, thus leaving the language-specific syntactic mapping to this extension.
We demonstrate our approach by reengineering the MOLTO Phrasebook application
grammar."
"The computational handling of Modern Standard Arabic is a challenge in the
field of natural language processing due to its highly rich morphology.
However, several authors have pointed out that the Arabic morphological system
is in fact extremely regular. The existing Arabic morphological analyzers have
exploited this regularity to variable extent, yet we believe there is still
some scope for improvement. Taking inspiration in traditional Arabic prosody,
we have designed and implemented a compact and simple morphological system
which in our opinion takes further advantage of the regularities encountered in
the Arabic morphological system. The output of the system is a large-scale
lexicon of inflected forms that has subsequently been used to create an Online
Interface for a morphological analyzer of Arabic verbs. The Jabalin Online
Interface is available at http://elvira.lllf.uam.es/jabalin/, hosted at the
LLI-UAM lab. The generation system is also available under a GNU GPL 3 license."
"In this paper, we tackle the problem of the translation of proper names. We
introduce our hypothesis according to which proper names can be translated more
often than most people seem to think. Then, we describe the construction of a
parallel multilingual corpus used to illustrate our point. We eventually
evaluate both the advantages and limits of this corpus in our study."
"An inter-rater agreement study is performed for readability assessment in
Bengali. A 1-7 rating scale was used to indicate different levels of
readability. We obtained moderate to fair agreement among seven independent
annotators on 30 text passages written by four eminent Bengali authors. As a by
product of our study, we obtained a readability-annotated ground truth dataset
in Bengali. ."
"Machine translation is the process of translating text from one language to
another. In this paper, Statistical Machine Translation is done on Assamese and
English language by taking their respective parallel corpus. A statistical
phrase based translation toolkit Moses is used here. To develop the language
model and to align the words we used two another tools IRSTLM, GIZA
respectively. BLEU score is used to check our translation system performance,
how good it is. A difference in BLEU scores is obtained while translating
sentences from Assamese to English and vice-versa. Since Indian languages are
morphologically very rich hence translation is relatively harder from English
to Assamese resulting in a low BLEU score. A statistical transliteration system
is also introduced with our translation system to deal basically with proper
nouns, OOV (out of vocabulary) words which are not present in our corpus."
"Machine Translation is the challenging problem for Indian languages. Every
day we can see some machine translators being developed, but getting a high
quality automatic translation is still a very distant dream . The correct
translated sentence for Hindi language is rarely found. In this paper, we are
emphasizing on English-Hindi language pair, so in order to preserve the correct
MT output we present a ranking system, which employs some machine learning
techniques and morphological features. In ranking no human intervention is
required. We have also validated our results by comparing it with human
ranking."
"Named Entity Recognition is always important when dealing with major Natural
Language Processing tasks such as information extraction, question-answering,
machine translation, document summarization etc so in this paper we put forward
a survey of Named Entities in Indian Languages with particular reference to
Assamese. There are various rule-based and machine learning approaches
available for Named Entity Recognition. At the very first of the paper we give
an idea of the available approaches for Named Entity Recognition and then we
discuss about the related research in this field. Assamese like other Indian
languages is agglutinative and suffers from lack of appropriate resources as
Named Entity Recognition requires large data sets, gazetteer list, dictionary
etc and some useful feature like capitalization as found in English cannot be
found in Assamese. Apart from this we also describe some of the issues faced in
Assamese while doing Named Entity Recognition."
"In this paper we present a fundamental lexical semantics of Sinhala language
and a Hidden Markov Model (HMM) based Part of Speech (POS) Tagger for Sinhala
language. In any Natural Language processing task, Part of Speech is a very
vital topic, which involves analysing of the construction, behaviour and the
dynamics of the language, which the knowledge could utilized in computational
linguistics analysis and automation applications. Though Sinhala is a
morphologically rich and agglutinative language, in which words are inflected
with various grammatical features, tagging is very essential for further
analysis of the language. Our research is based on statistical based approach,
in which the tagging process is done by computing the tag sequence probability
and the word-likelihood probability from the given corpus, where the linguistic
knowledge is automatically extracted from the annotated corpus. The current
tagger could reach more than 90% of accuracy for known words."
"We analyze a word embedding method in supervised tasks. It maps words on a
sphere such that words co-occurring in similar contexts lie closely. The
similarity of contexts is measured by the distribution of substitutes that can
fill them. We compared word embeddings, including more recent representations,
in Named Entity Recognition (NER), Chunking, and Dependency Parsing. We examine
our framework in multilingual dependency parsing as well. The results show that
the proposed method achieves as good as or better results compared to the other
word embeddings in the tasks we investigate. It achieves state-of-the-art
results in multilingual dependency parsing. Word embeddings in 7 languages are
available for public use."
"Previous attempts at RST-style discourse segmentation typically adopt
features centered on a single token to predict whether to insert a boundary
before that token. In contrast, we develop a discourse segmenter utilizing a
set of pairing features, which are centered on a pair of adjacent tokens in the
sentence, by equally taking into account the information from both tokens.
Moreover, we propose a novel set of global features, which encode
characteristics of the segmentation as a whole, once we have an initial
segmentation. We show that both the pairing and global features are useful on
their own, and their combination achieved an $F_1$ of 92.6% of identifying
in-sentence discourse boundaries, which is a 17.8% error-rate reduction over
the state-of-the-art performance, approaching 95% of human performance. In
addition, similar improvement is observed across different classification
frameworks."
"We present a novel approach for recognizing what we call targetable named
entities; that is, named entities in a targeted set (e.g, movies, books, TV
shows). Unlike many other NER systems that need to retrain their statistical
models as new entities arrive, our approach does not require such retraining,
which makes it more adaptable for types of entities that are frequently
updated. For this preliminary study, we focus on one entity type, movie title,
using data collected from Twitter. Our system is tested on two evaluation sets,
one including only entities corresponding to movies in our training set, and
the other excluding any of those entities. Our final model shows F1-scores of
76.19% and 78.70% on these evaluation sets, which gives strong evidence that
our approach is completely unbiased to any par- ticular set of entities found
during training."
"Identifying concepts and relationships in biomedical text enables knowledge
to be applied in computational analyses. Many biological natural language
process (BioNLP) projects attempt to address this challenge, but the state of
the art in BioNLP still leaves much room for improvement. Progress in BioNLP
research depends on large, annotated corpora for evaluating information
extraction systems and training machine learning models. Traditionally, such
corpora are created by small numbers of expert annotators often working over
extended periods of time. Recent studies have shown that workers on microtask
crowdsourcing platforms such as Amazon's Mechanical Turk (AMT) can, in
aggregate, generate high-quality annotations of biomedical text. Here, we
investigated the use of the AMT in capturing disease mentions in PubMed
abstracts. We used the NCBI Disease corpus as a gold standard for refining and
benchmarking our crowdsourcing protocol. After several iterations, we arrived
at a protocol that reproduced the annotations of the 593 documents in the
training set of this gold standard with an overall F measure of 0.872
(precision 0.862, recall 0.883). The output can also be tuned to optimize for
precision (max = 0.984 when recall = 0.269) or recall (max = 0.980 when
precision = 0.436). Each document was examined by 15 workers, and their
annotations were merged based on a simple voting method. In total 145 workers
combined to complete all 593 documents in the span of 1 week at a cost of $.06
per abstract per worker. The quality of the annotations, as judged with the F
measure, increases with the number of workers assigned to each task such that
the system can be tuned to balance cost against quality. These results
demonstrate that microtask crowdsourcing can be a valuable tool for generating
well-annotated corpora in BioNLP."
"In this paper, we describe the problem of cognate identification and its
relation to phylogenetic inference. We introduce subsequence based features for
discriminating cognates from non-cognates. We show that subsequence based
features perform better than the state-of-the-art string similarity measures
for the purpose of cognate identification. We use the cognate judgments for the
purpose of phylogenetic inference and observe that these classifiers infer a
tree which is close to the gold standard tree. The contribution of this paper
is the use of subsequence features for cognate identification and to employ the
cognate judgments for phylogenetic inference."
"Real-word spelling correction differs from non-word spelling correction in
its aims and its challenges. Here we show that the central problem in real-word
spelling correction is detection. Methods from non-word spelling correction,
which focus instead on selection among candidate corrections, do not address
detection adequately, because detection is either assumed in advance or heavily
constrained. As we demonstrate in this paper, merely discriminating between the
intended word and a random close variation of it within the context of a
sentence is a task that can be performed with high accuracy using
straightforward models. Trigram models are sufficient in almost all cases. The
difficulty comes when every word in the sentence is a potential error, with a
large set of possible candidate corrections. Despite their strengths, trigram
models cannot reliably find true errors without introducing many more, at least
not when used in the obvious sequential way without added structure. The
detection task exposes weakness not visible in the selection task."
"We present SimLex-999, a gold standard resource for evaluating distributional
semantic models that improves on existing resources in several important ways.
First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly
quantifies similarity rather than association or relatedness, so that pairs of
entities that are associated but not actually similar [Freud, psychology] have
a low rating. We show that, via this focus on similarity, SimLex-999
incentivizes the development of models with a different, and arguably wider
range of applications than those which reflect conceptual association. Second,
SimLex-999 contains a range of concrete and abstract adjective, noun and verb
pairs, together with an independent rating of concreteness and (free)
association strength for each pair. This diversity enables fine-grained
analyses of the performance of models on concepts of different types, and
consequently greater insight into how architectures can be improved. Further,
unlike existing gold standard evaluations, for which automatic approaches have
reached or surpassed the inter-annotator agreement ceiling, state-of-the-art
models perform well below this ceiling on SimLex-999. There is therefore plenty
of scope for SimLex-999 to quantify future improvements to distributional
semantic models, guiding the development of the next generation of
representation-learning architectures."
"In this work, we present an application of the recently proposed unsupervised
keyword extraction algorithm RAKE to a corpus of Polish legal texts from the
field of public procurement. RAKE is essentially a language and domain
independent method. Its only language-specific input is a stoplist containing a
set of non-content words. The performance of the method heavily depends on the
choice of such a stoplist, which should be domain adopted. Therefore, we
complement RAKE algorithm with an automatic approach to selecting non-content
words, which is based on the statistical properties of term distribution."
"Ferrer-i-Cancho (2015) presents a mathematical model of both the synchronic
and diachronic nature of word order based on the assumption that memory costs
are a never decreasing function of distance and a few very general linguistic
assumptions. However, even these minimal and seemingly obvious assumptions are
not as safe as they appear in light of recent typological and psycholinguistic
evidence. The interaction of word order and memory has further depths to be
explored."
"We provide a comparative study between neural word representations and
traditional vector spaces based on co-occurrence counts, in a number of
compositional tasks. We use three different semantic spaces and implement seven
tensor-based compositional models, which we then test (together with simpler
additive and multiplicative approaches) in tasks involving verb disambiguation
and sentence similarity. To check their scalability, we additionally evaluate
the spaces using simple compositional methods on larger-scale tasks with less
constrained language: paraphrase detection and dialogue act tagging. In the
more constrained tasks, co-occurrence vectors are competitive, although choice
of compositional method is important; on the larger-scale tasks, they are
outperformed by neural word embeddings, which show robust, stable performance
across the tasks."
"This paper provides a method for improving tensor-based compositional
distributional models of meaning by the addition of an explicit disambiguation
step prior to composition. In contrast with previous research where this
hypothesis has been successfully tested against relatively simple compositional
models, in our work we use a robust model trained with linear regression. The
results we get in two experiments show the superiority of the prior
disambiguation method and suggest that the effectiveness of this approach is
model-independent."
"We present STIR (STrongly Incremental Repair detection), a system that
detects speech repairs and edit terms on transcripts incrementally with minimal
latency. STIR uses information-theoretic measures from n-gram models as its
principal decision features in a pipeline of classifiers detecting the
different stages of repairs. Results on the Switchboard disfluency tagged
corpus show utterance-final accuracy on a par with state-of-the-art incremental
repair detection methods, but with better incremental accuracy, faster
time-to-detection and less computational overhead. We evaluate its performance
using incremental metrics and propose new repair processing evaluation
standards."
"The Linguistic Annotation Framework (LAF) provides a general, extensible
stand-off markup system for corpora. This paper discusses LAF-Fabric, a new
tool to analyse LAF resources in general with an extension to process the
Hebrew Bible in particular. We first walk through the history of the Hebrew
Bible as text database in decennium-wide steps. Then we describe how LAF-Fabric
may serve as an analysis tool for this corpus. Finally, we describe three
analytic projects/workflows that benefit from the new LAF representation:
  1) the study of linguistic variation: extract cooccurrence data of common
nouns between the books of the Bible (Martijn Naaijer); 2) the study of the
grammar of Hebrew poetry in the Psalms: extract clause typology (Gino Kalkman);
3) construction of a parser of classical Hebrew by Data Oriented Parsing:
generate tree structures from the database (Andreas van Cranenburgh)."
"We present an open source morphological analyzer for Japanese nouns, verbs
and adjectives. The system builds upon the morphological analyzing capabilities
of MeCab to incorporate finer details of classification such as politeness,
tense, mood and voice attributes. We implemented our analyzer in the form of a
finite state transducer using the open source finite state compiler FOMA
toolkit. The source code and tool is available at
https://bitbucket.org/skylander/yc-nlplab/."
"Neural language models learn word representations that capture rich
linguistic and conceptual information. Here we investigate the embeddings
learned by neural machine translation models. We show that translation-based
embeddings outperform those learned by cutting-edge monolingual models at
single-language tasks requiring knowledge of conceptual similarity and/or
syntactic role. The findings suggest that, while monolingual models learn
information about how concepts are related, neural-translation models better
capture their true ontological status."
"This paper proposes a methodology to prepare corpora in Arabic language from
online social network (OSN) and review site for Sentiment Analysis (SA) task.
The paper also proposes a methodology for generating a stopword list from the
prepared corpora. The aim of the paper is to investigate the effect of removing
stopwords on the SA task. The problem is that the stopwords lists generated
before were on Modern Standard Arabic (MSA) which is not the common language
used in OSN. We have generated a stopword list of Egyptian dialect and a
corpus-based list to be used with the OSN corpora. We compare the efficiency of
text classification when using the generated lists along with previously
generated lists of MSA and combining the Egyptian dialect list with the MSA
list. The text classification was performed using Na\""ive Bayes and Decision
Tree classifiers and two feature selection approaches, unigrams and bigram. The
experiments show that the general lists containing the Egyptian dialects words
give better performance than using lists of MSA stopwords only."
"Word alignment is an important natural language processing task that
indicates the correspondence between natural languages. Recently, unsupervised
learning of log-linear models for word alignment has received considerable
attention as it combines the merits of generative and discriminative
approaches. However, a major challenge still remains: it is intractable to
calculate the expectations of non-local features that are critical for
capturing the divergence between natural languages. We propose a contrastive
approach that aims to differentiate observed training examples from noises. It
not only introduces prior knowledge to guide unsupervised learning but also
cancels out partition functions. Based on the observation that the probability
mass of log-linear models for word alignment is usually highly concentrated, we
propose to use top-n alignments to approximate the expectations with respect to
posterior distributions. This allows for efficient and accurate calculation of
expectations of non-local features. Experiments show that our approach achieves
significant improvements over state-of-the-art unsupervised word alignment
methods."
"Statistics pedagogy values using a variety of examples. Thanks to text
resources on the Web, and since statistical packages have the ability to
analyze string data, it is now easy to use language-based examples in a
statistics class. Three such examples are discussed here. First, many types of
wordplay (e.g., crosswords and hangman) involve finding words with letters that
satisfy a certain pattern. Second, linguistics has shown that idiomatic pairs
of words often appear together more frequently than chance. For example, in the
Brown Corpus, this is true of the phrasal verb to throw up (p-value=7.92E-10.)
Third, a pangram contains all the letters of the alphabet at least once. These
are searched for in Charles Dickens' A Christmas Carol, and their lengths are
compared to the expected value given by the unequal probability coupon
collector's problem as well as simulations."
"Hybrid approaches for automatic vowelization of Arabic texts are presented in
this article. The process is made up of two modules. In the first one, a
morphological analysis of the text words is performed using the open source
morphological Analyzer AlKhalil Morpho Sys. Outputs for each word analyzed out
of context, are its different possible vowelizations. The integration of this
Analyzer in our vowelization system required the addition of a lexical database
containing the most frequent words in Arabic language. Using a statistical
approach based on two hidden Markov models (HMM), the second module aims to
eliminate the ambiguities. Indeed, for the first HMM, the unvowelized Arabic
words are the observed states and the vowelized words are the hidden states.
The observed states of the second HMM are identical to those of the first, but
the hidden states are the lists of possible diacritics of the word without its
Arabic letters. Our system uses Viterbi algorithm to select the optimal path
among the solutions proposed by Al Khalil Morpho Sys. Our approach opens an
important way to improve the performance of automatic vowelization of Arabic
texts for other uses in automatic natural language processing."
"Euphonic conjunctions (sandhis) form a very important aspect of Sanskrit
morphology and phonology. The traditional and modern methods of studying about
euphonic conjunctions in Sanskrit follow different methodologies. The former
involves a rigorous study of the Paninian system embodied in Panini's
Ashtadhyayi, while the latter usually involves the study of a few important
sandhi rules with the use of examples. The former is not suitable for
beginners, and the latter, not sufficient to gain a comprehensive understanding
of the operation of sandhi rules. This is so since there are not only numerous
sandhi rules and exceptions, but also complex precedence rules involved. The
need for a new ontology for sandhi-tutoring was hence felt. This work presents
a comprehensive ontology designed to enable a student-user to learn in stages
all about euphonic conjunctions and the relevant aphorisms of Sanskrit grammar
and to test and evaluate the progress of the student-user. The ontology forms
the basis of a multimedia sandhi tutor that was given to different categories
of users including Sanskrit scholars for extensive and rigorous testing."
"Natural logic offers a powerful relational conception of meaning that is a
natural counterpart to distributed semantic representations, which have proven
valuable in a wide range of sophisticated language tasks. However, it remains
an open question whether it is possible to train distributed representations to
support the rich, diverse logical reasoning captured by natural logic. We
address this question using two neural network-based models for learning
embeddings: plain neural networks and neural tensor networks. Our experiments
evaluate the models' ability to learn the basic algebra of natural logic
relations from simulated data and from the WordNet noun graph. The overall
positive results are promising for the future of learned distributed
representations in the applied modeling of logical semantics."
"This paper proposes the use of dependent types for pragmatic phenomena such
as pronoun binding and presupposition resolution as a type-theoretic
alternative to formalisms such as Discourse Representation Theory and Dynamic
Semantics."
"We study the performance of Arabic text classification combining various
techniques: (a) tfidf vs. dependency syntax, for feature selection and
weighting; (b) class association rules vs. support vector machines, for
classification. The Arabic text is used in two forms: rootified and lightly
stemmed. The results we obtain show that lightly stemmed text leads to better
performance than rootified text; that class association rules are better suited
for small feature sets obtained by dependency syntax constraints; and, finally,
that support vector machines are better suited for large feature sets based on
morphological feature selection criteria."
"This paper describes our resource-building results for an eight-week JHU
Human Language Technology Center of Excellence Summer Camp for Applied Language
Exploration (SCALE-2009) on Semantically-Informed Machine Translation.
Specifically, we describe the construction of a modality annotation scheme, a
modality lexicon, and two automated modality taggers that were built using the
lexicon and annotation scheme. Our annotation scheme is based on identifying
three components of modality: a trigger, a target and a holder. We describe how
our modality lexicon was produced semi-automatically, expanding from an initial
hand-selected list of modality trigger words and phrases. The resulting
expanded modality lexicon is being made publicly available. We demonstrate that
one tagger---a structure-based tagger---results in precision around 86%
(depending on genre) for tagging of a standard LDC data set. In a machine
translation application, using the structure-based tagger to annotate English
modalities on an English-Urdu training corpus improved the translation quality
score for Urdu by 0.3 Bleu points in the face of sparse training data."
"We describe a visualization tool that can be used to view the change in
meaning of words over time. The tool makes use of existing (static) word
embedding datasets together with a timestamped $n$-gram corpus to create {\em
temporal} word embeddings."
"Applying natural language processing for mining and intelligent information
access to tweets (a form of microblog) is a challenging, emerging research
area. Unlike carefully authored news text and other longer content, tweets pose
a number of new challenges, due to their short, noisy, context-dependent, and
dynamic nature. Information extraction from tweets is typically performed in a
pipeline, comprising consecutive stages of language identification,
tokenisation, part-of-speech tagging, named entity recognition and entity
disambiguation (e.g. with respect to DBpedia). In this work, we describe a new
Twitter entity disambiguation dataset, and conduct an empirical analysis of
named entity recognition and disambiguation, investigating how robust a number
of state-of-the-art systems are on such noisy texts, what the main sources of
error are, and which problems should be further investigated to improve the
state of the art."
"We describe a paradigm for combining manual and automatic error correction of
noisy structured lexicographic data. Modifications to the structure and
underlying text of the lexicographic data are expressed in a simple,
interpreted programming language. Dictionary Manipulation Language (DML)
commands identify nodes by unique identifiers, and manipulations are performed
using simple commands such as create, move, set text, etc. Corrected lexicons
are produced by applying sequences of DML commands to the source version of the
lexicon. DML commands can be written manually to repair one-off errors or
generated automatically to correct recurring problems. We discuss advantages of
the paradigm for the task of editing digital bilingual dictionaries."
"Social media texts are significant information sources for several
application areas including trend analysis, event monitoring, and opinion
mining. Unfortunately, existing solutions for tasks such as named entity
recognition that perform well on formal texts usually perform poorly when
applied to social media texts. In this paper, we report on experiments that
have the purpose of improving named entity recognition on Turkish tweets, using
two different annotated data sets. In these experiments, starting with a
baseline named entity recognition system, we adapt its recognition rules and
resources to better fit Twitter language by relaxing its capitalization
constraint and by diacritics-based expansion of its lexical resources, and we
employ a simplistic normalization scheme on tweets to observe the effects of
these on the overall named entity recognition performance on Turkish tweets.
The evaluation results of the system with these different settings are provided
with discussions of these results."
"In this article, we describe an approach for automatic detection of
noun-adjective agreement errors in Bulgarian texts by explaining the necessary
steps required to develop a simple Java-based language processing application.
For this purpose, we use the GATE language processing framework, which is
capable of analyzing texts in Bulgarian language and can be embedded in
software applications, accessed through a set of Java APIs. In our example
application we also demonstrate how to use the functionality of GATE to perform
regular expressions over annotations for detecting agreement errors in simple
noun phrases formed by two words - attributive adjective and a noun, where the
attributive adjective precedes the noun. The provided code samples can also be
used as a starting point for implementing natural language processing
functionalities in software applications related to language processing tasks
like detection, annotation and retrieval of word groups meeting a specific set
of criteria."
"Suicide is among the leading causes of death in China. However, technical
approaches toward preventing suicide are challenging and remaining under
development. Recently, several actual suicidal cases were preceded by users who
posted microblogs with suicidal ideation to Sina Weibo, a Chinese social media
network akin to Twitter. It would therefore be desirable to detect suicidal
ideations from microblogs in real-time, and immediately alert appropriate
support groups, which may lead to successful prevention. In this paper, we
propose a real-time suicidal ideation detection system deployed over Weibo,
using machine learning and known psychological techniques. Currently, we have
identified 53 known suicidal cases who posted suicide notes on Weibo prior to
their deaths.We explore linguistic features of these known cases using a
psychological lexicon dictionary, and train an effective suicidal Weibo post
detection model. 6714 tagged posts and several classifiers are used to verify
the model. By combining both machine learning and psychological knowledge, SVM
classifier has the best performance of different classifiers, yielding an
F-measure of 68:3%, a Precision of 78:9%, and a Recall of 60:3%."
"The word2vec model and application by Mikolov et al. have attracted a great
amount of attention in recent two years. The vector representations of words
learned by word2vec models have been shown to carry semantic meanings and are
useful in various NLP tasks. As an increasing number of researchers would like
to experiment with word2vec or similar techniques, I notice that there lacks a
material that comprehensively explains the parameter learning process of word
embedding models in details, thus preventing researchers that are non-experts
in neural networks from understanding the working mechanism of such models.
  This note provides detailed derivations and explanations of the parameter
update equations of the word2vec models, including the original continuous
bag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization
techniques, including hierarchical softmax and negative sampling. Intuitive
interpretations of the gradient equations are also provided alongside
mathematical derivations.
  In the appendix, a review on the basics of neuron networks and
backpropagation is provided. I also created an interactive demo, wevi, to
facilitate the intuitive understanding of the model."
"The mathematical representation of semantics is a key issue for Natural
Language Processing (NLP). A lot of research has been devoted to finding ways
of representing the semantics of individual words in vector spaces.
Distributional approaches --- meaning distributed representations that exploit
co-occurrence statistics of large corpora --- have proved popular and
successful across a number of tasks. However, natural language usually comes in
structures beyond the word level, with meaning arising not only from the
individual words but also the structure they are contained in at the phrasal or
sentential level. Modelling the compositional process by which the meaning of
an utterance arises from the meaning of its parts is an equally fundamental
task of NLP.
  This dissertation explores methods for learning distributed semantic
representations and models for composing these into representations for larger
linguistic units. Our underlying hypothesis is that neural models are a
suitable vehicle for learning semantically rich representations and that such
representations in turn are suitable vehicles for solving important tasks in
natural language processing. The contribution of this thesis is a thorough
evaluation of our hypothesis, as part of which we introduce several new
approaches to representation learning and compositional semantics, as well as
multiple state-of-the-art models which apply distributed semantic
representations to various tasks in NLP."
"The paper aims to show how an application can be developed that converts the
English language into the Punjabi Language, and the same application can
convert the Text to Speech(TTS) i.e. pronounce the text. This application can
be really beneficial for those with special needs."
"Vector space word representations are learned from distributional information
of words in large corpora. Although such statistics are semantically
informative, they disregard the valuable information that is contained in
semantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This
paper proposes a method for refining vector space representations using
relational information from semantic lexicons by encouraging linked words to
have similar vector representations, and it makes no assumptions about how the
input vectors were constructed. Evaluated on a battery of standard lexical
semantic evaluation tasks in several languages, we obtain substantial
improvements starting with a variety of word vector models. Our refinement
method outperforms prior techniques for incorporating semantic lexicons into
the word vector training algorithms."
"The ability to extract public opinion from web portals such as review sites,
social networks and blogs will enable companies and individuals to form a view,
an attitude and make decisions without having to do lengthy and costly
researches and surveys. In this paper machine learning techniques are used for
determining the polarity of forum posts on kajgana which are written in
Macedonian language. The posts are classified as being positive, negative or
neutral. We test different feature metrics and classifiers and provide detailed
evaluation of their participation in improving the overall performance on a
manually generated dataset. By achieving 92% accuracy, we show that the
performance of systems for automated opinion mining is comparable to a human
evaluator, thus making it a viable option for text data analysis. Finally, we
present a few statistics derived from the forum posts using the developed
system."
"A graphical language addresses the need to communicate medical information in
a synthetic way. Medical concepts are expressed by icons conveying fast visual
information about patients' current state or about the known effects of drugs.
In order to increase the visual language's acceptance and usability, a natural
language generation interface is currently developed. In this context, this
paper describes the use of an informatics method ---graph transformation--- to
prepare data consisting of concepts in an OWL-DL ontology for use in a natural
language generation component. The OWL concept may be considered as a
star-shaped graph with a central node. The method transforms it into a graph
representing the deep semantic structure of a natural language phrase. This
work may be of future use in other contexts where ontology concepts have to be
mapped to half-formalized natural language expressions."
"In this paper we analyse network motifs in the co-occurrence directed
networks constructed from five different texts (four books and one portal) in
the Croatian language. After preparing the data and network construction, we
perform the network motif analysis. We analyse the motif frequencies and
Z-scores in the five networks. We present the triad significance profile for
five datasets. Furthermore, we compare our results with the existing results
for the linguistic networks. Firstly, we show that the triad significance
profile for the Croatian language is very similar with the other languages and
all the networks belong to the same family of networks. However, there are
certain differences between the Croatian language and other analysed languages.
We conclude that this is due to the free word-order of the Croatian language."
"Semantic parsing has made significant progress, but most current semantic
parsers are extremely slow (CKY-based) and rather primitive in representation.
We introduce three new techniques to tackle these problems. First, we design
the first linear-time incremental shift-reduce-style semantic parsing algorithm
which is more efficient than conventional cubic-time bottom-up semantic
parsers. Second, our parser, being type-driven instead of syntax-driven, uses
type-checking to decide the direction of reduction, which eliminates the need
for a syntactic grammar such as CCG. Third, to fully exploit the power of
type-driven semantic parsing beyond simple types (such as entities and truth
values), we borrow from programming language theory the concepts of subtype
polymorphism and parametric polymorphism to enrich the type system in order to
better guide the parsing. Our system learns very accurate parses in GeoQuery,
Jobs and Atis domains."
"This paper describes pre-processing phase of ontology graph generation system
from Punjabi text documents of different domains. This research paper focuses
on pre-processing of Punjabi text documents. Pre-processing is structured
representation of the input text. Pre-processing of ontology graph generation
includes allowing input restrictions to the text, removal of special symbols
and punctuation marks, removal of duplicate terms, removal of stop words,
extract terms by matching input terms with dictionary and gazetteer lists
terms."
"We present a method for coarse-grained cross-lingual alignment of comparable
texts: segments consisting of contiguous paragraphs that discuss the same theme
(e.g. history, economy) are aligned based on induced multilingual topics. The
method combines three ideas: a two-level LDA model that filters out words that
do not convey themes, an HMM that models the ordering of themes in the
collection of documents, and language-independent concept annotations to serve
as a cross-language bridge and to strengthen the connection between paragraphs
in the same segment through concept relations. The method is evaluated on
English and French data previously used for monolingual alignment. The results
show state-of-the-art performance in both monolingual and cross-lingual
settings."
"The functional approach to compositional distributional semantics considers
transitive verbs to be linear maps that transform the distributional vectors
representing nouns into a vector representing a sentence. We conduct an initial
investigation that uses a matrix consisting of the parameters of a logistic
regression classifier trained on a plausibility task as a transitive verb
function. We compare our method to a commonly used corpus-based method for
constructing a verb matrix and find that the plausibility training may be more
effective for disambiguation tasks."
"Many tasks in Natural Language Processing involve recognizing lexical
entailment. Two different approaches to this problem have been proposed
recently that are quite different from each other. The first is an asymmetric
similarity measure designed to give high scores when the contexts of the
narrower term in the entailment are a subset of those of the broader term. The
second is a supervised approach where a classifier is learned to predict
entailment given a concatenated latent vector representation of the word. Both
of these approaches are vector space models that use a single context vector as
a representation of the word. In this work, I study the effects of clustering
words into senses and using these multiple context vectors to infer entailment
using extensions of these two algorithms. I find that this approach offers some
improvement to these entailment algorithms."
"In the following article we elected to study with NooJ the lexis of a 17 th
century text, Mary Astell's seminal essay, A Serious Proposal to the Ladies,
part I, published in 1694. We first focused on the semantics to see how Astell
builds her vindication of the female sex, which words she uses to sensitise
women to their alienated condition and promote their education. Then we studied
the morphology of the lexemes (which is different from contemporary English)
used by the author, thanks to the NooJ tools we have devised for this purpose.
NooJ has great functionalities for lexicographic work. Its commands and graphs
prove to be most efficient in the spotting of archaic words or variants in
spelling. Introduction In our previous articles, we have studied the
singularities of 17 th century English within the framework of a diachronic
analysis thanks to syntactical and morphological graphs and thanks to the
dictionaries we have compiled from a corpus that may be expanded overtime. Our
early work was based on a limited corpus of English travel literature to Greece
in the 17 th century. This article deals with a late seventeenth century text
written by a woman philosopher and essayist, Mary Astell (1666--1731),
considered as one of the first English feminists. Astell wrote her essay at a
time in English history when women were ""the weaker vessel"" and their main
business in life was to charm and please men by their looks and submissiveness.
In this essay we will see how NooJ can help us analyse Astell's rhetoric (what
point of view does she adopt, does she speak in her own name, in the name of
all women, what is her representation of men and women and their relationships
in the text, what are the goals of education?). Then we will turn our attention
to the morphology of words in the text and use NooJ commands and graphs to
carry out a lexicographic inquiry into Astell's lexemes."
"Comment on ""Approaching human language with complex networks"" by Cong and Liu
(Physics of Life Reviews, Volume 11, Issue 4, December 2014, Pages 598-618)."
"Answer sentence selection is the task of identifying sentences that contain
the answer to a given question. This is an important problem in its own right
as well as in the larger context of open domain question answering. We propose
a novel approach to solving this task via means of distributed representations,
and learn to match questions with answers by considering their semantic
encoding. This contrasts prior work on this task, which typically relies on
classifiers with large numbers of hand-crafted syntactic and semantic features
and various external resources. Our approach does not require any feature
engineering nor does it involve specialist linguistic data, making this model
easily applicable to a wide range of domains and languages. Experimental
results on a standard benchmark dataset from TREC demonstrate that---despite
its simplicity---our model matches state of the art performance on the answer
sentence selection task."
"Entity type tagging is the task of assigning category labels to each mention
of an entity in a document. While standard systems focus on a small set of
types, recent work (Ling and Weld, 2012) suggests that using a large
fine-grained label set can lead to dramatic improvements in downstream tasks.
In the absence of labeled training data, existing fine-grained tagging systems
obtain examples automatically, using resolved entities and their types
extracted from a knowledge base. However, since the appropriate type often
depends on context (e.g. Washington could be tagged either as city or
government), this procedure can result in spurious labels, leading to poorer
generalization. We propose the task of context-dependent fine type tagging,
where the set of acceptable labels for a mention is restricted to only those
deducible from the local context (e.g. sentence or document). We introduce new
resources for this task: 12,017 mentions annotated with their context-dependent
fine types, and we provide baseline experimental results on this data."
"Neural machine translation, a recently proposed approach to machine
translation based purely on neural networks, has shown promising results
compared to the existing approaches such as phrase-based statistical machine
translation. Despite its recent success, neural machine translation has its
limitation in handling a larger vocabulary, as training complexity as well as
decoding complexity increase proportionally to the number of target words. In
this paper, we propose a method that allows us to use a very large target
vocabulary without increasing training complexity, based on importance
sampling. We show that decoding can be efficiently done even with the model
having a very large target vocabulary by selecting only a small subset of the
whole target vocabulary. The models trained by the proposed approach are
empirically found to outperform the baseline models with a small vocabulary as
well as the LSTM-based neural machine translation models. Furthermore, when we
use the ensemble of a few models with very large target vocabularies, we
achieve the state-of-the-art translation performance (measured by BLEU) on the
English->German translation and almost as high performance as state-of-the-art
English->French translation system."
"Synonym extraction is an important task in natural language processing and
often used as a submodule in query expansion, question answering and other
applications. Automatic synonym extractor is highly preferred for large scale
applications. Previous studies in synonym extraction are most limited to small
scale datasets. In this paper, we build a large dataset with 3.4 million
synonym/non-synonym pairs to capture the challenges in real world scenarios. We
proposed (1) a new cost function to accommodate the unbalanced learning
problem, and (2) a feature learning based deep neural network to model the
complicated relationships in synonym pairs. We compare several different
approaches based on SVMs and neural networks, and find out a novel feature
learning based neural network outperforms the methods with hand-assigned
features. Specifically, the best performance of our model surpasses the SVM
baseline with a significant 97\% relative improvement."
"Attributes of words and relations between two words are central to numerous
tasks in Artificial Intelligence such as knowledge representation, similarity
measurement, and analogy detection. Often when two words share one or more
attributes in common, they are connected by some semantic relations. On the
other hand, if there are numerous semantic relations between two words, we can
expect some of the attributes of one of the words to be inherited by the other.
Motivated by this close connection between attributes and relations, given a
relational graph in which words are inter- connected via numerous semantic
relations, we propose a method to learn a latent representation for the
individual words. The proposed method considers not only the co-occurrences of
words as done by existing approaches for word representation learning, but also
the semantic relations in which two words co-occur. To evaluate the accuracy of
the word representations learnt using the proposed method, we use the learnt
word representations to solve semantic word analogy problems. Our experimental
results show that it is possible to learn better word representations by using
semantic semantics between words."
"Universal Grammar (UG) theory has been one of the most important research
topics in linguistics since introduced five decades ago. UG specifies the
restricted set of languages learnable by human brain, and thus, many
researchers believe in its biological roots. Numerous empirical studies of
neurobiological and cognitive functions of the human brain, and of many natural
languages, have been conducted to unveil some aspects of UG. This, however,
resulted in different and sometimes contradicting theories that do not indicate
a universally unique grammar. In this research, we tackle the UG problem from
an entirely different perspective. We search for the Unique Universal Grammar
(UUG) that facilitates communication and knowledge transfer, the sole purpose
of a language. We formulate this UG and show that it is unique, intrinsic, and
cosmic, rather than humanistic. Initial analysis on a widespread natural
language already showed some positive results."
"Quantitative linguistics has been allowed, in the last few decades, within
the admittedly blurry boundaries of the field of complex systems. A growing
host of applied mathematicians and statistical physicists devote their efforts
to disclose regularities, correlations, patterns, and structural properties of
language streams, using techniques borrowed from statistics and information
theory. Overall, results can still be categorized as modest, but the prospects
are promising: medium- and long-range features in the organization of human
language -which are beyond the scope of traditional linguistics- have already
emerged from this kind of analysis and continue to be reported, contributing a
new perspective to our understanding of this most complex communication system.
This short book is intended to review some of these recent contributions."
"In this paper, we propose a new approach to construct a system of
transformation rules for the Part-of-Speech (POS) tagging task. Our approach is
based on an incremental knowledge acquisition method where rules are stored in
an exception structure and new rules are only added to correct the errors of
existing rules; thus allowing systematic control of the interaction between the
rules. Experimental results on 13 languages show that our approach is fast in
terms of training time and tagging speed. Furthermore, our approach obtains
very competitive accuracy in comparison to state-of-the-art POS and
morphological taggers."
"We investigate the hypothesis that word representations ought to incorporate
both distributional and relational semantics. To this end, we employ the
Alternating Direction Method of Multipliers (ADMM), which flexibly optimizes a
distributional objective on raw text and a relational objective on WordNet.
Preliminary results on knowledge base completion, analogy tests, and parsing
show that word representations trained on both objectives can give improvements
in some cases."
"We study sentiment analysis beyond the typical granularity of polarity and
instead use Plutchik's wheel of emotions model. We introduce RBEM-Emo as an
extension to the Rule-Based Emission Model algorithm to deduce such emotions
from human-written messages. We evaluate our approach on two different datasets
and compare its performance with the current state-of-the-art techniques for
emotion detection, including a recursive auto-encoder. The results of the
experimental study suggest that RBEM-Emo is a promising approach advancing the
current state-of-the-art in emotion detection."
"Recent works on word representations mostly rely on predictive models.
Distributed word representations (aka word embeddings) are trained to optimally
predict the contexts in which the corresponding words tend to appear. Such
models have succeeded in capturing word similarties as well as semantic and
syntactic regularities. Instead, we aim at reviving interest in a model based
on counts. We present a systematic study of the use of the Hellinger distance
to extract semantic representations from the word co-occurence statistics of
large text corpora. We show that this distance gives good performance on word
similarity and analogy tasks, with a proper type and size of context, and a
dimensionality reduction based on a stochastic low-rank approximation. Besides
being both simple and intuitive, this method also provides an encoding function
which can be used to infer unseen words or phrases. This becomes a clear
advantage compared to predictive models which must train these new words."
"In this work, automatic analysis of themes contained in a large corpora of
judgments from public procurement domain is performed. The employed technique
is unsupervised latent Dirichlet allocation (LDA). In addition, it is proposed,
to use LDA in conjunction with recently developed method of unsupervised
keyword extraction. Such an approach improves the interpretability of the
automatically obtained topics and allows for better computational performance.
The described analysis illustrates a potential of the method in detecting
recurring themes and discovering temporal trends in lodged contract appeals.
These results may be in future applied to improve information retrieval from
repositories of legal texts or as auxiliary material for legal analyses carried
out by human experts."
"The problem of word search in Sanskrit is inseparable from complexities that
include those caused by euphonic conjunctions and case-inflections. The
case-inflectional forms of a noun normally number 24 owing to the fact that in
Sanskrit there are eight cases and three numbers-singular, dual and plural. The
traditional method of generating these inflectional forms is rather elaborate
owing to the fact that there are differences in the forms generated between
even very similar words and there are subtle nuances involved. Further, it
would be a cumbersome exercise to generate and search for 24 forms of a word
during a word search in a large text, using the currently available
case-inflectional form generators. This study presents a new approach to
generating case-inflectional forms that is simpler to compute. Further, an
optimized model that is sufficient for generating only those word forms that
are required in a word search and is more than 80% efficient compared to the
complete case-inflectional forms generator, is presented in this study for the
first time."
"We investigate the hypothesis that word representations ought to incorporate
both distributional and relational semantics. To this end, we employ the
Alternating Direction Method of Multipliers (ADMM), which flexibly optimizes a
distributional objective on raw text and a relational objective on WordNet.
Preliminary results on knowledge base completion, analogy tests, and parsing
show that word representations trained on both objectives can give improvements
in some cases."
"Distributed representations of words have boosted the performance of many
Natural Language Processing tasks. However, usually only one representation per
word is obtained, not acknowledging the fact that some words have multiple
meanings. This has a negative effect on the individual word representations and
the language model as a whole. In this paper we present a simple model that
enables recent techniques for building word vectors to represent distinct
senses of polysemic words. In our assessment of this model we show that it is
able to effectively discriminate between words' senses and to do so in a
computationally efficient manner."
"Supertagging is an approach originally developed by Bangalore and Joshi
(1999) to improve the parsing efficiency. In the beginning, the scholars used
small training datasets and somewhat na\""ive smoothing techniques to learn the
probability distributions of supertags. Since its inception, the applicability
of Supertags has been explored for TAG (tree-adjoining grammar) formalism as
well as other related yet, different formalisms such as CCG. This article will
try to summarize the various chapters, relevant to statistical parsing, from
the most recent edited book volume (Bangalore and Joshi, 2010). The chapters
were selected so as to blend the learning of supertags, its integration into
full-scale parsing, and in semantic parsing."
"The bag-of-words (BOW) model is the common approach for classifying
documents, where words are used as feature for training a classifier. This
generally involves a huge number of features. Some techniques, such as Latent
Semantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been
designed to summarize documents in a lower dimension with the least semantic
information loss. Some semantic information is nevertheless always lost, since
only words are considered. Instead, we aim at using information coming from
n-grams to overcome this limitation, while remaining in a low-dimension space.
Many approaches, such as the Skip-gram model, provide good word vector
representations very quickly. We propose to average these representations to
obtain representations of n-grams. All n-grams are thus embedded in a same
semantic space. A K-means clustering can then group them into semantic
concepts. The number of features is therefore dramatically reduced and
documents can be represented as bag of semantic concepts. We show that this
model outperforms LSA and LDA on a sentiment classification task, and yields
similar results than a traditional BOW-model with far less features."
"In this work, we present a novel neural network based architecture for
inducing compositional crosslingual word representations. Unlike previously
proposed methods, our method fulfills the following three criteria; it
constrains the word-level representations to be compositional, it is capable of
leveraging both bilingual and monolingual data, and it is scalable to large
vocabularies and large quantities of data. The key component of our approach is
what we refer to as a monolingual inclusion criterion, that exploits the
observation that phrases are more closely semantically related to their
sub-phrases than to other randomly sampled phrases. We evaluate our method on a
well-established crosslingual document classification task and achieve results
that are either comparable, or greatly improve upon previous state-of-the-art
methods. Concretely, our method reaches a level of 92.7% and 84.4% accuracy for
the English to German and German to English sub-tasks respectively. The former
advances the state of the art by 0.9% points of accuracy, the latter is an
absolute improvement upon the previous state of the art by 7.7% points of
accuracy and an improvement of 33.0% in error reduction."
"Neural language models learn word representations, or embeddings, that
capture rich linguistic and conceptual information. Here we investigate the
embeddings learned by neural machine translation models, a recently-developed
class of neural language model. We show that embeddings from translation models
outperform those learned by monolingual models at tasks that require knowledge
of both conceptual similarity and lexical-syntactic role. We further show that
these effects hold when translating from both English to French and English to
German, and argue that the desirable properties of translation embeddings
should emerge largely independently of the source and target languages.
Finally, we apply a new method for training neural translation models with very
large vocabularies, and show that this vocabulary expansion algorithm results
in minimal degradation of embedding quality. Our embedding spaces can be
queried in an online demo and downloaded from our web page. Overall, our
analyses indicate that translation-based embeddings should be used in
applications that require concepts to be organised according to similarity
and/or lexical function, while monolingual embeddings are better suited to
modelling (nonspecific) inter-word relatedness."
"We consider learning representations of entities and relations in KBs using
the neural-embedding approach. We show that most existing models, including NTN
(Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized
under a unified learning framework, where entities are low-dimensional vectors
learned from a neural network and relations are bilinear and/or linear mapping
functions. Under this framework, we compare a variety of embedding models on
the link prediction task. We show that a simple bilinear formulation achieves
new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2%
vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach
that utilizes the learned relation embeddings to mine logical rules such as
""BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)"". We find that
embeddings learned from the bilinear objective are particularly good at
capturing relational semantics and that the composition of relations is
characterized by matrix multiplication. More interestingly, we demonstrate that
our embedding-based rule extraction approach successfully outperforms a
state-of-the-art confidence-based rule mining approach in mining Horn rules
that involve compositional reasoning."
"This paper presents an in-depth investigation on integrating neural language
models in translation systems. Scaling neural language models is a difficult
task, but crucial for real-world applications. This paper evaluates the impact
on end-to-end MT quality of both new and existing scaling techniques. We show
when explicitly normalising neural models is necessary and what optimisation
tricks one should use in such scenarios. We also focus on scalable training
algorithms and investigate noise contrastive estimation and diagonal contexts
as sources for further speed improvements. We explore the trade-offs between
neural models and back-off n-gram models and find that neural models make
strong candidates for natural language applications in memory constrained
environments, yet still lag behind traditional models in raw translation
quality. We conclude with a set of recommendations one should follow to build a
scalable neural language model for MT."
"Sign language, which is a medium of communication for deaf people, uses
manual communication and body language to convey meaning, as opposed to using
sound. This paper presents a prototype Malayalam text to sign language
translation system. The proposed system takes Malayalam text as input and
generates corresponding Sign Language. Output animation is rendered using a
computer generated model. This system will help to disseminate information to
the deaf people in public utility places like railways, banks, hospitals etc.
This will also act as an educational tool in learning Sign Language."
"SentiWordNet is an important lexical resource supporting sentiment analysis
in opinion mining applications. In this paper, we propose a novel approach to
construct a Vietnamese SentiWordNet (VSWN). SentiWordNet is typically generated
from WordNet in which each synset has numerical scores to indicate its opinion
polarities. Many previous studies obtained these scores by applying a machine
learning method to WordNet. However, Vietnamese WordNet is not available
unfortunately by the time of this paper. Therefore, we propose a method to
construct VSWN from a Vietnamese dictionary, not from WordNet. We show the
effectiveness of the proposed method by generating a VSWN with 39,561 synsets
automatically. The method is experimentally tested with 266 synsets with aspect
of positivity and negativity. It attains a competitive result compared with
English SentiWordNet that is 0.066 and 0.052 differences for positivity and
negativity sets respectively."
"Research into the stylistic properties of translations is an issue which has
received some attention in computational stylistics. Previous work by Rybicki
(2006) on the distinguishing of character idiolects in the work of Polish
author Henryk Sienkiewicz and two corresponding English translations using
Burrow's Delta method concluded that idiolectal differences could be observed
in the source texts and this variation was preserved to a large degree in both
translations. This study also found that the two translations were also highly
distinguishable from one another. Burrows (2002) examined English translations
of Juvenal also using the Delta method, results of this work suggest that some
translators are more adept at concealing their own style when translating the
works of another author whereas other authors tend to imprint their own style
to a greater extent on the work they translate. Our work examines the writing
of a single author, Norwegian playwright Henrik Ibsen, and these writings
translated into both German and English from Norwegian, in an attempt to
investigate the preservation of characterization, defined here as the
distinctiveness of textual contributions of characters."
"In this paper we present REG, a graph-based approach for study a fundamental
problem of Natural Language Processing (NLP): the automatic text summarization.
The algorithm maps a document as a graph, then it computes the weight of their
sentences. We have applied this approach to summarize documents in three
languages."
"Part of Speech (POS) is a very vital topic in Natural Language Processing
(NLP) task in any language, which involves analysing the construction of the
language, behaviours and the dynamics of the language, the knowledge that could
be utilized in computational linguistics analysis and automation applications.
In this context, dealing with unknown words (words do not appear in the lexicon
referred as unknown words) is also an important task, since growing NLP systems
are used in more and more new applications. One aid of predicting lexical
categories of unknown words is the use of syntactical knowledge of the
language. The distinction between open class words and closed class words
together with syntactical features of the language used in this research to
predict lexical categories of unknown words in the tagging process. An
experiment is performed to investigate the ability of the approach to parse
unknown words using syntactical knowledge without human intervention. This
experiment shows that the performance of the tagging process is enhanced when
word class distinction is used together with syntactic rules to parse sentences
containing unknown words in Sinhala language."
"Analysis of scripts plays an important role in paleography and in
quantitative linguistics. Especially in the field of digital paleography
quantitative features are much needed to differentiate glyphs. We describe an
elaborate set of metrics that quantify qualitative information contained in
characters and hence indirectly also quantify the scribal features. We broadly
divide the metrics into several categories and describe each individual metric
with its underlying qualitative significance. The metrics are largely derived
from the related area of gesture design and recognition. We also propose
several novel metrics. The proposed metrics are soundly grounded on the
principles of handwriting production and handwriting analysis. These computed
metrics could serve as descriptors for scripts and also be used for comparing
and analyzing scripts. We illustrate some quantitative analysis based on the
proposed metrics by applying it to the paleographic evolution of the medieval
Tamil script from Brahmi. We also outline future work."
"This paper is concerned with nearest neighbor search in distributional
semantic models. A normal nearest neighbor search only returns a ranked list of
neighbors, with no information about the structure or topology of the local
neighborhood. This is a potentially serious shortcoming of the mode of querying
a distributional semantic model, since a ranked list of neighbors may conflate
several different senses. We argue that the topology of neighborhoods in
semantic space provides important information about the different senses of
terms, and that such topological structures can be used for word-sense
induction. We also argue that the topology of the neighborhoods in semantic
space can be used to determine the semantic horizon of a point, which we define
as the set of neighbors that have a direct connection to the point. We
introduce relative neighborhood graphs as method to uncover the topological
properties of neighborhoods in semantic models. We also provide examples of
relative neighborhood graphs for three well-known semantic models; the PMI
model, the GloVe model, and the skipgram model."
"Turkic languages exhibit extensive and diverse etymological relationships
among lexical items. These relationships make the Turkic languages promising
for exploring automated translation lexicon induction by leveraging cognate and
other etymological information. However, due to the extent and diversity of the
types of relationships between words, it is not clear how to annotate such
information. In this paper, we present a methodology for annotating cognates
and etymological origin in Turkic languages. Our method strives to balance the
amount of research effort the annotator expends with the utility of the
annotations for supporting research on improving automated translation lexicon
induction."
"We consider phrase based Language Models (LM), which generalize the commonly
used word level models. Similar concept on phrase based LMs appears in speech
recognition, which is rather specialized and thus less suitable for machine
translation (MT). In contrast to the dependency LM, we first introduce the
exhaustive phrase-based LMs tailored for MT use. Preliminary experimental
results show that our approach outperform word based LMs with the respect to
perplexity and translation quality."
"Reordering is a challenge to machine translation (MT) systems. In MT, the
widely used approach is to apply word based language model (LM) which considers
the constituent units of a sentence as words. In speech recognition (SR), some
phrase based LM have been proposed. However, those LMs are not necessarily
suitable or optimal for reordering. We propose two phrase based LMs which
considers the constituent units of a sentence as phrases. Experiments show that
our phrase based LMs outperform the word based LM with the respect of
perplexity and n-best list re-ranking."
"Syntactic parsing is a necessary task which is required for NLP applications
including machine translation. It is a challenging task to develop a
qualitative parser for morphological rich and agglutinative languages.
Syntactic analysis is used to understand the grammatical structure of a natural
language sentence. It outputs all the grammatical information of each word and
its constituent. Also issues related to it help us to understand the language
in a more detailed way. This literature survey is groundwork to understand the
different parser development for Indian languages and various approaches that
are used to develop such tools and techniques. This paper provides a survey of
research papers from well known journals and conferences."
"Statistical methods have been widely employed in many practical natural
language processing applications. More specifically, complex networks concepts
and methods from dynamical systems theory have been successfully applied to
recognize stylistic patterns in written texts. Despite the large amount of
studies devoted to represent texts with physical models, only a few studies
have assessed the relevance of attributes derived from the analysis of
stylistic fluctuations. Because fluctuations represent a pivotal factor for
characterizing a myriad of real systems, this study focused on the analysis of
the properties of stylistic fluctuations in texts via topological analysis of
complex networks and intermittency measurements. The results showed that
different authors display distinct fluctuation patterns. In particular, it was
found that it is possible to identify the authorship of books using the
intermittency of specific words. Taken together, the results described here
suggest that the patterns found in stylistic fluctuations could be used to
analyze other related complex systems. Furthermore, the discovery of novel
patterns related to textual stylistic fluctuations indicates that these
patterns could be useful to improve the state of the art of many
stylistic-based natural language processing tasks."
"Given a set of terms from a given domain, how can we structure them into a
taxonomy without manual intervention? This is the task 17 of SemEval 2015. Here
we present our simple taxonomy structuring techniques which, despite their
simplicity, ranked first in this 2015 benchmark. We use large quantities of
text (English Wikipedia) and simple heuristics such as term overlap and
document and sentence co-occurrence to produce hypernym lists. We describe
these techniques and pre-sent an initial evaluation of results."
"Language model is one of the most important modules in statistical machine
translation and currently the word-based language model dominants this
community. However, many translation models (e.g. phrase-based models) generate
the target language sentences by rendering and compositing the phrases rather
than the words. Thus, it is much more reasonable to model dependency between
phrases, but few research work succeed in solving this problem. In this paper,
we tackle this problem by designing a novel phrase-based language model which
attempts to solve three key sub-problems: 1, how to define a phrase in language
model; 2, how to determine the phrase boundary in the large-scale monolingual
data in order to enlarge the training set; 3, how to alleviate the data
sparsity problem due to the huge vocabulary size of phrases. By carefully
handling these issues, the extensive experiments on Chinese-to-English
translation show that our phrase-based language model can significantly improve
the translation quality by up to +1.47 absolute BLEU score."
"We present a language complexity analysis of World of Warcraft (WoW)
community texts, which we compare to texts from a general corpus of web
English. Results from several complexity types are presented, including lexical
diversity, density, readability and syntactic complexity. The language of WoW
texts is found to be comparable to the general corpus on some complexity
measures, yet more specialized on other measures. Our findings can be used by
educators willing to include game-related activities into school curricula."
"Generating a novel textual description of an image is an interesting problem
that connects computer vision and natural language processing. In this paper,
we present a simple model that is able to generate descriptive sentences given
a sample image. This model has a strong focus on the syntax of the
descriptions. We train a purely bilinear model that learns a metric between an
image representation (generated from a previously trained Convolutional Neural
Network) and phrases that are used to described them. The system is then able
to infer phrases from a given image sample. Based on caption syntax statistics,
we propose a simple language model that can produce relevant descriptions for a
given test image using the phrases inferred. Our approach, which is
considerably simpler than state-of-the-art models, achieves comparable results
in two popular datasets for the task: Flickr30k and the recently proposed
Microsoft COCO."
"This paper discusses a new metric that has been applied to verify the quality
in translation between sentence pairs in parallel corpora of Arabic-English.
This metric combines two techniques, one based on sentence length and the other
based on compression code length. Experiments on sample test parallel
Arabic-English corpora indicate the combination of these two techniques
improves accuracy of the identification of satisfactory and unsatisfactory
sentence pairs compared to sentence length and compression code length alone.
The new method proposed in this research is effective at filtering noise and
reducing mis-translations resulting in greatly improved quality."
"This paper presents generalized probabilistic models for high-order
projective dependency parsing and an algorithmic framework for learning these
statistical models involving dependency trees. Partition functions and
marginals for high-order dependency trees can be computed efficiently, by
adapting our algorithms which extend the inside-outside algorithm to
higher-order cases. To show the effectiveness of our algorithms, we perform
experiments on three languages---English, Chinese and Czech, using maximum
conditional likelihood estimation for model training and L-BFGS for parameter
estimation. Our methods achieve competitive performance for English, and
outperform all previously reported dependency parsers for Chinese and Czech."
"Word reordering is one of the most difficult aspects of statistical machine
translation (SMT), and an important factor of its quality and efficiency.
Despite the vast amount of research published to date, the interest of the
community in this problem has not decreased, and no single method appears to be
strongly dominant across language pairs. Instead, the choice of the optimal
approach for a new translation task still seems to be mostly driven by
empirical trials. To orientate the reader in this vast and complex research
area, we present a comprehensive survey of word reordering viewed as a
statistical modeling challenge and as a natural language phenomenon. The survey
describes in detail how word reordering is modeled within different
string-based and tree-based SMT frameworks and as a stand-alone task, including
systematic overviews of the literature in advanced reordering modeling. We then
question why some approaches are more successful than others in different
language pairs. We argue that, besides measuring the amount of reordering, it
is important to understand which kinds of reordering occur in a given language
pair. To this end, we conduct a qualitative analysis of word reordering
phenomena in a diverse sample of language pairs, based on a large collection of
linguistic knowledge. Empirical results in the SMT literature are shown to
support the hypothesis that a few linguistic facts can be very useful to
anticipate the reordering characteristics of a language pair and to select the
SMT framework that best suits them."
"We develop novel first- and second-order features for dependency parsing
based on the Google Syntactic Ngrams corpus, a collection of subtree counts of
parsed sentences from scanned books. We also extend previous work on surface
$n$-gram features from Web1T to the Google Books corpus and from first-order to
second-order, comparing and analysing performance over newswire and web
treebanks.
  Surface and syntactic $n$-grams both produce substantial and complementary
gains in parsing accuracy across domains. Our best system combines the two
feature sets, achieving up to 0.8% absolute UAS improvements on newswire and
1.4% on web text."
"Recently proposed Skip-gram model is a powerful method for learning
high-dimensional word representations that capture rich semantic relationships
between words. However, Skip-gram as well as most prior work on learning word
representations does not take into account word ambiguity and maintain only
single representation per word. Although a number of Skip-gram modifications
were proposed to overcome this limitation and learn multi-prototype word
representations, they either require a known number of word meanings or learn
them using greedy heuristic approaches. In this paper we propose the Adaptive
Skip-gram model which is a nonparametric Bayesian extension of Skip-gram
capable to automatically learn the required number of representations for all
words at desired semantic resolution. We derive efficient online variational
learning algorithm for the model and empirically demonstrate its efficiency on
word-sense induction task."
"In this paper, we address the problems of Arabic Text Classification and
stemming using Transducers and Rational Kernels. We introduce a new stemming
technique based on the use of Arabic patterns (Pattern Based Stemmer). Patterns
are modelled using transducers and stemming is done without depending on any
dictionary. Using transducers for stemming, documents are transformed into
finite state transducers. This document representation allows us to use and
explore rational kernels as a framework for Arabic Text Classification.
Stemming experiments are conducted on three word collections and classification
experiments are done on the Saudi Press Agency dataset. Results show that our
approach, when compared with other approaches, is promising specially in terms
of Accuracy, Recall and F1."
"Statistical machine translation models have made great progress in improving
the translation quality. However, the existing models predict the target
translation with only the source- and target-side local context information. In
practice, distinguishing good translations from bad ones does not only depend
on the local features, but also rely on the global sentence-level information.
In this paper, we explore the source-side global sentence-level features for
target-side local translation prediction. We propose a novel
bilingually-constrained chunk-based convolutional neural network to learn
sentence semantic representations. With the sentence-level feature
representation, we further design a feed-forward neural network to better
predict translations using both local and global information. The large-scale
experiments show that our method can obtain substantial improvements in
translation quality over the strong baseline: the hierarchical phrase-based
translation model augmented with the neural network joint model."
"We reduce phrase-representation parsing to dependency parsing. Our reduction
is grounded on a new intermediate representation, ""head-ordered dependency
trees"", shown to be isomorphic to constituent trees. By encoding order
information in the dependency labels, we show that any off-the-shelf, trainable
dependency parser can be used to produce constituents. When this parser is
non-projective, we can perform discontinuous parsing in a very natural manner.
Despite the simplicity of our approach, experiments show that the resulting
parsers are on par with strong baselines, such as the Berkeley parser for
English and the best single system in the SPMRL-2014 shared task. Results are
particularly striking for discontinuous parsing of German, where we surpass the
current state of the art by a wide margin."
"We present a novel learning method for word embeddings designed for relation
classification. Our word embeddings are trained by predicting words between
noun pairs using lexical relation-specific features on a large unlabeled
corpus. This allows us to explicitly incorporate relation-specific information
into the word embeddings. The learned word embeddings are then used to
construct feature vectors for a relation classification model. On a
well-established semantic relation classification task, our method
significantly outperforms a baseline based on a previously introduced word
embedding method, and compares favorably to previous state-of-the-art models
that use syntactic information or manually constructed external resources."
"It is commonly accepted that machine translation is a more complex task than
part of speech tagging. But how much more complex? In this paper we make an
attempt to develop a general framework and methodology for computing the
informational and/or processing complexity of NLP applications and tasks. We
define a universal framework akin to a Turning Machine that attempts to fit
(most) NLP tasks into one paradigm. We calculate the complexities of various
NLP tasks using measures of Shannon Entropy, and compare `simple' ones such as
part of speech tagging to `complex' ones such as machine translation. This
paper provides a first, though far from perfect, attempt to quantify NLP tasks
under a uniform paradigm. We point out current deficiencies and suggest some
avenues for fruitful research."
"Hyperlinks and other relations in Wikipedia are a extraordinary resource
which is still not fully understood. In this paper we study the different types
of links in Wikipedia, and contrast the use of the full graph with respect to
just direct links. We apply a well-known random walk algorithm on two tasks,
word relatedness and named-entity disambiguation. We show that using the full
graph is more effective than just direct links by a large margin, that
non-reciprocal links harm performance, and that there is no benefit from
categories and infoboxes, with coherent results on both tasks. We set new
state-of-the-art figures for systems based on Wikipedia links, comparable to
systems exploiting several information sources and/or supervised machine
learning. Our approach is open source, with instruction to reproduce results,
and amenable to be integrated with complementary text-based methods."
"Most state-of-the-art systems today produce morphological analysis based only
on orthographic patterns. In contrast, we propose a model for unsupervised
morphological analysis that integrates orthographic and semantic views of
words. We model word formation in terms of morphological chains, from base
words to the observed words, breaking the chains into parent-child relations.
We use log-linear models with morpheme and word-level features to predict
possible parents, including their modifications, for each word. The limited set
of candidate parents for each word render contrastive estimation feasible. Our
model consistently matches or outperforms five state-of-the-art systems on
Arabic, English and Turkish."
"Recent work on end-to-end neural network-based architectures for machine
translation has shown promising results for En-Fr and En-De translation.
Arguably, one of the major factors behind this success has been the
availability of high quality parallel corpora. In this work, we investigate how
to leverage abundant monolingual corpora for neural machine translation.
Compared to a phrase-based and hierarchical baseline, we obtain up to $1.96$
BLEU improvement on the low-resource language pair Turkish-English, and $1.59$
BLEU on the focused domain task of Chinese-English chat messages. While our
method was initially targeted toward such tasks with less parallel data, we
show that it also extends to high resource languages such as Cs-En and De-En
where we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural
machine translation baselines, respectively."
"Morphological Analysis is an important branch of linguistics for any Natural
Language Processing Technology. Morphology studies the word structure and
formation of word of a language. In current scenario of NLP research,
morphological analysis techniques have become more popular day by day. For
processing any language, morphology of the word should be first analyzed.
Assamese language contains very complex morphological structure. In our work we
have used Apertium based Finite-State-Transducers for developing morphological
analyzer for Assamese Language with some limited domain and we get 72.7%
accuracy"
"We propose a novel convolutional architecture, named $gen$CNN, for word
sequence prediction. Different from previous work on neural network-based
language modeling and generation (e.g., RNN or LSTM), we choose not to greedily
summarize the history of words as a fixed length vector. Instead, we use a
convolutional neural network to predict the next word with the history of words
of variable length. Also different from the existing feedforward networks for
language modeling, our model can effectively fuse the local correlation and
global correlation in the word sequence, with a convolution-gating strategy
specifically designed for the task. We argue that our model can give adequate
representation of the history, and therefore can naturally exploit both the
short and long range dependencies. Our model is fast, easy to train, and
readily parallelized. Our extensive experiments on text generation and $n$-best
re-ranking in machine translation show that $gen$CNN outperforms the
state-of-the-arts with big margins."
"word2vec affords a simple yet powerful approach of extracting quantitative
variables from unstructured textual data. Over half of healthcare data is
unstructured and therefore hard to model without involved expertise in data
engineering and natural language processing. word2vec can serve as a bridge to
quickly gather intelligence from such data sources.
  In this study, we ran 650 megabytes of unstructured, medical chart notes from
the Providence Health & Services electronic medical record through word2vec. We
used two different approaches in creating predictive variables and tested them
on the risk of readmission for patients with COPD (Chronic Obstructive Lung
Disease). As a comparative benchmark, we ran the same test using the LACE risk
model (a single score based on length of stay, acuity, comorbid conditions, and
emergency department visits).
  Using only free text and mathematical might, we found word2vec comparable to
LACE in predicting the risk of readmission of COPD patients."
"In machine translation it is common phenomenon that machine-readable
dictionaries and standard parsing rules are not enough to ensure accuracy in
parsing and translating English phrases into Korean language, which is revealed
in misleading translation results due to consequent structural and semantic
ambiguities. This paper aims to suggest a solution to structural and semantic
ambiguities due to the idiomaticity and non-grammaticalness of phrases commonly
used in English language by applying bilingual phrase database in
English-Korean Machine Translation (EKMT). This paper firstly clarifies what
the phrase unit in EKMT is based on the definition of the English phrase,
secondly clarifies what kind of language unit can be the target of the phrase
database for EKMT, thirdly suggests a way to build the phrase database by
presenting the format of the phrase database with examples, and finally
discusses briefly the method to apply this bilingual phrase database to the
EKMT for structural and semantic disambiguation."
"This paper discusses the structure of Syntagma's Lexical Database (focused on
Italian). The basic database consists in four tables. Table Forms contains word
inflections, used by the POS-tagger for the identification of input-words.
Forms is related to Lemma. Table Lemma stores all kinds of grammatical features
of words, word-level semantic data and restrictions. In the table Meanings
meaning-related data are stored: definition, examples, domain, and semantic
information. Table Valency contains the argument structure of each meaning,
with syntactic and semantic features for each argument. The extended version of
SLD contains the links to Syntagma's Semantic Net and to the WordNet synsets of
other languages."
"This work addresses the problem of measuring how many languages a person
""effectively"" speaks given that some of the languages are close to each other.
In other words, to assign a meaningful number to her language portfolio.
Intuition says that someone who speaks fluently Spanish and Portuguese is
linguistically less proficient compared to someone who speaks fluently Spanish
and Chinese since it takes more effort for a native Spanish speaker to learn
Chinese than Portuguese. As the number of languages grows and their proficiency
levels vary, it gets even more complicated to assign a score to a language
portfolio. In this article we propose such a measure (""linguistic quotient"" -
LQ) that can account for these effects.
  We define the properties that such a measure should have. They are based on
the idea of coherent risk measures from the mathematical finance. Having laid
down the foundation, we propose one such a measure together with the algorithm
that works on languages classification tree as input.
  The algorithm together with the input is available online at lingvometer.com"
"Open domain relation extraction systems identify relation and argument
phrases in a sentence without relying on any underlying schema. However,
current state-of-the-art relation extraction systems are available only for
English because of their heavy reliance on linguistic tools such as
part-of-speech taggers and dependency parsers. We present a cross-lingual
annotation projection method for language independent relation extraction. We
evaluate our method on a manually annotated test set and present results on
three typologically different languages. We release these manual annotations
and extracted relations in 61 languages from Wikipedia."
"Dependency parsers are among the most crucial tools in natural language
processing as they have many important applications in downstream tasks such as
information retrieval, machine translation and knowledge acquisition. We
introduce the Yara Parser, a fast and accurate open-source dependency parser
based on the arc-eager algorithm and beam search. It achieves an unlabeled
accuracy of 93.32 on the standard WSJ test set which ranks it among the top
dependency parsers. At its fastest, Yara can parse about 4000 sentences per
second when in greedy mode (1 beam). When optimizing for accuracy (using 64
beams and Brown cluster features), Yara can parse 45 sentences per second. The
parser can be trained on any syntactic dependency treebank and different
options are provided in order to make it more flexible and tunable for specific
tasks. It is released with the Apache version 2.0 license and can be used for
both commercial and academic purposes. The parser can be found at
https://github.com/yahoo/YaraParser."
"Unsupervised word embeddings have been shown to be valuable as features in
supervised learning problems; however, their role in unsupervised problems has
been less thoroughly explored. In this paper, we show that embeddings can
likewise add value to the problem of unsupervised POS induction. In two
representative models of POS induction, we replace multinomial distributions
over the vocabulary with multivariate Gaussian distributions over word
embeddings and observe consistent improvements in eight languages. We also
analyze the effect of various choices while inducing word embeddings on
""downstream"" POS induction results."
"pymorphy2 is a morphological analyzer and generator for Russian and Ukrainian
languages. It uses large efficiently encoded lexi- cons built from OpenCorpora
and LanguageTool data. A set of linguistically motivated rules is developed to
enable morphological analysis and generation of out-of-vocabulary words
observed in real-world documents. For Russian pymorphy2 provides
state-of-the-arts morphological analysis quality. The analyzer is implemented
in Python programming language with optional C++ extensions. Emphasis is put on
ease of use, documentation and extensibility. The package is distributed under
a permissive open-source license, encouraging its use in both academic and
commercial setting."
"We describe a technique for attributing parts of a written text to a set of
unknown authors. Nothing is assumed to be known a priori about the writing
styles of potential authors. We use multiple independent clusterings of an
input text to identify parts that are similar and dissimilar to one another. We
describe algorithms necessary to combine the multiple clusterings into a
meaningful output. We show results of the application of the technique on texts
having multiple writing styles."
"This paper presents text normalization which is an integral part of any
text-to-speech synthesis system. Text normalization is a set of methods with a
task to write non-standard words, like numbers, dates, times, abbreviations,
acronyms and the most common symbols, in their full expanded form are
presented. The whole taxonomy for classification of non-standard words in
Croatian language together with rule-based normalization methods combined with
a lookup dictionary are proposed. Achieved token rate for normalization of
Croatian texts is 95%, where 80% of expanded words are in correct morphological
form."
"Discourse markers are universal linguistic events subject to language
variation. Although an extensive literature has already reported language
specific traits of these events, little has been said on their cross-language
behavior and on building an inventory of multilingual lexica of discourse
markers. This work describes new methods and approaches for the description,
classification, and annotation of discourse markers in the specific domain of
the Europarl corpus. The study of discourse markers in the context of
translation is crucial due to the idiomatic nature of these structures.
Multilingual lexica together with the functional analysis of such structures
are useful tools for the hard task of translating discourse markers into
possible equivalents from one language to another. Using Daniel Marcu's
validated discourse markers for English, extracted from the Brown Corpus, our
purpose is to build multilingual lexica of discourse markers for other
languages, based on machine translation techniques. The major assumption in
this study is that the usage of a discourse marker is independent of the
language, i.e., the rhetorical function of a discourse marker in a sentence in
one language is equivalent to the rhetorical function of the same discourse
marker in another language."
"Distributional models that learn rich semantic word representations are a
success story of recent NLP research. However, developing models that learn
useful representations of phrases and sentences has proved far harder. We
propose using the definitions found in everyday dictionaries as a means of
bridging this gap between lexical and phrasal semantics. Neural language
embedding models can be effectively trained to map dictionary definitions
(phrases) to (lexical) representations of the words defined by those
definitions. We present two applications of these architectures: ""reverse
dictionaries"" that return the name of a concept given a definition or
description and general-knowledge crossword question answerers. On both tasks,
neural language embedding models trained on definitions from a handful of
freely-available lexical resources perform as well or better than existing
commercial systems that rely on significant task-specific engineering. The
results highlight the effectiveness of both neural embedding architectures and
definition-based training for developing models that understand phrases and
sentences."
"Machine dialect interpretation assumes a real part in encouraging man-machine
correspondence and in addition men-men correspondence in Natural Language
Processing (NLP). Machine Translation (MT) alludes to utilizing machine to
change one dialect to an alternate. Statistical Machine Translation is a type
of MT consisting of Language Model (LM), Translation Model (TM) and decoder. In
this paper, Bengali to Assamese Statistical Machine Translation Model has been
created by utilizing Moses. Other translation tools like IRSTLM for Language
Model and GIZA-PP-V1.0.7 for Translation model are utilized within this
framework which is accessible in Linux situations. The purpose of the LM is to
encourage fluent output and the purpose of TM is to encourage similarity
between input and output, the decoder increases the probability of translated
text in target language. A parallel corpus of 17100 sentences in Bengali and
Assamese has been utilized for preparing within this framework. Measurable MT
procedures have not so far been generally investigated for Indian dialects. It
might be intriguing to discover to what degree these models can help the
immense continuous MT deliberations in the nation."
"The ability to classify spoken speech based on the style of speaking is an
important problem. With the advent of BPO's in recent times, specifically those
that cater to a population other than the local population, it has become
necessary for BPO's to identify people with certain style of speaking
(American, British etc). Today BPO's employ accent analysts to identify people
having the required style of speaking. This process while involving human bias,
it is becoming increasingly infeasible because of the high attrition rate in
the BPO industry. In this paper, we propose a new metric, which robustly and
accurately helps classify spoken speech based on the style of speaking. The
role of the proposed metric is substantiated by using it to classify real
speech data collected from over seventy different people working in a BPO. We
compare the performance of the metric against human experts who independently
carried out the classification process. Experimental results show that the
performance of the system using the novel metric performs better than two
different human expert."
"In general, self help systems are being increasingly deployed by service
based industries because they are capable of delivering better customer service
and increasingly the switch is to voice based self help systems because they
provide a natural interface for a human to interact with a machine. A speech
based self help system ideally needs a speech recognition engine to convert
spoken speech to text and in addition a language processing engine to take care
of any misrecognitions by the speech recognition engine. Any off-the-shelf
speech recognition engine is generally a combination of acoustic processing and
speech grammar. While this is the norm, we believe that ideally a speech
recognition application should have in addition to a speech recognition engine
a separate language processing engine to give the system better performance. In
this paper, we discuss ways in which the speech recognition engine and the
language processing engine can be combined to give a better user experience."
"This paper contributes a joint embedding model for predicting relations
between a pair of entities in the scenario of relation inference. It differs
from most stand-alone approaches which separately operate on either knowledge
bases or free texts. The proposed model simultaneously learns low-dimensional
vector representations for both triplets in knowledge repositories and the
mentions of relations in free texts, so that we can leverage the evidence both
resources to make more accurate predictions. We use NELL to evaluate the
performance of our approach, compared with cutting-edge methods. Results of
extensive experiments show that our model achieves significant improvement on
relation extraction."
"Several characteristics of written texts have been inferred from statistical
analysis derived from networked models. Even though many network measurements
have been adapted to study textual properties at several levels of complexity,
some textual aspects have been disregarded. In this paper, we study the
symmetry of word adjacency networks, a well-known representation of text as a
graph. A statistical analysis of the symmetry distribution performed in several
novels showed that most of the words do not display symmetric patterns of
connectivity. More specifically, the merged symmetry displayed a distribution
similar to the ubiquitous power-law distribution. Our experiments also revealed
that the studied metrics do not correlate with other traditional network
measurements, such as the degree or betweenness centrality. The effectiveness
of the symmetry measurements was verified in the authorship attribution task.
Interestingly, we found that specific authors prefer particular types of
symmetric motifs. As a consequence, the authorship of books could be accurately
identified in 82.5% of the cases, in a dataset comprising books written by 8
authors. Because the proposed measurements for text analysis are complementary
to the traditional approach, they can be used to improve the characterization
of text networks, which might be useful for related applications, such as those
relying on the identification of topical words and information retrieval."
"In applications involving conversational speech, data sparsity is a limiting
factor in building a better language model. We propose a simple,
language-independent method to quickly harvest large amounts of data from
Twitter to supplement a smaller training set that is more closely matched to
the domain. The techniques lead to a significant reduction in perplexity on
four low-resource languages even though the presence on Twitter of these
languages is relatively small. We also find that the Twitter text is more
useful for learning word classes than the in-domain text and that use of these
word classes leads to further reductions in perplexity. Additionally, we
introduce a method of using social and textual information to prioritize the
download queue during the Twitter crawling. This maximizes the amount of useful
data that can be collected, impacting both perplexity and vocabulary coverage."
"A pronoun resolution system which requires limited syntactic knowledge to
identify the antecedents of personal and reflexive pronouns in Turkish is
presented. As in its counterparts for languages like English, Spanish and
French, the core of the system is the constraints and preferences determined
empirically. In the evaluation phase, it performed considerably better than the
baseline algorithm used for comparison. The system is significant for its being
the first fully specified knowledge-poor computational framework for pronoun
resolution in Turkish where Turkish possesses different structural properties
from the languages for which knowledge-poor systems had been developed."
"Word embeddings -- distributed word representations that can be learned from
unlabelled data -- have been shown to have high utility in many natural
language processing applications. In this paper, we perform an extrinsic
evaluation of five popular word embedding methods in the context of four
sequence labelling tasks: POS-tagging, syntactic chunking, NER and MWE
identification. A particular focus of the paper is analysing the effects of
task-based updating of word representations. We show that when using word
embeddings as features, as few as several hundred training instances are
sufficient to achieve competitive results, and that word embeddings lead to
improvements over OOV words and out of domain. Perhaps more surprisingly, our
results indicate there is little difference between the different word
embedding methods, and that simple Brown clusters are often competitive with
word embeddings across all tasks we consider."
"In recent years, numerous studies have inferred personality and other traits
from people's online writing. While these studies are encouraging, more
information is needed in order to use these techniques with confidence. How do
linguistic features vary across different online media, and how much text is
required to have a representative sample for a person? In this paper, we
examine several large sets of online, user-generated text, drawn from Twitter,
email, blogs, and online discussion forums. We examine and compare
population-wide results for the linguistic measure LIWC, and the inferred
traits of Big5 Personality and Basic Human Values. We also empirically measure
the stability of these traits across different sized samples for each
individual. Our results highlight the importance of tuning models to each
online medium, and include guidelines for the minimum amount of text required
for a representative result."
"Entity Disambiguation aims to link mentions of ambiguous entities to a
knowledge base (e.g., Wikipedia). Modeling topical coherence is crucial for
this task based on the assumption that information from the same semantic
context tends to belong to the same topic. This paper presents a novel deep
semantic relatedness model (DSRM) based on deep neural networks (DNN) and
semantic knowledge graphs (KGs) to measure entity semantic relatedness for
topical coherence modeling. The DSRM is directly trained on large-scale KGs and
it maps heterogeneous types of knowledge of an entity from KGs to numerical
feature vectors in a latent space such that the distance between two
semantically-related entities is minimized. Compared with the state-of-the-art
relatedness approach proposed by (Milne and Witten, 2008a), the DSRM obtains
19.4% and 24.5% reductions in entity disambiguation errors on two publicly
available datasets respectively."
"This paper presents a pattern-based method that can be used to infer
adjectival scales, such as <lukewarm, warm, hot>, from a corpus. Specifically,
the proposed method uses lexical patterns to automatically identify and order
pairs of scalemates, followed by a filtering phase in which unrelated pairs are
discarded. For the filtering phase, several different similarity measures are
implemented and compared. The model presented in this paper is evaluated using
the current standard, along with a novel evaluation set, and shown to be at
least as good as the current state-of-the-art."
"Distributed vector representations for natural language vocabulary get a lot
of attention in contemporary computational linguistics. This paper summarizes
the experience of applying neural network language models to the task of
calculating semantic similarity for Russian. The experiments were performed in
the course of Russian Semantic Similarity Evaluation track, where our models
took from the 2nd to the 5th position, depending on the task.
  We introduce the tools and corpora used, comment on the nature of the shared
task and describe the achieved results. It was found out that Continuous
Skip-gram and Continuous Bag-of-words models, previously successfully applied
to English material, can be used for semantic modeling of Russian as well.
Moreover, we show that texts in Russian National Corpus (RNC) provide an
excellent training material for such models, outperforming other, much larger
corpora. It is especially true for semantic relatedness tasks (although
stacking models trained on larger corpora on top of RNC models improves
performance even more).
  High-quality semantic vectors learned in such a way can be used in a variety
of linguistic tasks and promise an exciting field for further study."
"Learning representations for semantic relations is important for various
tasks such as analogy detection, relational search, and relation
classification. Although there have been several proposals for learning
representations for individual words, learning word representations that
explicitly capture the semantic relations between words remains under
developed. We propose an unsupervised method for learning vector
representations for words such that the learnt representations are sensitive to
the semantic relations that exist between two words. First, we extract lexical
patterns from the co-occurrence contexts of two words in a corpus to represent
the semantic relations that exist between those two words. Second, we represent
a lexical pattern as the weighted sum of the representations of the words that
co-occur with that lexical pattern. Third, we train a binary classifier to
detect relationally similar vs. non-similar lexical pattern pairs. The proposed
method is unsupervised in the sense that the lexical pattern pairs we use as
train data are automatically sampled from a corpus, without requiring any
manual intervention. Our proposed method statistically significantly
outperforms the current state-of-the-art word representations on three
benchmark datasets for proportional analogy detection, demonstrating its
ability to accurately capture the semantic relations among words."
"In recent years, There has been a variety of research on discourse parsing,
particularly RST discourse parsing. Most of the recent work on RST parsing has
focused on implementing new types of features or learning algorithms in order
to improve accuracy, with relatively little focus on efficiency, robustness, or
practical use. Also, most implementations are not widely available. Here, we
describe an RST segmentation and parsing system that adapts models and feature
sets from various previous work, as described below. Its accuracy is near
state-of-the-art, and it was developed to be fast, robust, and practical. For
example, it can process short documents such as news articles or essays in less
than a second."
"Text segmentation task is an essential processing task for many of Natural
Language Processing (NLP) such as text summarization, text translation,
dialogue language understanding, among others. Turns segmentation considered
the key player in dialogue understanding task for building automatic
Human-Computer systems. In this paper, we introduce a novel approach to turn
segmentation into utterances for Egyptian spontaneous dialogues and Instance
Messages (IM) using Machine Learning (ML) approach as a part of automatic
understanding Egyptian spontaneous dialogues and IM task. Due to the lack of
Egyptian dialect dialogue corpus the system evaluated by our corpus includes
3001 turns, which are collected, segmented, and annotated manually from
Egyptian call-centers. The system achieves F1 scores of 90.74% and accuracy of
95.98%."
"Building dialogues systems interaction has recently gained considerable
attention, but most of the resources and systems built so far are tailored to
English and other Indo-European languages. The need for designing systems for
other languages is increasing such as Arabic language. For this reasons, there
are more interest for Arabic dialogue acts classification task because it a key
player in Arabic language understanding to building this systems. This paper
surveys different techniques for dialogue acts classification for Arabic. We
describe the main existing techniques for utterances segmentations and
classification, annotation schemas, and test corpora for Arabic dialogues
understanding that have introduced in the literature"
"Sarcasm is considered one of the most difficult problem in sentiment
analysis. In our ob-servation on Indonesian social media, for cer-tain topics,
people tend to criticize something using sarcasm. Here, we proposed two
additional features to detect sarcasm after a common sentiment analysis is
conducted. The features are the negativity information and the number of
interjection words. We also employed translated SentiWordNet in the sentiment
classification. All the classifications were conducted with machine learning
algorithms. The experimental results showed that the additional features are
quite effective in the sarcasm detection."
"The rise of social media such as blogs and social networks has fueled
interest in sentiment analysis. With the proliferation of reviews, ratings,
recommendations and other forms of online expression, online opinion has turned
into a kind of virtual currency for businesses looking to market their
products, identify new opportunities and manage their reputations, therefore
many are now looking to the field of sentiment analysis. In this paper, we
present a feature-based sentence level approach for Arabic sentiment analysis.
Our approach is using Arabic idioms/saying phrases lexicon as a key importance
for improving the detection of the sentiment polarity in Arabic sentences as
well as a number of novels and rich set of linguistically motivated features
contextual Intensifiers, contextual Shifter and negation handling), syntactic
features for conflicting phrases which enhance the sentiment classification
accuracy. Furthermore, we introduce an automatic expandable wide coverage
polarity lexicon of Arabic sentiment words. The lexicon is built with
gold-standard sentiment words as a seed which is manually collected and
annotated and it expands and detects the sentiment orientation automatically of
new sentiment words using synset aggregation technique and free online Arabic
lexicons and thesauruses. Our data focus on modern standard Arabic (MSA) and
Egyptian dialectal Arabic tweets and microblogs (hotel reservation, product
reviews, etc.). The experimental results using our resources and techniques
with SVM classifier indicate high performance levels, with accuracies of over
95%."
"Automatic Speech Recognition involves mainly two steps; feature extraction
and classification . Mel Frequency Cepstral Coefficient is used as one of the
prominent feature extraction techniques in ASR. Usually, the set of all 12 MFCC
coefficients is used as the feature vector in the classification step. But the
question is whether the same or improved classification accuracy can be
achieved by using a subset of 12 MFCC as feature vector. In this paper,
Fisher's ratio technique is used for selecting a subset of 12 MFCC coefficients
that contribute more in discriminating a pattern. The selected coefficients are
used in classification with Hidden Markov Model algorithm. The classification
accuracies that we get by using 12 coefficients and by using the selected
coefficients are compared."
"Statistical studies of languages have focused on the rank-frequency
distribution of words. Instead, we introduce here a measure of how word ranks
change in time and call this distribution \emph{rank diversity}. We calculate
this diversity for books published in six European languages since 1800, and
find that it follows a universal lognormal distribution. Based on the mean and
standard deviation associated with the lognormal distribution, we define three
different word regimes of languages: ""heads"" consist of words which almost do
not change their rank in time, ""bodies"" are words of general use, while ""tails""
are comprised by context-specific words and vary their rank considerably in
time. The heads and bodies reflect the size of language cores identified by
linguists for basic communication. We propose a Gaussian random walk model
which reproduces the rank variation of words in time and thus the diversity.
Rank diversity of words can be understood as the result of random variations in
rank, where the size of the variation depends on the rank itself. We find that
the core size is similar for all languages studied."
"We present an annotation schema as part of an effort to create a manually
annotated corpus for Arabic dialogue language understanding including spoken
dialogue and written ""chat"" dialogue for inquiry-answer domain. The proposed
schema handles mainly the request and response acts that occurs frequently in
inquiry-answer debate conversations expressing request services, suggests, and
offers. We applied the proposed schema on 83 Arabic inquiry-answer dialogues."
"Twitter, a popular social media outlet, has evolved into a vast source of
linguistic data, rich with opinion, sentiment, and discussion. Due to the
increasing popularity of Twitter, its perceived potential for exerting social
influence has led to the rise of a diverse community of automatons, commonly
referred to as bots. These inorganic and semi-organic Twitter entities can
range from the benevolent (e.g., weather-update bots, help-wanted-alert bots)
to the malevolent (e.g., spamming messages, advertisements, or radical
opinions). Existing detection algorithms typically leverage meta-data (time
between tweets, number of followers, etc.) to identify robotic accounts. Here,
we present a powerful classification scheme that exclusively uses the natural
language text from organic users to provide a criterion for identifying
accounts posting automated messages. Since the classifier operates on text
alone, it is flexible and may be applied to any textual data beyond the
Twitter-sphere."
"This thesis presents a study about the integration of information about
Multiword Expressions (MWEs) into parsing with Combinatory Categorial Grammar
(CCG). We build on previous work which has shown the benefit of adding
information about MWEs to syntactic parsing by implementing a similar pipeline
with CCG parsing. More specifically, we collapse MWEs to one token in training
and test data in CCGbank, a corpus which contains sentences annotated with CCG
derivations. Our collapsing algorithm however can only deal with MWEs when they
form a constituent in the data which is one of the limitations of our approach.
  We study the effect of collapsing training and test data. A parsing effect
can be obtained if collapsed data help the parser in its decisions and a
training effect can be obtained if training on the collapsed data improves
results. We also collapse the gold standard and show that our model
significantly outperforms the baseline model on our gold standard, which
indicates that there is a training effect. We show that the baseline model
performs significantly better on our gold standard when the data are collapsed
before parsing than when the data are collapsed after parsing which indicates
that there is a parsing effect. We show that these results can lead to improved
performance on the non-collapsed standard benchmark although we fail to show
that it does so significantly. We conclude that despite the limited settings,
there are noticeable improvements from using MWEs in parsing. We discuss ways
in which the incorporation of MWEs into parsing can be improved and hypothesize
that this will lead to more substantial results.
  We finally show that turning the MWE recognition part of the pipeline into an
experimental part is a useful thing to do as we obtain different results with
different recognizers."
"Word embedding, which refers to low-dimensional dense vector representations
of natural words, has demonstrated its power in many natural language
processing tasks. However, it may suffer from the inaccurate and incomplete
information contained in the free text corpus as training data. To tackle this
challenge, there have been quite a few works that leverage knowledge graphs as
an additional information source to improve the quality of word embedding.
Although these works have achieved certain success, they have neglected some
important facts about knowledge graphs: (i) many relationships in knowledge
graphs are \emph{many-to-one}, \emph{one-to-many} or even \emph{many-to-many},
rather than simply \emph{one-to-one}; (ii) most head entities and tail entities
in knowledge graphs come from very different semantic spaces. To address these
issues, in this paper, we propose a new algorithm named ProjectNet. ProjecNet
models the relationships between head and tail entities after transforming them
with different low-rank projection matrices. The low-rank projection can allow
non \emph{one-to-one} relationships between entities, while different
projection matrices for head and tail entities allow them to originate in
different semantic spaces. The experimental results demonstrate that ProjectNet
yields more accurate word embedding than previous works, thus leads to clear
improvements in various natural language processing tasks."
"Most state-of-the-art named entity recognition (NER) systems rely on
handcrafted features and on the output of other NLP tasks such as
part-of-speech (POS) tagging and text chunking. In this work we propose a
language-independent NER system that uses automatically learned features only.
Our approach is based on the CharWNN deep neural network, which uses word-level
and character-level representations (embeddings) to perform sequential
classification. We perform an extensive number of experiments using two
annotated corpora in two different languages: HAREM I corpus, which contains
texts in Portuguese; and the SPA CoNLL-2002 corpus, which contains texts in
Spanish. Our experimental results shade light on the contribution of neural
character embeddings for NER. Moreover, we demonstrate that the same neural
network which has been successfully applied to POS tagging can also achieve
state-of-the-art results for language-independet NER, using the same
hyperparameters, and without any handcrafted features. For the HAREM I corpus,
CharWNN outperforms the state-of-the-art system by 7.9 points in the F1-score
for the total scenario (ten NE classes), and by 7.2 points in the F1 for the
selective scenario (five NE classes)."
"Knowledge graph embedding refers to projecting entities and relations in
knowledge graph into continuous vector spaces. State-of-the-art methods, such
as TransE, TransH, and TransR build embeddings by treating relation as
translation from head entity to tail entity. However, previous models can not
deal with reflexive/one-to-many/many-to-one/many-to-many relations properly, or
lack of scalability and efficiency. Thus, we propose a novel method, flexible
translation, named TransF, to address the above issues. TransF regards relation
as translation between head entity vector and tail entity vector with flexible
magnitude. To evaluate the proposed model, we conduct link prediction and
triple classification on benchmark datasets. Experimental results show that our
method remarkably improve the performance compared with several
state-of-the-art baselines."
"Translation Memory (TM) systems are one of the most widely used translation
technologies. An important part of TM systems is the matching algorithm that
determines what translations get retrieved from the bank of available
translations to assist the human translator. Although detailed accounts of the
matching algorithms used in commercial systems can't be found in the
literature, it is widely believed that edit distance algorithms are used. This
paper investigates and evaluates the use of several matching algorithms,
including the edit distance algorithm that is believed to be at the heart of
most modern commercial TM systems. This paper presents results showing how well
various matching algorithms correlate with human judgments of helpfulness
(collected via crowdsourcing with Amazon's Mechanical Turk). A new algorithm
based on weighted n-gram precision that can be adjusted for translator length
preferences consistently returns translations judged to be most helpful by
translators for multiple domains and language pairs."
"We describe the latest improvements to the IBM English conversational
telephone speech recognition system. Some of the techniques that were found
beneficial are: maxout networks with annealed dropout rates; networks with a
very large number of outputs trained on 2000 hours of data; joint modeling of
partially unfolded recurrent neural networks and convolutional nets by
combining the bottleneck and output layers and retraining the resulting model;
and lastly, sophisticated language model rescoring with exponential and neural
network LMs. These techniques result in an 8.0% word error rate on the
Switchboard part of the Hub5-2000 evaluation test set which is 23% relative
better than our previous best published result."
"The development of methods to deal with the informative contents of the text
units in the matching process is a major challenge in automatic summary
evaluation systems that use fixed n-gram matching. The limitation causes
inaccurate matching between units in a peer and reference summaries. The
present study introduces a new Keyphrase based Summary Evaluator KpEval for
evaluating automatic summaries. The KpEval relies on the keyphrases since they
convey the most important concepts of a text. In the evaluation process, the
keyphrases are used in their lemma form as the matching text unit. The system
was applied to evaluate different summaries of Arabic multi-document data set
presented at TAC2011. The results showed that the new evaluation technique
correlates well with the known evaluation systems: Rouge1, Rouge2, RougeSU4,
and AutoSummENG MeMoG. KpEval has the strongest correlation with AutoSummENG
MeMoG, Pearson and spearman correlation coefficient measures are 0.8840, 0.9667
respectively."
"NLP tasks differ in the semantic information they require, and at this time
no single se- mantic representation fulfills all requirements. Logic-based
representations characterize sentence structure, but do not capture the graded
aspect of meaning. Distributional models give graded similarity ratings for
words and phrases, but do not capture sentence structure in the same detail as
logic-based approaches. So it has been argued that the two are complementary.
We adopt a hybrid approach that combines logic-based and distributional
semantics through probabilistic logic inference in Markov Logic Networks
(MLNs). In this paper, we focus on the three components of a practical system
integrating logical and distributional models: 1) Parsing and task
representation is the logic-based part where input problems are represented in
probabilistic logic. This is quite different from representing them in standard
first-order logic. 2) For knowledge base construction we form weighted
inference rules. We integrate and compare distributional information with other
sources, notably WordNet and an existing paraphrase collection. In particular,
we use our system to evaluate distributional lexical entailment approaches. We
use a variant of Robinson resolution to determine the necessary inference
rules. More sources can easily be added by mapping them to logical rules; our
system learns a resource-specific weight that corrects for scaling differences
between resources. 3) In discussing probabilistic inference, we show how to
solve the inference problems efficiently. To evaluate our approach, we use the
task of textual entailment (RTE), which can utilize the strengths of both
logic-based and distributional representations. In particular we focus on the
SICK dataset, where we achieve state-of-the-art results."
"Meaning of a word varies from one domain to another. Despite this important
domain dependence in word semantics, existing word representation learning
methods are bound to a single domain. Given a pair of
\emph{source}-\emph{target} domains, we propose an unsupervised method for
learning domain-specific word representations that accurately capture the
domain-specific aspects of word semantics. First, we select a subset of
frequent words that occur in both domains as \emph{pivots}. Next, we optimize
an objective function that enforces two constraints: (a) for both source and
target domain documents, pivots that appear in a document must accurately
predict the co-occurring non-pivots, and (b) word representations learnt for
pivots must be similar in the two domains. Moreover, we propose a method to
perform domain adaptation using the learnt word representations. Our proposed
method significantly outperforms competitive baselines including the
state-of-the-art domain-insensitive word representations, and reports best
sentiment classification accuracies for all domain-pairs in a benchmark
dataset."
"In this paper, we give an overview for the shared task at the 4th CCF
Conference on Natural Language Processing \& Chinese Computing (NLPCC 2015):
Chinese word segmentation and part-of-speech (POS) tagging for micro-blog
texts. Different with the popular used newswire datasets, the dataset of this
shared task consists of the relatively informal micro-texts. The shared task
has two sub-tasks: (1) individual Chinese word segmentation and (2) joint
Chinese word segmentation and POS Tagging. Each subtask has three tracks to
distinguish the systems with different resources. We first introduce the
dataset and task, then we characterize the different approaches of the
participating systems, report the test results, and provide a overview analysis
of these results. An online system is available for open registration and
evaluation at http://nlp.fudan.edu.cn/nlpcc2015."
"Learning vector representation for words is an important research field which
may benefit many natural language processing tasks. Two limitations exist in
nearly all available models, which are the bias caused by the context
definition and the lack of knowledge utilization. They are difficult to tackle
because these algorithms are essentially unsupervised learning approaches.
Inspired by deep learning, the authors propose a supervised framework for
learning vector representation of words to provide additional supervised fine
tuning after unsupervised learning. The framework is knowledge rich approacher
and compatible with any numerical vectors word representation. The authors
perform both intrinsic evaluation like attributional and relational similarity
prediction and extrinsic evaluations like the sentence completion and sentiment
analysis. Experiments results on 6 embeddings and 4 tasks with 10 datasets show
that the proposed fine tuning framework may significantly improve the quality
of the vector representation of words."
"In this introductory article we present the basics of an approach to
implementing computational interpreting of natural language aiming to model the
meanings of words and phrases. Unlike other approaches, we attempt to define
the meanings of text fragments in a composable and computer interpretable way.
We discuss models and ideas for detecting different types of semantic
incomprehension and choosing the interpretation that makes most sense in a
given context. Knowledge representation is designed for handling
context-sensitive and uncertain / imprecise knowledge, and for easy
accommodation of new information. It stores quantitative information capturing
the essence of the concepts, because it is crucial for working with natural
language understanding and reasoning. Still, the representation is general
enough to allow for new knowledge to be learned, and even generated by the
system. The article concludes by discussing some reasoning-related topics:
possible approaches to generation of new abstract concepts, and describing
situations and concepts in words (e.g. for specifying interpretation
difficulties)."
"This paper reports on the comparison of the subject and object of verbs in
their usage between phishing emails and legitimate emails. The purpose of this
research is to explore whether the syntactic structures and subjects and
objects of verbs can be distinguishable features for phishing detection. To
achieve the objective, we have conducted two series of experiments: the
syntactic similarity for sentences, and the subject and object of verb
comparison. The results of the experiments indicated that both features can be
used for some verbs, but more work has to be done for others."
"Sequence-to-sequence translation methods based on generation with a
side-conditioned language model have recently shown promising results in
several tasks. In machine translation, models conditioned on source side words
have been used to produce target-language text, and in image captioning, models
conditioned images have been used to generate caption text. Past work with this
approach has focused on large vocabulary tasks, and measured quality in terms
of BLEU. In this paper, we explore the applicability of such models to the
qualitatively different grapheme-to-phoneme task. Here, the input and output
side vocabularies are small, plain n-gram models do well, and credit is only
given when the output is exactly correct. We find that the simple
side-conditioned generation approach is able to rival the state-of-the-art, and
we are able to significantly advance the stat-of-the-art with bi-directional
long short-term memory (LSTM) neural networks that use the same alignment
information that is used in conventional approaches."
"We describe an approach to create a diverse set of predictions with spectral
learning of latent-variable PCFGs (L-PCFGs). Our approach works by creating
multiple spectral models where noise is added to the underlying features in the
training set before the estimation of each model. We describe three ways to
decode with multiple models. In addition, we describe a simple variant of the
spectral algorithm for L-PCFGs that is fast and leads to compact models. Our
experiments for natural language parsing, for English and German, show that we
get a significant improvement over baselines comparable to state of the art.
For English, we achieve the $F_1$ score of 90.18, and for German we achieve the
$F_1$ score of 83.38."
"Representation learning of knowledge bases (KBs) aims to embed both entities
and relations into a low-dimensional space. Most existing methods only consider
direct relations in representation learning. We argue that multiple-step
relation paths also contain rich inference patterns between entities, and
propose a path-based representation learning model. This model considers
relation paths as translations between entities for representation learning,
and addresses two key challenges: (1) Since not all relation paths are
reliable, we design a path-constraint resource allocation algorithm to measure
the reliability of relation paths. (2) We represent relation paths via semantic
composition of relation embeddings. Experimental results on real-world datasets
show that, as compared with baselines, our model achieves significant and
consistent improvements on knowledge base completion and relation extraction
from text."