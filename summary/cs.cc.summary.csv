summary
"A special case of the satisfiability problem, in which the clauses have a
hierarchical structure, is shown to be solvable in linear time, assuming that
the clauses have been represented in a convenient way."
"We discuss properties of recursive schemas related to McCarthy's ``91
function'' and to Takeuchi's triple recursion. Several theorems are proposed as
interesting candidates for machine verification, and some intriguing open
questions are raised."
"Hemaspaandra et al. proved that, for $m > 0$ and $0 < i < k - 1$: if
$\Sigma_i^p \BoldfaceDelta DIFF_m(\Sigma_k^p)$ is closed under complementation,
then $DIFF_m(\Sigma_k^p) = coDIFF_m(\Sigma_k^p)$. This sharply asymmetric
result fails to apply to the case in which the hypothesis is weakened by
allowing the $\Sigma_i^p$ to be replaced by any class in its difference
hierarchy. We so extend the result by proving that, for $s,m > 0$ and $0 < i <
k - 1$: if $DIFF_s(\Sigma_i^p) \BoldfaceDelta DIFF_m(\Sigma_k^p)$ is closed
under complementation, then $DIFF_m(\Sigma_k^p) = coDIFF_m(\Sigma_k^p)$."
"Ko [RAIRO 24, 1990] and Bruschi [TCS 102, 1992] showed that in some
relativized world, PSPACE (in fact, ParityP) contains a set that is immune to
the polynomial hierarchy (PH). In this paper, we study and settle the question
of (relativized) separations with immunity for PH and the counting classes PP,
C_{=}P, and ParityP in all possible pairwise combinations. Our main result is
that there is an oracle A relative to which C_{=}P contains a set that is
immune to BPP^{ParityP}. In particular, this C_{=}P^A set is immune to PH^{A}
and ParityP^{A}. Strengthening results of Tor\'{a}n [J.ACM 38, 1991] and Green
[IPL 37, 1991], we also show that, in suitable relativizations, NP contains a
C_{=}P-immune set, and ParityP contains a PP^{PH}-immune set. This implies the
existence of a C_{=}P^{B}-simple set for some oracle B, which extends results
of Balc\'{a}zar et al. [SIAM J.Comp. 14, 1985; RAIRO 22, 1988] and provides the
first example of a simple set in a class not known to be contained in PH. Our
proof technique requires a circuit lower bound for ``exact counting'' that is
derived from Razborov's [Mat. Zametki 41, 1987] lower bound for majority."
"We study the question of whether every P set has an easy (i.e.,
polynomial-time computable) census function. We characterize this question in
terms of unlikely collapses of language and function classes such as the
containment of #P_1 in FP, where #P_1 is the class of functions that count the
witnesses for tally NP sets. We prove that every #P_{1}^{PH} function can be
computed in FP^{#P_{1}^{#P_{1}}}. Consequently, every P set has an easy census
function if and only if every set in the polynomial hierarchy does. We show
that the assumption of #P_1 being contained in FP implies P = BPP and that PH
is contained in MOD_{k}P for each k \geq 2, which provides further evidence
that not all sets in P have an easy census function. We also relate a set's
property of having an easy census function to other well-studied properties of
sets, such as rankability and scalability (the closure of the rankable sets
under P-isomorphisms). Finally, we prove that it is no more likely that the
census function of any set in P can be approximated (more precisely, can be
n^{\alpha}-enumerated in time n^{\beta} for fixed \alpha and \beta) than that
it can be precisely computed in polynomial time."
"Building upon the known generalized-quantifier-based first-order
characterization of LOGCFL, we lay the groundwork for a deeper investigation.
Specifically, we examine subclasses of LOGCFL arising from varying the arity
and nesting of groupoidal quantifiers. Our work extends the elaborate theory
relating monoidal quantifiers to NC1 and its subclasses. In the absence of the
BIT predicate, we resolve the main issues: we show in particular that no single
outermost unary groupoidal quantifier with FO can capture all the context-free
languages, and we obtain the surprising result that a variant of Greibach's
``hardest context-free language'' is LOGCFL-complete under quantifier-free
BIT-free projections. We then prove that FO with unary groupoidal quantifiers
is strictly more expressive with the BIT predicate than without. Considering a
particular groupoidal quantifier, we prove that first-order logic with majority
of pairs is strictly more expressive than first-order with majority of
individuals. As a technical tool of independent interest, we define the notion
of an aperiodic nondeterministic finite automaton and prove that FO
translations are precisely the mappings computed by single-valued aperiodic
nondeterministic finite transducers."
"A notion of generalized quantifier in computational complexity theory is
explored and used to give a unified treatment of leaf language definability,
oracle separations, type 2 operators, and circuits with monoidal gates.
Relations to Lindstroem quantifiers are pointed out."
"We consider the problems of finding the lexicographically minimal (or
maximal) satisfying assignment of propositional formulae for different
restricted formula classes. It turns out that for each class from our
framework, the above problem is either polynomial time solvable or complete for
OptP. We also consider the problem of deciding if in the optimal assignment the
largest variable gets value 1. We show that this problem is either in P or P^NP
complete."
"We propose an algorithm of generating hard instances for the Satisfying
Assignment Search Problem (in short, SAT). The algorithm transforms instances
of the integer factorization problem into SAT instances efficiently by using
the Chinese Remainder Theorem. For example, it is possible to construct SAT
instances with about 5,600 variables that is as hard as factorizing 100 bit
integers."
"Let L be an infinite regular language on a totally ordered alphabet (A,<).
Feeding a finite deterministic automaton (with output) with the words of L
enumerated lexicographically with respect to < leads to an infinite sequence
over the output alphabet of the automaton. This process generalizes the concept
of k-automatic sequence for abstract numeration systems on a regular language
(instead of systems in base k). Here, I study the first properties of these
sequences and their relations with numeration systems."
"In the early 1980s, Selman's seminal work on positive Turing reductions
showed that positive Turing reduction to NP yields no greater computational
power than NP itself. Thus, positive Turing and Turing reducibility to NP
differ sharply unless the polynomial hierarchy collapses.
  We show that the situation is quite different for DP, the next level of the
boolean hierarchy. In particular, positive Turing reduction to DP already
yields all (and only) sets Turing reducibility to NP. Thus, positive Turing and
Turing reducibility to DP yield the same class. Additionally, we show that an
even weaker class, P(NP[1]), can be substituted for DP in this context."
"We continue the study of robust reductions initiated by Gavalda and Balcazar.
In particular, a 1991 paper of Gavalda and Balcazar claimed an optimal
separation between the power of robust and nondeterministic strong reductions.
Unfortunately, their proof is invalid. We re-establish their theorem.
  Generalizing robust reductions, we note that robustly strong reductions are
built from two restrictions, robust underproductivity and robust
overproductivity, both of which have been separately studied before in other
contexts. By systematically analyzing the power of these reductions, we explore
the extent to which each restriction weakens the power of reductions. We show
that one of these reductions yields a new, strong form of the Karp-Lipton
Theorem."
"It is known that for any class C closed under union and intersection, the
Boolean closure of C, the Boolean hierarchy over C, and the symmetric
difference hierarchy over C all are equal. We prove that these equalities hold
for any complexity class closed under intersection; in particular, they thus
hold for unambiguous polynomial time (UP). In contrast to the NP case, we prove
that the Hausdorff hierarchy and the nested difference hierarchy over UP both
fail to capture the Boolean closure of UP in some relativized worlds.
  Karp and Lipton proved that if nondeterministic polynomial time has sparse
Turing-complete sets, then the polynomial hierarchy collapses. We establish the
first consequences from the assumption that unambiguous polynomial time has
sparse Turing-complete sets: (a) UP is in Low_2, where Low_2 is the second
level of the low hierarchy, and (b) each level of the unambiguous polynomial
hierarchy is contained one level lower in the promise unambiguous polynomial
hierarchy than is otherwise known to be the case."
"We introduce a generalization of Selman's P-selectivity that yields a more
flexible notion of selectivity, called (polynomial-time) multi-selectivity, in
which the selector is allowed to operate on multiple input strings. Since our
introduction of this class, it has been used to prove the first known (and
optimal) lower bounds for generalized selectivity-like classes in terms of
EL_2, the second level of the extended low hierarchy. We study the resulting
selectivity hierarchy, denoted by SH, which we prove does not collapse. In
particular, we study the internal structure and the properties of SH and
completely establish, in terms of incomparability and strict inclusion, the
relations between our generalized selectivity classes and Ogihara's P-mc
(polynomial-time membership-comparable) classes. Although SH is a strictly
increasing infinite hierarchy, we show that the core results that hold for the
P-selective sets and that prove them structurally simple also hold for SH. In
particular, all sets in SH have small circuits; the NP sets in SH are in Low_2,
the second level of the low hierarchy within NP; and SAT cannot be in SH unless
P = NP. Finally, it is known that P-Sel, the class of P-selective sets, is not
closed under union or intersection. We provide an extended selectivity
hierarchy that is based on SH and that is large enough to capture those
closures of the P-selective sets, and yet, in contrast with the P-mc classes,
is refined enough to distinguish them."
"Can easy sets only have easy certificate schemes? In this paper, we study the
class of sets that, for all NP certificate schemes (i.e., NP machines), always
have easy acceptance certificates (i.e., accepting paths) that can be computed
in polynomial time. We also study the class of sets that, for all NP
certificate schemes, infinitely often have easy acceptance certificates.
  In particular, we provide equivalent characterizations of these classes in
terms of relative generalized Kolmogorov complexity, showing that they are
robust. We also provide structural conditions---regarding immunity and class
collapses---that put upper and lower bounds on the sizes of these two classes.
Finally, we provide negative results showing that some of our positive claims
are optimal with regard to being relativizable. Our negative results are proven
using a novel observation: we show that the classical ``wide spacing'' oracle
construction technique yields instant non-bi-immunity results. Furthermore, we
establish a result that improves upon Baker, Gill, and Solovay's classical
result that NP \neq P = NP \cap coNP holds in some relativized world."
"In 1876, Lewis Carroll proposed a voting system in which the winner is the
candidate who with the fewest changes in voters' preferences becomes a
Condorcet winner---a candidate who beats all other candidates in pairwise
majority-rule elections. Bartholdi, Tovey, and Trick provided a lower
bound---NP-hardness---on the computational complexity of determining the
election winner in Carroll's system. We provide a stronger lower bound and an
upper bound that matches our lower bound. In particular, determining the winner
in Carroll's system is complete for parallel access to NP, i.e., it is complete
for $\thetatwo$, for which it becomes the most natural complete problem known.
It follows that determining the winner in Carroll's elections is not
NP-complete unless the polynomial hierarchy collapses."
"We prove that the join of two sets may actually fall into a lower level of
the extended low hierarchy than either of the sets. In particular, there exist
sets that are not in the second level of the extended low hierarchy, EL_2, yet
their join is in EL_2. That is, in terms of extended lowness, the join operator
can lower complexity. Since in a strong intuitive sense the join does not lower
complexity, our result suggests that the extended low hierarchy is unnatural as
a complexity measure. We also study the closure properties of EL_ and prove
that EL_2 is not closed under certain Boolean operations. To this end, we
establish the first known (and optimal) EL_2 lower bounds for certain notions
generalizing Selman's P-selectivity, which may be regarded as an interesting
result in its own right."
"Rice's Theorem states that every nontrivial language property of the
recursively enumerable sets is undecidable. Borchert and Stephan initiated the
search for complexity-theoretic analogs of Rice's Theorem. In particular, they
proved that every nontrivial counting property of circuits is UP-hard, and that
a number of closely related problems are SPP-hard.
  The present paper studies whether their UP-hardness result itself can be
improved to SPP-hardness. We show that their UP-hardness result cannot be
strengthened to SPP-hardness unless unlikely complexity class containments
hold. Nonetheless, we prove that every P-constructibly bi-infinite counting
property of circuits is SPP-hard. We also raise their general lower bound from
unambiguous nondeterminism to constant-ambiguity nondeterminism."
"A decade ago, a beautiful paper by Wagner developed a ``toolkit'' that in
certain cases allows one to prove problems hard for parallel access to NP.
However, the problems his toolkit applies to most directly are not overly
natural. During the past year, problems that previously were known only to be
NP-hard or coNP-hard have been shown to be hard even for the class of sets
solvable via parallel access to NP. Many of these problems are longstanding and
extremely natural, such as the Minimum Equivalent Expression problem (which was
the original motivation for creating the polynomial hierarchy), the problem of
determining the winner in the election system introduced by Lewis Carroll in
1876, and the problem of determining on which inputs heuristic algorithms
perform well. In the present article, we survey this recent progress in raising
lower bounds."
"One way of suggesting that an NP problem may not be NP-complete is to show
that it is in the class UP. We suggest an analogous new approach---weaker in
strength of evidence but more broadly applicable---to suggesting that
concrete~NP problems are not NP-complete. In particular we introduce the class
EP, the subclass of NP consisting of those languages accepted by NP machines
that when they accept always have a number of accepting paths that is a power
of two. Since if any NP-complete set is in EP then all NP sets are in EP, it
follows---with whatever degree of strength one believes that EP differs from
NP---that membership in EP can be viewed as evidence that a problem is not
NP-complete.
  We show that the negation equivalence problem for OBDDs (ordered binary
decision diagrams) and the interchange equivalence problem for 2-dags are in
EP. We also show that for boolean negation the equivalence problem is in
EP^{NP}, thus tightening the existing NP^{NP} upper bound. We show that FewP,
bounded ambiguity polynomial time, is contained in EP, a result that is not
known to follow from the previous SPP upper bound. For the three problems and
classes just mentioned with regard to EP, no proof of membership/containment in
UP is known, and for the problem just mentioned with regard to EP^{NP}, no
proof of membership in UP^{NP} is known. Thus, EP is indeed a tool that gives
evidence against NP-completeness in natural cases where UP cannot currently be
applied."
"A generalization of numeration system in which the set N of the natural
numbers is recognizable by finite automata can be obtained by describing a
lexicographically ordered infinite regular language. Here we show that if P
belonging to Q[x] is a polynomial such that P(N) is a subset of N then we can
construct a numeration system in which the set of representations of P(N) is
regular. The main issue in this construction is to setup a regular language
with a density function equals to P(n+1)-P(n) for n large enough."
"We study the effect of query order on computational power, and show that
$\pjk$-the languages computable via a polynomial-time machine given one query
to the jth level of the boolean hierarchy followed by one query to the kth
level of the boolean hierarchy-equals $\redttnp{j+2k-1}$ if j is even and k is
odd, and equals $\redttnp{j+2k}$ otherwise. Thus, unless the polynomial
hierarchy collapses, it holds that for each $1\leq j \leq k$: $\pjk = \pkj \iff
(j=k) \lor (j{is even} \land k=j+1)$. We extend our analysis to apply to more
general query classes."
"During the past decade, nine papers have obtained increasingly strong
consequences from the assumption that boolean or bounded-query hierarchies
collapse. The final four papers of this nine-paper progression actually achieve
downward collapse---that is, they show that high-level collapses induce
collapses at (what beforehand were thought to be) lower complexity levels. For
example, for each $k\geq 2$ it is now known that if $\psigkone=\psigktwo$ then
$\ph=\sigmak$. This article surveys the history, the results, and the
technique---the so-called easy-hard method---of these nine papers."
"Do complexity classes have many-one complete sets if and only if they have
Turing-complete sets? We prove that there is a relativized world in which a
relatively natural complexity class-namely a downward closure of NP, \rsnnp -
has Turing-complete sets but has no many-one complete sets. In fact, we show
that in the same relativized world this class has 2-truth-table complete sets
but lacks 1-truth-table complete sets. As part of the groundwork for our
result, we prove that \rsnnp has many equivalent forms having to do with
ordered and parallel access to $\np$ and $\npinterconp$."
"Hemaspaandra, Hempel, and Wechsung [cs.CC/9909020] raised the following
questions: If one is allowed one question to each of two different information
sources, does the order in which one asks the questions affect the class of
problems that one can solve with the given access? If so, which order yields
the greater computational power?
  The answers to these questions have been learned-inasfar as they can be
learned without resolving whether or not the polynomial hierarchy collapses-for
both the polynomial hierarchy and the boolean hierarchy. In the polynomial
hierarchy, query order never matters. In the boolean hierarchy, query order
sometimes does not matter and, unless the polynomial hierarchy collapses,
sometimes does matter. Furthermore, the study of query order has yielded
dividends in seemingly unrelated areas, such as bottleneck computations and
downward translation of equality.
  In this article, we present some of the central results on query order. The
article is written in such a way as to encourage the reader to try his or her
own hand at proving some of these results. We also give literature pointers to
the quickly growing set of related results and applications."
"Hemaspaandra, Hempel, and Wechsung [cs.CC/9909020] initiated the field of
query order, which studies the ways in which computational power is affected by
the order in which information sources are accessed. The present paper studies,
for the first time, query order as it applies to the levels of the polynomial
hierarchy. We prove that the levels of the polynomial hierarchy are
order-oblivious. Yet, we also show that these ordered query classes form new
levels in the polynomial hierarchy unless the polynomial hierarchy collapses.
We prove that all leaf language classes - and thus essentially all standard
complexity classes - inherit all order-obliviousness results that hold for P."
"We study the computational power of machines that specify their own
acceptance types, and show that they accept exactly the languages that
$\manyonesharp$-reduce to NP sets. A natural variant accepts exactly the
languages that $\manyonesharp$-reduce to P sets. We show that these two classes
coincide if and only if $\psone = \psnnoplusbigohone$, where the latter class
denotes the sets acceptable via at most one question to $\sharpp$ followed by
at most a constant number of questions to $\np$."
"Downward collapse (a.k.a. upward separation) refers to cases where the
equality of two larger classes implies the equality of two smaller classes. We
provide an unqualified downward collapse result completely within the
polynomial hierarchy. In particular, we prove that, for k > 2, if $\psigkone =
\psigktwo$ then $\sigmak = \pik = \ph$. We extend this to obtain a more general
downward collapse result."
"Downward translation of equality refers to cases where a collapse of some
pair of complexity classes would induce a collapse of some other pair of
complexity classes that (a priori) one expects are smaller. Recently, the first
downward translation of equality was obtained that applied to the polynomial
hierarchy-in particular, to bounded access to its levels [cs.CC/9910007]. In
this paper, we provide a much broader downward translation that extends not
only that downward translation but also that translation's elegant enhancement
by Buhrman and Fortnow. Our work also sheds light on previous research on the
structure of refined polynomial hierarchies, and strengthens the connection
between the collapse of bounded query hierarchies and the collapse of the
polynomial hierarchy."
"Generalizations of numeration systems in which N is recognizable by a finite
automaton are obtained by describing a lexicographically ordered infinite
regular language L over a finite alphabet A. For these systems, we obtain a
characterization of recognizable sets of integers in terms of rational formal
series. We also show that, if the complexity of L is Theta (n^q) (resp. if L is
the complement of a polynomial language), then multiplication by an integer k
preserves recognizability only if k=t^{q+1} (resp. if k is not a power of the
cardinality of A) for some integer t. Finally, we obtain sufficient conditions
for the notions of recognizability and U-recognizability to be equivalent,
where U is some positional numeration system related to a sequence of integers."
"Withdrawn since -order- was overlooked. First order reductions without order
are much too weak to separate."
"We discuss the history and uses of the parallel census technique---an elegant
tool in the study of certain computational objects having polynomially bounded
census functions. A sequel will discuss advances (including Cai, Naik, and
Sivakumar [CNS95] and Glasser [Gla00]), some related to the parallel census
technique and some due to other approaches, in the complexity-class collapses
that follow if NP has sparse hard sets under reductions weaker than (full)
truth-table reductions."
"Rabi and Sherman present a cryptographic paradigm based on associative,
one-way functions that are strong (i.e., hard to invert even if one of their
arguments is given) and total. Hemaspaandra and Rothe proved that such powerful
one-way functions exist exactly if (standard) one-way functions exist, thus
showing that the associative one-way function approach is as plausible as
previous approaches. In the present paper, we study the degree of ambiguity of
one-way functions. Rabiand Sherman showed that no associative one-way function
(over a universe having at least two elements) can be unambiguous (i.e.,
one-to-one). Nonetheless, we prove that if standard, unambiguous, one-way
functions exist, then there exist strong, total, associative, one-way functions
that are $\mathcal{O}(n)$-to-one. This puts a reasonable upper bound on the
ambiguity."
"Rabi, Rivest, and Sherman alter the standard notion of noninvertibility to a
new notion they call strong noninvertibility, and show -- via explicit
cryptographic protocols for secret-key agreement ([RS93,RS97] attribute this to
Rivest and Sherman) and digital signatures [RS93,RS97] -- that strongly
noninvertible functions would be very useful components in protocol design.
Their definition of strong noninvertibility has a small twist (``respecting the
argument given'') that is needed to ensure cryptographic usefulness. In this
paper, we show that this small twist has a large, unexpected consequence:
Unless P=NP, some strongly noninvertible functions are invertible."
No P-immune set having exponential gaps is positive-Turing self-reducible.
"The top part of the preceding figure [figure appears in actual paper] shows
some classes from the (truth-table) bounded-query and boolean hierarchies. It
is well-known that if either of these hierarchies collapses at a given level,
then all higher levels of that hierarchy collapse to that same level. This is a
standard ``upward translation of equality'' that has been known for over a
decade. The issue of whether these hierarchies can translate equality {\em
downwards\/} has proven vastly more challenging. In particular, with regard to
the figure above, consider the following claim:
  $$P_{m-tt}^{\Sigma_k^p} = P_{m+1-tt}^{\Sigma_k^p} \implies
  DIFF_m(\Sigma_k^p) coDIFF_m(\Sigma_k^p) = BH(\Sigma_k^p). (*) $$
  This claim, if true, says that equality translates downwards between levels
of the bounded-query hierarchy and the boolean hierarchy levels that (before
the fact) are immediately below them.
  Until recently, it was not known whether (*) {\em ever\/} held, except for
the degenerate cases $m=0$ and $k=0$. Then Hemaspaandra, Hemaspaandra, and
Hempel \cite{hem-hem-hem:j:downward-translation} proved that (*) holds for all
$m$, for $k > 2$. Buhrman and Fortnow~\cite{buh-for:j:two-queries} then showed
that, when $k=2$, (*) holds for the case $m = 1$. In this paper, we prove that
for the case $k=2$, (*) holds for all values of $m$. Since there is an oracle
relative to which ``for $k=1$, (*) holds for all $m$'' fails
\cite{buh-for:j:two-queries}, our achievement of the $k=2$ case cannot to be
strengthened to $k=1$ by any relativizable proof technique. The new downward
translation we obtain also tightens the collapse in the polynomial hierarchy
implied by a collapse in the bounded-query hierarchy of the second level of the
polynomial hierarchy."
"We prove that computing a single pair of vertices that are mapped onto each
other by an isomorphism $\phi$ between two isomorphic graphs is as hard as
computing $\phi$ itself. This result optimally improves upon a result of
G\'{a}l et al. We establish a similar, albeit slightly weaker, result about
computing complete Hamiltonian cycles of a graph from partial Hamiltonian
cycles."
"We show that computing the lexicographically first four-coloring for planar
graphs is P^{NP}-hard. This result optimally improves upon a result of Khuller
and Vazirani who prove this problem to be NP-hard, and conclude that it is not
self-reducible in the sense of Schnorr, assuming P \neq NP. We discuss this
application to non-self-reducibility and provide a general related result."
"We show that there cannot be any algorithm that for a given nondeterministic
polynomial-time Turing machine determinates whether or not the language
recognized by this machine belongs to P"
"Given a tiling of a 2D grid with several types of tiles, we can count for
every row and column how many tiles of each type it intersects. These numbers
are called the_projections_. We are interested in the problem of reconstructing
a tiling which has given projections. Some simple variants of this problem,
involving tiles that are 1x1 or 1x2 rectangles, have been studied in the past,
and were proved to be either solvable in polynomial time or NP-complete. In
this note we make progress toward a comprehensive classification of various
tiling reconstruction problems, by proving NP-completeness results for several
sets of tiles."
"Let $M_k \seq \nats$ be a given set that consists of $k$ noncontiguous
integers. Define $\exactcolor{M_k}$ to be the problem of determining whether
$\chi(G)$, the chromatic number of a given graph $G$, equals one of the $k$
elements of the set $M_k$ exactly. In 1987, Wagner \cite{wag:j:min-max} proved
that $\exactcolor{M_k}$ is $\bhlevel{2k}$-complete, where $M_k = \{6k+1, 6k+3,
>..., 8k-1 \}$ and $\bhlevel{2k}$ is the $2k$th level of the boolean hierarchy
over $\np$. In particular, for $k = 1$, it is DP-complete to determine whether
$\chi(G) = 7$, where $\DP = \bhlevel{2}$. Wagner raised the question of how
small the numbers in a $k$-element set $M_k$ can be chosen such that
$\exactcolor{M_k}$ still is $\bhlevel{2k}$-complete. In particular, for $k =
1$, he asked if it is DP-complete to determine whether $\chi(G) = 4$. In this
note, we solve this question of Wagner and determine the precise threshold $t
\in \{4, 5, 6, 7\}$ for which the problem $\exactcolor{\{t\}}$ jumps from NP to
DP-completeness: It is DP-complete to determine whether $\chi(G) = 4$, yet
$\exactcolor{\{3\}}$ is in $\np$. More generally, for each $k \geq 1$, we show
that $\exactcolor{M_k}$ is $\bhlevel{2k}$-complete for $M_k = \{3k+1, 3k+3,...,
5k-1\}$."
"For both the edge deletion heuristic and the maximum-degree greedy heuristic,
we study the problem of recognizing those graphs for which that heuristic can
approximate the size of a minimum vertex cover within a constant factor of r,
where r is a fixed rational number. Our main results are that these problems
are complete for the class of problems solvable via parallel access to NP. To
achieve these main results, we also show that the restriction of the vertex
cover problem to those graphs for which either of these heuristics can find an
optimal solution remains NP-hard."
"We show that a Turing machine with two single-head one-dimensional tapes
cannot recognize the set {x2x'| x \in {0,1}^* and x' is a prefix of x} in real
time, although it can do so with three tapes, two two-dimensional tapes, or one
two-head tape, or in linear time with just one tape. In particular, this
settles the longstanding conjecture that a two-head Turing machine can
recognize more languages in real time if its heads are on the same
one-dimensional tape than if they are on separate one-dimensional tapes."
"We present a new approach to formal language theory using Kolmogorov
complexity. The main results presented here are an alternative for pumping
lemma(s), a new characterization for regular languages, and a new method to
separate deterministic context-free languages and nondeterministic context-free
languages. The use of the new `incompressibility arguments' is illustrated by
many examples. The approach is also successful at the high end of the Chomsky
hierarchy since one can quantify nonrecursiveness in terms of Kolmogorov
complexity. (This is a preliminary uncorrected version. The final version is
the one published in SIAM J. Comput., 24:2(1995), 398-410.)"
"Inducing association rules is one of the central tasks in data mining
applications. Quantitative association rules induced from databases describe
rich and hidden relationships holding within data that can prove useful for
various application purposes (e.g., market basket analysis, customer profiling,
and others). Even though such association rules are quite widely used in
practice, a thorough analysis of the computational complexity of inducing them
is missing. This paper intends to provide a contribution in this setting. To
this end, we first formally define quantitative association rule mining
problems, which entail boolean association rules as a special case, and then
analyze their computational complexities, by considering both the standard
cases, and a some special interesting case, that is, association rule induction
over databases with null values, fixed-size attribute set databases, sparse
databases, fixed threshold problems."
"In this paper we consider the computational complexity of the following
problem. Let $f$ be a Boolean polynomial. What value of $f$, 0 or 1, is taken
more frequently? The problem is solved in polynomial time for polynomials of
degrees 1,2. The next case of degree 3 appears to be PP-complete under
polynomial reductions in the class of promise problems. The proof is based on
techniques of quantum computation."
"NP-complete problems should be hard on some instances but those may be
extremely rare. On generic instances many such problems, especially related to
random graphs, have been proven easy. We show the intractability of random
instances of a graph coloring problem: this graph problem is hard on average
unless all NP problem under all samplable (i.e., generatable in polynomial
time) distributions are easy. Worst case reductions use special gadgets and
typically map instances into a negligible fraction of possible outputs. Ours
must output nearly random graphs and avoid any super-polynomial distortion of
probabilities. This poses significant technical difficulty."
"In 1977, Young proposed a voting scheme that extends the Condorcet Principle
based on the fewest possible number of voters whose removal yields a Condorcet
winner. We prove that both the winner and the ranking problem for Young
elections is complete for the class of problems solvable in polynomial time by
parallel access to NP. Analogous results for Lewis Carroll's 1876 voting scheme
were recently established by Hemaspaandra et al. In contrast, we prove that the
winner and ranking problems in Fishburn's homogeneous variant of Carroll's
voting scheme can be solved efficiently by linear programming."
"We prove lower bounds on the number of product gates in bilinear and
quadratic circuits that compute the product of two $n \cross n$ matrices over
finite fields. In particular we obtain the following results:
  1. We show that the number of product gates in any bilinear (or quadratic)
circuit that computes the product of two $n \cross n$ matrices over $F_2$ is at
least $3 n^2 - o(n^2)$.
  2. We show that the number of product gates in any bilinear circuit that
computes the product of two $n \cross n$ matrices over $F_p$ is at least $(2.5
+ \frac{1.5}{p^3 -1})n^2 -o(n^2)$.
  These results improve the former results of Bshouty '89 and Blaser '99 who
proved lower bounds of $2.5 n^2 - o(n^2)$."
"A theory of resource-bounded dimension is developed using gales, which are
natural generalizations of martingales. When the resource bound \Delta (a
parameter of the theory) is unrestricted, the resulting dimension is precisely
the classical Hausdorff dimension (sometimes called fractal dimension). Other
choices of the parameter \Delta yield internal dimension theories in E, E2,
ESPACE, and other complexity classes, and in the class of all decidable
problems. In general, if C is such a class, then every set X of languages has a
dimension in C, which is a real number dim(X|C) in [0,1]. Along with the
elements of this theory, two preliminary applications are presented:
  1. For every real number \alpha in (0,1/2), the set FREQ(<=\alpha),
consisting of all languages that asymptotically contain at most \alpha of all
strings, has dimension H(\alpha) -- the binary entropy of \alpha -- in E and in
E2.
  2. For every real number \alpha in (0,1), the set SIZE(\alpha* (2^n)/n),
consisting of all languages decidable by Boolean circuits of at most
\alpha*(2^n)/n gates, has dimension \alpha in ESPACE."
"A constructive version of Hausdorff dimension is developed using constructive
supergales, which are betting strategies that generalize the constructive
supermartingales used in the theory of individual random sequences. This
constructive dimension is used to assign every individual (infinite, binary)
sequence S a dimension, which is a real number dim(S) in the interval [0,1].
Sequences that are random (in the sense of Martin-Lof) have dimension 1, while
sequences that are decidable, \Sigma^0_1, or \Pi^0_1 have dimension 0. It is
shown that for every \Delta^0_2-computable real number \alpha in [0,1] there is
a \Delta^0_2 sequence S such that \dim(S) = \alpha.
  A discrete version of constructive dimension is also developed using
termgales, which are supergale-like functions that bet on the terminations of
(finite, binary) strings as well as on their successive bits. This discrete
dimension is used to assign each individual string w a dimension, which is a
nonnegative real number dim(w). The dimension of a sequence is shown to be the
limit infimum of the dimensions of its prefixes.
  The Kolmogorov complexity of a string is proven to be the product of its
length and its dimension. This gives a new characterization of algorithmic
information and a new proof of Mayordomo's recent theorem stating that the
dimension of a sequence is the limit infimum of the average Kolmogorov
complexity of its first n bits.
  Every sequence that is random relative to any computable sequence of
coin-toss biases that converge to a real number \beta in (0,1) is shown to have
dimension \H(\beta), the binary entropy of \beta."
"Goedel Incompleteness Theorem leaves open a way around it, vaguely perceived
for a long time but not clearly identified. (Thus, Goedel believed informal
arguments can answer any math question.) Closing this loophole does not seem
obvious and involves Kolmogorov complexity. (This is unrelated to, well studied
before, complexity quantifications of the usual Goedel effects.) I consider
extensions U of the universal partial recursive predicate (or, say, Peano
Arithmetic). I prove that any U either leaves an n-bit input (statement)
unresolved or contains nearly all information about the n-bit prefix of any
r.e. real r (which is n bits for some r). I argue that creating significant
information about a SPECIFIC math sequence is impossible regardless of the
methods used. Similar problems and answers apply to other unsolvability results
for tasks allowing multiple solutions, e.g. non-recursive tilings."
"This report presents notes from the first eight lectures of the class Many
Models of Complexity taught by Laszlo Lovasz at Princeton University in the
fall of 1990. The topic is evasiveness of graph properties: given a graph
property, how many edges of the graph an algorithm must check in the worst case
before it knows whether the property holds."
"We show that Closest Substring, one of the most important problems in the
field of biological sequence analysis, is W[1]-hard when parameterized by the
number k of input strings (and remains so, even over a binary alphabet). This
problem is therefore unlikely to be solvable in time O(f(k)\cdot n^{c}) for any
function f of k and constant c independent of k. The problem can therefore be
expected to be intractable, in any practical sense, for k>=3. Our result
supports the intuition that Closest Substring is computationally much harder
than the special case of Closest String, although both problems are
NP-complete. We also prove W[1]-hardness for other parameterizations in the
case of unbounded alphabet size. Our W[1]-hardness result for Closest Substring
generalizes to Consensus Patterns, a problem of similar significance in
computational biology."
"A hole has been found in the algorithm as given, where an eleventh hour
change admits a path inconsistency. The inconsistency arises due to an improper
closure of a path to a cycle against a root not supportive of the path. The
mathematical basis of the algorithm remains supportive of the intended
solution, and a revision of the algorithm is underway."
"We investigate the power of the most important lower bound technique in
randomized communication complexity, which is based on an evaluation of the
maximal size of approximately monochromatic rectangles, minimized over all
distributions on the inputs. While it is known that the 0-error version of this
bound is polynomially tight for deterministic communication, nothing in this
direction is known for constant error and randomized communication complexity.
We first study a one-sided version of this bound and obtain that its value lies
between the MA- and AM-complexities of the considered function. Hence the lower
bound actually works for a (communication complexity) class between MA cap
co-MA and AM cap co-AM. We also show that the MA-complexity of the disjointness
problem is Omega(sqrt(n)). Following this we consider the conjecture that the
lower bound method is polynomially tight for randomized communication
complexity. First we disprove a distributional version of this conjecture. Then
we give a combinatorial characterization of the value of the lower bound
method, in which the optimization over all distributions is absent. This
characterization is done by what we call a uniform threshold cover. We also
study relaxations of this notion, namely approximate majority covers and
majority covers, and compare these three notions in power, exhibiting
exponential separations. Each of these covers captures a lower bound method
previously used for randomized communication complexity."
"This paper considers the question of P = NP in context of the polynomial time
SAT algorithm. It posits proposition dependent on existence of conjectured
problem that even where the algorithm is shown to solve SAT in polynomial time
it remains theoretically possible for there to yet exist a
non-deterministically polynomial (NP) problem for which the algorithm does not
provide a polynomial (P) time solution. The paper leaves open as subject of
continuing research the question of existence of instance of conjectured
problem."
"Supergales, generalizations of supermartingales, have been used by Lutz
(2002) to define the constructive dimensions of individual binary sequences.
Here it is shown that gales, the corresponding generalizations of martingales,
can be equivalently used to define constructive dimension."
"We show that for a wide range of probability measures, constructive gales are
interchangable with constructive supergales for defining constructive Hausdorff
dimension, thus generalizing a previous independent result of Hitchcock
(cs.CC/0208043) and partially answering an open question of Lutz
(cs.CC/0203017)."
"Stephen Cook posited SAT is NP-Complete in 1971. If SAT is NP-Complete then,
as is generally accepted, any polynomial solution of it must also present a
polynomial solution of all NP decision problems. It is here argued, however,
that NP is not of necessity equivalent to P where it is shown that SAT is
contained in P. This due to a paradox, of nature addressed by both Godel and
Russell, in regards to the P-NP system in total."
"The model of cellular automata is fascinating because very simple local rules
can generate complex global behaviors. The relationship between local and
global function is subject of many studies. We tackle this question by using
results on communication complexity theory and, as a by-product, we provide
(yet another) classification of cellular automata."
"The paper (as posted originally) contains several errors. It has been
subsequently split into two papers, the corrected (and accepted for
publication) versions appear in the archive as papers cs.CC/0503082 and
cs.DM/0503083."
"The two most important notions of fractal dimension are {\it Hausdorff
dimension}, developed by Hausdorff (1919), and {\it packing dimension},
developed by Tricot (1982).
  Lutz (2000) has recently proven a simple characterization of Hausdorff
dimension in terms of {\it gales}, which are betting strategies that generalize
martingales. Imposing various computability and complexity constraints on these
gales produces a spectrum of effective versions of Hausdorff dimension.
  In this paper we show that packing dimension can also be characterized in
terms of gales. Moreover, even though the usual definition of packing dimension
is considerably more complex than that of Hausdorff dimension, our gale
characterization of packing dimension is an exact dual of -- and every bit as
simple as -- the gale characterization of Hausdorff dimension.
  Effectivizing our gale characterization of packing dimension produces a
variety of {\it effective strong dimensions}, which are exact duals of the
effective dimensions mentioned above.
  We develop the basic properties of effective strong dimensions and prove a
number of results relating them to fundamental aspects of randomness,
Kolmogorov complexity, prediction, Boolean circuit-size complexity,
polynomial-time degrees, and data compression."
Withdrawn. Silly notion and out of context.
"Bounds for the optimal tour length for a hypothetical TSP algorithm are
derived."
"We propose the ``Competing Salesmen Problem'' (CSP), a 2-player competitive
version of the classical Traveling Salesman Problem. This problem arises when
considering two competing salesmen instead of just one. The concern for a
shortest tour is replaced by the necessity to reach any of the customers before
the opponent does. In particular, we consider the situation where players take
turns, moving along one edge at a time within a graph G=(V,E). The set of
customers is given by a subset V_C V of the vertices. At any given time, both
players know of their opponent's position. A player wins if he is able to reach
a majority of the vertices in V_C before the opponent does. We prove that the
CSP is PSPACE-complete, even if the graph is bipartite, and both players start
at distance 2 from each other. We show that the starting player may lose the
game, even if both players start from the same vertex. For bipartite graphs, we
show that the starting player always can avoid a loss. We also show that the
second player can avoid to lose by more than one customer, when play takes
place on a graph that is a tree T, and V_C consists of leaves of T. For the
case where T is a star and V_C consists of n leaves of T, we give a simple and
fast strategy which is optimal for both players. If V_C consists not only of
leaves, the situation is more involved."
"We prove that the exact versions of the domatic number problem are complete
for the levels of the boolean hierarchy over NP. The domatic number problem,
which arises in the area of computer networks, is the problem of partitioning a
given graph into a maximum number of disjoint dominating sets. This number is
called the domatic number of the graph. We prove that the problem of
determining whether or not the domatic number of a given graph is {\em exactly}
one of k given values is complete for the 2k-th level of the boolean hierarchy
over NP. In particular, for k = 1, it is DP-complete to determine whether or
not the domatic number of a given graph equals exactly a given integer. Note
that DP is the second level of the boolean hierarchy over NP. We obtain similar
results for the exact versions of generalized dominating set problems and of
the conveyor flow shop problem. Our reductions apply Wagner's conditions
sufficient to prove hardness for the levels of the boolean hierarchy over NP."
"Madhu Sudan's work spans many areas of computer science theory including
computational complexity theory, the design of efficient algorithms,
algorithmic coding theory, and the theory of program checking and correcting.
  Two results of Sudan stand out in the impact they have had on the mathematics
of computation. The first work shows a probabilistic characterization of the
class NP -- those sets for which short and easily checkable proofs of
membership exist, and demonstrates consequences of this characterization to
classifying the complexity of approximation problems. The second work shows a
polynomial time algorithm for list decoding the Reed Solomon error correcting
codes.
  This short note will be devoted to describing Sudan's work on
probabilistically checkable proofs -- the so called {\it PCP theorem} and its
implications."
"We address lower bounds on the time complexity of algorithms solving the
propositional satisfiability problem. Namely, we consider two DPLL-type
algorithms, enhanced with the unit clause and pure literal heuristics.
Exponential lower bounds for solving satisfiability on provably satisfiable
formulas are proven."
"A bounded Kolmogorov-Loveland selection rule is an adaptive strategy for
recursively selecting a subsequence of an infinite binary sequence; such a
subsequence may be interpreted as the query sequence of a time-bounded Turing
machine. In this paper we show that if A is an algorithmically random sequence,
A_0 is selected from A via a bounded Kolmogorov-Loveland selection rule, and
A_1 denotes the sequence of nonselected bits of A, then A_1 is independent of
A_0; that is, A_1 is algorithmically random relative to A_0. This result has
been used by Kautz and Miltersen [1] to show that relative to a random oracle,
NP does not have p-measure zero (in the sense of Lutz [2]).
  [1] S. M. Kautz and P. B. Miltersen. Relative to a random oracle, NP is not
small. Journal of Computer and System Sciences, 53:235-250, 1996.
  [2] J. H. Lutz. Almost everywhere high nonuniform complexity. Journal of
Computer and System Sciences, 44:220-258, 1992."
"We prove lower bounds of order $n\log n$ for both the problem to multiply
polynomials of degree $n$, and to divide polynomials with remainder, in the
model of bounded coefficient arithmetic circuits over the complex numbers.
These lower bounds are optimal up to order of magnitude. The proof uses a
recent idea of R. Raz [Proc. 34th STOC 2002] proposed for matrix
multiplication. It reduces the linear problem to multiply a random circulant
matrix with a vector to the bilinear problem of cyclic convolution. We treat
the arising linear problem by extending J. Morgenstern's bound [J. ACM 20, pp.
305-306, 1973] in a unitarily invariant way. This establishes a new lower bound
on the bounded coefficient complexity of linear forms in terms of the singular
values of the corresponding matrix. In addition, we extend these lower bounds
for linear and bilinear maps to a model of circuits that allows a restricted
number of unbounded scalar multiplications."
"We prove lower bounds for the direct sum problem for two-party bounded error
randomised multiple-round communication protocols. Our proofs use the notion of
information cost of a protocol, as defined by Chakrabarti, Shi, Wirth and Yao
and refined further by Bar-Yossef, Jayram, Kumar and Sivakumar. Our main
technical result is a `compression' theorem saying that, for any probability
distribution $\mu$ over the inputs, a $k$-round private coin bounded error
protocol for a function $f$ with information cost $c$ can be converted into a
$k$-round deterministic protocol for $f$ with bounded distributional error and
communication cost $O(kc)$. We prove this result using a substate theorem about
relative entropy and a rejection sampling argument. Our direct sum result
follows from this `compression' result via elementary information theoretic
arguments.
  We also consider the direct sum problem in quantum communication. Using a
probabilistic argument, we show that messages cannot be compressed in this
manner even if they carry small information. Hence, new techniques may be
necessary to tackle the direct sum problem in quantum communication."
"Given a $k$-uniform hyper-graph, the E$k$-Vertex-Cover problem is to find the
smallest subset of vertices that intersects every hyper-edge. We present a new
multilayered PCP construction that extends the Raz verifier. This enables us to
prove that E$k$-Vertex-Cover is NP-hard to approximate within factor
$(k-1-\epsilon)$ for any $k \geq 3$ and any $\epsilon>0$. The result is
essentially tight as this problem can be easily approximated within factor $k$.
Our construction makes use of the biased Long-Code and is analyzed using
combinatorial properties of $s$-wise $t$-intersecting families of subsets."
"Juedes and Lutz (1995) proved a small span theorem for polynomial-time
many-one reductions in exponential time. This result says that for language A
decidable in exponential time, either the class of languages reducible to A
(the lower span) or the class of problems to which A can be reduced (the upper
span) is small in the sense of resource-bounded measure and, in particular,
that the degree of A is small. Small span theorems have been proven for
increasingly stronger polynomial-time reductions, and a small span theorem for
polynomial-time Turing reductions would imply BPP != EXP. In contrast to the
progress in resource-bounded measure, Ambos-Spies, Merkle, Reimann, and Stephan
(2001) showed that there is no small span theorem for the resource-bounded
dimension of Lutz (2000), even for polynomial-time many-one reductions.
  Resource-bounded scaled dimension, recently introduced by Hitchcock, Lutz,
and Mayordomo (2003), provides rescalings of resource-bounded dimension. We use
scaled dimension to further understand the contrast between measure and
dimension regarding polynomial-time spans and degrees. We strengthen prior
results by showing that the small span theorem holds for polynomial-time
many-one reductions in the -3rd-order scaled dimension, but fails to hold in
the -2nd-order scaled dimension. Our results also hold in exponential space.
  As an application, we show that determining the -2nd- or -1st-order scaled
dimension in ESPACE of the many-one complete languages for E would yield a
proof of P = BPP or P != PSPACE. On the other hand, it is shown unconditionally
that the complete languages for E have -3rd-order scaled dimension 0 in ESPACE
and -2nd- and -1st-order scaled dimension 1 in E."
"We survey a collective achievement of a group of researchers: the PCP
Theorems. They give new definitions of the class \np, and imply that computing
approximate solutions to many \np-hard problems is itself \np-hard. Techniques
developed to prove them have had many other consequences."
"An NP-hard combinatorial optimization problem $\Pi$ is said to have an {\em
approximation threshold} if there is some $t$ such that the optimal value of
$\Pi$ can be approximated in polynomial time within a ratio of $t$, and it is
NP-hard to approximate it within a ratio better than $t$. We survey some of the
known approximation threshold results, and discuss the pattern that emerges
from the known results."
"We survey recent developments in the study of probabilistic complexity
classes. While the evidence seems to support the conjecture that probabilism
can be deterministically simulated with relatively low overhead, i.e., that
$P=BPP$, it also indicates that this may be a difficult question to resolve. In
fact, proving that probabilistic algorithms have non-trivial deterministic
simulations is basically equivalent to proving circuit lower bounds, either in
the algebraic or Boolean models."
"Recent results established exponential lower bounds for the length of any
Resolution proof for the weak pigeonhole principle. More formally, it was
proved that any Resolution proof for the weak pigeonhole principle, with $n$
holes and any number of pigeons, is of length $\Omega(2^{n^{\epsilon}})$, (for
a constant $\epsilon = 1/3$). One corollary is that certain propositional
formulations of the statement $P \ne NP$ do not have short Resolution proofs.
After a short introduction to the problem of $P \ne NP$ and to the research
area of propositional proof complexity, I will discuss the above mentioned
lower bounds for the weak pigeonhole principle and the connections to the
hardness of proving $P \ne NP$."
"We consider the problem of evaluation of the weight enumerator of a binary
linear code. We show that the exact evaluation is hard for polynomial
hierarchy. More exactly, if WE is an oracle answering the solution of the
evaluation problem then P^WE=P^GapP. Also we consider the approximative
evaluation of the weight enumerator. In the case of approximation with additive
accuracy $2^{\alpha n}$, $\alpha$ is constant the problem is hard in the above
sense. We also prove that approximate evaluation at a single point $e^{\pi
i/4}$ is hard for $0<\al<\al_0\approx0.88$."
"In this note, we show that there is no deterministic and exact algorithm that
computes the permanent of a matrix in polynomial-time."
"The even cycle problem for both undirected and directed graphs has been the
topic of intense research in the last decade. In this paper, we study the
computational complexity of \emph{cycle length modularity problems}. Roughly
speaking, in a cycle length modularity problem, given an input (undirected or
directed) graph, one has to determine whether the graph has a cycle $C$ of a
specific length (or one of several different lengths), modulo a fixed integer.
We denote the two families (one for undirected graphs and one for directed
graphs) of problems by $(S,m)\hbox{-}{\rm UC}$ and $(S,m)\hbox{-}{\rm DC}$,
where $m \in \mathcal{N}$ and $S \subseteq \{0,1, ..., m-1\}$.
$(S,m)\hbox{-}{\rm UC}$ (respectively, $(S,m)\hbox{-}{\rm DC}$) is defined as
follows: Given an undirected (respectively, directed) graph $G$, is there a
cycle in $G$ whose length, modulo $m$, is a member of $S$? In this paper, we
fully classify (i.e., as either polynomial-time solvable or as ${\rm
NP}$-complete) each problem $(S,m)\hbox{-}{\rm UC}$ such that $0 \in S$ and
each problem $(S,m)\hbox{-}{\rm DC}$ such that $0 \notin S$. We also give a
sufficient condition on $S$ and $m$ for the following problem to be
polynomial-time computable: $(S,m)\hbox{-}{\rm UC}$ such that $0 \notin S$."
"We show that a certain representation of the matrix-product can be computed
with $n^{o(1)}$ multiplications. We also show, that siumilar representations of
matrices can be compressed enormously."
"We present and prove a theorem answering the question ""how many states does a
minimal deterministic finite automaton (DFA) that recognizes the set of base-b
numbers divisible by k have?"""
"A theory of one-tape (one-head) linear-time Turing machines is essentially
different from its polynomial-time counterpart since these machines are closely
related to finite state automata. This paper discusses structural-complexity
issues of one-tape Turing machines of various types (deterministic,
nondeterministic, reversible, alternating, probabilistic, counting, and quantum
Turing machines) that halt in linear time, where the running time of a machine
is defined as the length of any longest computation path. We explore structural
properties of one-tape linear-time Turing machines and clarify how the
machines' resources affect their computational patterns and power."
"In this note, we present a puzzle. We prove that Zermelo-Fraenkel set theory
is inconsistent by proving, using Zermelo-Fraenkel set theory, the false
statement that any algorithm that determines whether any $n \times n$ matrix
over $\mathbb F_2$, the finite field of order 2, is nonsingular must run in
exponential time in the worst-case scenario. The object of the puzzle is to
find the error in the proof."
"The algorithmic theory of randomness is well developed when the underlying
space is the set of finite or infinite sequences and the underlying probability
distribution is the uniform distribution or a computable distribution. These
restrictions seem artificial. Some progress has been made to extend the theory
to arbitrary Bernoulli distributions (by Martin-Loef), and to arbitrary
distributions (by Levin). We recall the main ideas and problems of Levin's
theory, and report further progress in the same framework.
  - We allow non-compact spaces (like the space of continuous functions,
underlying the Brownian motion).
  - The uniform test (deficiency of randomness) d_P(x) (depending both on the
outcome x and the measure P should be defined in a general and natural way.
  - We see which of the old results survive: existence of universal tests,
conservation of randomness, expression of tests in terms of description
complexity, existence of a universal measure, expression of mutual information
as ""deficiency of independence.
  - The negative of the new randomness test is shown to be a generalization of
complexity in continuous spaces; we show that the addition theorem survives.
  The paper's main contribution is introducing an appropriate framework for
studying these questions and related ones (like statistics for a general family
of distributions)."
"In this paper, we use resource-bounded dimension theory to investigate
polynomial size circuits. We show that for every $i\geq 0$, $\Ppoly$ has $i$th
order scaled $\pthree$-strong dimension 0. We also show that $\Ppoly^\io$ has
$\pthree$-dimension 1/2, $\pthree$-strong dimension 1. Our results improve
previous measure results of Lutz (1992) and dimension results of Hitchcock and
Vinodchandran (2004)."
"The Game of Life cellular automaton is a classical example of a massively
parallel collision-based computing device. The automaton exhibits mobile
patterns, gliders, and generators of the mobile patterns, glider guns, in its
evolution. We show how to construct the basic logical operations, AND, OR, NOT
in space-time configurations of the cellular automaton. Also decomposition of
complicated Boolean functions is discussed. Advantages of our technique are
demonstrated on an example of binary adder, realized via collision of glider
streams."
"We consider vertex colorings of graphs in which adjacent vertices have
distinct colors. A graph is $s$-chromatic if it is colorable in $s$ colors and
any coloring of it uses at least $s$ colors. The forcing chromatic number
$F(G)$ of an $s$-chromatic graph $G$ is the smallest number of vertices which
must be colored so that, with the restriction that $s$ colors are used, every
remaining vertex has its color determined uniquely. We estimate the
computational complexity of $F(G)$ relating it to the complexity class US
introduced by Blass and Gurevich. We prove that recognizing if $F(G)\le 2$ is
US-hard with respect to polynomial-time many-one reductions. Moreover, this
problem is coNP-hard even under the promises that $F(G)\le 3$ and $G$ is
3-chromatic. On the other hand, recognizing if $F(G)\le k$, for each constant
$k$, is reducible to a problem in US via disjunctive truth-table reduction.
  Similar results are obtained also for forcing variants of the clique and the
domination numbers of a graph."
"This paper provides a new conceptual perspective on survey propagation, which
is an iterative algorithm recently introduced by the statistical physics
community that is very effective in solving random k-SAT problems even with
densities close to the satisfiability threshold. We first describe how any SAT
formula can be associated with a novel family of Markov random fields (MRFs),
parameterized by a real number \rho \in [0,1]. We then show that applying
belief propagation--a well-known ``message-passing'' technique for estimating
marginal probabilities--to this family of MRFs recovers a known family of
algorithms, ranging from pure survey propagation at one extreme (\rho = 1) to
standard belief propagation on the uniform distribution over SAT assignments at
the other extreme (\rho = 0). Configurations in these MRFs have a natural
interpretation as partial satisfiability assignments, on which a partial order
can be defined. We isolate cores as minimal elements in this partial ordering,
which are also fixed points of survey propagation and the only assignments with
positive probability in the MRF for \rho=1. Our experimental results for k=3
suggest that solutions of random formulas typically do not possess non-trivial
cores. This makes it necessary to study the structure of the space of partial
assignments for \rho<1 and investigate the role of assignments that are very
close to being cores. To that end, we investigate the associated lattice
structure, and prove a weight-preserving identity that shows how any MRF with
\rho>0 can be viewed as a ``smoothed'' version of the uniform distribution over
satisfying assignments (\rho=0). Finally, we isolate properties of Gibbs
sampling and message-passing algorithms that are typical for an ensemble of
k-SAT problems."
"We survey results on the hardness of approximating combinatorial optimization
problems."
"We study Turing machines that are allowed absolutely no space overhead. The
only work space the machines have, beyond the fixed amount of memory implicit
in their finite-state control, is that which they can create by cannibalizing
the input bits' own space. This model more closely reflects the fixed-sized
memory of real computers than does the standard complexity-theoretic model of
linear space.
  Though some context-sensitive languages cannot be accepted by such machines,
we show that all context-free languages can be accepted nondeterministically in
polynomial time with absolutely no space overhead, and that all deterministic
context-free languages can be accepted deterministically in polynomial time
with absolutely no space overhead."
"Group and individual solutions are considered for hard problems such as
satisfiability problem. Time-space trade-off in a structured active memory
provides means to achieve lower time complexity for solutions of these
problems."
"We generalize the definition of a counter and counter reversal complexity and
investigate the power of generalized deterministic counter automata in terms of
language recognition."
"We prove several results about the relations between injectivity and
surjectivity for sand automata. Moreover, we begin the exploration of the
dynamical behavior of sand automata proving that the property of nilpotency is
undecidable. We believe that the proof technique used for this last result
might reveal useful for many other results in this context."
"This article introduces three invariance principles under which P is
different from NP. In the second part a theorem of convergence is proven. This
theorem states that for any language L there exists an infinite sequence of
languages from O(n) that converges to L."
"Recently, among experiments for realization of quantum computers, NMR quantum
computers have achieved the most impressive succession. There is a model of the
NMR quantum computation,namely Atsumi and Nishino's bulk quantum Turing
Machine. It assumes, however, an unnatural assumption with quantum mechanics.
We, then, define a more natural and quantum mechanically realizable modified
bulk quantum Turing Machine, and show its computational ability by comparing
complexity classes with quantum Turing Machine's counter part."
"This paper reviews the Church-Turing Thesis (or rather, theses) with
reference to their origin and application and considers some models of
""hypercomputation"", concentrating on perhaps the most straight-forward option:
Zeno machines (Turing machines with accelerating clock). The halting problem is
briefly discussed in a general context and the suggestion that it is an
inevitable companion of any reasonable computational model is emphasised. It is
hinted that claims to have ""broken the Turing barrier"" could be toned down and
that the important and well-founded role of Turing computability in the
mathematical sciences stands unchallenged."
"In the maximum constraint satisfaction problem (Max CSP), one is given a
finite collection of (possibly weighted) constraints on overlapping sets of
variables, and the goal is to assign values from a given domain to the
variables so as to maximize the number (or the total weight, for the weighted
case) of satisfied constraints. This problem is NP-hard in general, and,
therefore, it is natural to study how restricting the allowed types of
constraints affects the approximability of the problem. It is known that every
Boolean (that is, two-valued) Max CSP problem with a finite set of allowed
constraint types is either solvable exactly in polynomial time or else
APX-complete (and hence can have no polynomial time approximation scheme unless
P=NP. It has been an open problem for several years whether this result can be
extended to non-Boolean Max CSP, which is much more difficult to analyze than
the Boolean case. In this paper, we make the first step in this direction by
establishing this result for Max CSP over a three-element domain. Moreover, we
present a simple description of all polynomial-time solvable cases of our
problem. This description uses the well-known algebraic combinatorial property
of supermodularity. We also show that every hard three-valued Max CSP problem
contains, in a certain specified sense, one of the two basic hard Max CSP
problems which are the Maximum k-colourable subgraph problems for k=2,3."
"We prove fixed points results for sandpiles starting with arbitrary initial
conditions. We give an effective algorithm for computing such fixed points, and
we refine it in the particular case of SPM."
"We study the isomorphic implication problem for Boolean constraints. We show
that this is a natural analog of the subgraph isomorphism problem. We prove
that, depending on the set of constraints, this problem is in P, NP-complete,
or NP-hard, coNP-hard, and in parallel access to NP. We show how to extend the
NP-hardness and coNP-hardness to hardness for parallel access to NP for some
cases, and conjecture that this can be done in all cases."
"The connection between self-assembly and computation suggests that a shape
can be considered the output of a self-assembly ``program,'' a set of tiles
that fit together to create a shape. It seems plausible that the size of the
smallest self-assembly program that builds a shape and the shape's
descriptional (Kolmogorov) complexity should be related. We show that when
using a notion of a shape that is independent of scale, this is indeed so: in
the Tile Assembly Model, the minimal number of distinct tile types necessary to
self-assemble a shape, at some scale, can be bounded both above and below in
terms of the shape's Kolmogorov complexity. As part of the proof of the main
result, we sketch a general method for converting a program outputting a shape
as a list of locations into a set of tile types that self-assembles into a
scaled up version of that shape. Our result implies, somewhat
counter-intuitively, that self-assembly of a scaled-up version of a shape often
requires fewer tile types. Furthermore, the independence of scale in
self-assembly theory appears to play the same crucial role as the independence
of running time in the theory of computability. This leads to an elegant
formulation of languages of shapes generated by self-assembly. Considering
functions from integers to shapes, we show that the running-time complexity,
with respect to Turing machines, is polynomially equivalent to the scale
complexity of the same function implemented via self-assembly by a finite set
of tile types. Our results also hold for shapes defined by Wang tiling -- where
there is no sense of a self-assembly process -- except that here time
complexity must be measured with respect to non-deterministic Turing machines."
"The development of autonomous molecular computers capable of making
independent decisions in vivo regarding local drug administration may
revolutionize medical science. Recently Benenson at el (2004) have envisioned
one form such a ``smart drug'' may take by implementing an in vitro scheme, in
which a long DNA state molecule is cut repeatedly by a restriction enzyme in a
manner dependent upon the presence of particular short DNA ``rule molecules.''
To analyze the potential of their scheme in terms of the kinds of computations
it can perform, we study an abstraction assuming that a certain class of
restriction enzymes is available and reactions occur without error. We also
discuss how our molecular algorithms could perform with known restriction
enzymes. By exhibiting a way to simulate arbitrary circuits, we show that these
``Benenson automata'' are capable of computing arbitrary Boolean functions.
Further, we show that they are able to compute efficiently exactly those
functions computable by log-depth circuits. Computationally, we formalize a new
variant of limited width branching programs with a molecular implementation."
"The present paper presents and proves a proposition concerning the time
complexity of finite languages. It is shown herein, that for any finite
language (a language for which the set of words composing it is finite) there
is a Turing machine that computes the language in such a way that for any input
of length k the machine stops in, at most, k + 1 steps."
"The nondeterministic advice complexity of the P-selective sets is known to be
exactly linear. Regarding the deterministic advice complexity of the
P-selective sets--i.e., the amount of Karp--Lipton advice needed for
polynomial-time machines to recognize them in general--the best current upper
bound is quadratic [Ko, 1983] and the best current lower bound is linear
[Hemaspaandra and Torenvliet, 1996].
  We prove that every associatively P-selective set is commutatively,
associatively P-selective. Using this, we establish an algebraic sufficient
condition for the P-selective sets to have a linear upper bound (which thus
would match the existing lower bound) on their deterministic advice complexity:
If all P-selective sets are associatively P-selective then the deterministic
advice complexity of the P-selective sets is linear. The weakest previously
known sufficient condition was P=NP.
  We also establish related results for algebraic properties of, and advice
complexity of, the nondeterministically selective sets."
"In this paper we construct a cyclically invariant Boolean function whose
sensitivity is $\Theta(n^{1/3})$. This result answers two previously published
questions. Tur\'an (1984) asked if any Boolean function, invariant under some
transitive group of permutations, has sensitivity $\Omega(\sqrt{n})$. Kenyon
and Kutin (2004) asked whether for a ``nice'' function the product of
0-sensitivity and 1-sensitivity is $\Omega(n)$. Our function answers both
questions in the negative.
  We also prove that for minterm-transitive functions (a natural class of
Boolean functions including our example) the sensitivity is $\Omega(n^{1/3})$.
Hence for this class of functions sensitivity and block sensitivity are
polynomially related."
This submission has been withdrawn at the request of the author.
"Rush Hour Logic was introduced in [Flake&Baum99] as a model of computation
inspired by the ``Rush Hour'' toy puzzle, in which cars can move horizontally
or vertically within a parking lot. The authors show how the model supports
polynomial space computation, using certain car configurations as building
blocks to construct boolean circuits for a cpu and memory. They consider the
use of cars of length 3 crucial to their construction, and conjecture that cars
of size 2 only, which we'll call `Size 2 Rush Hour', do not support polynomial
space computation. We settle this conjecture by showing that the required
building blocks are constructible in Size 2 Rush Hour. Furthermore, we consider
Unit Rush Hour, which was hitherto believed to be trivial, show its relation to
maze puzzles, and provide empirical support for its hardness."
"We comment on a recent paper by D'Abramo [Chaos, Solitons & Fractals, 25
(2005) 29], focusing on the author's statement that an algorithm can produce a
list of strings containing at least one string whose algorithmic complexity is
greater than that of the entire list. We show that this statement, although
perplexing, is not as paradoxical as it seems when the definition of
algorithmic complexity is applied correctly."
"Rabi and Sherman [RS97,RS93] proved that the hardness of factoring is a
sufficient condition for there to exist one-way functions (i.e., p-time
computable, honest, p-time noninvertible functions; this paper is in the
worst-case model, not the average-case model) that are total, commutative, and
associative but not strongly noninvertible. In this paper we improve the
sufficient condition to ``P does not equal NP.''
  More generally, in this paper we completely characterize which types of
one-way functions stand or fall together with (plain) one-way
functions--equivalently, stand or fall together with P not equaling NP. We look
at the four attributes used in Rabi and Sherman's seminal work on algebraic
properties of one-way functions (see [RS97,RS93]) and subsequent
papers--strongness (of noninvertibility), totality, commutativity, and
associativity--and for each attribute, we allow it to be required to hold,
required to fail, or ``don't care.'' In this categorization there are 3^4 = 81
potential types of one-way functions. We prove that each of these 81
feature-laden types stand or fall together with the existence of (plain)
one-way functions."
"We survey results of a quarter century of work on computation by reversible
general-purpose computers (in this setting Turing machines), and general
reversible simulation of irreversible computations, with respect to energy-,
time- and space requirements."
"We prove that P-sel, the class of all P-selective sets, is EXP-immune, but is
not EXP/1-immune. That is, we prove that some infinite P-selective set has no
infinite EXP-time subset, but we also prove that every infinite P-selective set
has some infinite subset in EXP/1. Informally put, the immunity of P-sel is so
fragile that it is pierced by a single bit of information.
  The above claims follow from broader results that we obtain about the
immunity of the P-selective sets. In particular, we prove that for every
recursive function f, P-sel is DTIME(f)-immune. Yet we also prove that P-sel is
not \Pi_2^p/1-immune."
"In the recent years, several polynomial algorithms of a dynamical nature have
been proposed to address the graph isomorphism problem. In this paper we
propose a generalization of an approach exposed in cond-mat/0209112 and find
that this dynamical algorithm is covered by a combinatorial approach. It is
possible to infer that polynomial dynamical algorithms addressing graph
isomorphism are covered by suitable polynomial combinatorial approaches and
thus are tackled by the same weaknesses as the last ones."
"We show that Kolmogorov complexity and such its estimators as universal codes
(or data compression methods) can be applied for hypotheses testing in a
framework of classical mathematical statistics. The methods for identity
testing and nonparametric testing of serial independence for time series are
suggested."
"The rigidity of a matrix describes the minimal number of entries one has to
change to reduce matrix's rank to r. We give very simple combinatorial proof of
the lower bound for the rigidity of Sylvester (special case of Hadamard) matrix
that matches the best known result by de Wolf(2005) for Hadamard matrices
proved by quantum information theoretical arguments."
"The study of semifeasible algorithms was initiated by Selman's work a quarter
of century ago [Sel79,Sel81,Sel82]. Informally put, this research stream
studies the power of those sets L for which there is a deterministic (or in
some cases, the function may belong to one of various nondeterministic function
classes) polynomial-time function f such that when at least one of x and y
belongs to L, then f(x,y) \in L \cap \{x,y\}. The intuition here is that it is
saying: ``Regarding membership in L, if you put a gun to my head and forced me
to bet on one of x or y as belonging to L, my money would be on f(x,y).''
  In this article, we present a number of open problems from the theory of
semifeasible algorithms. For each we present its background and review what
partial results, if any, are known."
"The three domatic number problem asks whether a given undirected graph can be
partitioned into at least three dominating sets, i.e., sets whose closed
neighborhood equals the vertex set of the graph. Since this problem is
NP-complete, no polynomial-time algorithm is known for it. The naive
deterministic algorithm for this problem runs in time 3^n, up to polynomial
factors. In this paper, we design an exact deterministic algorithm for this
problem running in time 2.9416^n. Thus, our algorithm can handle problem
instances of larger size than the naive algorithm in the same amount of time.
We also present another deterministic and a randomized algorithm for this
problem that both have an even better performance for graphs with small maximum
degree."
"Graph clustering is the problem of identifying sparsely connected dense
subgraphs (clusters) in a given graph. Proposed clustering algorithms usually
optimize various fitness functions that measure the quality of a cluster within
the graph. Examples of such cluster measures include the conductance, the local
and relative densities, and single cluster editing. We prove that the decision
problems associated with the optimization tasks of finding the clusters that
are optimal with respect to these fitness measures are NP-complete."
"We present a new quantum complexity class, called MQ^2, which is contained in
AWPP. This class has a compact and simple mathematical definition, involving
only polynomial-time computable functions and a unitarity condition. It
contains both Deutsch-Jozsa's and Shor's algorithm, while its relation to BQP
is unknown. This shows that in the complexity class hierarchy, BQP is not an
extraordinary isolated island, but has ''siblings'' which as well can solve
prime-factorization."
"We study EC3, a variant of Exact Cover which is equivalent to Positive 1-in-3
SAT. Random instances of EC3 were recently used as benchmarks for simulations
of an adiabatic quantum algorithm. Empirical results suggest that EC3 has a
phase transition from satisfiability to unsatisfiability when the number of
clauses per variable r exceeds some threshold r* ~= 0.62 +- 0.01. Using the
method of differential equations, we show that if r <= 0.546 w.h.p. a random
instance of EC3 is satisfiable. Combined with previous results this limits the
location of the threshold, if it exists, to the range 0.546 < r* < 0.644."
"The open question, P=NP?, was presented by Cook (1971). In this paper, a
proof that P is not equal to NP is presented. In addition, it is shown that P
is not equal to the intersection of NP and co-NP. Finally, the exact inclusion
relationships between the classes P, NP and co-NP are presented."
"A rational number can be naturally presented by an arithmetic computation
(AC): a sequence of elementary arithmetic operations starting from a fixed
constant, say 1. The asymptotic complexity issues of such a representation are
studied e.g. in the framework of the algebraic complexity theory over arbitrary
field.
  Here we study a related problem of the complexity of performing arithmetic
operations and computing elementary predicates, e.g. ``='' or ``>'', on
rational numbers given by AC.
  In the first place, we prove that AC can be efficiently simulated by the
exact semidefinite programming (SDP).
  Secondly, we give a BPP-algorithm for the equality predicate.
  Thirdly, we put ``>''-predicate into the complexity class PSPACE.
  We conjecture that ``>''-predicate is hard to compute. This conjecture, if
true, would clarify the complexity status of the exact SDP - a well known open
problem in the field of mathematical programming."
"Arnold Beckmann defined the uniform reduct of a propositional proof system f
to be the set of those bounded arithmetical formulas whose propositional
translations have polynomial size f-proofs. We prove that the uniform reduct of
f + Extended Frege consists of all true bounded arithmetical formulas iff f +
Extended Frege simulates every proof system."
"We study reductions that limit the extreme adaptivity of Turing reductions.
In particular, we study reductions that make a rapid, structured progression
through the set to which they are reducing: Each query is strictly longer
(shorter) than the previous one. We call these reductions query-increasing
(query-decreasing) Turing reductions. We also study query-nonincreasing
(query-nondecreasing) Turing reductions. These are Turing reductions in which
the sequence of query lengths is nonincreasing (nondecreasing). We ask whether
these restrictions in fact limit the power of reductions. We prove that
query-increasing and query-decreasing Turing reductions are incomparable with
(that is, are neither strictly stronger than nor strictly weaker than)
truth-table reductions and are strictly weaker than Turing reductions. In
addition, we prove that query-nonincreasing and query-nondecreasing Turing
reductions are strictly stronger than truth-table reductions and strictly
weaker than Turing reductions. Despite the fact that we prove query-increasing
and query-decreasing Turing reductions to in the general case be strictly
weaker than Turing reductions, we identify a broad class of sets A for which
any set that Turing reduces to A will also reduce to A via both
query-increasing and query-decreasing Turing reductions. In particular, this
holds for all tight paddable sets, where a set is said to be tight paddable
exactly if it is paddable via a function whose output length is bounded tightly
both from above and from below in the length of the input. We prove that many
natural NP-complete problems such as satisfiability, clique, and vertex cover
are tight paddable."
"We consider the tile self-assembly model and how tile complexity can be
eliminated by permitting the temperature of the self-assembly system to be
adjusted throughout the assembly process. To do this, we propose novel
techniques for designing tile sets that permit an arbitrary length $m$ binary
number to be encoded into a sequence of $O(m)$ temperature changes such that
the tile set uniquely assembles a supertile that precisely encodes the
corresponding binary number. As an application, we show how this provides a
general tile set of size O(1) that is capable of uniquely assembling
essentially any $n\times n$ square, where the assembled square is determined by
a temperature sequence of length $O(\log n)$ that encodes a binary description
of $n$. This yields an important decrease in tile complexity from the required
$\Omega(\frac{\log n}{\log\log n})$ for almost all $n$ when the temperature of
the system is fixed. We further show that for almost all $n$, no tile system
can simultaneously achieve both $o(\log n)$ temperature complexity and
$o(\frac{\log n}{\log\log n})$ tile complexity, showing that both versions of
an optimal square building scheme have been discovered. This work suggests that
temperature change can constitute a natural, dynamic method for providing input
to self-assembly systems that is potentially superior to the current technique
of designing large tile sets with specific inputs hardwired into the tileset."
"We study a family of problems, called \prob{Maximum Solution}, where the
objective is to maximise a linear goal function over the feasible integer
assignments to a set of variables subject to a set of constraints. When the
domain is Boolean (i.e. restricted to $\{0,1\}$), the maximum solution problem
is identical to the well-studied \prob{Max Ones} problem, and the
approximability is completely understood for all restrictions on the underlying
constraints [Khanna et al., SIAM J. Comput., 30 (2001), pp. 1863-1920]. We
continue this line of research by considering domains containing more than two
elements. We present two main results: a complete classification for the
approximability of all maximal constraint languages over domains of cardinality
at most 4, and a complete classification of the approximability of the problem
when the set of allowed constraints contains all permutation constraints. Under
the assumption that a conjecture due to Szczepara holds, we give a complete
classification for all maximal constraint languages. These classes of languages
are well-studied in universal algebra and computer science; they have, for
instance, been considered in connection with machine learning and constraint
satisfaction. Our results are proved by using algebraic results from clone
theory and the results indicates that this approach is very powerful for
classifying the approximability of certain optimisation problems."
"In the maximum constraint satisfaction problem (MAX CSP), one is given a
finite collection of (possibly weighted) constraints on overlapping sets of
variables, and the goal is to assign values from a given finite domain to the
variables so as to maximize the number (or the total weight, for the weighted
case) of satisfied constraints. This problem is NP-hard in general, and,
therefore, it is natural to study how restricting the allowed types of
constraints affects the approximability of the problem. In this paper, we show
that any MAX CSP problem with a finite set of allowed constraint types, which
includes all fixed-value constraints (i.e., constraints of the form x=a), is
either solvable exactly in polynomial-time or else is APX-complete, even if the
number of occurrences of variables in instances are bounded. Moreover, we
present a simple description of all polynomial-time solvable cases of our
problem. This description relies on the well-known algebraic combinatorial
property of supermodularity."
"We propose a new complexity measure of space for the BSS model of
computation. We define LOGSPACE\_W and PSPACE\_W complexity classes over the
reals. We prove that LOGSPACE\_W is included in NC^2\_R and in P\_W, i.e. is
small enough for being relevant. We prove that the Real Circuit Decision
Problem is P\_R-complete under LOGSPACE\_W reductions, i.e. that LOGSPACE\_W is
large enough for containing natural algorithms. We also prove that PSPACE\_W is
included in PAR\_R."
"The 3-domatic number problem asks whether a given graph can be partitioned
intothree dominating sets. We prove that this problem can be solved by a
deterministic algorithm in time 2.695^n (up to polynomial factors). This result
improves the previous bound of 2.8805^n, which is due to Fomin, Grandoni,
Pyatkin, and Stepanov. To prove our result, we combine an algorithm by Fomin et
al. with Yamamoto's algorithm for the satisfiability problem. In addition, we
show that the 3-domatic number problem can be solved for graphs G with bounded
maximum degree Delta(G) by a randomized algorithm, whose running time is better
than the previous bound due to Riege and Rothe whenever Delta(G) >= 5. Our new
randomized algorithm employs Schoening's approach to constraint satisfaction
problems."
"The Mandelbrot set is an extremely well-known mathematical object that can be
described in a quite simple way but has very interesting and non-trivial
properties. This paper surveys some results that are known concerning the
(non-)computability of the set. It considers two models of decidability over
the reals (which have been treated much more thoroughly and technically by
Hertling (2005), Blum, Shub and Smale, Brattka (2003) and Weihrauch (1999 and
2003) among others), two over the computable reals (the Russian school and
hypercomputation) and a model over the rationals."
"We report the consequences of a destabilization process on a simulated
General Purpose Analog Computer. This new technology overcomes problems linked
with serial ambiguity, and provides an analog bias to encode algorithms whose
complexity is over polynomial. We also implicitly demonstrate how
countermesures of the Stochastic Aperture Degeneracy could efficiently reach
higher computational classes, and would open a road towards Analog Reverse Time
Computation."
"For a given finite set $\Sigma$ of matrices with nonnegative integer entries
we study the growth of $$ \max_t(\Sigma) = \max\{\|A_{1}... A_{t}\|: A_i \in
\Sigma\}.$$ We show how to determine in polynomial time whether the growth with
$t$ is bounded, polynomial, or exponential, and we characterize precisely all
possible behaviors."
"2-dimensional Matching Problem, which requires to find a matching of left- to
right-vertices in a balanced $2n$-vertex bipartite graph, is a well-known
polynomial problem, while various variants, like the 3-dimensional analogoue
(3DM, with triangles on a tripartite graph), or the Hamiltonian Circuit Problem
(HC, a restriction to ``unicyclic'' matchings) are among the main examples of
NP-hard problems, since the first Karp reduction series of 1972. The same holds
for the weighted variants of these problems, the Linear Assignment Problem
being polynomial, and the Numerical 3-Dimensional Matching and Travelling
Salesman Problem being NP-complete.
  In this paper we show that a small modification of the 2-dimensional Matching
and Assignment Problems in which for each $i \leq n/2$ it is required that
either $\pi(2i-1)=2i-1$ or $\pi(2i)=2i$, is a NP-complete problem. The proof is
by linear reduction from SAT (or NAE-SAT), with the size $n$ of the Matching
Problem being four times the number of edges in the factor graph representation
of the boolean problem. As a corollary, in combination with the simple linear
reduction of One-in-Two Matching to 3-Dimensional Matching, we show that SAT
can be linearly reduced to 3DM, while the original Karp reduction was only
cubic."
"Given a function based on the computation of an NP machine, can one in
general eliminate some solutions? That is, can one in general decrease the
ambiguity? This simple question remains, even after extensive study by many
researchers over many years, mostly unanswered. However, complexity-theoretic
consequences and enabling conditions are known. In this tutorial-style article
we look at some of those, focusing on the most natural framings: reducing the
number of solutions of NP functions, refining the solutions of NP functions,
and subtracting from or otherwise shrinking #P functions. We will see how small
advice strings are important here, but we also will see how increasing advice
size to achieve robustness is central to the proof of a key ambiguity-reduction
result for NP functions."
"We introduce the zeta number, natural halting probability and natural
complexity of a Turing machine and we relate them to Chaitin's Omega number,
halting probability, and program-size complexity. A classification of Turing
machines according to their zeta numbers is proposed: divergent, convergent and
tuatara. We prove the existence of universal convergent and tuatara machines.
Various results on (algorithmic) randomness and partial randomness are proved.
For example, we show that the zeta number of a universal tuatara machine is
c.e. and random. A new type of partial randomness, asymptotic randomness, is
introduced. Finally we show that in contrast to classical (algorithmic)
randomness--which cannot be naturally characterised in terms of plain
complexity--asymptotic randomness admits such a characterisation."
"We survey the average-case complexity of problems in NP.
  We discuss various notions of good-on-average algorithms, and present
completeness results due to Impagliazzo and Levin. Such completeness results
establish the fact that if a certain specific (but somewhat artificial) NP
problem is easy-on-average with respect to the uniform distribution, then all
problems in NP are easy-on-average with respect to all samplable distributions.
Applying the theory to natural distributional problems remain an outstanding
open question. We review some natural distributional problems whose
average-case complexity is of particular interest and that do not yet fit into
this theory.
  A major open question whether the existence of hard-on-average problems in NP
can be based on the P$\neq$NP assumption or on related worst-case assumptions.
We review negative results showing that certain proof techniques cannot prove
such a result. While the relation between worst-case and average-case
complexity for general NP problems remains open, there has been progress in
understanding the relation between different ``degrees'' of average-case
complexity. We discuss some of these ``hardness amplification'' results."
"We study the approximability of Max Ones when the number of variable
occurrences is bounded by a constant. For conservative constraint languages
(i.e., when the unary relations are included) we give a complete classification
when the number of occurrences is three or more and a partial classification
when the bound is two.
  For the non-conservative case we prove that it is either trivial or
equivalent to the corresponding conservative problem under polynomial-time
many-one reductions."
"M.Alekhnovich et al. recently have proposed a model of algorithms, called BT
model, which covers Greedy, Backtrack and Simple Dynamic Programming methods
and can be further divided into fixed, adaptive and fully adaptive three kinds,
and have proved exponential time lower bounds of exact and approximation
algorithms under adaptive BT model for Knapsack problem which are
$\Omega(2^{n/2}/\sqrt n)=\Omega(2^{0.5n}/\sqrt n)$ and
$\Omega((1/\epsilon)^{1/3.17})\approx\Omega((1/\epsilon)^{0.315})$(for
approximation ratio $1-\epsilon$) respectively (M. Alekhovich, A. Borodin, J.
Buresh-Oppenheim, R. Impagliazzo, A. Magen, and T. Pitassi, Toward a Model for
Backtracking and Dynamic Programming, \emph{Proceedings of Twentieth Annual
IEEE Conference on Computational Complexity}, pp308-322, 2005). In this note,
we slightly improved their lower bounds to
$\Omega(2^{(2-\epsilon)n/3}/\sqrt{n})\approx \Omega(2^{0.66n}/\sqrt{n})$ and
$\Omega((1/\epsilon)^{1/2.38})\approx\Omega((1/\epsilon)^{0.420})$, and
proposed as an open question what is the best achievable lower bounds for
knapsack under adaptive BT models."
"In 1975, Ladner showed that under the hypothesis that P is not equal to NP,
there exists a language which is neither in P, nor NP-complete. This result was
latter generalized by Schoning and several authors to various polynomial-time
complexity classes. We show here that such results also apply to linear-time
reductions on RAMs (resp. Turing machines), and hence allow for separation
results in linear-time classes similar to Ladner's ones for polynomial time."
"Considerable thought has been devoted to an adequate definition of the class
of infinite, random binary sequences (the sort of sequence that almost
certainly arises from flipping a fair coin indefinitely). The first
mathematical exploration of this problem was due to R. Von Mises, and based on
his concept of a ""selection function."" A decisive objection to Von Mises' idea
was formulated in a theorem offered by Jean Ville in 1939. It shows that some
sequences admitted by Von Mises as ""random"" in fact manifest a certain kind of
systematicity. Ville's proof is challenging, and an alternative approach has
appeared only in condensed form. We attempt to provide an expanded version of
the latter, alternative argument."
"In this note, we present an elegant argument that P is not NP by
demonstrating that the Meet-in-the-Middle algorithm must have the fastest
running-time of all deterministic and exact algorithms which solve the
SUBSET-SUM problem on a classical computer."
"Bellantoni and Cook have given a function-algebra characterization of the
polynomial-time computable functions via an unbounded recursion scheme which is
called safe recursion. Inspired by their work, we characterize the
exponential-time computable functions with the use of a safe variant of nested
recursion."
"In a previous paper, the sup-interpretation method was proposed as a new tool
to control memory resources of first order functional programs with pattern
matching by static analysis. Basically, a sup-interpretation provides an upper
bound on the size of function outputs. In this former work, a criterion, which
can be applied to terminating as well as non-terminating programs, was
developed in order to bound polynomially the stack frame size. In this paper,
we suggest a new criterion which captures more algorithms computing values
polynomially bounded in the size of the inputs. Since this work is related to
quasi-interpretations, we compare the two notions obtaining two main features.
The first one is that, given a program, we have heuristics for finding a
sup-interpretation when we consider polynomials of bounded degree. The other
one consists in the characterizations of the set of function computable in
polynomial time and in polynomial space."
"We study a generalized version of reversal bounded Turing machines where,
apart from several tapes on which the number of head reversals is bounded by
r(n), there are several further tapes on which head reversals remain
unrestricted, but size is bounded by s(n). Recently, such machines were
introduced as a formalization of a computation model that restricts random
access to external memory and internal memory space. Here, each of the tapes
with a restriction on the head reversals corresponds to an external memory
device, and the tapes of restricted size model internal memory. We use
ST(r(n),s(n),O(1)) to denote the class of all problems that can be solved by
deterministic Turing machines that comply to the above resource bounds.
Similarly, NST and RST, respectively, are used for the corresponding
nondeterministic and randomized classes.
  While previous papers focused on lower bounds for particular problems,
including sorting, the set equality problem, and several query evaluation
problems, the present paper addresses the relations between the (R,N)ST-classes
and classical complexity classes and investigates the structural complexity of
the (R,N)ST-classes. Our main results are (1) a trade-off between internal
memory space and external memory head reversals, (2) correspondences between
the (R,N)ST-classes and ``classical'' time-bounded, space-bounded,
reversal-bounded, and circuit complexity classes, and (3) hierarchies of
(R)ST-classes in terms of increasing numbers of head reversals on external
memory tapes."
"It will be shown that the polynomial time computable numbers form a field,
and especially an algebraically closed field."
"A function $f$ of a graph is called a complete graph invariant if the
isomorphism of graphs $G$ and $H$ is equivalent to the equality $f(G)=f(H)$.
If, in addition, $f(G)$ is a graph isomorphic to $G$, then $f$ is called a
canonical form for graphs. Gurevich proves that graphs have a polynomial-time
computable canonical form exactly when they have a polynomial-time computable
complete invariant. We extend this equivalence to the polylogarithmic-time
model of parallel computation for classes of graphs with bounded rigidity index
and for classes of graphs with small separators. In particular, our results
apply to three representative classes of graphs embeddable into a fixed
surface, namely, to 5-connected graphs, to 3-connected graphs admitting a
polyhedral embedding, and 3-connected graphs admitting a large-edge-width
embedding. Another application covers graphs with bounded treewidth. Since in
the latter case an NC complete-invariant algorithm is known, we conclude that
graphs of bounded treewidth have a canonical form (and even a canonical
labeling) computable in NC."
"This paper studies how well computable functions can be approximated by their
Fourier series. To this end, we equip the space of Lp-computable functions
(computable Lebesgue integrable functions) with a size notion, by introducing
Lp-computable Baire categories.
  We show that Lp-computable Baire categories satisfy the following three basic
properties. Singleton sets {f} (where f is Lp-computable) are meager, suitable
infinite unions of meager sets are meager, and the whole space of Lp-computable
functions is not meager. We give an alternative characterization of meager sets
via Banach Mazur games.
  We study the convergence of Fourier series for Lp-computable functions and
show that whereas for every p>1, the Fourier series of every Lp-computable
function f converges to f in the Lp norm, the set of L1-computable functions
whose Fourier series does not diverge almost everywhere is meager."
"We introduce two resource-bounded Baire category notions on small complexity
classes such as P, SUBEXP, and PSPACE and on probabilistic classes such as BPP,
which differ on how the corresponding finite extension strategies are computed.
We give an alternative characterization of small sets via resource-bounded
Banach-Mazur games.
  As an application of the first notion, we show that for almost every language
A (i.e. all except a meager class) computable in subexponential time,
P(A)=BPP(A). We also show that almost all languages in PSPACE do not have small
nonuniform complexity.
  We then switch to the second Baire category notion (called
locally-computable), and show that the class SPARSE is meager in P. We show
that in contrast to the resource-bounded measure case, meager-comeager laws can
be obtained for many standard complexity classes, relative to
locally-computable Baire category on BPP and PSPACE.
  Another topic where locally-computable Baire categories differ from
resource-bounded measure is regarding weak-completeness: we show that there is
no weak-completeness notion in P based on locally-computable Baire categories,
i.e. every P-weakly-complete set is complete for P. We also prove that the
class of complete sets for P under Turing-logspace reductions is meager in P,
if P is not equal to DSPACE(log n), and that the same holds unconditionally for
quasi-poly time.
  Finally we observe that locally-computable Baire categories are incomparable
with all existing resource-bounded measure notions on small complexity classes,
which might explain why those two settings seem to differ so fundamentally."
"Boolean satisfiability problems are an important benchmark for questions
about complexity, algorithms, heuristics and threshold phenomena. Recent work
on heuristics, and the satisfiability threshold has centered around the
structure and connectivity of the solution space. Motivated by this work, we
study structural and connectivity-related properties of the space of solutions
of Boolean satisfiability problems and establish various dichotomies in
Schaefer's framework.
  On the structural side, we obtain dichotomies for the kinds of subgraphs of
the hypercube that can be induced by the solutions of Boolean formulas, as well
as for the diameter of the connected components of the solution space. On the
computational side, we establish dichotomy theorems for the complexity of the
connectivity and st-connectivity questions for the graph of solutions of
Boolean formulas. Our results assert that the intractable side of the
computational dichotomies is PSPACE-complete, while the tractable side - which
includes but is not limited to all problems with polynomial time algorithms for
satisfiability - is in P for the st-connectivity question, and in coNP for the
connectivity question. The diameter of components can be exponential for the
PSPACE-complete cases, whereas in all other cases it is linear; thus, small
diameter and tractability of the connectivity problems are remarkably aligned.
The crux of our results is an expressibility theorem showing that in the
tractable cases, the subgraphs induced by the solution space possess certain
good structural properties, whereas in the intractable cases, the subgraphs can
be arbitrary."
"We introduce a new class VPSPACE of families of polynomials. Roughly
speaking, a family of polynomials is in VPSPACE if its coefficients can be
computed in polynomial space. Our main theorem is that if (uniform,
constant-free) VPSPACE families can be evaluated efficiently then the class PAR
of decision problems that can be solved in parallel polynomial time over the
real numbers collapses to P. As a result, one must first be able to show that
there are VPSPACE families which are hard to evaluate in order to separate over
the reals P from NP, or even from PAR."
"In this note, we show that the Traveling Salesman Problem cannot be solved in
polynomial-time on a classical computer."
This submission has been withdrawn at the request of the author.
"We consider a basic problem in the general data streaming model, namely, to
estimate a vector $f \in \Z^n$ that is arbitrarily updated (i.e., incremented
or decremented) coordinate-wise. The estimate $\hat{f} \in \Z^n$ must satisfy
$\norm{\hat{f}-f}_{\infty}\le \epsilon\norm{f}_1 $, that is, $\forall i
~(\abs{\hat{f}_i - f_i} \le \epsilon \norm{f}_1)$. It is known to have
$\tilde{O}(\epsilon^{-1})$ randomized space upper bound \cite{cm:jalgo},
$\Omega(\epsilon^{-1} \log (\epsilon n))$ space lower bound
\cite{bkmt:sirocco03} and deterministic space upper bound of
$\tilde{\Omega}(\epsilon^{-2})$ bits.\footnote{The $\tilde{O}$ and
$\tilde{\Omega}$ notations suppress poly-logarithmic factors in $n, \log
\epsilon^{-1}, \norm{f}_{\infty}$ and $\log \delta^{-1}$, where, $\delta$ is
the error probability (for randomized algorithm).} We show that any
deterministic algorithm for this problem requires space $\Omega(\epsilon^{-2}
(\log \norm{f}_1))$ bits."
"Suppose we have a family ${\cal F}$ of sets. For every $S \in {\cal F}$, a
set $D \subseteq S$ is a {\sf defining set} for $({\cal F},S)$ if $S$ is the
only element of $\cal{F}$ that contains $D$ as a subset. This concept has been
studied in numerous cases, such as vertex colorings, perfect matchings,
dominating sets, block designs, geodetics, orientations, and Latin squares.
  In this paper, first, we propose the concept of a defining set of a logical
formula, and we prove that the computational complexity of such a problem is
$\Sigma_2$-complete.
  We also show that the computational complexity of the following problem about
the defining set of vertex colorings of graphs is $\Sigma_2$-complete:
  {\sc Instance:} A graph $G$ with a vertex coloring $c$ and an integer $k$.
  {\sc Question:} If ${\cal C}(G)$ be the set of all $\chi(G)$-colorings of
$G$, then does $({\cal C}(G),c)$ have a defining set of size at most $k$?
  Moreover, we study the computational complexity of some other variants of
this problem."
"Using an approach that seems to be patterned after that of Yannakakis, Hofman
argues that an NP-complete problem cannot be formulated as a polynomial
bounded-sized linear programming problem. He then goes on to propose a
""construct"" that he claims to be a counter-example to recently published linear
programming formulations of the Traveling Salesman Problem (TSP) and the
Quadratic Assignment Problems (QAP), respectively. In this paper, we show that
Hofman's construct is flawed, and provide further proof that his
""counter-example"" is invalid."
"In a recent paper by S. Gubin [cs/0701023v1], a polynomial-time solution to
the 3SAT problem was presented as proof that P=NP. The proposed algorithm
cannot be made to work, which I shall demonstrate."
"We introduce QUEENS, a derivative chess problem based on the classical
n-queens problem. We prove that QUEENS is NP-complete, with respect to
polynomial-time reductions."
"We propose a computing model, the Two-Way Optical Interference Automata
(2OIA), that makes use of the phenomenon of optical interference. We introduce
this model to investigate the increase in power, in terms of language
recognition, of a classical Deterministic Finite Automaton (DFA) when endowed
with the facility of optical interference. The question is in the spirit of
Two-Way Finite Automata With Quantum and Classical States (2QCFA) [A. Ambainis
and J. Watrous, Two-way Finite Automata With Quantum and Classical States,
Theoretical Computer Science, 287 (1), 299-311, (2002)] wherein the classical
DFA is augmented with a quantum component of constant size. We test the power
of 2OIA against the languages mentioned in the above paper. We give efficient
2OIA algorithms to recognize languages for which 2QCFA machines have been shown
to exist, as well as languages whose status vis-a-vis 2QCFA has been posed as
open questions. Finally we show the existence of a language that cannot be
recognized by a 2OIA but can be recognized by an $O(n^3)$ space Turing machine."
"We develop techniques to investigate relativized hierarchical unambiguous
computation. We apply our techniques to generalize known constructs involving
relativized unambiguity based complexity classes (UP and \mathcal{UP}) to new
constructs involving arbitrary higher levels of the relativized unambiguous
polynomial hierarchy (UPH). Our techniques are developed on constraints imposed
by hierarchical arrangement of unambiguous nondeterministic polynomial-time
Turing machines, and so they differ substantially, in applicability and in
nature, from standard methods (such as the switching lemma [Hastad,
Computational Limitations of Small-Depth Circuits, MIT Press, 1987]), which
play roles in carrying out similar generalizations.
  Aside from achieving these generalizations, we resolve a question posed by
Cai, Hemachandra, and Vyskoc [J. Cai, L. Hemachandra, and J. Vyskoc, Promises
and fault-tolerant database access, In K. Ambos-Spies, S. Homer, and U.
Schoening, editors, Complexity Theory, pages 101-146. Cambridge University
Press, 1993] on an issue related to nonadaptive Turing access to UP and
adaptive smart Turing access to \mathcal{UP}."
"Two languages are ""finitely different"" if their symmetric difference is
finite. We consider the DFAs of finitely different regular languages and find
major structural similarities. We proceed to consider the smallest DFAs that
recognize a language finitely different from some given DFA. Such ""f-minimal""
DFAs are not unique, and this non-uniqueness is characterized. Finally, we
offer a solution to the minimization problem of finding such f-minimal DFAs."
An introductory paper to the graph k-colorability problem.
"This paper describes TSP exact solution of polynomial complexity. It is
considered properties of proposed method. Effectiveness of proposed solution is
illustrated by outcomes of computer modeling."
"The main contribution of this work is the definition of a quantifier-free
string theory T_1 suitable for formalizing ALOGTIME reasoning. After describing
L_1 -- a new, simple, algebraic characterization of the complexity class
ALOGTIME based on strings instead of numbers -- the theory T_1 is defined
(based on L_1), and a detailed formal development of T_1 is given.
  Then, theorems of T_1 are shown to translate into families of propositional
tautologies that have uniform polysize Frege proofs, T_1 is shown to prove the
soundness of a particular Frege system F, and F is shown to provably p-simulate
any proof system whose soundness can be proved in T_1. Finally, T_1 is compared
with other theories for ALOGTIME reasoning in the literature.
  To our knowledge, this is the first formal theory for ALOGTIME reasoning
whose basic objects are strings instead of numbers, and the first
quantifier-free theory formalizing ALOGTIME reasoning in which a direct proof
of the soundness of some Frege system has been given (in the case of
first-order theories, such a proof was first given by Arai for his theory AID).
Also, the polysize Frege proofs we give for the propositional translations of
theorems of T_1 are considerably simpler than those for other theories, and so
is our proof of the soundness of a particular F-system in T_1. Together with
the simplicity of T_1's recursion schemes, axioms, and rules these facts
suggest that T_1 is one of the most natural theories available for ALOGTIME
reasoning."
"We show how to calculate the finite-state dimension (equivalently, the
finite-state compressibility) of a saturated sets $X$ consisting of {\em all}
infinite sequences $S$ over a finite alphabet $\Sigma_m$ satisfying some given
condition $P$ on the asymptotic frequencies with which various symbols from
$\Sigma_m$ appear in $S$. When the condition $P$ completely specifies an
empirical probability distribution $\pi$ over $\Sigma_m$, i.e., a limiting
frequency of occurrence for {\em every} symbol in $\Sigma_m$, it has been known
since 1949 that the Hausdorff dimension of $X$ is precisely $\CH(\pi)$, the
Shannon entropy of $\pi$, and the finite-state dimension was proven to have
this same value in 2001.
  The saturated sets were studied by Volkmann and Cajar decades ago. It got
attention again only with the recent developments in multifractal analysis by
Barreira, Saussol, Schmeling, and separately Olsen. However, the powerful
methods they used -- ergodic theory and multifractal analysis -- do not yield a
value for the finite-state (or even computable) dimension in an obvious manner.
  We give a pointwise characterization of finite-state dimensions of saturated
sets. Simultaneously, we also show that their finite-state dimension and strong
dimension coincide with their Hausdorff and packing dimension respectively,
though the techniques we use are completely elementary. Our results
automatically extend to less restrictive effective settings (e.g.,
constructive, computable, and polynomial-time dimensions)."
"The Kronecker coefficient g_{\lambda \mu \nu} is the multiplicity of the
GL(V)\times GL(W)-irreducible V_\lambda \otimes W_\mu in the restriction of the
GL(X)-irreducible X_\nu via the natural map GL(V)\times GL(W) \to GL(V \otimes
W), where V, W are \mathbb{C}-vector spaces and X = V \otimes W. A fundamental
open problem in algebraic combinatorics is to find a positive combinatorial
formula for these coefficients.
  We construct two quantum objects for this problem, which we call the
nonstandard quantum group and nonstandard Hecke algebra. We show that the
nonstandard quantum group has a compact real form and its representations are
completely reducible, that the nonstandard Hecke algebra is semisimple, and
that they satisfy an analog of quantum Schur-Weyl duality.
  Using these nonstandard objects as a guide, we follow the approach of Adsul,
Sohoni, and Subrahmanyam to construct, in the case dim(V) = dim(W) =2, a
representation \check{X}_\nu of the nonstandard quantum group that specializes
to Res_{GL(V) \times GL(W)} X_\nu at q=1. We then define a global crystal basis
+HNSTC(\nu) of \check{X}_\nu that solves the two-row Kronecker problem: the
number of highest weight elements of +HNSTC(\nu) of weight (\lambda,\mu) is the
Kronecker coefficient g_{\lambda \mu \nu}. We go on to develop the beginnings
of a graphical calculus for this basis, along the lines of the U_q(\sl_2)
graphical calculus, and use this to organize the crystal components of
+HNSTC(\nu) into eight families. This yields a fairly simple, explicit and
positive formula for two-row Kronecker coefficients, generalizing a formula of
Brown, van Willigenburg, and Zabrocki. As a byproduct of the approach, we also
obtain a rule for the decomposition of Res_{GL_2 \times GL_2 \rtimes \S_2}
X_\nu into irreducibles."
"Description of a polynomial time reduction of SAT to 2-SAT of polynomial
size."
"This article has been withdrawn because it has been merged with the earlier
article GCT3 (arXiv: CS/0501076 [cs.CC]) in the series. The merged article is
now available as:
  Geometric Complexity Theory III: on deciding nonvanishing of a
Littlewood-Richardson Coefficient, Journal of Algebraic Combinatorics, vol. 36,
issue 1, 2012, pp. 103-110. (Authors: Ketan Mulmuley, Hari Narayanan and Milind
Sohoni)
  The new article in this GCT5 slot in the series is:
  Geometric Complexity Theory V: Equivalence between blackbox derandomization
of polynomial identity testing and derandomization of Noether's Normalization
Lemma, in the Proceedings of FOCS 2012 (abstract), arXiv:1209.5993 [cs.CC]
(full version) (Author: Ketan Mulmuley)"
"This article belongs to a series on geometric complexity theory (GCT), an
approach to the P vs. NP and related problems through algebraic geometry and
representation theory. The basic principle behind this approach is called the
flip. In essence, it reduces the negative hypothesis in complexity theory (the
lower bound problems), such as the P vs. NP problem in characteristic zero, to
the positive hypothesis in complexity theory (the upper bound problems):
specifically, to showing that the problems of deciding nonvanishing of the
fundamental structural constants in representation theory and algebraic
geometry, such as the well known plethysm constants--or rather certain relaxed
forms of these decision probelms--belong to the complexity class P. In this
article, we suggest a plan for implementing the flip, i.e., for showing that
these relaxed decision problems belong to P. This is based on the reduction of
the preceding complexity-theoretic positive hypotheses to mathematical
positivity hypotheses: specifically, to showing that there exist positive
formulae--i.e. formulae with nonnegative coefficients--for the structural
constants under consideration and certain functions associated with them. These
turn out be intimately related to the similar positivity properties of the
Kazhdan-Lusztig polynomials and the multiplicative structural constants of the
canonical (global crystal) bases in the theory of Drinfeld-Jimbo quantum
groups. The known proofs of these positivity properties depend on the Riemann
hypothesis over finite fields and the related results. Thus the reduction here,
in conjunction with the flip, in essence, says that the validity of the P vs.
NP conjecture in characteristic zero is intimately linked to the Riemann
hypothesis over finite fields and related problems."
"Moore introduced a class of real-valued ""recursive"" functions by analogy with
Kleene's formulation of the standard recursive functions. While his concise
definition inspired a new line of research on analog computation, it contains
some technical inaccuracies. Focusing on his ""primitive recursive"" functions,
we pin down what is problematic and discuss possible attempts to remove the
ambiguity regarding the behavior of the differential recursion operator on
partial functions. It turns out that in any case the purported relation to
differentially algebraic functions, and hence to Shannon's model of analog
computation, fails."
"Holzer and Holzer (Discrete Applied Mathematics 144(3):345--358, 2004) proved
that the Tantrix(TM) rotation puzzle problem is NP-complete. They also showed
that for infinite rotation puzzles, this problem becomes undecidable. We study
the counting version and the unique version of this problem. We prove that the
satisfiability problem parsimoniously reduces to the Tantrix(TM) rotation
puzzle problem. In particular, this reduction preserves the uniqueness of the
solution, which implies that the unique Tantrix(TM) rotation puzzle problem is
as hard as the unique satisfiability problem, and so is DP-complete under
polynomial-time randomized reductions, where DP is the second level of the
boolean hierarchy over NP."
"This paper has been withdrawn Abstract: This paper has been withdrawn by the
author due to the publication."
"We extend the transfer theorem of [KP2007] to the complex field. That is, we
investigate the links between the class VPSPACE of families of polynomials and
the Blum-Shub-Smale model of computation over C. Roughly speaking, a family of
polynomials is in VPSPACE if its coefficients can be computed in polynomial
space. Our main result is that if (uniform, constant-free) VPSPACE families can
be evaluated efficiently then the class PAR of decision problems that can be
solved in parallel polynomial time over the complex field collapses to P. As a
result, one must first be able to show that there are VPSPACE families which
are hard to evaluate in order to separate P from NP over C, or even from PAR."
"We examine a proof by Craig Alan Feinstein that P is not equal to NP. We
present counterexamples to claims made in his paper and expose a flaw in the
methodology he uses to make his assertions. The fault in his argument is the
incorrect use of reduction. Feinstein makes incorrect assumptions about the
complexity of a problem based on the fact that there is a more complex problem
that can be used to solve it. His paper introduces the terminology ""imaginary
processor"" to describe how it is possible to beat the brute force reduction he
offers to solve the Subset-Sum problem. The claims made in the paper would not
be validly established even were imaginary processors to exist."
"The class of problems complete for NP via first-order reductions is known to
be characterized by existential second-order sentences of a fixed form. All
such sentences are built around the so-called generalized IS-form of the
sentence that defines Independent-Set. This result can also be understood as
that every sentence that defines a NP-complete problem P can be decomposed in
two disjuncts such that the first one characterizes a fragment of P as hard as
Independent-Set and the second the rest of P. That is, a decomposition that
divides every such sentence into a quotient and residue modulo Independent-Set.
  In this paper, we show that this result can be generalized over a wide
collection of complexity classes, including the so-called nice classes.
Moreover, we show that such decomposition can be done for any complete problem
with respect to the given class, and that two such decompositions are
non-equivalent in general. Interestingly, our results are based on simple and
well-known properties of first-order reductions.ow that this result can be
generalized over a wide collection of complexity classes, including the
so-called nice classes. Moreover, we show that such decomposition can be done
for any complete problem with respect to the given class, and that two such
decompositions are non-equivalent in general. Interestingly, our results are
based on simple and well-known properties of first-order reductions."
"A study of assisted problem solving formalized via decompositions of
deterministic finite automata is initiated. The landscape of new types of
decompositions of finite automata this study uncovered is presented. Languages
with various degrees of decomposability between undecomposable and perfectly
decomposable are shown to exist."
"This article is a short introduction to generic case complexity, which is a
recently developed way of measuring the difficulty of a computational problem
while ignoring atypical behavior on a small set of inputs. Generic case
complexity applies to both recursively solvable and recursively unsolvable
problems."
"We give small universal Turing machines with state-symbol pairs of (6, 2),
(3, 3) and (2, 4). These machines are weakly universal, which means that they
have an infinitely repeated word to the left of their input and another to the
right. They simulate Rule 110 and are currently the smallest known weakly
universal Turing machines."
"A Boolean function is symmetric if it is invariant under all permutations of
its arguments; it is quasi-symmetric if it is symmetric with respect to the
arguments on which it actually depends. We present a test that accepts every
quasi-symmetric function and, except with an error probability at most delta>0,
rejects every function that differs from every quasi-symmetric function on at
least a fraction epsilon>0 of the inputs. For a function of n arguments, the
test probes the function at O((n/epsilon)\log(n/delta)) inputs. Our
quasi-symmetry test acquires information concerning the arguments on which the
function actually depends. To do this, it employs a generalization of the
property testing paradigm that we call attribute estimation. Like property
testing, attribute estimation uses random sampling to obtain results that have
only ""one-sided'' errors and that are close to accurate with high probability."
"A polynomial-time algorithm for computing the permanent in any field of
characteristic 3 is presented in this article. The principal objects utilized
for that purpose are the Cauchy and Vandermonde matrices, the discriminant
function and their generalizations of various types. Classical theorems on the
permanent such as the Binet-Minc identity and Borchadt's formula are widely
applied, while a special new technique involving the notion of limit re-defined
for fields of finite characteristics and corresponding computational methods
was developed in order to deal with a number of polynomial-time reductions. All
the constructions preserve a strictly algebraic nature ignoring the structure
of the basic field, while applying its infinite extensions for calculating
limits.
  A natural corollary of the polynomial-time computability of the permanent in
a field of a characteristic different from 2 is the non-uniform equality
between the complexity classes P and NP what is equivalent to RP=NP (Ref. [1])."
"Combining the the results of A.R. Meyer and L.J. Stockmeyer ""The Equivalence
Problem for Regular Expressions with Squaring Requires Exponential Space"", and
K.S. Booth ""Isomorphism testing for graphs, semigroups, and finite automata are
polynomiamlly equivalent problems"" shows that graph isomorphism is
PSPACE-complete."
"We investigate the following question: if a polynomial can be evaluated at
rational points by a polynomial-time boolean algorithm, does it have a
polynomial-size arithmetic circuit? We argue that this question is certainly
difficult. Answering it negatively would indeed imply that the constant-free
versions of the algebraic complexity classes VP and VNP defined by Valiant are
different. Answering this question positively would imply a transfer theorem
from boolean to algebraic complexity. Our proof method relies on Lagrange
interpolation and on recent results connecting the (boolean) counting hierarchy
to algebraic complexity classes. As a byproduct we obtain two additional
results: (i) The constant-free, degree-unbounded version of Valiant's
hypothesis that VP and VNP differ implies the degree-bounded version. This
result was previously known to hold for fields of positive characteristic only.
(ii) If exponential sums of easy to compute polynomials can be computed
efficiently, then the same is true of exponential products. We point out an
application of this result to the P=NP problem in the Blum-Shub-Smale model of
computation over the field of complex numbers."
"We study the structure of satisfying assignments of a random 3-SAT formula.
In particular, we show that a random formula of density 4.453 or higher almost
surely has no non-trivial ""core"" assignments. Core assignments are certain
partial assignments that can be extended to satisfying assignments, and have
been studied recently in connection with the Survey Propagation heuristic for
random SAT. Their existence implies the presence of clusters of solutions, and
they have been shown to exist with high probability below the satisfiability
threshold for k-SAT with k>8, by Achlioptas and Ricci-Tersenghi, STOC 2006. Our
result implies that either this does not hold for 3-SAT or the threshold
density for satisfiability in 3-SAT lies below 4.453.
  The main technical tool that we use is a novel simple application of the
first moment method."
"Deterministic and probabilistic communication protocols are introduced in
which parties can exchange the values of polynomials (rather than bits in the
usual setting). It is established a sharp lower bound $2n$ on the communication
complexity of recognizing the $2n$-dimensional orthant, on the other hand the
probabilistic communication complexity of its recognizing does not exceed 4. A
polyhedron and a union of hyperplanes are constructed in $\RR^{2n}$ for which a
lower bound $n/2$ on the probabilistic communication complexity of recognizing
each is proved. As a consequence this bound holds also for the EMPTINESS and
the KNAPSACK problems."
"This is a summary of the proof by G.E. Coxson that P-matrix recognition is
co-NP-complete. The result follows by a reduction from the MAX CUT problem
using results of S. Poljak and J. Rohn."
"It is a challenge to manage complex systems efficiently without confronting
NP-hard problems. To address the situation we suggest to use self-organization
processes of prime integer relations for information processing.
Self-organization processes of prime integer relations define correlation
structures of a complex system and can be equivalently represented by
transformations of two-dimensional geometrical patterns determining the
dynamics of the system and revealing its structural complexity. Computational
experiments raise the possibility of an optimality condition of complex systems
presenting the structural complexity of a system as a key to its optimization.
  From this perspective the optimization of a system could be all about the
control of the structural complexity of the system to make it consistent with
the structural complexity of the problem. The experiments also indicate that
the performance of a complex system may behave as a concave function of the
structural complexity. Therefore, once the structural complexity could be
controlled as a single entity, the optimization of a complex system would be
potentially reduced to a one-dimensional concave optimization irrespective of
the number of variables involved its description. This might open a way to a
new type of information processing for efficient management of complex systems."
"We give a trichotomy theorem for the complexity of approximately counting the
number of satisfying assignments of a Boolean CSP instance. Such problems are
parameterised by a constraint language specifying the relations that may be
used in constraints. If every relation in the constraint language is affine
then the number of satisfying assignments can be exactly counted in polynomial
time. Otherwise, if every relation in the constraint language is in the
co-clone IM_2 from Post's lattice, then the problem of counting satisfying
assignments is complete with respect to approximation-preserving reductions in
the complexity class #RH\Pi_1. This means that the problem of approximately
counting satisfying assignments of such a CSP instance is equivalent in
complexity to several other known counting problems, including the problem of
approximately counting the number of independent sets in a bipartite graph. For
every other fixed constraint language, the problem is complete for #P with
respect to approximation-preserving reductions, meaning that there is no fully
polynomial randomised approximation scheme for counting satisfying assignments
unless NP=RP."
"Holzer and Holzer (Discrete Applied Mathematics 144(3):345--358, 2004) proved
that the Tantrix(TM) rotation puzzle problem with four colors is NP-complete,
and they showed that the infinite variant of this problem is undecidable. In
this paper, we study the three-color and two-color Tantrix(TM) rotation puzzle
problems (3-TRP and 2-TRP) and their variants. Restricting the number of
allowed colors to three (respectively, to two) reduces the set of available
Tantrix(TM) tiles from 56 to 14 (respectively, to 8). We prove that 3-TRP and
2-TRP are NP-complete, which answers a question raised by Holzer and Holzer in
the affirmative. Since our reductions are parsimonious, it follows that the
problems Unique-3-TRP and Unique-2-TRP are DP-complete under randomized
reductions. We also show that the another-solution problems associated with
4-TRP, 3-TRP, and 2-TRP are NP-complete. Finally, we prove that the infinite
variants of 3-TRP and 2-TRP are undecidable."
"Algorithms testing two graphs for isomorphism known as yet in computer
science have exponential worst case complexity. In this paper we propose an
algorithm that has polynomial complexity and constructively supplies the
evidence that the graph isomorphism lies in P."
"Yatsenko gives a polynomial-time algorithm for solving the traveling salesman
problem. We examine the correctness of the algorithm and its construction. We
also comment on Yatsenko's evaluation of the algorithm."
"Using ideas from automata theory we design a new efficient (deterministic)
identity test for the \emph{noncommutative} polynomial identity testing problem
(first introduced and studied in \cite{RS05,BW05}). We also apply this idea to
the reconstruction of black-box noncommuting algebraic branching programs.
Assuming the black-box model allows us to query the ABP for the output at any
given gate, we can reconstruct an (equivalent) ABP in deterministic polynomial
time. Finally, we explore commutative identity testing when the coefficients of
the input polynomial come from an arbitrary finite commutative ring with unity."
"We obtain a lower bound of n^Omega(1) on the k-party randomized communication
complexity of the Disjointness function in the `Number on the Forehead' model
of multiparty communication when k is a constant. For k=o(loglog n), the bounds
remain super-polylogarithmic i.e. (log n)^omega(1). The previous best lower
bound for three players until recently was Omega(log n).
  Our bound separates the communication complexity classes NP^{CC}_k and
BPP^{CC}_k for k=o(loglog n). Furthermore, by the results of Beame, Pitassi and
Segerlind \cite{BPS07}, our bound implies proof size lower bounds for
tree-like, degree k-1 threshold systems and superpolynomial size lower bounds
for Lovasz-Schrijver proofs.
  Sherstov \cite{She07b} recently developed a novel technique to obtain lower
bounds on two-party communication using the approximate polynomial degree of
boolean functions. We obtain our results by extending his technique to the
multi-party setting using ideas from Chattopadhyay \cite{Cha07}.
  A similar bound for Disjointness has been recently and independently obtained
by Lee and Shraibman."
"We prove that every key exchange protocol in the random oracle model in which
the honest users make at most n queries to the oracle can be broken by an
adversary making O(n^2) queries to the oracle. This improves on the previous
Omega(n^6) query attack given by Impagliazzo and Rudich (STOC '89). Our bound
is optimal up to a constant factor since Merkle (CACM '78) gave an n query key
exchange protocol in this model that cannot be broken by an adversary making
o(n^2) queries."
"In this thesis, we study the place of regular languages within the
communication complexity setting. In particular, we are interested in the
non-deterministic communication complexity of regular languages.
  We show that a regular language has either O(1) or Omega(log n)
non-deterministic complexity. We obtain several linear lower bound results
which cover a wide range of regular languages having linear non-deterministic
complexity. These lower bound results also imply a result in semigroup theory:
we obtain sufficient conditions for not being in the positive variety Pol(Com).
  To obtain our results, we use algebraic techniques. In the study of regular
languages, the algebraic point of view pioneered by Eilenberg (\cite{Eil74})
has led to many interesting results. Viewing a semigroup as a computational
device that recognizes languages has proven to be prolific from both semigroup
theory and formal languages perspectives. In this thesis, we provide further
instances of such mutualism."
"We show that the Double Coset Membership problem for permutation groups
possesses perfect zero-knowledge proofs."
"We design a perfect zero-knowledge proof system for recognition if two
permutation groups are conjugate."
"A graph homomorphism is a vertex map which carries edges from a source graph
to edges in a target graph. We study the approximability properties of the
Weighted Maximum H-Colourable Subgraph problem (MAX H-COL). The instances of
this problem are edge-weighted graphs G and the objective is to find a subgraph
of G that has maximal total edge weight, under the condition that the subgraph
has a homomorphism to H; note that for H=K_k this problem is equivalent to MAX
k-CUT. To this end, we introduce a metric structure on the space of graphs
which allows us to extend previously known approximability results to larger
classes of graphs. Specifically, the approximation algorithms for MAX CUT by
Goemans and Williamson and MAX k-CUT by Frieze and Jerrum can be used to yield
non-trivial approximation results for MAX H-COL. For a variety of graphs, we
show near-optimality results under the Unique Games Conjecture. We also use our
method for comparing the performance of Frieze & Jerrum's algorithm with
Hastad's approximation algorithm for general MAX 2-CSP. This comparison is, in
most cases, favourable to Frieze & Jerrum."
"Composition of weighted transducers is a fundamental algorithm used in many
applications, including for computing complex edit-distances between automata,
or string kernels in machine learning, or to combine different components of a
speech recognition, speech synthesis, or information extraction system. We
present a generalization of the composition of weighted transducers, 3-way
composition, which is dramatically faster in practice than the standard
composition algorithm when combining more than two transducers. The worst-case
complexity of our algorithm for composing three transducers $T_1$, $T_2$, and
$T_3$ resulting in $T$, \ignore{depending on the strategy used, is $O(|T|_Q
d(T_1) d(T_3) + |T|_E)$ or $(|T|_Q d(T_2) + |T|_E)$,} is $O(|T|_Q \min(d(T_1)
d(T_3), d(T_2)) + |T|_E)$, where $|\cdot|_Q$ denotes the number of states,
$|\cdot|_E$ the number of transitions, and $d(\cdot)$ the maximum out-degree.
As in regular composition, the use of perfect hashing requires a pre-processing
step with linear-time expected complexity in the size of the input transducers.
In many cases, this approach significantly improves on the complexity of
standard composition. Our algorithm also leads to a dramatically faster
composition in practice. Furthermore, standard composition can be obtained as a
special case of our algorithm. We report the results of several experiments
demonstrating this improvement. These theoretical and empirical improvements
significantly enhance performance in the applications already mentioned."
"We show via two different algorithms that finding the length of the longest
path in planar directed acyclic graph (DAG) is in unambiguous logspace UL, and
also in the complement class co-UL. The result extends to toroidal DAGs as
well."
"An (encoded) decision problem is a pair (E, F) where E=words that encode
instances of the problem, F=words to be accepted. We use ""strings"" in a
technical sense. With an NP problem (E, F) we associate the ""logogram"" of F
relative to E, which conveys structural information on E, F, and how F is
embedded in E. The kernel Ker(P) of a program P that solves (E, F) consists of
those strings in the logogram that are used by P. There are relations between
Ker(P) and the complexity of P. We develop an application to SAT that relies
upon a property of internal independence of SAT. We show that SAT cannot have
in its logogram strings serving as collective certificates. As consequence, all
programs that solve SAT have same kernel."
"We review the connection between statistical mechanics and the analysis of
random optimization problems, with particular emphasis on the random k-SAT
problem. We discuss and characterize the different phase transitions that are
met in these problems, starting from basic concepts. We also discuss how
statistical mechanics methods can be used to investigate the behavior of local
search and decimation based algorithms."
"We study the approximability of predicates on $k$ variables from a domain
$[q]$, and give a new sufficient condition for such predicates to be
approximation resistant under the Unique Games Conjecture. Specifically, we
show that a predicate $P$ is approximation resistant if there exists a balanced
pairwise independent distribution over $[q]^k$ whose support is contained in
the set of satisfying assignments to $P$."
"The main goal of this paper is to put some known results in a common
perspective and to simplify their proofs. We start with a simple proof of a
result from (Vereshchagin, 2002) saying that $\limsup_n\KS(x|n)$ (here
$\KS(x|n)$ is conditional (plain) Kolmogorov complexity of $x$ when $n$ is
known) equals $\KS^{\mathbf{0'}(x)$, the plain Kolmogorov complexity with
$\mathbf{0'$-oracle. Then we use the same argument to prove similar results for
prefix complexity (and also improve results of (Muchnik, 1987) about limit
frequencies), a priori probability on binary tree and measure of effectively
open sets. As a by-product, we get a criterion of $\mathbf{0'}$ Martin-L\""of
randomness (called also 2-randomness) proved in (Miller, 2004): a sequence
$\omega$ is 2-random if and only if there exists $c$ such that any prefix $x$
of $\omega$ is a prefix of some string $y$ such that $\KS(y)\ge |y|-c$. (In the
1960ies this property was suggested in (Kolmogorov, 1968) as one of possible
randomness definitions; its equivalence to 2-randomness was shown in (Miller,
2004) while proving another 2-randomness criterion (see also (Nies et al.
2005)): $\omega$ is 2-random if and only if $\KS(x)\ge |x|-c$ for some $c$ and
infinitely many prefixes $x$ of $\omega$. Finally, we show that the low-basis
theorem can be used to get alternative proofs for these results and to improve
the result about effectively open sets; this stronger version implies the
2-randomness criterion mentioned in the previous sentence."
"The computational function of a matchgate is represented by its character
matrix. In this article, we show that all nonsingular character matrices are
closed under matrix inverse operation, so that for every $k$, the nonsingular
character matrices of $k$-bit matchgates form a group, extending the recent
work of Cai and Choudhary (2006) of the same result for the case of $k=2$, and
that the single and the two-bit matchgates are universal for matchcircuits,
answering a question of Valiant (2002)."
"The purpose of this paper is to provide efficient algorithms that decide
membership for classes of several Boolean hierarchies for which efficiency (or
even decidability) were previously not known. We develop new forbidden-chain
characterizations for the single levels of these hierarchies and obtain the
following results: - The classes of the Boolean hierarchy over level $\Sigma_1$
of the dot-depth hierarchy are decidable in $NL$ (previously only the
decidability was known). The same remains true if predicates mod $d$ for fixed
$d$ are allowed. - If modular predicates for arbitrary $d$ are allowed, then
the classes of the Boolean hierarchy over level $\Sigma_1$ are decidable. - For
the restricted case of a two-letter alphabet, the classes of the Boolean
hierarchy over level $\Sigma_2$ of the Straubing-Th\'erien hierarchy are
decidable in $NL$. This is the first decidability result for this hierarchy. -
The membership problems for all mentioned Boolean-hierarchy classes are
logspace many-one hard for $NL$. - The membership problems for quasi-aperiodic
languages and for $d$-quasi-aperiodic languages are logspace many-one complete
for $PSPACE$."
"We study the succinctness of the complement and intersection of regular
expressions. In particular, we show that when constructing a regular expression
defining the complement of a given regular expression, a double exponential
size increase cannot be avoided. Similarly, when constructing a regular
expression defining the intersection of a fixed and an arbitrary number of
regular expressions, an exponential and double exponential size increase,
respectively, can in worst-case not be avoided. All mentioned lower bounds
improve the existing ones by one exponential and are tight in the sense that
the target expression can be constructed in the corresponding time class, i.e.,
exponential or double exponential time. As a by-product, we generalize a
theorem by Ehrenfeucht and Zeiger stating that there is a class of DFAs which
are exponentially more succinct than regular expressions, to a fixed
four-letter alphabet. When the given regular expressions are one-unambiguous,
as for instance required by the XML Schema specification, the complement can be
computed in polynomial time whereas the bounds concerning intersection continue
to hold. For the subclass of single-occurrence regular expressions, we prove a
tight exponential lower bound for intersection."
"This paper presents efficient algorithms for testing the finite, polynomial,
and exponential ambiguity of finite automata with $\epsilon$-transitions. It
gives an algorithm for testing the exponential ambiguity of an automaton $A$ in
time $O(|A|_E^2)$, and finite or polynomial ambiguity in time $O(|A|_E^3)$.
These complexities significantly improve over the previous best complexities
given for the same problem. Furthermore, the algorithms presented are simple
and are based on a general algorithm for the composition or intersection of
automata. We also give an algorithm to determine the degree of polynomial
ambiguity of a finite automaton $A$ that is polynomially ambiguous in time
$O(|A|_E^3)$. Finally, we present an application of our algorithms to an
approximate computation of the entropy of a probabilistic automaton."
"We provide a non-explicit separation of the number-on-forehead communication
complexity classes RP and NP when the number of players is up to \delta log(n)
for any \delta<1. Recent lower bounds on Set-Disjointness [LS08,CA08] provide
an explicit separation between these classes when the number of players is only
up to o(loglog(n))."
"We exhibit a polynomial time computable plane curve GAMMA that has finite
length, does not intersect itself, and is smooth except at one endpoint, but
has the following property. For every computable parametrization f of GAMMA and
every positive integer n, there is some positive-length subcurve of GAMMA that
f retraces at least n times. In contrast, every computable curve of finite
length that does not intersect itself has a constant-speed (hence
non-retracing) parametrization that is computable relative to the halting
problem."
"In this paper, we exhibit a strong relation between the sand automata
configuration space and the cellular automata configuration space. This
relation induces a compact topology for sand automata, and a new context in
which sand automata are homeomorphic to cellular automata acting on a specific
subshift. We show that the existing topological results for sand automata,
including the Hedlund-like representation theorem, still hold. In this context,
we give a characterization of the cellular automata which are sand automata,
and study some dynamical behaviors such as equicontinuity. Furthermore, we deal
with the nilpotency. We show that the classical definition is not meaningful
for sand automata. Then, we introduce a suitable new notion of nilpotency for
sand automata. Finally, we prove that this simple dynamical behavior is
undecidable."
"The behavior of some stochastic chemical reaction networks is largely
unaffected by slight inaccuracies in reaction rates. We formalize the
robustness of state probabilities to reaction rate deviations, and describe a
formal connection between robustness and efficiency of simulation. Without
robustness guarantees, stochastic simulation seems to require computational
time proportional to the total number of reaction events. Even if the
concentration (molecular count per volume) stays bounded, the number of
reaction events can be linear in the duration of simulated time and total
molecular count. We show that the behavior of robust systems can be predicted
such that the computational work scales linearly with the duration of simulated
time and concentration, and only polylogarithmically in the total molecular
count. Thus our asymptotic analysis captures the dramatic speedup when
molecular counts are large, and shows that for bounded concentrations the
computation time is essentially invariant with molecular count. Finally, by
noticing that even robust stochastic chemical reaction networks are capable of
embedding complex computational problems, we argue that the linear dependence
on simulated time and concentration is likely optimal."
"The tendency of semidefinite programs to compose perfectly under product has
been exploited many times in complexity theory: for example, by Lovasz to
determine the Shannon capacity of the pentagon; to show a direct sum theorem
for non-deterministic communication complexity and direct product theorems for
discrepancy; and in interactive proof systems to show parallel repetition
theorems for restricted classes of games.
  Despite all these examples of product theorems--some going back nearly thirty
years--it was only recently that Mittal and Szegedy began to develop a general
theory to explain when and why semidefinite programs behave perfectly under
product. This theory captured many examples in the literature, but there were
also some notable exceptions which it could not explain--namely, an early
parallel repetition result of Feige and Lovasz, and a direct product theorem
for the discrepancy method of communication complexity by Lee, Shraibman, and
Spalek.
  We extend the theory of Mittal and Szegedy to explain these cases as well.
Indeed, to the best of our knowledge, our theory captures all examples of
semidefinite product theorems in the literature."
"In this paper we show that the following problem is NP-complete: Given an
alphabet $\Sigma$ and two strings over $\Sigma$, the question is whether there
exists a permutation of $\Sigma$ which is a subsequence of both of the given
strings."
"We reprove that the approximate degree of the OR function on n bits is
Omega(sqrt(n)). We consider a linear program which is feasible if and only if
there is an approximate polynomial for a given function, and apply the duality
theory. The duality theory says that the primal program has no solution if and
only if its dual has a solution. Therefore one can prove the nonexistence of an
approximate polynomial by exhibiting a dual solution, coined the dual
polynomial. We construct such a polynomial."
"Motivated by the quantum algorithm in \cite{MN05} for testing commutativity
of black-box groups, we study the following problem: Given a black-box finite
ring $R=\angle{r_1,...,r_k}$ where $\{r_1,r_2,...,r_k\}$ is an additive
generating set for $R$ and a multilinear polynomial $f(x_1,...,x_m)$ over $R$
also accessed as a black-box function $f:R^m\to R$ (where we allow the
indeterminates $x_1,...,x_m$ to be commuting or noncommuting), we study the
problem of testing if $f$ is an \emph{identity} for the ring $R$. More
precisely, the problem is to test if $f(a_1,a_2,...,a_m)=0$ for all $a_i\in R$.
  We give a quantum algorithm with query complexity $O(m(1+\alpha)^{m/2}
k^{\frac{m}{m+1}})$ assuming $k\geq (1+1/\alpha)^{m+1}$. Towards a lower bound,
we also discuss a reduction from a version of $m$-collision to this problem.
  We also observe a randomized test with query complexity $4^mmk$ and constant
success probability and a deterministic test with $k^m$ query complexity."
"The 3-\textsc{Hitting Set} problem is also called the \textsc{Vertex Cover}
problem on 3-uniform hypergraphs. In this paper, we address kernelizations of
the \textsc{Vertex Cover} problem on 3-uniform hypergraphs. We show that this
problem admits a linear kernel in three classes of 3-uniform hypergraphs. We
also obtain lower and upper bounds on the kernel size for them by the
parametric duality."
"We develop theory concerning non-uniform complexity in a setting in which the
notion of single-pass instruction sequence considered in program algebra is the
central notion. We define counterparts of the complexity classes P/poly and
NP/poly and formulate a counterpart of the complexity theoretic conjecture that
NP is not included in P/poly. In addition, we define a notion of completeness
for the counterpart of NP/poly using a non-uniform reducibility relation and
formulate complexity hypotheses which concern restrictions on the instruction
sequences used for computation. We think that the theory developed opens up an
additional way of investigating issues concerning non-uniform complexity."
"Generalised Satisfiability Problems (or Boolean Constraint Satisfaction
Problems), introduced by Schaefer in 1978, are a general class of problem which
allow the systematic study of the complexity of satisfiability problems with
different types of constraints. In 1979, Valiant introduced the complexity
class parity P, the problem of counting the number of solutions to NP problems
modulo two. Others have since considered the question of counting modulo other
integers.
  We give a dichotomy theorem for the complexity of counting the number of
solutions to Generalised Satisfiability Problems modulo integers. This follows
from an earlier result of Creignou and Hermann which gave a counting dichotomy
for these types of problem, and the dichotomy itself is almost identical.
Specifically, counting the number of solutions to a Generalised Satisfiability
Problem can be done in polynomial time if all the relations are affine.
Otherwise, except for one special case with k = 2, it is #_kP-complete."
"One of the strongest techniques available for showing lower bounds on quantum
communication complexity is the logarithm of the approximation rank of the
communication matrix--the minimum rank of a matrix which is entrywise close to
the communication matrix. This technique has two main drawbacks: it is
difficult to compute, and it is not known to lower bound quantum communication
complexity with entanglement.
  Linial and Shraibman recently introduced a norm, called gamma_2^{alpha}, to
quantum communication complexity, showing that it can be used to lower bound
communication with entanglement. Here the parameter alpha is a measure of
approximation which is related to the allowable error probability of the
protocol. This bound can be written as a semidefinite program and gives bounds
at least as large as many techniques in the literature, although it is smaller
than the corresponding alpha-approximation rank, rk_alpha. We show that in fact
log gamma_2^{alpha}(A)$ and log rk_{alpha}(A)$ agree up to small factors. As
corollaries we obtain a constant factor polynomial time approximation algorithm
to the logarithm of approximate rank, and that the logarithm of approximation
rank is a lower bound for quantum communication complexity with entanglement."
"Graph Isomorphism is the prime example of a computational problem with a wide
difference between the best known lower and upper bounds on its complexity. We
bridge this gap for a natural and important special case, planar graph
isomorphism, by presenting an upper bound that matches the known logspace
hardness [Lindell'92]. In fact, we show the formally stronger result that
planar graph canonization is in logspace. This improves the previously known
upper bound of AC1 [MillerReif'91].
  Our algorithm first constructs the biconnected component tree of a connected
planar graph and then refines each biconnected component into a triconnected
component tree. The next step is to logspace reduce the biconnected planar
graph isomorphism and canonization problems to those for 3-connected planar
graphs, which are known to be in logspace by [DattaLimayeNimbhorkar'08]. This
is achieved by using the above decomposition, and by making significant
modifications to Lindell's algorithm for tree canonization, along with changes
in the space complexity analysis.
  The reduction from the connected case to the biconnected case requires
further new ideas, including a non-trivial case analysis and a group theoretic
lemma to bound the number of automorphisms of a colored 3-connected planar
graph. This lemma is crucial for the reduction to work in logspace."
"We prove that the directed graph reachability problem (transitive closure)
can be solved by monotone fan-in 2 boolean circuits of depth (1/2+o(1))(log
n)^2, where n is the number of nodes. This improves the previous known upper
bound (1+o(1))(log n)^2. The proof is non-constructive, but we give a
constructive proof of the upper bound (7/8+o(1))(log n)^2."
"The proof of Toda's celebrated theorem that the polynomial hierarchy is
contained in $\P^{# P}$ relies on the fact that, under mild technical
conditions on the complexity class $C$, we have $\exists C \subset BP \cdot
\oplus C$. More concretely, there is a randomized reduction which transforms
nonempty sets and the empty set, respectively, into sets of odd or even size.
The customary method is to invoke Valiant's and Vazirani's randomized reduction
from NP to UP, followed by amplification of the resulting success probability
from $1/\poly(n)$ to a constant by combining the parities of $\poly(n)$ trials.
Here we give a direct algebraic reduction which achieves constant success
probability without the need for amplification. Our reduction is very simple,
and its analysis relies on well-known properties of the Legendre symbol in
finite fields."
"We motivate and prove a strong pumping lemma for regular tree languages. The
new lemma can be seen as the natural correspondent of Ogden's lemma for
context-free string languages."
"Heisenberg's uncertainty principle states that it is not possible to compute
both the position and momentum of an electron with absolute certainty. However,
this computational limitation, which is central to quantum mechanics, has no
counterpart in theoretical computer science. Here, I will show that we can
distinguish between the complexity classes P and NP when we consider intrinsic
uncertainty in our computations, and take uncertainty about whether a bit
belongs to the program code or machine input into account. Given intrinsic
uncertainty, every output is uncertain, and computations become meaningful only
in combination with a confidence level. In particular, it is impossible to
compute solutions with absolute certainty as this requires infinite run-time.
Considering intrinsic uncertainty, I will present a function that is in NP but
not in P, and thus prove that P is a proper subset of NP. I will also show that
all traditional hard decision problems have polynomial-time algorithms that
provide solutions with confidence under uncertainty."
"In connection with machine arithmetic, we are interested in systems of
constraints of the form x + k \leq y + k'. Over integers, the satisfiability
problem for such systems is polynomial time. The problem becomes NP complete if
we restrict attention to the residues for a fixed modulus N."
"In this paper we consider a nondeterministic computation by deterministic
multi-head 2-way automata having a read-only access to an auxiliary memory. The
memory contains additional data (a guess) and computation is successful iff it
is successful for some memory content. Also we consider the case of restricted
guesses in which a guess should satisfy some constraint. We show that the
standard complexity classes such as L, NL, P, NP, PSPACE can be characterized
in terms of these models of nondeterministic computation. These
characterizations differ from the well-known ones by absence of alternation."
"We show that the rank of a depth-3 circuit (over any field) that is simple,
minimal and zero is at most k^3\log d. The previous best rank bound known was
2^{O(k^2)}(\log d)^{k-2} by Dvir and Shpilka (STOC 2005). This almost resolves
the rank question first posed by Dvir and Shpilka (as we also provide a simple
and minimal identity of rank \Omega(k\log d)).
  Our rank bound significantly improves (dependence on k exponentially reduced)
the best known deterministic black-box identity tests for depth-3 circuits by
Karnin and Shpilka (CCC 2008). Our techniques also shed light on the
factorization pattern of nonzero depth-3 circuits, most strikingly: the rank of
linear factors of a simple, minimal and nonzero depth-3 circuit (over any
field) is at most k^3\log d.
  The novel feature of this work is a new notion of maps between sets of linear
forms, called ""ideal matchings"", used to study depth-3 circuits. We prove
interesting structural results about depth-3 identities using these techniques.
We believe that these can lead to the goal of a deterministic polynomial time
identity test for these circuits."
"We study the complexity of testing if two given matroids are isomorphic. The
problem is easily seen to be in $\Sigma_2^p$. In the case of linear matroids,
which are represented over polynomially growing fields, we note that the
problem is unlikely to be $\Sigma_2^p$-complete and is $\co\NP$-hard. We show
that when the rank of the matroid is bounded by a constant, linear matroid
isomorphism, and matroid isomorphism are both polynomial time many-one
equivalent to graph isomorphism. We give a polynomial time Turing reduction
from graphic matroid isomorphism problem to the graph isomorphism problem.
Using this, we are able to show that graphic matroid isomorphism testing for
planar graphs can be done in deterministic polynomial time. We then give a
polynomial time many-one reduction from bounded rank matroid isomorphism
problem to graphic matroid isomorphism, thus showing that all the above
problems are polynomial time equivalent. Further, for linear and graphic
matroids, we prove that the automorphism problem is polynomial time equivalent
to the corresponding isomorphism problems. In addition, we give a polynomial
time membership test algorithm for the automorphism group of a graphic matroid."
"Muchnik's theorem about simple conditional descriprion states that for all
words $a$ and $b$ there exists a short program $p$ transforming $a$ to $b$ that
has the least possible length and is simple conditional on $b$. This paper
presents a new proof of this theorem, based on extractors. Employing the
extractor technique, two new versions of Muchnik's theorem for space- and
time-bounded Kolmogorov complexity are proven."
"In this note, we generalize the results of arXiv:0901.2703v1 We show that all
one-way quantum finite automaton (QFA) models that are at least as general as
Kondacs-Watrous QFA's are equivalent in power to classical probabilistic finite
automata in this setting. Unlike their probabilistic counterparts, allowing the
tape head to stay put for some steps during its traversal of the input does
enlarge the class of languages recognized by such QFA's with unbounded error.
(Note that, the proof of Theorem 1 in the abstract was presented in the
previous version (arXiv:0901.2703v1).)"
"We establish a connection between non-deterministic communication complexity
and instance complexity, a measure of information based on algorithmic entropy.
Let $\overline{x}$, $\overline{y}$ and $Y_1(\overline{x})$ be respectively the
input known by Alice, the input known by Bob, and the set of all values of $y$
such that $f(\overline{x},y)=1$; a string is a witness of the non-deterministic
communication protocol iff it is a program $p$ that ""corresponds exactly"" to
the instance complexity $\ic^{f,t}(\overline{y}:Y_1(\overline{x}))$."
"In this paper, we show that for every constant $0 < \epsilon < 1/2$ and for
every constant $d \geq 2$, the minimum size of a depth $d$ Boolean circuit that
$\epsilon$-approximates Majority function on $n$ variables is
exp$(\Theta(n^{1/(2d-2)}))$. The lower bound for every $d \geq 2$ and the upper
bound for $d=2$ have been previously shown by O'Donnell and Wimmer [ICALP'07],
and the contribution of this paper is to give a matching upper bound for $d
\geq 3$."
"Here we prove an asymptotically optimal lower bound on the information
complexity of the k-party disjointness function with the unique intersection
promise, an important special case of the well known disjointness problem, and
the ANDk-function in the number in the hand model. Our (n/k) bound for
disjointness improves on an earlier (n/(k log k)) bound by Chakrabarti et al.
(2003), who obtained an asymptotically tight lower bound for one-way protocols,
but failed to do so for the general case. Our result eliminates both the gap
between the upper and the lower bound for unrestricted protocols and the gap
between the lower bounds for one-way protocols and unrestricted protocols."
"It has been observed in many places that constant-factor approximable
problems often admit polynomial or even linear problem kernels for their
decision versions, e.g., Vertex Cover, Feedback Vertex Set, and Triangle
Packing. While there exist examples like Bin Packing, which does not admit any
kernel unless P = NP, there apparently is a strong relation between these two
polynomial-time techniques. We add to this picture by showing that the natural
decision versions of all problems in two prominent classes of constant-factor
approximable problems, namely MIN F^+\Pi_1 and MAX NP, admit polynomial problem
kernels. Problems in MAX SNP, a subclass of MAX NP, are shown to admit kernels
with a linear base set, e.g., the set of vertices of a graph. This extends
results of Cai and Chen (JCSS 1997), stating that the standard
parameterizations of problems in MAX SNP and MIN F^+\Pi_1 are fixed-parameter
tractable, and complements recent research on problems that do not admit
polynomial kernelizations (Bodlaender et al. JCSS 2009)."
"We show that the permanent cannot be computed by DLOGTIME-uniform threshold
or arithmetic circuits of depth o(log log n) and polynomial size."
"The nondeterministic quantum finite automaton (NQFA) is the only known case
where a one-way quantum finite automaton (QFA) model has been shown to be
strictly superior in terms of language recognition power to its probabilistic
counterpart. We give a characterization of the class of languages recognized by
NQFA's, demonstrating that it is equal to the class of exclusive stochastic
languages. We also characterize the class of languages that are recognized
necessarily by two-sided error by QFA's. It is shown that these classes remain
the same when the QFA's used in their definitions are replaced by several
different model variants that have appeared in the literature. We prove several
closure properties of the related classes. The ramifications of these results
about classical and quantum sublogarithmic space complexity classes are
examined."
"We introduce a new technique proving formula size lower bounds based on the
linear programming bound originally introduced by Karchmer, Kushilevitz and
Nisan [11] and the theory of stable set polytope. We apply it to majority
functions and prove their formula size lower bounds improved from the classical
result of Khrapchenko [13]. Moreover, we introduce a notion of unbalanced
recursive ternary majority functions motivated by a decomposition theory of
monotone self-dual functions and give integrally matching upper and lower
bounds of their formula size. We also show monotone formula size lower bounds
of balanced recursive ternary majority functions improved from the quantum
adversary bound of Laplante, Lee and Szegedy [15]."
"A dichotomy theorem for counting problems due to Creignou and Hermann states
that or any nite set S of logical relations, the counting problem #SAT(S) is
either in FP, or #P-complete. In the present paper we show a dichotomy theorem
for polynomial evaluation. That is, we show that for a given set S, either
there exists a VNP-complete family of polynomials associated to S, or the
associated families of polynomials are all in VP. We give a concise
characterization of the sets S that give rise to ""easy"" and ""hard"" polynomials.
We also prove that several problems which were known to be #P-complete under
Turing reductions only are in fact #P-complete under many-one reductions."
"This paper investigates the existence of inseparable disjoint pairs of NP
languages and related strong hypotheses in computational complexity. Our main
theorem says that, if NP does not have measure 0 in EXP, then there exist
disjoint pairs of NP languages that are P-inseparable, in fact
TIME(2^(n^k))-inseparable. We also relate these conditions to strong hypotheses
concerning randomness and genericity of disjoint pairs."
"We show that any distribution on {-1,1}^n that is k-wise independent fools
any halfspace h with error \eps for k = O(\log^2(1/\eps) /\eps^2). Up to
logarithmic factors, our result matches a lower bound by Benjamini,
Gurel-Gurevich, and Peled (2007) showing that k = \Omega(1/(\eps^2 \cdot
\log(1/\eps))). Using standard constructions of k-wise independent
distributions, we obtain the first explicit pseudorandom generators G: {-1,1}^s
--> {-1,1}^n that fool halfspaces. Specifically, we fool halfspaces with error
eps and seed length s = k \log n = O(\log n \cdot \log^2(1/\eps) /\eps^2).
  Our approach combines classical tools from real approximation theory with
structural results on halfspaces by Servedio (Computational Complexity 2007)."
This article has been withdrawn
"Presentation of a Method for determining whether a problem 3Sat has solution,
and if yes to find one, in time max O(n^15). Is thus proved that the problem
3Sat is fully resolved in polynomial time and therefore that it is in P, by the
work of Cook and Levin, and can transform a SAT problem in a 3Sat in polynomial
time (ref. Karp), it follows that P = NP. Open Source program is available at
http://www.visainformatica.it/3sat"
"Recent breakthroughs in quantum query complexity have shown that any formula
of size n can be evaluated with O(sqrt(n)log(n)/log log(n)) many quantum
queries in the bounded-error setting [FGG08, ACRSZ07, RS08b, Rei09]. In
particular, this gives an upper bound on the approximate polynomial degree of
formulas of the same magnitude, as approximate polynomial degree is a lower
bound on quantum query complexity [BBCMW01].
  These results essentially answer in the affirmative a conjecture of O'Donnell
and Servedio [O'DS03] that the sign degree--the minimal degree of a polynomial
that agrees in sign with a function on the Boolean cube--of every formula of
size n is O(sqrt(n)).
  In this note, we show that sign degree is super-multiplicative under function
composition. Combining this result with the above mentioned upper bounds on the
quantum query complexity of formulas allows the removal of logarithmic factors
to show that the sign degree of every size n formula is at most sqrt(n)."
"ACAC 2009 is organized by the Athens University of Economics and Business
(AUEB) and it is the fourth in a series of meetings that aim to bring together
researchers working on all areas of the theory of algorithms and computational
complexity. These meetings are expected to serve as a lively forum for
presenting results that are in a preliminary stage or have been recently
presented in some major conference. For the first time this year all submitted
papers were reviewed and ACAC also offered to the authors the choice of
publishing their contribution (provided it has not been published anywhere else
before) with the post-proceedings of EPTCS (Electronic Proceedings in
Theoretical Computer Science)."
"Consider the ""Number in Hand"" multiparty communication complexity model,
where k players holding inputs x_1,...,x_k in {0,1}^n communicate to compute
the value f(x_1,...,x_k) of a function f known to all of them. The main lower
bound technique for the communication complexity of such problems is that of
partition arguments: partition the k players into two disjoint sets of players
and find a lower bound for the induced two-party communication complexity
problem.
  In this paper, we study the power of partition arguments. Our two main
results are very different in nature: (i) For randomized communication
complexity, we show that partition arguments may yield bounds that are
exponentially far from the true communication complexity. Specifically, we
prove that there exists a 3-argument function f whose communication complexity
is Omega(n), while partition arguments can only yield an Omega(log n) lower
bound. The same holds for nondeterministic communication complexity. (ii) For
deterministic communication complexity, we prove that finding significant gaps
between the true communication complexity and the best lower bound that can be
obtained via partition arguments, would imply progress on a generalized version
of the ""log-rank conjecture"" in communication complexity.
  We conclude with two results on the multiparty ""fooling set technique"",
another method for obtaining communication complexity lower bounds."
"We separate monotone analogues of L and NL by proving that any monotone
switching network solving directed connectivity on $n$ vertices must have size
at least $n^(\Omega(\lg(n)))$."
"Recursive analysis was introduced by A. Turing [1936], A. Grzegorczyk [1955],
and D. Lacombe [1955]. It is based on a discrete mechanical framework that can
be used to model computation over the real numbers. In this context the
computational complexity of real functions defined over compact domains has
been extensively studied. However, much less have been done for other kinds of
real functions. This article is divided into two main parts. The first part
investigates polynomial time computability of rational functions and the role
of continuity in such computation. On the one hand this is interesting for its
own sake. On the other hand it provides insights into polynomial time
computability of real functions for the latter, in the sense of recursive
analysis, is modeled as approximations of rational computations. The main
conclusion of this part is that continuity does not play any role in the
efficiency of computing rational functions. The second part defines polynomial
time computability of arbitrary real functions, characterizes it, and compares
it with the corresponding notion over rational functions. Assuming continuity,
the main conclusion is that there is a conceptual difference between polynomial
time computation over the rationals and the reals manifested by the fact that
there are polynomial time computable rational functions whose extensions to the
reals are not polynomial time computable and vice versa."
"Let x be a random vector coming from any k-wise independent distribution over
{-1,1}^n. For an n-variate degree-2 polynomial p, we prove that E[sgn(p(x))] is
determined up to an additive epsilon for k = poly(1/epsilon). This answers an
open question of Diakonikolas et al. (FOCS 2009). Using standard constructions
of k-wise independent distributions, we obtain a broad class of explicit
generators that epsilon-fool the class of degree-2 threshold functions with
seed length log(n)*poly(1/epsilon).
  Our approach is quite robust: it easily extends to yield that the
intersection of any constant number of degree-2 threshold functions is
epsilon-fooled by poly(1/epsilon)-wise independence. Our results also hold if
the entries of x are k-wise independent standard normals, implying for example
that bounded independence derandomizes the Goemans-Williamson hyperplane
rounding scheme.
  To achieve our results, we introduce a technique we dub multivariate
FT-mollification, a generalization of the univariate form introduced by Kane et
al. (SODA 2010) in the context of streaming algorithms. Along the way we prove
a generalized hypercontractive inequality for quadratic forms which takes the
operator norm of the associated matrix into account. These techniques may be of
independent interest."
"We study the computational power of polynomial threshold functions, that is,
threshold functions of real polynomials over the boolean cube. We provide two
new results bounding the computational power of this model.
  Our first result shows that low-degree polynomial threshold functions cannot
approximate any function with many influential variables. We provide a couple
of examples where this technique yields tight approximation bounds.
  Our second result relates to constructing pseudorandom generators fooling
low-degree polynomial threshold functions. This problem has received attention
recently, where Diakonikolas et al proved that $k$-wise independence suffices
to fool linear threshold functions. We prove that any low-degree polynomial
threshold function, which can be represented as a function of a small number of
linear threshold functions, can also be fooled by $k$-wise independence. We
view this as an important step towards fooling general polynomial threshold
functions, and we discuss a plausible approach achieving this goal based on our
techniques.
  Our results combine tools from real approximation theory, hyper-contractive
inequalities and probabilistic methods. In particular, we develop several new
tools in approximation theory which may be of independent interest."
"To make a joint decision, agents (or voters) are often required to provide
their preferences as linear orders. To determine a winner, the given linear
orders can be aggregated according to a voting protocol. However, in realistic
settings, the voters may often only provide partial orders. This directly leads
to the Possible Winner problem that asks, given a set of partial votes, whether
a distinguished candidate can still become a winner. In this work, we consider
the computational complexity of Possible Winner for the broad class of voting
protocols defined by scoring rules. A scoring rule provides a score value for
every position which a candidate can have in a linear order. Prominent examples
include plurality, k-approval, and Borda. Generalizing previous NP-hardness
results for some special cases, we settle the computational complexity for all
but one scoring rule. More precisely, for an unbounded number of candidates and
unweighted voters, we show that Possible Winner is NP-complete for all pure
scoring rules except plurality, veto, and the scoring rule defined by the
scoring vector (2,1,...,1,0), while it is solvable in polynomial time for
plurality and veto."
"We investigate the power of Algebraic Branching Programs (ABPs) augmented
with help polynomials, and constant-depth Boolean circuits augmented with help
functions. We relate the problem of proving explicit lower bounds in both these
models to the Remote Point Problem (introduced by Alon, Panigrahy, and Yekhanin
(RANDOM '09)). More precisely, proving lower bounds for ABPs with help
polynomials is related to the Remote Point Problem w.r.t. the rank metric, and
for constant-depth circuits with help functions it is related to the Remote
Point Problem w.r.t. the Hamming metric. For algebraic branching programs with
help polynomials with some degree restrictions we show exponential size lower
bounds for explicit polynomials."
"Motivated by problems of comparative genomics and paleogenomics, in [Chauve
et al., 2009], the authors introduced the Gapped Consecutive-Ones Property
Problem (k,delta)-C1P: given a binary matrix M and two integers k and delta,
can the columns of M be permuted such that each row contains at most k blocks
of ones and no two consecutive blocks of ones are separated by a gap of more
than delta zeros. The classical C1P problem, which is known to be polynomial is
equivalent to the (1,0)-C1P problem. They showed that the (2,delta)-C1P Problem
is NP-complete for all delta >= 2 and that the (3,1)-C1P problem is
NP-complete. They also conjectured that the (k,delta)-C1P Problem is
NP-complete for k >= 2, delta >= 1 and (k,delta) =/= (2,1). Here, we prove that
this conjecture is true. The only remaining case is the (2,1)-C1P Problem,
which could be polynomial-time solvable."
"We present a general method for converting any family of unsatisfiable CNF
formulas that is hard for one of the simplest proof systems, tree resolution,
into formulas that require large rank in any proof system that manipulates
polynomials or polynomial threshold functions of degree at most k (known as
Th(k) proofs). Such systems include Lovasz-Schrijver and Cutting Planes proof
systems as well as their high degree analogues.
  These are based on analyzing two new proof systems, denoted by T^cc(k) and
R^cc(k). The proof lines of T^cc(k) are arbitrary Boolean functions, each of
which can be evaluated by an efficient k-party randomized communication
protocol. They include Th{k-1} proofs as a special case. R^cc(k) proofs are
stronger and only require that each inference be locally checkable by an
efficient k-party randomized communication protocol.
  Our main results are the following:
  (1) When k is O(loglogn), for any unsatisfiable CNF formula F requiring
resolution rank r, there is a related CNF formula G=Lift_k(F) requiring
refutation rank r^Omega(1/k) log^O(1) n in all R^cc(k) systems.
  (2) There are strict hierarchies for T^cc(k) and R^cc(k) systems with respect
to k when k is O(loglogn in that there are unsatisfiable CNF formulas requiring
large rank R^cc(k) refutations but having log^O(1) n rank Th(k) refutations.
  (3) When k is O(loglogn) there are 2^(n^Omega(1/k)) lower bounds on the size
of tree-like T^cc(k) refutations for large classes of lifted CNF formulas.
  (4) A general method for producing integrality gaps for low rank R^cc(2)
inference (and hence Cutting Planes and Th(1) inference) based on related gaps
for low rank resolution. These gaps are optimal for MAX-2t-SAT."
"In this work we offer a significant improvement on the previous smallest
spiking neural P systems and solve the problem of finding the smallest possible
extended spiking neural P system. Paun and Paun gave a universal spiking neural
P system with 84 neurons and another that has extended rules with 49 neurons.
Subsequently, Zhang et al. reduced the number of neurons used to give
universality to 67 for spiking neural P systems and to 41 for the extended
model. Here we give a small universal spiking neural P system that has only 17
neurons and another that has extended rules with 5 neurons. All of the above
mentioned spiking neural P systems suffer from an exponential slow down when
simulating Turing machines. Using a more relaxed encoding technique we get a
universal spiking neural P system that has extended rules with only 4 neurons.
This latter spiking neural P system simulates 2-counter machines in linear time
and thus suffer from a double exponential time overhead when simulating Turing
machines. We show that extended spiking neural P systems with 3 neurons are
simulated by log-space bounded Turing machines, and so there exists no such
universal system with 3 neurons. It immediately follows that our 4-neuron
system is the smallest possible extended spiking neural P system that is
universal. Finally, we show that if we generalise the output technique we can
give a universal spiking neural P system with extended rules that has only 3
neurons. This system is also the smallest of its kind as a universal spiking
neural P system with extended rules and generalised output is not possible with
2 neurons."
"In this paper we will be concerned with a class of packing and covering
problems which includes Vertex Cover and Independent Set. Typically, one can
write an LP relaxation and then round the solution. In this paper, we explain
why the simple LP-based rounding algorithm for the \\VC problem is optimal
assuming the UGC. Complementing Raghavendra's result, our result generalizes to
a class of strict, covering/packing type CSPs."
"In this paper we study polynomial identity testing of sums of $k$ read-once
algebraic branching programs ($\Sigma_k$-RO-ABPs), generalizing the work in
(Shpilka and Volkovich 2008,2009), who considered sums of $k$ read-once
formulas ($\Sigma_k$-RO-formulas). We show that $\Sigma_k$-RO-ABPs are strictly
more powerful than $\Sigma_k$-RO-formulas, for any $k \leq \lfloor n/2\rfloor$,
where $n$ is the number of variables. We obtain the following results:
  1) Given free access to the RO-ABPs in the sum, we get a deterministic
algorithm that runs in time $O(k^2n^7s) + n^{O(k)}$, where $s$ bounds the size
of any largest RO-ABP given on the input. This implies we have a deterministic
polynomial time algorithm for testing whether the sum of a constant number of
RO-ABPs computes the zero polynomial.
  2) Given black-box access to the RO-ABPs computing the individual polynomials
in the sum, we get a deterministic algorithm that runs in time $k^2n^{O(\log
n)} + n^{O(k)}$.
  3) Finally, given only black-box access to the polynomial computed by the sum
of the $k$ RO-ABPs, we obtain an $n^{O(k + \log n)}$ time deterministic
algorithm."
"The multivariate resultant is a fundamental tool of computational algebraic
geometry. It can in particular be used to decide whether a system of n
homogeneous equations in n variables is satisfiable (the resultant is a
polynomial in the system's coefficients which vanishes if and only if the
system is satisfiable). In this paper we present several NP-hardness results
for testing whether a multivariate resultant vanishes, or equivalently for
deciding whether a square system of homogeneous equations is satisfiable. Our
main result is that testing the resultant for zero is NP-hard under
deterministic reductions in any characteristic, for systems of low-degree
polynomials with coefficients in the ground field (rather than in an
extension). We also observe that in characteristic zero, this problem is in the
Arthur-Merlin class AM if the generalized Riemann hypothesis holds true. In
positive characteristic, the best upper bound remains PSPACE."
"In this paper we show that BPP is truth-table reducible to the set of
Kolmogorov random strings R_K. It was previously known that PSPACE, and hence
BPP is Turing-reducible to R_K. The earlier proof relied on the adaptivity of
the Turing-reduction to find a Kolmogorov-random string of polynomial length
using the set R_K as oracle. Our new non-adaptive result relies on a new
fundamental fact about the set R_K, namely each initial segment of the
characteristic sequence of R_K is not compressible by recursive means. As a
partial converse to our claim we show that strings of high
Kolmogorov-complexity when used as advice are not much more useful than
randomly chosen strings."
"In the paper, we consider several types of queries for classical and new
problems of learning and testing read-once functions. In several cases, the
border between polynomial and exponential complexities is obtained."
"We reprove a result of Boppana and Lagarias: If Pi_2^P is different from
Sigma_2^P then there exists a partial function f that is computable by a
polynomial-size family of circuits, but no inverse of f is computable by a
polynomial-size family of circuits. We strengthen this result by showing that
there exist length-preserving total functions that are one-way by circuit size
and that are computable in uniform polynomial time. We also prove, if Pi_2^P is
different from Sigma_2^P, that there exist polynomially balanced total
surjective functions that are one-way by circuit size; here non-uniformity is
used."
"We completely classify the computational complexity of the list H-colouring
problem for graphs (with possible loops) in combinatorial and algebraic terms:
for every graph H the problem is either NP-complete, NL-complete, L-complete or
is first-order definable; descriptive complexity equivalents are given as well
via Datalog and its fragments. Our algebraic characterisations match important
conjectures in the study of constraint satisfaction problems."
"Reachability and shortest path problems are NL-complete for general graphs.
They are known to be in L for graphs of tree-width 2 [JT07]. However, for
graphs of tree-width larger than 2, no bound better than NL is known. In this
paper, we improve these bounds for k-trees, where k is a constant. In
particular, the main results of our paper are log-space algorithms for
reachability in directed k-trees, and for computation of shortest and longest
paths in directed acyclic k-trees.
  Besides the path problems mentioned above, we also consider the problem of
deciding whether a k-tree has a perfect macthing (decision version), and if so,
finding a perfect match- ing (search version), and prove that these two
problems are L-complete. These problems are known to be in P and in RNC for
general graphs, and in SPL for planar bipartite graphs [DKR08].
  Our results settle the complexity of these problems for the class of k-trees.
The results are also applicable for bounded tree-width graphs, when a
tree-decomposition is given as input. The technique central to our algorithms
is a careful implementation of divide-and-conquer approach in log-space, along
with some ideas from [JT07] and [LMR07]."
"In comparative genomic, the first step of sequence analysis is usually to
decompose two or more genomes into syntenic blocks that are segments of
homologous chromosomes. For the reliable recovery of syntenic blocks, noise and
ambiguities in the genomic maps need to be removed first. Maximal Strip
Recovery (MSR) is an optimization problem proposed by Zheng, Zhu, and Sankoff
for reliably recovering syntenic blocks from genomic maps in the midst of noise
and ambiguities. Given $d$ genomic maps as sequences of gene markers, the
objective of \msr{d} is to find $d$ subsequences, one subsequence of each
genomic map, such that the total length of syntenic blocks in these
subsequences is maximized. For any constant $d \ge 2$, a polynomial-time
2d-approximation for \msr{d} was previously known. In this paper, we show that
for any $d \ge 2$, \msr{d} is APX-hard, even for the most basic version of the
problem in which all gene markers are distinct and appear in positive
orientation in each genomic map. Moreover, we provide the first explicit lower
bounds on approximating \msr{d} for all $d \ge 2$. In particular, we show that
\msr{d} is NP-hard to approximate within $\Omega(d/\log d)$. From the other
direction, we show that the previous 2d-approximation for \msr{d} can be
optimized into a polynomial-time algorithm even if $d$ is not a constant but is
part of the input. We then extend our inapproximability results to several
related problems including \cmsr{d}, \gapmsr{\delta}{d}, and
\gapcmsr{\delta}{d}."
"Gap Hamming Distance is a well-studied problem in communication complexity,
in which Alice and Bob have to decide whether the Hamming distance between
their respective n-bit inputs is less than n/2-sqrt(n) or greater than
n/2+sqrt(n). We show that every k-round bounded-error communication protocol
for this problem sends a message of at least Omega(n/(k^2\log k)) bits. This
lower bound has an exponentially better dependence on the number of rounds than
the previous best bound, due to Brody and Chakrabarti. Our communication lower
bound implies strong space lower bounds on algorithms for a number of data
stream computations, such as approximating the number of distinct elements in a
stream.
  Subsequent to this result, the bound has been improved by some of us to the
optimal Omega(n), independent of k, by using different techniques."
"In this paper we study algebraic branching programs (ABPs) with restrictions
on the order and the number of reads of variables in the program. Given a
permutation $\pi$ of $n$ variables, for a $\pi$-ordered ABP ($\pi$-OABP), for
any directed path $p$ from source to sink, a variable can appear at most once
on $p$, and the order in which variables appear on $p$ must respect $\pi$. An
ABP $A$ is said to be of read $r$, if any variable appears at most $r$ times in
$A$. Our main result pertains to the identity testing problem. Over any field
$F$ and in the black-box model, i.e. given only query access to the polynomial,
we have the following result: read $r$ $\pi$-OABP computable polynomials can be
tested in $\DTIME[2^{O(r\log r \cdot \log^2 n \log\log n)}]$.
  Our next set of results investigates the computational limitations of OABPs.
It is shown that any OABP computing the determinant or permanent requires size
$\Omega(2^n/n)$ and read $\Omega(2^n/n^2)$. We give a multilinear polynomial
$p$ in $2n+1$ variables over some specifically selected field $G$, such that
any OABP computing $p$ must read some variable at least $2^n$ times. We show
that the elementary symmetric polynomial of degree $r$ in $n$ variables can be
computed by a size $O(rn)$ read $r$ OABP, but not by a read $(r-1)$ OABP, for
any $0 < 2r-1 \leq n$. Finally, we give an example of a polynomial $p$ and two
variables orders $\pi \neq \pi'$, such that $p$ can be computed by a read-once
$\pi$-OABP, but where any $\pi'$-OABP computing $p$ must read some variable at
least $2^n$"
"A PCP is a proof system for NP in which the proof can be checked by a
probabilistic verifier. The verifier is only allowed to read a very small
portion of the proof, and in return is allowed to err with some bounded
probability. The probability that the verifier accepts a false proof is called
the soundness error, and is an important parameter of a PCP system that one
seeks to minimize. Constructing PCPs with sub-constant soundness error and, at
the same time, a minimal number of queries into the proof (namely two) is
especially important due to applications for inapproximability.
  In this work we construct such PCP verifiers, i.e., PCPs that make only two
queries and have sub-constant soundness error. Our construction can be viewed
as a combinatorial alternative to the ""manifold vs. point"" construction, which
is the only construction in the literature for this parameter range. The
""manifold vs. point"" PCP is based on a low degree test, while our construction
is based on a direct product test. We also extend our construction to yield a
decodable PCP (dPCP) with the same parameters. By plugging in this dPCP into
the scheme of Dinur and Harsha (FOCS 2009) one gets an alternative construction
of the result of Moshkovitz and Raz (FOCS 2008), namely: a construction of
two-query PCPs with small soundness error and small alphabet size.
  Our construction of a PCP is based on extending the derandomized direct
product test of Impagliazzo, Kabanets and Wigderson (STOC 09) to a derandomized
parallel repetition theorem. More accurately, our PCP construction is obtained
in two steps. We first prove a derandomized parallel repetition theorem for
specially structured PCPs. Then, we show that any PCP can be transformed into
one that has the required structure, by embedding it on a de-Bruijn graph."
"The problems studied in this article originate from the Graph Motif problem
introduced by Lacroix et al. in the context of biological networks. The problem
is to decide if a vertex-colored graph has a connected subgraph whose colors
equal a given multiset of colors $M$. It is a graph pattern-matching problem
variant, where the structure of the occurrence of the pattern is not of
interest but the only requirement is the connectedness. Using an algebraic
framework recently introduced by Koutis et al., we obtain new FPT algorithms
for Graph Motif and variants, with improved running times. We also obtain
results on the counting versions of this problem, proving that the counting
problem is FPT if M is a set, but becomes W[1]-hard if M is a multiset with two
colors. Finally, we present an experimental evaluation of this approach on real
datasets, showing that its performance compares favorably with existing
software."
"Our primary motivation is the comparison of two different traditions used in
ICC to characterize the class FPTIME of the polynomial time computable
functions. On one side, FPTIME can be captured by Intuitionistic Light Affine
Logic (ILAL), a logic derived from Linear Logic, characterized by the
structural invariant Stratification. On the other side, FPTIME can be captured
by Safe Recursion on Notation (SRN), an algebra of functions based on
Predicative Recursion, a restriction of the standard recursion schema used to
defiine primitive recursive functions. Stratifiication and Predicative
Recursion seem to share common underlying principles, whose study is the main
subject of this work."
"We introduce a 2-round stochastic constraint-satisfaction problem, and show
that its approximation version is complete for (the promise version of) the
complexity class AM. This gives a `PCP characterization' of AM analogous to the
PCP Theorem for NP. Similar characterizations have been given for higher levels
of the Polynomial Hierarchy, and for PSPACE; however, we suggest that the
result for AM might be of particular significance for attempts to derandomize
this class.
  To test this notion, we pose some `Randomized Optimization Hypotheses'
related to our stochastic CSPs that (in light of our result) would imply
collapse results for AM. Unfortunately, the hypotheses appear over-strong, and
we present evidence against them. In the process we show that, if some language
in NP is hard-on-average against circuits of size 2^{Omega(n)}, then there
exist hard-on-average optimization problems of a particularly elegant form.
  All our proofs use a powerful form of PCPs known as Probabilistically
Checkable Proofs of Proximity, and demonstrate their versatility. We also use
known results on randomness-efficient soundness- and hardness-amplification. In
particular, we make essential use of the Impagliazzo-Wigderson generator; our
analysis relies on a recent Chernoff-type theorem for expander walks."
"This paper provides a bridge between the classical tiling theory and the
complex neighborhood self-assembling situations that exist in practice. The
neighborhood of a position in the plane is the set of coordinates which are
considered adjacent to it. This includes classical neighborhoods of size four,
as well as arbitrarily complex neighborhoods. A generalized tile system
consists of a set of tiles, a neighborhood, and a relation which dictates which
are the ""admissible"" neighboring tiles of a given tile. Thus, in correctly
formed assemblies, tiles are assigned positions of the plane in accordance to
this relation. We prove that any validly tiled path defined in a given but
arbitrary neighborhood (a zipper) can be simulated by a simple ""ribbon"" of
microtiles. A ribbon is a special kind of polyomino, consisting of a
non-self-crossing sequence of tiles on the plane, in which successive tiles
stick along their adjacent edge. Finally, we extend this construction to the
case of traditional tilings, proving that we can simulate
arbitrary-neighborhood tilings by simple-neighborhood tilings, while preserving
some of their essential properties."
"A pseudo-primitive word with respect to an antimorphic involution \theta is a
word which cannot be written as a catenation of occurrences of a strictly
shorter word t and \theta(t). Properties of pseudo-primitive words are
investigated in this paper. These properties link pseudo-primitive words with
essential notions in combinatorics on words such as primitive words,
(pseudo)-palindromes, and (pseudo)-commutativity. Their applications include an
improved solution to the extended Lyndon-Sch\""utzenberger equation u_1 u_2 ...
u_l = v_1 ... v_n w_1 ... w_m, where u_1, ..., u_l \in {u, \theta(u)}, v_1,
..., v_n \in {v, \theta(v)}, and w_1, ..., w_m \in {w, \theata(w)} for some
words u, v, w, integers l, n, m \ge 2, and an antimorphic involution \theta. We
prove that for l \ge 4, n,m \ge 3, this equation implies that u, v, w can be
expressed in terms of a common word t and its image \theta(t). Moreover,
several cases of this equation where l = 3 are examined."
"We study restricted computation models related to the Tree Evaluation
Problem}. The TEP was introduced in earlier work as a simple candidate for the
(*very*) long term goal of separating L and LogDCFL. The input to the problem
is a rooted, balanced binary tree of height h, whose internal nodes are labeled
with binary functions on [k] = {1,...,k} (each given simply as a list of k^2
elements of [k]), and whose leaves are labeled with elements of [k]. Each node
obtains a value in [k] equal to its binary function applied to the values of
its children, and the output is the value of the root. The first restricted
computation model, called Fractional Pebbling, is a generalization of the
black/white pebbling game on graphs, and arises in a natural way from the
search for good upper bounds on the size of nondeterministic branching programs
(BPs) solving the TEP - for any fixed h, if the binary tree of height h has
fractional pebbling cost at most p, then there are nondeterministic BPs of size
O(k^p) solving the height h TEP. We prove a lower bound on the fractional
pebbling cost of d-ary trees that is tight to within an additive constant for
each fixed d. The second restricted computation model we study is a semantic
restriction on (non)deterministic BPs solving the TEP - Thrifty BPs.
Deterministic (resp. nondeterministic) thrifty BPs suffice to implement the
best known algorithms for the TEP, based on black (resp. fractional) pebbling.
In earlier work, for each fixed h a lower bound on the size of deterministic
thrifty BPs was proved that is tight for sufficiently large k. We give an
alternative proof that achieves the same bound for all k. We show the same
bound still holds in a less-restricted model, and also that gradually weaker
lower bounds can be obtained for gradually weaker restrictions on the model."
"One of my recent papers transforms an NP-Complete problem into the question
of whether or not a feasible real solution exists to some Linear Program. The
unique feature of this Linear Program is that though there is no explicit bound
on the minimum required number of linear inequalities, which is most probably
exponential to the size of the NP-Complete problem, the Linear Program can
still be described efficiently. The reason for this efficient description is
that coefficients keep repeating in some pattern, even as the number of
inequalities is conveniently assumed to tend to Infinity. I discuss why this
convenient assumption does not change the feasibility result of the Linear
Program. I conclude with two Conjectures, which might help to make an efficient
decision on the feasibility of this Linear Program."
"In this paper we define a restricted version of Monotone NAE-3SAT and show
that it remains NP-Complete even under that restriction. We expect this result
would be useful in proving NP-Completeness results for problems on
$k$-colourable graphs ($k \ge 5$). We also prove the NP-Completeness of the
Triangle-Free Cut problem."
"Bulatov (2008) gave a dichotomy for the counting constraint satisfaction
problem #CSP. A problem from #CSP is characterised by a constraint language,
which is a fixed, finite set of relations over a finite domain D. An instance
of the problem uses these relations to constrain the variables in a larger set.
Bulatov showed that the problem of counting the satisfying assignments of
instances of any problem from #CSP is either in polynomial time (FP) or is
#P-complete. His proof draws heavily on techniques from universal algebra and
cannot be understood without a secure grasp of that field. We give an
elementary proof of Bulatov's dichotomy, based on succinct representations,
which we call frames, of a class of highly structured relations, which we call
strongly rectangular. We show that these are precisely the relations which are
invariant under a Mal'tsev polymorphism. En route, we give a simplification of
a decision algorithm for strongly rectangular constraint languages, due to
Bulatov and Dalmau (2006). We establish a new criterion for the #CSP dichotomy,
which we call strong balance, and we prove that this property is decidable. In
fact, we establish membership in NP. Thus, we show that the dichotomy is
effective, resolving the most important open question concerning the #CSP
dichotomy."
"We study deterministic extractors for oblivious bit-fixing sources (a.k.a.
resilient functions) and exposure-resilient functions with small min-entropy:
of the function's n input bits, k << n bits are uniformly random and unknown to
the adversary. We simplify and improve an explicit construction of extractors
for bit-fixing sources with sublogarithmic k due to Kamp and Zuckerman (SICOMP
2006), achieving error exponentially small in k rather than polynomially small
in k. Our main result is that when k is sublogarithmic in n, the short output
length of this construction (O(log k) output bits) is optimal for extractors
computable by a large class of space-bounded streaming algorithms.
  Next, we show that a random function is an extractor for oblivious bit-fixing
sources with high probability if and only if k is superlogarithmic in n,
suggesting that our main result may apply more generally. In contrast, we show
that a random function is a static (resp. adaptive) exposure-resilient function
with high probability even if k is as small as a constant (resp. log log n). No
explicit exposure-resilient functions achieving these parameters are known."
"The direct product problem is a fundamental question in complexity theory
which seeks to understand how the difficulty of computing a function on each of
k independent inputs scales with k. We prove the following direct product
theorem (DPT) for query complexity: if every T-query algorithm has success
probability at most 1 - eps in computing the Boolean function f on input
distribution Mu, then for alpha <= 1, every (alpha eps Tk)-query algorithm has
success probability at most (2^{alpha eps}(1 - eps))^k in computing the k-fold
direct product f^k correctly on k independent inputs from Mu. In light of
examples due to Shaltiel, this statement gives an essentially optimal tradeoff
between the query bound and the error probability. As a corollary, we show that
for an absolute constant alpha > 0, the worst-case success probability of any
(alpha R_2(f)k)-query randomized algorithm for f^k falls exponentially with k.
The best previous statement of this type, due to Klauck, Spalek, and de Wolf,
required a query bound of O(bs(f)k). The proof involves defining and analyzing
a collection of martingales associated with an algorithm attempting to solve
f^k. Our method is quite general and yields a new XOR lemma and threshold DPT
for the query model, as well as DPTs for the query complexity of learning
tasks, search problems, and tasks involving interaction with dyamic entities.
We also give a version of our DPT in which decision tree size is the resource
of interest."
"A completion of an m-by-n matrix A with entries in {0,1,*} is obtained by
setting all *-entries to constants 0 or 1. A system of semi-linear equations
over GF(2) has the form Mx=f(x), where M is a completion of A and f:{0,1}^n -->
{0,1}^m is an operator, the i-th coordinate of which can only depend on
variables corresponding to *-entries in the i-th row of A. We conjecture that
no such system can have more than 2^{n-c\cdot mr(A)} solutions, where c>0 is an
absolute constant and mr(A) is the smallest rank over GF(2) of a completion of
A. The conjecture is related to an old problem of proving super-linear lower
bounds on the size of log-depth boolean circuits computing linear operators x
--> Mx. The conjecture is also a generalization of a classical question about
how much larger can non-linear codes be than linear ones. We prove some special
cases of the conjecture and establish some structural properties of solution
sets."
"This is a short introduction to Kolmogorov Complexity. The interested reader
is referred to the text books by Cover & Thomas as well as Li & V\'itanyi,
which cover the fields of information theory and Kolmogorov complexity in depth
and with all the necessary rigor."
"We consider the problem of evaluating certain exponential sums. These sums
take the form $\sum_{x_1,...,x_n \in Z_N} e^{f(x_1,...,x_n) {2 \pi i / N}} $,
where each x_i is summed over a ring Z_N, and f(x_1,...,x_n) is a multivariate
polynomial with integer coefficients. We show that the sum can be evaluated in
polynomial time in n and log N when f is a quadratic polynomial. This is true
even when the factorization of N is unknown. Previously, this was known for a
prime modulus N. On the other hand, for very specific families of polynomials
of degree \ge 3, we show the problem is #P-hard, even for any fixed prime or
prime power modulus. This leads to a complexity dichotomy theorem - a complete
classification of each problem to be either computable in polynomial time or
#P-hard - for a class of exponential sums. These sums arise in the
classifications of graph homomorphisms and some other counting CSP type
problems, and these results lead to complexity dichotomy theorems. For the
polynomial-time algorithm, Gauss sums form the basic building blocks. For the
hardness results, we prove group-theoretic necessary conditions for
tractability. These tests imply that the problem is #P-hard for even very
restricted families of simple cubic polynomials over fixed modulus N."
"We introduce the Tree Evaluation Problem, show that it is in logDCFL (and
hence in P), and study its branching program complexity in the hope of
eventually proving a superlogarithmic space lower bound. The input to the
problem is a rooted, balanced d-ary tree of height h, whose internal nodes are
labeled with d-ary functions on [k] = {1,...,k}, and whose leaves are labeled
with elements of [k]. Each node obtains a value in [k] equal to its d-ary
function applied to the values of its d children. The output is the value of
the root. We show that the standard black pebbling algorithm applied to the
binary tree of height h yields a deterministic k-way branching program with
Theta(k^h) states solving this problem, and we prove that this upper bound is
tight for h=2 and h=3. We introduce a simple semantic restriction called
""thrifty"" on k-way branching programs solving tree evaluation problems and show
that the same state bound of Theta(k^h) is tight (up to a constant factor) for
all h >= 2 for deterministic thrifty programs. We introduce fractional pebbling
for trees and show that this yields nondeterministic thrifty programs with
Theta(k^{h/2+1}) states solving the Boolean problem ""determine whether the root
has value 1"". We prove that this bound is tight for h=2,3,4, and tight for
unrestricted nondeterministic k-way branching programs for h=2,3."
"We give some reductions among problems in (nonnegative) weighted #CSP which
restrict the class of functions that needs to be considered in computational
complexity studies. Our reductions can be applied to both exact and approximate
computation. In particular, we show that a recent dichotomy for unweighted #CSP
can be extended to rational-weighted #CSP."
"The \textbf{P} =? \textbf{NP} problem is an important problem in contemporary
mathematics and theoretical computer science. Many proofs have been proposed to
this problem. This paper proposes a theoretic proof for \textbf{P} =?
\textbf{NP} problem. The central idea of this proof is a recursive definition
for Turing machine (shortly TM) that accepts the encoding strings of valid TMs.
By the definition, an infinite sequence of TM is constructed, and it is proven
that the sequence includes all valid TMs. Based on these TMs, the class
\textbf{D} that includes all decidable languages is defined. By proving
\textbf{P}=\textbf{D}, the result \textbf{P}=\textbf{NP} is proven."
"Schoening presents a simple randomized algorithm for (d,k)-CSP problems with
running time (d(k-1)/k)^n poly(n). Here, d is the number of colors, k is the
size of the constraints, and n is the number of variables. A derandomized
version of this, given by Dantsin et al., achieves a running time of
(dk/(k+1))^n poly(n), inferior to Schoening's. We come up with a simple
modification of the deterministic algorithm, achieving a running time of
(d(k-1)/k * k^d/(k^d-1))^n \poly(n). Though not completely eleminating the gap,
this comes very close to the randomized bound for all but very small values of
d. Our main idea is to define a graph structure on the set of d colors to speed
up local search."
"In computable analysis, sequences of rational numbers which effectively
converge to a real number x are used as the (rho-) names of x. A real number x
is computable if it has a computable name, and a real function f is computable
if there is a Turing machine M which computes f in the sense that, M accepts
any rho-name of x as input and outputs a rho-name of f(x) for any x in the
domain of f. By weakening the effectiveness requirement of the convergence and
classifying the converging speeds of rational sequences, several interesting
classes of real numbers of weak computability have been introduced in
literature, e.g., in addition to the class of computable real numbers (EC), we
have the classes of semi-computable (SC), weakly computable (WC), divergence
bounded computable (DBC) and computably approximable real numbers (CA). In this
paper, we are interested in the weak computability of continuous real functions
and try to introduce an analogous classification of weakly computable real
functions. We present definitions of these functions by Turing machines as well
as by sequences of rational polygons and prove these two definitions are not
equivalent. Furthermore, we explore the properties of these functions, and
among others, show their closure properties under arithmetic operations and
composition."
"In this article we treat a notion of continuity for a multi-valued function F
and we compute the descriptive set-theoretic complexity of the set of all x for
which F is continuous at x. We give conditions under which the latter set is
either a G_\delta set or the countable union of G_\delta sets. Also we provide
a counterexample which shows that the latter result is optimum under the same
conditions. Moreover we prove that those conditions are necessary in order to
obtain that the set of points of continuity of F is Borel i.e., we show that if
we drop some of the previous conditions then there is a multi-valued function F
whose graph is a Borel set and the set of points of continuity of F is not a
Borel set. Finally we give some analogue results regarding a stronger notion of
continuity for a multi-valued function. This article is motivated by a question
of M. Ziegler in ""Real Computation with Least Discrete Advice: A Complexity
Theory of Nonuniform Computability with Applications to Linear Algebra"",
(submitted)."
"In this paper, we study the computability of the initial value problem of the
Combined KdV equation. It is shown that, for any integer s>2, the nonlinear
solution operator which maps an initial condition data to the solution of the
Combined KdV equation can be computed by a Turing machine."
"In a recent paper, two multi-representations for the measurable sets in a
computable measure space have been introduced, which prove to be topologically
complete w.r.t. certain topological properties. In this contribution, we show
them recursively complete w.r.t. computability of measure and set-theoretical
operations."
"The paper studies randomness extraction from sources with bounded
independence and the issue of independence amplification of sources, using the
framework of Kolmogorov complexity. The dependency of strings $x$ and $y$ is
${\rm dep}(x,y) = \max\{C(x) - C(x \mid y), C(y) - C(y\mid x)\}$, where
$C(\cdot)$ denotes the Kolmogorov complexity. It is shown that there exists a
computable Kolmogorov extractor $f$ such that, for any two $n$-bit strings with
complexity $s(n)$ and dependency $\alpha(n)$, it outputs a string of length
$s(n)$ with complexity $s(n)- \alpha(n)$ conditioned by any one of the input
strings. It is proven that the above are the optimal parameters a Kolmogorov
extractor can achieve. It is shown that independence amplification cannot be
effectively realized. Specifically, if (after excluding a trivial case) there
exist computable functions $f_1$ and $f_2$ such that ${\rm dep}(f_1(x,y),
f_2(x,y)) \leq \beta(n)$ for all $n$-bit strings $x$ and $y$ with ${\rm
dep}(x,y) \leq \alpha(n)$, then $\beta(n) \geq \alpha(n) - O(\log n)$."
"The paper gives estimations for the sizes of the the following sets: (1) the
set of strings that have a given dependency with a fixed string, (2) the set of
strings that are pairwise \alpha independent, (3) the set of strings that are
mutually \alpha independent. The relevant definitions are as follows: C(x) is
the Kolmogorov complexity of the string x. A string y has \alpha -dependency
with a string x if C(y) - C(y|x) \geq \alpha. A set of strings {x_1, \ldots,
x_t} is pairwise \alpha-independent if for all i different from j, C(x_i) -
C(x_i | x_j) \leq \alpha. A tuple of strings (x_1, \ldots, x_t) is mutually
\alpha-independent if C(x_{\pi(1)} \ldots x_{\pi(t)}) \geq C(x_1) + \ldots +
C(x_t) - \alpha, for every permutation \pi of [t]."
"In their paper on the ""chasm at depth four"", Agrawal and Vinay have shown
that polynomials in m variables of degree O(m) which admit arithmetic circuits
of size 2^o(m) also admit arithmetic circuits of depth four and size 2^o(m).
This theorem shows that for problems such as arithmetic circuit lower bounds or
black-box derandomization of identity testing, the case of depth four circuits
is in a certain sense the general case. In this paper we show that smaller
depth four circuits can be obtained if we start from polynomial size arithmetic
circuits. For instance, we show that if the permanent of n*n matrices has
circuits of size polynomial in n, then it also has depth 4 circuits of size
n^O(sqrt(n)*log(n)). Our depth four circuits use integer constants of
polynomial size. These results have potential applications to lower bounds and
deterministic identity testing, in particular for sums of products of sparse
univariate polynomials. We also give an application to boolean circuit
complexity, and a simple (but suboptimal) reduction to polylogarithmic depth
for arithmetic circuits of polynomial size and polynomially bounded degree."
"This article describes a formal strategy of geometric complexity theory (GCT)
to resolve the {\em self referential paradox} in the $P$ vs. $NP$ and related
problems. The strategy, called the {\em flip}, is to go for {\em explicit
proofs} of these problems. By an explicit proof we mean a proof that constructs
proof certificates of hardness that are easy to verify, construct and decode.
The main result in this paper says that (1) any proof of the arithmetic
implication of the $P$ vs. $NP$ conjecture is close to an explicit proof in the
sense that it can be transformed into an explicit proof by proving in addition
that arithmetic circuit identity testing can be derandomized in a blackbox
fashion, and (2) stronger forms of these arithmetic hardness and
derandomization conjectures together imply a polynomial time algorithm for a
formidable explicit construction problem in algebraic geometry. This may
explain why these conjectures, which look so elementary at the surface, have
turned out to be so hard."
"This paper introduces a knowledge recognition algorithm (KRA) that is both a
Turing machine algorithm and an Oracle Turing machine algorithm. By definition
KRA is a non-deterministic language recognition algorithm. Simultaneously it
can be implemented as a deterministic Turing machine algorithm. KRA applies
mirrored perceptual-conceptual languages to learn member-class relations
between the two languages iteratively and retrieve information through
deductive and reductive recognition from one language to another. The novelty
of KRA is that the conventional concept of relation is adjusted. The
computation therefore becomes efficient bidirectional string mapping."
"We study Boolean circuits as a representation of Boolean functions and
consider different equivalence, audit, and enumeration problems. For a number
of restricted sets of gate types (bases) we obtain efficient algorithms, while
for all other gate types we show these problems are at least NP-hard."
"We show that for every probability p with 0 < p < 1, computation of
all-terminal graph reliability with edge failure probability p requires time
exponential in Omega(m/ log^2 m) for simple graphs of m edges under the
Exponential Time Hypothesis."
"We prove that quantum Turing machines are strictly superior to probabilistic
Turing machines in function computation for any space bound $ o(\log(n)) $."
"The Shortest Path Reconfiguration problem has as input a graph G (with unit
edge lengths) with vertices s and t, and two shortest st-paths P and Q. The
question is whether there exists a sequence of shortest st-paths that starts
with P and ends with Q, such that subsequent paths differ in only one vertex.
This is called a rerouting sequence.
  This problem is shown to be PSPACE-complete. For claw-free graphs and chordal
graphs, it is shown that the problem can be solved in polynomial time, and that
shortest rerouting sequences have linear length. For these classes, it is also
shown that deciding whether a rerouting sequence exists between all pairs of
shortest st-paths can be done in polynomial time. Finally, a polynomial time
algorithm for counting the number of isolated paths is given."
"This paper introduces a knowledge recognition algorithm (KRA) for solving the
3SAT problem in polynomial time. KRA learns member-class relations and
retrieves information through deductive and reductive iterative reasoning. It
applies the principle of Chinese COVA* (equivalent to a set of eight 3-variable
conjunctive clauses) and eliminates the ""OR"" operation to solve 3-SAT problem.
That is, KRA does not search the assignment directly. It recognizes the
complements as rejections at each level of the set through iterative set
relation recognition. KRA recognizes which conjunctive 3-variable-clause is not
satisfiable. If all the eight clauses of any set of 3-variable clauses are
rejected, then there is not an assignment for the formula. If there is at least
one clause in each set that remains, then there is at least one assignment that
is the union of clauses of each set. If there is more than one clause in each
set that remains, then there are multiple assignments that are the unions of
the clauses of each set respectively."
"We investigate the complexity of counting Eulerian tours ({\sc #ET}) and its
variations from two perspectives---the complexity of exact counting and the
complexity w.r.t. approximation-preserving reductions (AP-reductions
\cite{MR2044886}). We prove that {\sc #ET} is #P-complete even for planar
4-regular graphs.
  A closely related problem is that of counting A-trails ({\sc #A-trails}) in
graphs with rotational embedding schemes (so called maps). Kotzig
\cite{MR0248043} showed that {\sc #A-trails} can be computed in polynomial time
for 4-regular plane graphs (embedding in the plane is equivalent to giving a
rotational embedding scheme). We show that for 4-regular maps the problem is
#P-hard. Moreover, we show that from the approximation viewpoint {\sc
#A-trails} in 4-regular maps captures the essence of {\sc #ET}, that is, we
give an AP-reduction from {\sc #ET} in general graphs to {\sc #A-trails} in
4-regular maps. The reduction uses a fast mixing result for a card shuffling
problem \cite{MR2023023}.
  In order to understand whether #{\sc A-trails} in 4-regular maps can
AP-reduce to #{\sc ET} in 4-regular graphs, we investigate a problem in which
transitions in vertices are weighted (this generalizes both #{\sc A-trails} and
#{\sc ET}). In the 4-regular case we show that {\sc A-trails} can be used to
simulate any vertex weights and provide evidence that {\sc ET} can simulate
only a limited set of vertex weights."
"Many theorems about Kolmogorov complexity rely on existence of combinatorial
objects with specific properties. Usually the probabilistic method gives such
objects with better parameters than explicit constructions do. But the
probabilistic method does not give ""effective"" variants of such theorems, i.e.
variants for resource-bounded Kolmogorov complexity. We show that a ""naive
derandomization"" approach of replacing these objects by the output of
Nisan-Wigderson pseudo-random generator may give polynomial-space variants of
such theorems.
  Specifically, we improve the preceding polynomial-space analogue of Muchnik's
conditional complexity theorem. I.e., for all $a$ and $b$ there exists a
program $p$ of least possible length that transforms $a$ to $b$ and is simple
conditional on $b$. Here all programs work in polynomial space and all
complexities are measured with logarithmic accuracy instead of polylogarithmic
one in the previous work."
"In 1994, S.G. Matthews introduced the notion of partial metric space in order
to obtain a suitable mathematical tool for program verification [Ann. New York
Acad. Sci. 728 (1994), 183-197]. He gave an application of this new structure
to parallel computing by means of a partial metric version of the celebrated
Banach fixed point theorem [Theoret. Comput. Sci. 151 (1995), 195-205]. Later
on, M.P. Schellekens introduced the theory of complexity (quasi-metric) spaces
as a part of the development of a topological foundation for the asymptotic
complexity analysis of programs and algorithms [Elec- tronic Notes in Theoret.
Comput. Sci. 1 (1995), 211-232]. The applicability of this theory to the
asymptotic complexity analysis of Divide and Conquer algorithms was also
illustrated by Schellekens. In particular, he gave a new proof, based on the
use of the aforenamed Banach fixed point theorem, of the well-known fact that
Mergesort al- gorithm has optimal asymptotic average running time of computing.
In this paper, motivated by the utility of partial metrics in Computer Science,
we discuss whether the Matthews fixed point theorem is a suitable tool to
analyze the asymptotic complexity of algorithms in the spirit of Schellekens.
Specifically, we show that a slight modification of the well-known Baire
partial metric on the set of all words over an alphabet constitutes an
appropriate tool to carry out the asymptotic complexity analysis of algorithms
via fixed point methods without the need for assuming the convergence condition
inherent to the defini- tion of the complexity space in the Shellekens
framework. Finally, in order to illustrate and to validate the developed theory
we apply our results to analyze the asymptotic complexity of Quicksort,
Mergesort and Largesort algorithms."
"In this paper we present a new path order for rewrite systems, the
exponential path order EPOSTAR. Suppose a term rewrite system is compatible
with EPOSTAR, then the runtime complexity of this rewrite system is bounded
from above by an exponential function. Furthermore, the class of function
computed by a rewrite system compatible with EPOSTAR equals the class of
functions computable in exponential time on a Turing maschine."
"We present a simple deterministic gap-preserving reduction from SAT to the
Minimum Distance of Code Problem over $\F_2$. We also show how to extend the
reduction to work over any finite field. Previously a randomized reduction was
known due to Dumer, Micciancio, and Sudan, which was recently derandomized by
Cheng and Wan. These reductions rely on highly non-trivial coding theoretic
constructions whereas our reduction is elementary.
  As an additional feature, our reduction gives a constant factor hardness even
for asymptotically good codes, i.e., having constant rate and relative
distance. Previously it was not known how to achieve deterministic reductions
for such codes."
"In this paper, we study the complexity of computing the determinant of a
matrix over a non-commutative algebra. In particular, we ask the question,
""over which algebras, is the determinant easier to compute than the permanent?""
Towards resolving this question, we show the following hardness and easiness of
noncommutative determinant computation.
  * [Hardness] Computing the determinant of an n \times n matrix whose entries
are themselves 2 \times 2 matrices over a field is as hard as computing the
permanent over the field. This extends the recent result of Arvind and
Srinivasan, who proved a similar result which however required the entries to
be of linear dimension.
  * [Easiness] Determinant of an n \times n matrix whose entries are themselves
d \times d upper triangular matrices can be computed in poly(n^d) time.
  Combining the above with the decomposition theorem of finite dimensional
algebras (in particular exploiting the simple structure of 2 \times 2 matrix
algebras), we can extend the above hardness and easiness statements to more
general algebras as follows. Let A be a finite dimensional algebra over a
finite field with radical R(A).
  * [Hardness] If the quotient A/R(A) is non-commutative, then computing the
determinant over the algebra A is as hard as computing the permanent.
  * [Easiness] If the quotient A/R(A) is commutative and furthermore, R(A) has
nilpotency index d (i.e., the smallest d such that R(A)d = 0), then there
exists a poly(n^d)-time algorithm that computes determinants over the algebra
A.
  In particular, for any constant dimensional algebra A over a finite field,
since the nilpotency index of R(A) is at most a constant, we have the following
dichotomy theorem: if A/R(A) is commutative, then efficient determinant
computation is feasible and otherwise determinant is as hard as permanent."
"We introduce the NP-complete problem 3SAT_N and extend Tovey's results to a
classification theorem for this problem. This theorem leads us to generalize
the concept of truth assignments for SAT to aggressive truth assignments for
3SAT_N. We introduce the concept of a set compatible with the P and NP problem,
and prove that all aggressive truth assignments are pseudo-algorithms. We
combine algorithm, pseudo-algorithm and diagonalization method to study the
complexity of 3SAT_N and the P versus NP problem. The main result is P != NP."
"We answer a problem posed in (G\'al, Kouck\'y, McKenzie 2008) regarding a
restricted model of small-space computation, tailored for solving the GEN
problem. They define two variants of ""incremental branching programs"", the
syntactic variant defined by a restriction on the graph-theoretic paths in the
program, and the more-general semantic variant in which the same restriction is
enforced only on the consistent paths - those that are followed by at least one
input. They show that exponential size is required for the syntactic variant,
but leave open the problem of superpolynomial lower bounds for the semantic
variant. Here we give an exponential lower bound for the semantic variant by
generalizing lower bound arguments, from earlier work, for a similar restricted
model tailored for solving a special case of GEN called Tree Evaluation."
"Random 3CNF formulas constitute an important distribution for measuring the
average-case behavior of propositional proof systems. Lower bounds for random
3CNF refutations in many propositional proof systems are known. Most notably
are the exponential-size resolution refutation lower bounds for random 3CNF
formulas with $\Omega(n^{1.5-\epsilon}) $ clauses [Chvatal and Szemeredi
(1988), Ben-Sasson and Wigderson (2001)]. On the other hand, the only known
non-trivial upper bound on the size of random 3CNF refutations in a
non-abstract propositional proof system is for resolution with
$\Omega(n^{2}/\log n) $ clauses, shown by Beame et al. (2002). In this paper we
show that already standard propositional proof systems, within the hierarchy of
Frege proofs, admit short refutations for random 3CNF formulas, for
sufficiently large clause-to-variable ratio. Specifically, we demonstrate
polynomial-size propositional refutations whose lines are $TC^0$ formulas
(i.e., $TC^0$-Frege proofs) for random 3CNF formulas with $ n $ variables and $
\Omega(n^{1.4}) $ clauses.
  The idea is based on demonstrating efficient propositional correctness proofs
of the random 3CNF unsatisfiability witnesses given by Feige, Kim and Ofek
(2006). Since the soundness of these witnesses is verified using spectral
techniques, we develop an appropriate way to reason about eigenvectors in
propositional systems. To carry out the full argument we work inside weak
formal systems of arithmetic and use a general translation scheme to
propositional proofs."
"We study the communication complexity of symmetric XOR functions, namely
functions $f: \{0,1\}^n \times \{0,1\}^n \rightarrow \{0,1\}$ that can be
formulated as $f(x,y)=D(|x\oplus y|)$ for some predicate $D: \{0,1,...,n\}
\rightarrow \{0,1\}$, where $|x\oplus y|$ is the Hamming weight of the bitwise
XOR of $x$ and $y$. We give a public-coin randomized protocol in the
Simultaneous Message Passing (SMP) model, with the communication cost matching
the known lower bound for the \emph{quantum} and \emph{two-way} model up to a
logarithm factor. As a corollary, this closes a quadratic gap between quantum
lower bound and randomized upper bound for the one-way model, answering an open
question raised in Shi and Zhang \cite{SZ09}."
"We show that BPP has either SUBEXP-dimension zero (randomness is easy) or
BPP=EXP (randomness is intractable)."
"A general theory of resource-bounded measurability and measure is developed.
Starting from any feasible probability measure $\nu$ on the Cantor space $\C$
and any suitable complexity class $C \subseteq \C$, the theory identifies the
subsets of $\C$ that are $\nu$-measurable in $C$ and assigns measures to these
sets, thereby endowing $C$ with internal measure-theoretic structure. Classes
to which the theory applies include various exponential time and space
complexity classes, the class of all decidable languages, and the Cantor space
itself, on which the resource-bounded theory is shown to agree with the
classical theory.
  The sets that are $\nu$-measurable in $C$ are shown to form an algebra
relative to which $\nu$-measure is well-behaved. This algebra is also shown to
be complete and closed under sufficiently uniform infinitary unions and
intersections, and $\nu$-measure in $C$ is shown to have the appropriate
additivity and monotone convergence properties with respect to such infinitary
operations.
  A generalization of the classical Kolmogorov zero-one law is proven, showing
that when $\nu$ is any feasible coin-toss probability measure on $\C$, every
set that is $\nu$-measurable in $C$ and (like most complexity classes)
invariant under finite alterations must have $\nu$-measure 0 or $\nu$-measure 1
in $C$.
  The theory is presented here is based on resource-bounded martingale
splitting operators, which are type-2 functionals, each of which maps $\N
\times {\cal D}_\nu$ into ${\cal D}_\nu \times {\cal D}_\nu$, where ${\cal
D}_\nu$ is the set of all $\nu$-martingales. This type-2 aspect of the theory
appears to be essential for general $\nu$-measure in complexity classes $C$,
but the sets of $\nu$-measure 0 or 1 in C are shown to be characterized by the
success conditions for martingales (type-1 functions) that have been used in
resource-bounded measure to date."
"Randomness extraction is the process of constructing a source of randomness
of high quality from one or several sources of randomness of lower quality. The
problem can be modeled using probability distributions and min-entropy to
measure their quality and also by using individual strings and Kolmogorov
complexity to measure their quality. Complexity theorists are more familiar
with the first approach. In this paper we survey the second approach. We
present the connection between extractors and Kolmogorov extractors and the
basic positive and negative results concerning Kolmogorov complexity
extraction."
"We develop a pseudo-random generator to fool degree-$d$ polynomial threshold
functions with respect to the Gaussian distribution. For $c>0$ any constant, we
construct a pseudo-random generator that fools such functions to within
$\epsilon$ and has seed length $\log(n) 2^{O(d)} \epsilon^{-4-c}$."
"The minimization problem for propositional formulas is an important
optimization problem in the second level of the polynomial hierarchy. In
general, the problem is Sigma-2-complete under Turing reductions, but
restricted versions are tractable. We study the complexity of minimization for
formulas in two established frameworks for restricted propositional logic: The
Post framework allowing arbitrarily nested formulas over a set of Boolean
connectors, and the constraint setting, allowing generalizations of CNF
formulas. In the Post case, we obtain a dichotomy result: Minimization is
solvable in polynomial time or coNP-hard. This result also applies to Boolean
circuits. For CNF formulas, we obtain new minimization algorithms for a large
class of formulas, and give strong evidence that we have covered all
polynomial-time cases."
"This paper presents a new representation of natural numbers and discusses its
consequences for computability and computational complexity. The paper argues
that the introduction of the first Peano axiom in the traditional definition of
natural numbers is not essential. It claims that natural numbers remain usable
in traditional ways without assuming the existence of at least one natural
number. However, the uncertainty about the existence of natural numbers
translates into every computation and introduces intrinsic uncertainty that
cannot be avoided. The uncertainty in the output of a computation can be
reduced, though, at the expense of a longer runtime and thus higher complexity.
For the new representation of natural numbers, the paper claims that, with the
first Peano axiom, P is equal to NP, and that without the first Peano axiom, P
becomes a proper subset of NP."
"We show that if DTIME[2^{O(n)}] is not included in DSPACE[2^{o(n)}], then,
for every set B in PSPACE, all strings x in B of length n can be represented by
a string compressed(x) of length at most log (|B^{=n}|) + O(log n), such that a
polynomial-time algorithm, given compressed(x), can distinguish x from all the
other strings in B^{=n}. Modulo the O(log n) additive trem, this achieves the
information-theoretical optimum for string compression."
"We consider the following problem that arises in outsourced storage: a user
stores her data $x$ on a remote server but wants to audit the server at some
later point to make sure it actually did store $x$. The goal is to design a
(randomized) verification protocol that has the property that if the server
passes the verification with some reasonably high probability then the user can
rest assured that the server is storing $x$.
  In this work we present an optimal solution (in terms of the user's storage
and communication) while at the same time ensuring that a server that passes
the verification protocol with any reasonable probability will store, to within
a small \textit{additive} factor, $C(x)$ bits of information, where $C(x)$ is
the plain Kolmogorov complexity of $x$. (Since we cannot prevent the server
from compressing $x$, $C(x)$ is a natural upper bound.) The proof of security
of our protocol combines Kolmogorov complexity with list decoding and unlike
previous work that relies upon cryptographic assumptions, we allow the server
to have unlimited computational power. To the best of our knowledge, this is
the first work that combines Kolmogorov complexity and list decoding.
  Our framework is general enough to capture extensions where the user splits
up $x$ and stores the fragment across multiple servers and our verification
protocol can handle non-responsive servers and colluding servers. As a
by-product, we also get a proof of retrievability. Finally, our results also
have an application in `storage enforcement' schemes, which in turn have an
application in trying to update a remote server that is potentially infected
with a virus."
"The symbolic representation of a number should be considered as a data
structure, and the choice of data structure depends on the arithmetic
operations that are to be performed. Numbers are almost universally represented
using position based notations based on exponential powers of a base number -
usually 10. This representations is computationally efficient for the standard
arithmetic operations, but it is not efficient for factorisation. This has led
to a common confusion that factorisation is inherently computationally hard. We
propose a new representation of the natural numbers based on bags and using the
prime successor function as a primitive - prime bags (PBs). This data structure
is more efficient for most arithmetic operations, and enables numbers can be
efficiently factored. However, it also has the interesting feature that
addition appears to be computationally hard. PBs have an interesting
alternative interpretation as partitions of numbers represented in the standard
way, and this reveals a novel relationship between prime numbers and the
partition function. The PB representation can be extended to rational and
irrational numbers, and this provides the most direct proof of the
irrationality of the square root of 2. I argue that what needs to be ultimately
understood is not the peculiar computation complexity properties of the decimal
system (e.g. factorisation), but rather what arithmetical operator trade-offs
are generally possible."
"Recently there has been much interest in Gowers uniformity norms from the
perspective of theoretical computer science. This is mainly due to the fact
that these norms provide a method for testing whether the maximum correlation
of a function $f:\mathbb{F}_p^n \rightarrow \mathbb{F}_p$ with polynomials of
degree at most $d \le p$ is non-negligible, while making only a constant number
of queries to the function. This is an instance of {\em correlation testing}.
In this framework, a fixed test is applied to a function, and the acceptance
probability of the test is dependent on the correlation of the function from
the property. This is an analog of {\em proximity oblivious testing}, a notion
coined by Goldreich and Ron, in the high error regime. In this work, we study
general properties which are affine invariant and which are correlation
testable using a constant number of queries. We show that any such property (as
long as the field size is not too small) can in fact be tested by Gowers
uniformity tests, and hence having correlation with the property is equivalent
to having correlation with degree $d$ polynomials for some fixed $d$. We stress
that our result holds also for non-linear properties which are affine
invariant. This completely classifies affine invariant properties which are
correlation testable. The proof is based on higher-order Fourier analysis.
Another ingredient is a nontrivial extension of a graph theoretical theorem of
Erd\""os, Lov\'asz and Spencer to the context of additive number theory."
"There are several forms of irreducibility in computing systems, ranging from
undecidability to intractability to nonlinearity. This paper is an exploration
of the conceptual issues that have arisen in the course of investigating
speed-up and slowdown phenomena in small Turing machines. We present the
results of a test that may spur experimental approaches to the notion of
computational irreducibility. The test involves a systematic attempt to outrun
the computation of a large number of small Turing machines (all 3 and 4 state,
2 symbol) by means of integer sequence prediction using a specialized function
finder program. This massive experiment prompts an investigation into rates of
convergence of decision procedures and the decidability of sets in addition to
a discussion of the (un)predictability of deterministic computing systems in
practice. We think this investigation constitutes a novel approach to the
discussion of an epistemological question in the context of a computer
simulation, and thus represents an interesting exploration at the boundary
between philosophical concerns and computational experiments."
"Arc-annotated sequences are useful in representing the structural information
of RNA and protein sequences. The longest arc-preserving common subsequence
problem has been introduced as a framework for studying the similarity of
arc-annotated sequences. In this paper, we consider arc-annotated sequences
with various arc structures. We consider the longest arc preserving common
subsequence problem. In particular, we show that the decision version of the
1-{\sc fragment LAPCS(crossing,chain)} and the decision version of the 0-{\sc
diagonal LAPCS(crossing,chain)} are {\bf NP}-complete for some fixed alphabet
$\Sigma$ such that $|\Sigma| = 2$. Also we show that if $|\Sigma| = 1$, then
the decision version of the 1-{\sc fragment LAPCS(unlimited, plain)} and the
decision version of the 0-{\sc diagonal LAPCS(unlimited, plain)} are {\bf
NP}-complete."
"We present a deterministic polynomial-time algorithm that solves the
3-satisfiability problem."
"For a connected graph G=(V,E), a subset U of V is called a disconnected cut
if U disconnects the graph and the subgraph induced by U is disconnected as
well. We show that the problem to test whether a graph has a disconnected cut
is NP-complete. This problem is polynomially equivalent to the following
problems: testing if a graph has a 2K2-partition, testing if a graph allows a
vertex-surjective homomorphism to the reflexive 4-cycle and testing if a graph
has a spanning subgraph that consists of at most two bicliques. Hence, as an
immediate consequence, these three decision problems are NP-complete as well.
This settles an open problem frequently posed in each of the four settings."
"We survey known results about the complexity of surjective homomorphism
problems, studied in the context of related problems in the literature such as
list homomorphism, retraction and compaction. In comparison with these
problems, surjective homomorphism problems seem to be harder to classify and we
examine especially three concrete problems that have arisen from the
literature, two of which remain of open complexity."
"It is well known that the inverse function of y = x with the derivative y' =
1 is x = y, the inverse function of y = c with the derivative y' = 0 is
inexistent, and so on. Hence, on the assumption that the noninvertibility of
the univariate increasing function y = f(x) with x > 0 is in direct proportion
to the growth rate reflected by its derivative, the authors put forward a
method of comparing difficulties in inverting two functions on a continuous or
discrete interval called asymptotic granularity reduction (AGR) which
integrates asymptotic analysis with logarithmic granularities, and is an
extension and a complement to polynomial time (Turing) reduction (PTR). Prove
by AGR that inverting y = x ^ x (mod p) is computationally harder than
inverting y = g ^ x (mod p), and inverting y = g ^ (x ^ n) (mod p) is
computationally equivalent to inverting y = g ^ x (mod p), which are compatible
with the results from PTR. Besides, apply AGR to the comparison of inverting y
= x ^ n (mod p) with y = g ^ x (mod p), y = g ^ (g1 ^ x) (mod p) with y = g ^ x
(mod p), and y = x ^ n + x + 1 (mod p) with y = x ^ n (mod p) in difficulty,
and observe that the results are consistent with existing facts, which further
illustrates that AGR is suitable for comparison of inversion problems in
difficulty. Last, prove by AGR that inverting y = (x ^ n)(g ^ x) (mod p) is
computationally equivalent to inverting y = g ^ x (mod p) when PTR can not be
utilized expediently. AGR with the assumption partitions the complexities of
problems more detailedly, and finds out some new evidence for the security of
cryptosystems."
"In this paper, we focus on nonlinear infinite-norm minimization problems that
have many applications, especially in computer science and operations research.
We set a reliable Lagrangian dual aproach for solving this kind of problems in
general, and based on this method, we propose an algorithm for the mixed linear
and nonlinear infinite-norm minimization cases with numerical results."
"This note is a commentary on, and critique of, Andre Luiz Barbosa's paper
entitled ""P != NP Proof."" Despite its provocative title, what the paper is
seeking to do is not to prove P \neq NP in the standard sense in which that
notation is used in the literature. Rather, Barbosa is (and is aware that he
is) arguing that a different meaning should be associated with the notation P
\neq NP, and he claims to prove the truth of the statement P \neq NP in his
quite different sense of that statement. However, we note that (1) the paper
fails even on its own terms, as due to a uniformity problem, the paper's proof
does not establish, even in its unusual sense of the notation, that P \neq NP;
and (2) what the paper means by the claim P \neq NP in fact implies that P \neq
NP holds even under the standard meaning that that notation has in the
literature (and so it is exceedingly unlikely that Barbosa's proof can be fixed
any time soon)."
"One of the important problems in multiprocessor systems is Task Graph
Scheduling. Task Graph Scheduling is an NP-Hard problem. Both learning automata
and genetic algorithms are search tools which are used for solving many NP-Hard
problems. In this paper a new hybrid method based on Genetic Algorithm and
Learning Automata is proposed. The proposed algorithm begins with an initial
population of randomly generated chromosomes and after some stages, each
chromosome maps to an automaton. Experimental results show that superiority of
the proposed algorithm over the current approaches."
"Zen Puzzle Garden (ZPG) is a one-player puzzle game. In this paper, we prove
that deciding the solvability of ZPG is NP-complete."
"Since human randomness production has been studied and widely used to assess
executive functions (especially inhibition), many measures have been suggested
to assess the degree to which a sequence is random-like. However, each of them
focuses on one feature of randomness, leading authors to have to use multiple
measures. Here we describe and advocate for the use of the accepted universal
measure for randomness based on algorithmic complexity, by means of a novel
previously presented technique using the the definition of algorithmic
probability. A re-analysis of the classical Radio Zenith data in the light of
the proposed measure and methodology is provided as a study case of an
application."
"One approach to confronting computational hardness is to try to understand
the contribution of various parameters to the running time of algorithms and
the complexity of computational tasks. Almost no computational tasks in real
life are specified by their size alone. It is not hard to imagine that some
parameters contribute more intractability than others and it seems reasonable
to develop a theory of computational complexity which seeks to exploit this
fact. Such a theory should be able to address the needs of practicioners in
algorithmics. The last twenty years have seen the development of such a theory.
This theory has a large number of successes in terms of a rich collection of
algorithmic techniques both practical and theoretical, and a fine-grained
intractability theory. Whilst the theory has been widely used in a number of
areas of applications including computational biology, linguistics, VLSI
design, learning theory and many others, knowledge of the area is highly
varied. We hope that this article will show both the basic theory and point at
the wide array of techniques available. Naturally the treatment is condensed,
and the reader who wants more should go to the texts, Downey and Fellows, Flum
and Grohe, Niedermeier, and the upcoming undergraduate text Downey and Fellows."
"Width parameterizations of SAT, such as tree-width and path-width, enable the
study of computationally more tractable and practical SAT instances. We give
two simple algorithms. One that runs simultaneously in time-space
$(O^*(2^{2tw(\phi)}), O^*(2^{tw(\phi)}))$ and another that runs in time-space
$(O^*(3^{tw(\phi)\log{|\phi|}}),|\phi|^{O(1)})$, where $tw(\phi)$ is the
tree-width of a formula $\phi$ with $|\phi|$ many clauses and variables. This
partially answers the question of Alekhnovitch and Razborov, who also gave
algorithms exponential both in time and space, and asked whether the space can
be made smaller. We conjecture that every algorithm for this problem that runs
in time $2^{tw(\phi)\mathbf{o(\log{|\phi|})}}$ necessarily blows up the space
to exponential in $tw(\phi)$.
  We introduce a novel way to combine the two simple algorithms that allows us
to trade \emph{constant} factors in the exponents between running time and
space. Our technique gives rise to a family of algorithms controlled by two
parameters. By fixing one parameter we obtain an algorithm that runs in
time-space $(O^*(3^{1.441(1-\epsilon)tw(\phi)\log{|\phi|}}), O^*(2^{2\epsilon
tw(\phi)}))$, for every $0<\epsilon<1$. We systematically study the limitations
of this technique, and show that these algorithmic results are the best
achievable using this technique.
  We also study further the computational complexity of width parameterizations
of SAT. We prove non-sparsification lower bounds for formulas of path-width
$\omega(\log|\phi|)$, and a separation between the complexity of path-width and
tree-width parametrized SAT modulo plausible complexity assumptions."
"In this note we give a new separation between sensitivity and block
sensitivity of Boolean functions: $bs(f)=(2/3)s(f)^2-(1/3)s(f)$."
"We investigate the complexity of algorithms counting ones in different sets
of operations. With addition and logical operations (but no shift)
$O(\log^2(n))$ steps suffice to count ones. Parity can be computed with
complexity $O(\log(n))$, which is the same bound as for methods using
shift-operations. If multiplication is available, a solution of time complexity
$O(\log^*(n))$ is possible improving the known bound $O(\log\log(n))$."
"The Possible Winner problem asks, given an election where the voters'
preferences over the candidates are specified only partially, whether a
designated candidate can become a winner by suitably extending all the votes.
Betzler and Dorn [1] proved a result that is only one step away from a full
dichotomy of this problem for the important class of pure scoring rules in the
case of unweighted voters and an unbounded number of candidates: Possible
Winner is NP-complete for all pure scoring rules except plurality, veto, and
the scoring rule with vector (2,1,...,1,0), but is solvable in polynomial time
for plurality and veto. We take the final step to a full dichotomy by showing
that Possible Winner is NP-complete also for the scoring rule with vector
(2,1,...,1,0)."
"An important tool in the study of the complexity of Constraint Satisfaction
Problems (CSPs) is the notion of a relational clone, which is the set of all
relations expressible using primitive positive formulas over a particular set
of base relations. Post's lattice gives a complete classification of all
Boolean relational clones, and this has been used to classify the computational
difficulty of CSPs. Motivated by a desire to understand the computational
complexity of (weighted) counting CSPs, we develop an analogous notion of
functional clones and study the landscape of these clones. One of these clones
is the collection of log-supermodular (lsm) functions, which turns out to play
a significant role in classifying counting CSPs. In the conservative case
(where all nonnegative unary functions are available), we show that there are
no functional clones lying strictly between the clone of lsm functions and the
total clone (containing all functions). Thus, any counting CSP that contains a
single nontrivial non-lsm function is computationally as hard to approximate as
any problem in #P. Furthermore, we show that any non-trivial functional clone
(in a sense that will be made precise) contains the binary function ""implies"".
As a consequence, in the conservative case, all non-trivial counting CSPs are
as hard as #BIS, the problem of counting independent sets in a bipartite graph.
Given the complexity-theoretic results, it is natural to ask whether the
""implies"" clone is equivalent to the clone of lsm functions. We use the Mobius
transform and the Fourier transform to show that these clones coincide
precisely up to arity 3. It is an intriguing open question whether the lsm
clone is finitely generated. Finally, we investigate functional clones in which
only restricted classes of unary functions are available."
"We investigate the computational complexity of the empire colouring problem
(as defined by Percy Heawood in 1890) for maps containing empires formed by
exactly $r > 1$ countries each. We prove that the problem can be solved in
polynomial time using $s$ colours on maps whose underlying adjacency graph has
no induced subgraph of average degree larger than $s/r$. However, if $s \geq
3$, the problem is NP-hard even if the graph is a forest of paths of arbitrary
lengths (for any $r \geq 2$, provided $s < 2r - \sqrt{2r + 1/4+ 3/2).
Furthermore we obtain a complete characterization of the problem's complexity
for the case when the input graph is a tree, whereas our result for arbitrary
planar graphs fall just short of a similar dichotomy. Specifically, we prove
that the empire colouring problem is NP-hard for trees, for any $r \geq 2$, if
$3 \leq s \leq 2r-1$ (and polynomial time solvable otherwise). For arbitrary
planar graphs we prove NP-hardness if $s<7$ for $r=2$, and $s < 6r-3$, for $r
\geq 3$. The result for planar graphs also proves the NP-hardness of colouring
with less than 7 colours graphs of thickness two and less than $6r-3$ colours
graphs of thickness $r \geq 3$."
"Define Minimum Soapy Union (MinSU) as the following optimization problem:
given a $k$-tuple $(X_1, X_2,..., X_k)$ of finite integer sets, find a
$k$-tuple $(t_1, t_2,..., t_k)$ of integers that minimizes the cardinality of
$(X_1 + t_1) \cup (X_2 + t_2) \cup ... \cup (X_n + t_k)$. We show that MinSU is
NP-complete, APX-hard, and polynomial for fixed $k$. MinSU appears naturally in
the context of protein shotgun sequencing: Here, the protein is cleaved into
short and overlapping peptides, which are then analyzed by tandem mass
spectrometry. To improve the quality of such spectra, one then asks for the
mass of the unknown prefix (the shift) of the spectrum, such that the resulting
shifted spectra show a maximum agreement. For real-world data the problem is
even more complicated than our definition of MinSU; but our intractability
results clearly indicate that it is unlikely to find a polynomial time
algorithm for shotgun protein sequencing."
"We give a unified treatment to optimization problems that can be expressed in
the form of nonnegative-real-weighted Boolean constraint satisfaction problems.
Creignou, Khanna, Sudan, Trevisan, and Williamson studied the complexity of
approximating their optimal solutions whose optimality is measured by the sums
of outcomes of constraints. To explore a wider range of optimization constraint
satisfaction problems, following an early work of Marchetti-Spaccamela and
Romano, we study the case where the optimality is measured by products of
constraints' outcomes. We completely classify those problems into three
categories: PO problems, NPO-hard problems, and intermediate problems that lie
between the former two categories. To prove this trichotomy theorem, we analyze
characteristics of nonnegative-real-weighted constraints using a variant of the
notion of T-constructibility developed earlier for complex-weighted counting
constraint satisfaction problems."
"For data ciphering a key is usually needed as a base, so it is indispensable
to have one that is strong and trustworthy, so as to keep others from accessing
the ciphered data. This requires a pseudo-random number generator that would
provide such a key, so it is proposed to work with cellular automata helped
along with \emph{Mathematica} to check that the rules and to what level are
actually pseudo-random. This project centers on the examination of possible
mathematical rules, analyzing their characteristics in a detailed manner, and
submitting them to a set of randomness tests with the end of knowing which of
them will enable us to obtain those pseudo-random numbers that will conform the
key for data ciphering."
"Constraint satisfaction problems have been studied in numerous fields with
practical and theoretical interests. In recent years, major breakthroughs have
been made in a study of counting constraint satisfaction problems (or #CSPs).
In particular, a computational complexity classification of bounded-degree
#CSPs has been discovered for all degrees except for two, where the ""degree"" of
an input instance is the maximal number of times that each input variable
appears in a given set of constraints. Despite the efforts of recent studies,
however, a complexity classification of degree-2 #CSPs has eluded from our
understandings. This paper challenges this open problem and gives its partial
solution by applying two novel proof techniques--T_{2}-constructibility and
parametrized symmetrization--which are specifically designed to handle
""arbitrary"" constraints under randomized approximation-preserving reductions.
We partition entire constraints into four sets and we classify the
approximation complexity of all degree-2 #CSPs whose constraints are drawn from
two of the four sets into two categories: problems computable in
polynomial-time or problems that are at least as hard as #SAT. Our proof
exploits a close relationship between complex-weighted degree-2 #CSPs and
Holant problems, which are a natural generalization of complex-weighted #CSPs."
"This paper talk about that NP is not AL and P, P is not NC, NC is not NL, and
NL is not L. The point about this paper is the depend relation of the problem
that need other problem's result to compute it. I show the structure of depend
relation that could divide each complexity classes."
"The material in this note is now superseded by arXiv:1108.5288v4.
  Bulatov et al. [1] defined the operation of (efficient)
pps_\omega-definability in order to study the computational complexity of
certain approximate counting problems. They asked whether all log-supermodular
functions can be defined by binary implication and unary functions in this
sense. We give a negative answer to this question."
"This project investigates the potential of computers to solve complex tasks
such as games. The paper proves that the complexity of a generalized version of
spider solitaire is NP-Complete and uses much of structure of the proof that
FreeCell is NP-Hard in the paper Helmert, M. ""Complexity Results for Standard
Benchmark Domains in Planning."" Artificial Intelligence 143.2 (2003): 219-62.
Print. A given decision problem falls in to the class NP-Complete if it is
proven to be both in NP and in NP-Hard. To prove that this is the case the
paper shows that, not only do the kinds of possible moves that can be reversed
prove this, but it is also shown that no spider solitaire game of size n will
take more than a polynomial number of moves to complete if such a completion is
possible. The paper reduces 3-SAT to SpiderSolitaire (the name used throughout
the proof when referring to the generalized version of popular solitaire
variant ""Spider Solitaire"") by showing that any 3-SAT instance can be
replicated using an appropriately arranged initial tableau. The example
provided reinforces the proof of NP-Hardness and helps to make the proof easier
to understand, but the definitive proof lies in the equations providing
instruction on how to set up any 3-SAT instance of clause size C as a instance
of SpiderSolitaire."
"In graph realization problems one is given a degree sequence and the task is
to decide whether there is a graph whose vertex degrees match to the given
sequence. This realization problem is known to be polynomial-time solvable when
the graph is directed or undirected. In contrary, we show NP-completeness for
the problem of realizing a given sequence of pairs of positive integers
(representing indegrees and outdegrees) with a directed acyclic graph,
answering an open question of Berger and M\""uller-Hannemann [FCT 2011].
Furthermore, we classify the problem as fixed-parameter tractable with respect
to the parameter ""maximum degree""."
"The question of whether the complexity class P is equal to the complexity
class NP has been a seemingly intractable problem for over 4 decades. It has
been clear that if an algorithm existed that would solve the problems in the NP
class in polynomial time then P would equal NP. However, no one has yet been
able to create that algorithm or to successfully prove that such an algorithm
cannot exist. The algorithm that will be presented in this paper solves the
3-satisfiability or 3-CNF-SAT problem, which has been proven to be NP-complete."
"Let $({\bf U},{\bf S},d)$ be an instance of Set Cover Problem, where ${\bf
U}=\{u_1,...,u_n\}$ is a $n$ element ground set, ${\bf S}=\{S_1,...,S_m\}$ is a
set of $m$ subsets of ${\bf U}$ satisfying $\bigcup_{i=1}^m S_i={\bf U}$ and
$d$ is a positive integer. In STOC 1993 M. Bellare, S. Goldwasser, C. Lund and
A. Russell proved the NP-hardness to distinguish the following two cases of
${\bf GapSetCover_{\eta}}$ for any constant $\eta > 1$. The Yes case is the
instance for which there is an exact cover of size $d$ and the No case is the
instance for which any cover of ${\bf U}$ from ${\bf S}$ has size at least
$\eta d$. This was improved by R. Raz and S. Safra in STOC 1997 about the
NP-hardness for ${\bf GapSetCover}_{clogm}$ for some constant $c$. In this
paper we prove that restricted parameter range subproblem is easy. For any
given function of $n$ satisfying $\eta(n) \geq 1$, we give a polynomial time
algorithm not depending on $\eta(n)$ to distinguish between
  {\bf YES:} The instance $({\bf U},{\bf S}, d)$ where $d>\frac{2 |{\bf
S}|}{3\eta(n)-1}$, for which there exists an exact cover of size at most $d$;
  {\bf NO:} The instance $({\bf U},{\bf S}, d)$ where $d>\frac{2 |{\bf
S}|}{3\eta(n)-1}$, for which any cover from ${\bf S}$ has size larger than
$\eta(n) d$.
  The polynomial time reduction of this restricted parameter range set cover
problem is constructed by using the lattice."
"We survey some work concerned with small universal Turing machines, cellular
automata, tag systems, and other simple models of computation. For example it
has been an open question for some time as to whether the smallest known
universal Turing machines of Minsky, Rogozhin, Baiocchi and Kudlek are
efficient (polynomial time) simulators of Turing machines. These are some of
the most intuitively simple computational devices and previously the best known
simulations were exponentially slow. We discuss recent work that shows that
these machines are indeed efficient simulators. In addition, another related
result shows that Rule 110, a well-known elementary cellular automaton, is
efficiently universal. We also discuss some old and new universal program size
results, including the smallest known universal Turing machines. We finish the
survey with results on generalised and restricted Turing machine models
including machines with a periodic background on the tape (instead of a blank
symbol), multiple tapes, multiple dimensions, and machines that never write to
their tape. We then discuss some ideas for future work."
"We study the complexity of valued constraint satisfaction problems (VCSP). A
problem from VCSP is characterised by a \emph{constraint language}, a fixed set
of cost functions over a finite domain. An instance of the problem is specified
by a sum of cost functions from the language and the goal is to minimise the
sum.
  We consider the case of languages containing all possible unary cost
functions. In the case of languages consisting of only $\{0,\infty\}$-valued
cost functions (i.e. relations), such languages have been called
\emph{conservative} and studied by Bulatov [LICS'03] and recently by Barto
[LICS'11]. Since we study valued languages, we call a language conservative if
it contains all finite-valued unary cost functions. The complexity of
conservative valued languages has been studied by Cohen et al. [AIJ'06] for
languages over Boolean domains, by Deineko et al. [JACM'08] for
$\{0,1\}$-valued languages (a.k.a Max-CSP), and by Takhanov [STACS'10] for
$\{0,\infty\}$-valued languages containing all finite-valued unary cost
functions (a.k.a. Min-Cost-Hom).
  We prove a Schaefer-like dichotomy theorem for conservative valued languages:
if all cost functions in the language satisfy a certain condition (specified by
a complementary combination of \emph{STP and MJN multimorphisms}), then any
instance can be solved in polytime (via a new algorithm developed in this
paper), otherwise the language is NP-hard. This is the \emph{first} complete
complexity classification of \emph{general-valued constraint languages} over
non-Boolean domains.
  This generalises previous results by Takhanov [STACS'10] and (a subset of
results) by Cohen et al. [AIJ'06] and Deineko et al. [JACM'08]. Moreover, our
results do not rely on any computer-assisted search as in Deineko et al.
[JACM'08], and provide a powerful tool for proving hardness of finite- and
general-valued languages."
"Maximum surjective constraint satisfaction problems (Max-Sur-CSPs) are
computational problems where we are given a set of variables denoting values
from a finite domain B and a set of constraints on the variables. A solution to
such a problem is a surjective mapping from the set of variables to B such that
the number of satisfied constraints is maximized. We study the approximation
performance that can be acccchieved by algorithms for these problems, mainly by
investigating their relation with Max-CSPs (which are the corresponding
problems without the surjectivity requirement). Our work gives a complexity
dichotomy for Max-Sur-CSP(B) between PTAS and APX-complete, under the
assumption that there is a complexity dichotomy for Max-CSP(B) between PO and
APX-complete, which has already been proved on the Boolean domain and 3-element
domains."
"We introduce the notion of a robust parameterized arithmetic circuit for the
evaluation of algebraic families of multivariate polynomials. Based on this
notion, we present a computation model, adapted to Scientific Computing, which
captures all known branching parsimonious symbolic algorithms in effective
Algebraic Geometry. We justify this model by arguments from Software
Engineering. Finally we exhibit a class of simple elimination problems of
effective Algebraic Geometry which require exponential time to be solved by
branching parsimonious algorithms of our computation model."
"We investigate the problem of cryptanalysis as a problem belonging to the
class NP. A class of problems UF is defined for which the time constructing any
feasible solution is polynomial. The properties of the problems of NP, which
may be one-way functions, are established."
"In earlier work, we gave an oracle separating the relational versions of BQP
and the polynomial hierarchy, and showed that an oracle separating the decision
versions would follow from what we called the Generalized Linial-Nisan (GLN)
Conjecture: that ""almost k-wise independent"" distributions are
indistinguishable from the uniform distribution by constant-depth circuits. The
original Linial-Nisan Conjecture was recently proved by Braverman; we offered a
$200 prize for the generalized version. In this paper, we save ourselves $200
by showing that the GLN Conjecture is false, at least for circuits of depth 3
and higher. As a byproduct, our counterexample also implies that Pi2P is not
contained in P^NP relative to a random oracle with probability 1. It has been
conjectured since the 1980s that PH is infinite relative to a random oracle,
but the highest levels of PH previously proved separate were NP and coNP.
Finally, our counterexample implies that the famous results of Linial, Mansour,
and Nisan, on the structure of AC0 functions, cannot be improved in several
interesting respects."
"We consider the complexity of two questions on polynomials given by
arithmetic circuits: testing whether a monomial is present and counting the
number of monomials. We show that these problems are complete for subclasses of
the counting hierarchy which had few or no known natural complete problems. We
also study these questions for circuits computing multilinear polynomials."
"This paper divide some complexity class by using fixpoint and fixpointless
area of Decidable Universal Turing Machine (UTM). Decidable Deterministic
Turing Machine (DTM) have fixpointless combinator that add no extra resources
(like Negation), but UTM makes some fixpoint in the combinator. This means that
we can jump out of the fixpointless combinator system by making more complex
problem from diagonalisation argument of UTM.
  As a concrete example, we proof L is not P . We can make Polynomial time UTM
that emulate all Logarithm space DTM (LDTM). LDTM set close under Negation,
therefore UTM does not close under LDTM set. (We can proof this theorem like
halting problem and time/space hierarchy theorem, and also we can extend this
proof to divide time/space limited DTM set.) In the same way, we proof P is not
NP. These are new hierarchy that use UTM and Negation."
"We consider the weighted antimonotone and the weighted monotone
satisfiability problems on normalized circuits of depth at most $t \geq 2$,
abbreviated {\sc wsat$^-[t]$} and {\sc wsat$^+[t]$}, respectively. These
problems model the weighted satisfiability of antimonotone and monotone
propositional formulas (including weighted anitmonoone/monotone {\sc cnf-sat})
in a natural way, and serve as the canonical problems in the definition of the
parameterized complexity hierarchy. We characterize the parameterized
complexity of {\sc wsat$^-[t]$} and {\sc wsat$^+[t]$} with respect to the genus
of the circuit. For {\sc wsat$^-[t]$}, which is $W[t]$-complete for odd $t$ and
$W[t-1]$-complete for even $t$, the characterization is precise: We show that
{\sc wsat$^-[t]$} is fixed-parameter tractable (FPT) if the genus of the
circuit is $n^{o(1)}$ ($n$ is the number of the variables in the circuit), and
that it has the same $W$-hardness as the general {\sc wsat$^-[t]$} problem
(i.e., with no restriction on the genus) if the genus is $n^{O(1)}$. For {\sc
wsat$^+[2]$} (i.e., weighted monotone {\sc cnf-sat}), which is $W[2]$-complete,
the characterization is also precise: We show that {\sc wsat$^+[2]$} is FPT if
the genus is $n^{o(1)}$ and $W[2]$-complete if the genus is $n^{O(1)}$. For
{\sc wsat$^+[t]$} where $t > 2$, which is $W[t]$-complete for even $t$ and
$W[t-1]$-complete for odd $t$, we show that it is FPT if the genus is
$O(\sqrt{\log{n}})$, and that it has the same $W$-hardness as the general {\sc
wsat$^+[t]$} problem if the genus is $n^{O(1)}$."
"We give several improvements on the known hardness of the unique shortest
vector problem. - We give a deterministic reduction from the shortest vector
problem to the unique shortest vector problem. As a byproduct, we get
deterministic NP-hardness for unique shortest vector problem in the
$\ell_\infty$ norm. - We give a randomized reduction from SAT to
uSVP_{1+1/poly(n)}. This shows that uSVP_{1+1/poly(n)} is NP-hard under
randomized reductions. - We show that if GapSVP_\gamma \in coNP (or coAM) then
uSVP_{\sqrt{\gamma}} \in coNP (coAM respectively). This simplifies previously
known uSVP_{n^{1/4}} \in coAM proof by Cai \cite{Cai98} to uSVP_{(n/\log
n)^{1/4}} \in coAM, and additionally generalizes it to uSVP_{n^{1/4}} \in coNP.
- We give a deterministic reduction from search-uSVP_\gamma to the
decision-uSVP_{\gamma/2}. We also show that the decision-uSVP is {\bf NP}-hard
for randomized reductions, which does not follow from Kumar-Sivakumar
\cite{KS01}."
"This paper provides the first general technique for proving information lower
bounds on two-party unbounded-rounds communication problems. We show that the
discrepancy lower bound, which applies to randomized communication complexity,
also applies to information complexity. More precisely, if the discrepancy of a
two-party function $f$ with respect to a distribution $\mu$ is $Disc_\mu f$,
then any two party randomized protocol computing $f$ must reveal at least
$\Omega(\log (1/Disc_\mu f))$ bits of information to the participants. As a
corollary, we obtain that any two-party protocol for computing a random
function on $\{0,1\}^n\times\{0,1\}^n$ must reveal $\Omega(n)$ bits of
information to the participants.
  In addition, we prove that the discrepancy of the Greater-Than function is
$\Omega(1/\sqrt{n})$, which provides an alternative proof to the recent proof
of Viola \cite{Viola11} of the $\Omega(\log n)$ lower bound on the
communication complexity of this well-studied function and, combined with our
main result, proves the tight $\Omega(\log n)$ lower bound on its information
complexity.
  The proof of our main result develops a new simulation procedure that may be
of an independent interest. In a very recent breakthrough work of Kerenidis et
al. \cite{kerenidis2012lower}, this simulation procedure was the main building
block for proving that almost all known lower bound techniques for
communication complexity (and not just discrepancy) apply to information
complexity."
"The paper serves as the first contribution towards the development of the
theory of efficiency: a unifying framework for the currently disjoint theories
of information, complexity, communication and computation. Realizing the
defining nature of the brute force approach in the fundamental concepts in all
of the above mentioned fields, the paper suggests using efficiency or
improvement over the brute force algorithm as a common unifying factor
necessary for the creation of a unified theory of information manipulation. By
defining such diverse terms as randomness, knowledge, intelligence and
computability in terms of a common denominator we are able to bring together
contributions from Shannon, Levin, Kolmogorov, Solomonoff, Chaitin, Yao and
many others under a common umbrella of the efficiency theory."
"There are errors in the algorithm proposed by Narendra Chaudhari [2]
purporting to solve the 3-sat problem in polynomial time. The present paper
present instances for which the algorithm outputs erroneous results."
"We initiate the study of constraint satisfaction problems (CSPs) in the
presence of counting quantifiers, which may be seen as variants of CSPs in the
mould of quantified CSPs (QCSPs). We show that a single counting quantifier
strictly between exists^1:=exists and exists^n:=forall (the domain being of
size n) already affords the maximal possible complexity of QCSPs (which have
both exists and forall), being Pspace-complete for a suitably chosen template.
Next, we focus on the complexity of subsets of counting quantifiers on clique
and cycle templates. For cycles we give a full trichotomy -- all such problems
are in L, NP-complete or Pspace-complete. For cliques we come close to a
similar trichotomy, but one case remains outstanding. Afterwards, we consider
the generalisation of CSPs in which we augment the extant quantifier
exists^1:=exists with the quantifier exists^j (j not 1). Such a CSP is already
NP-hard on non-bipartite graph templates. We explore the situation of this
generalised CSP on bipartite templates, giving various conditions for both
tractability and hardness -- culminating in a classification theorem for
general graphs. Finally, we use counting quantifiers to solve the complexity of
a concrete QCSP whose complexity was previously open."
"We initiate the complexity theoretic study of the problem of computing the
bits of (real) algebraic numbers. This extends the work of Yap on computing the
bits of transcendental numbers like \pi, in Logspace.
  Our main result is that computing a bit of a fixed real algebraic number is
in C=NC1\subseteq Logspace when the bit position has a verbose (unary)
representation and in the counting hierarchy when it has a succinct (binary)
representation.
  Our tools are drawn from elementary analysis and numerical analysis, and
include the Newton-Raphson method. The proof of our main result is entirely
elementary, preferring to use the elementary Liouville's theorem over the much
deeper Roth's theorem for algebraic numbers.
  We leave the possibility of proving non-trivial lower bounds for the problem
of computing the bits of an algebraic number given the bit position in binary,
as our main open question. In this direction we show very limited progress by
proving a lower bound for rationals."
"Computing supertrees is a central problem in phylogenetics. The supertree
method that is by far the most widely used today was introduced in 1992 and is
called Matrix Representation with Parsimony analysis (MRP). Matrix
Representation using Flipping (MRF)}, which was introduced in 2002, is an
interesting variant of MRP: MRF is arguably more relevant that MRP and various
efficient implementations of MRF have been presented. From a theoretical point
of view, implementing MRF or MRP is solving NP-hard optimization problems. The
aim of this paper is to study the approximability and the fixed-parameter
tractability of the optimization problem corresponding to MRF, namely
Minimum-Flip Supertree. We prove strongly negative results."
"We describe the Turing Machine, list some of its many influences on the
theory of computation and complexity of computations, and illustrate its
importance."
"In this paper, we show a direct product theorm in the model of two-party
bounded-round public-coin randomized communication complexity. For a relation f
subset of X times Y times Z (X,Y,Z are finite sets), let R^{(t), pub}_e (f)
denote the two-party t-message public-coin communication complexity of f with
worst case error e. We show that for any relation f and positive integer k:
R^{(t), pub}_{1 - 2^{-Omega(k/t^2)}}(f^k) = Omega(k/t (R^{(t), pub}_{1/3}(f) -
O(t^2))) . In particular, it implies a strong direct product theorem for the
two-party constant-message public-coin randomized communication complexity of
all relations f.
  Our result for example implies a strong direct product theorem for the
pointer chasing problem. This problem has been well studied for understanding
round v/s communication trade-offs in both classical and quantum communication
protocols.
  We show our result using information theoretic arguments. Our arguments and
techniques build on the ones used in [Jain 2011], where a strong direct product
theorem for the two-party one-way public-coin communication complexity of all
relations is shown (that is the special case of our result when t=1). One key
tool used in our work and also in [Jain 2011] is a message compression
technique due to [Braverman and Rao 2011], who used it to show a direct sum
theorem for the two-party bounded-round public-coin randomized communication
complexity of all relations. Another important tool that we use is a correlated
sampling protocol, which for example, has been used in [Holenstein 2007] for
proving a parallel repetition theorem for two-prover games."
"We show that one can approximate the least fixed point solution for a
multivariate system of monotone probabilistic polynomial equations in time
polynomial in both the encoding size of the system of equations and in
log(1/\epsilon), where \epsilon > 0 is the desired additive error bound of the
solution. (The model of computation is the standard Turing machine model.)
  We use this result to resolve several open problems regarding the
computational complexity of computing key quantities associated with some
classic and heavily studied stochastic processes, including multi-type
branching processes and stochastic context-free grammars."
"We propose a new order, the small polynomial path order (sPOP* for short).
The order sPOP* provides a characterisation of the class of polynomial time
computable function via term rewrite systems. Any polynomial time computable
function gives rise to a rewrite system that is compatible with sPOP*. On the
other hand any function defined by a rewrite system compatible with sPOP* is
polynomial time computable. Technically sPOP* is a tamed recursive path order
with product status. Its distinctive feature is the precise control provided.
For any rewrite system that is compatible with sPOP* that makes use of
recursion up to depth d, the (innermost) runtime complexity is bounded from
above by a polynomial of degree d."
"We identify a sub-class of BQP that captures certain structural commonalities
among many quantum algorithms including Shor's algorithms. This class does not
contain all of BQP (e.g. Grover's algorithm does not fall into this class). Our
main result is that any algorithm in this class that measures at most O(log n)
qubits can be simulated by classical randomized polynomial time algorithms.
This does not dequantize Shor's algorithm (as the latter measures n qubits) but
our work also highlights a new potentially hard function for cryptographic
applications.
  Our main technical contribution is (to the best of our knowledge) a new exact
characterization of certain sums of Fourier-type coefficients (with
exponentially many summands)."
"The representation of polynomials by arithmetic circuits evaluating them is
an alternative data structure which allowed considerable progress in polynomial
equation solving in the last fifteen years. We present a circuit based
computation model which captures all known symbolic elimination algorithms in
effective algebraic geometry and show the intrinsically exponential complexity
character of elimination in this complexity model."
"We establish some general schemes relating the computational complexity of a
video game to the presence of certain common elements or mechanics, such as
destroyable paths, collectible items, doors opened by keys or activated by
buttons or pressure plates, etc. Then we apply such ""metatheorems"" to several
video games published between 1980 and 1998, including Pac-Man, Tron, Lode
Runner, Boulder Dash, Deflektor, Mindbender, Pipe Mania, Skweek, Prince of
Persia, Lemmings, Doom, Puzzle Bobble~3, and Starcraft. We obtain both new
results, and improvements or alternative proofs of previously known results."
"In this paper we study the computational complexity of the game of Scrabble.
We prove the PSPACE-completeness of a derandomized model of the game, answering
an open question of Erik Demaine and Robert Hearn."
"We consider computations of a Turing machine under noise that causes
consecutive violations of the machine's transition function. Given a constant
upper bound B on the size of bursts of faults, we construct a Turing machine
M(B) subject to faults that can simulate any fault-free machine under the
condition that bursts are not closer to each other than V for an appropriate V
= O(B^2)."
"We analyze the computational complexity of solving the three ""temporal rift""
puzzles in the recent popular video game Final Fantasy XIII-2. We show that the
Tile Trial puzzle is NP-hard and we provide an efficient algorithm for solving
the Crystal Bonds puzzle. We also show that slight generalizations of the
Crystal Bonds and Hands of Time puzzles are NP-hard."
"In order to find out the limiting speed of solving a specific problem using
computer, this essay provides a method based on information entropy. The
relationship between the minimum computational complexity and information
entropy change is illustrated. A few examples are served as evidence of such
connection. Meanwhile some basic rules of modeling problems are established.
Finally, the nature of solving problems with computer programs is disclosed to
support this theory and a redefinition of information entropy in this filed is
proposed. This will develop a new field of science."
"Proof systems for the Relativized Propositional Calculus are defined and
compared."
"This paper presents a study on two data structures that have been used to
model several problems in computer science: and/or graphs and x-y graphs. An
and/or graph is an acyclic digraph containing a source, such that every vertex
v has a label f(v) \in {and,or} and edges represent dependency relations
between vertices: a vertex labeled and depends on all of its out-neighbors,
while a vertex labeled or depends on only one of its out-neighbors. X-y graphs
are defined as a natural generalization of and/or graphs: every vertex vi of an
x-y graph has a label xi-yi to mean that vi depends on xi of its yi
out-neighbors. We analyze the complexity of the optimization problems
Min-and/or and Min-x-y, which consist of finding solution subgraphs of optimal
weight for and/or and x-y graphs, respectively. Motivated by the large
applicability as well as the hardness of Min-and/or and Min-x-y, we study new
complexity aspects of such problems, both from a classical and a parameterized
point of view. We prove that Min-and/or remains NP-hard even for a very
restricted family of and/or graphs where edges have weight one and or-vertices
have out-degree at most two (apart from other property related to some
in-degrees), and that deciding whether there is a solution subtree with weight
exactly k of a given x-y tree is also NP-hard. We also show that: (i) the
parameterized problem Min-and/or(k, r), which asks whether there is a solution
subgraph of weight at most k where every or-vertex has at most r out-edges with
the same weight, is FPT; (ii) the parameterized problem Min-and/or0(k), whose
domain includes and/or graphs allowing zero-weight edges, is W[2]-hard; (iii)
the parameterized problem Min-x-y(k) is W[1]-hard."
"An extractor is a function that receives some randomness and either
""improves"" it or produces ""new"" randomness. There are statistical and
algorithmical specifications of this notion. We study an algorithmical one
called Kolmogorov extractors and modify it to resource-bounded version of
Kolmogorov complexity. Following Zimand we prove the existence of such objects
with certain parameters. The utilized technique is ""naive"" derandomization: we
replace random constructions employed by Zimand by pseudo-random ones obtained
by Nisan-Wigderson generator."
"We study classes of Dynamic Programming (DP) algorithms which, due to their
algebraic definitions, are closely related to coefficient extraction methods.
DP algorithms can easily be modified to exploit sparseness in the DP table
through memorization. Coefficient extraction techniques on the other hand are
both space-efficient and parallelisable, but no tools have been available to
exploit sparseness.
  We investigate the systematic use of homomorphic hash functions to combine
the best of these methods and obtain improved space-efficient algorithms for
problems including LINEAR SAT, SET PARTITION, and SUBSET SUM. Our algorithms
run in time proportional to the number of nonzero entries of the last segment
of the DP table, which presents a strict improvement over sparse DP. The last
property also gives an improved algorithm for CNF SAT with sparse projections."
"The Church-Turing thesis states that any sufficiently powerful computational
model which captures the notion of algorithm is computationally equivalent to
the Turing machine. This equivalence usually holds both at a computability
level and at a computational complexity level modulo polynomial reductions.
However, the situation is less clear in what concerns models of computation
using real numbers, and no analog of the Church-Turing thesis exists for this
case. Recently it was shown that some models of computation with real numbers
were equivalent from a computability perspective. In particular it was shown
that Shannon's General Purpose Analog Computer (GPAC) is equivalent to
Computable Analysis. However, little is known about what happens at a
computational complexity level. In this paper we shed some light on the
connections between this two models, from a computational complexity level, by
showing that, modulo polynomial reductions, computations of Turing machines can
be simulated by GPACs, without the need of using more (space) resources than
those used in the original Turing computation, as long as we are talking about
bounded computations. In other words, computations done by the GPAC are as
space-efficient as computations done in the context of Computable Analysis."
"With using of multi-nary logic analytic formulas proposition that ""kSAT is in
P and could be solved in $O(n^{3.5})$"" was proved"
"We propose a type system to analyze the time consumed by multi-threaded
imperative programs with a shared global memory, which delineates a class of
safe multi-threaded programs. We demonstrate that a safe multi-threaded program
runs in polynomial time if (i) it is strongly terminating wrt a
non-deterministic scheduling policy or (ii) it terminates wrt a deterministic
and quiet scheduling policy. As a consequence, we also characterize the set of
polynomial time functions. The type system presented is based on the
fundamental notion of data tiering, which is central in implicit computational
complexity. It regulates the information flow in a computation. This aspect is
interesting in that the type system bears a resemblance to typed based
information flow analysis and notions of non-interference. As far as we know,
this is the first characterization by a type system of polynomial time
multi-threaded programs."
"A class of valued constraint satisfaction problems (VCSPs) is characterised
by a valued constraint language, a fixed set of cost functions on a finite
domain. An instance of the problem is specified by a sum of cost functions from
the language with the goal to minimise the sum. This framework includes and
generalises well-studied constraint satisfaction problems (CSPs) and maximum
constraint satisfaction problems (Max-CSPs).
  Our main result is a precise algebraic characterisation of valued constraint
languages whose instances can be solved exactly by the basic linear programming
relaxation. Using this result, we obtain tractability of several novel and
previously widely-open classes of VCSPs, including problems over valued
constraint languages that are: (1) submodular on arbitrary lattices; (2)
bisubmodular (also known as k-submodular) on arbitrary finite domains; (3)
weakly (and hence strongly) tree-submodular on arbitrary trees."
"Hybrid logic with binders is an expressive specification language. Its
satisfiability problem is undecidable in general. If frames are restricted to N
or general linear orders, then satisfiability is known to be decidable, but of
non-elementary complexity. In this paper, we consider monotone hybrid logics
(i.e., the Boolean connectives are conjunction and disjunction only) over N and
general linear orders. We show that the satisfiability problem remains
non-elementary over linear orders, but its complexity drops to
PSPACE-completeness over N. We categorize the strict fragments arising from
different combinations of modal and hybrid operators into NP-complete and
tractable (i.e. complete for NC1or LOGSPACE). Interestingly, NP-completeness
depends only on the fragment and not on the frame. For the cases above NP,
satisfiability over linear orders is harder than over N, while below NP it is
at most as hard. In addition we examine model-theoretic properties of the
fragments in question."
"In this paper, the author puts forward a variation of Feige's Hypothesis,
which claims that it is hard on average refuting Unbalanced Max 3-XOR under
biased assignments on a natural distribution. Under this hypothesis, the author
strengthens the previous known hardness for approximating Minimum Unique Game,
$5/4-\epsilon$, by proving that Min 2-Lin-2 is hard to within $3/2-\epsilon$
and strengthens the previous known hardness for approximating Small Set
Expansion, $4/3-\epsilon$, by proving that Min Bisection is hard to approximate
within $3-\epsilon$. In addition, the author discusses the limitation of this
method to show that it can strengthen the hardness for approximating Minimum
Unique Game to $2-\kappa$ where $\kappa$ is a small absolute positive, but is
short of proving $\omega_k(1)$ hardness for Minimum Unique Game (or Small Set
Expansion), by assuming a generalization of this hypothesis on Unbalanced Max
k-CSP with Samorodnitsky-Trevisan hypergraph predicate."
"It is well known that search SVP is equivalent to optimization SVP. However,
the former reduction from search SVP to optimization SVP by Kannan needs
polynomial times calls to the oracle that solves the optimization SVP. In this
paper, a new rank-preserving reduction is presented with only one call to the
optimization SVP oracle. It is obvious that the new reduction needs the least
calls, and improves Kannan's classical result. What's more, the idea also leads
a similar direct reduction from search CVP to optimization CVP with only one
call to the oracle."
"An integer polynomial $p$ of $n$ variables is called a \emph{threshold gate}
for a Boolean function $f$ of $n$ variables if for all $x \in \zoon$ $f(x)=1$
if and only if $p(x)\geq 0$. The \emph{weight} of a threshold gate is the sum
of its absolute values.
  In this paper we study how large a weight might be needed if we fix some
function and some threshold degree. We prove $2^{\Omega(2^{2n/5})}$ lower bound
on this value. The best previous bound was $2^{\Omega(2^{n/8})}$ (Podolskii,
2009).
  In addition we present substantially simpler proof of the weaker
$2^{\Omega(2^{n/4})}$ lower bound. This proof is conceptually similar to other
proofs of the bounds on weights of nonlinear threshold gates, but avoids a lot
of technical details arising in other proofs. We hope that this proof will help
to show the ideas behind the construction used to prove these lower bounds."
"We show that the Minesweeper game is PP-hard, when the object is to locate
all mines with the highest probability. When the probability of locating all
mines may be infinitesimal, the Minesweeper game is even PSPACE-complete. In
our construction, the player can reveal a boolean circuit in polynomial time,
after guessing an initial square with no surrounding mines, a guess that has 99
percent probability of success. Subsequently, the mines must be located with a
maximum probability of success.
  Furthermore, we show that determining the solvability of a partially
uncovered Minesweeper board is NP-complete with hexagonal and triangular grids
as well as a square grid, extending a similar result for square grids only by
R. Kaye. Actually finding the mines with a maximum probability of success is
again PP-hard or PSPACE-complete respectively.
  Our constructions are in such a way that the number of mines can be computed
in polynomial time and hence a possible mine counter does not provide
additional information. The results are obtained by replacing the dyadic gates
in [3] by two primitives which makes life more easy in this context."
"A novel topological and computational method for 'motion' is described.
Motion is constrained by inequalities in terms of Kolmogorov Complexity.
Causality is obtained as the output of a high-pass filter, passing through only
high values of Kolmogorov Complexity. Motion under the electromagnetic field
described with immediate relationship with Subscript[G, 2] Holonomy group and
its corresponding dense free 2-subgroup. Similar to Causality, Spin emerges as
an immediate and inevitable consequence of high values of Kolmogorov
Complexity. Consequently, the physical laws are nothing but a low-pass filter
for small values of Kolmogorov Complexity."
"Existing definitions of the relativizations of \NCOne, \L\ and \NL\ do not
preserve the inclusions $\NCOne \subseteq \L$, $\NL\subseteq \ACOne$. We start
by giving the first definitions that preserve them. Here for \L\ and \NL\ we
define their relativizations using Wilson's stack oracle model, but limit the
height of the stack to a constant (instead of $\log(n)$). We show that the
collapse of any two classes in $\{\ACZm, \TCZ, \NCOne, \L, \NL\}$ implies the
collapse of their relativizations. Next we exhibit an oracle $\alpha$ that
makes $\ACk(\alpha)$ a proper hierarchy. This strengthens and clarifies the
separations of the relativized theories in [Takeuti, 1995]. The idea is that a
circuit whose nested depth of oracle gates is bounded by $k$ cannot compute
correctly the $(k+1)$ compositions of every oracle function. Finally we develop
theories that characterize the relativizations of subclasses of \Ptime\ by
modifying theories previously defined by the second two authors. A function is
provably total in a theory iff it is in the corresponding relativized class,
and hence the oracle separations imply separations for the relativized
theories."
"This paper proposes a thought experiment to search for efficient bounded
algorithms of NPC problems by machine enumeration. The key contributions are:
  -- On Universal Turing Machines, a program's time complexity should be
characterized as: execution time(n) = loading time(n) + running time(n).
  -- Introduces the concept of bounded algorithms; proposes a comparison based
criterion to decide if a bounded algorithm is inefficient; and establishes the
length upper bound of efficient bounded programs.
  -- Introduces the growth rate characteristic function to evaluate program
complexity, which is more easily machine checkable based on observations.
  -- Raises the theoretical question: if there exists any bounded algorithm
with polynomial execution time for NPC problems."
"Motivated by the pervasiveness of strong inapproximability results for
Max-CSPs, we introduce a relaxed notion of an approximate solution of a
Max-CSP. In this relaxed version, loosely speaking, the algorithm is allowed to
replace the constraints of an instance by some other (possibly real-valued)
constraints, and then only needs to satisfy as many of the new constraints as
possible.
  To be more precise, we introduce the following notion of a predicate $P$
being \emph{useful} for a (real-valued) objective $Q$: given an almost
satisfiable Max-$P$ instance, there is an algorithm that beats a random
assignment on the corresponding Max-$Q$ instance applied to the same sets of
literals. The standard notion of a nontrivial approximation algorithm for a
Max-CSP with predicate $P$ is exactly the same as saying that $P$ is useful for
$P$ itself.
  We say that $P$ is useless if it is not useful for any $Q$. This turns out to
be equivalent to the following pseudo-randomness property: given an almost
satisfiable instance of Max-$P$ it is hard to find an assignment such that the
induced distribution on $k$-bit strings defined by the instance is not
essentially uniform.
  Under the Unique Games Conjecture, we give a complete and simple
characterization of useful Max-CSPs defined by a predicate: such a Max-CSP is
useless if and only if there is a pairwise independent distribution supported
on the satisfying assignments of the predicate. It is natural to also consider
the case when no negations are allowed in the CSP instance, and we derive a
similar complete characterization (under the UGC) there as well.
  Finally, we also include some results and examples shedding additional light
on the approximability of certain Max-CSPs."
"We show that given a satisfiable instance of the 2-to-1 Label Cover problem,
it is NP-hard to find a $(23/24 + \eps)$-satisfying assignment."
"A counting constraint satisfaction problem (#CSP) asks for the number of ways
to satisfy a given list of constraints, drawn from a fixed constraint language
\Gamma. We study how hard it is to evaluate this number approximately. There is
an interesting partial classification, due to Dyer, Goldberg, Jalsenius and
Richerby, of Boolean constraint languages when the degree of instances is
bounded by d>=3 - every variable appears in at most d constraints - under the
assumption that ""pinning"" is allowed as part of the instance. We study the d=2
case under the stronger assumption that ""variable weights"" are allowed as part
of the instance. We give a dichotomy: in each case, either the #CSP is
tractable, or one of two important open problems, #BIS or #PM, reduces to the
#CSP."
"We prove a complexity dichotomy theorem for Holant problems over an arbitrary
set of complex-valued symmetric constraint functions F on Boolean variables.
This extends and unifies all previous dichotomies for Holant problems on
symmetric constraint functions (taking values without a finite modulus). We
define and characterize all symmetric vanishing signatures. They turned out to
be essential to the complete classification of Holant problems. The dichotomy
theorem has an explicit tractability criterion expressible in terms of
holographic transformations. A Holant problem defined by a set of constraint
functions F is solvable in polynomial time if it satisfies this tractability
criterion, and is #P-hard otherwise. The tractability criterion can be
intuitively stated as follows: A set F is tractable if (1) every function in F
has arity at most two, or (2) F is transformable to an affine type, or (3) F is
transformable to a product type, or (4) F is vanishing, combined with the right
type of binary functions, or (5) F belongs to a special category of vanishing
type Fibonacci gates. The proof of this theorem utilizes many previous
dichotomy theorems on Holant problems and Boolean #CSP. Holographic
transformations play an indispensable role as both a proof technique and in the
statement of the tractability criterion."
"The factor graph of an instance of a symmetric constraint satisfaction
problem on n Boolean variables and m constraints (CSPs such as k-SAT, k-AND,
k-LIN) is a bipartite graph describing which variables appear in which
constraints. The factor graph describes the instance up to the polarity of the
variables, and hence there are up to 2km instances of the CSP that share the
same factor graph. It is well known that factor graphs with certain structural
properties make the underlying CSP easier to either solve exactly (e.g., for
tree structures) or approximately (e.g., for planar structures). We are
interested in the following question: is there a factor graph for which if one
can solve every instance of the CSP with this particular factor graph, then one
can solve every instance of the CSP regardless of the factor graph (and
similarly, for approximation)? We call such a factor graph universal. As one
needs different factor graphs for different values of n and m, this gives rise
to the notion of a family of universal factor graphs. We initiate a systematic
study of universal factor graphs, and present some results for max-kSAT. Our
work has connections with the notion of preprocessing as previously studied for
closest codeword and closest lattice-vector problems, with proofs for the PCP
theorem, and with tests for the long code. Many questions remain open."
"We revisit various PTAS's (Polynomial Time Approximation Schemes) for
minimization versions of dense problems, and show that they can be performed
with sublinear query complexity. This means that not only do we obtain a
(1+eps)-approximation to the NP-Hard problems in polynomial time, but also
avoid reading the entire input. This setting is particularly advantageous when
the price of reading parts of the input is high, as is the case, for examples,
where humans provide the input. Trading off query complexity with approximation
is the raison d'etre of the field of learning theory, and of the ERM (Empirical
Risk Minimization) setting in particular. A typical ERM result, however, does
not deal with computational complexity. We discuss two particular problems for
which (a) it has already been shown that sublinear querying is sufficient for
obtaining a (1 + eps)-approximation using unlimited computational power (an ERM
result), and (b) with full access to input, we could get a
(1+eps)-approximation in polynomial time (a PTAS). Here we show that neither
benefit need be sacrificed. We get a PTAS with efficient query complexity."
"We establish tight bounds on the amount on nonuniformity that is necessary
for extracting a string with randomness rate 1 from a single source of
randomness with lower randomness rate. More precisely, as instantiations of
more general results, we show that while O(1) amount of advice regarding the
source is not enough for extracting a string with randomness rate 1 from a
source string with constant subunitary random rate, \omega(1) amount of advice
is."
"We study the streaming complexity of the membership problem of 1-turn-Dyck2
and Dyck2 when there are a few errors in the input string.
  1-turn-Dyck2 with errors: We prove that there exists a randomized one-pass
algorithm that given x checks whether there exists a string x' in 1-turn-Dyck2
such that x is obtained by flipping at most $k$ locations of x' using:
  - O(k log n) space, O(k log n) randomness, and poly(k log n) time per item
and with error at most 1/poly(n). - O(k^{1+epsilon} + log n) space for every 0
<= epsilon <= 1, O(log n) randomness, O(polylog(n) + poly(k)) time per item,
with error at most 1/8.
  Here, we also prove that any randomized one-pass algorithm that makes error
at most k/n requires at least Omega(k log(n/k)) space to accept strings which
are exactly k-away from strings in 1-turn-Dyck2 and to reject strings which are
exactly (k+2)-away from strings in 1-turn-Dyck2. Since 1-turn-Dyck2 and the
Hamming Distance problem are closely related we also obtain new upper and lower
bounds for this problem.
  Dyck2 with errors: We prove that there exists a randomized one-pass algorithm
that given x checks whether there exists a string x' in Dyck2 such that x is
obtained from x' by changing (in some restricted manner) at most k positions
using:
  - O(k log n + sqrt(n log n)) space, O(k log n) randomness, poly(k log n) time
per element and with error at most 1/poly(n). - O(k^(1+epsilon)+ sqrt(n log n))
space for every 0 <= epsilon <= 1, O(log n) randomness, O(polylog(n) + poly(k))
time per element, with error at most 1/8."
"The PCP Theorem is one of the most stunning results in computational
complexity theory, a culmination of a series of results regarding proof
checking it exposes some deep structure of computational problems. As a
surprising side-effect, it also gives strong non-approximability results. In
this paper we initiate the study of proof checking within the scope of
Parameterized Complexity. In particular we adapt and extend the PCP[n log log
n, n log log n] result of Feige et al. to several parameterized classes, and
discuss some corollaries."
"The Traveling Salesman Problem is one of the most studied problems in
computational complexity and its approximability has been a long standing open
question. Currently, the best known inapproximability threshold known is
220/219 due to Papadimitriou and Vempala. Here, using an essentially different
construction and also relying on the work of Berman and Karpinski on bounded
occurrence CSPs, we give an alternative and simpler inapproximability proof
which improves the bound to 185/184."
"A link between Kolmogorov Complexity and geometry is uncovered. A similar
concept of projection and vector decomposition is described for Kolmogorov
Complexity. By using a simple approximation to the Kolmogorov Complexity, coded
in Mathematica, the derived formulas are tested and used to study the geometry
of Light Cone."
"Assuming the Unique Games Conjecture, we show strong inapproximability
results for two natural vertex deletion problems on directed graphs: for any
integer $k\geq 2$ and arbitrary small $\epsilon > 0$, the Feedback Vertex Set
problem and the DAG Vertex Deletion problem are inapproximable within a factor
$k-\epsilon$ even on graphs where the vertices can be almost partitioned into
$k$ solutions. This gives a more structured and therefore stronger UGC-based
hardness result for the Feedback Vertex Set problem that is also simpler
(albeit using the ""It Ain't Over Till It's Over"" theorem) than the previous
hardness result.
  In comparison to the classical Feedback Vertex Set problem, the DAG Vertex
Deletion problem has received little attention and, although we think it is a
natural and interesting problem, the main motivation for our inapproximability
result stems from its relationship with the classical Discrete Time-Cost
Tradeoff Problem. More specifically, our results imply that the deadline
version is NP-hard to approximate within any constant assuming the Unique Games
Conjecture. This explains the difficulty in obtaining good approximation
algorithms for that problem and further motivates previous alternative
approaches such as bicriteria approximations."
"We show that the bilinear complexity of multiplication in a non-split
quaternion algebra over a field of characteristic distinct from 2 is 8. This
question is motivated by the problem of characterising algebras of almost
minimal rank studied by Blaeser and de Voltaire in [1].
  This paper is a translation of a report submitted by the author to the XI
international seminar ""Discrete mathematics and applications"" (in Russian)."
"The present work proves that P=NP. The proof, presented in this work, is a
constructive one: the program of a polynomial time deterministic multi-tape
Turing machine M_ExistsAcceptingPath, that determines if there exists an
accepting computational path of a polynomial time non-deterministic single-tape
Turing machine M_NP, is constructed (machine M_ExistsAcceptingPath is different
for each Turing machine M_NP). Machine M_ExistsAcceptingPath is based on
reduction to problem LP (linear programming) instead of reduction to problem
3-CNF-SAT which is commonly used. In more detail, machine M_AcceptingPath uses
a reduction of the initial string problem to another string problem TCPE
(defined in the paper) that is NP-complete and decidable in polynomial time.
The time complexity of machine M_ExistsAcceptingPath is O(t(n)^{272}) wherein
t(n) is an upper bound of the time complexity of machine M_NP."
"We study the complexity of approximately solving the weighted counting
constraint satisfaction problem #CSP(F). In the conservative case, where F
contains all unary functions, there is a classification known for the case in
which the domain of functions in F is Boolean. In this paper, we give a
classification for the more general problem where functions in F have an
arbitrary finite domain. We define the notions of weak log-modularity and weak
log-supermodularity. We show that if F is weakly log-modular, then #CSP(F)is in
FP. Otherwise, it is at least as difficult to approximate as #BIS, the problem
of counting independent sets in bipartite graphs. #BIS is complete with respect
to approximation-preserving reductions for a logically-defined complexity class
#RHPi1, and is believed to be intractable. We further sub-divide the #BIS-hard
case. If F is weakly log-supermodular, then we show that #CSP(F) is as easy as
a (Boolean) log-supermodular weighted #CSP. Otherwise, we show that it is
NP-hard to approximate. Finally, we give a full trichotomy for the arity-2
case, where #CSP(F) is in FP, or is #BIS-equivalent, or is equivalent in
difficulty to #SAT, the problem of approximately counting the satisfying
assignments of a Boolean formula in conjunctive normal form. We also discuss
the algorithmic aspects of our classification."
"We prove that uniform circuits of size n can be evaluated in space O(n/log
n). Thus, Space(O(n)) is not in uniform Size(o(n*log n)). For uniformity, we
only require that the circuit is O(n/log n)-Space uniform. We also generalize
the construction to prove that a machine with O(n^delta) (delta<1) internal
storage and O(2^n^delta) length single-bit-access read-write RAM that does only
O(n) RAM reads (1 bit per read) can be simulated in space O(n * log log n / log
n)."
"An O(Nn^2 + n^2) time algorithm to enumerate all N models of a Boolean 2-CNF
with n variables is presented. Using don't care symbols the models are output
in clusters rather than one by one. Computer experiments confirm the high
efficiency of the method."
"We study the exponential time complexity of approximate counting satisfying
assignments of CNFs. We reduce the problem to deciding satisfiability of a CNF.
Our reduction preserves the number of variables of the input formula and thus
also preserves the exponential complexity of approximate counting.
  Our algorithm is also similar to an algorithm which works particular well in
practice for which however no approximation guarantee was known. Towards an
analysis of our reduction we provide a new inequality similar to the
Bonami-Beckner hypercontractive inequality."
"In 1990 Subramanian defined the complexity class CC as the set of problems
log-space reducible to the comparator circuit value problem (CCV). He and Mayr
showed that NL \subseteq CC \subseteq P, and proved that in addition to CCV
several other problems are complete for CC, including the stable marriage
problem, and finding the lexicographically first maximal matching in a
bipartite graph. We are interested in CC because we conjecture that it is
incomparable with the parallel class NC which also satisfies NL \subseteq NC
\subseteq P, and note that this conjecture implies that none of the CC-complete
problems has an efficient polylog time parallel algorithm. We provide evidence
for our conjecture by giving oracle settings in which relativized CC and
relativized NC are incomparable.
  We give several alternative definitions of CC, including (among others) the
class of problems computed by uniform polynomial-size families of comparator
circuits supplied with copies of the input and its negation, the class of
problems AC^0-reducible to CCV, and the class of problems computed by uniform
AC^0 circuits with CCV gates. We also give a machine model for CC, which
corresponds to its characterization as log-space uniform polynomial-size
families of comparator circuits. These various characterizations show that CC
is a robust class. The main technical tool we employ is universal comparator
circuits.
  Other results include a simpler proof of NL \subseteq CC, and an explanation
of the relation between the Gale-Shapley algorithm and Subramanian's algorithm
for stable marriage.
  This paper continues the previous work of Cook, L\^e and Ye which focused on
Cook-Nguyen style uniform proof complexity, answering several open questions
raised in that paper."
"We study the computational and structural aspects of countable
two-dimensional SFTs and other subshifts. Our main focus is on the topological
derivatives and subpattern posets of these objects, and our main results are
constructions of two-dimensional countable subshifts with interesting
properties. We present an SFT whose iterated derivatives are maximally complex
from the computational point of view, a sofic shift whose subpattern poset
contains an infinite descending chain, a family of SFTs whose finite subpattern
posets contain arbitrary finite posets, and a natural example of an SFT with
infinite Cantor-Bendixon rank."
"Kolmogorov-Martin-Lof Randomness concept is extended from computable to
enumerable distributions. This allows definitions of various other properties,
such as mutual information in infinite sequences. Enumerable distributions (as
well as distributions faced in some finite multi-party settings) are
semimeasures; handling those requires some amount of care."
"We consider the problem of approximating the partition function of the
hard-core model on planar graphs of degree at most 4. We show that when the
activity lambda is sufficiently large, there is no fully polynomial randomised
approximation scheme for evaluating the partition function unless NP=RP. The
result extends to a nearby region of the parameter space in a more general
two-state spin system with three parameters. We also give a polynomial-time
randomised approximation scheme for the logarithm of the partition function."
"Compressed sensing is a technique for finding sparse solutions to
underdetermined linear systems. This technique relies on properties of the
sensing matrix such as the restricted isometry property. Sensing matrices that
satisfy this property with optimal parameters are mainly obtained via
probabilistic arguments. Deciding whether a given matrix satisfies the
restricted isometry property is a non-trivial computational problem. Indeed, we
show in this paper that restricted isometry parameters cannot be approximated
in polynomial time within any constant factor under the assumption that the
hidden clique problem is hard. Moreover, on the positive side we propose an
improvement on the brute-force enumeration algorithm for checking the
restricted isometry property."
"We consider a natural variation of the concept of stabbing a segment by a
simple polygon: a segment is stabbed by a simple polygon $\mathcal{P}$ if at
least one of its two endpoints is contained in $\mathcal{P}$. A segment set $S$
is stabbed by $\mathcal{P}$ if every segment of $S$ is stabbed by
$\mathcal{P}$. We show that if $S$ is a set of pairwise disjoint segments, the
problem of computing the minimum perimeter polygon stabbing $S$ can be solved
in polynomial time. We also prove that for general segments the problem is
NP-hard. Further, an adaptation of our polynomial-time algorithm solves an open
problem posed by L\""offler and van Kreveld [Algorithmica 56(2), 236--269
(2010)] about finding a maximum perimeter convex hull for a set of imprecise
points modeled as line segments."
"The NP-hard Metric Dimension problem is to decide for a given graph G and a
positive integer k whether there is a vertex subset of size at most k that
separates all vertex pairs in G. Herein, a vertex v separates a pair {u,w} if
the distance (length of a shortest path) between v and u is different from the
distance of v and w. We give a polynomial-time computable reduction from the
Bipartite Dominating Set problem to Metric Dimension on maximum degree three
graphs such that there is a one-to-one correspondence between the solution sets
of both problems. There are two main consequences of this: First, it proves
that Metric Dimension on maximum degree three graphs is W[2]-complete with
respect to the parameter k. This answers an open question concerning the
parameterized complexity of Metric Dimension posed by D\'iaz et al. [ESA'12]
and already mentioned by Lokshtanov [Dagstuhl seminar, 2009]. Additionally, it
implies that Metric Dimension cannot be solved in n^{o(k)} time, unless the
assumption FPT \neq W[1] fails. This proves that a trivial n^{O(k)} algorithm
is probably asymptotically optimal.
  Second, as Bipartite Dominating Set is inapproximable within o(log n), it
follows that Metric Dimension on maximum degree three graphs is also
inapproximable by a factor of o(log n), unless NP=P. This strengthens the
result of Hauptmann et al. [JDA 2012] who proved APX-hardness on bounded-degree
graphs."
"This work is concerned with the proof-complexity of certifying that
optimization problems do \emph{not} have good solutions. Specifically we
consider bounded-degree ""Sum of Squares"" (SOS) proofs, a powerful algebraic
proof system introduced in 1999 by Grigoriev and Vorobjov. Work of Shor,
Lasserre, and Parrilo shows that this proof system is automatizable using
semidefinite programming (SDP), meaning that any $n$-variable degree-$d$ proof
can be found in time $n^{O(d)}$. Furthermore, the SDP is dual to the well-known
Lasserre SDP hierarchy, meaning that the ""$d/2$-round Lasserre value"" of an
optimization problem is equal to the best bound provable using a degree-$d$ SOS
proof. These ideas were exploited in a recent paper by Barak et al.\ (STOC
2012) which shows that the known ""hard instances"" for the Unique-Games problem
are in fact solved close to optimally by a constant level of the Lasserre SDP
hierarchy.
  We continue the study of the power of SOS proofs in the context of difficult
optimization problems. In particular, we show that the Balanced-Separator
integrality gap instances proposed by Devanur et al.\ can have their optimal
value certified by a degree-4 SOS proof. The key ingredient is an SOS proof of
the KKL Theorem. We also investigate the extent to which the Khot--Vishnoi
Max-Cut integrality gap instances can have their optimum value certified by an
SOS proof. We show they can be certified to within a factor .952 ($> .878$)
using a constant-degree proof. These investigations also raise an interesting
mathematical question: is there a constant-degree SOS proof of the Central
Limit Theorem?"
"Manipulation, bribery, and control are well-studied ways of changing the
outcome of an election. Many voting rules are, in the general case,
computationally resistant to some of these manipulative actions. However when
restricted to single-peaked electorates, these rules suddenly become easy to
manipulate. Recently, Faliszewski, Hemaspaandra, and Hemaspaandra studied the
computational complexity of strategic behavior in nearly single-peaked
electorates. These are electorates that are not single-peaked but close to it
according to some distance measure.
  In this paper we introduce several new distance measures regarding
single-peakedness. We prove that determining whether a given profile is nearly
single-peaked is NP-complete in many cases. For one case we present a
polynomial-time algorithm. In case the single-peaked axis is given, we show
that determining the distance is always possible in polynomial time.
Furthermore, we explore the relations between the new notions introduced in
this paper and existing notions from the literature."
"We consider the problem of constructing explicit Hitting sets for
Combinatorial Shapes, a class of statistical tests first studied by Gopalan,
Meka, Reingold, and Zuckerman (STOC 2011). These generalize many well-studied
classes of tests, including symmetric functions and combinatorial rectangles.
Generalizing results of Linial, Luby, Saks, and Zuckerman (Combinatorica 1997)
and Rabani and Shpilka (SICOMP 2010), we construct hitting sets for
Combinatorial Shapes of size polynomial in the alphabet, dimension, and the
inverse of the error parameter. This is optimal up to polynomial factors. The
best previous hitting sets came from the Pseudorandom Generator construction of
Gopalan et al., and in particular had size that was quasipolynomial in the
inverse of the error parameter.
  Our construction builds on natural variants of the constructions of Linial et
al. and Rabani and Shpilka. In the process, we construct fractional perfect
hash families and hitting sets for combinatorial rectangles with stronger
guarantees. These might be of independent interest."
"The material of the article is devoted to the most complicated and
interesting problem -- a problem of P = NP?. This research was presented to
mathematical community in Hyderabad during International Congress of
Mathematicians. But there it was published in a very brief form, so this
article is an attempt to give those, who are interested in the problem, my
reasoning on the theme. It is not a proof in full, because it is very difficult
to prove something, which is not provable, but it seems that these reasoning
will help us to understand the problem of the combinatorial explosion more
deeply and to realize in full all the problems to which we are going because of
the combinatorial explosion. Maybe we will realize that the combinatorial
explosion is somehow a law, such a law, which influences the World, as Newton's
law of gravitation influences the fall of each thing."
"This paper talks about that monotone circuit is P-Complete.
  Decision problem that include P-Complete is mapping that classify input with
a similar property. Therefore equivalence relation of input value is important
for computation. But monotone circuit cannot compute the equivalence relation
of the value because monotone circuit can compute only monotone function.
Therefore, I make the value constraint explicitly in the input and monotone
circuit can compute equivalence relation. As a result, we can compute
P-Complete problem with monotone circuit. We can reduce implicit value
constraint to explicit with logarithm space. Therefore, monotone circuit is
P-Complete."
"A shuffle of two strings is formed by interleaving the characters into a new
string, keeping the characters of each string in order. A string is a square if
it is a shuffle of two identical strings. There is a known polynomial time
dynamic programming algorithm to determine if a given string z is the shuffle
of two given strings x,y; however, it has been an open question whether there
is a polynomial time algorithm to determine if a given string z is a square. We
resolve this by proving that this problem is NP-complete via a many-one
reduction from 3- Partition."
"We introduce a model of probabilistic debate checking, where a silent
resource-bounded verifier reads a dialogue about the membership of the string
in the language under consideration between a prover and a refuter. Our model
combines and generalizes the concepts of one-way interactive proof systems,
games of incomplete information, and probabilistically checkable
complete-information debate systems. We consider debates of partial and zero
information, where the prover is prevented from seeing some or all of the
messages of the refuter, as well as those of complete information. The classes
of languages with debates checkable by verifiers operating under severe bounds
on the memory and randomness are studied.
  We give full characterizations of versions of these classes corresponding to
simultaneous bounds of O(1) space and O(1) random bits, and of logarithmic
space and polynomial time. It turns out that constant-space verifiers, which
can only check complete-information debates for regular languages
deterministically, can check for membership in any language in P when allowed
to use a constant number of random bits. Similar increases also occur for zero-
and partial- information debates, from NSPACE(n) to PSPACE, and from E to
EXPTIME, respectively. Adding logarithmic space to these constant-randomness
verifiers does not change their power. When logspace debate checkers are
restricted to run in polynomial time without a bound on the number of random
bits, the class of debatable languages equals PSPACE for all debate types. We
also present a result on the hardness of approximating the quantified max word
problem for matrices that is a corollary of this characterization."
"Among all sequences that satisfy a divide-and-conquer recurrence, the
sequences that are rational with respect to a numeration system are certainly
the most immediate and most essential. Nevertheless, until recently they have
not been studied from the asymptotic standpoint. We show how a mechanical
process permits to compute their asymptotic expansion. It is based on linear
algebra, with Jordan normal form, joint spectral radius, and dilation
equations. The method is compared with the analytic number theory approach,
based on Dirichlet series and residues, and new ways to compute the Fourier
series of the periodic functions involved in the expansion are developed. The
article comes with an extended bibliography."
"We study the problem of conjunctive query evaluation relative to a class of
queries; this problem is formulated here as the relational homomorphism problem
relative to a class of structures A, wherein each instance must be a pair of
structures such that the first structure is an element of A. We present a
comprehensive complexity classification of these problems, which strongly links
graph-theoretic properties of A to the complexity of the corresponding
homomorphism problem. In particular, we define a binary relation on graph
classes, which is a preorder, and completely describe the resulting hierarchy
given by this relation. This relation is defined in terms of a notion which we
call graph deconstruction and which is a variant of the well-known notion of
tree decomposition. We then use this hierarchy of graph classes to infer a
complexity hierarchy of homomorphism problems which is comprehensive up to a
computationally very weak notion of reduction, namely, a parameterized version
of quantifier-free first-order reduction. In doing so, we obtain a
significantly refined complexity classification of homomorphism problems, as
well as a unifying, modular, and conceptually clean treatment of existing
complexity classifications. We then present and develop the theory of
Ehrenfeucht-Fraisse-style pebble games which solve the homomorphism problems
where the cores of the structures in A have bounded tree depth. Finally, we use
our framework to classify the complexity of model checking existential
sentences having bounded quantifier rank."
"Current discrete randomness and information conservation inequalities are
over total recursive functions, i.e. restricted to deterministic processing.
This restriction implies that an algorithm can break algorithmic randomness
conservation inequalities. We address this issue by proving tight bounds of
randomness and information conservation with respect to recursively enumerable
transformations, i.e. processing by algorithms. We also show conservation of
randomness of finite strings with respect to enumerable distributions, i.e.
semicomputable semi-measures."
"There is a number of known NP class problems, and majority of them have been
shown to be equivalent to others. In particular now it is clear that
construction of a Gr\""{o}bner basis (or Buchberger algorithm) must be one of
equivalent problems, but there was no example. In the following paper the
reduction is constructed."
"This survey describes, at an introductory level, the algebraic complexity
framework originally proposed by Leslie Valiant in 1979, and some of the
insights that have been obtained more recently."
"A low-degree test is a collection of simple, local rules for checking the
proximity of an arbitrary function to a low-degree polynomial. Each rule
depends on the function's values at a small number of places. If a function
satisfies many rules then it is close to a low-degree polynomial. Low-degree
tests play an important role in the development of probabilistically checkable
proofs.
  In this paper we present two improvements to the efficiency of low-degree
tests. Our first improvement concerns the smallest field size over which a
low-degree test can work. We show how to test that a function is a degree $d$
polynomial over prime fields of size only $d+2$.
  Our second improvement shows a better efficiency of the low-degree test of
Rubinfeld and Sudan (Proc. SODA 1992) than previously known. We show concrete
applications of this improvement via the notion of ""locally checkable codes"".
This improvement translates into better tradeoffs on the size versus probe
complexity of probabilistically checkable proofs than previously known."
"A proof system for a language L is a function f such that Range(f) is exactly
L. In this paper, we look at proofsystems from a circuit complexity point of
view and study proof systems that are computationally very restricted. The
restriction we study is: they can be computed by bounded fanin circuits of
constant depth (NC^0), or of O(log log n) depth but with O(1) alternations
(polylog AC^0). Each output bit depends on very few input bits; thus such proof
systems correspond to a kind of local error-correction on a theorem-proof pair.
  We identify exactly how much power we need for proof systems to capture all
regular languages. We show that all regular language have polylog AC^0 proof
systems, and from a previous result (Beyersdorff et al, MFCS 2011, where NC^0
proof systems were first introduced), this is tight. Our technique also shows
that MAJ has polylog AC^0 proof system. We explore the question of whether TAUT
has NC^0 proof systems. Addressing this question about 2TAUT, and since 2TAUT
is closely related to reachability in graphs, we ask the same question about
Reachability. We show that both Undirected Reachability and Directed
UnReachability have NC^0 proof systems, but Directed Reachability is still
open.
  In the context of how much power is needed for proof systems for languages in
NP, we observe that proof systems for a good fraction of languages in NP do not
need the full power of AC^0; they have SAC^0 or coSAC^0 proof systems."
"We show improved NP-hardness of approximating Ordering Constraint
Satisfaction Problems (OCSPs). For the two most well-studied OCSPs, Maximum
Acyclic Subgraph and Maximum Betweenness, we prove inapproximability of
$14/15+\epsilon$ and $1/2+\epsilon$.
  An OCSP is said to be approximation resistant if it is hard to approximate
better than taking a uniformly random ordering. We prove that the Maximum
Non-Betweenness Problem is approximation resistant and that there are width-$m$
approximation-resistant OCSPs accepting only a fraction $1 / (m/2)!$ of
assignments. These results provide the first examples of
approximation-resistant OCSPs subject only to P $\neq$ \NP."
"1-in-3 SAT is an NP-complete variant of 3-SAT\ where a ""clause"" is satisfied
iff exactly one of its three literal is satisfied. We present here an exact
algorithm solving \oit\ in time $O^*(1.260^n)$."
"We show that for any Boolean function f on {0,1}^n, the bounded-error quantum
communication complexity of XOR functions $f\circ \oplus$ satisfies that
$Q_\epsilon(f\circ \oplus) = O(2^d (\log\|\hat f\|_{1,\epsilon} + \log
\frac{n}{\epsilon}) \log(1/\epsilon))$, where d is the F2-degree of f, and
$\|\hat f\|_{1,\epsilon} = \min_{g:\|f-g\|_\infty \leq \epsilon} \|\hat f\|_1$.
This implies that the previous lower bound $Q_\epsilon(f\circ \oplus) =
\Omega(\log\|\hat f\|_{1,\epsilon})$ by Lee and Shraibman \cite{LS09} is tight
for f with low F2-degree. The result also confirms the quantum version of the
Log-rank Conjecture for low-degree XOR functions. In addition, we show that the
exact quantum communication complexity satisfies $Q_E(f) = O(2^d \log \|\hat
f\|_0)$, where $\|\hat f\|_0$ is the number of nonzero Fourier coefficients of
f. This matches the previous lower bound $Q_E(f(x,y)) = \Omega(\log rank(M_f))$
by Buhrman and de Wolf \cite{BdW01} for low-degree XOR functions."
"A new class of functions is presented. The structure of the algorithm,
particularly the selection criteria (branching), is used to define the
fundamental property of the new class. The most interesting property of the new
functions is that instances are easy to compute but if input to the function is
vague the description of a function is exponentially complex. This property
puts a new light on randomness especially on the random oracle model with a
couple of practical examples of random oracle implementation. Consequently,
there is a new interesting viewpoint on computational complexity in general."
"Curve samplers are sampling algorithms that proceed by viewing the domain as
a vector space over a finite field, and randomly picking a low-degree curve in
it as the sample. Curve samplers exhibit a nice property besides the sampling
property: the restriction of low-degree polynomials over the domain to the
sampled curve is still low-degree. This property is often used in combination
with the sampling property and has found many applications, including PCP
constructions, local decoding of codes, and algebraic PRG constructions.
  The randomness complexity of curve samplers is a crucial parameter for its
applications. It is known that (non-explicit) curve samplers using $O(\log
N+\log(1/\delta))$ random bits exist, where $N$ is the domain size and $\delta$
is the confidence error. The question of explicitly constructing
randomness-efficient curve samplers was first raised in \cite{TU06} where they
obtained curve samplers with near-optimal randomness complexity.
  We present an explicit construction of low-degree curve samplers with {\em
optimal} randomness complexity (up to a constant factor), sampling curves of
degree $\left(m\log_q(1/\delta)\right)^{O(1)}$ in $\mathbb{F}_q^m$. Our
construction is a delicate combination of several components, including
extractor machinery, limited independence, iterated sampling, and
list-recoverable codes."
"To study groups with small Dehn's function, Olshanskii and Sapir developed a
new invariant of bipartite chords diagrams and applied it to hub-free
realization of S-machines. In this paper we consider this new invariant
together with groups constructed from S-machines containing the hub relation.
The idea is to study the links between the topology of the asymptotic cones and
polynomial time computations. Indeed it is known that the topology of such
metric space depends on diagrams without hubs that do not correspond to the
computations of the considered S-machine. This work gives sufficient conditions
that avoid this misbehaviour, but as we shall see the method has a significant
drawback."
"Complexity measures are designed to capture complex behavior and quantify
*how* complex, according to that measure, that particular behavior is. It can
be expected that different complexity measures from possibly entirely different
fields are related to each other in a non-trivial fashion. Here we study small
Turing machines (TMs) with two symbols, and two and three states. For any
particular such machine $\tau$ and any particular input $x$ we consider what we
call the 'space-time' diagram which is the collection of consecutive tape
configurations of the computation $\tau(x)$. In our setting, we define fractal
dimension of a Turing machine as the limiting fractal dimension of the
corresponding space-time diagram. It turns out that there is a very strong
relation between the fractal dimension of a Turing machine of the
above-specified type and its runtime complexity. In particular, a TM with three
states and two colors runs in at most linear time iff its dimension is 2, and
its dimension is 1 iff it runs in super-polynomial time and it uses polynomial
space. If a TM runs in time $O(x^n)$ we have empirically verified that the
corresponding dimension is $(n+1)/n$, a result that we can only partially
prove. We find the results presented here remarkable because they relate two
completely different complexity measures: the geometrical fractal dimension on
the one side versus the time complexity of a computation on the other side."
"The fermionant can be seen as a generalization of both the permanent (for
$k=-1$) and the determinant. We demonstrate that it is VNP-complete for most
cases. Furthermore it is #P-complete for the cases. The immanant is also a
generalization of the permanent (for a Young diagram with a single line) and of
the determinant (when the Young diagram is a column). We demonstrate that the
immanant of any family of Young diagrams with bounded width and at least n
boxes at the right of the first column is VNP-complete."
"We derive new time-space tradeoff lower bounds and algorithms for exactly
computing statistics of input data, including frequency moments, element
distinctness, and order statistics, that are simple to calculate for sorted
data. We develop a randomized algorithm for the element distinctness problem
whose time T and space S satisfy T in O (n^{3/2}/S^{1/2}), smaller than
previous lower bounds for comparison-based algorithms, showing that element
distinctness is strictly easier than sorting for randomized branching programs.
This algorithm is based on a new time and space efficient algorithm for finding
all collisions of a function f from a finite set to itself that are reachable
by iterating f from a given set of starting points. We further show that our
element distinctness algorithm can be extended at only a polylogarithmic factor
cost to solve the element distinctness problem over sliding windows, where the
task is to take an input of length 2n-1 and produce an output for each window
of length n, giving n outputs in total. In contrast, we show a time-space
tradeoff lower bound of T in Omega(n^2/S) for randomized branching programs to
compute the number of distinct elements over sliding windows. The same lower
bound holds for computing the low-order bit of F_0 and computing any frequency
moment F_k, k neq 1. This shows that those frequency moments and the decision
problem F_0 mod 2 are strictly harder than element distinctness. We complement
this lower bound with a T in O(n^2/S) comparison-based deterministic RAM
algorithm for exactly computing F_k over sliding windows, nearly matching both
our lower bound for the sliding-window version and the comparison-based lower
bounds for the single-window version. We further exhibit a quantum algorithm
for F_0 over sliding windows with T in O(n^{3/2}/S^{1/2}). Finally, we consider
the computations of order statistics over sliding windows."
"Given a family of subsets $\mathcal S$ over a set of elements~$X$ and two
integers~$p$ and~$k$, Max k-Set Cover consists of finding a subfamily~$\mathcal
T \subseteq \mathcal S$ of cardinality at most~$k$, covering at least~$p$
elements of~$X$. This problem is W[2]-hard when parameterized by~$k$, and FPT
when parameterized by $p$. We investigate the parameterized approximability of
the problem with respect to parameters~$k$ and~$p$. Then, we show that Max
Sat-k, a satisfiability problem generalizing Max k-Set Cover, is also FPT with
respect to parameter~$p$."
"We give deterministic black-box polynomial identity testing algorithms for
multilinear read-once oblivious algebraic branching programs (ROABPs), in
n^(lg^2 n) time. Further, our algorithm is oblivious to the order of the
variables. This is the first sub-exponential time algorithm for this model.
Furthermore, our result has no known analogue in the model of read-once
oblivious boolean branching programs with unknown order, as despite recent work
there is no known pseudorandom generator for this model with sub-polynomial
seed-length (for unbounded-width branching programs).
  This result extends and generalizes the result of Forbes and Shpilka that
obtained a n^(lg n)-time algorithm when given the order. We also extend and
strengthen the work of Agrawal, Saha and Saxena that gave a black-box algorithm
running in time exp((lg n)^d) for set-multilinear formulas of depth d. We note
that the model of multilinear ROABPs contains the model of set-multilinear
algebraic branching programs, which itself contains the model of
set-multilinear formulas of arbitrary depth. We obtain our results by
recasting, and improving upon, the ideas of Agrawal, Saha and Saxena. We phrase
the ideas in terms of rank condensers and Wronskians, and show that our results
improve upon the classical multivariate Wronskian, which may be of independent
interest.
  In addition, we give the first n^(lglg n) black-box polynomial identity
testing algorithm for the so called model of diagonal circuits. This model,
introduced by Saxena has recently found applications in the work of Mulmuley,
as well as in the work of Gupta, Kamath, Kayal, Saptharishi. Previously work
had given n^(lg n)-time algorithms for this class. More generally, our result
holds for any model computing polynomials whose partial derivatives (of all
orders) span a low dimensional linear space."
"This paper illustrates the richness of the concept of regular sets of time
bounds and demonstrates its application to problems of computational
complexity. There is a universe of bounds whose regular subsets allow to
represent several time complexity classes of common interest and are linearly
ordered with respect to the confinality relation which implies the inclusion
between the corresponding complexity classes. By means of classical results of
complexity theory, the separation of determinism from nondeterminism is
possible for a variety of sets of bounds below $n\cdot\log^*(n)$. The system of
all regular bound sets ordered by confinality allows the order-isomorphic
embedding of, e.g., the ordered set of real numbers or the Cantor discontinuum."
"The time complexity of the presented in 2013 by the author small universal
Petri nets with the pairs of places/transitions numbers (14,42) and (14,29) was
estimated as exponential. In the present paper, it is shown, that their slight
modification and interpretation as timed Petri nets with multichannel
transitions, introduced by the author in 1991, allows obtaining polynomial time
complexity. The modification concerns using only inhibitor arcs to control
transitions' firing in multiple instances and employing an inverse control flow
represented by moving zero. Thus, small universal Petri nets are efficient that
justifies their application as models of high performance computations."
"For any time bound f, let H(f) denote the hierarchy conjecture which means
that the restriction of the numbers of work tapes of deterministic Turing
machines to some b generates an infinite hierarchy of proper subclasses
DTIME_b(f) \subset \DTIME(f). We show that H(f) implies separations of
deterministic from nondeterministic time classes. H(f) follows from the gap
property, G(f), which says that there is a time-constructible bound f_2 such
that f \in o(f_2) and DTIME(f)=DTIME(f_2). G(f) implies further separations.
All these relationships relativize."
"Recently, Gupta et.al. [GKKS2013] proved that over Q any $n^{O(1)}$-variate
and $n$-degree polynomial in VP can also be computed by a depth three
$\Sigma\Pi\Sigma$ circuit of size $2^{O(\sqrt{n}\log^{3/2}n)}$. Over fixed-size
finite fields, Grigoriev and Karpinski proved that any $\Sigma\Pi\Sigma$
circuit that computes $Det_n$ (or $Perm_n$) must be of size $2^{\Omega(n)}$
[GK1998]. In this paper, we prove that over fixed-size finite fields, any
$\Sigma\Pi\Sigma$ circuit for computing the iterated matrix multiplication
polynomial of $n$ generic matrices of size $n\times n$, must be of size
$2^{\Omega(n\log n)}$. The importance of this result is that over fixed-size
fields there is no depth reduction technique that can be used to compute all
the $n^{O(1)}$-variate and $n$-degree polynomials in VP by depth 3 circuits of
size $2^{o(n\log n)}$. The result [GK1998] can only rule out such a possibility
for depth 3 circuits of size $2^{o(n)}$.
  We also give an example of an explicit polynomial ($NW_{n,\epsilon}(X)$) in
VNP (not known to be in VP), for which any $\Sigma\Pi\Sigma$ circuit computing
it (over fixed-size fields) must be of size $2^{\Omega(n\log n)}$. The
polynomial we consider is constructed from the combinatorial design. An
interesting feature of this result is that we get the first examples of two
polynomials (one in VP and one in VNP) such that they have provably stronger
circuit size lower bounds than Permanent in a reasonably strong model of
computation.
  Next, we prove that any depth 4
$\Sigma\Pi^{[O(\sqrt{n})]}\Sigma\Pi^{[\sqrt{n}]}$ circuit computing
$NW_{n,\epsilon}(X)$ (over any field) must be of size $2^{\Omega(\sqrt{n}\log
n)}$. To the best of our knowledge, the polynomial $NW_{n,\epsilon}(X)$ is the
first example of an explicit polynomial in VNP such that it requires
$2^{\Omega(\sqrt{n}\log n)}$ size depth four circuits, but no known matching
upper bound."
"Building on work of Cai, F\""urer, and Immerman \cite{CFI92}, we show two
hardness results for the Graph Isomorphism problem. First, we show that there
are pairs of nonisomorphic $n$-vertex graphs $G$ and $H$ such that any
sum-of-squares (SOS) proof of nonisomorphism requires degree $\Omega(n)$. In
other words, we show an $\Omega(n)$-round integrality gap for the Lasserre SDP
relaxation. In fact, we show this for pairs $G$ and $H$ which are not even
$(1-10^{-14})$-isomorphic. (Here we say that two $n$-vertex, $m$-edge graphs
$G$ and $H$ are $\alpha$-isomorphic if there is a bijection between their
vertices which preserves at least $\alpha m$ edges.) Our second result is that
under the {\sc R3XOR} Hypothesis \cite{Fei02} (and also any of a class of
hypotheses which generalize the {\sc R3XOR} Hypothesis), the \emph{robust}
Graph Isomorphism problem is hard. I.e.\ for every $\epsilon > 0$, there is no
efficient algorithm which can distinguish graph pairs which are
$(1-\epsilon)$-isomorphic from pairs which are not even
$(1-\epsilon_0)$-isomorphic for some universal constant $\epsilon_0$. Along the
way we prove a robust asymmetry result for random graphs and hypergraphs which
may be of independent interest."
"We prove that there is, in every direction in Euclidean space, a line that
misses every computably random point. We also prove that there exist, in every
direction in Euclidean space, arbitrarily long line segments missing every
double exponential time random point."
"Consider QBF, the Quantified Boolean Formula problem, as a combinatorial game
ruleset. The problem is rephrased as determining the winner of the game where
two opposing players take turns assigning values to boolean variables. In this
paper, three common variations of games are applied to create seven new games:
whether each player is restricted to where they may play, which values they may
set variables to, or the condition they are shooting for at the end of the
game. The complexity for determining which player can win is analyzed for all
games. Of the seven, two are trivially in P and the other five are
PSPACE-complete. These varying properties are common for combinatorial games;
reductions from these five hard games can simplify the process for showing the
PSPACE-hardness of other games."
"Two polynomials $f, g \in \mathbb{F}[x_1, \ldots, x_n]$ are called
shift-equivalent if there exists a vector $(a_1, \ldots, a_n) \in \mathbb{F}^n$
such that the polynomial identity $f(x_1+a_1, \ldots, x_n+a_n) \equiv
g(x_1,\ldots,x_n)$ holds. Our main result is a new randomized algorithm that
tests whether two given polynomials are shift equivalent. Our algorithm runs in
time polynomial in the circuit size of the polynomials, to which it is given
black box access. This complements a previous work of Grigoriev (Theoretical
Computer Science, 1997) who gave a deterministic algorithm running in time
$n^{O(d)}$ for degree $d$ polynomials.
  Our algorithm uses randomness only to solve instances of the Polynomial
Identity Testing (PIT) problem. Hence, if one could de-randomize PIT (a
long-standing open problem in complexity) a de-randomization of our algorithm
would follow. This establishes an equivalence between de-randomizing
shift-equivalence testing and de-randomizing PIT (both in the black-box and the
white-box setting). For certain restricted models, such as Read Once Branching
Programs, we already obtain a deterministic algorithm using existing PIT
results."
"In this work we introduce, both for classical communication complexity and
query complexity, a modification of the 'partition bound' introduced by Jain
and Klauck [2010]. We call it the 'public-coin partition bound'. We show that
(the logarithm to the base two of) its communication complexity and query
complexity versions form, for all relations, a quadratically tight lower bound
on the public-coin randomized communication complexity and randomized query
complexity respectively."
"Modify the Blum-Shub-Smale model of computation replacing the permitted
computational primitives (the real field operations) with any finite set $B$ of
real functions semialgebraic over the rationals. Consider the class of boolean
decision problems that can be solved in polynomial time in the new model by
machines with no machine constants. How does this class depend on $B$? We prove
that it is always contained in the class obtained for $B = \{+, -, \times\}$.
Moreover, if $B$ is a set of continuous semialgebraic functions containing $+$
and $-$, and such that arbitrarily small numbers can be computed using $B$,
then we have the following dichotomy: either our class is $\mathsf P$ or it
coincides with the class obtained for $B = \{+, -, \times\}$."
"We introduce a new concept, which we call partition expanders. The basic idea
is to study quantitative properties of graphs in a slightly different way than
it is in the standard definition of expanders. While in the definition of
expanders it is required that the number of edges between any pair of
sufficiently large sets is close to the expected number, we consider partitions
and require this condition only for most of the pairs of blocks. As a result,
the blocks can be substantially smaller.
  We show that for some range of parameters, to be a partition expander a
random graph needs exponentially smaller degree than any expander would require
in order to achieve similar expanding properties.
  We apply the concept of partition expanders in communication complexity.
First, we give a PRG for the SMP model of the optimal seed length, n+O(log k).
Second, we compare the model of SMP to that of Simultaneous Two-Way
Communication, and give a new separation that is stronger both qualitatively
and quantitatively than the previously known ones."
"The new model of quantum computation is proposed, for which an effective
algorithm of solving any task in NP is described. The work is based and
inspired be the Grover's algorithm for solving NP-tasks with quadratic speedup
compared to the classical computation model. The provided model and algorithm
exhibit the exponential speedup over that described by Grover."
"In this short note, the author shows that the gap problem of some 3-XOR is
NP-hard and can be solved by running Charikar\&Wirth's SDP algorithm for two
rounds. To conclude, the author proves that $P=NP$."
"We use existential Diophantine predicates carefully reinterpreted over the
reals and the time complexity of Tarski algebra to show that 3-CNF SAT is in
n^O(log^{gamma} n) time for an absolute positive constant gamma."
"We study the performances of stochastic heuristic search algorithms on
Uniquely Extendible Constraint Satisfaction Problems with random inputs. We
show that, for any heuristic preserving the Poissonian nature of the underlying
instance, the (heuristic-dependent) largest ratio $\alpha_a$ of constraints per
variables for which a search algorithm is likely to find solutions is smaller
than the critical ratio $\alpha_d$ above which solutions are clustered and
highly correlated. In addition we show that the clustering ratio can be reached
when the number k of variables per constraints goes to infinity by the
so-called Generalized Unit Clause heuristic."
"These are lectures notes for the introductory graduate courses on geometric
complexity theory (GCT) in the computer science department, the university of
Chicago. Part I consists of the lecture notes for the course given by the first
author in the spring quarter, 2007. It gives introduction to the basic
structure of GCT. Part II consists of the lecture notes for the course given by
the second author in the spring quarter, 2003. It gives introduction to
invariant theory with a view towards GCT. No background in algebraic geometry
or representation theory is assumed. These lecture notes in conjunction with
the article \cite{GCTflip1}, which describes in detail the basic plan of GCT
based on the principle called the flip, should provide a high level picture of
GCT assuming familiarity with only basic notions of algebra, such as groups,
rings, fields etc."
"Geometric complexity theory (GCT) is an approach to the $P$ vs. $NP$ and
related problems through algebraic geometry and representation theory. This
article gives a high-level exposition of the basic plan of GCT based on the
principle, called the flip, without assuming any background in algebraic
geometry or representation theory."
"This article describes a {\em nonstandard} quantum group that may be used to
derive a positive formula for the plethysm problem, just as the standard
(Drinfeld-Jimbo) quantum group can be used to derive the positive
Littlewood-Richardson rule for arbitrary complex semisimple Lie groups. The
sequel \cite{GCT8} gives conjecturally correct algorithms to construct
canonical bases of the coordinate rings of these nonstandard quantum groups and
canonical bases of the dually paired nonstandard deformations of the symmetric
group algebra. A positive $#P$-formula for the plethysm constant follows from
the conjectural properties of these canonical bases and the duality and
reciprocity conjectures herein."
"This article gives conjecturally correct algorithms to construct canonical
bases of the irreducible polynomial representations and the matrix coordinate
rings of the nonstandard quantum groups in GCT4 and GCT7, and canonical bases
of the dually paired nonstandard deformations of the symmetric group algebra
therein. These are generalizations of the canonical bases of the irreducible
polynomial representations and the matrix coordinate ring of the standard
quantum group, as constructed by Kashiwara and Lusztig, and the Kazhdan-Lusztig
basis of the Hecke algebra. A positive ($#P$-) formula for the well-known
plethysm constants follows from their conjectural properties and the duality
and reciprocity conjectures in \cite{GCT7}."
"This paper discusses why P and NP are likely to be different. It analyses the
essence of the concepts and points out that P and NP might be diverse by sheer
definition. It also speculates that P and NP may be unequal due to natural
laws."
"Finite automata with weights in the max-plus semiring are considered. The
main result is: it is decidable in an effective way whether a series that is
recognized by a finitely ambiguous max-plus automaton is unambiguous, or is
sequential. A collection of examples is given to illustrate the hierarchy of
max-plus series with respect to ambiguity."
"The isolation lemma of Mulmuley et al \cite{MVV87} is an important tool in
the design of randomized algorithms and has played an important role in several
nontrivial complexity upper bounds. On the other hand, polynomial identity
testing is a well-studied algorithmic problem with efficient randomized
algorithms and the problem of obtaining efficient \emph{deterministic} identity
tests has received a lot of attention recently. The goal of this note is to
compare the isolation lemma with polynomial identity testing: 1. We show that
derandomizing reasonably restricted versions of the isolation lemma implies
circuit size lower bounds. We derive the circuit lower bounds by examining the
connection between the isolation lemma and polynomial identity testing. We give
a randomized polynomial-time identity test for noncommutative circuits of
polynomial degree based on the isolation lemma. Using this result, we show that
derandomizing the isolation lemma implies noncommutative circuit size lower
bounds. The restricted versions of the isolation lemma we consider are natural
and would suffice for the standard applications of the isolation lemma. 2. From
the result of Klivans-Spielman \cite{KS01} we observe that there is a
randomized polynomial-time identity test for commutative circuits of polynomial
degree, also based on a more general isolation lemma for linear forms.
Consequently, derandomization of (a suitable version of) this isolation lemma
also implies circuit size lower bounds in the commutative setting."
"The purpose of this article is to examine and limit the conditions in which
the P complexity class could be equivalent to the NP complexity class. Proof is
provided by demonstrating that as the number of clauses in a NP-complete
problem approaches infinity, the number of input sets processed per computation
performed also approaches infinity when solved by a polynomial time solution.
It is then possible to determine that the only deterministic optimization of a
NP-complete problem that could prove P = NP would be one that examines no more
than a polynomial number of input sets for a given problem.
  It is then shown that subdividing the set of all possible input sets into a
representative polynomial search partition is a problem in the FEXP complexity
class. The findings of this article are combined with the findings of other
articles in this series of 4 articles. The final conclusion will be
demonstrated that P =/= NP."
"Previously the author has demonstrated that a representative polynomial
search partition is required to solve a NP-complete problem in deterministic
polynomial time. It has also been demonstrated that finding such a partition
can only be done in deterministic polynomial time if the form of the problem
provides a simple method for producing the partition. It is the purpose of this
article to demonstrate that no deterministic polynomial time method exists to
produce a representative polynomial search partition for the Knapsack problem."
"Razborov and Rudich have shown that so-called ""natural proofs"" are not useful
for separating P from NP unless hard pseudorandom number generators do not
exist. This famous result is widely regarded as a serious barrier to proving
strong lower bounds in circuit complexity theory.
  By definition, a natural combinatorial property satisfies two conditions,
constructivity and largeness. Our main result is that if the largeness
condition is weakened slightly, then not only does the Razborov-Rudich proof
break down, but such ""almost-natural"" (and useful) properties provably exist.
Specifically, under the same pseudorandomness assumption that Razborov and
Rudich make, a simple, explicit property that we call ""discrimination"" suffices
to separate P/poly from NP; discrimination is nearly linear-time computable and
almost large, having density 2^{-q(n)} where q is a quasi-polynomial function.
For those who hope to separate P from NP using random function properties in
some sense, discrimination is interesting, because it is constructive, yet may
be thought of as a minor alteration of a property of a random function.
  The proof relies heavily on the self-defeating character of natural proofs.
Our proof technique also yields an unconditional result, namely that there
exist almost-large and useful properties that are constructive, if we are
allowed to call non-uniform low-complexity classes ""constructive."" We note,
though, that this unconditional result can also be proved by a more
conventional counting argument.
  Finally, we give an alternative proof, communicated to us by Salil Vadhan at
FOCS 2008, of one of our theorems, and we make some speculative remarks on the
future prospects for proving strong circuit lower bounds."
"We give the first algorithm that is both query-efficient and time-efficient
for testing whether an unknown function $f: \{0,1\}^n \to \{0,1\}$ is an
$s$-sparse GF(2) polynomial versus $\eps$-far from every such polynomial. Our
algorithm makes $\poly(s,1/\eps)$ black-box queries to $f$ and runs in time $n
\cdot \poly(s,1/\eps)$. The only previous algorithm for this testing problem
\cite{DLM+:07} used poly$(s,1/\eps)$ queries, but had running time exponential
in $s$ and super-polynomial in $1/\eps$.
  Our approach significantly extends the ``testing by implicit learning''
methodology of \cite{DLM+:07}. The learning component of that earlier work was
a brute-force exhaustive search over a concept class to find a hypothesis
consistent with a sample of random examples. In this work, the learning
component is a sophisticated exact learning algorithm for sparse GF(2)
polynomials due to Schapire and Sellie \cite{SchapireSellie:96}. A crucial
element of this work, which enables us to simulate the membership queries
required by \cite{SchapireSellie:96}, is an analysis establishing new
properties of how sparse GF(2) polynomials simplify under certain restrictions
of ``low-influence'' sets of variables."
"Representations of Boolean functions by real polynomials play an important
role in complexity theory. Typically, one is interested in the least degree of
a polynomial p(x_1,...,x_n) that approximates or sign-represents a given
Boolean function f(x_1,...,x_n). This article surveys a new and growing body of
work in communication complexity that centers around the dual objects, i.e.,
polynomials that certify the difficulty of approximating or sign-representing a
given function. We provide a unified guide to the following results, complete
with all the key proofs:
  (1) Sherstov's Degree/Discrepancy Theorem, which translates lower bounds on
the threshold degree of a Boolean function into upper bounds on the discrepancy
of a related function;
  (2) Two different methods for proving lower bounds on bounded-error
communication based on the approximate degree: Sherstov's pattern matrix method
and Shi and Zhu's block composition method;
  (3) Extension of the pattern matrix method to the multiparty model, obtained
by Lee and Shraibman and by Chattopadhyay and Ada, and the resulting improved
lower bounds for DISJOINTNESS;
  (4) David and Pitassi's separation of NP and BPP in multiparty communication
complexity for k=(1-eps)log n players."
"This is the third article in a series of four articles dealing with the P vs.
NP question. The purpose of this work is to demonstrate that the methods used
in the first two articles of this series are not affected by oracle
relativizations. Furthermore, the solution to the P vs. NP problem is actually
independent of oracle relativizations."
"We review a minimum set of notions from our previous paper on structural
properties of SAT at arXiv:0802.1790 that will allow us to define and discuss
the ""complete internal independence"" of a decision problem. This property is
strictly stronger than the independence property that was called ""strong
internal independence"" in cited paper. We show that SAT exhibits this property.
We argue that this form of independence of a decision problem is the strongest
possible for a problem. By relying upon this maximally strong form of internal
independence, we reformulate in more strict terms the informal remarks on
possible exponentiality of SAT that concluded our previous paper. The net
result of that reformulation is a hint for a proof for SAT being exponential.
We conjecture that a complete proof of that proposition can be obtained by
strictly following the line of given hint of proof."
"We show that the isomorphism of 3-connected planar graphs can be decided in
deterministic log-space. This improves the previously known bound UL$\cap$coUL
of Thierauf and Wagner."
"The relationship between the complexity classes P and NP is an unsolved
question in the field of theoretical computer science. In this paper, we look
at the link between the P - NP question and the ""Deterministic"" versus ""Non
Deterministic"" nature of a problem, and more specifically at the temporal
nature of the complexity within the NP class of problems. Let us remind that
the NP class is called the class of ""Non Deterministic Polynomial"" languages.
Using the meta argument that results in Mathematics should be ""time
independent"" as they are reproducible, the paper shows that the P!=NP assertion
is impossible to prove in the a-temporal framework of Mathematics. In a
previous version of the report, we use a similar argument based on randomness
to show that the P = NP assertion was also impossible to prove, but this part
of the paper was shown to be incorrect. So, this version deletes it. In fact,
this paper highlights the time dependence of the complexity for any NP problem,
linked to some pseudo-randomness in its heart."
"Muchnik's theorem about simple conditional descriptions states that for all
strings $a$ and $b$ there exists a short program $p$ transforming $a$ to $b$
that has the least possible length and is simple conditional on $b$. In this
paper we present two new proofs of this theorem. The first one is based on the
on-line matching algorithm for bipartite graphs. The second one, based on
extractors, can be generalized to prove a version of Muchnik's theorem for
space-bounded Kolmogorov complexity."
"Aslam presents an algorithm he claims will count the number of perfect
matchings in any incomplete bipartite graph with an algorithm in the
function-computing version of NC, which is itself a subset of FP. Counting
perfect matchings is known to be #P-complete; therefore if Aslam's algorithm is
correct, then NP=P. However, we show that Aslam's algorithm does not correctly
count the number of perfect matchings and offer an incomplete bipartite graph
as a concrete counter-example."
"Although whether P equals NP is an important, open problem in computer
science, and although Jaeger's 2008 paper, ""Solving the P/NP Problem Under
Intrinsic Uncertainty"" (arXiv:0811.0463) presents an attempt at tackling the
problem by discussing the possibility that all computation is uncertain to some
degree, there are a number of logical oversights present in that paper which
preclude it from serious consideration toward having resolved P-versus-NP.
There are several differences between the model of computation presented in
Jaeger's paper and the standard model, as well as several bold assumptions that
are not well supported in Jaeger's paper or in the literature. In addition, we
find several omissions of rigorous proof that ultimately weaken this paper to a
point where it cannot be considered a candidate solution to the P-versus-NP
problem."
"In this paper we formulate and study the problem of representing groups on
graphs. We show that with respect to polynomial time turing reducibility, both
abelian and solvable group representability are all equivalent to graph
isomorphism, even when the group is presented as a permutation group via
generators. On the other hand, the representability problem for general groups
on trees is equivalent to checking, given a group $G$ and $n$, whether a
nontrivial homomorphism from $G$ to $S_n$ exists. There does not seem to be a
polynomial time algorithm for this problem, in spite of the fact that tree
isomorphism has polynomial time algorithm."
"Computational complexity is examined using the principle of increasing
entropy. To consider computation as a physical process from an initial instance
to the final acceptance is motivated because many natural processes have been
recognized to complete in non-polynomial time (NP). The irreversible process
with three or more degrees of freedom is found intractable because, in terms of
physics, flows of energy are inseparable from their driving forces. In
computational terms, when solving problems in the class NP, decisions will
affect subsequently available sets of decisions. The state space of a
non-deterministic finite automaton is evolving due to the computation itself
hence it cannot be efficiently contracted using a deterministic finite
automaton that will arrive at a solution in super-polynomial time. The solution
of the NP problem itself is verifiable in polynomial time (P) because the
corresponding state is stationary. Likewise the class P set of states does not
depend on computational history hence it can be efficiently contracted to the
accepting state by a deterministic sequence of dissipative transformations.
Thus it is concluded that the class P set of states is inherently smaller than
the set of class NP. Since the computational time to contract a given set is
proportional to dissipation, the computational complexity class P is a subset
of NP."
"Recent investigations show insertion-deletion systems of small size that are
not complete and cannot generate all recursively enumerable languages. However,
if additional computational distribution mechanisms like P systems are added,
then the computational completeness is achieved in some cases. In this article
we take two insertion-deletion systems that are not computationally complete,
consider them in the framework of P systems and show that the computational
power is strictly increased by proving that any recursively enumerable language
can be generated. At the end some open problems are presented."
"We introduce the notion of a stable instance for a discrete optimization
problem, and argue that in many practical situations only sufficiently stable
instances are of interest. The question then arises whether stable instances of
NP--hard problems are easier to solve. In particular, whether there exist
algorithms that solve correctly and in polynomial time all sufficiently stable
instances of some NP--hard problem. The paper focuses on the Max--Cut problem,
for which we show that this is indeed the case."
"In this paper we introduce a general framework for defining the depth of a
sequence with respect to a class of observers. We show that our general
framework captures all depth notions introduced in complexity theory so far. We
review most such notions, show how they are particular cases of our general
depth framework, and review some classical results about the different depth
notions."
"Symport/antiport P systems provide a very simple machinery inspired by
corresponding operations in the living cell. It turns out that systems of small
descriptional complexity are needed to achieve the universality by these
systems. This makes them a good candidate for small universal devices replacing
register machines for different simulations, especially when a simulating
parallel machinery is involved. This article contains survey of these systems
and presents different trade-offs between parameters."
"We prove that if a subset X of the integer Cartesian plane weakly
self-assembles at temperature 1 in a deterministic (Winfree) tile assembly
system satisfying a natural condition known as *pumpability*, then X is a
finite union of doubly periodic sets. This shows that only the most simple of
infinite shapes and patterns can be constructed using pumpable temperature 1
tile assembly systems, and gives strong evidence for the thesis that
temperature 2 or higher is required to carry out general-purpose computation in
a tile assembly system. Finally, we show that general-purpose computation is
possible at temperature 1 if negative glue strengths are allowed in the tile
assembly model."
"We review some recent results related to the self-assembly of infinite
structures in the Tile Assembly Model. These results include impossibility
results, as well as novel tile assembly systems in which shapes and patterns
that represent various notions of computation self-assemble. Several open
questions are also presented and motivated."
"A resource-bounded version of the statement ""no algorithm recognizes all
non-halting Turing machines"" is equivalent to an infinitely often (i.o.)
superpolynomial speedup for the time required to accept any coNP-complete
language and also equivalent to a superpolynomial speedup in proof length in
propositional proof systems for tautologies, each of which implies P!=NP. This
suggests a correspondence between the properties 'has no algorithm at all' and
'has no best algorithm' which seems relevant to open problems in computational
and proof complexity."
"We propose models for lobbying in a probabilistic environment, in which an
actor (called ""The Lobby"") seeks to influence voters' preferences of voting for
or against multiple issues when the voters' preferences are represented in
terms of probabilities. In particular, we provide two evaluation criteria and
two bribery methods to formally describe these models, and we consider the
resulting forms of lobbying with and without issue weighting. We provide a
formal analysis for these problems of lobbying in a stochastic environment, and
determine their classical and parameterized complexity depending on the given
bribery/evaluation criteria and on various natural parameterizations.
Specifically, we show that some of these problems can be solved in polynomial
time, some are NP-complete but fixed-parameter tractable, and some are
W[2]-complete. Finally, we provide approximability and inapproximability
results for these problems and several variants."
"This paper provides a further refinement to the previous response by
introducing new structures and algorithms for counting VMPs of common ER and
hence for counting the perfect matchings. The sequential time complexity of
this $\mathbf{\#P}$-complete problem is shown to be $O(n^{33})$."
"The Kolmogorov complexity of the word w is equal to the length of the
shortest concatenation of program Z and its input x with which the word w is
computed by the universal turing machine U. The question introduced in this
paper is the following: How long do the shortest programs run for?"
"We show that for any rational p \in [1,\infty) except p = 1, 2, unless P =
NP, there is no polynomial-time algorithm for approximating the matrix p-norm
to arbitrary relative precision. We also show that for any rational p\in
[1,\infty) including p = 1, 2, unless P = NP, there is no polynomial-time
algorithm approximates the \infty, p mixed norm to some fixed relative
precision."
"Geometric complexity theory (GCT) is an approach to the P vs. NP and related
problems. This article gives its complexity theoretic overview without assuming
any background in algebraic geometry or representation theory."
"Geometric complexity theory (GCT) is an approach to the $P$ vs. $NP$ and
related problems. A high level overview of this research plan and the results
obtained so far was presented in a series of three lectures in the Institute of
Advanced study, Princeton, Feb 9-11, 2009. This article contains the material
covered in those lectures after some revision, and gives a mathematical
overview of GCT. No background in algebraic geometry, representation theory or
quantum groups is assumed."
"Motivated by the result that an `approximate' evaluation of the Jones
polynomial of a braid at a $5^{th}$ root of unity can be used to simulate the
quantum part of any algorithm in the quantum complexity class BQP, and results
relating BQP to the counting class GapP, we introduce a form of additive
approximation which can be used to simulate a function in BQP. We show that all
functions in the classes #P and GapP have such an approximation scheme under
certain natural normalisations. However we are unable to determine whether the
particular functions we are motivated by, such as the above evaluation of the
Jones polynomial, can be approximated in this way. We close with some open
problems motivated by this work."
"Knowledge extraction is a fundamental notion, modelling machine possession of
values (witnesses) in a computational complexity sense. The notion provides an
essential tool for cryptographic protocol design and analysis, enabling one to
argue about the internal state of protocol players without ever looking at this
supposedly secret state. However, when transactions are concurrent (e.g., over
the Internet) with players possessing public-keys (as is common in
cryptography), assuring that entities ``know'' what they claim to know, where
adversaries may be well coordinated across different transactions, turns out to
be much more subtle and in need of re-examination. Here, we investigate how to
formally treat knowledge possession by parties (with registered public-keys)
interacting over the Internet. Stated more technically, we look into the
relative power of the notion of ``concurrent knowledge-extraction'' (CKE) in
the concurrent zero-knowledge (CZK) bare public-key (BPK) model."
"A strong direct product theorem states that if we want to compute $k$
independent instances of a function, using less than $k$ times the resources
needed for one instance, then the overall success probability will be
exponentially small in $k$. We establish such a theorem for the randomized
communication complexity of the Disjointness problem, i.e., with communication
$const\cdot kn$ the success probability of solving $k$ instances of size $n$
can only be exponentially small in $k$. We show that this bound even holds for
$AM$ communication protocols with limited ambiguity. This also implies a new
lower bound for Disjointness in a restricted 3-player NOF protocol, and optimal
communication-space tradeoffs for Boolean matrix product. Our main result
follows from a solution to the dual of a linear programming problem, whose
feasibility comes from a so-called Intersection Sampling Lemma that generalizes
a result by Razborov."
"Many programmers belive that Turing-based machines cannot think. We also
believe in this, however it is interesting to note that the most sophisticated
machines are not programmed by human beings. We have only discovered them. In
this paper, using well-known Busy Beaver and Placid Platypus machines, we
generate further very similar, but not exactly the same machines. We have found
a recombinated BB_5 machine which can make 70.740.809 steps before halting."
"Constraint satisfaction problems (or CSPs) have been extensively studied in,
for instance, artificial intelligence, database theory, graph theory, and
statistical physics. From a practical viewpoint, it is beneficial to
approximately solve those CSPs. When one tries to approximate the total number
of truth assignments that satisfy all Boolean-valued constraints for
(unweighted) Boolean CSPs, there is a known trichotomy theorem by which all
such counting problems are neatly classified into exactly three categories
under polynomial-time (randomized) approximation-preserving reductions. In
contrast, we obtain a dichotomy theorem of approximate counting for
complex-weighted Boolean CSPs, provided that all complex-valued unary
constraints are freely available to use. It is the expressive power of free
unary constraints that enables us to prove such a stronger, complete
classification theorem. This discovery makes a step forward in the quest for
the approximation-complexity classification of all counting CSPs. To deal with
complex weights, we employ proof techniques of factorization and arity
reduction along the line of solving Holant problems. Moreover, we introduce a
novel notion of T-constructibility that naturally induces
approximation-preserving reducibility. Our result also gives an approximation
analogue of the dichotomy theorem on the complexity of exact counting for
complex-weighted Boolean CSPs."
"We consider the problem of exact identification for read-once functions over
arbitrary Boolean bases. We introduce a new type of queries (subcube identity
ones), discuss its connection to previously known ones, and study the
complexity of the problem in question. Besides these new queries, learning
algorithms are allowed to use classic membership ones. We present a technique
of modeling an equivalence query with a polynomial number of membership and
subcube identity ones, thus establishing (under certain conditions) a
polynomial upper bound on the complexity of the problem. We show that in some
circumstances, though, equivalence queries cannot be modeled with a polynomial
number of subcube identity and membership ones. We construct an example of an
infinite Boolean basis with an exponential lower bound on the number of
membership and subcube identity queries required for exact identification. We
prove that for any finite subset of this basis, the problem remains polynomial."
"The first section starts with the basic definitions following mainly the
notations of the book written by E. Kushilevitz and N. Nisan. At the end of the
first section I examine tree-balancing.
  In the second section I summarize the well-known lower bound methods and
prove the exact complexity of certain functions.
  In the first part of the third section I introduce the random complexity and
prove the basic lemmas about it. In the second part I prove a better lower
bound for the complexity of all random functions. In the third part I introduce
and compare several upper bounds for the complexity of the identity function.
  In the fourth section I examine the well-known Direct-sum conjecture. I
introduce a different model of computation then prove that it is the same as
the original one up to a constant factor. This new model is used to bound the
Amortized Time Complexity of a function by the number of the leaves of its
protocol-tree. After this I examine the Direct-sum problem in case of Partial
Information and in the Random case.
  In the last section I introduce the well-known hierarchy classes, the
reducibility and the completeness of series of functions. Then I define the
class PSPACE and Oracles in the communication complexity model and prove some
basic claims about them."
"This paper has been withdrawn by the author due to a misunderstanding about
3QBF."
"The work in this paper is to initiate a theory of testing monomials in
multivariate polynomials. The central question is to ask whether a polynomial
represented by certain economically compact structure has a multilinear
monomial in its sum-product expansion. The complexity aspects of this problem
and its variants are investigated with two folds of objectives. One is to
understand how this problem relates to critical problems in complexity, and if
so to what extent. The other is to exploit possibilities of applying algebraic
properties of polynomials to the study of those problems. A series of results
about $\Pi\Sigma\Pi$ and $\Pi\Sigma$ polynomials are obtained in this paper,
laying a basis for further study along this line."
"This paper is our second step towards developing a theory of testing
monomials in multivariate polynomials. The central question is to ask whether a
polynomial represented by an arithmetic circuit has some types of monomials in
its sum-product expansion. The complexity aspects of this problem and its
variants have been investigated in our first paper by Chen and Fu (2010),
laying a foundation for further study. In this paper, we present two pairs of
algorithms. First, we prove that there is a randomized $O^*(p^k)$ time
algorithm for testing $p$-monomials in an $n$-variate polynomial of degree $k$
represented by an arithmetic circuit, while a deterministic $O^*(6.4^k + p^k)$
time algorithm is devised when the circuit is a formula, here $p$ is a given
prime number. Second, we present a deterministic $O^*(2^k)$ time algorithm for
testing multilinear monomials in $\Pi_m\Sigma_2\Pi_t\times \Pi_k\Pi_3$
polynomials, while a randomized $O^*(1.5^k)$ algorithm is given for these
polynomials. The first algorithm extends the recent work by Koutis (2008) and
Williams (2009) on testing multilinear monomials. Group algebra is exploited in
the algorithm designs, in corporation with the randomized polynomial identity
testing over a finite field by Agrawal and Biswas (2003), the deterministic
noncommunicative polynomial identity testing by Raz and Shpilka (2005) and the
perfect hashing functions by Chen {\em at el.} (2007). Finally, we prove that
testing some special types of multilinear monomial is W[1]-hard, giving
evidence that testing for specific monomials is not fixed-parameter tractable."
"This paper is our third step towards developing a theory of testing monomials
in multivariate polynomials and concentrates on two problems: (1) How to
compute the coefficients of multilinear monomials; and (2) how to find a
maximum multilinear monomial when the input is a $\Pi\Sigma\Pi$ polynomial. We
first prove that the first problem is \#P-hard and then devise a $O^*(3^ns(n))$
upper bound for this problem for any polynomial represented by an arithmetic
circuit of size $s(n)$. Later, this upper bound is improved to $O^*(2^n)$ for
$\Pi\Sigma\Pi$ polynomials. We then design fully polynomial-time randomized
approximation schemes for this problem for $\Pi\Sigma$ polynomials. On the
negative side, we prove that, even for $\Pi\Sigma\Pi$ polynomials with terms of
degree $\le 2$, the first problem cannot be approximated at all for any
approximation factor $\ge 1$, nor {\em ""weakly approximated""} in a much relaxed
setting, unless P=NP. For the second problem, we first give a polynomial time
$\lambda$-approximation algorithm for $\Pi\Sigma\Pi$ polynomials with terms of
degrees no more a constant $\lambda \ge 2$. On the inapproximability side, we
give a $n^{(1-\epsilon)/2}$ lower bound, for any $\epsilon >0,$ on the
approximation factor for $\Pi\Sigma\Pi$ polynomials. When terms in these
polynomials are constrained to degrees $\le 2$, we prove a $1.0476$ lower
bound, assuming $P\not=NP$; and a higher $1.0604$ lower bound, assuming the
Unique Games Conjecture."
"This paper argues an existence of a class of functions where function own
input makes function description. That fact have impact to the wide spectrum of
phenomena such as negative findings of Random Oracle Model in cryptography,
complexity in some rules of cellular automata (Wolfram rule 30) and determinism
in the true randomness to name just a few."
"Determining the maximal separation between sensitivity and block sensitivity
of Boolean functions is of interest for computational complexity theory. We
construct a sequence of Boolean functions with bs(f) = 1/2 s(f)^2 + 1/2 s(f).
The best known separation previously was bs(f) = 1/2 s(f)^2 due to Rubinstein.
We also report results of computer search for functions with at most 12
variables."
"Valiant introduced matchgate computation and holographic algorithms. A number
of seemingly exponential time problems can be solved by this novel algorithmic
paradigm in polynomial time. We show that, in a very strong sense, matchgate
computations and holographic algorithms based on them provide a universal
methodology to a broad class of counting problems studied in statistical
physics community for decades. They capture precisely those problems which are
#P-hard on general graphs but computable in polynomial time on planar graphs.
  More precisely, we prove complexity dichotomy theorems in the framework of
counting CSP problems. The local constraint functions take Boolean inputs, and
can be arbitrary real-valued symmetric functions. We prove that, every problem
in this class belongs to precisely three categories: (1) those which are
tractable (i.e., polynomial time computable) on general graphs, or (2) those
which are \#P-hard on general graphs but ractable on planar graphs, or (3)
those which are #P-hard even on planar graphs. The classification criteria are
explicit. Moreover, problems in category (2) are tractable on planar graphs
precisely by holographic algorithms with matchgates."
"The complexity of graph homomorphism problems has been the subject of intense
study. It is a long standing open problem to give a (decidable) complexity
dichotomy theorem for the partition function of directed graph homomorphisms.
In this paper, we prove a decidable complexity dichotomy theorem for this
problem and our theorem applies to all non-negative weighted form of the
problem: given any fixed matrix A with non-negative algebraic entries, the
partition function Z_A(G) of directed graph homomorphisms from any directed
graph G is either tractable in polynomial time or #P-hard, depending on the
matrix A. The proof of the dichotomy theorem is combinatorial, but involves the
definition of an infinite family of graph homomorphism problems. The proof of
its decidability is algebraic using properties of polynomials."
"We study the complexity of valued constraint satisfaction problems (VCSP). A
problem from VCSP is characterised by a \emph{constraint language}, a fixed set
of cost functions over a finite domain. An instance of the problem is specified
by a sum of cost functions from the language and the goal is to minimise the
sum. We consider the case of so-called \emph{conservative} languages; that is,
languages containing all unary cost functions, thus allowing arbitrary
restrictions on the domains of the variables. This problem has been studied by
Bulatov [LICS'03] for $\{0,\infty\}$-valued languages (i.e. CSP), by
Cohen~\etal\ (AIJ'06) for Boolean domains, by Deineko et al. (JACM'08) for
$\{0,1\}$-valued cost functions (i.e. Max-CSP), and by Takhanov (STACS'10) for
$\{0,\infty\}$-valued languages containing all finite-valued unary cost
functions (i.e. Min-Cost-Hom).
  We give an elementary proof of a complete complexity classification of
conservative finite-valued languages: we show that every conservative
finite-valued language is either tractable or NP-hard. This is the \emph{first}
dichotomy result for finite-valued VCSPs over non-Boolean domains."
"For current state-of-the-art DPLL SAT-solvers the two main bottlenecks are
the amounts of time and memory used. In proof complexity, these resources
correspond to the length and space of resolution proofs. There has been a long
line of research investigating these proof complexity measures, but while
strong results have been established for length, our understanding of space and
how it relates to length has remained quite poor. In particular, the question
whether resolution proofs can be optimized for length and space simultaneously,
or whether there are trade-offs between these two measures, has remained
essentially open.
  In this paper, we remedy this situation by proving a host of length-space
trade-off results for resolution. Our collection of trade-offs cover almost the
whole range of values for the space complexity of formulas, and most of the
trade-offs are superpolynomial or even exponential and essentially tight. Using
similar techniques, we show that these trade-offs in fact extend to the
exponentially stronger k-DNF resolution proof systems, which operate with
formulas in disjunctive normal form with terms of bounded arity k. We also
answer the open question whether the k-DNF resolution systems form a strict
hierarchy with respect to space in the affirmative.
  Our key technical contribution is the following, somewhat surprising,
theorem: Any CNF formula F can be transformed by simple variable substitution
into a new formula F' such that if F has the right properties, F' can be proven
in essentially the same length as F, whereas on the other hand the minimal
number of lines one needs to keep in memory simultaneously in any proof of F'
is lower-bounded by the minimal number of variables needed simultaneously in
any proof of F. Applying this theorem to so-called pebbling formulas defined in
terms of pebble games on directed acyclic graphs, we obtain our results."
"We determine the computational complexity of approximately counting the total
weight of variable assignments for every complex-weighted Boolean constraint
satisfaction problem (or CSP) with any number of additional unary (i.e., arity
1) constraints, particularly, when degrees of input instances are bounded from
above by a fixed constant. All degree-1 counting CSPs are obviously solvable in
polynomial time. When the instance's degree is more than two, we present a
dichotomy theorem that classifies all counting CSPs admitting free unary
constraints into exactly two categories. This classification theorem extends,
to complex-weighted problems, an earlier result on the approximation complexity
of unweighted counting Boolean CSPs of bounded degree. The framework of the
proof of our theorem is based on a theory of signature developed from Valiant's
holographic algorithms that can efficiently solve seemingly intractable
counting CSPs. Despite the use of arbitrary complex weight, our proof of the
classification theorem is rather elementary and intuitive due to an extensive
use of a novel notion of limited T-constructibility. For the remaining degree-2
problems, in contrast, they are as hard to approximate as Holant problems,
which are a generalization of counting CSPs."
"It is well known that Sokoban is PSPACE-complete (Culberson 1998) and several
of its variants are NP-hard (Demaine et al. 2003). In this paper we prove the
NP-hardness of some variants of Sokoban where the warehouse keeper can only
pull boxes."
"We study optimisation problems that can be formulated as valued constraint
satisfaction problems (VCSP). A problem from VCSP is characterised by a
\emph{constraint language}, a fixed set of cost functions taking finite and
infinite costs over a finite domain. An instance of the problem is specified by
a sum of cost functions from the language and the goal is to minimise the sum.
We are interested in \emph{tractable} constraint languages; that is, languages
that give rise to VCSP instances solvable in polynomial time. Cohen et al.
(AIJ'06) have shown that constraint languages that admit the MJN multimorphism
are tractable. Moreover, using a minimisation algorithm for submodular
functions, Cohen et al. (TCS'08) have shown that constraint languages that
admit an STP (symmetric tournament pair) multimorphism are tractable.
  We generalise these results by showing that languages admitting the MJN
multimorphism on a subdomain and an STP multimorphisms on the complement of the
subdomain are tractable. The algorithm is a reduction to the algorithm for
languages admitting an STP multimorphism."
"We study the complexity of valued constraint satisfaction problems (VCSP). A
problem from VCSP is characterised by a \emph{constraint language}, a fixed set
of cost functions over a finite domain. An instance of the problem is specified
by a sum of cost functions from the language and the goal is to minimise the
sum. We consider the case of so-called \emph{conservative} languages; that is,
languages containing all unary cost functions, thus allowing arbitrary
restrictions on the domains of the variables. We prove a Schaefer-like
dichotomy theorem for this case: if all cost functions in the language satisfy
a certain condition (specified by a complementary combination of \emph{STP and
MJN multimorphisms}) then any instance can be solved in polynomial time by the
algorithm of Kolmogorov and Zivny (arXiv:1008.3104v1), otherwise the language
is NP-hard. This generalises recent results of Takhanov (STACS'10) who
considered $\{0,\infty\}$-valued languages containing additionally all
finite-valued unary cost functions, and Kolmogorov and Zivny
(arXiv:1008.1555v1) who considered \emph{finite-valued} conservative languages."
"Properties of Boolean functions on the hypercube invariant with respect to
linear transformations of the domain are among the most well-studied properties
in the context of property testing. In this paper, we study the fundamental
class of linear-invariant properties called matroid freeness properties. These
properties have been conjectured to essentially coincide with all testable
linear-invariant properties, and a recent sequence of works has established
testability for increasingly larger subclasses. One question left open,
however, is whether the infinitely many syntactically different properties
recently shown testable in fact correspond to new, semantically distinct ones.
This is a crucial issue since it has also been shown that there exist
subclasses of these properties for which an infinite set of syntactically
different representations collapse into one of a small, finite set of
properties, all previously known to be testable.
  An important question is therefore to understand the semantics of matroid
freeness properties, and in particular when two syntactically different
properties are truly distinct. We shed light on this problem by developing a
method for determining the relation between two matroid freeness properties P
and Q. Furthermore, we show that there is a natural subclass of matroid
freeness properties such that for any two properties P and Q from this
subclass, a strong dichotomy must hold: either P is contained in Q or the two
properties are ""well separated."" As an application of this method, we exhibit
new, infinite hierarchies of testable matroid freeness properties such that at
each level of the hierarchy, there are functions that are far from all
functions lying in lower levels of the hierarchy. Our key technical tool is an
apparently new notion of maps between linear matroids, called matroid
homomorphisms, that might be of independent interest."
"We present a selection of known as well as new variants of the Sensitivity
Conjecture and point out some weaker versions that are also open."
"The Small-Set Expansion Hypothesis (Raghavendra, Steurer, STOC 2010) is a
natural hardness assumption concerning the problem of approximating the edge
expansion of small sets in graphs. This hardness assumption is closely
connected to the Unique Games Conjecture (Khot, STOC 2002). In particular, the
Small-Set Expansion Hypothesis implies the Unique Games Conjecture
(Raghavendra, Steurer, STOC 2010).
  Our main result is that the Small-Set Expansion Hypothesis is in fact
equivalent to a variant of the Unique Games Conjecture. More precisely, the
hypothesis is equivalent to the Unique Games Conjecture restricted to instance
with a fairly mild condition on the expansion of small sets. Alongside, we
obtain the first strong hardness of approximation results for the Balanced
Separator and Minimum Linear Arrangement problems. Before, no such hardness was
known for these problems even assuming the Unique Games Conjecture.
  These results not only establish the Small-Set Expansion Hypothesis as a
natural unifying hypothesis that implies the Unique Games Conjecture, all its
consequences and, in addition, hardness results for other problems like
Balanced Separator and Minimum Linear Arrangement, but our results also show
that the Small-Set Expansion Hypothesis problem lies at the combinatorial heart
of the Unique Games Conjecture.
  The key technical ingredient is a new way of exploiting the structure of the
Unique Games instances obtained from the Small-Set Expansion Hypothesis via
(Raghavendra, Steurer, 2010). This additional structure allows us to modify
standard reductions in a way that essentially destroys their local-gadget
nature. Using this modification, we can argue about the expansion in the graphs
produced by the reduction without relying on expansion properties of the
underlying Unique Games instance (which would be impossible for a local-gadget
reduction)."
"The relationship between the complexity classes P and NP is a question that
has not yet been answered by the Theory of Computation. The existence of a
language in NP, proven not to belong to P, is sufficient evidence to establish
the separation of P from NP. If a language is not recursive, it can't belong to
the complexity class NP. We find a problem in NP which is not in P; because if
it would be present in that class, then it will imply that some undecidable
problem will be in NP too. That's why it can be confirmed by reduction ad
absurdum the following result: P doesn't equal NP. This new problem named
Certifying is to find a possible input given a particular deterministic Turing
machine named Certified Turing machine and its output."
"A celebrated theorem of Savitch states that NSPACE(S) is contained in
DSPACE(S^2). In particular, Savitch gave a deterministic algorithm to solve
ST-CONNECTIVITY (an NL-complete problem) using O(log^2{n}) space, implying NL
is in DSPACE(log^2{n}). While Savitch's theorem itself has not been improved in
the last four decades, studying the space complexity of several special cases
of ST-CONNECTIVITY has provided new insights into the space-bounded complexity
classes.
  In this paper, we introduce new kind of graph connectivity problems which we
call graph realizability problems. All of our graph realizability problems are
generalizations of UNDIRECTED ST-CONNECTIVITY. ST-REALIZABILITY, the most
general graph realizability problem, is LogCFL-complete. We define the
corresponding complexity classes that lie between L and LogCFL and study their
relationships.
  As special cases of our graph realizability problems we define two natural
problems, BALANCED ST-CONNECTIVITY and POSITIVE BALANCED ST-CONNECTIVITY, that
lie between L and NL. We present a deterministic O(lognloglogn) space algorithm
for BALANCED ST-CONNECTIVITY. More generally we prove that SGSLogCFL, a
generalization of BALANCED ST-CONNECTIVITY, is contained in
DSPACE(lognloglogn). To achieve this goal we generalize several concepts (such
as graph squaring and transitive closure) and algorithms (such as parallel
algorithms) known in the context of UNDIRECTED ST-CONNECTIVITY."
"We introduce a new technique for proving kernelization lower bounds, called
cross-composition. A classical problem L cross-composes into a parameterized
problem Q if an instance of Q with polynomially bounded parameter value can
express the logical OR of a sequence of instances of L. Building on work by
Bodlaender et al. (ICALP 2008) and using a result by Fortnow and Santhanam
(STOC 2008) we show that if an NP-complete problem cross-composes into a
parameterized problem Q then Q does not admit a polynomial kernel unless the
polynomial hierarchy collapses. Our technique generalizes and strengthens the
recent techniques of using OR-composition algorithms and of transferring the
lower bounds via polynomial parameter transformations. We show its
applicability by proving kernelization lower bounds for a number of important
graphs problems with structural (non-standard) parameterizations, e.g.,
Chromatic Number, Clique, and Weighted Feedback Vertex Set do not admit
polynomial kernels with respect to the vertex cover number of the input graphs
unless the polynomial hierarchy collapses, contrasting the fact that these
problems are trivially fixed-parameter tractable for this parameter. We have
similar lower bounds for Feedback Vertex Set."
"Constraint Satisfaction Problems (CSP) constitute a convenient way to capture
many combinatorial problems. The general CSP is known to be NP-complete, but
its complexity depends on a template, usually a set of relations, upon which
they are constructed. Following this template, there exist tractable and
intractable instances of CSPs. It has been proved that for each CSP problem
over a given set of relations there exists a corresponding CSP problem over
graphs of unary functions belonging to the same complexity class. In this short
note we show a dichotomy theorem for every finite domain D of CSP built upon
graphs of homogeneous co-Boolean functions, i.e., unary functions sharing the
Boolean range {0, 1}."
"Throughout this article we develop and change the definitions and the ideas
in ""arXiv:1006.4939"", in order to consider the efficiency of functions and
complexity time problems. The central idea here is effective enumeration and
listing, and efficiency of function which is defined between two sets proposed
in basic definitions. More in detail, it might be that h and g were co-order
but the velocity of them be different."
"Border basis detection (BBD) is described as follows: given a set of
generators of an ideal, decide whether that set of generators is a border basis
of the ideal with respect to some order ideal. The motivation for this problem
comes from a similar problem related to Gr\""obner bases termed as Gr\""obner
basis detection (GBD) which was proposed by Gritzmann and Sturmfels (1993). GBD
was shown to be NP-hard by Sturmfels and Wiegelmann (1996). In this paper, we
investigate the computational complexity of BBD and show that it is
NP-complete."
"The results of Strassen and Raz show that good enough tensor rank lower
bounds have implications for algebraic circuit/formula lower bounds.
  We explore tensor rank lower and upper bounds, focusing on explicit tensors.
For odd d, we construct field-independent explicit 0/1 tensors T:[n]^d->F with
rank at least 2n^(floor(d/2))+n-Theta(d log n). This matches (over F_2) or
improves (all other fields) known lower bounds for d=3 and improves (over any
field) for odd d>3.
  We also explore a generalization of permutation matrices, which we denote
permutation tensors. We show, by counting, that there exists an order-3
permutation tensor with super-linear rank. We also explore a natural class of
permutation tensors, which we call group tensors. For any group G, we define
the group tensor T_G^d:G^d->F, by T_G^d(g_1,...,g_d)=1 iff g_1 x ... x g_d=1_G.
We give two upper bounds for the rank of these tensors. The first uses
representation theory and works over large fields F, showing (among other
things) that rank_F(T_G^d)<= |G|^(d/2). We also show that if this upper bound
is tight, then super-linear tensor rank lower bounds would follow. The second
upper bound uses interpolation and only works for abelian G, showing that over
any field F that rank_F(T_G^d)<= O(|G|^(1+log d)log^(d-1)|G|). In either case,
this shows that many permutation tensors have far from maximal rank, which is
very different from the matrix case and thus eliminates many natural candidates
for high tensor rank.
  We also explore monotone tensor rank. We give explicit 0/1 tensors T:[n]^d->F
that have tensor rank at most dn but have monotone tensor rank exactly n^(d-1).
This is a nearly optimal separation."
"The synchronized bit communication model, defined recently by Impagliazzo and
Williams in \emph{Communication complexity with synchronized clocks}, CCC '10,
is a communication model which allows the participants to share a common clock.
The main open problem posed in this paper was the following: does the
synchronized bit model allow a logarithmic speed-up for all functions over the
standard deterministic model of communication? We resolve this question in the
negative by showing that the Median function, whose communication complexity is
$O(\log n)$, does not admit polytime synchronized bit protocol with
communication complexity $O\left(\log^{1-\epsilon} n\right)$ for any $\epsilon
> 0$. Our results follow by a new round-communication trade-off for the Median
function in the standard model, which easily translates to its hardness in the
synchronized bit model."
"We give a new algorithm for Unique Games which is based on purely {\em
spectral} techniques, in contrast to previous work in the area, which relies
heavily on semidefinite programming (SDP). Given a highly satisfiable instance
of Unique Games, our algorithm is able to recover a good assignment. The
approximation guarantee depends only on the completeness of the game, and not
on the alphabet size, while the running time depends on spectral properties of
the {\em Label-Extended} graph associated with the instance of Unique Games.
  We further show that on input the integrality gap instance of Khot and
Vishnoi, our algorithm runs in quasi-polynomial time and decides that the
instance if highly unsatisfiable. Notably, when run on this instance, the
standard SDP relaxation of Unique Games {\em fails}. As a special case, we also
re-derive a polynomial time algorithm for Unique Games on expander constraint
graphs.
  The main ingredient of our algorithm is a technique to effectively use the
full spectrum of the underlying graph instead of just the second eigenvalue,
which is of independent interest. The question of how to take advantage of the
full spectrum of a graph in the design of algorithms has been often studied,
but no significant progress was made prior to this work."
"We report new results on the complexity of the valued constraint satisfaction
problem (VCSP). Under the unique games conjecture, the approximability of
finite-valued VCSP is fairly well-understood. However, there is yet no
characterisation of VCSPs that can be solved exactly in polynomial time. This
is unsatisfactory, since such results are interesting from a combinatorial
optimisation perspective; there are deep connections with, for instance,
submodular and bisubmodular minimisation. We consider the Min and Max CSP
problems (i.e. where the cost functions only attain values in {0,1}) over
four-element domains and identify all tractable fragments. Similar
classifications were previously known for two- and three-element domains. In
the process, we introduce a new class of tractable VCSPs based on a
generalisation of submodularity. We also extend and modify a graph-based
technique by Kolmogorov and Zivny (originally introduced by Takhanov) for
efficiently obtaining hardness results in our setting. This allow us to prove
the result without relying on computer-assisted case analyses (which otherwise
are fairly common when studying the complexity and approximability of VCSPs.)
The hardness results are further simplified by the introduction of powerful
reduction techniques."
"We show that the class BPP is in NP and coNP. This paper has been withdrawn
by the author because B and B' are probabilistic and nonequalities 10 cannot be
checked in polynomial time."
"We give a simpler proof, via query elimination, of a result due to O'Donnell,
Saks, Schramm and Servedio, which shows a lower bound on the zero-error
randomized query complexity of a function f in terms of the maximum influence
of any variable of f. Our lower bound also applies to the two-sided error
distributional query complexity of f, and it allows an immediate extension
which can be used to prove stronger lower bounds for some functions."
"In this paper, the author defines Generalized Unique Game Problem (GUGP),
where weights of the edges are allowed to be negative. Two special types of
GUGP are illuminated, GUGP-NWA, where the weights of all edges are negative,
and GUGP-PWT($\rho$), where the total weight of all edges are positive and the
negative-positive ratio is at most $\rho$. The author investigates the
counterpart of the Unique Game Conjecture on GUGP-PWT($\rho$). The author shows
that Unique Game Conjecture on GUGP-PWT(1) holds true, and Unique Game
Conjecture on GUGP-PWT(1/2) holds true, if the 2-to-1 Conjecture holds true.
The author poses an open problem whether Unique Game Conjecture holds true on
GUGP-PWT($\rho$) with $0<\rho<1$."
"Scribe notes from the 2012 Barbados Workshop on Computational Complexity. A
series of lectures on Analysis of Boolean Functions by Ryan O'Donnell, with a
guest lecture by Per Austrin."
"Stencil computations on low dimensional grids are kernels of many scientific
applications including finite difference methods used to solve partial
differential equations. On typical modern computer architectures, such stencil
computations are limited by the performance of the memory subsystem, namely by
the bandwidth between main memory and the cache. This work considers the
computation of star stencils, like the 5-point and 7-point stencil, in the
external memory model and parallel external memory model and analyses the
constant of the leading term of the non-compulsory I/Os. While optimizing
stencil computations is an active field of research, there has been a
significant gap between the lower bounds and the performance of the algorithms
so far. In two dimensions, this work provides matching constants for lower and
upper bounds closing a multiplicative gap of 4. In three dimensions, the bounds
match up to a factor of $\sqrt{2}$ improving the known results by a factor of
$2 \sqrt{3}\sqrt{B}$, where $B$ is the block (cache line) size of the external
memory model. For dimensions $d\geq 4$, the lower bound is improved between a
factor of $4$ and $6$. For arbitrary dimension~$d$, the first analysis of the
constant of the leading term of the non-compulsory I/Os is presented. For
$d\geq 3$ the lower and upper bound match up to a factor of
$\sqrt[d-1]{d!}\approx \frac{d}{e}$."
"In this note, we prove a version of Tarui's Theorem in communication
complexity, namely $PH^{cc} \subseteq BP\cdot PP^{cc}$. Consequently, every
measure for $PP^{cc}$ leads to a measure for $PH^{cc}$, subsuming a result of
Linial and Shraibman that problems with high mc-rigidity lie outside the
polynomial hierarchy. By slightly changing the definition of mc-rigidity
(arbitrary instead of uniform distribution), it is then evident that the class
$M^{cc}$ of problems with low mc-rigidity equals $BP\cdot PP^{cc}$. As $BP\cdot
PP^{cc} \subseteq PSPACE^{cc}$, this rules out the possibility, that had been
left open, that even polynomial space is contained in $M^{cc}$."
"According to the real \tau-conjecture, the number of real roots of a sum of
products of sparse polynomials should be polynomially bounded in the size of
such an expression. It is known that this conjecture implies a superpolynomial
lower bound on the arithmetic circuit complexity of the permanent.
  In this paper, we use the Wronksian determinant to give an upper bound on the
number of real roots of sums of products of sparse polynomials. The proof
technique is quite versatile; it can in particular be applied to some sparse
geometric problems that do not originate from arithmetic circuit complexity.
The paper should therefore be of interest to researchers from these two
communities (complexity theory and sparse polynomial systems)."
"A two-state spin system is specified by a 2 x 2 matrix
  A = {A_{0,0} A_{0,1}, A_{1,0} A_{1,1}} = {\beta 1, 1 \gamma} where \beta,
\gamma \ge 0. Given an input graph G=(V,E), the partition function Z_A(G) of a
system is defined as
  Z_A(G) = \sum_{\sigma: V -> {0,1}} \prod_{(u,v) \in E} A_{\sigma(u),
\sigma(v)}
  We prove inapproximability results for the partition function in the region
specified by the non-uniqueness condition from phase transition for the Gibbs
measure. More specifically, assuming NP \ne RP, for any fixed \beta, \gamma in
the unit square, there is no randomized polynomial-time algorithm that
approximates Z_A(G) for d-regular graphs G with relative error \epsilon =
10^{-4}, if d = \Omega(\Delta(\beta,\gamma)), where \Delta(\beta,\gamma) >
1/(1-\beta\gamma) is the uniqueness threshold. Up to a constant factor, this
hardness result confirms the conjecture that the uniqueness phase transition
coincides with the transition from computational tractability to intractability
for Z_A(G). We also show a matching inapproximability result for a region of
parameters \beta, \gamma outside the unit square, and all our results
generalize to partition functions with an external field."
"Given a DNF formula on n variables, the two natural size measures are the
number of terms or size s(f), and the maximum width of a term w(f). It is
folklore that short DNF formulas can be made narrow. We prove a converse,
showing that narrow formulas can be sparsified. More precisely, any width w DNF
irrespective of its size can be $\epsilon$-approximated by a width $w$ DNF with
at most $(w\log(1/\epsilon))^{O(w)}$ terms.
  We combine our sparsification result with the work of Luby and Velikovic to
give a faster deterministic algorithm for approximately counting the number of
satisfying solutions to a DNF. Given a formula on n variables with poly(n)
terms, we give a deterministic $n^{\tilde{O}(\log \log(n))}$ time algorithm
that computes an additive $\epsilon$ approximation to the fraction of
satisfying assignments of f for $\epsilon = 1/\poly(\log n)$. The previous best
result due to Luby and Velickovic from nearly two decades ago had a run-time of
$n^{\exp(O(\sqrt{\log \log n}))}$."
"Admin note: withdrawn by arXiv admin because of the use of a pseudonym, in
violation of arXiv policy."
"A c-coloring of G(n,m)=n x m is a mapping of G(n,m) into {1,...,c} such that
no four corners forming a rectangle have the same color. In 2009 a challenge
was proposed via the internet to find a 4-coloring of G(17,17). This attracted
considerable attention from the popular mathematics community. A coloring was
produced; however, finding it proved to be difficult. The question arises: is
the problem of grid coloring is difficult in general? We present three results
that support this conjecture, (1) an NP completeness result, (2) a lower bound
on Tree-resolution, (3) a lower bound on Tree-CP proofs. Note that items (2)
and (3) yield statements from Ramsey Theory which are of size polynomial in
their parameters and require exponential size in various proof systems."
"In this paper, we build on the idea of Valiant \cite{Val79a} and
Ben-Dor/Halevi \cite{Ben93}, that is, to count the number of satisfying
solutions of a boolean formula via computing the permanent of a specially
constructed matrix. We show that the Desnanot-Jacobi identity ($\dji$) prevents
Valiant's original approach to achieve a parsimonious reduction to the
permanent over a field of characteristic two. As the next step, since the
computation of the permanent is $#\classP$-complete, we make use of the
equality of the permanent and the number of perfect matchings in an unweighted
graph's bipartite double cover. Whenever this bipartite double cover (BDC) is
planar, the number of perfect matchings can be counted in polynomial time using
Kasteleyn's algorithm \cite{Kas67}. To enforce planarity of the BDC, we replace
Valiant's original gadgets with new gadgets and describe what properties these
gadgets must have. We show that the property of \textit{circular planarity}
plays a crucial role to find the correct gadgets for a counting problem. To
circumvent the $\dji$-barrier, we switch over to fields
$\mathbb{Z}/p\mathbb{Z}$, for a prime $p > 2$.
  With this approach we are able to count the number of solutions for
$\forestdreisat$ formulas in randomized polynomial time. Finally, we present a
conjecture that states which kind of generalized gadgets can not be found,
since otherwise one could prove $\classRP = \classNP$. The conjecture
establishes a relationship between the determinants of the minors of a graph
$\grG$'s adjacency matrix and the \textit{circular planar} structure of
$\grG$'s BDC regarding a given set of nodes."
"We determine the constructive dimension of points in random translates of the
Cantor set. The Cantor set ""cancels randomness"" in the sense that some of its
members, when added to Martin-Lof random reals, identify a point with lower
constructive dimension than the random itself. In particular, we find the
Hausdorff dimension of the set of points in a Cantor set translate with a given
constructive dimension."
"We propose to solve any algorithm on discrete variables by a technique of
statistical estimation using deterministic convex analysis. In this framework,
the variables are represented by their probability and the distinction between
the complexity classes vanishes. The method is illustrated by solving the 3-SAT
problem in polynomial time."
"Universal algebra and clone theory have proven to be a useful tool in the
study of constraint satisfaction problems since the complexity, up to logspace
reductions, is determined by the set of polymorphisms of the constraint
language. For classifications where primitive positive definitions are
unsuitable, such as size-preserving reductions, weaker closure operations may
be necessary. In this article we consider strong partial clones which can be
seen as a more fine-grained framework than Post's lattice where each clone
splits into an interval of strong partial clones. We investigate these
intervals and give simple relational descriptions, weak bases, of the largest
elements. The weak bases have a highly regular form and are in many cases
easily relatable to the smallest members in the intervals, which suggests that
the lattice of strong partial clones is considerably simpler than the full
lattice of partial clones."
"We discuss approximability and inapproximability in FPT-time for a large
class of subset problems where a feasible solution $S$ is a subset of the input
data and the value of $S$ is $|S|$. The class handled encompasses many
well-known graph, set, or satisfiability problems such as Dominating Set,
Vertex Cover, Set Cover, Independent Set, Feedback Vertex Set, etc. In a first
time, we introduce the notion of intersective approximability that generalizes
the one of safe approximability and show strong parameterized inapproximability
results for many of the subset problems handled. Then, we study approximability
of these problems with respect to the dual parameter $n-k$ where $n$ is the
size of the instance and $k$ the standard parameter. More precisely, we show
that under such a parameterization, many of these problems, while
W[$\cdot$]-hard, admit parameterized approximation schemata."
"Knowledge Compilation (KC) studies compilation of boolean functions f into
some formalism F, which allows to answer all queries of a certain kind in
polynomial time. Due to its relevance for SAT solving, we concentrate on the
query type ""clausal entailment"" (CE), i.e., whether a clause C follows from f
or not, and we consider subclasses of CNF, i.e., clause-sets F with special
properties. In this report we do not allow auxiliary variables (except of the
Outlook), and thus F needs to be equivalent to f.
  We consider the hierarchies UC_k <= WC_k, which were introduced by the
authors in 2012. Each level allows CE queries. The first two levels are
well-known classes for KC. Namely UC_0 = WC_0 is the same as PI as studied in
KC, that is, f is represented by the set of all prime implicates, while UC_1 =
WC_1 is the same as UC, the class of unit-refutation complete clause-sets
introduced by del Val 1994. We show that for each k there are (sequences of)
boolean functions with polysize representations in UC_{k+1}, but with an
exponential lower bound on representations in WC_k. Such a separation was
previously only know for k=0. We also consider PC < UC, the class of
propagation-complete clause-sets. We show that there are (sequences of) boolean
functions with polysize representations in UC, while there is an exponential
lower bound for representations in PC. These separations are steps towards a
general conjecture determining the representation power of the hierarchies PC_k
< UC_k <= WC_k. The strong form of this conjecture also allows auxiliary
variables, as discussed in depth in the Outlook."
"A 1976 theorem of Chaitin can be used to show that arbitrarily dense sets of
lengths n have a paucity of trivial strings (only a bounded number of strings
of length n having trivially low plain Kolmogorov complexities). We use the
probabilistic method to give a new proof of this fact. This proof is much
simpler than previously published proofs, and it gives a tighter paucity bound."
"The logical depth with significance $b$ of a finite binary string $x$ is the
shortest running time of a binary program for $x$ that can be compressed by at
most $b$ bits. There is another definition of logical depth. We give two
theorems about the quantitative relation between these versions: the first
theorem concerns a variation of a known fact with a new proof, the second
theorem and its proof are new. We select the above version of logical depth and
show the following. There is an infinite sequence of strings of increasing
length such that for each $j$ there is a $b$ such that the logical depth of the
$j$th string as a function of $j$ is incomputable (it rises faster than any
computable function) but with $b$ replaced by $b+1$ the resuling function is
computable. Hence the maximal gap between the logical depths resulting from
incrementing appropriate $b$'s by 1 rises faster than any computable function.
All functions mentioned are upper bounded by the Busy Beaver function. Since
for every string its logical depth is nonincreasing in $b$, the minimal
computation time of the shortest programs for the sequence of strings as a
function of $j$ rises faster than any computable function but not so fast as
the Busy Beaver function."
"The Immerman-Szelepcsenyi Theorem uses an algorithm for co-st- connectivity
based on inductive counting to prove that NLOGSPACE is closed un- der
complementation. We want to investigate whether counting is necessary for this
theorem to hold. Concretely, we show that Nondeterministic Jumping Graph
Autmata (ND-JAGs) (pebble automata on graphs), on several families of Cayley
graphs, are equal in power to nondeterministic logspace Turing machines that
are given such graphs as a linear encoding. In particular, it follows that
ND-JAGs can solve co-st-connectivity on those graphs. This came as a surprise
since Cook and Rackoff showed that deterministic JAGs cannot solve
st-connectivity on many Cayley graphs due to their high self-similarity (every
neighbourhood looks the same). Thus, our results show that on these graphs,
nondeterminism provably adds computational power. The families of Cayley graphs
we consider include Cayley graphs of abelian groups and of all finite simple
groups irrespective of how they are presented and graphs corresponding to
groups generated by various product constructions, in- cluding iterated ones.
We remark that assessing the precise power of nondeterministic JAGs and in par-
ticular whether they can solve co-st-connectivity on arbitrary graphs is left
as an open problem by Edmonds, Poon and Achlioptas. Our results suggest a
positive answer to this question and in particular considerably limit the
search space for a potential counterexample."
"We recall from previous work a model-independent framework of computational
complexity theory. Notably for the present paper, the framework allows
formalization of the issues of precision that present themselves when one
considers physical, error-prone (especially analogue rather than digital)
computational systems. We take as a case study the ray-tracing problem, a
Turing-machine-incomputable problem that can, in apparent violation of the
Church-Turing thesis, nonetheless be said to be solved by certain optical
computers; however, we apply the framework of complexity theory so as to
formalize the intuition that the purported super-Turing power of these
computers in fact vanishes once precision is properly considered."
"In this paper, two structural results concerning low degree polynomials over
finite fields are given. The first states that over any finite field
$\mathbb{F}$, for any polynomial $f$ on $n$ variables with degree $d \le
\log(n)/10$, there exists a subspace of $\mathbb{F}^n$ with dimension $\Omega(d
\cdot n^{1/(d-1)})$ on which $f$ is constant. This result is shown to be tight.
Stated differently, a degree $d$ polynomial cannot compute an affine disperser
for dimension smaller than $\Omega(d \cdot n^{1/(d-1)})$. Using a recursive
argument, we obtain our second structural result, showing that any degree $d$
polynomial $f$ induces a partition of $F^n$ to affine subspaces of dimension
$\Omega(n^{1/(d-1)!})$, such that $f$ is constant on each part.
  We extend both structural results to more than one polynomial. We further
prove an analog of the first structural result to sparse polynomials (with no
restriction on the degree) and to functions that are close to low degree
polynomials. We also consider the algorithmic aspect of the two structural
results.
  Our structural results have various applications, two of which are:
  * Dvir [CC 2012] introduced the notion of extractors for varieties, and gave
explicit constructions of such extractors over large fields. We show that over
any finite field, any affine extractor is also an extractor for varieties with
related parameters. Our reduction also holds for dispersers, and we conclude
that Shaltiel's affine disperser [FOCS 2011] is a disperser for varieties over
$F_2$.
  * Ben-Sasson and Kopparty [SIAM J. C 2012] proved that any degree 3 affine
disperser over a prime field is also an affine extractor with related
parameters. Using our structural results, and based on the work of Kaufman and
Lovett [FOCS 2008] and Haramaty and Shpilka [STOC 2010], we generalize this
result to any constant degree."
"In the field of algorithmic self-assembly, a long-standing unproven
conjecture has been that of the NP-hardness of binary pattern tile set
synthesis (2-PATS). The $k$-PATS problem is that of designing a tile assembly
system with the smallest number of tile types which will self-assemble an input
pattern of $k$ colors. Of both theoretical and practical significance, $k$-PATS
has been studied in a series of papers which have shown $k$-PATS to be NP-hard
for $k = 60$, $k = 29$, and then $k = 11$. In this paper, we close the
fundamental conjecture that 2-PATS is NP-hard, concluding this line of study.
  While most of our proof relies on standard mathematical proof techniques, one
crucial lemma makes use of a computer-assisted proof, which is a relatively
novel but increasingly utilized paradigm for deriving proofs for complex
mathematical problems. This tool is especially powerful for attacking
combinatorial problems, as exemplified by the proof of the four color theorem
by Appel and Haken (simplified later by Robertson, Sanders, Seymour, and
Thomas) or the recent important advance on the Erd\H{o}s discrepancy problem by
Konev and Lisitsa using computer programs. We utilize a massively parallel
algorithm and thus turn an otherwise intractable portion of our proof into a
program which requires approximately a year of computation time, bringing the
use of computer-assisted proofs to a new scale. We fully detail the algorithm
employed by our code, and make the code freely available online."
"We devise a new pseudorandom generator against degree 2 polynomial threshold
functions in the Gaussian setting. We manage to achieve $\epsilon$ error with
seed length polylogarithmic in $\epsilon$ and the dimension, and exponential
improvement over previous constructions."
"The Fourier Transform is one of the most important linear transformations
used in science and engineering. Cooley and Tukey's Fast Fourier Transform
(FFT) from 1964 is a method for computing this transformation in time $O(n\log
n)$. Achieving a matching lower bound in a reasonable computational model is
one of the most important open problems in theoretical computer science.
  In 2014, improving on his previous work, Ailon showed that if an algorithm
speeds up the FFT by a factor of $b=b(n)\geq 1$, then it must rely on
computing, as an intermediate ""bottleneck"" step, a linear mapping of the input
with condition number $\Omega(b(n))$. Our main result shows that a factor $b$
speedup implies existence of not just one but $\Omega(n)$ $b$-ill conditioned
bottlenecks occurring at $\Omega(n)$ different steps, each causing information
from independent (orthogonal) components of the input to either overflow or
underflow. This provides further evidence that beating FFT is hard. Our result
also gives the first quantitative tradeoff between computation speed and
information loss in Fourier computation on fixed word size architectures. The
main technical result is an entropy analysis of the Fourier transform under
transformations of low trace, which is interesting in its own right."
"We prove exponential lower bounds on the size of homogeneous depth 4
arithmetic circuits computing an explicit polynomial in $VP$. Our results hold
for the {\it Iterated Matrix Multiplication} polynomial - in particular we show
that any homogeneous depth 4 circuit computing the $(1,1)$ entry in the product
of $n$ generic matrices of dimension $n^{O(1)}$ must have size
$n^{\Omega(\sqrt{n})}$.
  Our results strengthen previous works in two significant ways.
  Our lower bounds hold for a polynomial in $VP$. Prior to our work, Kayal et
al [KLSS14] proved an exponential lower bound for homogeneous depth 4 circuits
(over fields of characteristic zero) computing a poly in $VNP$. The best known
lower bounds for a depth 4 homogeneous circuit computing a poly in $VP$ was the
bound of $n^{\Omega(\log n)}$ by [LSS, KLSS14].Our exponential lower bounds
also give the first exponential separation between general arithmetic circuits
and homogeneous depth 4 arithmetic circuits. In particular they imply that the
depth reduction results of Koiran [Koi12] and Tavenas [Tav13] are tight even
for reductions to general homogeneous depth 4 circuits (without the restriction
of bounded bottom fanin).
  Our lower bound holds over all fields. The lower bound of [KLSS14] worked
only over fields of characteristic zero. Prior to our work, the best lower
bound for homogeneous depth 4 circuits over fields of positive characteristic
was $n^{\Omega(\log n)}$ [LSS, KLSS14]."
"Patterned self-assembly tile set synthesis (PATS) aims at finding a minimum
tile set to uniquely self-assemble a given rectangular color pattern. For $k
\ge 1$, $k$-PATS is a variant of PATS that restricts input patterns to those
with at most $k$ colors. We prove the {\bf NP}-hardness of 29-PATS, where the
best known is that of 60-PATS."
"Let $f\colon \{-1,1\}^n \to [-1,1]$ have degree $d$ as a multilinear
polynomial. It is well-known that the total influence of $f$ is at most $d$.
Aaronson and Ambainis asked whether the total $L_1$ influence of $f$ can also
be bounded as a function of $d$. Ba\v{c}kurs and Bavarian answered this
question in the affirmative, providing a bound of $O(d^3)$ for general
functions and $O(d^2)$ for homogeneous functions. We improve on their results
by providing a bound of $d^2$ for general functions and $O(d\log d)$ for
homogeneous functions. In addition, we prove a bound of $d/(2 \pi)+o(d)$ for
monotone functions, and provide a matching example."
"Let $G=(V,E)$ be a simple graph with $|V|=n$ nodes and $|E|=m$ links, a
subset $K \subseteq V$ of \emph{terminals}, a vector $p=(p_1,\ldots,p_m) \in
[0,1]^m$ and a positive integer $d$, called \emph{diameter}. We assume nodes
are perfect but links fail stochastically and independently, with probabilities
$q_i=1-p_i$. The \emph{diameter-constrained reliability} (DCR for short), is
the probability that the terminals of the resulting subgraph remain connected
by paths composed by $d$ links, or less. This number is denoted by
$R_{K,G}^{d}(p)$.
  The general DCR computation is inside the class of
$\mathcal{N}\mathcal{P}$-Hard problems, since is subsumes the complexity that a
random graph is connected. In this paper, the computational complexity of
DCR-subproblems is discussed in terms of the number of terminal nodes $k=|K|$
and diameter $d$. Either when $d=1$ or when $d=2$ and $k$ is fixed, the DCR is
inside the class $\mathcal{P}$ of polynomial-time problems. The DCR turns
$\mathcal{N}\mathcal{P}$-Hard when $k \geq 2$ is a fixed input parameter and
$d\geq 3$.
  The case where $k=n$ and $d \geq 2$ is fixed are not studied in prior
literature. Here, the $\mathcal{N}\mathcal{P}$-Hardness of this case is
established."
"We show that an effective version of Siegel's Theorem on finiteness of
integer solutions and an application of elementary Galois theory are key
ingredients in a complexity classification of some Holant problems. These
Holant problems, denoted by Holant(f), are defined by a symmetric ternary
function f that is invariant under any permutation of the k >= 3 domain
elements. We prove that Holant(f) exhibits a complexity dichotomy. This
dichotomy holds even when restricted to planar graphs. A special case of this
result is that counting edge k-colorings is #P-hard over planar 3-regular
graphs for k >= 3. In fact, we prove that counting edge k-colorings is #P-hard
over planar r-regular graphs for all k >= r >= 3. The problem is
polynomial-time computable in all other parameter settings. The proof of the
dichotomy theorem for Holant(f) depends on the fact that a specific polynomial
p(x,y) has an explicitly listed finite set of integer solutions, and the
determination of the Galois groups of some specific polynomials. In the
process, we also encounter the Tutte polynomial, medial graphs, Eulerian
partitions, Puiseux series, and a certain lattice condition on the (logarithm
of) the roots of polynomials."
"We extend the Ax-Katz theorem for a single polynomial from finite fields to
the rings Z_m with m composite. This extension not only yields the analogous
result, but gives significantly higher divisibility bounds. We conjecture what
computer runs suggest is the optimal result for any m, and prove a special case
of it. The special case is for m = 2^r and polynomials of degree 2. Our results
also yield further properties of the solution spaces. Polynomials modulo
composites are the focus of some computational complexity lower bound
frontiers, while those modulo 2^r arise in the simulation of quantum circuits.
We give some prospective applications of this research."
"There is a close connection between Direct Product and XOR lemmas in the
sense that in many settings, we can prove one given the other. The known
reductions that are used for the above purpose are either in the non-uniform
setting or give non-matching parameters. By non-matching parameter we mean that
$k$-wise Direct Product lemma implies $k'$-wise XOR lemma (and vice versa) for
$k \neq k'$. In this work, we discuss reductions between $k$-wise Direct
Product and $k$-wise XOR lemmas. That is, we show that if the $k$-wise direct
product lemma holds, then so does the $k$-wise XOR lemma and vice versa. We
show that even though there is a perfectly uniform reduction in one direction,
the reduction in the other direction requires some amount of non-uniformity. We
give reductions in both directions matching information-theoretic bounds up to
polynomial factors. Our techniques also give a small quantitative improvement
over the known results about proving $k$-wise XOR lemma using $2k$-wise Direct
Product lemma."
"This paper is a critique of version three of Joonmo Kim's paper entitled ""P
is not equal to NP by Modus Tollens. [arXiv:1403.4143v3]"" After summarizing
Kim's proof, we note that the logic that Kim uses is inconsistent, which
provides evidence that the proof is invalid. To show this, we will consider two
reasonable interpretations of Kim's definitions, and show that ""P is not equal
to NP"" does not seem to follow in an obvious way using any of them."
"A computable real function F on [0,1] is constructed such that there exists
an exponential time algorithm for the evaluation of the function on [0,1] on
Turing machine but there does not exist any polynomial time algorithm for the
evaluation of the function on [0,1] on Turing machine (moreover, it holds for
any rational point on (0,1))"
"We study depth lower bounds against non-monotone circuits, parametrized by a
new measure of non-monotonicity: the orientation of a function $f$ is the
characteristic vector of the minimum sized set of negated variables needed in
any DeMorgan circuit computing $f$. We prove trade-off results between the
depth and the weight/structure of the orientation vectors in any circuit $C$
computing the Clique function on an $n$ vertex graph. We prove that if $C$ is
of depth $d$ and each gate computes a Boolean function with orientation of
weight at most $w$ (in terms of the inputs to $C$), then $d \times w$ must be
$\Omega(n)$. In particular, if the weights are $o(\frac{n}{\log^k n})$, then
$C$ must be of depth $\omega(\log^k n)$. We prove a barrier for our general
technique. However, using specific properties of the Clique function and the
Karchmer-Wigderson framework (Karchmer and Wigderson, 1988), we go beyond the
limitations and obtain lower bounds when the weight restrictions are less
stringent. We then study the depth lower bounds when the structure of the
orientation vector is restricted. Asymptotic improvements to our results (in
the restricted setting), separates NP from NC. As our main tool, we generalize
Karchmer-Wigderson gamefor monotone functions to work for non-monotone circuits
parametrized by the weight/structure of the orientation. We also prove
structural results about orientation and prove connections between number of
negations and weight of orientations required to compute a function."
"A polynomial algorithm for solving ""Hamiltonian circuit"" problem is presented
in the paper. Computational complexity of the algorithm is equal
$O(n^{8}{\log_2}^2n)$ where $n$ is the cardinality of the observed graph vertex
set. Thus the polynomial solvability for ${\mathcal NP}$-complete problems is
proved."
"In this paper we investigate the computational complexity of solving ordinary
differential equations (ODEs) $y^{\prime}=p(y)$ over \emph{unbounded time
domains}, where $p$ is a vector of polynomials. Contrarily to the bounded
(compact) time case, this problem has not been well-studied, apparently due to
the ""intuition"" that it can always be reduced to the bounded case by using
rescaling techniques. However, as we show in this paper, rescaling techniques
do not seem to provide meaningful insights on the complexity of this problem,
since the use of such techniques introduces a dependence on parameters which
are hard to compute.
  We present algorithms which numerically solve these ODEs over unbounded time
domains. These algorithms have guaranteed accuracy, i.e. given some arbitrarily
large time $t$ and error bound $\varepsilon$ as input, they will output a value
$\tilde{y}$ which satisfies $\|y(t)-\tilde{y}\|\leq\varepsilon$. We analyze the
complexity of these algorithms and show that they compute $\tilde{y}$ in time
polynomial in several quantities including the time $t$, the accuracy of the
output $\varepsilon$ and the length of the curve $y$ from $0$ to $t$, assuming
it exists until time $t$. We consider both algebraic complexity and bit
complexity."
"Motivated by the recent developments on the complexity of
non-com\-mu\-ta\-tive determinant and permanent [Chien et al.\ STOC 2011,
Bl\""aser ICALP 2013, Gentry CCC 2014] we attempt at obtaining a tight
characterization of hard instances of non-commutative permanent.
  We show that computing Cayley permanent and determinant on weight\-ed
adjacency matrices of graphs of component size six is $\#{\sf P}$ complete on
algebras that contain $2\times 2$ matrices and the permutation group $S_3$.
Also, we prove a lower bound of $2^{\Omega(n)}$ on the size of branching
programs computing the Cayley permanent on adjacency matrices of graphs with
component size bounded by two. Further, we observe that the lower bound holds
for almost all graphs of component size two.
  On the positive side, we show that the Cayley permanent on graphs of
component size $c$ can be computed in time $n^{c{\sf poly}(t)}$, where $t$ is a
parameter depending on the labels of the vertices.
  Finally, we exhibit polynomials that are equivalent to the Cayley permanent
polynomial but are easy to compute over commutative domains."
"In this paper, we continue the study of the geometry of Brownian motions
which are encoded by Kolmogorov-Chaitin random reals (complex oscillations). We
unfold Kolmogorov-Chaitin complexity in the context of Brownian motion and
specifically to phenomena emerging from the random geometric patterns generated
by a Brownian motion."
"We study a stability property of probability laws with respect to small
violations of algorithmic randomness. A sufficient condition of stability is
presented in terms of Schnorr tests of algorithmic randomness. Most probability
laws, like the strong law of large numbers, the law of iterated logarithm, and
even Birkhoff's pointwise ergodic theorem for ergodic transformations, are
stable in this sense. Nevertheless, the phenomenon of instability occurs in
ergodic theory. Firstly, the stability property of the Birkhoff's ergodic
theorem is non-uniform. Moreover, a computable non-ergodic measure preserving
transformation can be constructed such that ergodic theorem is non-stable. We
also show that any universal data compression scheme is also non-stable with
respect to the class of all computable ergodic measures."
"We prove that for every sign matrix $A$ there is a deterministic
communication protocol that uses $O(corr_{1/4}(A)\log^2 rk(A))$ bits of
communication, where $corr_{1/4}(A)$ is the corruption/rectangle bound with
error $1/4$. This bound generalizes several of the known upper bounds on
deterministic communication complexity, involving nondeterministic complexity,
randomized complexity, information complexity notions, and rank.
  It also implies that the corruption bound is a lower bound on exact quantum
communication complexity, if and only if quantum communication is polynomially
equivalent to deterministic communication complexity."
"We study structural aspects of randomized parameterized computation. We
introduce a new class ${\sf W[P]}$-${\sf PFPT}$ as a natural parameterized
analogue of ${\sf PP}$. Our definition uses the machine based characterization
of the parameterized complexity class ${\sf W[P]}$ obtained by Chen et.al [TCS
2005]. We translate most of the structural properties and characterizations of
the class ${\sf PP}$ to the new class ${W[P]}$-${\sf PFPT}$.
  We study a parameterization of the polynomial identity testing problem based
on the degree of the polynomial computed by the arithmetic circuit. We obtain a
parameterized analogue of the well known Schwartz-Zippel lemma [Schwartz, JACM
80 and Zippel, EUROSAM 79].
  Additionally, we introduce a parameterized variant of permanent, and prove
its $\#W[1]$ completeness."
"On example of tasks of class NP the questions concerning accuracy of work of
already existing and possible in the future algorithms for the solution of
tasks on discrete structures are considered."
"An instance of Max CSP is a finite collection of constraints on a set of
variables, and the goal is to assign values to the variables that maximises the
number of satisfied constraints. Max CSP captures many well-known problems
(such as Max k-SAT and Max Cut) and is consequently NP-hard. Thus, it is
natural to study how restrictions on the allowed constraint types (or
constraint languages) affect the complexity and approximability of Max CSP. The
PCP theorem is equivalent to the existence of a constraint language for which
Max CSP has a hard gap at location 1, i.e. it is NP-hard to distinguish between
satisfiable instances and instances where at most some constant fraction of the
constraints are satisfiable. All constraint languages, for which the CSP
problem (i.e., the problem of deciding whether all constraints can be
satisfied) is currently known to be NP-hard, have a certain algebraic property.
We prove that any constraint language with this algebraic property makes Max
CSP have a hard gap at location 1 which, in particular, implies that such
problems cannot have a PTAS unless P = NP. We then apply this result to Max CSP
restricted to a single constraint type; this class of problems contains, for
instance, Max Cut and Max DiCut. Assuming P $\neq$ NP, we show that such
problems do not admit PTAS except in some trivial cases. Our results hold even
if the number of occurrences of each variable is bounded by a constant. We use
these results to partially answer open questions and strengthen results by
Engebretsen et al. [Theor. Comput. Sci., 312 (2004), pp. 17--45], Feder et al.
[Discrete Math., 307 (2007), pp. 386--392], Krokhin and Larose [Proc.
Principles and Practice of Constraint Programming (2005), pp. 388--402], and
Jonsson and Krokhin [J. Comput. System Sci., 73 (2007), pp. 691--702]"
"M.Aleknovich et al. have recently proposed a model of algorithms, called BT
model, which generalizes both the priority model of Borodin, Nielson and
Rackoff, as well as a simple dynamic programming model by Woeginger. BT model
can be further divided into three kinds of fixed, adaptive and fully adaptive
ones. They have proved exponential time lower bounds of exact and approximation
algorithms under adaptive BT model for Knapsack problem. Their exact lower
bound is $\Omega(2^{0.5n}/\sqrt{n})$, in this paper, we slightly improve the
exact lower bound to about $\Omega(2^{0.69n}/\sqrt{n})$, by the same technique,
with related parameters optimized."
"We show that disjointness requires randomized communication
Omega(n^{1/(k+1)}/2^{2^k}) in the general k-party number-on-the-forehead model
of complexity. The previous best lower bound for k >= 3 was log(n)/(k-1). Our
results give a separation between nondeterministic and randomized multiparty
number-on-the-forehead communication complexity for up to k=log log n - O(log
log log n) many players. Also by a reduction of Beame, Pitassi, and Segerlind,
these results imply subexponential lower bounds on the size of proofs needed to
refute certain unsatisfiable CNFs in a broad class of proof systems, including
tree-like Lovasz-Schrijver proofs."
"In Direct Sum problems [KRW], one tries to show that for a given
computational model, the complexity of computing a collection of finite
functions on independent inputs is approximately the sum of their individual
complexities. In this paper, by contrast, we study the diversity of ways in
which the joint computational complexity can behave when all the functions are
evaluated on a common input. We focus on the deterministic decision tree model,
with depth as the complexity measure; in this model we prove a result to the
effect that the 'obvious' constraints on joint computational complexity are
essentially the only ones.
  The proof uses an intriguing new type of cryptographic data structure called
a `mystery bin' which we construct using a small polynomial separation between
deterministic and unambiguous query complexity shown by Savicky. We also pose a
variant of the Direct Sum Conjecture of [KRW] which, if proved for a single
family of functions, could yield an analogous result for models such as the
communication model."
"This is the final article in a series of four articles. Richard Karp has
proven that a deterministic polynomial time solution to K-SAT will result in a
deterministic polynomial time solution to all NP-Complete problems. However, it
is demonstrated that a deterministic polynomial time solution to any
NP-Complete problem does not necessarily produce a deterministic polynomial
time solution to all NP-Complete problems."
"Multi-letter {\it quantum finite automata} (QFAs) were a new one-way QFA
model proposed recently by Belovs, Rosmanis, and Smotrovs (LNCS, Vol. 4588,
Springer, Berlin, 2007, pp. 60-71), and they showed that multi-letter QFAs can
accept with no error some regular languages ($(a+b)^{*}b$) that are
unacceptable by the one-way QFAs. In this paper, we continue to study
multi-letter QFAs. We mainly focus on two issues: (1) we show that
$(k+1)$-letter QFAs are computationally more powerful than $k$-letter QFAs,
that is, $(k+1)$-letter QFAs can accept some regular languages that are
unacceptable by any $k$-letter QFA. A comparison with the one-way QFAs is made
by some examples; (2) we prove that a $k_{1}$-letter QFA ${\cal A}_1$ and
another $k_{2}$-letter QFA ${\cal A}_2$ are equivalent if and only if they are
$(n_{1}+n_{2})^{4}+k-1$-equivalent, and the time complexity of determining the
equivalence of two multi-letter QFAs using this method is
$O(n^{12}+k^{2}n^{4}+kn^{8})$, where $n_{1}$ and $n_{2}$ are the numbers of
states of ${\cal A}_{1}$ and ${\cal A}_{2}$, respectively, and
$k=\max(k_{1},k_{2})$. Some other issues are addressed for further
consideration."
"Scarf's lemma is one of the fundamental results in combinatorics, originally
introduced to study the core of an N-person game. Over the last four decades,
the usefulness of Scarf's lemma has been demonstrated in several important
combinatorial problems seeking ""stable"" solutions. However, the complexity of
the computational version of Scarf's lemma (SCARF) remained open. In this
paper, we prove that SCARF is complete for the complexity class PPAD. This
proves that SCARF is as hard as the computational versions of Brouwer's fixed
point theorem and Sperner's lemma. Hence, there is no polynomial-time algorithm
for SCARF unless PPAD \subseteq P. We also show that fractional stable paths
problem and finding strong fractional kernels in digraphs are PPAD-hard."
"In this paper we define a construct called a time-graph. A complete
time-graph of order n is the cartesian product of a complete graph with n
vertices and a linear graph with n vertices. A time-graph of order n is given
by a subset of the set of edges E(n) of such a graph. The notion of a
hamiltonian time-graph is defined in a natural way and we define the
Hamiltonian time-graph problem (HAMTG) as : Given a time-graph is it
hamiltonian ? We show that the Hamiltonian path problem (HAMP) can be
transformed to HAMTG in polynomial time. We then define certain vector spaces
of functions from E(n) and E(n)xE(n) to B = {0,1}, the field of two elements
and derive certain properties of these spaces. We give two conjectures about
these spaces and prove that if any one of these conjectures is true, we get a
polynomial time algorithm for the Hamiltonian path problem. Since the
Hamiltonian path problem is NP-complete we obtain the proof of P = NP provided
any one of the two conjectures is true."
"The Graph Automata have been the paradigm in the expression of utilizing
Graphs as a language. Matrix Graph grammars \cite{Pedro} are an algebratization
of graph rewriting systems. Here we present the dual of this formalizm which
some extensions which we term Graph Field Automata The advantage to this
approach is a framework for expressing machines that can use Matrix Graph
Grammars."
"We prove that two-way probabilistic and quantum finite automata (2PFA's and
2QFA's) can be considerably more concise than both their one-way versions
(1PFA's and 1QFA's), and two-way nondeterministic finite automata (2NFA's). For
this purpose, we demonstrate several infinite families of regular languages
which can be recognized with some fixed probability greater than $ {1/2} $ by
just tuning the transition amplitudes of a 2QFA (and, in one case, a 2PFA) with
a constant number of states, whereas the sizes of the corresponding 1PFA's,
1QFA's and 2NFA's grow without bound. We also show that 2QFA's with mixed
states can support highly efficient probability amplification. The weakest
known model of computation where quantum computers recognize more languages
with bounded error than their classical counterparts is introduced."
"Graph homomorphism has been studied intensively. Given an m x m symmetric
matrix A, the graph homomorphism function is defined as \[Z_A (G) =
\sum_{f:V->[m]} \prod_{(u,v)\in E} A_{f(u),f(v)}, \] where G = (V,E) is any
undirected graph. The function Z_A can encode many interesting graph
properties, including counting vertex covers and k-colorings. We study the
computational complexity of Z_A for arbitrary symmetric matrices A with
algebraic complex values. Building on work by Dyer and Greenhill, Bulatov and
Grohe, and especially the recent beautiful work by Goldberg, Grohe, Jerrum and
Thurley, we prove a complete dichotomy theorem for this problem. We show that
Z_A is either computable in polynomial-time or #P-hard, depending explicitly on
the matrix A. We further prove that the tractability criterion on A is
polynomial-time decidable."
"In an article [3] published recently in this journal, it was shown that when
k >= 3, the problem of deciding whether the distinguishing chromatic number of
a graph is at most k is NP-hard. We consider the problem when k = 2. In regards
to the issue of solvability in polynomial time, we show that the problem is at
least as hard as graph automorphism but no harder than graph isomorphism."
"This paper concerns the self-assembly of scaled-up versions of arbitrary
finite shapes. We work in the multiple temperature model that was introduced by
Aggarwal, Cheng, Goldwasser, Kao, and Schweller (Complexities for Generalized
Models of Self-Assembly, SODA 2004). The multiple temperature model is a
natural generalization of Winfree's abstract tile assembly model, where the
temperature of a tile system is allowed to be shifted up and down as
self-assembly proceeds. We first exhibit two constant-size tile sets in which
scaled-up versions of arbitrary shapes self-assemble. Our first tile set has
the property that each scaled shape self-assembles via an asymptotically
""Kolmogorov-optimum"" temperature sequence but the scaling factor grows with the
size of the shape being assembled. In contrast, our second tile set assembles
each scaled shape via a temperature sequence whose length is proportional to
the number of points in the shape but the scaling factor is a constant
independent of the shape being assembled. We then show that there is no
constant-size tile set that can uniquely assemble an arbitrary (non-scaled,
connected) shape in the multiple temperature model, i.e., the scaling is
necessary for self-assembly. This answers an open question of Kao and Schweller
(Reducing Tile Complexity for Self-Assembly Through Temperature Programming,
SODA 2006), who asked whether such a tile set existed."
"In the theory of algorithmic randomness, several notions of random sequence
are defined via a game-theoretic approach, and the notions that received most
attention are perhaps Martin-Loef randomness and computable randomness. The
latter notion was introduced by Schnorr and is rather natural: an infinite
binary sequence is computably random if no total computable strategy succeeds
on it by betting on bits in order. However, computably random sequences can
have properties that one may consider to be incompatible with being random, in
particular, there are computably random sequences that are highly compressible.
The concept of Martin-Loef randomness is much better behaved in this and other
respects, on the other hand its definition in terms of martingales is
considerably less natural. Muchnik, elaborating on ideas of Kolmogorov and
Loveland, refined Schnorr's model by also allowing non-monotonic strategies,
i.e. strategies that do not bet on bits in order. The subsequent
``non-monotonic'' notion of randomness, now called Kolmogorov-Loveland
randomness, has been shown to be quite close to Martin-Loef randomness, but
whether these two classes coincide remains a fundamental open question. As
suggested by Miller and Nies, we study in this paper weak versions of
Kolmogorov-Loveland randomness, where the betting strategies are non-adaptive
(i.e., the positions of the bits to bet on should be decided before the game).
We obtain a full classification of the different notions we consider."
"We investigate the arithmetic formula complexity of the elementary symmetric
polynomials S(k,n). We show that every multilinear homogeneous formula
computing S(k,n) has size at least k^(Omega(log k))n, and that product-depth d
multilinear homogeneous formulas for S(k,n) have size at least
2^(Omega(k^{1/d}))n. Since S(n,2n) has a multilinear formula of size O(n^2), we
obtain a superpolynomial separation between multilinear and multilinear
homogeneous formulas. We also show that S(k,n) can be computed by homogeneous
formulas of size k^(O(log k))n, answering a question of Nisan and Wigderson.
Finally, we present a superpolynomial separation between monotone and
non-monotone formulas in the noncommutative setting, answering a question of
Nisan."
"We provide an overview of theories of continuous time computation. These
theories allow us to understand both the hardness of questions related to
continuous time dynamical systems and the computational power of continuous
time analog models. We survey the existing models, summarizing results, and
point to relevant references in the literature."
"The motivation for this paper is to study the complexity of constant-width
arithmetic circuits. Our main results are the following.
  1. For every k > 1, we provide an explicit polynomial that can be computed by
a linear-sized monotone circuit of width 2k but has no subexponential-sized
monotone circuit of width k. It follows, from the definition of the polynomial,
that the constant-width and the constant-depth hierarchies of monotone
arithmetic circuits are infinite, both in the commutative and the
noncommutative settings.
  2. We prove hardness-randomness tradeoffs for identity testing constant-width
commutative circuits analogous to [KI03,DSY08]."
"This paper demonstrates that P \not= NP. The way was to generalize the
traditional definitions of the classes P and NP, to construct an artificial
problem (a generalization to SAT: The XG-SAT, much more difficult than the
former) and then to demonstrate that it is in NP but not in P (where the
classes P and NP are generalized and called too simply P and NP in this paper,
and then it is explained why the traditional classes P and NP should be fixed
and replaced by these generalized ones into Theory of Computer Science). The
demonstration consists of: 1. Definition of Restricted Type X Program; 2.
Definition of the General Extended Problem of Satisfiability of a Boolean
Formula - XG-SAT; 3. Generalization to classes P and NP; 4. Demonstration that
the XG-SAT is in NP; 5. Demonstration that the XG-SAT is not in P; 6.
Demonstration that the Baker-Gill-Solovay Theorem does not refute the proof; 7.
Demonstration that the Razborov-Rudich Theorem does not refute the proof; 8.
Demonstration that the Aaronson-Wigderson Theorem does not refute the proof."
"Motivated by the Hadamard product of matrices we define the Hadamard product
of multivariate polynomials and study its arithmetic circuit and branching
program complexity. We also give applications and connections to polynomial
identity testing. Our main results are the following. 1. We show that
noncommutative polynomial identity testing for algebraic branching programs
over rationals is complete for the logspace counting class $\ceql$, and over
fields of characteristic $p$ the problem is in $\ModpL/\Poly$. 2.We show an
exponential lower bound for expressing the Raz-Yehudayoff polynomial as the
Hadamard product of two monotone multilinear polynomials. In contrast the
Permanent can be expressed as the Hadamard product of two monotone multilinear
formulas of quadratic size."
"To determine if two lists of numbers are the same set, we sort both lists and
see if we get the same result. The sorted list is a canonical form for the
equivalence relation of set equality. Other canonical forms arise in graph
isomorphism algorithms, and the equality of permutation groups given by
generators. To determine if two graphs are cospectral (have the same
eigenvalues), however, we compute their characteristic polynomials and see if
they are the same; the characteristic polynomial is a complete invariant for
the equivalence relation of cospectrality. This is weaker than a canonical
form, and it is not known whether a polynomial-time canonical form for
cospectrality exists. Note that it is a priori possible for an equivalence
relation to be decidable in polynomial time without either a complete invariant
or canonical form.
  Blass and Gurevich (SIAM J. Comput., 1984) ask whether these conditions on
equivalence relations -- having an FP canonical form, having an FP complete
invariant, and simply being in P -- are in fact different. They showed that
this question requires non-relativizing techniques to resolve. Here we extend
their results, and give new connections to probabilistic and quantum
computation."
"A polynomial identity testing algorithm must determine whether a given input
polynomial is identically equal to 0. We give a deterministic black-box
identity testing algorithm for univariate polynomials of the form $\sum_{j=0}^t
c_j X^{\alpha_j} (a + b X)^{\beta_j}$. From our algorithm we derive an
exponential lower bound for representations of polynomials such as
$\prod_{i=1}^{2^n} (X^i-1)$ under this form. It has been conjectured that these
polynomials are hard to compute by general arithmetic circuits. Our result
shows that the ""hardness from derandomization"" approach to lower bounds is
feasible for a restricted class of arithmetic circuits. The proof is based on
techniques from algebraic number theory, and more precisely on properties of
the height function of algebraic numbers."
"Algorithmic tools for graphs of small treewidth are used to address questions
in complexity theory. For both arithmetic and Boolean circuits, it is shown
that any circuit of size $n^{O(1)}$ and treewidth $O(\log^i n)$ can be
simulated by a circuit of width $O(\log^{i+1} n)$ and size $n^c$, where $c =
O(1)$, if $i=0$, and $c=O(\log \log n)$ otherwise. For our main construction,
we prove that multiplicatively disjoint arithmetic circuits of size $n^{O(1)}$
and treewidth $k$ can be simulated by bounded fan-in arithmetic formulas of
depth $O(k^2\log n)$. From this we derive the analogous statement for
syntactically multilinear arithmetic circuits, which strengthens a theorem of
Mahajan and Rao. As another application, we derive that constant width
arithmetic circuits of size $n^{O(1)}$ can be balanced to depth $O(\log n)$,
provided certain restrictions are made on the use of iterated multiplication.
Also from our main construction, we derive that Boolean bounded fan-in circuits
of size $n^{O(1)}$ and treewidth $k$ can be simulated by bounded fan-in
formulas of depth $O(k^2\log n)$. This strengthens in the non-uniform setting
the known inclusion that $SC^0 \subseteq NC^1$. Finally, we apply our
construction to show that {\sc reachability} for directed graphs of bounded
treewidth is in $LogDCFL$."
"In (Kabanets, Impagliazzo, 2004) it is shown how to decide the circuit
polynomial identity testing problem (CPIT) in deterministic subexponential
time, assuming hardness of some explicit multilinear polynomial family for
arithmetical circuits. In this paper, a special case of CPIT is considered,
namely low-degree non-singular matrix completion (NSMC). For this subclass of
problems it is shown how to obtain the same deterministic time bound, using a
weaker assumption in terms of determinantal complexity.
  Hardness-randomness tradeoffs will also be shown in the converse direction,
in an effort to make progress on Valiant's VP versus VNP problem. To separate
VP and VNP, it is known to be sufficient to prove that the determinantal
complexity of the m-by-m permanent is $m^{\omega(\log m)}$. In this paper it is
shown, for an appropriate notion of explicitness, that the existence of an
explicit multilinear polynomial family with determinantal complexity
m^{\omega(\log m)}$ is equivalent to the existence of an efficiently computable
generator $G_n$ for multilinear NSMC with seed length $O(n^{1/\sqrt{\log n}})$.
The latter is a combinatorial object that provides an efficient deterministic
black-box algorithm for NSMC. ``Multilinear NSMC'' indicates that $G_n$ only
has to work for matrices $M(x)$ of $poly(n)$ size in $n$ variables, for which
$det(M(x))$ is a multilinear polynomial."
"The threshold degree of a Boolean function f:{0,1}^n->{-1,+1} is the least
degree of a real polynomial p such that f(x)=sgn p(x). We construct two
halfspaces on {0,1}^n whose intersection has threshold degree Theta(sqrt n), an
exponential improvement on previous lower bounds. This solves an open problem
due to Klivans (2002) and rules out the use of perceptron-based techniques for
PAC learning the intersection of two halfspaces, a central unresolved challenge
in computational learning. We also prove that the intersection of two majority
functions has threshold degree Omega(log n), which is tight and settles a
conjecture of O'Donnell and Servedio (2003).
  Our proof consists of two parts. First, we show that for any nonconstant
Boolean functions f and g, the intersection f(x)^g(y) has threshold degree O(d)
if and only if ||f-F||_infty + ||g-G||_infty < 1 for some rational functions F,
G of degree O(d). Second, we settle the least degree required for approximating
a halfspace and a majority function to any given accuracy by rational
functions.
  Our technique further allows us to make progress on Aaronson's challenge
(2008) and contribute strong direct product theorems for polynomial
representations of composed Boolean functions of the form F(f_1,...,f_n). In
particular, we give an improved lower bound on the approximate degree of the
AND-OR tree."
"We study the maximization version of the fundamental graph coloring problem.
Here the goal is to color the vertices of a k-colorable graph with k colors so
that a maximum fraction of edges are properly colored (i.e. their endpoints
receive different colors). A random k-coloring properly colors an expected
fraction 1-1/k of edges. We prove that given a graph promised to be
k-colorable, it is NP-hard to find a k-coloring that properly colors more than
a fraction ~1-O(1/k} of edges. Previously, only a hardness factor of 1-O(1/k^2)
was known. Our result pins down the correct asymptotic dependence of the
approximation factor on k. Along the way, we prove that approximating the
Maximum 3-colorable subgraph problem within a factor greater than 32/33 is
NP-hard. Using semidefinite programming, it is known that one can do better
than a random coloring and properly color a fraction 1-1/k +2 ln k/k^2 of edges
in polynomial time. We show that, assuming the 2-to-1 conjecture, it is hard to
properly color (using k colors) more than a fraction 1-1/k + O(ln k/ k^2) of
edges of a k-colorable graph."
"0-1 Knapsack is a fundamental NP-complete problem. In this article we prove
that it remains NP-complete even when the weights of the objects in the packing
constraints and their values in the objective function satisfy specific
stringent conditions: the values are integral powers of the weights of the
objects."
"Concurrent non-malleability (CNM) is central for cryptographic protocols
running concurrently in environments such as the Internet. In this work, we
formulate CNM in the bare public-key (BPK) model, and show that round-efficient
concurrent non-malleable cryptography with full adaptive input selection can be
established, in general, with bare public-keys (where, in particular, no
trusted assumption is made). Along the way, we clarify the various subtleties
of adaptive concurrent non-malleability in the bare public-key model."
"We prove two main results on how arbitrary linear threshold functions $f(x) =
\sign(w\cdot x - \theta)$ over the $n$-dimensional Boolean hypercube can be
approximated by simple threshold functions.
  Our first result shows that every $n$-variable threshold function $f$ is
$\eps$-close to a threshold function depending only on $\Inf(f)^2 \cdot
\poly(1/\eps)$ many variables, where $\Inf(f)$ denotes the total influence or
average sensitivity of $f.$ This is an exponential sharpening of Friedgut's
well-known theorem \cite{Friedgut:98}, which states that every Boolean function
$f$ is $\eps$-close to a function depending only on $2^{O(\Inf(f)/\eps)}$ many
variables, for the case of threshold functions. We complement this upper bound
by showing that $\Omega(\Inf(f)^2 + 1/\epsilon^2)$ many variables are required
for $\epsilon$-approximating threshold functions.
  Our second result is a proof that every $n$-variable threshold function is
$\eps$-close to a threshold function with integer weights at most $\poly(n)
\cdot 2^{\tilde{O}(1/\eps^{2/3})}.$ This is a significant improvement, in the
dependence on the error parameter $\eps$, on an earlier result of
\cite{Servedio:07cc} which gave a $\poly(n) \cdot 2^{\tilde{O}(1/\eps^{2})}$
bound. Our improvement is obtained via a new proof technique that uses strong
anti-concentration bounds from probability theory. The new technique also gives
a simple and modular proof of the original \cite{Servedio:07cc} result, and
extends to give low-weight approximators for threshold functions under a range
of probability distributions beyond just the uniform distribution."
"We study the natural question of constructing pseudorandom generators (PRGs)
for low-degree polynomial threshold functions (PTFs). We give a PRG with
seed-length log n/eps^{O(d)} fooling degree d PTFs with error at most eps.
Previously, no nontrivial constructions were known even for quadratic threshold
functions and constant error eps. For the class of degree 1 threshold functions
or halfspaces, we construct PRGs with much better dependence on the error
parameter eps and obtain a PRG with seed-length O(log n + log^2(1/eps)).
Previously, only PRGs with seed length O(log n log^2(1/eps)/eps^2) were known
for halfspaces. We also obtain PRGs with similar seed lengths for fooling
halfspaces over the n-dimensional unit sphere.
  The main theme of our constructions and analysis is the use of invariance
principles to construct pseudorandom generators. We also introduce the notion
of monotone read-once branching programs, which is key to improving the
dependence on the error rate eps for halfspaces. These techniques may be of
independent interest."
"The threshold degree of a function f:{0,1}^n->{-1,+1} is the least degree of
a real polynomial p with f(x)=sgn p(x). We prove that the intersection of two
halfspaces on {0,1}^n has threshold degree Omega(n), which matches the trivial
upper bound and completely answers a question due to Klivans (2002). The best
previous lower bound was Omega(sqrt n). Our result shows that the intersection
of two halfspaces on {0,1}^n only admits a trivial 2^{Theta(n)}-time learning
algorithm based on sign-representation by polynomials, unlike the advances
achieved in PAC learning DNF formulas and read-once Boolean formulas. The proof
introduces a new technique of independent interest, based on Fourier analysis
and matrix theory."
"We describe new lower bounds for randomized communication complexity and
query complexity which we call the partition bounds. They are expressed as the
optimum value of linear programs. For communication complexity we show that the
partition bound is stronger than both the rectangle/corruption bound and the
\gamma_2/generalized discrepancy bounds. In the model of query complexity we
show that the partition bound is stronger than the approximate polynomial
degree and classical adversary bounds. We also exhibit an example where the
partition bound is quadratically larger than polynomial degree and classical
adversary bounds."
"A parameterized problem consists of a classical problem and an additional
component, the so-called parameter. This point of view allows a formal
definition of preprocessing: Given a parameterized instance (I,k), a polynomial
kernelization computes an equivalent instance (I',k') of size and parameter
bounded by a polynomial in k. We give a complete classification of Min Ones
Constraint Satisfaction problems, i.e., Min Ones SAT(\Gamma), with respect to
admitting or not admitting a polynomial kernelization (unless NP \subseteq
coNP/poly). For this we introduce the notion of mergeability. If all relations
of the constraint language \Gamma are mergeable, then a new variant of
sunflower kernelization applies, based on non-zero-closed cores. We obtain a
kernel with O(k^{d+1}) variables and polynomial total size, where d is the
maximum arity of a constraint in \Gamma, comparing nicely with the bound of
O(k^{d-1}) vertices for the less general and arguably simpler d-Hitting Set
problem. Otherwise, any relation in \Gamma that is not mergeable permits us to
construct a log-cost selection formula, i.e., an n-ary selection formula with
O(log n) true local variables. From this we can construct our lower bound using
recent results by Bodlaender et al. as well as Fortnow and Santhanam, proving
that there is no polynomial kernelization, unless NP \subseteq coNP/poly and
the polynomial hierarchy collapses to the third level."
"This paper presents the following results on sets that are complete for NP.
  1. If there is a problem in NP that requires exponential time at almost all
lengths, then every many-one NP-complete set is complete under
length-increasing reductions that are computed by polynomial-size circuits. 2.
If there is a problem in coNP that cannot be solved by polynomial-size
nondeterministic circuits, then every many-one complete set is complete under
length-increasing reductions that are computed by polynomial-size circuits. 3.
If there exist a one-way permutation that is secure against subexponential-size
circuits and there is a hard tally language in NP intersect coNP, then there is
a Turing complete language for NP that is not many-one complete. Our first two
results use worst-case hardness hypotheses whereas earlier work that showed
similar results relied on average-case or almost-everywhere hardness
assumptions. The use of average-case and worst-case hypotheses in the last
result is unique as previous results obtaining the same consequence relied on
almost-everywhere hardness results."
"We show that the Tile Assembly Model exhibits a strong notion of universality
where the goal is to give a single tile assembly system that simulates the
behavior of any other tile assembly system. We give a tile assembly system that
is capable of simulating a very wide class of tile systems, including itself.
Specifically, we give a tile set that simulates the assembly of any tile
assembly system in a class of systems that we call \emph{locally consistent}:
each tile binds with exactly the strength needed to stay attached, and that
there are no glue mismatches between tiles in any produced assembly.
  Our construction is reminiscent of the studies of \emph{intrinsic
universality} of cellular automata by Ollinger and others, in the sense that
our simulation of a tile system $T$ by a tile system $U$ represents each tile
in an assembly produced by $T$ by a $c \times c$ block of tiles in $U$, where
$c$ is a constant depending on $T$ but not on the size of the assembly $T$
produces (which may in fact be infinite). Also, our construction improves on
earlier simulations of tile assembly systems by other tile assembly systems (in
particular, those of Soloveichik and Winfree, and of Demaine et al.) in that we
simulate the actual process of self-assembly, not just the end result, as in
Soloveichik and Winfree's construction, and we do not discriminate against
infinite structures. Both previous results simulate only temperature 1 systems,
whereas our construction simulates tile assembly systems operating at
temperature 2."
"The Graph Isomorphism problem restricted to graphs of bounded treewidth or
bounded tree distance width are known to be solvable in polynomial time
[Bod90],[YBFT99]. We give restricted space algorithms for these problems
proving the following results: - Isomorphism for bounded tree distance width
graphs is in L and thus complete for the class. We also show that for this kind
of graphs a canon can be computed within logspace. - For bounded treewidth
graphs, when both input graphs are given together with a tree decomposition,
the problem of whether there is an isomorphism which respects the
decompositions (i.e. considering only isomorphisms mapping bags in one
decomposition blockwise onto bags in the other decomposition) is in L. - For
bounded treewidth graphs, when one of the input graphs is given with a tree
decomposition the isomorphism problem is in LogCFL. - As a corollary the
isomorphism problem for bounded treewidth graphs is in LogCFL. This improves
the known TC1 upper bound for the problem given by Grohe and Verbitsky
[GroVer06]."
"We prove a complexity dichotomy theorem for Holant Problems on 3-regular
graphs with an arbitrary complex-valued edge function. Three new techniques are
introduced: (1) higher dimensional iterations in interpolation; (2) Eigenvalue
Shifted Pairs, which allow us to prove that a pair of combinatorial gadgets in
combination succeed in proving #P-hardness; and (3) algebraic symmetrization,
which significantly lowers the symbolic complexity of the proof for
computational complexity. With holographic reductions the classification
theorem also applies to problems beyond the basic model."
"We construct pseudorandom generators that fool functions of halfspaces
(threshold functions) under a very broad class of product distributions. This
class includes not only familiar cases such as the uniform distribution on the
discrete cube, the uniform distribution on the solid cube, and the multivariate
Gaussian distribution, but also includes any product of discrete distributions
with probabilities bounded away from 0.
  Our first main result shows that a recent pseudorandom generator construction
of Meka and Zuckerman [MZ09], when suitably modifed, can fool arbitrary
functions of d halfspaces under product distributions where each coordinate has
bounded fourth moment. To eps-fool any size-s, depth-d decision tree of
halfspaces, our pseudorandom generator uses seed length O((d log(ds/eps)+log n)
log(ds/eps)). For monotone functions of d halfspaces, the seed length can be
improved to O((d log(d/eps)+log n) log(d/eps)). We get better bounds for larger
eps; for example, to 1/polylog(n)-fool all monotone functions of (log n)= log
log n halfspaces, our generator requires a seed of length just O(log n). Our
second main result generalizes the work of Diakonikolas et al. [DGJ+09] to show
that bounded independence suffices to fool functions of halfspaces under
product distributions. Assuming each coordinate satisfies a certain stronger
moment condition, we show that any function computable by a size-s, depth-d
decision tree of halfspaces is eps-fooled by O(d^4s^2/eps^2)-wise independence."
"We report progress on the \NL vs \UL problem. [-] We show unconditionally
that the complexity class $\ReachFewL\subseteq\UL$. This improves on the
earlier known upper bound $\ReachFewL \subseteq \FewL$. [-] We investigate the
complexity of min-uniqueness - a central notion in studying the \NL vs \UL
problem. We show that min-uniqueness is necessary and sufficient for showing
$\NL =\UL$. We revisit the class $\OptL[\log n]$ and show that {\sc
ShortestPathLength} - computing the length of the shortest path in a DAG, is
complete for $\OptL[\log n]$. We introduce $\UOptL[\log n]$, an unambiguous
version of $\OptL[\log n]$, and show that (a) $\NL =\UL$ if and only if
$\OptL[\log n] = \UOptL[\log n]$, (b) $\LogFew \leq \UOptL[\log n] \leq \SPL$.
[-] We show that the reachability problem over graphs embedded on 3 pages is
complete for \NL. This contrasts with the reachability problem over graphs
embedded on 2 pages which is logspace equivalent to the reachability problem in
planar graphs and hence is in \UL."
"Boolean functions with symmetry properties are interesting from a complexity
theory perspective; extensive research has shown that these functions, if
nonconstant, must have high `complexity' according to various measures.
  In recent work of this type, Sun gave bounds on the block sensitivity of
nonconstant Boolean functions invariant under a transitive permutation group.
Sun showed that all such functions satisfy bs(f) = Omega(N^{1/3}), and that
there exists such a function for which bs(f) = O(N^{3/7}ln N). His example
function belongs to a subclass of transitively invariant functions called the
minterm-transitive functions (defined in earlier work by Chakraborty).
  We extend these results in two ways. First, we show that nonconstant
minterm-transitive functions satisfy bs(f) = Omega(N^{3/7}). Thus Sun's example
function has nearly minimal block sensitivity for this subclass. Second, we
give an improved example: a minterm-transitive function for which bs(f) =
O(N^{3/7}ln^{1/7}N)."
"Random Number Generators play a critical role in a number of important
applications. In practice, statistical testing is employed to gather evidence
that a generator indeed produces numbers that appear to be random. In this
paper, we reports on the studies that were conducted on the compressed data
using 8 compression algorithms or compressors. The test results suggest that
the output of compression algorithms or compressors has bad randomness, the
compression algorithms or compressors are not suitable as random number
generator. We also found that, for the same compression algorithm, there exists
positive correlation relationship between compression ratio and randomness,
increasing the compression ratio increases randomness of compressed data. As
time permits, additional randomness testing efforts will be conducted."
"Removed by arXiv administration.
  This article was plagiarized directly from Stephen Cook's description of the
problem for the Clay Mathematics Institute. See
http://gauss.claymath.org:8888/millennium/P_vs_NP/pvsnp.pdf for the original
text."
"We show that all languages accepted in time f(n) >= n^2 can be accepted in
space O(f(n)^{1/2})_and_ in time O(f(n)). The proof is carried out by
simulation, based on the idea of guessing the sequences of internal states of
the simulated TM when entering certain critical cells, whose location is also
guessed. Our method cannot be generalised easily to many-tapes TMs, and in no
case can it be relativised."
"The m-sophistication of a finite binary string x is introduced as a
generalization of some parameter in the proof that complexity of complexity is
rare. A probabilistic near sufficient statistic of x is given which length is
upper bounded by the m-sophistication of x within small additive terms. This
shows that m-sophistication is lower bounded by coarse sophistication and upper
bounded by sophistication within small additive terms. It is also shown that
m-sophistication and coarse sophistication can not be approximated by an upper
or lower semicomputable function, not even within very large error."
"The degree of a CSP instance is the maximum number of times that a variable
may appear in the scope of constraints. We consider the approximate counting
problem for Boolean CSPs with bounded-degree instances, for constraint
languages containing the two unary constant relations {0} and {1}. When the
maximum degree is at least 25 we obtain a complete classification of the
complexity of this problem. It is exactly solvable in polynomial-time if every
relation in the constraint language is affine. It is equivalent to the problem
of approximately counting independent sets in bipartite graphs if every
relation can be expressed as conjunctions of {0}, {1} and binary implication.
Otherwise, there is no FPRAS unless NP=RP. For lower degree bounds, additional
cases arise in which the complexity is related to the complexity of
approximately counting independent sets in hypergraphs."
"The parity decision tree model extends the decision tree model by allowing
the computation of a parity function in one step. We prove that the
deterministic parity decision tree complexity of any Boolean function is
polynomially related to the non-deterministic complexity of the function or its
complement. We also show that they are polynomially related to an analogue of
the block sensitivity. We further study parity decision trees in their
relations with an intermediate variant of the decision trees, as well as with
communication complexity."
"We explore the intricate interdependent relationship among counting problems,
considered from three frameworks for such problems: Holant Problems, counting
CSP and weighted H-colorings. We consider these problems for general complex
valued functions that take boolean inputs. We show that results from one
framework can be used to derive results in another, and this happens in both
directions. Holographic reductions discover an underlying unity, which is only
revealed when these counting problems are investigated in the complex domain
$\mathbb{C}$. We prove three complexity dichotomy theorems, leading to a
general theorem for Holant$^c$ problems. This is the natural class of Holant
problems where one can assign constants 0 or 1. More specifically, given any
signature grid on $G=(V,E)$ over a set ${\mathscr F}$ of symmetric functions,
we completely classify the complexity to be in P or #P-hard, according to
${\mathscr F}$, of \[\sum_{\sigma: E \rightarrow \{0,1\}} \prod_{v\in V}
f_v(\sigma\mid_{E(v)}),\] where $f_v \in {\mathscr F} \cup \{{\bf 0}, {\bf
1}\}$ ({\bf 0}, {\bf 1} are the unary constant 0, 1 functions). Not only is
holographic reduction the main tool, but also the final dichotomy can be only
naturally stated in the language of holographic transformations. The proof goes
through another dichotomy theorem on boolean complex weighted #CSP."
"We prove that NP differs from coNP and coNP is not a subset of MA in the
number-on-forehead model of multiparty communication complexity for up to k =
(1-\epsilon)log(n) players, where \epsilon>0 is any constant. Specifically, we
construct a function F with co-nondeterministic complexity O(log(n)) and
Merlin-Arthur complexity n^{\Omega(1)}. The problem was open for k > 2."
"In this paper, we study the complexity of computing locally optimal solutions
for weighted versions of standard set problems such as SetCover, SetPacking,
and many more. For our investigation, we use the framework of PLS, as defined
in Johnson et al., [JPY88]. We show that for most of these problems, computing
a locally optimal solution is already PLS-complete for a simple neighborhood of
size one. For the local search versions of weighted SetPacking and SetCover, we
derive tight bounds for a simple neighborhood of size two. To the best of our
knowledge, these are one of the very few PLS results about local search for
weighted standard set problems."
"We study possible formulations of algebraic propositional proof systems
operating with noncommutative formulas. We observe that a simple formulation
gives rise to systems at least as strong as Frege---yielding a semantic way to
define a Cook-Reckhow (i.e., polynomially verifiable) algebraic analog of Frege
proofs, different from that given in [BIKPRS96,GH03]. We then turn to an
apparently weaker system, namely, polynomial calculus (PC) where polynomials
are written as ordered formulas (PC over ordered formulas, for short): an
ordered polynomial is a noncommutative polynomial in which the order of
products in every monomial respects a fixed linear order on variables; an
algebraic formula is ordered if the polynomial computed by each of its
subformulas is ordered. We show that PC over ordered formulas is strictly
stronger than resolution, polynomial calculus and polynomial calculus with
resolution (PCR) and admits polynomial-size refutations for the pigeonhole
principle and the Tseitin's formulas. We conclude by proposing an approach for
establishing lower bounds on PC over ordered formulas proofs, and related
systems, based on properties of lower bounds on noncommutative formulas.
  The motivation behind this work is developing techniques incorporating rank
arguments (similar to those used in algebraic circuit complexity) for
establishing lower bounds on propositional proofs."
"In this paper the minimum spanning tree problem with uncertain edge costs is
discussed. In order to model the uncertainty a discrete scenario set is
specified and a robust framework is adopted to choose a solution. The min-max,
min-max regret and 2-stage min-max versions of the problem are discussed. The
complexity and approximability of all these problems are explored. It is proved
that the min-max and min-max regret versions with nonnegative edge costs are
hard to approximate within $O(\log^{1-\epsilon} n)$ for any $\epsilon>0$ unless
the problems in NP have quasi-polynomial time algorithms. Similarly, the
2-stage min-max problem cannot be approximated within $O(\log n)$ unless the
problems in NP have quasi-polynomial time algorithms. In this paper randomized
LP-based approximation algorithms with performance ratio of $O(\log^2 n)$ for
min-max and 2-stage min-max problems are also proposed."
"We study the problem of generating monomials of a polynomial in the context
of enumeration complexity. In this setting, the complexity measure is the delay
between two solutions and the total time. We present two new algorithms for
restricted classes of polynomials, which have a good delay and the same global
running time as the classical ones. Moreover they are simple to describe, use
little evaluation points and one of them is parallelizable. We introduce three
new complexity classes, TotalPP, IncPP and DelayPP, which are probabilistic
counterparts of the most common classes for enumeration problems, hoping that
randomization will be a tool as strong for enumeration as it is for decision.
Our interpolation algorithms proves that a lot of interesting problems are in
these classes like the enumeration of the spanning hypertrees of a 3-uniform
hypergraph.
  Finally we give a method to interpolate a degree 2 polynomials with an
acceptable (incremental) delay. We also prove that finding a specified monomial
in a degree 2 polynomial is hard unless RP = NP. It suggests that there is no
algorithm with a delay as good (polynomial) as the one we achieve for
multilinear polynomials."
"We study the parameterized control complexity of fallback voting, a voting
system that combines preference-based with approval voting. Electoral control
is one of many different ways for an external agent to tamper with the outcome
of an election. We show that adding and deleting candidates in fallback voting
are W[2]-hard for both the constructive and destructive case, parameterized by
the amount of action taken by the external agent. Furthermore, we show that
adding and deleting voters in fallback voting are W[2]-hard for the
constructive case, parameterized by the amount of action taken by the external
agent, and are in FPT for the destructive case."
"We show that it is Unique Games-hard to approximate the maximum of a
submodular function to within a factor 0.695, and that it is Unique Games-hard
to approximate the maximum of a symmetric submodular function to within a
factor 0.739. These results slightly improve previous results by Feige,
Mirrokni and Vondr\'ak (FOCS 2007) who showed that these problems are NP-hard
to approximate to within $3/4 + \epsilon \approx 0.750$ and $5/6 + \epsilon
\approx 0.833$, respectively."
"Hartmanis used Kolmogorov complexity to provide an alternate proof of the
classical result of Baker, Gill, and Solovay that there is an oracle relative
to which P is not NP. We refine the technique to strengthen the result,
constructing an oracle relative to which a conjecture of Lipton is false."
"We consider a model of algorithmic self-assembly of geometric shapes out of
square Wang tiles studied in SODA 2010, in which there are two types of tiles
(e.g., constructed out of DNA and RNA material) and one operation that destroys
all tiles of a particular type (e.g., an RNAse enzyme destroys all RNA tiles).
We show that a single use of this destruction operation enables much more
efficient construction of arbitrary shapes. In particular, an arbitrary shape
can be constructed using an asymptotically optimal number of distinct tile
types (related to the shape's Kolmogorov complexity), after scaling the shape
by only a logarithmic factor. By contrast, without the destruction operation,
the best such result has a scale factor at least linear in the size of the
shape, and is connected only by a spanning tree of the scaled tiles. We also
characterize a large collection of shapes that can be constructed efficiently
without any scaling."
"A polynomial identity testing algorithm must determine whether an input
polynomial (given for instance by an arithmetic circuit) is identically equal
to 0. In this paper, we show that a deterministic black-box identity testing
algorithm for (high-degree) univariate polynomials would imply a lower bound on
the arithmetic complexity of the permanent. The lower bounds that are known to
follow from derandomization of (low-degree) multivariate identity testing are
weaker. To obtain our lower bound it would be sufficient to derandomize
identity testing for polynomials of a very specific norm: sums of products of
sparse polynomials with sparse coefficients. This observation leads to new
versions of the Shub-Smale tau-conjecture on integer roots of univariate
polynomials. In particular, we show that a lower bound for the permanent would
follow if one could give a good enough bound on the number of real roots of
sums of products of sparse polynomials (Descartes' rule of signs gives such a
bound for sparse polynomials and products thereof). In this third version of
our paper we show that the same lower bound would follow even if one could only
prove a slightly superpolynomial upper bound on the number of real roots. This
is a consequence of a new result on reduction to depth 4 for arithmetic
circuits which we establish in a companion paper. We also show that an even
weaker bound on the number of real roots would suffice to obtain a lower bound
on the size of depth 4 circuits computing the permanent."
"We investigate the space complexity of certain perfect matching problems over
bipartite graphs embedded on surfaces of constant genus (orientable or
non-orientable). We show that the problems of deciding whether such graphs have
(1) a perfect matching or not and (2) a unique perfect matching or not, are in
the logspace complexity class \SPL. Since \SPL\ is contained in the logspace
counting classes $\oplus\L$ (in fact in \modk\ for all $k\geq 2$), \CeqL, and
\PL, our upper bound places the above-mentioned matching problems in these
counting classes as well. We also show that the search version, computing a
perfect matching, for this class of graphs is in $\FL^{\SPL}$. Our results
extend the same upper bounds for these problems over bipartite planar graphs
known earlier. As our main technical result, we design a logspace computable
and polynomially bounded weight function which isolates a minimum weight
perfect matching in bipartite graphs embedded on surfaces of constant genus. We
use results from algebraic topology for proving the correctness of the weight
function."
"We consider boolean circuits computing n-operators f:{0,1}^n --> {0,1}^n. As
gates we allow arbitrary boolean functions; neither fanin nor fanout of gates
is restricted. An operator is linear if it computes n linear forms, that is,
computes a matrix-vector product y=Ax over GF(2). We prove the existence of
n-operators requiring about n^2 wires in any circuit, and linear n-operators
requiring about n^2/\log n wires in depth-2 circuits, if either all output
gates or all gates on the middle layer are linear."
"We consider the problem of finding a local optimum for Max-Cut with
FLIP-neighborhood, in which exactly one node changes the partition. Schaeffer
and Yannakakis (SICOMP, 1991) showed PLS-completeness of this problem on graphs
with unbounded degree. On the other side, Poljak (SICOMP, 1995) showed that in
cubic graphs every FLIP local search takes O(n^2) steps, where n is the number
of nodes. Due to the huge gap between degree three and unbounded degree,
Ackermann, Roeglin, and Voecking (JACM, 2008) asked for the smallest d for
which the local Max-Cut problem with FLIP-neighborhood on graphs with maximum
degree d is PLS-complete. In this paper, we prove that the computation of a
local optimum on graphs with maximum degree five is PLS-complete. Thus, we
solve the problem posed by Ackermann et al. almost completely by showing that d
is either four or five (unless PLS is in P). On the other side, we also prove
that on graphs with degree O(log n) every FLIP local search has probably
polynomial smoothed complexity. Roughly speaking, for any instance, in which
the edge weights are perturbated by a (Gaussian) random noise with variance
\sigma^2, every FLIP local search terminates in time polynomial in n and
\sigma^{-1}, with probability 1-n^{-\Omega(1)}. Putting both results together,
we may conclude that although local Max-Cut is likely to be hard on graphs with
bounded degree, it can be solved in polynomial time for slightly perturbated
instances with high probability."
"Loveland complexity is a variant of Kolmogorov complexity, where it is asked
to output separately the bits of the desired string, instead of the string
itself. Similarly to the resource-bounded Kolmogorov sets we define Loveland
sets. We highlight a structural connection between resource-bounded Loveland
sets and some advice complexity classes. This structural connection enables us
to map to advice complexity classes some properties of Kolmogorov sets first
noticed by Hartmanis and thoroughly investigated in Longpr\'e's thesis: 1.
Non-inclusion properties of Loveland sets result in hierarchy properties on the
corresponding advice complexity classes; 2. Immunity properties of Loveland
sets result in the non-existence of natural proofs between the corresponding
advice complexity classes, in the sense of Razborov & Rudich."
"We note that for each k \in {0,1,2, ...} the following holds: NE has
(nonuniform) ACC^k circuits if and only if NE has P^{NE}-uniform ACC^k
circuits. And we mention how to get analogous results for other circuit and
complexity classes."
"This paper studies a generalization of multi-prover interactive proofs in
which a verifier interacts with two competing teams of provers: one team
attempts to convince the verifier to accept while the other attempts to
convince the verifier to reject. Each team consists of two provers who jointly
implement a no-signaling strategy. No-signaling strategies are a curious class
of joint strategy that cannot in general be implemented without communication
between the provers, yet cannot be used as a black box to establish
communication between them. Attention is restricted in this paper to two-turn
interactions in which the verifier asks questions of each of the four provers
and decides whether to accept or reject based on their responses.
  We prove that the complexity class of decision problems that admit two-turn
interactive proofs with competing teams of no-signaling provers is a subset of
PSPACE. This upper bound matches existing PSPACE lower bounds on the following
two disparate and weaker classes of interactive proof:
  1. Two-turn multi-prover interactive proofs with only one team of
no-signaling provers.
  2. Two-turn competing-prover interactive proofs with only one prover per
team.
  Our result implies that the complexity of these two models is unchanged by
the addition of a second competing team of no-signaling provers in the first
case and by the addition of a second no-signaling prover to each team in the
second case. Moreover, our result unifies and subsumes prior PSPACE upper
bounds on these classes."
"We present a simple Logspace algorithm that solves the Unary Subset-Sum
problem."
"We define and study a new notion of ""robust simulations"" between complexity
classes which is intermediate between the traditional notions of
infinitely-often and almost-everywhere, as well as a corresponding notion of
""significant separations"". A language L has a robust simulation in a complexity
class C if there is a language in C which agrees with L on arbitrarily large
polynomial stretches of input lengths. There is a significant separation of L
from C if there is no robust simulation of L in C. The new notion of simulation
is a cleaner and more natural notion of simulation than the infinitely-often
notion. We show that various implications in complexity theory such as the
collapse of PH if NP = P and the Karp-Lipton theorem have analogues for robust
simulations. We then use these results to prove that most known separations in
complexity theory, such as hierarchy theorems, fixed polynomial circuit lower
bounds, time-space tradeoffs, and the theorems of Allender and Williams, can be
strengthened to significant separations, though in each case, an almost
everywhere separation is unknown.
  Proving our results requires several new ideas, including a completely
different proof of the hierarchy theorem for non-deterministic polynomial time
than the ones previously known."
"Computational complexity of the design problem for a network with a target
value of Region-Based Component Decomposition Number (RBCDN) has been proven to
be NP-complete."
"We investigate the complexity of integration and derivative for multivariate
polynomials in the standard computation model. The integration is in the unit
cube $[0,1]^d$ for a multivariate polynomial, which has format $f(x_1,\cdots,
x_d)=p_1(x_1,\cdots, x_d)p_2(x_1,\cdots, x_d)\cdots p_k(x_1,\cdots, x_d)$,
where each $p_i(x_1,\cdots, x_d)=\sum_{j=1}^d q_j(x_j)$ with all single
variable polynomials $q_j(x_j)$ of degree at most two and constant
coefficients. We show that there is no any factor polynomial time approximation
for the integration $\int_{[0,1]^d}f(x_1,\cdots,x_d)d_{x_1}\cdots d_{x_d}$
unless $P=NP$. For the complexity of multivariate derivative, we consider the
functions with the format $f(x_1,\cdots, x_d)=p_1(x_1,\cdots,
x_d)p_2(x_1,\cdots, x_d)\cdots p_k(x_1,\cdots, x_d),$ where each
$p_i(x_1,\cdots, x_d)$ is of degree at most $2$ and $0,1$ coefficients. We also
show that unless $P=NP$, there is no any factor polynomial time approximation
to its derivative ${\partial f^{(d)}(x_1,\cdots, x_d)\over \partial x_1\cdots
\partial x_d}$ at the origin point $(x_1,\cdots, x_d)=(0,\cdots,0)$. Our
results show that the derivative may not be easier than the integration in high
dimension. We also give some tractable cases of high dimension integration and
derivative."
"A long standing open problem in the computational complexity theory is to
separate NE from BPP, which is a subclass of $NP_T(NP\cap P/poly)$. In this
paper, we show that $NE\not\subseteq NP_(NP \cap$
Nonexponentially-Dense-Class), where Nonexponentially-Dense-Class is the class
of languages A without exponential density (for each constant c>0,$|A^{\le
n}|\le 2^{n^c}$ for infinitely many integers n).
  Our result implies $NE\not\subseteq NP_T({pad(NP, g(n))})$ for every time
constructible super-polynomial function g(n) such as
$g(n)=n^{\ceiling{\log\ceiling{\log n}}}$, where Pad(NP, g(n)) is class of all
languages $L_B=\{s10^{g(|s|)-|s|-1}:s\in B\}$ for $B\in NP$. We also show
$NE\not\subseteq NP_T(P_{tt}(NP)\cap Tally)$."
"In this survey we investigate the application of the Adleman-Lipton model on
Rural Postman problem, which given an undirected graph $G=(V,E)$ with positive
integer lengths on each of its edges and a subset $E^{'}\subseteq E$, asks
whether there exists a hamiltonian circuit that includes each edge of $E^{'}$
and has total cost (sum of edge lengths) less or equal to a given integer B (we
are allowed to use any edges of the set $E-E^{'}$, but we must use all edges of
the set $E'$). The Rural Postman problem (RPP) is a very interesting
NP-complete problem used, especially, in network optimization. RPP is actually
a special case of the Route Inspection problem, where we need to traverse all
edges of an undirected graph at a minimum total cost. As all NP-complete
problems, it currently admits no efficient solution and if actually $P\neq NP$
as it is widely accepted to be, it cannot admit a polynomial time algorithm to
solve it. The application of the Adleman-Lipton model on this problem, provides
an efficient way to solve RPP, as it is the fact for many other hard problems
on which the Adleman-Lipton model has been applied. In this survey, we provide
a polynomial algorithm based on the Lipton-Adleman model, which solves the RPP
in $\mathcal{O}(n^{2})$ time, where n refers to the input of the problem."
"The aim of this thesis is to determine classes of NP relations for which
random generation and approximate counting problems admit an efficient
solution. Since efficient rank implies efficient random generation, we first
investigate some classes of NP relations admitting efficient ranking. On the
other hand, there are situations in which efficient random generation is
possible even when ranking is computationally infeasible. We introduce the
notion of ambiguous description as a tool for random generation and approximate
counting in such cases and show, in particular, some applications to the case
of formal languages. Finally, we discuss a limit of an heuristic for
combinatorial optimization problems based on the random initialization of local
search algorithms showing that derandomizing such heuristic can be, in some
cases, #P-hard."
"This paper proposes new notions of polynomial depth (called monotone poly
depth), based on a polynomial version of monotone Kolmogorov complexity. We
show that monotone poly depth satisfies all desirable properties of depth
notions i.e., both trivial and random sequences are not monotone poly deep,
monotone poly depth satisfies the slow growth law i.e., no simple process can
transform a non deep sequence into a deep one, and monotone poly deep sequences
exist (unconditionally). We give two natural examples of deep sets, by showing
that both the set of Levin-random strings and the set of Kolmogorov random
strings are monotone poly deep."
"In this note, we show that the linear programming for computing the
quasi-additive bound of the formula size of a Boolean function presented by
Ueno [MFCS'10] is equivalent to the dual problem of the linear programming
relaxation of an integer programming for computing the protocol partition
number. Together with the result of Ueno [MFCS'10], our results imply that
there exists no gap between our integer programming for computing the protocol
partition number and its linear programming relaxation."
"We prove a complexity dichotomy theorem for all non-negative weighted
counting Constraint Satisfaction Problems (CSP). This caps a long series of
important results on counting problems including unweighted and weighted graph
homomorphisms and the celebrated dichotomy theorem for unweighted #CSP. Our
dichotomy theorem gives a succinct criterion for tractability. If a set F of
constraint functions satisfies the criterion, then the counting CSP problem
defined by F is solvable in polynomial time; if it does not satisfy the
criterion, then the problem is #P-hard. We furthermore show that the question
of whether F satisfies the criterion is decidable in NP.
  Surprisingly, our tractability criterion is simpler than the previous
criteria for the more restricted classes of problems, although when specialized
to those cases, they are logically equivalent. Our proof mainly uses Linear
Algebra, and represents a departure from Universal Algebra, the dominant
methodology in recent years."
"In this paper we review our current results concerning the computational
power of quantum read-once branching programs. First of all, based on the
circuit presentation of quantum branching programs and our variant of quantum
fingerprinting technique, we show that any Boolean function with linear
polynomial presentation can be computed by a quantum read-once branching
program using a relatively small (usually logarithmic in the size of input)
number of qubits. Then we show that the described class of Boolean functions is
closed under the polynomial projections."
"This paper studies the computational complexity of the Edge Packing problem
and the Vertex Packing problem. The edge packing problem (denoted by
$\bar{EDS}$) and the vertex packing problem (denoted by $\bar{DS} $) are linear
programming duals of the edge dominating set problem and the dominating set
problem respectively. It is shown that these two problems are equivalent to the
set packing problem with respect to hardness of approximation and parametric
complexity. It follows that $\bar{EDS}$ and $\bar{DS}$ cannot be approximated
asymptotically within a factor of $O(N^{1/2-\epsilon})$ for any $\epsilon>0$
unless $NP=ZPP$ where, $N$ is the number of vertices in the given graph. This
is in contrast with the fact that the edge dominating set problem is
2-approximable where as the dominating set problem is known to have an $O(\log$
$|V|)$ approximation algorithm. It also follows from our proof that $\bar{EDS}$
and $\bar{DS}$ are $W[1]$-complete."
"We prove a strong Symmetry of Information relation for random strings (in the
sense of Kolmogorov complexity) and establish tight bounds on the amount on
nonuniformity that is necessary for extracting a string with randomness rate 1
from a single source of randomness. More precisely, as instantiations of more
general results, we show: (1) For all n-bit random strings x and y, x is random
conditioned by y if and only if y is random conditioned by x, and (2) while
O(1) amount of advice regarding the source is not enough for extracting a
string with randomness rate 1 from a source string with constant random rate,
\omega(1) amount of advice is. The proofs use Kolmogorov extractors as the main
technical device."
"We study the (non-uniform) quantified constraint satisfaction problem QCSP(H)
as H ranges over partially reflexive forests. We obtain a complexity-theoretic
dichotomy: QCSP(H) is either in NL or is NP-hard. The separating condition is
related firstly to connectivity, and thereafter to accessibility from all
vertices of H to connected reflexive subgraphs. In the case of partially
reflexive paths, we give a refinement of our dichotomy: QCSP(H) is either in NL
or is Pspace-complete."
"The Gr\""obner basis detection (GBD) is defined as follows: Given a set of
polynomials, decide whether there exists -and if ""yes"" find- a term order such
that the set of polynomials is a Gr\""obner basis. This problem was shown to be
NP-hard by Sturmfels and Wiegelmann. We show that GBD when studied in the
context of zero dimensional ideals is also NP-hard. An algorithm to solve GBD
for zero dimensional ideals is also proposed which runs in polynomial time if
the number of indeterminates is a constant."
"Quadratic assignment problem is one of the great challenges in combinatorial
optimization. It has many applications in Operations research and Computer
Science. In this paper, the author extends the most-used rounding approach to a
one-parametric optimization model for the quadratic assignment problems. A
near-optimum parameter is also predestinated. The numerical experiments confirm
the efficiency."
"Abstract geometrical computation can solve hard combinatorial problems
efficiently: we showed previously how Q-SAT can be solved in bounded space and
time using instance-specific signal machines and fractal parallelization. In
this article, we propose an approach for constructing a particular generic
machine for the same task. This machine deploies the Map/Reduce paradigm over a
fractal structure. Moreover our approach is modular: the machine is constructed
by combining modules. In this manner, we can easily create generic machines for
solving satifiability variants, such as SAT, #SAT, MAX-SAT."
"Ben-Sasson and Sudan (RSA 2006) showed that repeated tensor products of
linear codes with a very large distance are locally testable. Due to the
requirement of a very large distance the associated tensor products could be
applied only over sufficiently large fields. Then Meir (SICOMP 2009) used this
result (as a black box) to present a combinatorial construction of locally
testable codes that match best known parameters. As a consequence, this
construction was obtained over sufficiently large fields.
  In this paper we improve the result of Ben-Sasson and Sudan and show that for
\emph{any} linear codes the associated tensor products are locally testable.
Consequently, the construction of Meir can be taken over any field, including
the binary field.
  Moreover, a combination of our result with the result of Spielman (IEEE IT,
1996) implies a construction of linear codes (over any field) that combine the
following properties: have constant rate and constant relative distance; have
blocklength $n$ and testable with $n^{\epsilon}$ queries, for any constant
$\epsilon > 0$; linear time encodable and linear-time decodable from a constant
fraction of errors.
  Furthermore, a combination of our result with the result of Guruswami et al.
(STOC 2009) implies a similar corollary regarding the list-decodable codes."
"Polynomial identity testing and arithmetic circuit lower bounds are two
central questions in algebraic complexity theory. It is an intriguing fact that
these questions are actually related. One of the authors of the present paper
has recently proposed a ""real {\tau}-conjecture"" which is inspired by this
connection. The real {\tau}-conjecture states that the number of real roots of
a sum of products of sparse univariate polynomials should be polynomially
bounded. It implies a superpolynomial lower bound on the size of arithmetic
circuits computing the permanent polynomial. In this paper we show that the
real {\tau}-conjecture holds true for a restricted class of sums of products of
sparse polynomials. This result yields lower bounds for a restricted class of
depth-4 circuits: we show that polynomial size circuits from this class cannot
compute the permanent, and we also give a deterministic polynomial identity
testing algorithm for the same class of circuits."
"The combined Universal Probability M(D) of strings x in sets D is close to
max M({x}) over x in D: their ~logs differ by at most D's information j=I(D:H)
about the halting sequence H. Thus if all x have complexity K(x) >k, D carries
>i bits of information on each its x where i+j ~ k. Note that there are no ways
to generate D with significant I(D:H)."
"We investigate the complexity of the model checking problem for
intuitionistic and modal propositional logics over transitive Kripke models.
More specific, we consider intuitionistic logic IPC, basic propositional logic
BPL, formal propositional logic FPL, and Jankov's logic KC. We show that the
model checking problem is P-complete for the implicational fragments of all
these intuitionistic logics. For BPL and FPL we reach P-hardness even on the
implicational fragment with only one variable. The same hardness results are
obtained for the strictly implicational fragments of their modal companions.
Moreover, we investigate whether formulas with less variables and additional
connectives make model checking easier. Whereas for variable free formulas
outside of the implicational fragment, FPL model checking is shown to be in
LOGCFL, the problem remains P-complete for BPL."
"The metric dimension of a graph $G$ is the size of a smallest subset $L
\subseteq V(G)$ such that for any $x,y \in V(G)$ with $x\not= y$ there is a $z
\in L$ such that the graph distance between $x$ and $z$ differs from the graph
distance between $y$ and $z$. Even though this notion has been part of the
literature for almost 40 years, prior to our work the computational complexity
of determining the metric dimension of a graph was still very unclear. In this
paper, we show tight complexity boundaries for the Metric Dimension problem. We
achieve this by giving two complementary results. First, we show that the
Metric Dimension problem on planar graphs of maximum degree $6$ is NP-complete.
Then, we give a polynomial-time algorithm for determining the metric dimension
of outerplanar graphs."
"In this paper we prove lower bounds on randomized multiparty communication
complexity, both in the \emph{blackboard model} (where each message is written
on a blackboard for all players to see) and (mainly) in the
\emph{message-passing model}, where messages are sent player-to-player. We
introduce a new technique for proving such bounds, called
\emph{symmetrization}, which is natural, intuitive, and often easy to use.
  For example, for the problem where each of $k$ players gets a bit-vector of
length $n$, and the goal is to compute the coordinate-wise XOR of these
vectors, we prove a tight lower bounds of $\Omega(nk)$ in the blackboard model.
For the same problem with AND instead of XOR, we prove a lower bounds of
roughly $\Omega(nk)$ in the message-passing model (assuming $k \le n/3200$) and
$\Omega(n \log k)$ in the blackboard model. We also prove lower bounds for
bit-wise majority, for a graph-connectivity problem, and for other problems;
the technique seems applicable to a wide range of other problems as well. All
of our lower bounds allow randomized communication protocols with two-sided
error.
  We also use the symmetrization technique to prove several direct-sum-like
results for multiparty communication."
"Satisfying spin-assignments in triangulations of a surface are states of
minimum energy of the antiferromagnetic Ising model on triangulations which
correspond (via geometric duality) to perfect matchings in cubic bridgeless
graphs. In this work we show that it is NP-complete to decide whether or not a
surface triangulation admits a satisfying spin-assignment, and that it is
#P-complete to determine the number of such assignments. Both results are
derived via an elaborate (and atypical) reduction that maps a Boolean formula
in 3-conjunctive normal form into a triangulation of an orientable closed
surface."
"This paper formally proposes a problem about the efficient utilization of the
four dimensional space-time. Given a cuboid container, a finite number of rigid
cuboid items, and the time length that each item should be continuous baked in
the container, the problem asks to arrange the starting time for each item
being placed into the container and to arrange the position and orientation for
each item at each instant during its continuous baking period such that the
total time length the container be utilized is as short as possible. Here all
side dimensions of the container and of the items are positive real numbers
arbitrarily given. Differs from the classical packing problems, the position
and orientation of each item in the container could be changed over time.
Therefore, according to above mathematical model, the four-dimensional
space-time can be utilized more truly and more fully. This paper then proves
that there exists an exact algorithm that could solve the problem by finite
operations, so we say this problem is weak computable. Based on the
understanding of this computability proof, it is expected to design effective
approximate algorithms in the near future. A piggyback work completed is a
strict proof on the weak computability over general and natural case of the
three-dimensional cuboid packing decision problem that all parameters are
positive real numbers."
"In this paper we present a Hashed-Path Traveling Salesperson Problem (HPTSP),
a new type of problem which has the interesting property of having no
polynomial time solutions. Next we show that HPTSP is in the class NP by
demonstrating that local information about sub-routes is insufficient to
compute the complete value of each route. As a consequence, via Ladner's
theorem, we show that the class NPI is non-empty."
"The long code is a central tool in hardness of approximation, especially in
questions related to the unique games conjecture. We construct a new code that
is exponentially more efficient, but can still be used in many of these
applications. Using the new code we obtain exponential improvements over
several known results, including the following:
  1. For any eps > 0, we show the existence of an n vertex graph G where every
set of o(n) vertices has expansion 1 - eps, but G's adjacency matrix has more
than exp(log^delta n) eigenvalues larger than 1 - eps, where delta depends only
on eps. This answers an open question of Arora, Barak and Steurer (FOCS 2010)
who asked whether one can improve over the noise graph on the Boolean hypercube
that has poly(log n) such eigenvalues.
  2. A gadget that reduces unique games instances with linear constraints
modulo K into instances with alphabet k with a blowup of K^polylog(K),
improving over the previously known gadget with blowup of 2^K.
  3. An n variable integrality gap for Unique Games that that survives
exp(poly(log log n)) rounds of the SDP + Sherali Adams hierarchy, improving on
the previously known bound of poly(log log n).
  We show a connection between the local testability of linear codes and small
set expansion in certain related Cayley graphs, and use this connection to
derandomize the noise graph on the Boolean hypercube."
"We show that testing inclusion between languages represented by regular
expressions with numerical occurrence indicators (RE#s) is NP-hard, even if the
expressions satisfy the requirement of ""unambiguity"", which is required for XML
Schema content model expressions."
"Given a graph and a root, the Maximum Bounded Rooted-Tree Packing (MBRTP)
problem aims at finding K rooted-trees that span the largest subset of
vertices, when each vertex has a limited outdegree. This problem is motivated
by peer-to-peer streaming overlays in under-provisioned systems. We prove that
the MBRTP problem is NP-complete. We present two polynomial-time algorithms
that computes an optimal solution on complete graphs and trees respectively."
"I will discuss the recent proof that the complexity class NEXP
(nondeterministic exponential time) lacks nonuniform ACC circuits of polynomial
size. The proof will be described from the perspective of someone trying to
discover it."
"L (Logarithmic space) versus NL (Non-deterministic logarithmic space) is one
of the great open problems in computational complexity theory. In the paper
""Bounds on monotone switching networks for directed connectivity"", we separated
monotone analogues of L and NL using a model called the switching network
model. In particular, by considering inputs consisting of just a path and
isolated vertices, we proved that any monotone switching network solving
directed connectivity on $N$ vertices must have size at least
$N^{\Omega(\lg(N))}$ and this bound is tight. If we could show a similar result
for general switching networks solving directed connectivity, then this would
prove that $L \neq NL$. However, proving lower bounds for general switching
networks solving directed connectivity requires proving stronger lower bounds
on monotone switching networks for directed connectivity. To work towards this
goal, we investigated a different set of inputs which we believed to be hard
for monotone switching networks to solve and attempted to prove similar lower
size bounds. Instead, we found that this set of inputs is actually easy for
monotone switching networks for directed connectivity to solve, yet if we
restrict ourselves to certain-knowledge switching networks, which are a simple
and intuitive subclass of monotone switching networks for directed
connectivity, then these inputs are indeed hard to solve. In this paper, we
give this set of inputs, demonstrate a ""weird"" polynomially-sized monotone
switching network for directed connectivity which solves this set of inputs,
and prove that no polynomially-sized certain-knowledge switching network can
solve this set of inputs, thus proving that monotone switching networks for
directed connectivity are strictly more powerful than certain-knowledge
switching networks."
"We give a complexity dichotomy theorem for the counting Constraint
Satisfaction Problem (#CSP in short) with complex weights. To this end, we give
three conditions for its tractability. Let F be any finite set of
complex-valued functions, then we prove that #CSP(F) is solvable in polynomial
time if all three conditions are satisfied; and is #P-hard otherwise.
  Our complexity dichotomy generalizes a long series of important results on
counting problems: (a) the problem of counting graph homomorphisms is the
special case when there is a single symmetric binary function in F; (b) the
problem of counting directed graph homomorphisms is the special case when there
is a single not-necessarily-symmetric binary function in F; and (c) the
standard form of #CSP is when all functions in F take values in {0,1}."
"We consider the Moran process, as generalized by Lieberman, Hauert and Nowak
(Nature, 433:312--316, 2005). A population resides on the vertices of a finite,
connected, undirected graph and, at each time step, an individual is chosen at
random with probability proportional to its assigned 'fitness' value. It
reproduces, placing a copy of itself on a neighbouring vertex chosen uniformly
at random, replacing the individual that was there. The initial population
consists of a single mutant of fitness $r>0$ placed uniformly at random, with
every other vertex occupied by an individual of fitness 1. The main quantities
of interest are the probabilities that the descendants of the initial mutant
come to occupy the whole graph (fixation) and that they die out (extinction);
almost surely, these are the only possibilities. In general, exact computation
of these quantities by standard Markov chain techniques requires solving a
system of linear equations of size exponential in the order of the graph so is
not feasible. We show that, with high probability, the number of steps needed
to reach fixation or extinction is bounded by a polynomial in the number of
vertices in the graph. This bound allows us to construct fully polynomial
randomized approximation schemes (FPRAS) for the probability of fixation (when
$r\geq 1$) and of extinction (for all $r>0$)."
"We explore several concepts for analyzing the intuitive notion of
computational irreducibility and we propose a robust formal definition, first
in the field of cellular automata and then in the general field of any
computable function f from N to N. We prove that, through a robust definition
of what means ""to be unable to compute the nth step without having to follow
the same path than simulating the automaton or the function"", this implies
genuinely, as intuitively expected, that if the behavior of an object is
computationally irreducible, no computation of its nth state can be faster than
the simulation itself."
"We prove the formula C(a,b) = K(a|C(a,b)) + C(b|a,C(a,b)) + O(1) that
expresses the plain complexity of a pair in terms of prefix and plain
conditional complexities of its components."
"This paper talks about difference between P and NP by using topological space
that mean resolution principle. I pay attention to restrictions of antecedent
and consequent in resolution, and show what kind of influence the restrictions
have for difference of structure between P and NP regarding relations of
relation.
  First, I show the restrictions of antecedent and consequent in resolution
principle. Antecedents connect each other, and consequent become a linkage
between these antecedents. And we can make consequent as antecedents product by
using some resolutions which have same joint variable. We can determine these
consequents reducible and irreducible.
  Second, I introduce RCNF that mean topology of resolution principle in CNF.
RCNF is HornCNF and that variable values are presence of restrictions of CNF
formula clauses. RCNF is P-Complete.
  Last, I introduce TCNF that have 3CNF's character which relate 2 variables
relations with 1 variable. I show CNF complexity by using CCNF that combine
some TCNF. TCNF is NP-Complete and product irreducible. I introduce CCNF that
connect TCNF like Moore graph. We cannot reduce CCNF to RCNF with polynomial
size. Therefore, TCNF is not in P."
"Smoothed analysis is a new way of analyzing algorithms introduced by Spielman
and Teng (J. ACM, 2004). Classical methods like worst-case or average-case
analysis have accompanying complexity classes, like P and AvgP, respectively.
While worst-case or average-case analysis give us a means to talk about the
running time of a particular algorithm, complexity classes allows us to talk
about the inherent difficulty of problems.
  Smoothed analysis is a hybrid of worst-case and average-case analysis and
compensates some of their drawbacks. Despite its success for the analysis of
single algorithms and problems, there is no embedding of smoothed analysis into
computational complexity theory, which is necessary to classify problems
according to their intrinsic difficulty.
  We propose a framework for smoothed complexity theory, define the relevant
classes, and prove some first hardness results (of bounded halting and tiling)
and tractability results (binary optimization problems, graph coloring,
satisfiability). Furthermore, we discuss extensions and shortcomings of our
model and relate it to semi-random models."
"We consider the problem of testing whether a Boolean function has Fourier
degree $\leq k$ or it is $\epsilon$-far from any Boolean function with Fourier
degree $\leq k$. We improve the known lower bound of $\Omega(k)$
\cite{BBM11,CGM10}, to $\Omega(k/\sqrt{\epsilon})$. The lower bound uses the
recently discovered connections between property testing and communication
complexity by Blais \textit{et. al.} \cite{BBM11}"
"We consider the problems of determining the feasibility of a linear
congruence, producing a solution to a linear congruence, and finding a spanning
set for the nullspace of an integer matrix, where each problem is considered
modulo an arbitrary constant k>1. These problems are known to be complete for
the logspace modular counting classes {Mod_k L} = {coMod_k L} in special case
that k is prime (Buntrock et al, 1992). By considering variants of standard
logspace function classes --- related to #L and functions computable by UL
machines, but which only characterize the number of accepting paths modulo k
--- we show that these problems of linear algebra are also complete for
{coMod_k L} for any constant k>1.
  Our results are obtained by defining a class of functions FUL_k which are low
for {Mod_k L} and {coMod_k L} for k>1, using ideas similar to those used in the
case of k prime in (Buntrock et al, 1992) to show closure of Mod_k L under NC^1
reductions (including {Mod_k L} oracle reductions). In addition to the results
above, we briefly consider the relationship of the class FUL_k for arbitrary
moduli k to the class {F.coMod_k L} of functions whose output symbols are
verifiable by {coMod_k L} algorithms; and consider what consequences such a
comparison may have for oracle closure results of the form {Mod_k L}^{Mod_k L}
= {Mod_k L} for composite k."
"The problems studied in this paper originate from Graph Motif, a problem
introduced in 2006 in the context of biological networks. Informally speaking,
it consists in deciding if a multiset of colors occurs in a connected subgraph
of a vertex-colored graph. Due to the high rate of noise in the biological
data, more flexible definitions of the problem have been outlined. We present
in this paper two inapproximability results for two different optimization
variants of Graph Motif: one where the size of the solution is maximized, the
other when the number of substitutions of colors to obtain the motif from the
solution is minimized. We also study a decision version of Graph Motif where
the connectivity constraint is replaced by the well known notion of graph
modularity. While the problem remains NP-complete, it allows algorithms in FPT
for biologically relevant parameterizations."
"The results of Raghavendra (2008) show that assuming Khot's Unique Games
Conjecture (2002), for every constraint satisfaction problem there exists a
generic semi-definite program that achieves the optimal approximation factor.
This result is existential as it does not provide an explicit optimal rounding
procedure nor does it allow to calculate exactly the Unique Games hardness of
the problem.
  Obtaining an explicit optimal approximation scheme and the corresponding
approximation factor is a difficult challenge for each specific approximation
problem. An approach for determining the exact approximation factor and the
corresponding optimal rounding was established in the analysis of MAX-CUT (KKMO
2004) and the use of the Invariance Principle (MOO 2005). However, this
approach crucially relies on results explicitly proving optimal partitions in
Gaussian space. Until recently, Borell's result (Borell 1985) was the only
non-trivial Gaussian partition result known.
  In this paper we derive the first explicit optimal approximation algorithm
and the corresponding approximation factor using a new result on Gaussian
partitions due to Isaksson and Mossel (2012). This Gaussian result allows us to
determine exactly the Unique Games Hardness of MAX-3-EQUAL. In particular, our
results show that Zwick algorithm for this problem achieves the optimal
approximation factor and prove that the approximation achieved by the algorithm
is $\approx 0.796$ as conjectured by Zwick.
  We further use the previously known optimal Gaussian partitions results to
obtain a new Unique Games Hardness factor for MAX-k-CSP : Using the well known
fact that jointly normal pairwise independent random variables are fully
independent, we show that the the UGC hardness of Max-k-CSP is $\frac{\lceil
(k+1)/2 \rceil}{2^{k-1}}$, improving on results of Austrin and Mossel (2009)."
"We study the computational complexity of a perfect-information two-player
game proposed by Aigner and Fromme. The game takes place on an undirected graph
where n simultaneously moving cops attempt to capture a single robber, all
moving at the same speed. The players are allowed to pick their starting
positions at the first move. The question of the computational complexity of
deciding this game was raised in the '90s by Goldstein and Reingold. We prove
that the game is hard for PSPACE."
"We show that polynomial-time randomness (p-randomness) is preserved under a
variety of familiar operations, including addition and multiplication by a
nonzero polynomial-time computable real number. These results follow from a
general theorem: If $I$ is an open interval in the reals, $f$ is a function
mapping $I$ into the reals, and $r$ in $I$ is p-random, then $f(r)$ is p-random
provided
  1. $f$ is p-computable on the dyadic rational points in $I$, and
  2. $f$ varies sufficiently at $r$, i.e., there exists a real constant $C > 0$
such that either (a) $(f(x) - f(r))/(x-r) > C$ for all $x$ in $I$ with $x \ne
r$, or (b) $(f(x) - f(r))(x-r) < -C$ for all $x$ in $I$ with $x \ne r$.
  Our theorem implies in particular that any analytic function about a
p-computable point whose power series has uniformly p-computable coefficients
preserves p-randomness in its open interval of absolute convergence. Such
functions include all the familiar functions from first-year calculus."
"Peter Gacs showed (Gacs 1974) that for every n there exists a bit string x of
length n whose plain complexity C(x) has almost maximal conditional complexity
relative to x, i.e., C(C(x)|x) > log n - log^(2) n - O(1). (Here log^(2) i =
log log i.) Following Elena Kalinina (Kalinina 2011), we provide a simple
game-based proof of this result; modifying her argument, we get a better (and
tight) bound log n - O(1). We also show the same bound for prefix-free
complexity.
  Robert Solovay showed (Solovay 1975) that infinitely many strings x have
maximal plain complexity but not maximal prefix complexity (among the strings
of the same length): for some c there exist infinitely many x such that |x| -
C(x) < c and |x| + K(|x|) - K(x) > log^(2) |x| - c log^(3) |x|. In fact, the
results of Solovay and Gacs are closely related. Using the result above, we
provide a short proof for Solovay's result. We also generalize it by showing
that for some c and for all n there are strings x of length n with n - C (x) <
c and n + K(n) - K(x) > K(K(n)|n) - 3 K(K(K(n)|n)|n) - c. We also prove a close
upper bound K(K(n)|n) + O(1).
  Finally, we provide a direct game proof for Joseph Miller's generalization
(Miller 2006) of the same Solovay's theorem: if a co-enumerable set (a set with
c.e. complement) contains for every length a string of this length, then it
contains infinitely many strings x such that |x| + K(|x|) - K(x) > log^(2) |x|
+ O(log^(3) |x|)."
"Chordal graphs are intersection graphs of subtrees of a tree T. We
investigate the complexity of the partial representation extension problem for
chordal graphs. A partial representation specifies a tree T' and some pre-drawn
subtrees of T'. It asks whether it is possible to construct a representation
inside a modified tree T which extends the partial representation (i.e, keeps
the pre-drawn subtrees unchanged).
  We consider four modifications of T' and get vastly different problems. In
some cases, it is interesting to consider the complexity even if just T' is
given and no subtree is pre-drawn. Also, we consider three well-known
subclasses of chordal graphs: Proper interval graphs, interval graphs and path
graphs. We give an almost complete complexity characterization.
  We further study the parametrized complexity of the problems when
parametrized by the number of pre-drawn subtrees, the number of components and
the size of the tree T'. We describe an interesting relation with integer
partition problems. The problem Partition is used for all NP-completeness
reductions. The extension of interval graphs when the space in T' is limited is
""equivalent"" to the BinPacking problem."
"In this research paper, the problem of optimization of a quadratic form over
the convex hull generated by the corners of hypercube is attempted and solved.
Some results related to stable states/vectors, anti-stable states/vectors (over
the hypercube) are discussed. Some results related to the computation of global
optimum stable state (an NP hard problem) are discussed. It is hoped that the
results shed light on resolving the P \neq NP problem."
"It is already shown that a Boolean function for a NP-complete problem can be
computed by a polynomial-sized circuit if its variables have enough number of
automorphisms. Looking at this previous study from the different perspective
gives us the idea that the small number of automorphisms might be a barrier for
a polynomial time solution for NP-complete problems. Here I show that by
interpreting a Boolean circuit as a graph, the small number of graph
automorphisms and the large number of subgraph automorphisms in the circuit
establishes the exponential circuit lower bound for NP-complete problems. As
this strategy violates the largeness condition in Natural proof, this result
shows that P!=NP without any contradictions to the existence of pseudorandom
functions."
"Holant problems are a general framework to study the algorithmic complexity
of counting problems. Both counting constraint satisfaction problems and graph
homomorphisms are special cases. All previous results of Holant problems are
over the Boolean domain. In this paper, we give the first dichotomy theorem for
Holant problems for domain size $>2$. We discover unexpected tractable families
of counting problems, by giving new polynomial time algorithms. This paper also
initiates holographic reductions in domains of size $>2$. This is our main
algorithmic technique, and is used for both tractable families and hardness
reductions. The dichotomy theorem is the following: For any complex-valued
symmetric function ${\bf F}$ with arity 3 on domain size 3, we give an explicit
criterion on ${\bf F}$, such that if ${\bf F}$ satisfies the criterion then the
problem ${\rm Holant}^*({\bf F})$ is computable in polynomial time, otherwise
${\rm Holant}^*({\bf F})$ is #P-hard."
"The Canadian traveler problem (CTP) is the problem of traversing a given
graph, where some of the edges may be blocked - a state which is revealed only
upon reaching an incident vertex. Originally stated by Papadimitriou and
Yannakakis (1991), the adversarial version of CTP was shown to be
PSPACE-complete, with the stochastic version shown to be #P-hard. We show that
stochastic CTP is also PSPACE-complete: initially proving PSPACE-hardness for
the dependent version of stochastic CTP,and proceeding with gadgets that allow
us to extend the proof to the independent case. Since for disjoint-path graphs,
CTP can be solved in polynomial time, we examine the complexity of the more
general remote-sensing CTP, and show that it is NP-hard even for disjoint-path
graphs."
"Presentation for a talk ""Two betting strategies that predict all compressible
sequences"" given at Seventh International Conference on Computability,
Complexity and Randomness (CCR 2012)
http://www.newton.ac.uk/programmes/SAS/seminars/070217001.html"
"We continue to study the notion of cancellation-free linear circuits. We show
that every matrix can be computed by a cancellation- free circuit, and almost
all of these are at most a constant factor larger than the optimum linear
circuit that computes the matrix. It appears to be easier to prove statements
about the structure of cancellation-free linear circuits than for linear
circuits in general. We prove two nontrivial superlinear lower bounds. We show
that a cancellation-free linear circuit computing the $n\times n$ Sierpinski
gasket matrix must use at least 1/2 n logn gates, and that this is tight. This
supports a conjecture by Aaronson. Furthermore we show that a proof strategy
for proving lower bounds on monotone circuits can be almost directly converted
to prove lower bounds on cancellation-free linear circuits. We use this
together with a result from extremal graph theory due to Andreev to prove a
lower bound of {\Omega}(n^(2- \epsilon)) for infinitely many $n \times n$
matrices for every $\epsilon > 0$ for. These lower bounds for concrete matrices
are almost optimal since all matrices can be computed with $O(n^2/\log n)$
gates."
"A class of valued constraint satisfaction problems (VCSPs) is characterised
by a valued constraint language, a fixed set of cost functions on a finite
domain. An instance of the problem is specified by a sum of cost functions from
the language with the goal to minimise the sum.
  We study which classes of finite-valued languages can be solved exactly by
the basic linear programming relaxation (BLP). Thapper and Zivny showed [20]
that if BLP solves the language then the language admits a binary commutative
fractional polymorphism. We prove that the converse is also true. This leads to
a necessary and a sufficient condition which can be checked in polynomial time
for a given language. In contrast, the previous necessary and sufficient
condition due to [20] involved infinitely many inequalities.
  More recently, Thapper and Zivny [21] showed (using, in particular, a
technique introduced in this paper) that core languages that do not satisfy our
condition are NP-hard. Taken together, these results imply that a finite-valued
language can either be solved using Linear Programming or is NP-hard."
"A strong direct product theorem states that, in order to solve k instances of
a problem, if we provide less than k times the resource required to compute one
instance, then the probability of overall success is exponentially small in k.
In this paper, we consider the model of two-way public-coin communication
complexity and show a strong direct product theorem for all relations in terms
of the smooth rectangle bound, introduced by Jain and Klauck as a generic lower
bound method in this model. Our result therefore uniformly implies a strong
direct product theorem for all relations for which an (asymptotically) optimal
lower bound can be provided using the smooth rectangle bound, for example Inner
Product, Greater-Than, Set-Disjointness, Gap-Hamming Distance etc. Our result
also implies near optimal direct product results for several important
functions and relations used to show exponential separations between classical
and quantum communication complexity, for which near optimal lower bounds are
provided using the rectangle bound, for example by Raz [1999], Gavinsky [2008]
and Klartag and Regev [2011]. In fact we are not aware of any relation for
which it is known that the smooth rectangle bound does not provide an optimal
lower bound. This lower bound subsumes many of the other lower bound methods,
for example the rectangle bound (a.k.a the corruption bound), the smooth
discrepancy bound (a.k.a the \gamma_2 bound) which in turn subsumes the
discrepancy bound, the subdistribution bound and the conditional min-entropy
bound.
  We show our result using information theoretic arguments. A key tool we use
is a sampling protocol due to Braverman [2012], in fact a modification of it
used by Kerenidis, Laplante, Lerays, Roland and Xiao [2012]."
"We call a depth-4 formula C set-depth-4 if there exists a (unknown) partition
(X_1,...,X_d) of the variable indices [n] that the top product layer respects,
i.e. C(x) = \sum_{i=1}^k \prod_{j=1}^{d} f_{i,j}(x_{X_j}), where f_{i,j} is a
sparse polynomial in F[x_{X_j}]. Extending this definition to any depth - we
call a depth-Delta formula C (consisting of alternating layers of Sigma and Pi
gates, with a Sigma-gate on top) a set-depth-Delta formula if every Pi-layer in
C respects a (unknown) partition on the variables; if Delta is even then the
product gates of the bottom-most Pi-layer are allowed to compute arbitrary
monomials.
  In this work, we give a hitting-set generator for set-depth-Delta formulas
(over any field) with running time polynomial in exp(({Delta}^2 log s)^{Delta -
1}), where s is the size bound on the input set-depth-Delta formula. In other
words, we give a quasi-polynomial time blackbox polynomial identity test for
such constant-depth formulas. Previously, the very special case of Delta=3
(also known as set-multilinear depth-3 circuits) had no known sub-exponential
time hitting-set generator. This was declared as an open problem by Shpilka &
Yehudayoff (FnT-TCS 2010); the model being first studied by Nisan & Wigderson
(FOCS 1995). Our work settles this question, not only for depth-3 but, up to
depth epsilon.log s / loglog s, for a fixed constant epsilon < 1.
  The technique is to investigate depth-Delta formulas via depth-(Delta-1)
formulas over a Hadamard algebra, after applying a `shift' on the variables. We
propose a new algebraic conjecture about the low-support rank-concentration in
the latter formulas, and manage to prove it in the case of set-depth-Delta
formulas."
"We study the problem of obtaining deterministic black-box polynomial identity
testing algorithms (PIT) for algebraic branching programs (ABPs) that are
read-once and oblivious. This class has an deterministic white-box polynomial
identity testing algorithm (due to Raz and Shpilka), but prior to this work
there was no known such black-box algorithm.
  The main result of this work gives the first quasi-polynomial sized hitting
sets for size S circuits from this class, when the order of the variables is
known. As our hitting set is of size exp(lg^2 S), this is analogous (in the
terminology of boolean pseudorandomness) to a seed-length of lg^2 S, which is
the seed length of the pseudorandom generators of Nisan and
Impagliazzo-Nisan-Wigderson for read-once oblivious boolean branching programs.
  Our results are stronger for branching programs of bounded width, where we
give a hitting set of size exp(lg^2 S/lglg S), corresponding to a seed length
of lg^2 S/lglg S. This is in stark contrast to the known results for read-once
oblivious boolean branching programs of bounded width, where no pseudorandom
generator (or hitting set) with seed length o(lg^2 S) is known.
  In follow up work, we strengthened a result of Mulmuley, and showed that
derandomizing a particular case of the Noether Normalization Lemma is reducible
to black-box PIT of read-once oblivious ABPs. Using the results of the present
work, this gives a derandomization of Noether Normalization in that case, which
Mulmuley conjectured would difficult due to its relations to problems in
algebraic geometry.
  We also show that several other circuit classes can be black-box reduced to
read-once oblivious ABPs, including set-multilinear ABPs (a generalization of
depth-3 set-multilinear formulas), non-commutative ABPs (generalizing
non-commutative formulas), and (semi-)diagonal depth-4 circuits (as introduced
by Saxena)."
"The theory of random real numbers is exceedingly well-developed, and
fascinating from many points of view. It is also quite challenging
mathematically. The present notes are intended as no more than a gateway to the
larger theory. They review just the most elementary part of the theory (bearing
on Kolmogorov- and ML-randomness). We hope that the simple arguments presented
here will encourage the enterprising student to examine richer treatments of
the subject available elsewhere, notably, in Downey and Hirschfeldt (2010)."
"This paper is concerned with the automated complexity analysis of term
rewrite systems (TRSs for short) and the ramification of these in implicit
computational complexity theory (ICC for short). We introduce a novel path
order with multiset status, the polynomial path order POP*. Essentially relying
on the principle of predicative recursion as proposed by Bellantoni and Cook,
its distinct feature is the tight control of resources on compatible TRSs: The
(innermost) runtime complexity of compatible TRSs is polynomially bounded. We
have implemented the technique, as underpinned by our experimental evidence our
approach to the automated runtime complexity analysis is not only feasible, but
compared to existing methods incredibly fast. As an application in the context
of ICC we provide an order-theoretic characterisation of the polytime
computable functions. To be precise, the polytime computable functions are
exactly the functions computable by an orthogonal constructor TRS compatible
with POP*."
"We investigate the algebraic complexity of tensor calulus. We consider a
generalization of iterated matrix product to tensors and show that the
resulting formulas exactly capture VP, the class of polynomial families
efficiently computable by arithmetic circuits. This gives a natural and robust
characterization of this complexity class that despite its naturalness is not
very well understood so far."
"We examine the power of silent constant-space probabilistic verifiers that
watch asymmetric debates (where one side is unable to see some of the messages
of the other) between two deterministic provers, and try to determine who is
right. We prove that probabilistic verifiers outperform their deterministic
counterparts as asymmetric debate checkers. It is shown that the membership
problem for every language in NSPACE(s(n)) has a 2^{s(n)}-time debate where one
prover is completely blind to the other one, for polynomially bounded space
constructible s(n). When partial information is allowed to be seen by the
handicapped prover, the class of languages debatable in 2^{s(n)} time contains
TIME(2^{s(n)}), so a probabilistic finite automaton can solve any decision
problem in P with small error in polynomial time with the aid of such a debate.
We also compare our systems with those with a single prover, and with
competing-prover interactive proof systems."
"We study the parameterized complexity of domination-type problems.
(sigma,rho)-domination is a general and unifying framework introduced by Telle:
a set D of vertices of a graph G is (sigma,rho)-dominating if for any v in D,
|N(v)\cap D| in sigma and for any $v\notin D, |N(v)\cap D| in rho. We mainly
show that for any sigma and rho the problem of (sigma,rho)-domination is W[2]
when parameterized by the size of the dominating set. This general statement is
optimal in the sense that several particular instances of
(sigma,rho)-domination are W[2]-complete (e.g. Dominating Set). We also prove
that (sigma,rho)-domination is W[2] for the dual parameterization, i.e. when
parameterized by the size of the dominated set. We extend this result to a
class of domination-type problems which do not fall into the
(sigma,rho)-domination framework, including Connected Dominating Set. We also
consider problems of coding theory which are related to domination-type
problems with parity constraints. In particular, we prove that the problem of
the minimal distance of a linear code over Fq is W[2] for both standard and
dual parameterizations, and W[1]-hard for the dual parameterization.
  To prove W[2]-membership of the domination-type problems we extend the
Turing-way to parameterized complexity by introducing a new kind of non
deterministic Turing machine with the ability to perform `blind' transitions,
i.e. transitions which do not depend on the content of the tapes. We prove that
the corresponding problem Short Blind Multi-Tape Non-Deterministic Turing
Machine is W[2]-complete. We believe that this new machine can be used to prove
W[2]-membership of other problems, not necessarily related to domination"
"We present an iterative approach to constructing pseudorandom generators,
based on the repeated application of mild pseudorandom restrictions. We use
this template to construct pseudorandom generators for combinatorial rectangles
and read-once CNFs and a hitting set generator for width-3 branching programs,
all of which achieve near-optimal seed-length even in the low-error regime: We
get seed-length O(log (n/epsilon)) for error epsilon. Previously, only
constructions with seed-length O(\log^{3/2} n) or O(\log^2 n) were known for
these classes with polynomially small error.
  The (pseudo)random restrictions we use are milder than those typically used
for proving circuit lower bounds in that we only set a constant fraction of the
bits at a time. While such restrictions do not simplify the functions
drastically, we show that they can be derandomized using small-bias spaces."
"Assume $D$ is a finite set and $R$ is a finite set of functions from $D$ to
the natural numbers. An instance of the minimum $R$-cost homomorphism problem
($MinHom_R$) is a set of variables $V$ subject to specified constraints
together with a positive weight $c_{vr}$ for each combination of $v \in V$ and
$r \in R$. The aim is to find a function $f:V \rightarrow D$ such that $f$
satisfies all constraints and $\sum_{v \in V} \sum_{r \in R} c_{vr}r(f(v))$ is
minimized.
  This problem unifies well-known optimization problems such as the minimum
cost homomorphism problem and the maximum solution problem, and this makes it a
computationally interesting fragment of the valued CSP framework for
optimization problems. We parameterize $MinHom_R\left(\Gamma\right)$ by {\em
constraint languages}, i.e. sets $\Gamma$ of relations that are allowed in
constraints. A constraint language is called {\em conservative} if every unary
relation is a member of it; such constraint languages play an important role in
understanding the structure of constraint problems. The dichotomy conjecture
for $MinHom_R$ is the following statement: if $\Gamma$ is a constraint
language, then $MinHom_R\left(\Gamma\right)$ is either polynomial-time solvable
or NP-complete. For $MinHom$ the dichotomy result has been recently obtained
[Takhanov, STACS, 2010] and the goal of this paper is to expand this result to
the case of $MinHom_R$ with conservative constraint language. For arbitrary $R$
this problem is still open, but assuming certain restrictions on $R$ we prove a
dichotomy. As a consequence of this result we obtain a dichotomy for the
conservative maximum solution problem."
"We study the computational complexity of exact minimisation of
rational-valued discrete functions. Let $\Gamma$ be a set of rational-valued
functions on a fixed finite domain; such a set is called a finite-valued
constraint language. The valued constraint satisfaction problem,
$\operatorname{VCSP}(\Gamma)$, is the problem of minimising a function given as
a sum of functions from $\Gamma$. We establish a dichotomy theorem with respect
to exact solvability for all finite-valued constraint languages defined on
domains of arbitrary finite size.
  We show that every constraint language $\Gamma$ either admits a binary
symmetric fractional polymorphism in which case the basic linear programming
relaxation solves any instance of $\operatorname{VCSP}(\Gamma)$ exactly, or
$\Gamma$ satisfies a simple hardness condition that allows for a
polynomial-time reduction from Max-Cut to $\operatorname{VCSP}(\Gamma)$."
The method in this paper is wrong.
"One of the earliest and best-known application of the probabilistic method is
the proof of existence of a 2 log n$-Ramsey graph, i.e., a graph with n nodes
that contains no clique or independent set of size 2 log n. The explicit
construction of such a graph is a major open problem. We show that a reasonable
hardness assumption implies that in polynomial time one can construct a list
containing polylog(n) graphs such that most of them are 2 log n-Ramsey."
"Spark plays a great role in studying uniqueness of sparse solutions of the
underdetermined linear equations. In this article, we derive a new lower bound
of spark. As an application, we obtain a new criterion for the uniqueness of
sparse solutions of linear equations."
"We show that given a 3-colorable graph, it is NP-hard to find a 3-coloring
with $(16/17 + \eps)$ of the edges bichromatic. In a related result, we show
that given a satisfiable instance of the 2-to-1 Label Cover problem, it is
NP-hard to find a $(23/24 + \eps)$-satisfying assignment."
"In this paper, we survey the complexity of distinct methods that allow the
programmer to synthesize a sup-interpretation, a function providing an upper-
bound on the size of the output values computed by a program. It consists in a
static space analysis tool without consideration of the time consumption.
Although clearly related, sup-interpretation is independent from termination
since it only provides an upper bound on the terminating computations. First,
we study some undecidable properties of sup-interpretations from a theoretical
point of view. Next, we fix term rewriting systems as our computational model
and we show that a sup-interpretation can be obtained through the use of a
well-known termination technique, the polynomial interpretations. The drawback
is that such a method only applies to total functions (strongly normalizing
programs). To overcome this problem we also study sup-interpretations through
the notion of quasi-interpretation. Quasi-interpretations also suffer from a
drawback that lies in the subterm property. This property drastically restricts
the shape of the considered functions. Again we overcome this problem by
introducing a new notion of interpretations mainly based on the dependency
pairs method. We study the decidability and complexity of the
sup-interpretation synthesis problem for all these three tools over sets of
polynomials. Finally, we take benefit of some previous works on termination and
runtime complexity to infer sup-interpretations."
"In the present paper we show a dichotomy theorem for the complexity of
polynomial evaluation. We associate to each graph H a polynomial that encodes
all graphs of a fixed size homomorphic to H. We show that this family is
computable by arithmetic circuits in constant depth if H has a loop or no edge
and that it is hard otherwise (i.e., complete for VNP, the arithmetic class
related to #P). We also demonstrate the hardness over the rational field of cut
eliminator, a polynomial defined by B\""urgisser which is known to be neither VP
nor VNP-complete in the field of two elements, if VP is not equal to VNP (VP is
the class of polynomials computable by arithmetic circuit of polynomial size)."
"This article provide new approach to solve P vs NP problem by using
cardinality of bases function. About NP-Complete problems, we can divide to
infinite disjunction of P-Complete problems. These P-Complete problems are
independent of each other in disjunction. That is, NP-Complete problem is in
infinite dimension function space that bases are P-Complete. The other hand,
any P-Complete problem have at most a finite number of P-Complete basis. The
reason is that each P problems have at most finite number of Least fixed point
operator. Therefore, we cannot describe NP-Complete problems in P. We can also
prove this result from incompleteness of P."
"We study connections between Natural Proofs, derandomization, and the problem
of proving ""weak"" circuit lower bounds such as ${\sf NEXP} \not\subset {\sf
TC^0}$. Natural Proofs have three properties: they are constructive (an
efficient algorithm $A$ is embedded in them), have largeness ($A$ accepts a
large fraction of strings), and are useful ($A$ rejects all strings which are
truth tables of small circuits). Strong circuit lower bounds that are
""naturalizing"" would contradict present cryptographic understanding, yet the
vast majority of known circuit lower bound proofs are naturalizing. So it is
imperative to understand how to pursue un-Natural Proofs. Some heuristic
arguments say constructivity should be circumventable: largeness is inherent in
many proof techniques, and it is probably our presently weak techniques that
yield constructivity. We prove:
  $\bullet$ Constructivity is unavoidable, even for $\sf NEXP$ lower bounds.
Informally, we prove for all ""typical"" non-uniform circuit classes ${\cal C}$,
${\sf NEXP} \not\subset {\cal C}$ if and only if there is a polynomial-time
algorithm distinguishing some function from all functions computable by ${\cal
C}$-circuits. Hence ${\sf NEXP} \not\subset {\cal C}$ is equivalent to
exhibiting a constructive property useful against ${\cal C}$.
  $\bullet$ There are no $\sf P$-natural properties useful against ${\cal C}$
if and only if randomized exponential time can be ""derandomized"" using truth
tables of circuits from ${\cal C}$ as random seeds. Therefore the task of
proving there are no $\sf P$-natural properties is inherently a derandomization
problem, weaker than but implied by the existence of strong pseudorandom
functions.
  These characterizations are applied to yield several new results, including
improved ${\sf ACC}^0$ lower bounds and new unconditional derandomizations."
"We show some facts regarding the question whether, for any number $n$, the
length of the shortest Addition Multiplications Chain (AMC) computing $n$ is
polynomial in the length of the shortest division-free Straight Line Program
(SLP) that computes $n$.
  If the answer to this question is ""yes"", then we can show a stronger upper
bound for $\mathrm{PosSLP}$, the important problem which essentially captures
the notion of efficient computation over the reals. If the answer is ""no"", then
this would demonstrate how subtraction helps generating integers
super-polynomially faster, given that addition and multiplication can be done
in unit time.
  In this paper, we show that, for almost all numbers, AMCs and SLPs need same
asymptotic length for computation. However, for one specific form of numbers,
SLPs are strictly more powerful than AMCs by at least one step of computation."
"We investigate the complexity of uniform OR circuits and AND circuits of
polynomial-size and depth. As their name suggests, OR circuits have OR gates as
their computation gates, as well as the usual input, output and constant (0/1)
gates. As is the norm for Boolean circuits, our circuits have multiple sink
gates, which implies that an OR circuit computes an OR function on some subset
of its input variables. Determining that subset amounts to solving a number of
reachability questions on a polynomial-size directed graph (which input gates
are connected to the output gate?), taken from a very sparse set of graphs.
However, it is not obvious whether or not this (restricted) reachability
problem can be solved, by say, uniform AC^0 circuits (constant depth,
polynomial-size, AND, OR, NOT gates). This is one reason why characterizing the
power of these simple-looking circuits in terms of uniform classes turns out to
be intriguing. Another is that the model itself seems particularly natural and
worthy of study.
  Our goal is the systematic characterization of uniform polynomial-size OR
circuits, and AND circuits, in terms of known uniform machine-based complexity
classes. In particular, we consider the languages reducible to such uniform
families of OR circuits, and AND circuits, under a variety of reduction types.
We give upper and lower bounds on the computational power of these language
classes. We find that these complexity classes are closely related to tallyNL,
the set of unary languages within NL, and to sets reducible to tallyNL.
Specifically, for a variety of types of reductions (many-one, conjunctive truth
table, disjunctive truth table, truth table, Turing) we give characterizations
of languages reducible to OR circuit classes in terms of languages reducible to
tallyNL classes. Then, some of these OR classes are shown to coincide, and some
are proven to be distinct. We give analogous results for AND circuits. Finally,
for many of our OR circuit classes, and analogous AND circuit classes, we prove
whether or not the two classes coincide, although we leave one such inclusion
open."
"We give a nontrivial algorithm for the satisfiability problem for cn-wire
threshold circuits of depth two which is better than exhaustive search by a
factor 2^{sn} where s= 1/c^{O(c^2)}. We believe that this is the first
nontrivial satisfiability algorithm for cn-wire threshold circuits of depth
two. The independently interesting problem of the feasibility of sparse 0-1
integer linear programs is a special case. To our knowledge, our algorithm is
the first to achieve constant savings even for the special case of Integer
Linear Programming. The key idea is to reduce the satisfiability problem to the
Vector Domination Problem, the problem of checking whether there are two
vectors in a given collection of vectors such that one dominates the other
component-wise.
  We also provide a satisfiability algorithm with constant savings for depth
two circuits with symmetric gates where the total weighted fan-in is at most
cn.
  One of our motivations is proving strong lower bounds for TC^0 circuits,
exploiting the connection (established by Williams) between satisfiability
algorithms and lower bounds. Our second motivation is to explore the connection
between the expressive power of the circuits and the complexity of the
corresponding circuit satisfiability problem."
"Our main result is a formulation and proof of the reverse hypercontractive
inequality in the sum-of-squares (SOS) proof system. As a consequence we show
that for any constant $0 < \gamma \leq 1/4$, the SOS/Lasserre SDP hierarchy at
degree $4\lceil \frac{1}{4\gamma}\rceil$ certifies the statement ""the maximum
independent set in the Frankl--R\""odl graph $\mathrm{FR}^{n}_{\gamma}$ has
fractional size~$o(1)$"". Here $\mathrm{FR}^{n}_{\gamma} = (V,E)$ is the graph
with $V = \{0,1\}^n$ and $(x,y) \in E$ whenever $\Delta(x,y) = (1-\gamma)n$ (an
even integer). In particular, we show the degree-$4$ SOS algorithm certifies
the chromatic number lower bound ""$\chi(\mathrm{FR}^{n}_{1/4}) = \omega(1)$"",
even though $\mathrm{FR}^{n}_{1/4}$ is the canonical integrality gap instance
for which standard SDP relaxations cannot even certify
""$\chi(\mathrm{FR}^{n}_{1/4}) > 3$"". Finally, we also give an SOS proof of (a
generalization of) the sharp $(2,q)$-hypercontractive inequality for any even
integer $q$."
"Is it possible to find a shortest description for a binary string? The
well-known answer is ""no, Kolmogorov complexity is not computable."" Faced with
this barrier, one might instead seek a short list of candidates which includes
a laconic description. Remarkably such approximations exist. This paper
presents an efficient algorithm which generates a polynomial-size list
containing an optimal description for a given input string. Along the way, we
employ expander graphs and randomness dispersers to obtain an Explicit Online
Matching Theorem for bipartite graphs and a refinement of Muchnik's Conditional
Complexity Theorem. Our main result extends recent work by Bauwens, Mahklin,
Vereschchagin, and Zimand."
"This article describes the solvability of HornSAT and CNFSAT.
  Unsatisfiable HornCNF have partially ordered set that is made by causation of
each clauses. In this partially ordered set, Truth value assignment that is
false in each clauses become simply connected space. Therefore, if we reduce
CNFSAT to HornSAT, we must make such partially ordered set in HornSAT. But
CNFSAT have correlations of each clauses, the partially ordered set is not in
polynomial size.
  Therefore, we cannot reduce CNFSAT to HornSAT in polynomial size."
"Let G=(V,E) be a graph. Let k < |V| be an integer. Let O_k be the number of
edge induced subgraphs of G having k vertices and an odd number of edges. Let
E_k be the number of edge induced subgraphs of G having k vertices and an even
number of edges. Let D_k = O_k - E_k. The ODD EVEN DELTA problem consists in
computing D_k, given G and k. We show that such problem is #P-hard, even on
3-regular bipartite planar graphs."
"Thapper and Zivny [STOC'13] recently classified the complexity of VCSP for
all finite-valued constraint languages. However, the complexity of VCSPs for
constraint languages that are not finite-valued remains poorly understood. In
this paper we study the complexity of two such VCSPs, namely Min-Cost-Hom and
Min-Sol. We obtain a full classification for the complexity of Min-Sol on
domains that contain at most three elements and for the complexity of
conservative Min-Cost-Hom on arbitrary finite domains. Our results answer a
question raised by Takhanov [STACS'10, COCOON'10]."
"In the book Boolean Function Complexity by Stasys Jukna, two lower bound
techniques for Tree-like Cutting Plane proofs (henceforth, ""Tree-CP proofs"")
using Karchmer-Widgerson type communication games (henceforth, ""KW games"") are
presented: The first, applicable to Tree-CP proofs with bounded coefficients,
translates Omega(t) deterministic lower bounds on KW games to 2^Omega(t/log n)
lower bounds on Tree-CP proof size. The second, applicable to Tree-CP proofs
with unbounded coefficients, translates Omega(t) randomized lower bounds on KW
games to 2^Omega(t/log^2 n) lower bounds on Tree-CP proof size.
  The textbook proof in the latter case uses a O(log^2 n)-bit randomized
protocol for the GreaterThan function. However, Nisan mentioned using the ideas
of Feige, et al. to construct a O(log n + log(1/epsilon))-bit randomized
protocol for GreaterThan. Nisan did not explicitly give the proof, though later
results in his paper assume such a protocol.
  In this short exposition, we present the full O(log n + log(1/epsilon))-bit
randomized protocol for the GreaterThan function based on the ideas of Feige,
et al. for ""noisy binary search."" As an application, we show how to translate
Omega(t) randomized lower bounds on KW games to 2^Omega(t/log n) lower bounds
on Tree-CP proof size in the unbounded coefficient case. This equates
randomness with coefficient size for the Tree-CP/KW game lower bound method.
  We believe that, while the O(log n + log(1/epsilon))-bit randomized protocol
for GreaterThan is a ""known"" result, the explicit connection to Tree-CP proof
size lower bounds given here is new."
"We contribute to the program of proving lower bounds on the size of branching
programs solving the Tree Evaluation Problem introduced by Cook et. al. (2012).
Proving a super-polynomial lower bound for the size of nondeterministic thrifty
branching programs (NTBP) would separate $NL$ from $P$ for thrifty models
solving the tree evaluation problem. First, we show that {\em Read-Once NTBPs}
are equivalent to whole black-white pebbling algorithms thus showing a tight
lower bound (ignoring polynomial factors) for this model.
  We then introduce a weaker restriction of NTBPs called {\em Bitwise
Independence}. The best known NTBPs (of size $O(k^{h/2+1})$) for the tree
evaluation problem given by Cook et. al. (2012) are Bitwise Independent. As our
main result, we show that any Bitwise Independent NTBP solving $TEP_{2}^{h}(k)$
must have at least $\frac{1}{2}k^{h/2}$ states. Prior to this work, lower
bounds were known for NTBPs only for fixed heights $h=2,3,4$ (See Cook et. al.
(2012)). We prove our results by associating a fractional black-white pebbling
strategy with any bitwise independent NTBP solving the Tree Evaluation Problem.
Such a connection was not known previously even for fixed heights.
  Our main technique is the entropy method introduced by Jukna and Z{\'a}k
(2001) originally in the context of proving lower bounds for read-once
branching programs. We also show that the previous lower bounds given by Cook
et. al. (2012) for deterministic branching programs for Tree Evaluation Problem
can be obtained using this approach. Using this method, we also show tight
lower bounds for any $k$-way deterministic branching program solving Tree
Evaluation Problem when the instances are restricted to have the same group
operation in all internal nodes."
"We study the class of languages, denoted by $\MIP[k, 1-\epsilon, s]$, which
have $k$-prover games where each prover just sends a \emph{single} bit, with
completeness $1-\epsilon$ and soundness error $s$. For the case that $k=1$
(i.e., for the case of interactive proofs), Goldreich, Vadhan and Wigderson
({\em Computational Complexity'02}) demonstrate that $\SZK$ exactly
characterizes languages having 1-bit proof systems with""non-trivial"" soundness
(i.e., $1/2 < s \leq 1-2\epsilon$). We demonstrate that for the case that
$k\geq 2$, 1-bit $k$-prover games exhibit a significantly richer structure:
  + (Folklore) When $s \leq \frac{1}{2^k} - \epsilon$, $\MIP[k, 1-\epsilon, s]
= \BPP$;
  + When $\frac{1}{2^k} + \epsilon \leq s < \frac{2}{2^k}-\epsilon$, $\MIP[k,
1-\epsilon, s] = \SZK$;
  + When $s \ge \frac{2}{2^k} + \epsilon$, $\AM \subseteq \MIP[k, 1-\epsilon,
s]$;
  + For $s \le 0.62 k/2^k$ and sufficiently large $k$, $\MIP[k, 1-\epsilon, s]
\subseteq \EXP$;
  + For $s \ge 2k/2^{k}$, $\MIP[k, 1, 1-\epsilon, s] = \NEXP$.
  As such, 1-bit $k$-prover games yield a natural ""quantitative"" approach to
relating complexity classes such as $\BPP$,$\SZK$,$\AM$, $\EXP$, and $\NEXP$.
We leave open the question of whether a more fine-grained hierarchy (between
$\AM$ and $\NEXP$) can be established for the case when $s \geq \frac{2}{2^k} +
\epsilon$."
"A constraint satisfaction problem (CSP) is said to be \emph{approximation
resistant} if it is hard to approximate better than the trivial algorithm which
picks a uniformly random assignment. Assuming the Unique Games Conjecture, we
give a characterization of approximation resistance for $k$-partite CSPs
defined by an even predicate."
"We consider an instance of the following problem: Parties P_1,..., P_k each
receive an input x_i, and a coordinator (distinct from each of these parties)
wishes to compute f(x_1,..., x_k) for some predicate f. We are interested in
one-round protocols where each party sends a single message to the coordinator;
there is no communication between the parties themselves. What is the minimum
communication complexity needed to compute f, possibly with bounded error?
  We prove tight bounds on the one-round communication complexity when f
corresponds to the promise problem of distinguishing sums (namely, determining
which of two possible values the {x_i} sum to) or the problem of determining
whether the {x_i} sum to a particular value. Similar problems were studied
previously by Nisan and in concurrent work by Viola. Our proofs rely on basic
theorems from additive combinatorics, but are otherwise elementary."
"For a finite binary string $x$ its logical depth $d$ for significance $b$ is
the shortest running time of a program for $x$ of length $K(x)+b$. There is
another definition of logical depth. We give a new proof that the two versions
are close. There is an infinite sequence of strings of consecutive lengths such
that for every string there is a $b$ such that incrementing $b$ by 1 makes the
associated depths go from incomputable to computable. The maximal gap between
depths resulting from incrementing appropriate $b$'s by 1 is incomputable. The
size of this gap is upper bounded by the Busy Beaver function. Both the upper
and the lower bound hold for the depth with significance 0. As a consequence,
the minimal computation time of the associated shortest programs rises faster
than any computable function but not so fast as the Busy Beaver function."
"We prove that for sufficiently large K, it is NP-hard to color K-colorable
graphs with less than 2^{K^{1/3}} colors. This improves the previous result of
K versus K^{O(log K)} in Khot [14]."
"The linear complementarity problem, LCP(q,M), is defined as follows. For
given M,q find z such that q+Mz>=0, z>=0, z(q + M z)=0,or certify that there is
no such z. It is well known that the problem of finding a Nash equilibrium for
a bimatrix game (2-NASH) can be formulated as a linear complementarity problem
(LCP). In addition, 2-NASH is known to be complete in the complexity class PPAD
(Polynomial-time Parity Argument Directed). However, the ingeniously
constructed reduction (which is designed for any PPAD problem) is very
complicated, so while of great theoretical significance, it is not practical
for actually solving an LCP via 2-NASH, and it may not provide the potential
insight that can be gained from studying the game obtained from a problem
formulated as an LCP (e.g. market equilibrium). The main goal of this paper is
the construction of a simple explicit reduction of any LCP(q,M) that can be
verified as belonging to PPAD via the graph induced by the generic Lemke
algorithm with some positive covering vector d, to a symmetric 2-NASH. In
particular, any endpoint of this graph (with the exception of the initial point
of the algorithm) corresponds to either a solution or to a so-called secondary
ray. Thus, an LCP problem is verified as belonging to PPAD if any secondary ray
can be used to construct, in polynomial time, a certificate that there is no
solution to the problem. We achieve our goal by showing that for any M,q and a
positive d satisfying a certain nondegeneracy assumption with respect to M, we
can simply and directly construct a symmetric 2-NASH whose Nash equilibria
correspond one-to-one to the end points of the graph induced by LCP(q,M) and
the Lemke algorithm with a covering vector d. We note that for a given M the
reduction works for all positive d with the exception of a subset of measure 0."
"The main result of this paper is an optimal strong direct product result for
the two-party public-coin randomized communication complexity of the Tribes
function. This is proved by providing an alternate proof of the optimal lower
bound of \Omega(n) for the randomised communication complexity of the Tribes
function using the so-called smooth-rectangle bound, introduced by Jain and
Klauck [JK10]. The optimal \Omega(n) lower bound for Tribes was originally
proved by Jayram, Kumar and Sivakumar [JKS03], using a more powerful lower
bound technique, namely the information complexity bound. The information
complexity bound is known to be at least as strong a lower bound method as the
smooth-rectangle bound [KLL+12]. On the other hand, we are not aware of any
function or relation for which the smooth-rectangle bound is (asymptotically)
smaller than its public-coin randomized communication complexity. The optimal
direct product for Tribes is obtained by combining our smooth-rectangle bound
for tribes with the strong direct product result of Jain and Yao [JY12] in
terms of smooth-rectangle bound."
"In this letter, we revisit the {\em orbit problem}, which was studied in
\cite{HAR69,SHA79,KL86}. In \cite{KL86}, Kannan and Lipton proved that this
problem is decidable in polynomial time. In this paper, we study the {\em
approximate orbit problem}, and show that this problem is decidable except for
one case."
"In this paper we present a combination framework for polynomial complexity
analysis of term rewrite systems. The framework covers both derivational and
runtime complexity analysis. We present generalisations of powerful complexity
techniques, notably a generalisation of complexity pairs and (weak) dependency
pairs. Finally, we also present a novel technique, called dependency graph
decomposition, that in the dependency pair setting greatly increases
modularity. We employ the framework in the automated complexity tool TCT. TCT
implements a majority of the techniques found in the literature, witnessing
that our framework is general enough to capture a very brought setting."
"Bauwens, Mahklin, Vereshchagin and Zimand [ECCC TR13-007] and Teutsch
[arxiv:1212.6104] have shown that given a string x it is possible to construct
in polynomial time a list containing a short description of it. We simplify
their technique and present a shorter proof of this result."
"We introduce the polynomial coefficient matrix and identify maximum rank of
this matrix under variable substitution as a complexity measure for
multivariate polynomials. We use our techniques to prove super-polynomial lower
bounds against several classes of non-multilinear arithmetic circuits. In
particular, we obtain the following results :
  As our main result, we prove that any homogeneous depth-3 circuit for
computing the product of $d$ matrices of dimension $n \times n$ requires
$\Omega(n^{d-1}/2^d)$ size. This improves the lower bounds by Nisan and
Wigderson(1995) when $d=\omega(1)$.
  There is an explicit polynomial on $n$ variables and degree at most
$\frac{n}{2}$ for which any depth-3 circuit $C$ of product dimension at most
$\frac{n}{10}$ (dimension of the space of affine forms feeding into each
product gate) requires size $2^{\Omega(n)}$. This generalizes the lower bounds
against diagonal circuits proved by Saxena(2007). Diagonal circuits are of
product dimension 1.
  We prove a $n^{\Omega(\log n)}$ lower bound on the size of product-sparse
formulas. By definition, any multilinear formula is a product-sparse formula.
Thus, our result extends the known super-polynomial lower bounds on the size of
multilinear formulas by Raz(2006).
  We prove a $2^{\Omega(n)}$ lower bound on the size of partitioned arithmetic
branching programs. This result extends the known exponential lower bound on
the size of ordered arithmetic branching programs given by Jansen(2008)."
"Sensor network localization problem is to determine the position of the
sensor nodes in a network given pairwise distance measurements. Such problem
can be formulated as a polynomial minimization via the least squares method.
This paper presents a canonical duality theory for solving this challenging
problem. It is shown that the nonconvex minimization problem can be
reformulated as a concave maximization dual problem over a convex set in a
symmetrical matrix space, and hence can be solved efficiently by combining a
general (linear or quadratic) perturbation technique with existing optimization
techniques. Applications are illustrated by solving some relatively large-scale
problems. Our results show that the general sensor network localization problem
is not NP-hard unless its canonical dual problem has no solution."
"We completely characterise the complexity in the decision tree model of
computing composite relations of the form h = g(f^1,...,f^n), where each
relation f^i is boolean-valued. Immediate corollaries include a direct sum
theorem for decision tree complexity and a tight characterisation of the
decision tree complexity of iterated boolean functions."
"For a function $f$ over the discrete cube, the total $L_1$ influence of $f$
is defined as $\sum_{i=1}^n \|\partial_i f\|_1$, where $\partial_i f$ denotes
the discrete derivative of $f$ in the direction $i$. In this work, we show that
the total $L_1$ influence of a $[-1,1]$-valued function $f$ can be upper
bounded by a polynomial in the degree of $f$, resolving affirmatively an open
problem of Aaronson and Ambainis (ITCS 2011). The main challenge here is that
the $L_1$ influences do not admit an easy Fourier analytic representation. In
our proof, we overcome this problem by introducing a new analytic quantity
$\mathcal I_p(f)$, relating this new quantity to the total $L_1$ influence of
$f$. This new quantity, which roughly corresponds to an average of the total
$L_1$ influences of some ensemble of functions related to $f$, has the benefit
of being much easier to analyze, allowing us to resolve the problem of Aaronson
and Ambainis. We also give an application of the theorem to graph theory, and
discuss the connection between the study of bounded functions over the cube and
the quantum query complexity of partial functions where Aaronson and Ambainis
encountered this question."
"Given any fixed integer $q\ge 2$, a $q$-monomial is of the format
$\displaystyle x^{s_1}_{i_1}x^{s_2}_{i_2}...x_{i_t}^{s_t}$ such that $1\le s_j
\le q-1$, $1\le j \le t$. $q$-monomials are natural generalizations of
multilinear monomials. Recent research on testing multilinear monomials and
$q$-monomails for prime $q$ in multivariate polynomials relies on the property
that $Z_q$ is a field when $q\ge 2 $ is prime. When $q>2$ is not prime, it
remains open whether the problem of testing $q$-monomials can be solved in some
compatible complexity. In this paper, we present a randomized $O^*(7.15^k)$
algorithm for testing $q$-monomials of degree $k$ that are found in a
multivariate polynomial that is represented by a tree-like circuit with a
polynomial size, thus giving a positive, affirming answer to the above
question. Our algorithm works regardless of the primality of $q$ and improves
upon the time complexity of the previously known algorithm for testing
$q$-monomials for prime $q>7$."
"The $\epsilon$-approximate degree of a Boolean function $f: \{-1, 1\}^n \to
\{-1, 1\}$ is the minimum degree of a real polynomial that approximates $f$ to
within $\epsilon$ in the $\ell_\infty$ norm. We prove several lower bounds on
this important complexity measure by explicitly constructing solutions to the
dual of an appropriate linear program. Our first result resolves the
$\epsilon$-approximate degree of the two-level AND-OR tree for any constant
$\epsilon > 0$. We show that this quantity is $\Theta(\sqrt{n})$, closing a
line of incrementally larger lower bounds. The same lower bound was recently
obtained independently by Sherstov using related techniques. Our second result
gives an explicit dual polynomial that witnesses a tight lower bound for the
approximate degree of any symmetric Boolean function, addressing a question of
\v{S}palek. Our final contribution is to reprove several Markov-type
inequalities from approximation theory by constructing explicit dual solutions
to natural linear programs. These inequalities underly the proofs of many of
the best-known approximate degree lower bounds, and have important uses
throughout theoretical computer science."
"We study the (non-uniform) quantified constraint satisfaction problem QCSP(H)
as H ranges over partially reflexive cycles. We obtain a complexity-theoretic
dichotomy: QCSP(H) is either in NL or is NP-hard. The separating conditions are
somewhat esoteric hence the epithet ""wavy line of tractability""."
"In this paper, we devise two algorithms for the problem of testing
$q$-monomials of degree $k$ in any multivariate polynomial represented by a
circuit, regardless of the primality of $q$. One is an $O^*(2^k)$ time
randomized algorithm. The other is an $O^*(12.8^k)$ time deterministic
algorithm for the same $q$-monomial testing problem but requiring the
polynomials to be represented by tree-like circuits. Several applications of
$q$-monomial testing are also given, including a deterministic $O^*(12.8^{mk})$
upper bound for the $m$-set $k$-packing problem."
"A unary constraint (on the Boolean domain) is a function from {0,1} to the
set of real numbers. A free use of auxiliary unary constraints given besides
input instances has proven to be useful in establishing a complete
classification of the computational complexity of approximately solving
weighted counting Boolean constraint satisfaction problems (or #CSPs). In
particular, two special constant unary constraints are a key to an arity
reduction of arbitrary constraints, sufficient for the desired classification.
In an exact counting model, both constant unary constraints are always assumed
to be available since they can be eliminated efficiently using an arbitrary
nonempty set of constraints. In contrast, we demonstrate in an approximate
counting model, that at least one of them is efficiently approximated and thus
eliminated approximately by a nonempty constraint set. This fact directly leads
to an efficient construction of polynomial-time randomized
approximation-preserving Turing reductions (or AP-reductions) from #CSPs with
designated constraints to any given #CSPs composed of symmetric real-valued
constraints of arbitrary arities even in the presence of arbitrary extra unary
constraints."
"We extend the well known characterization of $\vpws$ as the class of
polynomials computed by polynomial size arithmetic branching programs to other
complexity classes. In order to do so we add additional memory to the
computation of branching programs to make them more expressive. We show that
allowing different types of memory in branching programs increases the
computational power even for constant width programs. In particular, this leads
to very natural and robust characterizations of $\vp$ and $\vnp$ by branching
programs with memory."
"We show that the affirmation $P\subseteq NP$ (in computer science)
erroneously and we prove the justice of the hypotesis J.Edmonds's $P\neq NP$.
We show further that all the $NP$-complete problems is not polynomial and we
give the classification of the problems with the polynomial certificates."
"We say that a graph with $n$ vertices is $c$-Ramsey if it does not contain
either a clique or an independent set of size $c \log n$. We define a CNF
formula which expresses this property for a graph $G$. We show a
superpolynomial lower bound on the length of resolution proofs that $G$ is
$c$-Ramsey, for every graph $G$. Our proof makes use of the fact that every
Ramsey graph must contain a large subgraph with some of the statistical
properties of the random graph."
"We consider the problem of Data Flow Analysis over monotone data flow
frameworks with a finite lattice. The problem of computing the Maximum Fixed
Point (MFP) solution is shown to be P-complete even when the lattice has just
four elements. This shows that the problem is unlikely to be efficiently
parallelizable. It is also shown that the problem of computing the Meet Over
all Paths (MOP) solution is NL-complete (and hence efficiently parallelizable)
when the lattice is finite even for non-monotone data flow frameworks. These
results appear in contrast with the fact that when the lattice is not finite,
solving the MOP problem is undecidable and hence significantly harder than the
MFP problem which is polynomial time computable for lattices of finite height."
"We define a class of functions termed ""Computable in the Limit"", based on the
Machine Learning paradigm of ""Identification in the Limit"". A function is
Computable in the Limit if it defines a property P_p of a recursively
enumerable class A of recursively enumerable data sequences S in A, such that
each data sequence S is generated by a total recursive function s that
enumerates . Let the index s represent the data sequence S. The property
P_p(s)=x is computed by a partial recursive function f_p(s,t) such that there
exists a u where f_p(s,u)=x and for all t>=u, f_p(s,t)=x if it converges. Since
the index s is known, this is not an identification problem - instead it is
computing a common property of the sequences in A. We give a Normal Form
Theorem for properties that are Computable in the Limit, similar to Kleene's
Normal Form Theorem. We also give some examples of sets that are Computable in
the Limit, and derive some properties of Canonical and Complexity Bound
Enumerations of classes of total functions, and show that no full enumeration
of all indices of Turing machines TM_i that compute a given total function can
be Computable in the Limit."
"The computational complexity of a problem arising in the context of sparse
optimization is considered, namely, the projection onto the set of $k$-cosparse
vectors w.r.t. some given matrix $\Omeg$. It is shown that this projection
problem is (strongly) \NP-hard, even in the special cases in which the matrix
$\Omeg$ contains only ternary or bipolar coefficients. Interestingly, this is
in contrast to the projection onto the set of $k$-sparse vectors, which is
trivially solved by keeping only the $k$ largest coefficients."
"We study a collection of concepts and theorems that laid the foundation of
matchgate computation. This includes the signature theory of planar matchgates,
and the parallel theory of characters of not necessarily planar matchgates. Our
aim is to present a unified and, whenever possible, simplified account of this
challenging theory. Our results include: (1) A direct proof that Matchgate
Identities (MGI) are necessary and sufficient conditions for matchgate
signatures. This proof is self-contained and does not go through the character
theory. More importantly it rectifies a gap in the existing proof. (2) A proof
that Matchgate Identities already imply the Parity Condition. (3) A simplified
construction of a crossover gadget. This is used in the proof of sufficiency of
MGI for matchgate signatures. This is also used to give a proof of equivalence
between the signature theory and the character theory which permits omittable
nodes. (4) A direct construction of matchgates realizing all
matchgate-realizable symmetric signatures."
"An essential problem in the design of holographic algorithms is to decide
whether the required signatures can be realized by matchgates under a suitable
basis transformation (SRP). For holographic algorithms on domain size 2, [1, 2,
4, 5] have built a systematical theory. In this paper, we reduce SRP on domain
size k>2 to SRP on domain size 2 for holographic algorithms on bases of rank 2.
Furthermore, we generalize the collapse theorem of [3] to domain size k>2."
"Given a boolean n by n matrix A we consider arithmetic circuits for computing
the transformation x->Ax over different semirings. Namely, we study three
circuit models: monotone OR-circuits, monotone SUM-circuits (addition of
non-negative integers), and non-monotone XOR-circuits (addition modulo 2). Our
focus is on \emph{separating} these models in terms of their circuit
complexities. We give three results towards this goal:
  (1) We prove a direct sum type theorem on the monotone complexity of tensor
product matrices. As a corollary, we obtain matrices that admit OR-circuits of
size O(n), but require SUM-circuits of size \Omega(n^{3/2}/\log^2n).
  (2) We construct so-called \emph{k-uniform} matrices that admit XOR-circuits
of size O(n), but require OR-circuits of size \Omega(n^2/\log^2n).
  (3) We consider the task of \emph{rewriting} a given OR-circuit as a
XOR-circuit and prove that any subquadratic-time algorithm for this task
violates the strong exponential time hypothesis."
"In this paper, we prove tight lower bounds on the smallest degree of a
nonzero polynomial in the ideal generated by $MOD_q$ or $\neg MOD_q$ in the
polynomial ring $F_p[x_1, \ldots, x_n]/(x_1^2 = x_1, \ldots, x_n^2 = x_n)$,
$p,q$ are coprime, which is called \emph{immunity} over $F_p$. The immunity of
$MOD_q$ is lower bounded by $\lfloor (n+1)/2 \rfloor$, which is achievable when
$n$ is a multiple of $2q$; the immunity of $\neg MOD_q$ is exactly $\lfloor
(n+q-1)/q \rfloor$ for every $q$ and $n$. Our result improves the previous
bound $\lfloor \frac{n}{2(q-1)} \rfloor$ by Green.
  We observe how immunity over $F_p$ is related to $\acc$ circuit lower bound.
For example, if the immunity of $f$ over $F_p$ is lower bounded by $n/2 -
o(\sqrt{n})$, and $|1_f| = \Omega(2^n)$, then $f$ requires $\acc$ circuit of
exponential size to compute."
"We show that if DTIME[2^O(n)] is not included in DSPACE[2^o(n)], then, for
every set B in PSPACE/poly, all strings x in B of length n can be represented
by a string compressed(x) of length at most log(|B^{=n}|)+O(log n), such that a
polynomial-time algorithm, given compressed(x), can distinguish x from all the
other strings in B^{=n}. Modulo the O(log n) additive term, this achieves the
information-theoretic optimum for string compression. We also observe that
optimal compression is not possible for sets more complex than PSPACE/poly
because for any time-constructible superpolynomial function t, there is a set A
computable in space t(n) such that at least one string x of length n requires
compressed(x) to be of length 2 log(|A^=n|)."
"In this paper we study the two player randomized communication complexity of
the sparse set disjointness and the exists-equal problems and give matching
lower and upper bounds (up to constant factors) for any number of rounds for
both of these problems. In the sparse set disjointness problem, each player
receives a k-subset of [m] and the goal is to determine whether the sets
intersect. For this problem, we give a protocol that communicates a total of
O(k\log^{(r)}k) bits over r rounds and errs with very small probability. Here
we can take r=\log^{*}k to obtain a O(k) total communication \log^{*}k-round
protocol with exponentially small error probability, improving on the O(k)-bits
O(\log k)-round constant error probability protocol of Hastad and Wigderson
from 1997.
  In the exist-equal problem, the players receive vectors x,y\in [t]^n and the
goal is to determine whether there exists a coordinate i such that x_i=y_i.
Namely, the exists-equal problem is the OR of n equality problems. Observe that
exists-equal is an instance of sparse set disjointness with k=n, hence the
protocol above applies here as well, giving an O(n\log^{(r)}n) upper bound. Our
main technical contribution in this paper is a matching lower bound: we show
that when t=\Omega(n), any r-round randomized protocol for the exists-equal
problem with error probability at most 1/3 should have a message of size
\Omega(n\log^{(r)}n). Our lower bound holds even for super-constant r <=
\log^*n, showing that any O(n) bits exists-equal protocol should have \log^*n -
O(1) rounds."
"We study Boolean functions with sparse Fourier coefficients or small spectral
norm, and show their applications to the Log-rank Conjecture for XOR functions
f(x\oplus y) --- a fairly large class of functions including well studied ones
such as Equality and Hamming Distance. The rank of the communication matrix M_f
for such functions is exactly the Fourier sparsity of f. Let d be the F2-degree
of f and D^CC(f) stand for the deterministic communication complexity for
f(x\oplus y). We show that 1. D^CC(f) = O(2^{d^2/2} log^{d-2} ||\hat f||_1). In
particular, the Log-rank conjecture holds for XOR functions with constant
F2-degree. 2. D^CC(f) = O(d ||\hat f||_1) = O(\sqrt{rank(M_f)}\logrank(M_f)).
We obtain our results through a degree-reduction protocol based on a variant of
polynomial rank, and actually conjecture that its communication cost is already
\log^{O(1)}rank(M_f). The above bounds also hold for the parity decision tree
complexity of f, a measure that is no less than the communication complexity
(up to a factor of 2).
  Along the way we also show several structural results about Boolean functions
with small F2-degree or small spectral norm, which could be of independent
interest. For functions f with constant F2-degree: 1) f can be written as the
summation of quasi-polynomially many indicator functions of subspaces with
\pm-signs, improving the previous doubly exponential upper bound by Green and
Sanders; 2) being sparse in Fourier domain is polynomially equivalent to having
a small parity decision tree complexity; 3) f depends only on polylog||\hat
f||_1 linear functions of input variables. For functions f with small spectral
norm: 1) there is an affine subspace with co-dimension O(||\hat f||_1) on which
f is a constant; 2) there is a parity decision tree with depth O(||\hat f||_1
log ||\hat f||_0)."
"A new class UF of problems is introduced, strictly included in the class NP,
which arises in the analysis of the time verifying the intermediate results of
computations. The implications of the introduction of this class are
considered. First of all, we prove that $P\not= NP$ and establish that it needs
to consider the problem ""P vs UF"" instead the problem ""P vs NP"". Also, we
determine the set-theoretical of properties of a one-way functions that used in
cryptology."
"The Fourier Entropy-Influence (FEI) conjecture of Friedgut and Kalai [FK96]
seeks to relate two fundamental measures of Boolean function complexity: it
states that $H[f] \leq C Inf[f]$ holds for every Boolean function $f$, where
$H[f]$ denotes the spectral entropy of $f$, $Inf[f]$ is its total influence,
and $C > 0$ is a universal constant. Despite significant interest in the
conjecture it has only been shown to hold for a few classes of Boolean
functions.
  Our main result is a composition theorem for the FEI conjecture. We show that
if $g_1,...,g_k$ are functions over disjoint sets of variables satisfying the
conjecture, and if the Fourier transform of $F$ taken with respect to the
product distribution with biases $E[g_1],...,E[g_k]$ satisfies the conjecture,
then their composition $F(g_1(x^1),...,g_k(x^k))$ satisfies the conjecture. As
an application we show that the FEI conjecture holds for read-once formulas
over arbitrary gates of bounded arity, extending a recent result [OWZ11] which
proved it for read-once decision trees. Our techniques also yield an explicit
function with the largest known ratio of $C \geq 6.278$ between $H[f]$ and
$Inf[f]$, improving on the previous lower bound of 4.615."
"We prove a negative result on the power of a model of algorithmic
self-assembly for which it has been notoriously difficult to find general
techniques and results. Specifically, we prove that Winfree's abstract Tile
Assembly Model, when restricted to use noncooperative tile binding, is not
intrinsically universal. This stands in stark contrast to the recent result
that, via cooperative binding, the abstract Tile Assembly Model is indeed
intrinsically universal. Noncooperative self-assembly, also known as
""temperature 1"", is where tiles bind to each other if they match on one or more
sides, whereas cooperative binding requires binding on multiple sides. Our
result shows that the change from single- to multi-sided binding qualitatively
improves the kinds of dynamics and behavior that these models of nanoscale
self-assembly are capable of. Our lower bound on simulation power holds in both
two and three dimensions; the latter being quite surprising given that
three-dimensional noncooperative tile assembly systems simulate Turing
machines. On the positive side, we exhibit a three-dimensional noncooperative
self-assembly tile set capable of simulating any two-dimensional noncooperative
self-assembly system.
  Our negative result can be interpreted to mean that Turing universal
algorithmic behavior in self-assembly does not imply the ability to simulate
arbitrary algorithmic self-assembly processes."
"The set disjointness problem is one of the most fundamental and well-studied
problems in communication complexity. In this problem Alice and Bob hold sets
$S, T \subseteq [n]$, respectively, and the goal is to decide if $S \cap T =
\emptyset$. Reductions from set disjointness are a canonical way of proving
lower bounds in data stream algorithms, data structures, and distributed
computation. In these applications, often the set sizes $|S|$ and $|T|$ are
bounded by a value $k$ which is much smaller than $n$. This is referred to as
small set disjointness. A major restriction in the above applications is the
number of rounds that the protocol can make, which, e.g., translates to the
number of passes in streaming applications. A fundamental question is thus in
understanding the round complexity of the small set disjointness problem. For
an essentially equivalent problem, called OR-Equality, Brody et. al showed that
with $r$ rounds of communication, the randomized communication complexity is
$\Omega(k \ilog^r k)$, where$\ilog^r k$ denotes the $r$-th iterated logarithm
function. Unfortunately their result requires the error probability of the
protocol to be $1/k^{\Theta(1)}$. Since na\""ive amplification of the success
probability of a protocol from constant to $1-1/k^{\Theta(1)}$ blows up the
communication by a $\Theta(\log k)$ factor, this destroys their improvements
over the well-known lower bound of $\Omega(k)$ which holds for any number of
rounds. They pose it as an open question to achieve the same $\Omega(k \ilog^r
k)$ lower bound for protocols with constant error probability. We answer this
open question by showing that the $r$-round randomized communication complexity
of ${\sf OREQ}_{n,k}$, and thus also of small set disjointness, with {\it
constant error probability} is $\Omega(k \ilog^r k)$, asymptotically matching
known upper bounds for ${\sf OREQ}_{n,k}$ and small set disjointness."
"A CSP with n variables ranging over a domain of d values can be solved by
brute-force in d^n steps (omitting a polynomial factor). With a more careful
approach, this trivial upper bound can be improved for certain natural
restrictions of the CSP. In this paper we establish theoretical limits to such
improvements, and draw a detailed landscape of the subexponential-time
complexity of CSP.
  We first establish relations between the subexponential-time complexity of
CSP and that of other problems, including CNF-Sat. We exploit this connection
to provide tight characterizations of the subexponential-time complexity of CSP
under common assumptions in complexity theory. For several natural CSP
parameters, we obtain threshold functions that precisely dictate the
subexponential-time complexity of CSP with respect to the parameters under
consideration.
  Our analysis provides fundamental results indicating whether and when one can
significantly improve on the brute-force search approach for solving CSP."
"This article describes about the difference of resolution structure and size
between HornSAT and CNFSAT. We can compute HornSAT by using clauses causality.
Therefore we can compute proof diagram by using Log space reduction. But we
must compute CNFSAT by using clauses correlation. Therefore we cannot compute
proof diagram by using Log space reduction, and reduction of CNFSAT is not
P-Complete."
"We prove two complexity results about the H-index concerned with the Google
scholar merge operation on one's scientific articles. The results show that,
although it is hard to merge one's articles in an optimal way, it is easy to
merge them in such a way that one's H-index increases. This suggests the need
for an alternative scientific performance measure that is resistant to this
type of manipulation."
"We study the complexity of approximating the vertex expansion of graphs $G =
(V,E)$, defined as \[ \Phi^V := \min_{S \subset V} n \cdot \frac{|N(S)|}{|S| |V
\backslash S|}. \]
  We give a simple polynomial-time algorithm for finding a subset with vertex
expansion $O(\sqrt{OPT \log d})$ where $d$ is the maximum degree of the graph.
Our main result is an asymptotically matching lower bound: under the Small Set
Expansion (SSE) hypothesis, it is hard to find a subset with expansion less
than $C\sqrt{OPT \log d}$ for an absolute constant $C$. In particular, this
implies for all constant $\epsilon > 0$, it is SSE-hard to distinguish whether
the vertex expansion $< \epsilon$ or at least an absolute constant. The
analogous threshold for edge expansion is $\sqrt{OPT}$ with no dependence on
the degree; thus our results suggest that vertex expansion is harder to
approximate than edge expansion. In particular, while Cheeger's algorithm can
certify constant edge expansion, it is SSE-hard to certify constant vertex
expansion in graphs.
  Our proof is via a reduction from the {\it Unique Games} instance obtained
from the \SSE hypothesis to the vertex expansion problem. It involves the
definition of a smoother intermediate problem we call {\sf Analytic Vertex
Expansion} which is representative of both the vertex expansion and the
conductance of the graph. Both reductions (from the UGC instance to this
problem and from this problem to vertex expansion) use novel proof ideas."
"Given a computable probability measure P over natural numbers or infinite
binary sequences, there is no method that can produce an arbitrarily large
sample such that all its members are typical of P. This paper also contains
upper bounds on the minimal encoding length of a predicate (over the set of
natural numbers) consistent with another predicate over a finite domain."
"In a previous paper, we provided a formal definition for the concept of
computational irreducibility (CIR), i.e. the fact for a function f from N to N
that it is impossible to compute f(n) without following approximately the same
path than computing successively all the values f(i) from i=1 to n. Our
definition is based on the concept of E Turing machines (for Enumerating Turing
Machines) and on the concept of approximation of E Turing machines for which we
also gave a formal definition. We precise here these definitions through some
modifications intended to improve the robustness of the concept. We introduce
then a new concept: the Computational Analogy and prove some properties of
computationally analog functions. Computational Analogy is an equivalence
relation which allows partitioning the set of computable functions in classes
whose members have the same properties regarding to their computational
irreducibility and their computational complexity."
"We consider logic-based argumentation in which an argument is a pair (Fi,al),
where the support Fi is a minimal consistent set of formulae taken from a given
knowledge base (usually denoted by De) that entails the claim al (a formula).
We study the complexity of three central problems in argumentation: the
existence of a support Fi ss De, the validity of a support and the relevance
problem (given psi is there a support Fi such that psi ss Fi?). When arguments
are given in the full language of propositional logic these problems are
computationally costly tasks, the validity problem is DP-complete, the others
are SigP2-complete. We study these problems in Schaefer's famous framework
where the considered propositional formulae are in generalized conjunctive
normal form. This means that formulae are conjunctions of constraints build
upon a fixed finite set of Boolean relations Ga (the constraint language). We
show that according to the properties of this language Ga, deciding whether
there exists a support for a claim in a given knowledge base is either
polynomial, NP-complete, coNP-complete or SigP2-complete. We present a
dichotomous classification, P or DP-complete, for the verification problem and
a trichotomous classification for the relevance problem into either polynomial,
NP-complete, or SigP2-complete. These last two classifications are obtained by
means of algebraic tools."
"The following two decision problems capture the complexity of comparing
integers or rationals that are succinctly represented in
product-of-exponentials notation, or equivalently, via arithmetic circuits
using only multiplication and division gates, and integer inputs:
  Input instance: four lists of positive integers: a_1, ...., a_n ; b_1,....,
b_n ; c_1,....,c_m ; d_1, ...., d_m ; where each of the integers is represented
in binary.
  Problem 1 (equality testing): Decide whether a_1^{b_1} a_2^{b_2} ....
a_n^{b_n} = c_1^{d_1} c_2^{d_2} .... c_m^{d_m} .
  Problem 2 (inequality testing): Decide whether a_1^{b_1} a_2^{b_2} ...
a_n^{b_n} >= c_1^{d_1} c_2^{d_2} .... c_m^{d_m} .
  Problem 1 is easily decidable in polynomial time using a simple iterative
algorithm. Problem 2 is much harder. We observe that the complexity of Problem
2 is intimately connected to deep conjectures and results in number theory. In
particular, if a refined form of the ABC conjecture formulated by Baker in 1998
holds, or if the older Lang-Waldschmidt conjecture (formulated in 1978) on
linear forms in logarithms holds, then Problem 2 is decidable in P-time (in the
standard Turing model of computation). Moreover, it follows from the best
available quantitative bounds on linear forms in logarithms, e.g., by Baker and
W\""{u}stholz (1993) or Matveev (2000), that if m and n are fixed universal
constants then Problem 2 is decidable in P-time (without relying on any
conjectures). This latter fact was observed earlier by Shub (1993).
  We describe one application: P-time maximum probability parsing for arbitrary
stochastic context-free grammars (where \epsilon-rules are allowed)."
"So far, following the works of A.M. Turing, the algorithms were considered as
the mathematical abstraction from which we could write programs for computers
whose principle was based on the theoretical concept of Turing machine. We
start here from the observation that natural algorithms or rather algorithms of
the nature which are massively parallel, autoadaptative and reproductible, and
for which we do not know how they really work, nor why, are not easily
specified by the current theoretical model of Universal Turing machine, or
Universal Computer. In particular the aspects of communications, evolutionary
rules (rulers), random (unpredictable) events, just like the genetic code, are
taken into account only by subtleties which oblige to break the theory. We
shall propose one \textit{universal model} of algorithm called machine-alpha
which contains and generalizes the existing models. --- Jusqu'ici, suite aux
travaux de A.M.Turing [Turing, 1936], les algorithmes ont \'et\'e vus comme
l'abstraction \`a partir de laquelle on pouvait \'ecrire des programmes pour
des ordinateurs dont le principe \'etait lui-m\^eme issu du concept th\'eorique
de machine de Turing. Nous partons ici du constat que les algorithmes naturels
ou plut\^ot les algorithmes de la nature, massivement parall\`eles,
autoadaptatifs et auto reproductibles, dont on ne sait pas comment ils
fonctionnent r\'eellement, ni pourquoi, ne sont pas ais\'ement sp\'ecifi\'es
par le mod\`ele th\'eorique actuel de Machine de Turing Universelle, ou de
Calculateur Universel ; en particulier les aspects de communications, de
r\`egles \'evolutives, d' \'ev\'enements al\'eatoires, \`a l'image du code
g\'en\'etique, ne sont pris en compte que par ajout d'artifices \`a la
th\'eorie. Nous nous proposons ici de montrer comment aborder ces probl\`emes
en repensant le mod\`ele th\'eorique. Nous proposerons un mod\`ele
d'algorithme, appel\'e ici machine-\alpha qui contient et g\'en\'eralise les
mod\`eles existants."
"Asymptotic notations are heavily used while analysing runtimes of algorithms.
Present paper argues that some of these usages are non trivial, therefore
incurring errors in communication of ideas. After careful reconsidera- tion of
the various existing notations a new notation is proposed. This notation has
similarities with the other heavily used notations like Big-Oh, Big Theta,
while being more accurate when describing the order relationship. It has been
argued that this notation is more suitable for describing algorithm runtime
than Big-Oh."
"Koiran showed that if a $n$-variate polynomial of degree $d$ (with
$d=n^{O(1)}$) is computed by a circuit of size $s$, then it is also computed by
a homogeneous circuit of depth four and of size
$2^{O(\sqrt{d}\log(d)\log(s))}$. Using this result, Gupta, Kamath, Kayal and
Saptharishi gave a $\exp(O(\sqrt{d\log(d)\log(n)\log(s)}))$ upper bound for the
size of the smallest depth three circuit computing a $n$-variate polynomial of
degree $d=n^{O(1)}$ given by a circuit of size $s$.
  We improve here Koiran's bound. Indeed, we show that if we reduce an
arithmetic circuit to depth four, then the size becomes
$\exp(O(\sqrt{d\log(ds)\log(n)}))$. Mimicking Gupta, Kamath, Kayal and
Saptharishi's proof, it also implies the same upper bound for depth three
circuits.
  This new bound is not far from optimal in the sense that Gupta, Kamath, Kayal
and Saptharishi also showed a $2^{\Omega(\sqrt{d})}$ lower bound for the size
of homogeneous depth four circuits such that gates at the bottom have fan-in at
most $\sqrt{d}$. Finally, we show that this last lower bound also holds if the
fan-in is at least $\sqrt{d}$."
"Assuming the Generalised Riemann Hypothesis (GRH), we show that for all k,
there exist polynomials with coefficients in $\MA$ having no arithmetic
circuits of size O(n^k) over the complex field (allowing any complex constant).
We also build a family of polynomials that can be evaluated in AM having no
arithmetic circuits of size O(n^k). Then we investigate the link between
fixed-polynomial size circuit bounds in the Boolean and arithmetic settings. In
characteristic zero, it is proved that $\NP \not\subset \size(n^k)$, or $\MA
\subset \size(n^k)$, or NP=MA imply lower bounds on the circuit size of uniform
polynomials in n variables from the class VNP over the complex field, assuming
GRH. In positive characteristic p, uniform polynomials in VNP have circuits of
fixed-polynomial size if and only if both VP=VNP over F_p and Mod_pP has
circuits of fixed-polynomial size."
"We show that most arithmetic circuit lower bounds and relations between lower
bounds naturally fit into the representation-theoretic framework suggested by
geometric complexity theory (GCT), including: the partial derivatives technique
(Nisan-Wigderson), the results of Razborov and Smolensky on $AC^0[p]$,
multilinear formula and circuit size lower bounds (Raz et al.), the degree
bound (Strassen, Baur-Strassen), the connected components technique (Ben-Or),
depth 3 arithmetic circuit lower bounds over finite fields
(Grigoriev-Karpinski), lower bounds on permanent versus determinant
(Mignon-Ressayre, Landsberg-Manivel-Ressayre), lower bounds on matrix
multiplication (B\""{u}rgisser-Ikenmeyer) (these last two were already known to
fit into GCT), the chasms at depth 3 and 4 (Gupta-Kayal-Kamath-Saptharishi;
Agrawal-Vinay; Koiran), matrix rigidity (Valiant) and others. That is, the
original proofs, with what is often just a little extra work, already provide
representation-theoretic obstructions in the sense of GCT for their respective
lower bounds. This enables us to expose a new viewpoint on GCT, whereby it is a
natural unification and broad generalization of known results. It also shows
that the framework of GCT is at least as powerful as known methods, and gives
many new proofs-of-concept that GCT can indeed provide significant asymptotic
lower bounds. This new viewpoint also opens up the possibility of fruitful
two-way interactions between previous results and the new methods of GCT; we
provide several concrete suggestions of such interactions. For example, the
representation-theoretic viewpoint of GCT naturally provides new properties to
consider in the search for new lower bounds."
"In this paper, we consider several property testing problems and ask how the
query complexity depends on the distance parameter $\eps$. We achieve new lower
bounds in this setting for the problems of testing whether a function is
monotone and testing whether the function has low Fourier degree. For
monotonicity testing, our lower bound matches the recent upper bound of
Chakrabarty and Seshadhri."
"We formalize a combinatorial principle, called the 3XOR principle, due to
Feige, Kim and Ofek (2006), as a family of unsatisfiable propositional formulas
for which refutations of small size in any propositional proof system that
possesses the feasible interpolation property imply an efficient deterministic
refutation algorithm for random 3SAT with n variables and \Omega(n^{1.4})
clauses. Such small size refutations would improve the state-of-the-art (with
respect to the clause density) efficient refutation algorithm, which works only
for \Omega(n^{1.5}) many clauses (Feige and Ofek (2007)).
  We demonstrate polynomial-size refutations of the 3XOR principle in
resolution operating with disjunctions of quadratic equations with small
integer coefficients, denoted R(quad); this is a weak extension of cutting
planes with small coefficients. We show that R(quad) is weakly automatizable
iff R(lin) is weakly automatizable, where R(lin) is similar to R(quad) but with
linear instead of quadratic equations (introduced in Raz and Tzameret (2008)).
This reduces the problem of refuting random 3CNF with n variables and
\Omega(n^{1.4}) clauses to the interpolation problem of R(quad) and to the weak
automatizability of R(lin)."
"Holographic algorithms with matchgates are a novel approach to design
polynomial time computation. It uses Kasteleyn's algorithm for perfect
matchings, and more importantly a holographic reduction . The two fundamental
parameters of a holographic reduction are the domain size $k$ of the underlying
problem, and the basis size $\ell$. A holographic reduction transforms the
computation to matchgates by a linear transformation that maps to (a tensor
product space of) a linear space of dimension $2^{\ell}$. We prove a sharp
basis collapse theorem, that shows that for domain size 3 and 4, all
non-trivial holographic reductions have basis size $\ell$ collapse to 1 and 2
respectively. The main proof techniques are Matchgates Identities, and a Group
Property of matchgates signatures."
"We propose an analytical framework for studying parallel repetition, a basic
product operation for one-round two-player games. In this framework, we
consider a relaxation of the value of a game, $\mathrm{val}_+$, and prove that
for projection games, it is both multiplicative (under parallel repetition) and
a good approximation for the true value.
  These two properties imply a parallel repetition bound as $$
\mathrm{val}(G^{\otimes k}) \approx \mathrm{val}_+(G^{\otimes k}) =
\mathrm{val}_+(G)^{k} \approx \mathrm{val}(G)^{k}. $$
  Using this framework, we can also give a short proof for the NP-hardness of
Label-Cover$(1,\delta)$ for all $\delta>0$, starting from the basic PCP
theorem.
  We prove the following new results:
  - A parallel repetition bound for projection games with small soundness.
Previously, it was not known whether parallel repetition decreases the value of
such games. This result implies stronger inapproximability bounds for Set-Cover
and Label-Cover.
  - An improved bound for few parallel repetitions of projection games, showing
that Raz's counterexample is tight even for a small number of repetitions.
  Our techniques also allow us to bound the value of the direct product of
multiple games, namely, a bound on $\mathrm{val}(G_1\otimes ...\otimes G_k)$
for different projection games $G_1,...,G_k$."
"We study the notion of ""cancellation-free"" circuits. This is a restriction of
linear Boolean circuits (XOR circuits), but can be considered as being
equivalent to previously studied models of computation. The notion was coined
by Boyar and Peralta in a study of heuristics for a particular circuit
minimization problem. They asked how large a gap there can be between the
smallest cancellation-free circuit and the smallest linear circuit. We show
that the difference can be a factor $\Omega(n/\log^{2}n)$. This improves on a
recent result by Sergeev and Gashkov who have studied a similar problem.
Furthermore, our proof holds for circuits of constant depth. We also study the
complexity of computing the Sierpinski matrix using cancellation-free circuits
and give a tight $\Omega(n\log n)$ lower bound."
"The class P is in fact a proper sub-class of NP. We explore topological
properties of the Hamming space 2^[n] where [n]={1, 2,..., n}. With the
developed theory, we show: (i) a theorem that is closely related to Erdos and
Rado's sunflower lemma, and claims a stronger statement in most cases, (ii) a
new approach to prove the exponential monotone circuit complexity of the clique
problem, (iii) NC \ne NP through the impossibility of a Boolean circuit with
poly-log depth to compute cliques, based on the construction of (ii), and (iv)
P \ne NP through the exponential circuit complexity of the clique problem,
based on the construction of (iii). Item (i) leads to the existence of a
sunflower with a small core in certain families of sets, which is not an
obvious consequence of the sunflower lemma. In (iv), we show that any Boolean
circuit computing the clique function CLIQUE_{n,k} (k=n^{1/4}) has a size
exponential in n. Thus, we will separate P/poly from NP also. Razborov and
Rudich showed strong evidence that no natural proof can prove exponential
circuit complexity of a Boolean function. We confirm that the proofs for (iii)
and (iv) are not natural."
"In this paper we give an exponential lower bound for Cunningham's least
recently considered (round-robin) rule as applied to parity games, Markhov
decision processes and linear programs. This improves a recent subexponential
bound of Friedmann for this rule on these problems. The round-robin rule fixes
a cyclical order of the variables and chooses the next pivot variable starting
from the previously chosen variable and proceeding in the given circular order.
It is perhaps the simplest example from the class of history-based pivot rules.
Our results are based on a new lower bound construction for parity games. Due
to the nature of the construction we are also able to obtain an exponential
lower bound for the round-robin rule applied to acyclic unique sink
orientations of hypercubes (AUSOs). Furthermore these AUSOs are realizable as
polytopes. We believe these are the first such results for history based rules
for AUSOs, realizable or not. The paper is self-contained and requires no
previous knowledge of parity games."
"This paper demonstrates the relativity of Computability and Nondeterministic;
the nondeterministic is just Turing's undecidable Decision rather than the
Nondeterministic Polynomial time.
  Based on analysis about TM, UM, DTM, NTM, Turing Reducible, beta-reduction,
P-reducible, isomorph, tautology, semi-decidable, checking relation, the oracle
and NP-completeness, etc., it reinterprets The Church-Turing Thesis that is
equivalent of the Polynomial time and actual time; it redefines the NTM based
on its undecidable set of its internal state. It comes to the conclusions: The
P-reducible is misdirected from the Turing Reducible with its oracle; The
NP-completeness is a reversal to The Church-Turing Thesis; The Cook-Levin
theorem is an equipollent of two uncertains. This paper brings forth new
concepts: NP (nondeterministic problem) and NP-algorithm (defined as the
optimal algorithm to get the best fit approximation value of NP). P versus NP
is the relativity of Computability and Nondeterministic, P/=NP. The
NP-algorithm is effective approximate way to NP by TM."
"Obtaining a non-trivial (super-linear) lower bound for computation of the
Fourier transform in the linear circuit model has been a long standing open
problem. All lower bounds so far have made strong restrictions on the
computational model. One of the most well known results, by Morgenstern from
1973, provides an $\Omega(n \log n)$ lower bound for the \emph{unnormalized}
FFT when the constants used in the computation are bounded. The proof uses a
potential function related to a determinant. The determinant of the
unnormalized Fourier transform is $n^{n/2}$, and thus by showing that it can
grow by at most a constant factor after each step yields the result.
  This classic result, however, does not explain why the \emph{normalized}
Fourier transform, which has a unit determinant, should take $\Omega(n\log n)$
steps to compute. In this work we show that in a layered linear circuit model
restricted to unitary $2\times 2$ gates, one obtains an $\Omega(n\log n)$ lower
bound. The well known FFT works in this model. The main argument concluded from
this work is that a potential function that might eventually help proving the
$\Omega(n\log n)$ conjectured lower bound for computation of Fourier transform
is not related to matrix determinant, but rather to a notion of matrix entropy."
"The polynomial hierarchy is a grading of problems by difficulty, including P,
NP and coNP as the best known classes. The promise polynomial hierarchy is
similar, but extended to include promise problems. It turns out that the
promise polynomial hierarchy is considerably simpler to work with, and many
open questions about the polynomial hierarchy can be resolved in the promise
polynomial hierarchy.
  Our main theorem is that, in the world of promise problems, if phi has a weak
(Turing, Cook) reduction to SAT then phi has a strong (Karp, many-one)
reduction to UVAL2, where UVAL2(f) is the promise problem of finding the unique
x such that f(x,y)=1 for all y. We also give a complete promise problem for the
promise problem equivalent of UP intersect coUP."
"In the study of random access machines (RAMs) it has been shown that the
availability of an extra input integer, having no special properties other than
being sufficiently large, is enough to reduce the computational complexity of
some problems. However, this has only been shown so far for specific problems.
We provide a characterization of the power of such extra inputs for general
problems. To do so, we first correct a classical result by Simon and Szegedy
(1992) as well as one by Simon (1981). In the former we show mistakes in the
proof and correct these by an entirely new construction, with no great change
to the results. In the latter, the original proof direction stands with only
minor modifications, but the new results are far stronger than those of Simon
(1981). In both cases, the new constructions provide the theoretical tools
required to characterize the power of arbitrary large numbers."
"We study two computational problems, parameterised by a fixed tree H.
#HomsTo(H) is the problem of counting homomorphisms from an input graph G to H.
#WHomsTo(H) is the problem of counting weighted homomorphisms to H, given an
input graph G and a weight function for each vertex v of G. Even though H is a
tree, these problems turn out to be sufficiently rich to capture all of the
known approximation behaviour in #P. We give a complete trichotomy for
#WHomsTo(H). If H is a star then #WHomsTo(H) is in FP. If H is not a star but
it does not contain a certain induced subgraph J_3 then #WHomsTo(H) is
equivalent under approximation-preserving (AP) reductions to #BIS, the problem
of counting independent sets in a bipartite graph. This problem is complete for
the class #RHPi_1 under AP-reductions. Finally, if H contains an induced J_3
then #WHomsTo(H) is equivalent under AP-reductions to #SAT, the problem of
counting satisfying assignments to a CNF Boolean formula. Thus, #WHomsTo(H) is
complete for #P under AP-reductions. The results are similar for #HomsTo(H)
except that a rich structure emerges if H contains an induced J_3. We show that
there are trees H for which #HomsTo(H) is #SAT-equivalent (disproving a
plausible conjecture of Kelk). There is an interesting connection between these
homomorphism-counting problems and the problem of approximating the partition
function of the ferromagnetic Potts model. In particular, we show that for a
family of graphs J_q, parameterised by a positive integer q, the problem
#HomsTo(H) is AP-interreducible with the problem of approximating the partition
function of the q-state Potts model. It was not previously known that the Potts
model had a homomorphism-counting interpretation. We use this connection to
obtain some additional upper bounds for the approximation complexity of
#HomsTo(J_q)."
"We are considering RAMs $N_{n}$, with wordlength $n=2^{d}$, whose arithmetic
instructions are the arithmetic operations multiplication and addition modulo
$2^{n}$, the unary function $ \min\lbrace 2^{x}, 2^{n}-1\rbrace$, the binary
functions $\lfloor x/y\rfloor $ (with $\lfloor x/0 \rfloor =0$), $\max(x,y)$,
$\min(x,y)$, and the boolean vector operations $\wedge,\vee,\neg$ defined on
$0,1$ sequences of length $n$. It also has the other RAM instructions. The size
of the memory is restricted only by the address space, that is, it is $2^{n}$
words. The RAMs has a finite instruction set, each instruction is encoded by a
fixed natural number independently of $n$. Therefore a program $P$ can run on
each machine $N_{n}$, if $n=2^{d}$ is sufficiently large. We show that there
exists an $\epsilon>0$ and a program $P$, such that it satisfies the following
two conditions.
  (i) For all sufficiently large $n=2^{d}$, if $P$ running on $N_{n}$ gets an
input consisting of two words $a$ and $b$, then, in constant time, it gives a
$0,1$ output $P_{n}(a,b)$.
  (ii) Suppose that $Q$ is a program such that for each sufficiently large
$n=2^{d}$, if $Q$, running on $N_{n}$, gets a word $a$ of length $n$ as an
input, then it decides whether there exists a word $b$ of length $n$ such that
$P_{n}(a,b)=0$. Then, for infinitely many positive integers $d$, there exists a
word $a$ of length $n=2^{d}$, such that the running time of $Q$ on $N_{n}$ at
input $a$ is at least $\epsilon (\log d)^{\frac{1}{2}} (\log \log d)^{-1}$."
"Block sensitivity ($bs(f)$), certificate complexity ($C(f)$) and fractional
certificate complexity ($C^*(f)$) are three fundamental combinatorial measures
of complexity of a boolean function $f$. It has long been known that $bs(f)
\leq C^{\ast}(f) \leq C(f) =O(bs(f)^2)$. We provide an infinite family of
examples for which $C(f)$ grows quadratically in $C^{\ast}(f)$ (and also
$bs(f)$) giving optimal separations between these measures. Previously the
biggest separation known was $C(f)=C^{\ast}(f)^{\log_{4.5}5}$. We also give a
family of examples for which $C^{\ast}(f)=\Omega(bs(f)^{3/2})$.
  These examples are obtained by composing boolean functions in various ways.
Here the composition $f \circ g$ of $f$ with $g$ is obtained by substituting
for each variable of $f$ a copy of $g$ on disjoint sets of variables. To
construct and analyse these examples we systematically investigate the
behaviour under function composition of these measures and also the sensitivity
measure $s(f)$. The measures $s(f)$, $C(f)$ and $C^{\ast}(f)$ behave nicely
under composition: they are submultiplicative (where measure $m$ is
submultiplicative if $m(f \circ g) \leq m(f)m(g)$) with equality holding under
some fairly general conditions. The measure $bs(f)$ is qualitatively different:
it is not submultiplicative. This qualitative difference was not noticed in the
previous literature and we correct some errors that appeared in previous
papers. We define the composition limit of a measure $m$ at function $f$,
$m^{\lim}(f)$ to be the limit as $k$ grows of $m(f^{(k)})^{1/k}$, where
$f^{(k)}$ is the iterated composition of $f$ with itself $k$-times. For any
function $f$ we show that $bs^{\lim}(f) = (C^*)^{\lim}(f)$ and characterize
$s^{\lim}(f), (C^*)^{\lim}(f)$, and $C^{\lim}(f)$ in terms of the largest
eigenvalue of a certain set of $2\times 2$ matrices associated with $f$."
"Let $\cal{P}$ be an affine invariant property of functions $\mathbb{F}_p^n
\to [R]$ for fixed $p$ and $R$. We show that if $\cal{P}$ is locally testable
with a constant number of queries, then one can estimate the distance of a
function $f$ from $\cal{P}$ with a constant number of queries. This was
previously unknown even for simple properties such as cubic polynomials over
$\mathbb{F}_2$.
  Our test is simple: take a restriction of $f$ to a constant dimensional
affine subspace, and measure its distance from $\cal{P}$. We show that by
choosing the dimension large enough, this approximates with high probability
the global distance of $f$ from $\cP$. The analysis combines the approach of
Fischer and Newman [SIAM J. Comp 2007] who established a similar result for
graph properties, with recently developed tools in higher order Fourier
analysis, in particular those developed in Bhattacharyya et al. [STOC 2013]."
"For a property $P$ and a sub-property $P'$, we say that $P$ is $P'$-partially
testable with $q$ queries if there exists an algorithm that distinguishes, with
high probability, inputs in $P'$ from inputs $\epsilon$-far from $P$ by using
$q$ queries. There are natural properties that require many queries to test,
but can be partitioned into a small number of subsets for which they are
partially testable with very few queries.
  We prove the existence of a property $P$ such that the only subsets $P'$ for
which $P$ is $P'$-partially testable are very small. To prove this we introduce
new techniques for proving property testing lower bounds. In addition to
obtaining some broad-brush criteria for non-testability, this implies a lower
bound on the possibility of PCPPs with a sublinear proof. This also implies
lower bounds on MAPs, a notion newly defined by Gur and Rothblum.
  The new techniques rely on analyzing a proposed partial tester. We show that
the queries performed by a tester must, with high probability, query indexes
where a uniformly random member of the sub-property has low entropy. We then
show how one can aggregate the ""entropy loss"" to deduce that a random choice in
the sub-property must have low entropy, and therefore the sub-property must be
small.
  We develop two techniques for aggregating the entropy loss. A simpler
technique that applies to non-adaptive testers is based on partitioning the
input bits into high query probability parts and parts where there is an
entropy loss when conditioned on the high probability parts. Against adaptive
testers we develop a technique based on constructing a decision tree. The
root-to-leaf paths in this tree rearrange the input into parts where each
exhibits entropy loss when conditioned on the path prefix. This decision tree
is constructed by combining carefully selected decision trees from those used
by the adaptive testing algorithm."
"The well known Boole-Shannon expansion of Boolean functions in several
variables (with co-efficients in a Boolean algebra $B$) is also known in more
general form in terms of expansion in a set $\Phi$ of orthonormal functions.
However, unlike the one variable step of this expansion an analogous
elimination theorem and consistency is not well known. This article proves such
an elimination theorem for a special class of Boolean functions denoted
$B(\Phi)$. When the orthonormal set $\Phi$ is of polynomial size in number $n$
of variables, the consistency of a Boolean equation $f=0$ can be determined in
polynomial number of $B$-operations. A characterization of $B(\Phi)$ is also
shown and an elimination based procedure for computing consistency of Boolean
equations is proposed."
"We present an explicit pseudorandom generator for oblivious, read-once,
permutation branching programs of constant width that can read their input bits
in any order. The seed length is $O(\log^2 n)$, where $n$ is the length of the
branching program. The previous best seed length known for this model was
$n^{1/2+o(1)}$, which follows as a special case of a generator due to
Impagliazzo, Meka, and Zuckerman (FOCS 2012) (which gives a seed length of
$s^{1/2+o(1)}$ for arbitrary branching programs of size $s$). Our techniques
also give seed length $n^{1/2+o(1)}$ for general oblivious, read-once branching
programs of width $2^{n^{o(1)}}$, which is incomparable to the results of
Impagliazzo et al.Our pseudorandom generator is similar to the one used by
Gopalan et al. (FOCS 2012) for read-once CNFs, but the analysis is quite
different; ours is based on Fourier analysis of branching programs. In
particular, we show that an oblivious, read-once, regular branching program of
width $w$ has Fourier mass at most $(2w^2)^k$ at level $k$, independent of the
length of the program."
"Patterned self-assembly tile set synthesis PATS is the problem of finding a
minimal tile set which uniquely self-assembles into a given pattern. Czeizler
and Popa proved the NP-completeness of PATS and Seki showed that the PATS
problem is already NP-complete for patterns with 60 colors. In search for the
minimal number of colors such that PATS remains NP-complete, we introduce
multiple bound PATS (mbPATS) where we allow bounds for the numbers of tile
types of each color. We show that mbPATS is NP-complete for patterns with just
three colors and, as a byproduct of this result, we also obtain a novel proof
for the NP-completeness of PATS which is more concise than the previous proofs."
"Complexity theory offers a variety of concise computational models for
computing boolean functions - branching programs, circuits, decision trees and
ordered binary decision diagrams to name a few. A natural question that arises
in this context with respect to any such model is this:
  Given a function f:{0,1}^n \to {0,1}, can we compute the optimal complexity
of computing f in the computational model in question? (according to some
desirable measure).
  A critical issue regarding this question is how exactly is f given, since a
more elaborate description of f allows the algorithm to use more computational
resources. Among the possible representations are black-box access to f (such
as in computational learning theory), a representation of f in the desired
computational model or a representation of f in some other model. One might
conjecture that if f is given as its complete truth table (i.e., a list of f's
values on each of its 2^n possible inputs), the most elaborate description
conceivable, then any computational model can be efficiently computed, since
the algorithm computing it can run poly(2^n) time. Several recent studies show
that this is far from the truth - some models have efficient and simple
algorithms that yield the desired result, others are believed to be hard, and
for some models this problem remains open.
  In this thesis we will discuss the computational complexity of this question
regarding several common types of computational models. We shall present
several new hardness results and efficient algorithms, as well as new proofs
and extensions for known theorems, for variants of decision trees, formulas and
branching programs."
"In recent years, finding new satisfiability algorithms for various circuit
classes has been a very active line of research. Despite considerable progress,
we are still far away from a definite answer on which circuit classes allow
fast satisfiability algorithms. This survey takes a (far from exhaustive) look
at some recent satisfiability algorithms for a range of circuit classes and
high- lights common themes. A special focus is given to connections between
satisfiability algorithms and circuit lower bounds. A second focus is on
reductions from satisfiability algorithms to a range of polynomial time
problems, such as matrix multiplication and the Vector Domination Problem."
"Sensitivity \cite{CD82,CDR86} and block sensitivity \cite{Nisan91} are two
important complexity measures of Boolean functions. A longstanding open problem
in decision tree complexity, the ""Sensitivity versus Block Sensitivity""
question, proposed by Nisan and Szegedy \cite{Nisan94} in 1992, is whether
these two complexity measures are polynomially related, i.e., whether
$bs(f)=O(s(f)^{O(1)})$.
  We prove an new upper bound on block sensitivity in terms of sensitivity:
$bs(f) \leq 2^{s(f)-1} s(f)$. Previously, the best upper bound on block
sensitivity was $bs(f) \leq (\frac{e}{\sqrt{2\pi}}) e^{s(f)} \sqrt{s(f)}$ by
Kenyon and Kutin \cite{KK}. We also prove that if $\min\{s_0(f),s_1(f)\}$ is a
constant, then sensitivity and block sensitivity are linearly related, i.e.
$bs(f)=O(s(f))$."
"Given a symmetric D*D matrix M over {0,1,*}, a list M-partition of a graph G
is a partition of G's vertices into D parts associated with the rows of M. The
part of each vertex is chosen from a given list so that no edge of G maps to a
0 in M and no non-edge of G maps to a 1 in M. Many important graph-theoretic
structures can be represented as list M-partitions, such as graph colourings,
split graphs and homogeneous sets and pairs, which arise in the proofs of the
weak and strong perfect graph conjectures. There has been quite a bit of work
on determining for which matrices M computations involving list M-partitions
are tractable. We focus on counting list M-partitions, given a graph G and a
list for each vertex of G. We identify a set of ""tractable"" matrices and give
an algorithm that counts list M-partitions in polynomial time for every (fixed)
matrix M in this set. The algorithm uses data structures such as sparse-dense
partitions and subcube decompositions to reduce each instance to a sequence of
instances in which the lists restrict access to portions of M in which the
interaction of 0s and 1s is controlled. We solve the resulting restricted
instances by converting them into counting constraint satisfaction problems
(#CSPs) which we solve using arc-consistency. For every matrix M for which our
algorithm fails, we show that counting list M-partitions is #P-complete.
Further, we give an explicit characterisation of the dichotomy theorem:
counting list M-partitions is in FP if M has a structure called a
derectangularising sequence; otherwise, counting list M-partitions is #P-hard.
We show that the meta-problem of determining whether a given matrix has a
derectangularising sequence is NP-complete. Finally, we show that lists can be
used to encode cardinality restrictions in M-partitions problems and use this
to give a polynomial-time algorithm for counting homogeneous pairs in graphs."
"Any positive word comprised of random sequence of tokens form a finite
alphabet can be reduced (without change of length) using an appropriate size
Braid group relationships. Surprisingly the Braid relations dramatically reduce
the Kolmogorov Complexity of the original random word and do so in distinct
bands of (rate of change) values with gaps in between. Distribution of these
bands are estimated and empirical statistics collected by actually coding
approximations to the Kolmogorov Complexity (in Mathematica 9.0).
Lempel-Ziv-Welch lossless compression algorithm techniques used to estimate the
distribution for gaped bands. Evidence provided that such distributions of
reduction in Kolmogorov Complexity based upon Braid groups are universal i.e.
they can model more general algebraic structures other than Braid groups."
"T\""uring's argument that there can be no machine computing the diagonal on
the enumeration of the computable sequences is not a demonstration."
"In this paper we study the computational complexity of the (extended) minimum
cost homomorphism problem (Min-Cost-Hom) as a function of a constraint
language, i.e. a set of constraint relations and cost functions that are
allowed to appear in instances. A wide range of natural combinatorial
optimisation problems can be expressed as Min-Cost-Homs and a classification of
their complexity would be highly desirable, both from a direct, applied point
of view as well as from a theoretical perspective.
  Min-Cost-Hom can be understood either as a flexible optimisation version of
the constraint satisfaction problem (CSP) or a restriction of the
(general-valued) valued constraint satisfaction problem (VCSP). Other
optimisation versions of CSPs such as the minimum solution problem (Min-Sol)
and the minimum ones problem (Min-Ones) are special cases of Min-Cost-Hom.
  The study of VCSPs has recently seen remarkable progress. A complete
classification for the complexity of finite-valued languages on arbitrary
finite domains has been obtained Thapper and Zivny [STOC'13]. However,
understanding the complexity of languages that are not finite-valued appears to
be more difficult. Min-Cost-Hom allows us to study problematic languages of
this type without having to deal with with the full generality of the VCSP. A
recent classification for the complexity of three-element Min-Sol, Uppman
[ICALP'13], takes a step in this direction. In this paper we extend this result
considerably by determining the complexity of three-element Min-Cost-Hom."
"Tavenas has recently proved that any n^{O(1)}-variate and degree n polynomial
in VP can be computed by a depth-4 circuit of size 2^{O(\sqrt{n}\log n)}. So to
prove VP not equal to VNP, it is sufficient to show that an explicit polynomial
in VNP of degree n requires 2^{\omega(\sqrt{n}\log n)} size depth-4 circuits.
Soon after Tavenas's result, for two different explicit polynomials, depth-4
circuit size lower bounds of 2^{\Omega(\sqrt{n}\log n)} have been proved Kayal
et al. and Fournier et al. In particular, using combinatorial design Kayal et
al.\ construct an explicit polynomial in VNP that requires depth-4 circuits of
size 2^{\Omega(\sqrt{n}\log n)} and Fournier et al.\ show that iterated matrix
multiplication polynomial (which is in VP) also requires 2^{\Omega(\sqrt{n}\log
n)} size depth-4 circuits.
  In this paper, we identify a simple combinatorial property such that any
polynomial f that satisfies the property would achieve similar circuit size
lower bound for depth-4 circuits. In particular, it does not matter whether f
is in VP or in VNP. As a result, we get a very simple unified lower bound
analysis for the above mentioned polynomials.
  Another goal of this paper is to compare between our current knowledge of
depth-4 circuit size lower bounds and determinantal complexity lower bounds. We
prove the that the determinantal complexity of iterated matrix multiplication
polynomial is \Omega(dn) where d is the number of matrices and n is the
dimension of the matrices. So for d=n, we get that the iterated matrix
multiplication polynomial achieves the current best known lower bounds in both
fronts: depth-4 circuit size and determinantal complexity. To the best of our
knowledge, a \Theta(n) bound for the determinantal complexity for the iterated
matrix multiplication polynomial was known only for constant d>1 by Jansen."
"One can associate to any bivariate polynomial P(X,Y) its Newton polygon. This
is the convex hull of the points (i,j) such that the monomial X^i Y^j appears
in P with a nonzero coefficient. We conjecture that when P is expressed as a
sum of products of sparse polynomials, the number of edges of its Newton
polygon is polynomially bounded in the size of such an expression. We show that
this ""tau-conjecture for Newton polygons,"" even in a weak form, implies that
the permanent polynomial is not computable by polynomial size arithmetic
circuits. We make the same observation for a weak version of an earlier ""real
tau-conjecture."" Finally, we make some progress toward the tau-conjecture for
Newton polygons using recent results from combinatorial geometry."
"The parameterized complexity of a problem is considered ""settled"" once it has
been shown to lie in FPT or to be complete for a class in the W-hierarchy or a
similar parameterized hierarchy. Several natural parameterized problems have,
however, resisted such a classification. At least in some cases, the reason is
that upper and lower bounds for their parameterized space complexity have
recently been obtained that rule out completeness results for parameterized
time classes. In this paper, we make progress in this direction by proving that
the associative generability problem and the longest common subsequence problem
are complete for parameterized space classes. These classes are defined in
terms of different forms of bounded nondeterminism and in terms of simultaneous
time--space bounds. As a technical tool we introduce a ""union operation"" that
translates between problems complete for classical complexity classes and for
W-classes."
"We study the delay (also known as depth) of circuits that simulate finite
automata, showing that only certain growth rates (as a function of the number
$n$ of steps simulated) are possible. A classic result due to Ofman
(rediscovered and popularized by Ladner and Fischer) says that delay $O(\log
n)$ is always sufficient. We show that if the automaton is ""generalized
definite"", then delay O(1) is sufficient, but otherwise delay $\Omega(\log n)$
is necessary; there are no intermediate growth rates. We also consider
""physical"" (rather than ""logical"") delay, whereby we consider the lengths of
wires when inputs and outputs are laid out along a line. In this case, delay
O(n) is clearly always sufficient. We show that if the automaton is ""definite"",
then delay O(1) is sufficient, but otherwise delay $\Omega(n)$ is necessary;
again there are no intermediate growth rates. Inspired by an observation of
Burks, Goldstein and von Neumann concerning the average delay due to carry
propagation in ripple-carry adders, we derive conditions for the average
physical delay to be reduced from O(n) to $O(\log n)$, or to O(1), when the
inputs are independent and uniformly distributed random variables; again there
are no intermediate growth rates. Finally we consider an extension of this last
result to a situation in which the inputs are not independent and uniformly
distributed, but rather are produced by a non-stationary Markov process, and in
which the computation is not performed by a single automaton, but rather by a
sequence of automata acting in alternating directions."
"This work studies the hardness of finding independent sets in hypergraphs
which are either 2-colorable or are almost 2-colorable, i.e. can be 2-colored
after removing a small fraction of vertices and the incident hyperedges. To be
precise, say that a hypergraph is (1-eps)-almost 2-colorable if removing an eps
fraction of its vertices and all hyperedges incident on them makes the
remaining hypergraph 2-colorable. In particular we prove the following results.
  For an arbitrarily small constant gamma > 0, there is a constant xi > 0, such
that, given a 4-uniform hypergraph on n vertices which is (1 - eps)-almost
2-colorable for eps = 2^{-(log n)^xi}, it is quasi-NP-hard to find an
independent set of n/(2^{(log n)^{1-gamma}}) vertices.
  For any constants eps, delta > 0, given as input a 3-uniform hypergraph on
$n$ vertices which is (1-eps)-almost 2-colorable, it is NP-hard to find an
independent set of delta n vertices. Assuming the d-to-1 Games Conjecture the
following holds.
  For any constant delta > 0, given a 2-colorable 3-uniform hypergraph on n
vertices, it is NP-hard to find an independent set of delta n vertices.
  The hardness result on independent set in almost 2-colorable 3-uniform
hypergraphs was earlier known only assuming the Unique Games Conjecture. In
this work we prove the result unconditionally. For independent sets in
2-colorable 3-uniform hypergaphs we prove the first strong hardness result,
albeit assuming the d-to-1 Games Conjecture. Our result on almost 2-colorable
4-uniform hypergraphs gives the first nearly polynomial hardness factor for
independent set in hypergraphs which are (almost) colorable with constantly
many colors. It partially bridges the gap between the previous best lower bound
of poly(log n) and the algorithmic upper bounds of n^{Omega(1)}."
"We give two new characterizations of ($\F_2$-linear) locally testable
error-correcting codes in terms of Cayley graphs over $\F_2^h$:
  \begin{enumerate} \item A locally testable code is equivalent to a Cayley
graph over $\F_2^h$ whose set of generators is significantly larger than $h$
and has no short linear dependencies, but yields a shortest-path metric that
embeds into $\ell_1$ with constant distortion. This extends and gives a
converse to a result of Khot and Naor (2006), which showed that codes with
large dual distance imply Cayley graphs that have no low-distortion embeddings
into $\ell_1$.
  \item A locally testable code is equivalent to a Cayley graph over $\F_2^h$
that has significantly more than $h$ eigenvalues near 1, which have no short
linear dependencies among them and which ""explain"" all of the large
eigenvalues. This extends and gives a converse to a recent construction of
Barak et al. (2012), which showed that locally testable codes imply Cayley
graphs that are small-set expanders but have many large eigenvalues.
\end{enumerate}"
"We study, in the context of algorithmic randomness, the closed amenable
subgroups of the symmetric group $S_\infty$ of a countable set. In this paper
we address this problem by investigating a link between the symmetries
associated with Ramsey Fra\""iss\'e order classes and algorithmic randomness."
"In this paper we consider skew bisubmodular functions as introduced in [9].
We construct a convex extension of a skew bisubmodular function which we call
Lov\'asz extension in correspondence to the submodular case. We use this
extension to show that skew bisubmodular functions given by an oracle can be
minimised in polynomial time."
"The Tree Evaluation Problem was introduced by Cook et al. in 2010 as a
candidate for separating P from L and NL. The most general space lower bounds
known for the Tree Evaluation Problem require a semantic restriction on the
branching programs and use a connection to well-known pebble games to generate
a bottleneck argument. These bounds are met by corresponding upper bounds
generated by natural implementations of optimal pebbling algorithms. In this
paper we extend these ideas to a variety of restricted families of both
deterministic and non-deterministic branching programs, proving tight lower
bounds under these restricted models. We also survey and unify known lower
bounds in our ""pebbling argument"" framework."
"We establish a generic form of hardness amplification for the approximability
of constant-depth Boolean circuits by polynomials. Specifically, we show that
if a Boolean circuit cannot be pointwise approximated by low-degree polynomials
to within constant error in a certain one-sided sense, then an OR of disjoint
copies of that circuit cannot be pointwise approximated even with very high
error. As our main application, we show that for every sequence of degrees
$d(n)$, there is an explicit depth-three circuit $F: \{-1,1\}^n \to \{-1,1\}$
of polynomial-size such that any degree-$d$ polynomial cannot pointwise
approximate $F$ to error better than
$1-\exp\left(-\tilde{\Omega}(nd^{-3/2})\right)$. As a consequence of our main
result, we obtain an $\exp\left(-\tilde{\Omega}(n^{2/5})\right)$ upper bound on
the the discrepancy of a function in AC$^0$, and an
$\exp\left(\tilde{\Omega}(n^{2/5})\right)$ lower bound on the threshold weight
of AC$^0$, improving over the previous best results of
$\exp\left(-\Omega(n^{1/3})\right)$ and $\exp\left(\Omega(n^{1/3})\right)$
respectively.
  Our techniques also yield a new lower bound of
$\Omega\left(n^{1/2}/\log^{(d-2)/2}(n)\right)$ on the approximate degree of the
AND-OR tree of depth $d$, which is tight up to polylogarithmic factors for any
constant $d$, as well as new bounds for read-once DNF formulas. In turn, these
results imply new lower bounds on the communication and circuit complexity of
these classes, and demonstrate strong limitations on existing PAC learning
algorithms."
"We use critical block sensitivity, a new complexity measure introduced by
Huynh and Nordstr\""om (STOC 2012), to study the communication complexity of
search problems. To begin, we give a simple new proof of the following central
result of Huynh and Nordstr\""om: if $S$ is a search problem with critical block
sensitivity $b$, then every randomised two-party protocol solving a certain
two-party lift of $S$ requires $\Omega(b)$ bits of communication. Besides
simplicity, our proof has the advantage of generalising to the multi-party
setting. We combine these results with new critical block sensitivity lower
bounds for Tseitin and Pebbling search problems to obtain the following
applications:
  (1) Monotone Circuit Depth: We exhibit a monotone $n$-variable function in NP
whose monotone circuits require depth $\Omega(n/\log n)$; previously, a bound
of $\Omega(\sqrt{n})$ was known (Raz and Wigderson, JACM 1992). Moreover, we
prove a $\Theta(\sqrt{n})$ monotone depth bound for a function in monotone P.
  (2) Proof Complexity: We prove new rank lower bounds as well as obtain the
first length--space lower bounds for semi-algebraic proof systems, including
Lov\'asz--Schrijver and Lasserre (SOS) systems. In particular, these results
extend and simplify the works of Beame et al. (SICOMP 2007) and Huynh and
Nordstr\""om."
"We show that derandomizing polynomial identity testing over an arbitrary
finite field implies that NEXP does not have polynomial size boolean circuits.
In other words, for any finite field F(q) of size q, $PIT_q\in
NSUBEXP\Rightarrow NEXP\not\subseteq P/poly$, where $PIT_q$ is the polynomial
identity testing problem over F(q), and NSUBEXP is the nondeterministic
subexpoential time class of languages. Our result is in contract to Kabanets
and Impagliazzo's existing theorem that derandomizing the polynomial identity
testing in the integer ring Z implies that NEXP does have polynomial size
boolean circuits or permanent over Z does not have polynomial size arithmetic
circuits."
"We reduce non-deterministic time $T \ge 2^n$ to a 3SAT instance $\phi$ of
quasilinear size $|\phi| = T \cdot \log^{O(1)} T$ such that there is an
explicit circuit $C$ that on input an index $i$ of $\log |\phi|$ bits outputs
the $i$th clause, and each output bit of $C$ depends on $O(1)$ input bits. The
previous best result was $C$ in NC$^1$. Even in the simpler setting of
polynomial size $|\phi| = \poly(T)$ the previous best result was $C$ in AC$^0$.
  More generally, for any time $T \ge n$ and parameter $r \leq n$ we obtain
$\log_2 |\phi| = \max(\log T, n/r) + O(\log n) + O(\log\log T)$ and each output
bit of $C$ is a decision tree of depth $O(\log r)$.
  As an application, we tighten Williams' connection between satisfiability
algorithms and circuit lower bounds (STOC 2010; SIAM J. Comput. 2013)."
"We study the minimum number of constraints needed to formulate random
instances of the maximum stable set problem via linear programs (LPs), in two
distinct models. In the uniform model, the constraints of the LP are not
allowed to depend on the input graph, which should be encoded solely in the
objective function. There we prove a $2^{\Omega(n/ \log n)}$ lower bound with
probability at least $1 - 2^{-2^n}$ for every LP that is exact for a randomly
selected set of instances; each graph on at most n vertices being selected
independently with probability $p \geq 2^{-\binom{n/4}{2}+n}$. In the
non-uniform model, the constraints of the LP may depend on the input graph, but
we allow weights on the vertices. The input graph is sampled according to the
G(n, p) model. There we obtain upper and lower bounds holding with high
probability for various ranges of p. We obtain a super-polynomial lower bound
all the way from $p = \Omega(\log^{6+\varepsilon} / n)$ to $p = o (1 / \log
n)$. Our upper bound is close to this as there is only an essentially quadratic
gap in the exponent, which currently also exists in the worst-case model.
Finally, we state a conjecture that would close this gap, both in the
average-case and worst-case models."
"Let $D$, called the domain, be a fixed finite set and let $\Gamma$, called
the valued constraint language, be a fixed set of functions of the form
$f:D^m\to\mathbb{Q}\cup\{\infty\}$, where different functions might have
different arity $m$. We study the valued constraint satisfaction problem
parametrised by $\Gamma$, denoted by VCSP$(\Gamma)$. These are minimisation
problems given by $n$ variables and the objective function given by a sum of
functions from $\Gamma$, each depending on a subset of the $n$ variables.
Finite-valued constraint languages contain functions that take on only rational
values and not infinite values.
  Our main result is a precise algebraic characterisation of valued constraint
languages whose instances can be solved exactly by the basic linear programming
relaxation (BLP). For a valued constraint language $\Gamma$, BLP is a decision
procedure for $\Gamma$ if and only if $\Gamma$ admits a symmetric fractional
polymorphism of every arity. For a finite-valued constraint language $\Gamma$,
BLP is a decision procedure if and only if $\Gamma$ admits a symmetric
fractional polymorphism of some arity, or equivalently, if $\Gamma$ admits a
symmetric fractional polymorphism of arity 2.
  Using these results, we obtain tractability of several novel classes of
problems, including problems over valued constraint languages that are: (1)
submodular on arbitrary lattices; (2) $k$-submodular on arbitrary finite
domains; (3) weakly (and hence strongly) tree-submodular on arbitrary trees."
"Counting independent sets on bipartite graphs (#BIS) is considered a
canonical counting problem of intermediate approximation complexity. It is
conjectured that #BIS neither has an FPRAS nor is as hard as #SAT to
approximate. We study #BIS in the general framework of two-state spin systems
on bipartite graphs. We define two notions, nearly-independent phase-correlated
spins and unary symmetry breaking. We prove that it is #BIS-hard to approximate
the partition function of any 2-spin system on bipartite graphs supporting
these two notions. As a consequence, we classify the complexity of
approximating the partition function of antiferromagnetic 2-spin systems on
bounded-degree bipartite graphs."
"We draw two incomplete, biased maps of challenges in computational complexity
lower bounds."
"Let $F$ be the field of $q$ elements.
  We investigate the following Ramsey coloring problem for vector spaces: Given
a vector space $\F^n$, give a coloring of the points of $F^n$ with two colors
such that no affine line (i.e., affine subspace of dimension $1$) is
monochromatic. Our main result is as follows:
  For any $q\geq 25\cdot n$ and $n>4$, we give an explicit coloring $D:F^n\ar
\set{0,1}$ such that for every affine line $l\subseteq F^n$, $D(l)=\set{0,1}$.
Previously this was known only for $q\geq c\cdot n^2$ for some constant $c$
\cite{GR05}. We note that this beats the random coloring for which the expected
number of monochromatic lines will be 0 only when $q\geq c\cdot n\log n$ for
some constant $c$. Furthermore, our coloring will be `almost balanced' on every
affine line. Let us state this formally in the lanuage of \emph{extractors}. We
say that a function $D:F^n\mapsto \set{0,1}$ is a \afsext{1}{\eps} if for every
affine line $l\subseteq \F^n$, $D(X)$ is $\eps$-close to uniform when $X$ is
uniformly distributed over $l$. We construct a \afsext{1}{\eps} with $\eps =
\Omega(\sqrt{n/q})$ whenever $q\geq c\cdot n$ for some constant $c$.
  The previous result of \cite{GR05} gave a \afsext{1}{\eps} only for
$q=\Omega(n^2)$."
"The $3x+1$ problem, also called the Collatz conjecture, is a very interesting
unsolved mathematical problem related to computer science. This paper
generalized this problem by relaxing the constraints, i.e., generalizing this
deterministic process to non-deterministic process, and set up three models.
This paper analyzed the ergodicity of these models and proved that the
ergodicity of the Collatz process in positive integer field holds, i.e., all
the positive integers can be transformed to 1 by the iterations of the Collatz
function."
"We give a deterministic polynomial space construction for nearly optimal
eps-nets with respect to any input n-dimensional convex body K and norm |.|.
More precisely, our algorithm can build and iterate over an eps-net of K with
respect to |.| in time 2^O(n) x (size of the optimal net) using only
poly(n)-space. This improves on previous constructions of Alon et al [STOC
2013] which achieve either a 2^O(n) approximation or an n^O(n) approximation of
the optimal net size using 2^n space and poly(n)-space respectively. As in
their work, our algorithm relies on the mathematically classical approach of
building thin lattice coverings of space, which reduces the task of
constructing eps-nets to the problem of enumerating lattice points. Our main
technical contribution is a deterministic 2^O(n)-time and poly(n)-space
construction of thin lattice coverings of space with respect to any convex
body, where enumeration in these lattices can be efficiently performed using
poly(n)-space. This also yields the first existential construction of
poly(n)-space enumerable thin covering lattices for general convex bodies,
which we believe is of independent interest. Our construction combines the use
of the M-ellipsoid from convex geometry with lattice sparsification and
densification techniques.
  As an application, we give a 2^O(n)(1+1/eps)^n time and poly(n)-space
deterministic algorithm for computing a (1+eps)^n approximation to the volume
of a general convex body, which nearly matches the lower bounds for volume
estimation in the oracle model (the dependence on eps is larger by a factor 2
in the exponent). This improves on the previous results of Dadush and Vempala
[PNAS 2013], which gave the above result only for symmetric bodies and achieved
a dependence on eps of (1+log^{5/2}(1/eps)/eps^2)^n."
"In recent years, a very exciting and promising method for proving lower
bounds for arithmetic circuits has been proposed. This method combines the
method of {\it depth reduction} developed in the works of Agrawal-Vinay [AV08],
Koiran [Koi12] and Tavenas [Tav13], and the use of the shifted partial
derivative complexity measure developed in the works of Kayal [Kay12] and Gupta
et al [GKKS13a]. These results inspired a flurry of other beautiful results and
strong lower bounds for various classes of arithmetic circuits, in particular a
recent work of Kayal et al [KSS13] showing superpolynomial lower bounds for
{\it regular} arithmetic formulas via an {\it improved depth reduction} for
these formulas. It was left as an intriguing question if these methods could
prove superpolynomial lower bounds for general (homogeneous) arithmetic
formulas, and if so this would indeed be a breakthrough in arithmetic circuit
complexity.
  In this paper we study the power and limitations of depth reduction and
shifted partial derivatives for arithmetic formulas. We do it via studying the
class of depth 4 homogeneous arithmetic circuits. We show: (1) the first {\it
superpolynomial lower bounds} for the class of homogeneous depth 4 circuits
with top fan-in $o(\log n)$. The core of our result is to show {\it improved
depth reduction} for these circuits. (2) We show that improved depth reduction
{\it is not possible} when the top fan-in is $\Omega(\log n)$. In particular
this shows that the depth reduction procedure of Koiran and Tavenas [Koi12,
Tav13] cannot be improved even for homogeneous formulas, thus strengthening the
results of Fournier et al [FLMS13] who showed that depth reduction is tight for
circuits, and answering some of the main open questions of [KSS13, FLMS13]."
"We study the complexity theory for the local distributed setting introduced
by Korman, Peleg and Fraigniaud. They have defined three complexity classes LD
(Local Decision), NLD (Nondeterministic Local Decision) and NLD^#n. The class
LD consists of all languages which can be decided with a constant number of
communication rounds. The class NLD consists of all languages which can be
verified by a nondeterministic algorithm with a constant number of
communication rounds. In order to define the nondeterministic classes, they
have transferred the notation of nondeterminism into the distributed setting by
the use of certificates and verifiers. The class NLD^#n consists of all
languages which can be verified by a nondeterministic algorithm where each node
has access to an oracle for the number of nodes. They have shown the hierarchy
LD subset NLD subset NLD^#n.
  Our main contributions are strict hierarchies within the classes defined by
Korman, Peleg and Fraigniaud. We define additional complexity classes: the
class LD(t) consists of all languages which can be decided with at most t
communication rounds. The class NLD-O(f) consists of all languages which can be
verified by a local verifier such that the size of the certificates that are
needed to verify the language are bounded by a function from O(f). Our main
results are refined strict hierarchies within these nondeterministic classes."
"A $c$-short program for a string $x$ is a description of $x$ of length at
most $C(x) + c$, where $C(x)$ is the Kolmogorov complexity of $x$. We show that
there exists a randomized algorithm that constructs a list of $n$ elements that
contains a $O(\log n)$-short program for $x$. We also show a polynomial-time
randomized construction that achieves the same list size for $O(\log^2
n)$-short programs. These results beat the lower bounds shown by Bauwens et al.
\cite{bmvz:c:shortlist} for deterministic constructions of such lists. We also
prove tight lower bounds for the main parameters of our result. The
constructions use only $O(\log n)$ ($O(\log^2 n)$ for the polynomial-time
result) random bits . Thus using only few random bits it is possible to do
tasks that cannot be done by any deterministic algorithm regardless of its
running time."
"We prove improved inapproximability results for hypergraph coloring using the
low-degree polynomial code (aka, the 'short code' of Barak et. al. [FOCS 2012])
and the techniques proposed by Dinur and Guruswami [FOCS 2013] to incorporate
this code for inapproximability results. In particular, we prove
quasi-NP-hardness of the following problems on $n$-vertex hyper-graphs:
  * Coloring a 2-colorable 8-uniform hypergraph with
$2^{2^{\Omega(\sqrt{\log\log n})}}$ colors.
  * Coloring a 4-colorable 4-uniform hypergraph with
$2^{2^{\Omega(\sqrt{\log\log n})}}$ colors.
  * Coloring a 3-colorable 3-uniform hypergraph with $(\log
n)^{\Omega(1/\log\log\log n)}$ colors.
  In each of these cases, the hardness results obtained are (at least)
exponentially stronger than what was previously known for the respective cases.
In fact, prior to this result, polylog n colors was the strongest quantitative
bound on the number of colors ruled out by inapproximability results for
O(1)-colorable hypergraphs.
  The fundamental bottleneck in obtaining coloring inapproximability results
using the low- degree long code was a multipartite structural restriction in
the PCP construction of Dinur-Guruswami. We are able to get around this
restriction by simulating the multipartite structure implicitly by querying
just one partition (albeit requiring 8 queries), which yields our result for
2-colorable 8-uniform hypergraphs. The result for 4-colorable 4-uniform
hypergraphs is obtained via a 'query doubling' method. For 3-colorable
3-uniform hypergraphs, we exploit the ternary domain to design a test with an
additive (as opposed to multiplicative) noise function, and analyze its
efficacy in killing high weight Fourier coefficients via the pseudorandom
properties of an associated quadratic form."
"We give the first super-polynomial separation in the power of bounded-depth
boolean formulas vs. circuits. Specifically, we consider the problem Distance
$k(n)$ Connectivity, which asks whether two specified nodes in a graph of size
$n$ are connected by a path of length at most $k(n)$. This problem is solvable
(by the recursive doubling technique) on {\bf circuits} of depth $O(\log k)$
and size $O(kn^3)$. In contrast, we show that solving this problem on {\bf
formulas} of depth $\log n/(\log\log n)^{O(1)}$ requires size $n^{\Omega(\log
k)}$ for all $k(n) \leq \log\log n$. As corollaries:
  (i) It follows that polynomial-size circuits for Distance $k(n)$ Connectivity
require depth $\Omega(\log k)$ for all $k(n) \leq \log\log n$. This matches the
upper bound from recursive doubling and improves a previous $\Omega(\log\log
k)$ lower bound of Beame, Pitassi and Impagliazzo [BIP98].
  (ii) We get a tight lower bound of $s^{\Omega(d)}$ on the size required to
simulate size-$s$ depth-$d$ circuits by depth-$d$ formulas for all $s(n) =
n^{O(1)}$ and $d(n) \leq \log\log\log n$. No lower bound better than
$s^{\Omega(1)}$ was previously known for any $d(n) \nleq O(1)$.
  Our proof technique is centered on a new notion of pathset complexity, which
roughly speaking measures the minimum cost of constructing a set of (partial)
paths in a universe of size $n$ via the operations of union and relational
join, subject to certain density constraints. Half of our proof shows that
bounded-depth formulas solving Distance $k(n)$ Connectivity imply upper bounds
on pathset complexity. The other half is a combinatorial lower bound on pathset
complexity."
"A sumtest for a discrete semimeasure $P$ is a function $f$ mapping bitstrings
to non-negative rational numbers such that \[
  \sum P(x)f(x) \le 1 \,.
  \] Sumtests are the discrete analogue of Martin-L\""of tests. The behavior of
sumtests for computable $P$ seems well understood, but for some applications
lower semicomputable $P$ seem more appropriate. In the case of tests for
independence, it is natural to consider upper semicomputable tests (see
[B.Bauwens and S.Terwijn, Theory of Computing Systems 48.2 (2011): 247-268]).
  In this paper, we characterize upper semicomputable sumtests relative to any
lower semicomputable semimeasures using Kolmogorov complexity. It is studied to
what extend such tests are pathological: can upper semicomputable sumtests for
$m(x)$ be large? It is shown that the logarithm of such tests does not exceed
$\log |x| + O(\log^{(2)} |x|)$ (where $|x|$ denotes the length of $x$ and
$\log^{(2)} = \log\log$) and that this bound is tight, i.e. there is a test
whose logarithm exceeds $\log |x| - O(\log^{(2)} |x|$) infinitely often.
Finally, it is shown that for each such test $e$ the mutual information of a
string with the Halting problem is at least $\log e(x)-O(1)$; thus $e$ can only
be large for ``exotic'' strings."
"The depth-$3$ model has recently gained much importance, as it has become a
stepping-stone to understanding general arithmetic circuits. Its restriction to
multilinearity has known exponential lower bounds but no nontrivial blackbox
identity tests. In this paper we take a step towards designing such
hitting-sets. We define a notion of distance for multilinear depth-$3$ circuits
(say, in $n$ variables and $k$ product gates) that measures how far are the
partitions from a mere refinement. The $1$-distance strictly subsumes the
set-multilinear model, while $n$-distance captures general multilinear
depth-$3$. We design a hitting-set in time poly($n^{\delta\log k}$) for
$\delta$-distance. Further, we give an extension of our result to models where
the distance is large (close to $n$) but it is small when restricted to certain
variables. This implies the first subexponential whitebox PIT for the sum of
constantly many set-multilinear depth-$3$ circuits.
  We also explore a new model of read-once algebraic branching programs (ROABP)
where the factor-matrices are invertible (called invertible-factor ROABP). We
design a hitting-set in time poly($\text{size}^{w^2}$) for width-$w$
invertible-factor ROABP. Further, we could do without the invertibility
restriction when $w=2$. Previously, the best result for width-$2$ ROABP was
quasi-polynomial time (Forbes-Saptharishi-Shpilka, arXiv 2013).
  The common thread in all these results is the phenomenon of low-support `rank
concentration'. We exploit the structure of these models to prove
rank-concentration after a `small shift' in the variables. Our proof techniques
are stronger than the results of Agrawal-Saha-Saxena (STOC 2013) and
Forbes-Saptharishi-Shpilka (arXiv 2013); giving us quasi-polynomial-time
hitting-sets for models where no subexponential whitebox algorithms were known
before."
"In this work, we study the parity complexity measures
${\mathsf{C}^{\oplus}_{\min}}[f]$ and ${\mathsf{DT^{\oplus}}}[f]$.
${\mathsf{C}^{\oplus}_{\min}}[f]$ is the \emph{parity kill number} of $f$, the
fewest number of parities on the input variables one has to fix in order to
""kill"" $f$, i.e. to make it constant. ${\mathsf{DT^{\oplus}}}[f]$ is the depth
of the shortest \emph{parity decision tree} which computes $f$. These
complexity measures have in recent years become increasingly important in the
fields of communication complexity \cite{ZS09, MO09, ZS10, TWXZ13} and
pseudorandomness \cite{BK12, Sha11, CT13}.
  Our main result is a composition theorem for ${\mathsf{C}^{\oplus}_{\min}}$.
The $k$-th power of $f$, denoted $f^{\circ k}$, is the function which results
from composing $f$ with itself $k$ times. We prove that if $f$ is not a parity
function, then ${\mathsf{C}^{\oplus}_{\min}}[f^{\circ k}] \geq
\Omega({\mathsf{C}_{\min}}[f]^{k}).$ In other words, the parity kill number of
$f$ is essentially supermultiplicative in the \emph{normal} kill number of $f$
(also known as the minimum certificate complexity).
  As an application of our composition theorem, we show lower bounds on the
parity complexity measures of $\mathsf{Sort}^{\circ k}$ and $\mathsf{HI}^{\circ
k}$. Here $\mathsf{Sort}$ is the sort function due to Ambainis \cite{Amb06},
and $\mathsf{HI}$ is Kushilevitz's hemi-icosahedron function \cite{NW95}. In
doing so, we disprove a conjecture of Montanaro and Osborne \cite{MO09} which
had applications to communication complexity and computational learning theory.
In addition, we give new lower bounds for conjectures of \cite{MO09,ZS10} and
\cite{TWXZ13}."
"This work revisits the PCP Verifiers used in the works of Hastad [Has01],
Guruswami et al.[GHS02], Holmerin[Hol02] and Guruswami[Gur00] for satisfiable
Max-E3-SAT and Max-Ek-Set-Splitting, and independent set in 2-colorable
4-uniform hypergraphs. We provide simpler and more efficient PCP Verifiers to
prove the following improved hardness results: Assuming that NP\not\subseteq
DTIME(N^{O(loglog N)}),
  There is no polynomial time algorithm that, given an n-vertex 2-colorable
4-uniform hypergraph, finds an independent set of n/(log n)^c vertices, for
some constant c > 0.
  There is no polynomial time algorithm that satisfies 7/8 + 1/(log n)^c
fraction of the clauses of a satisfiable Max-E3-SAT instance of size n, for
some constant c > 0.
  For any fixed k >= 4, there is no polynomial time algorithm that finds a
partition splitting (1 - 2^{-k+1}) + 1/(log n)^c fraction of the k-sets of a
satisfiable Max-Ek-Set-Splitting instance of size n, for some constant c > 0.
  Our hardness factor for independent set in 2-colorable 4-uniform hypergraphs
is an exponential improvement over the previous results of Guruswami et
al.[GHS02] and Holmerin[Hol02]. Similarly, our inapproximability of (log
n)^{-c} beyond the random assignment threshold for Max-E3-SAT and
Max-Ek-Set-Splitting is an exponential improvement over the previous bounds
proved in [Has01], [Hol02] and [Gur00]. The PCP Verifiers used in our results
avoid the use of a variable bias parameter used in previous works, which leads
to the improved hardness thresholds in addition to simplifying the analysis
substantially. Apart from standard techniques from Fourier Analysis, for the
first mentioned result we use a mixing estimate of Markov Chains based on
uniform reverse hypercontractivity over general product spaces from the work of
Mossel et al.[MOS13]."
"Given $f:\{-1, 1\}^n \rightarrow \{-1, 1\}$, define the \emph{spectral
distribution} of $f$ to be the distribution on subsets of $[n]$ in which the
set $S$ is sampled with probability $\widehat{f}(S)^2$. Then the Fourier
Entropy-Influence (FEI) conjecture of Friedgut and Kalai (1996) states that
there is some absolute constant $C$ such that $\operatorname{H}[\widehat{f}^2]
\leq C\cdot\operatorname{Inf}[f]$. Here, $\operatorname{H}[\widehat{f}^2]$
denotes the Shannon entropy of $f$'s spectral distribution, and
$\operatorname{Inf}[f]$ is the total influence of $f$. This conjecture is one
of the major open problems in the analysis of Boolean functions, and settling
it would have several interesting consequences.
  Previous results on the FEI conjecture have been largely through direct
calculation. In this paper we study a natural interpretation of the conjecture,
which states that there exists a communication protocol which, given subset $S$
of $[n]$ distributed as $\widehat{f}^2$, can communicate the value of $S$ using
at most $C\cdot\operatorname{Inf}[f]$ bits in expectation.
  Using this interpretation, we are able show the following results:
  1. First, if $f$ is computable by a read-$k$ decision tree, then
$\operatorname{H}[\widehat{f}^2] \leq 9k\cdot \operatorname{Inf}[f]$.
  2. Next, if $f$ has $\operatorname{Inf}[f] \geq 1$ and is computable by a
decision tree with expected depth $d$, then $\operatorname{H}[\widehat{f}^2]
\leq 12d\cdot \operatorname{Inf}[f]$.
  3. Finally, we give a new proof of the main theorem of O'Donnell and Tan
(ICALP 2013), i.e. that their FEI$^+$ conjecture composes.
  In addition, we show that natural improvements to our decision tree results
would be sufficient to prove the FEI conjecture in its entirety. We believe
that our methods give more illuminating proofs than previous results about the
FEI conjecture."
"We show that if NC$^1 \neq$ L, then for every element $\alpha$ of the
alternating group $A_t$, circuits of depth $O(\log t)$ cannot distinguish
between a uniform vector over $(A_t)^t$ with product $= \alpha$ and one with
product $=$ identity. Combined with a recent construction by the author and
Viola in the setting of leakage-resilient cryptography [STOC '13], this gives a
compiler that produces circuits withstanding leakage from NC$^1$ (assuming
NC$^1 \neq$ L). For context, leakage from NC$^1$ breaks nearly all previous
constructions, and security against leakage from P is impossible. %In the
multi-query setting, circuits produced by this compiler use a simple secure
hardware component.
  We build on work by Cook and McKenzie [J.\ Algorithms '87] establishing the
relationship between L $=$ logarithmic space and the symmetric group $S_t$. Our
techniques include a novel algorithmic use of commutators to manipulate the
cycle structure of permutations in $A_t$."
"Symmetric Datalog, a fragment of the logic programming language Datalog, is
conjectured to capture all constraint satisfaction problems (CSP) in L.
Therefore developing tools that help us understand whether or not a CSP can be
defined in symmetric Datalog is an important task. It is widely known that a
CSP is definable in Datalog and linear Datalog if and only if that CSP has
bounded treewidth and bounded pathwidth duality, respectively. In the case of
symmetric Datalog, Bulatov, Krokhin and Larose ask for such a duality (2008).
We provide two such dualities, and give applications. In particular, we give a
short and simple new proof of the result of Dalmau and Larose that ""Maltsev +
Datalog -> symmetric Datalog"" (2008).
  In the second part of the paper, we provide some evidence for the conjecture
of Dalmau (2002) that every CSP in NL is definable in linear Datalog. Our
results also show that a wide class of CSPs-CSPs which do not have bounded
pathwidth duality (e.g., the P-complete Horn-3Sat problem)-cannot be defined by
any polynomial size family of monotone read-once nondeterministic branching
programs."
"A graph $G$ is 3-colorable if and only if it maps homomorphically to the
complete 3-vertex graph $K_3$. The last condition can be checked by a
$k$-consistency algorithm where the parameter $k$ has to be chosen large
enough, dependent on $G$. Let $W(G)$ denote the minimum $k$ sufficient for this
purpose. For a non-3-colorable graph $G$, $W(G)$ is equal to the minimum $k$
such that $G$ can be distinguished from $K_3$ in the $k$-variable
existential-positive first-order logic. We define the dynamic width of the
3-colorability problem as the function $W(n)=\max_G W(G)$, where the maximum is
taken over all non-3-colorable $G$ with $n$ vertices.
  The assumption $\mathrm{NP}\ne\mathrm{P}$ implies that $W(n)$ is unbounded.
Indeed, a lower bound $W(n)=\Omega(\log\log n/\log\log\log n)$ follows
unconditionally from the work of Nesetril and Zhu on bounded treewidth duality.
The Exponential Time Hypothesis implies a much stronger bound
$W(n)=\Omega(n/\log n)$ and indeed we unconditionally prove that
$W(n)=\Omega(n)$. In fact, an even stronger statement is true: A first-order
sentence distinguishing any 3-colorable graph on $n$ vertices from any
non-3-colorable graph on $n$ vertices must have $\Omega(n)$ variables.
  On the other hand, we observe that $W(G)\le 3\,\alpha(G)+1$ and $W(G)\le
n-\alpha(G)+1$ for every non-3-colorable graph $G$ with $n$ vertices, where
$\alpha(G)$ denotes the independence number of $G$. This implies that
$W(n)\le\frac34\,n+1$, improving on the trivial upper bound $W(n)\le n$.
  We also show that $W(G)>\frac1{16}\, g(G)$ for every non-3-colorable graph
$G$, where $g(G)$ denotes the girth of $G$.
  Finally, we consider the function $W(n)$ over planar graphs and prove that
$W(n)=\Theta(\sqrt n)$ in the case."
"In this paper, we prove superpolynomial lower bounds for the class of
homogeneous depth 4 arithmetic circuits. We give an explicit polynomial in VNP
of degree $n$ in $n^2$ variables such that any homogeneous depth 4 arithmetic
circuit computing it must have size $n^{\Omega(\log \log n)}$.
  Our results extend the works of Nisan-Wigderson [NW95] (which showed
superpolynomial lower bounds for homogeneous depth 3 circuits),
Gupta-Kamath-Kayal-Saptharishi and Kayal-Saha-Saptharishi [GKKS13, KSS13]
(which showed superpolynomial lower bounds for homogeneous depth 4 circuits
with bounded bottom fan-in), Kumar-Saraf [KS13a] (which showed superpolynomial
lower bounds for homogeneous depth 4 circuits with bounded top fan-in) and
Raz-Yehudayoff and Fournier-Limaye-Malod-Srinivasan [RY08, FLMS13] (which
showed superpolynomial lower bounds for multilinear depth 4 circuits). Several
of these results in fact showed exponential lower bounds.
  The main ingredient in our proof is a new complexity measure of {\it bounded
support} shifted partial derivatives. This measure allows us to prove
exponential lower bounds for homogeneous depth 4 circuits where all the
monomials computed at the bottom layer have {\it bounded support} (but possibly
unbounded degree/fan-in), strengthening the results of Gupta et al and Kayal et
al [GKKS13, KSS13]. This new lower bound combined with a careful ""random
restriction"" procedure (that transforms general depth 4 homogeneous circuits to
depth 4 circuits with bounded support) gives us our final result."
"Motivated by the fundamental lower bounds questions in proof complexity, we
initiate the study of matrix identities as hard instances for strong proof
systems. A matrix identity of $d \times d$ matrices over a field $\mathbb{F}$,
is a non-commutative polynomial $f(x_1,\ldots,x_n)$ over $\mathbb{F}$ such that
$f$ vanishes on every $d \times d$ matrix assignment to its variables.
  We focus on arithmetic proofs, which are proofs of polynomial identities
operating with arithmetic circuits and whose axioms are the polynomial-ring
axioms (these proofs serve as an algebraic analogue of the Extended Frege
propositional proof system; and over $GF(2)$ they constitute formally a
sub-system of Extended Frege [HT12]). We introduce a decreasing in strength
hierarchy of proof systems within arithmetic proofs, in which the $d$th level
is a sound and complete proof system for proving $d \times d$ matrix identities
(over a given field). For each level $d>2$ in the hierarchy, we establish a
proof-size lower bound in terms of the number of variables in the matrix
identity proved: we show the existence of a family of matrix identities $f_n$
with $n$ variables, such that any proof of $f_n=0$ requires $\Omega(n^{2d})$
number of lines. The lower bound argument uses fundamental results from the
theory of algebras with polynomial identities together with a generalization of
the arguments in [Hru11].
  We then set out to study matrix identities as hard instances for (full)
arithmetic proofs. We present two conjectures, one about non-commutative
arithmetic circuit complexity and the other about proof complexity, under which
up to exponential-size lower bounds on arithmetic proofs (in terms of the
arithmetic circuit size of the identities proved) hold. Finally, we discuss the
applicability of our approach to strong propositional proof systems such as
Extended Frege."
"The reachability problem in cooperating systems is known to be
PSPACE-complete. We show here that this problem remains PSPACE-complete when we
restrict the communication structure between the subsystems in various ways.
For this purpose we introduce two basic and incomparable subclasses of
cooperating systems that occur often in practice and provide respective
reductions. The subclasses we consider consist of cooperating systems the
communication structure of which forms a line respectively a star."
"We remark that the AKS primality testing algorithm [Annals of Mathematics 160
(2), 2004] needs about 1,000,000,000 G (gigabyte) storage space for a number of
1024 bits. The requirement is very hard to meet. The complexity class P which
contains all decision problems that can be solved by a deterministic Turing
machine using a polynomial amount of computation time, is generally believed to
be ``easy"". We point out that the time is estimated only in terms of the amount
of arithmetic operations. It does not comprise the time for reading and writing
data on the tape in a Turing machine. The flaw makes some deterministic
polynomial time algorithms impractical, and humbles the importance of P=NP
question."
"In many practical problems it is not necessary to compute the DFT in a
perfect manner including some radar problems. In this article a new
multiplication free algorithm for approximate computation of the DFT is
introduced. All multiplications $(a\times b)$ in DFT are replaced by an
operator which computes $sign(a\times b)(|a|+|b|)$. The new transform is
especially useful when the signal processing algorithm requires correlations.
Ambiguity function in radar signal processing requires high number of
multiplications to compute the correlations. This new additive operator is used
to decrease the number of multiplications. Simulation examples involving
passive radars are presented."
"We study the problem of \emph{robust satisfiability} of systems of nonlinear
equations, namely, whether for a given continuous function
$f:\,K\to\mathbb{R}^n$ on a~finite simplicial complex $K$ and $\alpha>0$, it
holds that each function $g:\,K\to\mathbb{R}^n$ such that $\|g-f\|_\infty \leq
\alpha$, has a root in $K$. Via a reduction to the extension problem of maps
into a sphere, we particularly show that this problem is decidable in
polynomial time for every fixed $n$, assuming $\dim K \le 2n-3$. This is a
substantial extension of previous computational applications of
\emph{topological degree} and related concepts in numerical and interval
analysis. Via a reverse reduction we prove that the problem is undecidable when
$\dim K\ge 2n-2$, where the threshold comes from the \emph{stable range} in
homotopy theory. For the lucidity of our exposition, we focus on the setting
when $f$ is piecewise linear. Such functions can approximate general continuous
functions, and thus we get approximation schemes and undecidability of the
robust satisfiability in other possible settings."
"Let $\mathcal{P}$ be a property of function $\mathbb{F}_p^n \to \{0,1\}$ for
a fixed prime $p$. An algorithm is called a tester for $\mathcal{P}$ if, given
a query access to the input function $f$, with high probability, it accepts
when $f$ satisfies $\mathcal{P}$ and rejects when $f$ is ""far"" from satisfying
$\mathcal{P}$. In this paper, we give a characterization of affine-invariant
properties that are (two-sided error) testable with a constant number of
queries. The characterization is stated in terms of decomposition theorems,
which roughly claim that any function can be decomposed into a structured part
that is a function of a constant number of polynomials, and a pseudo-random
part whose Gowers norm is small. We first give an algorithm that tests whether
the structured part of the input function has a specific form. Then we show
that an affine-invariant property is testable with a constant number of queries
if and only if it can be reduced to the problem of testing whether the
structured part of the input function is close to one of a constant number of
candidates."
"We study the extent of independence needed to approximate the product of
bounded random variables in expectation, a natural question that has
applications in pseudorandomness and min-wise independent hashing.
  For random variables whose absolute value is bounded by $1$, we give an error
bound of the form $\sigma^{\Omega(k)}$ where $k$ is the amount of independence
and $\sigma^2$ is the total variance of the sum. Previously known bounds only
applied in more restricted settings, and were quanitively weaker. We use this
to give a simpler and more modular analysis of a construction of min-wise
independent hash functions and pseudorandom generators for combinatorial
rectangles due to Gopalan et al., which also slightly improves their
seed-length.
  Our proof relies on a new analytic inequality for the elementary symmetric
polynomials $S_k(x)$ for $x \in \mathbb{R}^n$ which we believe to be of
independent interest. We show that if $|S_k(x)|,|S_{k+1}(x)|$ are small
relative to $|S_{k-1}(x)|$ for some $k>0$ then $|S_\ell(x)|$ is also small for
all $\ell > k$. From these, we derive tail bounds for the elementary symmetric
polynomials when the inputs are only $k$-wise independent."
"Sensitivity, certificate complexity and block sensitivity are widely used
Boolean function complexity measures. A longstanding open problem, proposed by
Nisan and Szegedy, is whether sensitivity and block sensitivity are
polynomially related. Motivated by the constructions of functions which achieve
the largest known separations, we study the relation between 1-certificate
complexity and 0-sensitivity and 0-block sensitivity.
  Previously the best known lower bound was $C_1(f)\geq \frac{bs_0(f)}{2
s_0(f)}$, achieved by Kenyon and Kutin. We improve this to $C_1(f)\geq \frac{3
bs_0(f)}{2 s_0(f)}$. While this improvement is only by a constant factor, this
is quite important, as it precludes achieving a superquadratic separation
between $bs(f)$ and $s(f)$ by iterating functions which reach this bound. In
addition, this bound is tight, as it matches the construction of Ambainis and
Sun up to an additive constant."
"We study the direct-sum problem for $k$-party ``Number On the Forehead''
(NOF) deterministic communication complexity. We prove several positive
results, showing that the complexity of computing a function $f$ in this model,
on $\ell$ instances, may be significantly cheaper than $\ell$ times the
complexity of computing $f$ on a single instance. Quite surprisingly, we show
that this is the case for ``most'' (boolean, $k$-argument) functions. We then
formalize two-types of sufficient conditions on a NOF protocol $Q$, for a
single instance, each of which guarantees some communication complexity savings
when appropriately extending $Q$ to work on $\ell$ instances. One such
condition refers to what each party needs to know about inputs of the other
parties, and the other condition, additionally, refers to the communication
pattern that the single-instance protocol $Q$ uses. In both cases, the tool
that we use is ``multiplexing'': we combine messages sent in parallel
executions of protocols for a single instance, into a single message for the
multi-instance (direct-sum) case, by xoring them with each other."
"The exact computation of the number of distinct elements (frequency moment
$F_0$) is a fundamental problem in the study of data streaming algorithms. We
denote the length of the stream by $n$ where each symbol is drawn from a
universe of size $m$. While it is well known that the moments $F_0,F_1,F_2$ can
be approximated by efficient streaming algorithms, it is easy to see that exact
computation of $F_0,F_2$ requires space $\Omega(m)$. In previous work, Cormode
et al. therefore considered a model where the data stream is also processed by
a powerful helper, who provides an interactive proof of the result. They gave
such protocols with a polylogarithmic number of rounds of communication between
helper and verifier for all functions in NC. This number of rounds
$\left(O(\log^2 m) \;\text{in the case of} \;F_0 \right)$ can quickly make such
protocols impractical.
  Cormode et al. also gave a protocol with $\log m +1$ rounds for the exact
computation of $F_0$ where the space complexity is $O\left(\log m \log n+\log^2
m\right)$ but the total communication $O\left(\sqrt{n}\log m\left(\log n+ \log
m \right)\right)$. They managed to give $\log m$ round protocols with
$\operatorname{polylog}(m,n)$ complexity for many other interesting problems
including $F_2$, Inner product, and Range-sum, but computing $F_0$ exactly with
polylogarithmic space and communication and $O(\log m)$ rounds remained open.
  In this work, we give a streaming interactive protocol with $\log m$ rounds
for exact computation of $F_0$ using $O\left(\log m \left(\,\log n + \log m
\log\log m\,\right)\right)$ bits of space and the communication is $O\left(
\log m \left(\,\log n +\log^3 m (\log\log m)^2 \,\right)\right)$. The update
time of the verifier per symbol received is $O(\log^2 m)$."
"We study the computational complexity of two Boolean nonlinearity measures:
the nonlinearity and the multiplicative complexity. We show that if one-way
functions exist, no algorithm can compute the multiplicative complexity in time
$2^{O(n)}$ given the truth table of length $2^n$, in fact under the same
assumption it is impossible to approximate the multiplicative complexity within
a factor of $(2-\epsilon)^{n/2}$. When given a circuit, the problem of
determining the multiplicative complexity is in the second level of the
polynomial hierarchy. For nonlinearity, we show that it is #P hard to compute
given a function represented by a circuit."
"The paper presents an algebraic framework for optimization problems
expressible as Valued Constraint Satisfaction Problems. Our results generalize
the algebraic framework for the decision version (CSPs) provided by Bulatov et
al. [SICOMP 2005]. We introduce the notions of weighted algebras and varieties
and use the Galois connection due to Cohen et al. [SICOMP 2013] to link VCSP
languages to weighted algebras. We show that the difficulty of VCSP depends
only on the weighted variety generated by the associated weighted algebra.
Paralleling the results for CSPs we exhibit a reduction to cores and rigid
cores which allows us to focus on idempotent weighted varieties. Further, we
propose an analogue of the Algebraic CSP Dichotomy Conjecture; prove the
hardness direction and verify that it agrees with known results for VCSPs on
two-element sets [Cohen et al. 2006], finite-valued VCSPs [Thapper and Zivny
2013] and conservative VCSPs [Kolmogorov and Zivny 2013]."
"Obtaining a non-trivial (super-linear) lower bound for computation of the
Fourier transform in the linear circuit model has been a long standing open
problem for over 40 years.
  An early result by Morgenstern from 1973, provides an $\Omega(n \log n)$
lower bound for the unnormalized Fourier transform when the constants used in
the computation are bounded. The proof uses a potential function related to a
determinant. The result does not explain why the normalized Fourier transform
(of unit determinant) should be difficult to compute in the same model. Hence,
the result is not scale insensitive.
  More recently, Ailon (2013) showed that if only unitary 2-by-2 gates are
used, and additionally no extra memory is allowed, then the normalized Fourier
transform requires $\Omega(n\log n)$ steps. This rather limited result is also
sensitive to scaling, but highlights the complexity inherent in the Fourier
transform arising from introducing entropy, unlike, say, the identity matrix
(which is as complex as the Fourier transform using Morgenstern's arguments,
under proper scaling).
  In this work we extend the arguments of Ailon (2013). In the first extension,
which is also the main contribution, we provide a lower bound for computing any
scaling of the Fourier transform. Our restriction is that, the composition of
all gates up to any point must be a well conditioned linear transformation. The
lower bound is $\Omega(R^{-1}n\log n)$, where $R$ is the uniform condition
number. Second, we assume extra space is allowed, as long as it contains
information of bounded norm at the end of the computation.
  The main technical contribution is an extension of matrix entropy used in
Ailon (2013) for unitary matrices to a potential function computable for any
matrix, using Shannon entropy on ""quasi-probabilities""."
"We prove that playing Candy Crush to achieve a given score in a fixed number
of swaps is NP-hard."
"The problem of deciding if a Traveling Salesman Problem (TSP) tour is minimal
was proved to be coNP-complete by Papadimitriou and Steiglitz. We give an
alternative proof based on a polynomial time reduction from 3SAT. Like the
original proof, our reduction also shows that given a graph $G$ and an
Hamiltonian path of $G$, it is NP-complete to check if $G$ contains an
Hamiltonian cycle (Restricted Hamiltonian Cycle problem)."
"In this paper, we consider the Target Set Selection problem: given a graph
and a threshold value $thr(v)$ for any vertex $v$ of the graph, find a minimum
size vertex-subset to ""activate"" s.t. all the vertices of the graph are
activated at the end of the propagation process. A vertex $v$ is activated
during the propagation process if at least $thr(v)$ of its neighbors are
activated. This problem models several practical issues like faults in
distributed networks or word-to-mouth recommendations in social networks. We
show that for any functions $f$ and $\rho$ this problem cannot be approximated
within a factor of $\rho(k)$ in $f(k) \cdot n^{O(1)}$ time, unless FPT = W[P],
even for restricted thresholds (namely constant and majority thresholds). We
also study the cardinality constraint maximization and minimization versions of
the problem for which we prove similar hardness results."
"Counting the solution number of combinational optimization problems is an
important topic in the study of computational complexity, especially on the
#P-complete complexity class. In this paper, we first investigate some
organizations of Vertex-Cover unfrozen subgraphs by the underlying connectivity
and connected components of unfrozen vertices. Then, a Vertex-Cover Solution
Number Counting Algorithm is proposed and its complexity analysis is provided,
the results of which fit very well with the simulations and have better
performance than those by 1-RSB in a neighborhood of c = e for random graphs.
Base on the algorithm, variation and fluctuation on the solution number
statistics are studied to reveal the evolution mechanism of the solution
numbers. Besides, marginal probability distributions on the solution space are
investigated on both random graph and scale-free graph to illustrate different
evolution characteristics of their solution spaces. Thus, doing solution number
counting based on graph expression of solution space should be an alternative
and meaningful way to study the hardness of NP-complete and #P-complete
problems, and appropriate algorithm design can help to achieve better
approximations of solving combinational optimization problems and the
corresponding counting problems."
"An artificially designed Turing Machine algorithm $\mathbf{M}_{}^{o}$
generates the instances of the satisfiability problem, and check their
satisfiability. Under the assumption $\mathcal{P}=\mathcal{NP}$, we show that
$\mathbf{M}_{}^{o}$ has a certain property, which, without the assumption,
$\mathbf{M}_{}^{o}$ does not have. This leads to $\mathcal{P}\neq\mathcal{NP}$
$ $ by modus tollens."
"The combined universal probability M(D) of strings x in sets D is close to
max_{x \in D} M({x}): their ~ logs differ by at most D's information j = I(D:H)
about the halting sequence H. Thus if all x have complexity K(x) > k, D carries
> i bits of information on each x where i+j ~ k. Note, there are no ways
(whether natural or artificial) to generate D with significant I(D:H)."
"The twentieth century has seen the rise of a new type of video games targeted
at a mass audience of ""casual"" gamers. Many of these games require the player
to swap items in order to form matches of three and are collectively known as
\emph{tile-matching match-three games}. Among these, the most influential one
is arguably \emph{Bejeweled} in which the matched items (gems) pop and the
above gems fall in their place. Bejeweled has been ported to many different
platforms and influenced an incredible number of similar games. Very recently
one of them, named \emph{Candy Crush Saga} enjoyed a huge popularity and
quickly went viral on social networks. We generalize this kind of games by only
parameterizing the size of the board, while all the other elements (such as the
rules or the number of gems) remain unchanged. Then, we prove that answering
many natural questions regarding such games is actually \NP-Hard. These
questions include determining if the player can reach a certain score, play for
a certain number of turns, and others. We also
\href{http://candycrush.isnphard.com}{provide} a playable web-based
implementation of our reduction."
"Numerous popular abstract strategy games ranging from Hex and Havannah to
Lines of Action belong to the class of connection games. Still, very few
complexity results on such games have been obtained since Hex was proved
PSPACE-complete in the early eighties. We study the complexity of two
connection games among the most widely played. Namely, we prove that Havannah
and TwixT are PSPACE-complete. The proof for Havannah involves a reduction from
Generalized Geography and is based solely on ring-threats to represent the
input graph. On the other hand, the reduction for TwixT builds up on previous
work as it is a straightforward encoding of Hex."
"We present an explicit pseudorandom generator for oblivious, read-once,
width-$3$ branching programs, which can read their input bits in any order. The
generator has seed length $\tilde{O}( \log^3 n ).$ The previously best known
seed length for this model is $n^{1/2+o(1)}$ due to Impagliazzo, Meka, and
Zuckerman (FOCS '12). Our work generalizes a recent result of Reingold,
Steinke, and Vadhan (RANDOM '13) for \textit{permutation} branching programs.
The main technical novelty underlying our generator is a new bound on the
Fourier growth of width-3, oblivious, read-once branching programs.
Specifically, we show that for any $f:\{0,1\}^n\rightarrow \{0,1\}$ computed by
such a branching program, and $k\in [n],$ $$\sum_{s\subseteq [n]: |s|=k} \left|
\hat{f}[s] \right| \leq n^2 \cdot (O(\log n))^k,$$ where $\widehat{f}[s] =
\mathbb{E}\left[f[U] \cdot (-1)^{s \cdot U}\right]$ is the standard Fourier
transform over $\mathbb{Z}_2^n$. The base $O(\log n)$ of the Fourier growth is
tight up to a factor of $\log \log n$."
"This paper focuses on bounding the total communication complexity of
collapsing protocols for multiparty pointer jumping problem ($MPJ_k^n$). Brody
and Chakrabati in \cite{bc08} proved that in such setting one of the players
must communicate at least $n - 0.5\log{n}$ bits. Liang in \cite{liang} has
shown protocol matching this lower bound on maximum complexity. His protocol,
however, was behaving worse than the trivial one in terms of total complexity
(number of bits sent by all players). He conjectured that achieving total
complexity better then the trivial one is impossible. In this paper we prove
this conjecture. Namely, we show that for a collapsing protocol for $MPJ_k^n$,
the total communication complexity is at least $n-2$ which closes the gap
between lower and upper bound for total complexity of $MPJ_k^n$ in collapsing
setting."
"We present several results on comparative complexity for different variants
of OBDD models.
  - We present some results on comparative complexity of classical and quantum
OBDDs. We consider a partial function depending on parameter k such that for
any k > 0 this function is computed by an exact quantum OBDD of width 2 but any
classical OBDD (deterministic or stable bounded error probabilistic) needs
width 2k+1.
  - We consider quantum and classical nondeterminism. We show that quantum
nondeterminism can be more efficient than classical one. In particular, an
explicit function is presented which is computed by a quantum nondeterministic
OBDD with constant width but any classical nondeterministic OBDD for this
function needs non-constant width.
  - We also present new hierarchies on widths of deterministic and
non-deterministic OBDDs. We focus both on small and large widths."
"We study the structure of sets $S\subseteq\{0, 1\}^n$ with small sensitivity.
The well-known Simon's lemma says that any $S\subseteq\{0, 1\}^n$ of
sensitivity $s$ must be of size at least $2^{n-s}$. This result has been useful
for proving lower bounds on sensitivity of Boolean functions, with applications
to the theory of parallel computing and the ""sensitivity vs. block sensitivity""
conjecture.
  In this paper, we take a deeper look at the size of such sets and their
structure. We show an unexpected ""gap theorem"": if $S\subseteq\{0, 1\}^n$ has
sensitivity $s$, then we either have $|S|=2^{n-s}$ or $|S|\geq \frac{3}{2}
2^{n-s}$. This is shown via classifying such sets into sets that can be
constructed from low-sensitivity subsets of $\{0, 1\}^{n'}$ for $n'<n$ and
irreducible sets which cannot be constructed in such a way and then proving a
lower bound on the size of irreducible sets.
  This provides new insights into the structure of low sensitivity subsets of
the Boolean hypercube $\{0, 1\}^n$."
"The rank of a finite algebraic structure with a single binary operation is
the minimum number of elements needed to express every other element under the
closure of the operation. In the case of groups, the previous best algorithm
for computing rank used polylogarithmic space. We reduce the best upper bounds
on the complexity of computing rank for groups and for quasigroups. This paper
proves that the rank problem for these algebraic structures can be verified by
highly restricted models of computation given only very short certificates of
correctness.
  Specifically, we prove that the problem of deciding whether the rank of a
finite quasigroup, given as a Cayley table, is smaller than a specified number
is decidable by a circuit of depth $O(\log \log n)$ augmented with $O(\log^2
n)$ nondeterministic bits. Furthermore, if the quasigroup is a group, then the
problem is also decidable by a Turing machine using $O(\log n)$ space and
$O(\log^2 n)$ bits of nondeterminism with the ability to read the
nondeterministic bits multiple times. Finally, we provide similar results for
related problems on other algebraic structures and other kinds of rank. These
new upper bounds are significant improvements, especially for groups. In
general, the lens of limited nondeterminism provides an easy way to improve
many simple algorithms, like the ones presented here, and we suspect it will be
especially useful for other algebraic algorithms."
"Tropical circuits are circuits with Min and Plus, or Max and Plus operations
as gates. Their importance stems from their intimate relation to dynamic
programming algorithms. The power of tropical circuits lies somewhere between
that of monotone boolean circuits and monotone arithmetic circuits. In this
paper we present some lower bounds arguments for tropical circuits, and hence,
for dynamic programs."
"Obtaining lower bounds for NP-hard problems has for a long time been an
active area of research. Recent algebraic techniques introduced by Jonsson et
al. (SODA 2013) show that the time complexity of the parameterized SAT($\cdot$)
problem correlates to the lattice of strong partial clones. With this ordering
they isolated a relation $R$ such that SAT($R$) can be solved at least as fast
as any other NP-hard SAT($\cdot$) problem. In this paper we extend this method
and show that such languages also exist for the max ones problem
(MaxOnes($\Gamma$)) and the Boolean valued constraint satisfaction problem over
finite-valued constraint languages (VCSP($\Delta$)). With the help of these
languages we relate MaxOnes and VCSP to the exponential time hypothesis in
several different ways."
"Given a graph $G$ and a parameter $k$, the $k$-biclique problem asks whether
$G$ contains a complete bipartite subgraph $K_{k,k}$. This is the most easily
stated problem on graphs whose parameterized complexity is still unknown. We
provide an fpt-reduction from $k$-clique to $k$-biclique, thus solving this
longstanding open problem.
  Our reduction use a class of bipartite graphs with a threshold property of
independent interest. More specifically, for positive integers $n$, $s$ and
$t$, we consider a bipartite graph $G=(A\;\dot\cup\;B, E)$ such that $A$ can be
partitioned into $A=V_1\;\dot\cup \;V_2\;\dot\cup\cdots\dot\cup\; V_n$ and for
every $s$ distinct indices $i_1\cdots i_s$, there exist $v_{i_1}\in
V_{i_1}\cdots v_{i_s}\in V_{i_s}$ such that $v_{i_1}\cdots v_{i_s}$ have at
least $t+1$ common neighbors in $B$; on the other hand, every $s+1$ distinct
vertices in $A$ have at most $t$ common neighbors in $B$.
  Using the Paley-type graphs and Weil's character sum theorem, we show that
for $t=(s+1)!$ and $n$ large enough, such threshold bipartite graphs can be
computed in $n^{O(1)}$. One corollary of our reduction is that there is no
$f(k)\cdot n^{o(k)}$ time algorithm to decide whether a graph contains a
subgraph isomorphic to $K_{k!,k!}$ unless the ETH(Exponential Time Hypothesis)
fails. We also provide a probabilistic construction with better parameters
$t=\Theta(s^2)$, which indicates that $k$-biclique has no $f(k)\cdot
n^{o(\sqrt{k})}$-time algorithm unless 3-SAT with $m$ clauses can be solved in
$2^{o(m)}$-time with high probability. Our result also implies the dichotomy
classification of the parameterized complexity of cardinality constrain
satisfaction problem and the inapproximability of maximum $k$-intersection
problem."
"In this paper we study the behaviour at infinity of the Fourier transform of
Radon measures supported by the images of fractal sets under an algorithmically
random Brownian motion. We show that, under some computability conditions on
these sets, the Fourier transform of the associated measures have, relative to
the Hausdorff dimensions of these sets, optimal asymptotic decay at infinity.
The argument relies heavily on a direct characterisation, due to Asarin and
Pokrovskii, of algorithmically random Brownian motion in terms of the prefix
free Kolmogorov complexity of finite binary sequences. The study also
necessitates a closer look at the potential theory over fractals from a
computable point of view."
"Professor Alan Selman has been a giant in the field of computational
complexity for the past forty years. This article is an appreciation, on the
occasion of his retirement, of some of the most lovely concepts and results
that Alan has contributed to the field."
"In a recent work of Bei, Chen and Zhang (STOC 2013), a trial and error model
of computing was introduced, and applied to some constraint satisfaction
problems. In this model the input is hidden by an oracle which, for a candidate
assignment, reveals some information about a violated constraint if the
assignment is not satisfying. In this paper we initiate a systematic study of
constraint satisfaction problems in the trial and error model. To achieve this,
we first adopt a formal framework for CSPs, and based on this framework we
define several types of revealing oracles. Our main contribution is to develop
a transfer theorem for each type of the revealing oracle, under a broad class
of parameters. To any hidden CSP with a specific type of revealing oracle, the
transfer theorem associates another, potentially harder CSP in the normal
setting, such that their complexities are polynomial time equivalent. This in
principle transfers the study of a large class of hidden CSPs, possibly with a
promise on the instances, to the study of CSPs in the normal setting. We then
apply the transfer theorems to get polynomial-time algorithms or hardness
results for hidden CSPs, including satisfaction problems, monotone graph
properties, isomorphism problems, and the exact version of the Unique Games
problem."
"Given a matrix $A$ with $n$ rows, a number $k<n$, and $0<\delta < 1$, $A$ is
$(k,\delta)$-RIP (Restricted Isometry Property) if, for any vector $x \in
\mathbb{R}^n$, with at most $k$ non-zero co-ordinates, $$(1-\delta) \|x\|_2
\leq \|A x\|_2 \leq (1+\delta)\|x\|_2$$ In many applications, such as
compressed sensing and sparse recovery, it is desirable to construct RIP
matrices with a large $k$ and a small $\delta$. Given the efficacy of random
constructions in generating useful RIP matrices, the problem of certifying the
RIP parameters of a matrix has become important.
  In this paper, we prove that it is hard to approximate the RIP parameters of
a matrix assuming the Small-Set-Expansion-Hypothesis. Specifically, we prove
that for any arbitrarily large constant $C>0$ and any arbitrarily small
constant $0<\delta<1$, there exists some $k$ such that given a matrix $M$, it
is SSE-Hard to distinguish the following two cases:
  - (Highly RIP) $M$ is $(k,\delta)$-RIP.
  - (Far away from RIP) $M$ is not $(k/C, 1-\delta)$-RIP.
  Most of the previous results on the topic of hardness of RIP certification
only hold for certification when $\delta=o(1)$. In practice, it is of interest
to understand the complexity of certifying a matrix with $\delta$ being close
to $\sqrt{2}-1$, as it suffices for many real applications to have matrices
with $\delta = \sqrt{2}-1$. Our hardness result holds for any constant
$\delta$. Specifically, our result proves that even if $\delta$ is indeed very
small, i.e. the matrix is in fact \emph{strongly RIP}, certifying that the
matrix exhibits \emph{weak RIP} itself is SSE-Hard.
  In order to prove the hardness result, we prove a variant of the Cheeger's
Inequality for sparse vectors."
"We establish a lower bound for deciding the satisfiability of the conjunction
of any two Boolean formulas from a set called a full representation of Boolean
functions of $n$ variables - a set containing a Boolean formula to represent
each Boolean function of $n$ variables. The contradiction proof first assumes
that there exists a Turing machine with $k$ symbols in its tape alphabet that
correctly decides the satisfiability of the conjunction of any two Boolean
formulas from such a set by making fewer than $2^nlog_k2$ moves. By using
multiple runs of this Turing machine, with one run for each Boolean function of
$n$ variables, the proof derives a contradiction by showing that this Turing
machine is unable to correctly decide the satisfiability of the conjunction of
at least one pair of Boolean formulas from a full representation of
$n$-variable Boolean functions if the machine makes fewer than $2^nlog_k2$
moves. This lower bound holds for any full representation of Boolean functions
of $n$ variables, even if a full representation consists solely of minimized
Boolean formulas derived by a Boolean minimization method. We discuss why the
lower bound fails to hold for satisfiability of certain restricted formulas,
such as 2CNF satisfiability, XOR-SAT, and HORN-SAT. We also relate the lower
bound to 3CNF satisfiability. The lower bound does not depend on sequentiality
of access to the tape squares and will hold even if a machine is capable of
non-sequential access."
"We establish a lower bound of $2^n$ conditional branches for deciding the
satisfiability of the conjunction of any two Boolean formulas from a set called
a full representation of Boolean functions of $n$ variables - a set containing
a Boolean formula to represent each Boolean function of $n$ variables. The
contradiction proof first assumes that there exists a Post machine (Post's
Formulation 1) that correctly decides the satisfiability of the conjunction of
any two Boolean formulas from such a set by following an execution path that
includes fewer than $2^n$ conditional branches. By using multiple runs of this
Post machine, with one run for each Boolean function of $n$ variables, the
proof derives a contradiction by showing that this Post machine is unable to
correctly decide the satisfiability of the conjunction of at least one pair of
Boolean formulas from a full representation of $n$-variable Boolean functions
if the machine executes fewer than $2^n$ conditional branches. This lower bound
of $2^n$ conditional branches holds for any full representation of Boolean
functions of $n$ variables, even if a full representation consists solely of
minimized Boolean formulas derived by a Boolean minimization method. We discuss
why the lower bound fails to hold for satisfiability of certain restricted
formulas, such as 2CNF satisfiability, XOR-SAT, and HORN-SAT. We also relate
the lower bound to 3CNF satisfiability. The lower bound does not depend on
sequentiality of access to the boxes in the symbol space and will hold even if
a machine is capable of non-sequential access."
"We present a general framework for good CNF-representations of boolean
constraints, to be used for translating decision problems into SAT problems
(i.e., deciding satisfiability for conjunctive normal forms). We apply it to
the representation of systems of XOR-constraints, also known as systems of
linear equations over the two-element field, or systems of parity constraints.
  The general framework defines the notion of ""representation"", and provides
several methods to measure the quality of the representation by the complexity
(""hardness"") needed for making implicit ""knowledge"" of the representation
explicit (to a SAT-solving mechanism). We obtain general upper and lower
bounds.
  Applied to systems of XOR-constraints, we show a super-polynomial lower bound
on ""good"" representations under very general circumstances. A corresponding
upper bound shows fixed-parameter tractability in the number of constraints.
  The measurement underlying this upper bound ignores the auxiliary variables
needed for shorter representations of XOR-constraints. Improved upper bounds
(for special cases) take them into account, and a rich picture begins to
emerge, under the various hardness measurements."
"We give a $n^{O(\log n)}$-time ($n$ is the input size) blackbox polynomial
identity testing algorithm for unknown-order read-once oblivious algebraic
branching programs (ROABP). The best result known for this class was
$n^{O(\log^2 n)}$ due to Forbes-Saptharishi-Shpilka (STOC 2014), and that too
only for multilinear ROABP. We get rid of their exponential dependence on the
individual degree. With this, we match the time-complexity for the unknown
order ROABP with the known order ROABP (due to Forbes-Shpilka (FOCS 2013)) and
also with the depth-$3$ set-multilinear circuits (due to Agrawal-Saha-Saxena
(STOC 2013)). Our proof is simpler and involves a new technique called basis
isolation.
  The depth-$3$ model has recently gained much importance, as it has become a
stepping-stone to understanding general arithmetic circuits. Its restriction to
multilinearity has known exponential lower bounds but no nontrivial blackbox
identity tests. In this paper, we take a step towards designing such
hitting-sets. We give the first subexponential whitebox PIT for the sum of
constantly many set-multilinear depth-$3$ circuits. To achieve this, we define
notions of distance and base sets. Distance, for a multilinear depth-$3$
circuit, measures how far are the partitions from a mere refinement. We design
a hitting-set in time $n^{O(d \log n)}$ for $d$-distance. Further, we give an
extension of our result to models where the distance is large but it is small
when restricted to certain base sets (of variables).
  We also explore a new model of ROABP where the factor-matrices are invertible
(called invertible-factor ROABP). We design a hitting-set in time
poly($n^{w^2}$) for width-$w$ invertible-factor ROABP. Further, we could do
without the invertibility restriction when $w=2$. Previously, the best result
for width-$2$ ROABP was quasi-polynomial time (Forbes-Saptharishi-Shpilka, STOC
2014)."
"We consider polyhedral versions of Kannan and Lipton's Orbit Problem (STOC
'80 and JACM '86)---determining whether a target polyhedron V may be reached
from a starting point x under repeated applications of a linear transformation
A in an ambient vector space Q^m. In the context of program verification, very
similar reachability questions were also considered and left open by Lee and
Yannakakis in (STOC '92). We present what amounts to a complete
characterisation of the decidability landscape for the Polyhedron-Hitting
Problem, expressed as a function of the dimension m of the ambient space,
together with the dimension of the polyhedral target V: more precisely, for
each pair of dimensions, we either establish decidability, or show hardness for
longstanding number-theoretic open problems."
"A fundamental problem in program verification concerns the termination of
simple linear loops of the form x := u ; while Bx >= b do {x := Ax + a} where x
is a vector of variables, u, a, and c are integer vectors, and A and B are
integer matrices. Assuming the matrix A is diagonalisable, we give a decision
procedure for the problem of whether, for all initial integer vectors u, such a
loop terminates. The correctness of our algorithm relies on sophisticated tools
from algebraic and analytic number theory, Diophantine geometry, and real
algebraic geometry. To the best of our knowledge, this is the first substantial
advance on a 10-year-old open problem of Tiwari (2004) and Braverman (2006)."
"We prove that the Fourier dimension of any Boolean function with Fourier
sparsity $s$ is at most $O\left(s^{2/3}\right)$. Our proof method yields an
improved bound of $\widetilde{O}(\sqrt{s})$ assuming a conjecture of
Tsang~\etal~\cite{tsang}, that for every Boolean function of sparsity $s$ there
is an affine subspace of $\mathbb{F}_2^n$ of co-dimension $O(\poly\log s)$
restricted to which the function is constant. This conjectured bound is tight
upto poly-logarithmic factors as the Fourier dimension and sparsity of the
address function are quadratically separated. We obtain these bounds by
observing that the Fourier dimension of a Boolean function is equivalent to its
non-adaptive parity decision tree complexity, and then bounding the latter."
"We construct explicit Boolean square matrices whose rectifier complexity
(OR-complexity) differs significantly from the complexity of their complement
matrices."
"This paper will analyze several quadratic-time solvable problems, and will
classify them into two classes: problems that are solvable in truly
subquadratic time (that is, in time $O(n^{2-\epsilon})$ for some $\epsilon>0$)
and problems that are not, unless the well known Strong Exponential Time
Hypothesis (SETH) is false. In particular, we will prove that some
quadratic-time solvable problems are indeed easier than expected. We will
provide an algorithm that computes the transitive closure of a directed graph
in time $O(mn^{\frac{\omega+1}{4}})$, where $m$ denotes the number of edges in
the transitive closure and $\omega$ is the exponent for matrix multiplication.
As a side effect, we will prove that our algorithm runs in time
$O(n^{\frac{5}{3}})$ if the transitive closure is sparse. The same time bounds
hold if we want to check whether a graph is transitive, by replacing m with the
number of edges in the graph itself. As far as we know, this is the fastest
algorithm for sparse transitive digraph recognition. Finally, we will apply our
algorithm to the comparability graph recognition problem (dating back to 1941),
obtaining the first truly subquadratic algorithm. The second part of the paper
deals with hardness results. Starting from an artificial quadratic-time
solvable variation of the k-SAT problem, we will construct a graph of Karp
reductions, proving that a truly subquadratic-time algorithm for any of the
problems in the graph falsifies SETH. The analyzed problems are the following:
computing the subset graph, finding dominating sets, computing the betweenness
centrality of a vertex, computing the minimum closeness centrality, and
computing the hyperbolicity of a pair of vertices. We will also be able to
include in our framework three proofs already appeared in the literature,
concerning the graph diameter computation, local alignment of strings and
orthogonality of vectors."
"We consider the multiplicative complexity of Boolean functions with multiple
bits of output, studying how large a multiplicative complexity is necessary and
sufficient to provide a desired nonlinearity. For so-called $\Sigma\Pi\Sigma$
circuits, we show that there is a tight connection between error correcting
codes and circuits computing functions with high nonlinearity. Combining this
with known coding theory results, we show that functions with $n$ inputs and
$n$ outputs with the highest possible nonlinearity must have at least $2.32n$
AND gates. We further show that one cannot prove stronger lower bounds by only
appealing to the nonlinearity of a function; we show a bilinear circuit
computing a function with almost optimal nonlinearity with the number of AND
gates being exactly the length of such a shortest code.
  Additionally we provide a function which, for general circuits, has
multiplicative complexity at least $2n-3$.
  Finally we study the multiplicative complexity of ""almost all'' functions. We
show that every function with $n$ bits of input and $m$ bits of output can be
computed using at most $2.5(1+o(1))\sqrt{m2^n}$ AND gates."
"Given a symmetric matrix $M\in \{0,1,*\}^{D\times D}$, an $M$-partition of a
graph $G$ is a function from $V(G)$ to $D$ such that no edge of $G$ is mapped
to a $0$ of $M$ and no non-edge to a $1$. We give a computer-assisted proof
that, when $|D|=4$, the problem of counting the $M$-partitions of an input
graph is either in FP or is #P-complete. Tractability is proved by reduction to
the related problem of counting list $M$-partitions; intractability is shown
using a gadget construction and interpolation. We use a computer program to
determine which of the two cases holds for all but a small number of matrices,
which we resolve manually to establish the dichotomy. We conjecture that the
dichotomy also holds for $|D|>4$. More specifically, we conjecture that, for
any symmetric matrix $M\in\{0,1,*\}^{D\times D}$, the complexity of counting
$M$-partitions is the same as the related problem of counting list
$M$-partitions."
"In a recent result, Khot and Saket [FOCS 2014] proved the quasi-NP-hardness
of coloring a 2-colorable 12-uniform hypergraph with $2^{(\log n)^{\Omega(1)}}$
colors. This result was proved using a novel outer PCP verifier which had a
strong soundness guarantee. In this note, we show that we can reduce the arity
of their result by modifying their 12-query inner verifier to an 8-query inner
verifier based on the hypergraph coloring hardness reductions of Guruswami et.
al. [STOC 2014]. More precisely, we prove quasi-NP-hardness of the following
problems on n-vertex hypergraphs.
  - Coloring a 2-colorable 8-uniform hypergraph with $2^{(\log n)^{\Omega(1)}}$
colors.
  - Coloring a 4-colorable 4-uniform hypergraph with $2^{(\log n)^{\Omega(1)}}$
colors."
"The study of graph products is a major research topic and typically concerns
the term $f(G*H)$, e.g., to show that $f(G*H)=f(G)f(H)$. In this paper, we
study graph products in a non-standard form $f(R[G*H]$ where $R$ is a
""reduction"", a transformation of any graph into an instance of an intended
optimization problem. We resolve some open problems as applications.
  (1) A tight $n^{1-\epsilon}$-approximation hardness for the minimum
consistent deterministic finite automaton (DFA) problem, where $n$ is the
sample size. Due to Board and Pitt [Theoretical Computer Science 1992], this
implies the hardness of properly learning DFAs assuming $NP\neq RP$ (the
weakest possible assumption).
  (2) A tight $n^{1/2-\epsilon}$ hardness for the edge-disjoint paths (EDP)
problem on directed acyclic graphs (DAGs), where $n$ denotes the number of
vertices.
  (3) A tight hardness of packing vertex-disjoint $k$-cycles for large $k$.
  (4) An alternative (and perhaps simpler) proof for the hardness of properly
learning DNF, CNF and intersection of halfspaces [Alekhnovich et al., FOCS 2004
and J. Comput.Syst.Sci. 2008]."
"In the present paper, we construct an algorithm for the evaluation of real
Riemann zeta function $\zeta(s)$ for all real $s$, $s>1$, in polynomial time
and linear space on Turing machines in Ko-Friedman model. The algorithms is
based on a series expansion of real Riemann zeta function $\zeta(s)$ (the
series globally convergents) and uses algorithms for the evaluation of real
function $(1+x)^h$ and hypergeometric series in polynomial time and linear
space.
  The algorithm from the present paper modified in an obvious way to work with
the complex numbers can be used to evaluate complex Riemann zeta function
$\zeta(s)$ for $s=\sigma+\mathbf{i}t$, $\sigma\ne 1$ (so, also for the case of
$\sigma<1$), in polynomial time and linear space in $n$ wherein $2^{-n}$ is a
precision of the computation; the modified algorithm will be also polynomial
time and linear space in $\lceil \log_2(t)\rceil$ and exponential time and
exponential space in $\lceil \log_2(\sigma)\rceil$."
"In the present paper it is shown that real function $g(x)=\int_{0}^{x}f(t)dt$
is a linear-space computable real function on interval $[0,1]$ if $f$ is a
linear-space computable $C^2[0,1]$ real function on interval $[0,1]$, and this
result does not depend on any open question in the computational complexity
theory. The time complexity of computable real functions and integration of
computable real functions is considered in the context of Ko-Friedman model
which is based on the notion of Cauchy functions computable by Turing machines.
  In addition, a real computable function $f$ is given such that
$\int_{0}^{1}f\in FDSPACE(n^2)_{C[a,b]}$ but $\int_{0}^{1}f\notin FP_{C[a,b]}$
if $FP\ne#P$."
"We suggest a diagrammatic model of computation based on an axiom of
distributivity. A diagram of a decorated coloured tangle, similar to those that
appear in low dimensional topology, plays the role of a circuit diagram.
Equivalent diagrams represent bisimilar computations. We prove that our model
of computation is Turing complete, and that with bounded resources it can
moreover decide any language in complexity class IP, sometimes with better
performance parameters than corresponding classical protocols."
"We give a $O^*(k^{O(k)})$ time isomorphism testing algorithm for graphs of
eigenvalue multiplicity bounded by $k$ which improves on the previous best
running time bound of $O^*(2^{O(k^2/\log k)})$."
"Conservative constraint satisfaction problems (CSPs) constitute an important
particular case of the general CSP, in which the allowed values of each
variable can be restricted in an arbitrary way. Problems of this type are well
studied for graph homomorphisms. A dichotomy theorem characterizing
conservative CSPs solvable in polynomial time and proving that the remaining
ones are NP-complete was proved by Bulatov in 2003. Its proof, however, is
quite long and technical. A shorter proof of this result based on the absorbing
subuniverses technique was suggested by Barto in 2011. In this paper we give a
short elementary prove of the dichotomy theorem for the conservative CSP."
"The Petri net approach proves to be effective to tackle the $P$ vs $NP$
problem. A safe acyclic Petri net (PN) is associated with some Exactly-1 3SAT
formula, in which a clause is an exactly-1 disjunction $\dot{\vee}$ of
literals. A clause also corresponds to a set of conflicting transitions in the
PN. Some 2SAT/XOR-SAT formula arisen in the inversed PN checks if the truth
assignment of a literal (a transition firing) $z_v$ is ""incompatible"" for the
satisfiability of the 3SAT formula (the reachability of the target state in the
inversed PN). If $z_v$ is incompatible, then $z_v$ is discarded and
$\overline{z}_v$ becomes true. Therefore, a clause $(\overline{z}_v \dot{\vee}
z_i \dot{\vee} z_j)$ reduces to the conjunction $(\overline{z}_v \wedge
\overline{z}_i \wedge \overline{z}_j)$, and a 3-literal clause $(z_v \dot{\vee}
z_u \dot{\vee} z_x)$ reduces to the 2-literal clause $(z_u \oplus z_x)$. This
reduction facilitates checking un/satisfiability; the 3SAT formula is
un/satisfiable iff the target state of the inversed PN is un/reachable. The
solution complexity is $O(n^5)$. Therefore, $P = NP = \text{co}NP$."
"We prove that a variant of 2048, a popular online puzzle game, is
PSPACE-Complete. Our hardness result holds for a version of the problem where
the player has oracle access to the computer player's moves. Specifically, we
show that for an $n \times n$ game board $\mathcal{G}$, computing a sequence of
moves to reach a particular configuration $\mathbb{C}$ from an initial
configuration $\mathbb{C}_0$ is PSPACE-Complete. Our reduction is from
Nondeterministic Constraint Logic (NCL). We also show that determining whether
or not there exists a fixed sequence of moves $\mathcal{S} \in \{\Uparrow,
\Downarrow, \Leftarrow, \Rightarrow\}^k$ of length $k$ that results in a
winning configuration for an $n \times n$ game board is fixed-parameter
tractable (FPT). We describe an algorithm to solve this problem in $O(4^k n^2)$
time."
"As far as I know, at the time that I originally devised this result (1998),
this was the first constructive proof that, for any integer $k$, there is a
language in $\Sigma_2^P$ that cannot be simulated by a family of logic circuits
of size $n^k$. However, this result had previously been proved
non-constructively: see Cai and Watanabe [CW08] for more information on the
history of this problem.
  This constructive proof is based upon constructing a language $\Gamma$
derived from the satisfiabiility problem, and a language $\Lambda_k$ defined by
an alternating Turing machine. We show that the union of $\Gamma$ and
$\Lambda_k$ cannot be simulated by circuits of size $n^k$."
"We study the relationship between two measures of Boolean functions;
\emph{algebraic thickness} and \emph{normality}. For a function $f$, the
algebraic thickness is a variant of the \emph{sparsity}, the number of nonzero
coefficients in the unique GF(2) polynomial representing $f$, and the normality
is the largest dimension of an affine subspace on which $f$ is constant. We
show that for $0 < \epsilon<2$, any function with algebraic thickness
$n^{3-\epsilon}$ is constant on some affine subspace of dimension
$\Omega\left(n^{\frac{\epsilon}{2}}\right)$. Furthermore, we give an algorithm
for finding such a subspace. We show that this is at most a factor of
$\Theta(\sqrt{n})$ from the best guaranteed, and when restricted to the
technique used, is at most a factor of $\Theta(\sqrt{\log n})$ from the best
guaranteed. We also show that a concrete function, majority, has algebraic
thickness $\Omega\left(2^{n^{1/6}}\right)$."
"We study the circuit complexity of boolean functions in a certain infinite
basis. The basis consists of all functions that take value $1$ on antichains
over the boolean cube. We prove that the circuit complexity of the parity
function and the majority function of $n$ variables in this basis is $\lfloor
\frac{n+1}{2} \rfloor$ and $\left\lfloor \frac{n}{2} \right \rfloor +1$
respectively. We show that the asymptotic of the maximum complexity of
$n$-variable boolean functions in this basis equals $n.$"
"We define the lower and upper mutual dimensions $mdim(x:y)$ and $Mdim(x:y)$
between any two points $x$ and $y$ in Euclidean space. Intuitively these are
the lower and upper densities of the algorithmic information shared by $x$ and
$y$. We show that these quantities satisfy the main desiderata for a
satisfactory measure of mutual algorithmic information. Our main theorem, the
data processing inequality for mutual dimension, says that, if $f:\mathbb{R}^m
\rightarrow \mathbb{R}^n$ is computable and Lipschitz, then the inequalities
$mdim(f(x):y) \leq mdim(x:y)$ and $Mdim(f(x):y) \leq Mdim(x:y)$ hold for all $x
\in \mathbb{R}^m$ and $y \in \mathbb{R}^t$. We use this inequality and related
inequalities that we prove in like fashion to establish conditions under which
various classes of computable functions on Euclidean space preserve or
otherwise transform mutual dimensions between points."
"The analysis of several algorithms and data structures can be framed as a
peeling process on a random hypergraph: vertices with degree less than k and
their adjacent edges are removed until no vertices of degree less than k are
left. Often the question is whether the remaining hypergraph, the k-core, is
empty or not. In some settings, it may be possible to remove either vertices or
edges from the hypergraph before peeling, at some cost. For example, in hashing
applications where keys correspond to edges and buckets to vertices, one might
use an additional side data structure, commonly referred to as a stash, to
separately handle some keys in order to avoid collisions. The natural question
in such cases is to find the minimum number of edges (or vertices) that need to
be stashed in order to realize an empty k-core. We show that both these
problems are NP-complete for all $k \geq 2$ on graphs and regular hypergraphs,
with the sole exception being that the edge variant of stashing is solvable in
polynomial time for $k = 2$ on standard (2-uniform) graphs."
"We present and study a framework in which one can present alternation-based
lower bounds on proof length in proof systems for quantified Boolean formulas.
A key notion in this framework is that of proof system ensemble, which is
(essentially) a sequence of proof systems where, for each, proof checking can
be performed in the polynomial hierarchy. We introduce a proof system ensemble
called relaxing QU-res which is based on the established proof system
QU-resolution. Our main results include an exponential separation of the
tree-like and general versions of relaxing QU-res, and an exponential lower
bound for relaxing QU-res; these are analogs of classical results in
propositional proof complexity."
"When can $t$ terminal pairs in an $m \times n$ grid be connected by $t$
vertex-disjoint paths that cover all vertices of the grid? We prove that this
problem is NP-complete. Our hardness result can be compared to two previous
NP-hardness proofs: Lynch's 1975 proof without the ``cover all vertices''
constraint, and Kotsuma and Takenaga's 2010 proof when the paths are restricted
to have the fewest possible corners within their homotopy class. The latter
restriction is a common form of the famous Nikoli puzzle \emph{Numberlink}; our
problem is another common form of Numberlink, sometimes called \emph{Zig-Zag
Numberlink} and popularized by the smartphone app \emph{Flow Free}."
"We prove that the problem of reconstructing a permutation
$\pi_1,\dotsc,\pi_n$ of the integers $[1\dotso n]$ given the absolute
differences $|\pi_{i+1}-\pi_i|$, $i = 1,\dotsc,n-1$ is NP-complete. As an
intermediate step we first prove the NP-completeness of the decision version of
a new puzzle game that we call Crazy Frog Puzzle. The permutation
reconstruction from differences is one of the simplest combinatorial problems
that have been proved to be computationally intractable."
"We prove that computing an evolutionary ordering of a family of sets, i.e. an
ordering where each set intersects with --but is not included in-- the union
earlier sets, is NP-hard."
"We propose a new model of a weakly random source that admits randomness
extraction. Our model of additive sources includes such natural sources as
uniform distributions on arithmetic progressions (APs), generalized arithmetic
progressions (GAPs), and Bohr sets, each of which generalizes affine sources.
We give an explicit extractor for additive sources with linear min-entropy over
both $\mathbb{Z}_p$ and $\mathbb{Z}_p^n$, for large prime $p$, although our
results over $\mathbb{Z}_p^n$ require that the source further satisfy a
list-decodability condition. As a corollary, we obtain explicit extractors for
APs, GAPs, and Bohr sources with linear min-entropy, although again our results
over $\mathbb{Z}_p^n$ require the list-decodability condition. We further
explore special cases of additive sources. We improve previous constructions of
line sources (affine sources of dimension 1), requiring a field of size linear
in $n$, rather than $\Omega(n^2)$ by Gabizon and Raz. This beats the
non-explicit bound of $\Theta(n \log n)$ obtained by the probabilistic method.
We then generalize this result to APs and GAPs."
"We prove that for writing the 3 by 3 permanent polynomial as a determinant of
a matrix consisting only of zeros, ones, and variables as entries, a 7 by 7
matrix is required. Our proof is computer based and uses the enumeration of
bipartite graphs. Furthermore, we analyze sequences of polynomials that are
determinants of polynomially sized matrices consisting only of zeros, ones, and
variables. We show that these are exactly the sequences in the complexity class
of constant free polynomially sized (weakly) skew circuits."
"We define a reduction mechanism for LP and SDP formulations that degrades
approximation factors in a controlled fashion. Our reduction mechanism is a
minor restriction of classical reductions establishing inapproximability in the
context of PCP theorems. As a consequence we establish strong linear
programming inapproximability (for LPs with a polynomial number of constraints)
for many problems. In particular we obtain a $3/2-\varepsilon$
inapproximability for VertexCover answering an open question in
[arXiv:1309.0563] and we answer a weak version of our sparse graph conjecture
posed in [arXiv:1311.4001] showing an inapproximability factor of
$1/2+\varepsilon$ for bounded degree IndependentSet. In the case of SDPs, we
obtain inapproximability results for these problems relative to the
SDP-inapproximability of MaxCUT. Moreover, using our reduction framework we are
able to reproduce various results for CSPs from [arXiv:1309.0563] via simple
reductions from Max-2-XOR."
"The hypercube 2-segmentation problem is a certain biclustering problem that
was previously claimed to be NP-hard, but for which there does not appear to be
a publicly available proof of NP-hardness. This manuscript provides such a
proof."
"Decomposable Negation Normal Forms (DNNFs) are Boolean circuits in negation
normal form where the subcircuits leading into each AND gate are defined on
disjoint sets of variables. We prove a strongly exponential lower bound on the
size of DNNFs for a class of CNF formulas built from expander graphs. As a
corollary, we obtain a strongly exponential separation between DNNFs and CNF
formulas in prime implicates form. This settles an open problem in the area of
knowledge compilation (Darwiche and Marquis, 2002)."
"Technology trends will cause data movement to account for the majority of
energy expenditure and execution time on emerging computers. Therefore,
computational complexity will no longer be a sufficient metric for comparing
algorithms, and a fundamental characterization of data access complexity will
be increasingly important. The problem of developing lower bounds for data
access complexity has been modeled using the formalism of Hong & Kung's
red/blue pebble game for computational directed acyclic graphs (CDAGs).
However, previously developed approaches to lower bounds analysis for the
red/blue pebble game are very limited in effectiveness when applied to CDAGs of
real programs, with computations comprised of multiple sub-computations with
differing DAG structure. We address this problem by developing an approach for
effectively composing lower bounds based on graph decomposition. We also
develop a static analysis algorithm to derive the asymptotic data-access lower
bounds of programs, as a function of the problem size and cache size."
"Below is a translation from my Russian paper. I added references, unavailable
to me in Moscow. Similar results have been also given in [Schnorr Stumpf 75]
(see also [Lynch 75]). Earlier relevant work (classical theorems like
Compression, Speed-up, etc.) was done in [Tseitin 56, Rabin 59, Hartmanis
Stearns 65, Blum 67, Trakhtenbrot 67, Meyer Fischer 72].
  I translated only the part with the statement of the results. Instead of the
proof part I appended a later (1979, unpublished) proof sketch of a slightly
tighter version. The improvement is based on the results of [Meyer Winklmann
78, Sipser 78]. Meyer and Winklmann extended earlier versions to machines with
a separate input and working tape, thus allowing complexities smaller than the
input length (down to its log). Sipser showed the space-bounded Halting Problem
to require only additive constant overhead. The proof in the appendix below
employs both advances to extend the original proofs to machines with a fixed
alphabet and a separate input and working space. The extension has no (even
logarithmic) restrictions on complexity and no overhead (beyond an additive
constant). The sketch is very brief and a more detailed exposition is expected
later: [Seiferas Meyer]."
"Sensitivity conjecture is a longstanding and fundamental open problem in the
area of complexity measures of Boolean functions and decision tree complexity.
The conjecture postulates that the maximum sensitivity of a Boolean function is
polynomially related to other major complexity measures. Despite much attention
to the problem and major advances in analysis of Boolean functions in the past
decade, the problem remains wide open with no positive result toward the
conjecture since the work of Kenyon and Kutin from 2004.
  In this work, we present new upper bounds for various complexity measures in
terms of sensitivity improving the bounds provided by Kenyon and Kutin.
Specifically, we show that deg(f)^{1-o(1)}=O(2^{s(f)}) and C(f) < 2^{s(f)-1}
s(f); these in turn imply various corollaries regarding the relation between
sensitivity and other complexity measures, such as block sensitivity, via known
results. The gap between sensitivity and other complexity measures remains
exponential but these results are the first improvement for this difficult
problem that has been achieved in a decade."
"In this paper, we address the question of whether the recent derandomization
results obtained by the use of the low-degree long code can be extended to
other product settings. We consider two settings: (1) the graph product results
of Alon, Dinur, Friedgut and Sudakov [GAFA, 2004] and (2) the ""majority is
stablest"" type of result obtained by Dinur, Mossel and Regev [SICOMP, 2009] and
Dinur and Shinkar [In Proc. APPROX, 2010] while studying the hardness of
approximate graph coloring.
  In our first result, we show that there exists a considerably smaller
subgraph of $K_3^{\otimes R}$ which exhibits the following property (shown for
$K_3^{\otimes R}$ by Alon et al.): independent sets close in size to the
maximum independent set are well approximated by dictators.
  The ""majority is stablest"" type of result of Dinur et al. and Dinur and
Shinkar shows that if there exist two sets of vertices $A$ and $B$ in
$K_3^{\otimes R}$ with very few edges with one endpoint in $A$ and another in
$B$, then it must be the case that the two sets $A$ and $B$ share a single
influential coordinate. In our second result, we show that a similar ""majority
is stablest"" statement holds good for a considerably smaller subgraph of
$K_3^{\otimes R}$. Furthermore using this result, we give a more efficient
reduction from Unique Games to the graph coloring problem, leading to improved
hardness of approximation results for coloring."
"The problem of constructing pseudorandom generators that fool halfspaces has
been studied intensively in recent times. For fooling halfspaces over the
hypercube with polynomially small error, the best construction known requires
seed-length O(log^2 n) (MekaZ13). Getting the seed-length down to O(log(n)) is
a natural challenge in its own right, which needs to be overcome in order to
derandomize RL. In this work we make progress towards this goal by obtaining
near-optimal generators for two important special cases:
  1) We give a near optimal derandomization of the Chernoff bound for
independent, uniformly random bits. Specifically, we show how to generate a x
in {1,-1}^n using $\tilde{O}(\log (n/\epsilon))$ random bits such that for any
unit vector u, <u,x> matches the sub-Gaussian tail behaviour predicted by the
Chernoff bound up to error eps.
  2) We construct a generator which fools halfspaces with {0,1,-1} coefficients
with error eps with a seed-length of $\tilde{O}(\log(n/\epsilon))$. This
includes the important special case of majorities.
  In both cases, the best previous results required seed-length of $O(\log n +
\log^2(1/\epsilon))$.
  Technically, our work combines new Fourier-analytic tools with the iterative
dimension reduction techniques and the gradually increasing independence
paradigm of previous works (KaneMN11, CelisRSW13, GopalanMRTV12)."
"We prove that completing an untimed, unbounded track in TrackMania Nations
Forever is NP-complete by using a reduction from 3-SAT and showing that a
solution can be checked in polynomial time."
"Bloxorz is an online puzzle game where players move a 1 by 1 by 2 block by
tilting it on a subset of the two dimensional grid. Bloxorz features switches
that open and close trapdoors. The puzzle is to move the block from its initial
position to an upright position on the destination square. We show that the
problem of deciding whether a given Bloxorz level is solvable is
PSPACE-complete and that this remains so even when all trapdoors are initially
closed or all trapdoors are initially open. We also answer an open question of
Viglietta, showing that 2-buttons are sufficient for PSPACE-hardness of general
puzzle games. We also examine the hardness of some variants of Bloxorz,
including variants where the block is a 1 by 1 by 1 cube, and variants with
single-use tiles."
"Halfspaces or linear threshold functions are widely studied in complexity
theory, learning theory and algorithm design. In this work we study the natural
problem of constructing pseudorandom generators (PRGs) for halfspaces over the
sphere, aka spherical caps, which besides being interesting and basic geometric
objects, also arise frequently in the analysis of various randomized algorithms
(e.g., randomized rounding). We give an explicit PRG which fools spherical caps
within error $\epsilon$ and has an almost optimal seed-length of $O(\log n +
\log(1/\epsilon) \cdot \log\log(1/\epsilon))$. For an inverse-polynomially
growing error $\epsilon$, our generator has a seed-length optimal up to a
factor of $O( \log \log {(n)})$. The most efficient PRG previously known (due
to Kane, 2012) requires a seed-length of $\Omega(\log^{3/2}{(n)})$ in this
setting. We also obtain similar constructions to fool halfspaces with respect
to the Gaussian distribution.
  Our construction and analysis are significantly different from previous works
on PRGs for halfspaces and build on the iterative dimension reduction ideas of
Kane et. al. (2011) and Celis et. al. (2013), the \emph{classical moment
problem} from probability theory and explicit constructions of \emph{orthogonal
designs} based on the seminal work of Bourgain and Gamburd (2011) on expansion
in Lie groups."
"A secure set $S$ in a graph is defined as a set of vertices such that for any
$X\subseteq S$ the majority of vertices in the neighborhood of $X$ belongs to
$S$. It is known that deciding whether a set $S$ is secure in a graph is
co-NP-complete. However, it is still open how this result contributes to the
actual complexity of deciding whether for a given graph $G$ and integer $k$, a
non-empty secure set for $G$ of size at most $k$ exists. In this work, we
pinpoint the complexity of this problem by showing that it is
$\Sigma^P_2$-complete. Furthermore, the problem has so far not been subject to
a parameterized complexity analysis that considers structural parameters. In
the present work, we prove that the problem is $W[1]$-hard when parameterized
by treewidth. This is surprising since the problem is known to be FPT when
parameterized by solution size and ""subset problems"" that satisfy this property
usually tend to be FPT for bounded treewidth as well. Finally, we give an upper
bound by showing membership in XP, and we provide a positive result in the form
of an FPT algorithm for checking whether a given set is secure on graphs of
bounded treewidth."
"A locally-optimal structure is a combinatorial structure such as a maximal
independent set that cannot be improved by certain (greedy) local moves, even
though it may not be globally optimal. It is trivial to construct an
independent set in a graph. It is easy to (greedily) construct a maximal
independent set. However, it is NP-hard to construct a globally-optimal
(maximum) independent set. In general, constructing a locally-optimal structure
is somewhat more difficult than constructing an arbitrary structure, and
constructing a globally-optimal structure is more difficult than constructing a
locally-optimal structure. The same situation arises with listing. The
differences between the problems become obscured when we move from listing to
counting because nearly everything is #P-complete. However, we highlight an
interesting phenomenon that arises in approximate counting, where the situation
is apparently reversed. Specifically, we show that counting maximal independent
sets is complete for #P with respect to approximation-preserving reductions,
whereas counting all independent sets, or counting maximum independent sets is
complete for an apparently smaller class, $\mathrm{\#RH}\Pi_1$ which has a
prominent role in the complexity of approximate counting. Motivated by the
difficulty of approximately counting maximal independent sets in bipartite
graphs, we also study the problem of approximately counting other
locally-optimal structures that arise in algorithmic applications, particularly
problems involving minimal separators and minimal edge separators. Minimal
separators have applications via fixed-parameter-tractable algorithms for
constructing triangulations and phylogenetic trees. Although exact
(exponential-time) algorithms exist for listing these structures, we show that
the counting problems are #P-complete with respect to both exact and
approximation-preserving reductions."
"A read-once oblivious arithmetic branching program (ROABP) is an arithmetic
branching program (ABP) where each variable occurs in at most one layer. We
give the first polynomial time whitebox identity test for a polynomial computed
by a sum of constantly many ROABPs. We also give a corresponding blackbox
algorithm with quasi-polynomial time complexity $n^{O(\log n)}$. In both the
cases, our time complexity is double exponential in the number of ROABPs.
  ROABPs are a generalization of set-multilinear depth-$3$ circuits. The prior
results for the sum of constantly many set-multilinear depth-$3$ circuits were
only slightly better than brute-force, i.e. exponential-time.
  Our techniques are a new interplay of three concepts for ROABP: low
evaluation dimension, basis isolating weight assignment and low-support rank
concentration. We relate basis isolation to rank concentration and extend it to
a sum of two ROABPs using evaluation dimension (or partial derivatives)."
"An emerging theory of ""linear-algebraic pseudorandomness"" aims to understand
the linear-algebraic analogs of fundamental Boolean pseudorandom objects where
the rank of subspaces plays the role of the size of subsets. In this work, we
study and highlight the interrelationships between several such algebraic
objects such as subspace designs, dimension expanders, seeded rank condensers,
two-source rank condensers, and rank-metric codes. In particular, with the
recent construction of near-optimal subspace designs by Guruswami and Kopparty
as a starting point, we construct good (seeded) rank condensers (both lossless
and lossy versions), which are a small collection of linear maps $\mathbb{F}^n
\to \mathbb{F}^t$ for $t \ll n$ such that for every subset of $\mathbb{F}^n$ of
small rank, its rank is preserved (up to a constant factor in the lossy case)
by at least one of the maps.
  We then compose a tensoring operation with our lossy rank condenser to
construct constant-degree dimension expanders over polynomially large fields.
That is, we give $O(1)$ explicit linear maps $A_i:\mathbb{F}^n\to \mathbb{F}^n$
such that for any subspace $V \subseteq \mathbb{F}^n$ of dimension at most
$n/2$, $\dim\bigl( \sum_i A_i(V)\bigr) \ge (1+\Omega(1)) \dim(V)$. Previous
constructions of such constant-degree dimension expanders were based on
Kazhdan's property $T$ (for the case when $\mathbb{F}$ has characteristic zero)
or monotone expanders (for every field $\mathbb{F}$); in either case the
construction was harder than that of usual vertex expanders. Our construction,
on the other hand, is simpler.
  Via an equivalence to linear rank-metric codes, we then construct optimal
lossless two-source condensers. We then use our seeded rank condensers to
obtain near-optimal lossy two-source condensers for constant rank sources."
"In this paper we give subexponential size hitting sets for bounded depth
multilinear arithmetic formulas. Using the known relation between black-box PIT
and lower bounds we obtain lower bounds for these models.
  For depth-3 multilinear formulas, of size $\exp(n^\delta)$, we give a hitting
set of size $\exp(\tilde{O}(n^{2/3 + 2\delta/3}))$. This implies a lower bound
of $\exp(\tilde{\Omega}(n^{1/2}))$ for depth-3 multilinear formulas, for some
explicit polynomial.
  For depth-4 multilinear formulas, of size $\exp(n^\delta)$, we give a hitting
set of size $\exp(\tilde{O}(n^{2/3 + 4\delta/3}))$. This implies a lower bound
of $\exp(\tilde{\Omega}(n^{1/4}))$ for depth-4 multilinear formulas, for some
explicit polynomial.
  A regular formula consists of alternating layers of $+,\times$ gates, where
all gates at layer $i$ have the same fan-in. We give a hitting set of size
(roughly) $\exp\left(n^{1- \delta} \right)$, for regular depth-$d$ multilinear
formulas of size $\exp(n^\delta)$, where $\delta = O(\frac{1}{\sqrt{5}^d})$.
This result implies a lower bound of roughly
$\exp(\tilde{\Omega}(n^{\frac{1}{\sqrt{5}^d}}))$ for such formulas.
  We note that better lower bounds are known for these models, but also that
none of these bounds was achieved via construction of a hitting set. Moreover,
no lower bound that implies such PIT results, even in the white-box model, is
currently known.
  Our results are combinatorial in nature and rely on reducing the underlying
formula, first to a depth-4 formula, and then to a read-once algebraic
branching program (from depth-3 formulas we go straight to read-once algebraic
branching programs)."
"We continue the study of covering complexity of constraint satisfaction
problems (CSPs) initiated by Guruswami, Hastad and Sudan [SIAM J. Computing,
31(6):1663--1686, 2002] and Dinur and Kol [In Proc. $28$th IEEE Conference on
Computational Complexity, 2013]. The covering number of a CSP instance $\Phi$,
denoted by $\nu(\Phi)$ is the smallest number of assignments to the variables
of $\Phi$, such that each constraint of $\Phi$ is satisfied by at least one of
the assignments. We show the following results regarding how well efficient
algorithms can approximate the covering number of a given CSP instance.
  - Assuming a covering unique games conjecture, introduced by Dinur and Kol,
we show that for every non-odd predicate $P$ over any constant sized alphabet
and every integer $K$, it is NP-hard to distinguish between $P$-CSP instances
(i.e., CSP instances where all the constraints are of type $P$) which are
coverable by a constant number of assignments and those whose covering number
is at least $K$. Previously, Dinur and Kol, using the same covering unique
games conjecture, had shown a similar hardness result for every non-odd
predicate over the Boolean alphabet that supports a pairwise independent
distribution. Our generalization yields a complete characterization of CSPs
over constant sized alphabet $\Sigma$ that are hard to cover since CSP's over
odd predicates are trivially coverable with $|\Sigma|$ assignments.
  - For a large class of predicates that are contained in the $2k$-LIN
predicate, we show that it is quasi-NP-hard to distinguish between instances
which have covering number at most two and covering number at least
$\Omega(\log\log n)$. This generalizes the $4$-LIN result of Dinur and Kol that
states it is quasi-NP-hard to distinguish between $4$-LIN-CSP instances which
have covering number at most two and covering number at least $\Omega(\log
\log\log n)$."
"We first prove a new separating hyperplane theorem characterizing when a pair
of compact convex subsets $K, K'$ of the Euclidean space intersect, and when
they are disjoint. The theorem is distinct from classical separation theorems.
It generalizes the {\it distance duality} proved in our earlier work for
testing the membership of a distinguished point in the convex hull of a finite
point set. Next by utilizing the theorem, we develop a substantially
generalized and stronger version of the {\it Triangle Algorithm} introduced in
the previous work to perform any of the following three tasks: (1) To compute a
pair $(p,p') \in K \times K'$, where either the Euclidean distance $d(p,p')$ is
to within a prescribed tolerance, or the orthogonal bisecting hyperplane of the
line segment $pp'$ separates the two sets; (2) When $K$ and $K'$ are disjoint,
to compute $(p,p') \in K \times K'$ so that $d(p,p')$ approximates $d(K,K')$ to
within a prescribed tolerance; (3) When $K$ and $K'$ are disjoint, to compute a
pair of parallel supporting hyperplanes $H,H'$ so that $d(H,H')$ is to within a
prescribed tolerance of the optimal margin. The worst-case complexity of each
iteration is solving a linear objective over $K$ or $K'$. The resulting
algorithm is a fully polynomial-time approximation scheme for such important
special cases as when $K$ and $K'$ are convex hulls of finite points sets, or
the intersection of a finite number of halfspaces. The results find many
theoretical and practical applications, such as in machine learning,
statistics, linear, quadratic and convex programming. In particular, in a
separate article we report on a comparison of the Triangle Algorithm and SMO
for solving the hard margin problem. In future work we extend the applications
to combinatorial and NP-complete problems."
"In this paper, we will show dichotomy theorems for the computation of
polynomials corresponding to evaluation of graph homomorphisms in Valiant's
model. We are given a fixed graph $H$ and want to find all graphs, from some
graph class, homomorphic to this $H$. These graphs will be encoded by a family
of polynomials.
  We give dichotomies for the polynomials for cycles, cliques, trees,
outerplanar graphs, planar graphs and graphs of bounded genus."
"Braid is a 2008 puzzle game centered around the ability to reverse time. We
show that Braid can simulate an arbitrary computation. Our construction makes
no use of Braid's unique time mechanics, and therefore may apply to many other
video games.
  We also show that a plausible ""bounded"" variant of Braid lies within
2-EXPSPACE. Our proof relies on a technical lemma about Turing machines which
may be of independent interest. Namely, define a braidlike Turing machine to be
a Turing machine that, when it writes to the tape, deletes all data on the tape
to the right of the head. We prove that deciding the behavior of such a machine
lies in EXPSPACE."
"We consider two CSP problems: the first CSP encodes 2D Sperner's lemma for
the standard triangulation of the right triangle on $n^2$ small triangles; the
second CSP encodes the fact that it is impossible to match cells of $n \times
n$ square to arrows (two horizontal, two vertical and four diagonal) such that
arrows in two cells with a common edge differ by at most $45^\circ$, and all
arrows on the boundary of the square do not look outside (this fact is a
corollary of the Brower's fixed point theorem). We prove that the tree-like
resolution complexities of these CSPs are $2^{\Theta(n)}$. For Sperner's lemma
our result implies $\Omega(n)$ lower bound on the number of request to colors
of vertices that is enough to make in order to find a trichromatic triangle;
this lower bound was originally proved by Crescenzi and Silvestri.
  CSP based on Sperner's lemma is related with the $\rm PPAD$-complete problem.
We show that CSP corresponding to arrows is also related with a $\rm
PPAD$-complete problem."
"In this paper the reason why entropy reduction (negentropy) can be used to
measure the complexity of any computation was first elaborated both in the
aspect of mathematics and informational physics. In the same time the
equivalence of computation and information was clearly stated. Then the
complexities of three specific problems: logical compare, sorting and SAT, were
analyzed and measured. The result showed SAT was a problem with exponential
complexity which naturally leads to the conclusion that no efficient algorithm
exists to solve it. That's to say: NP!=P."
"Motivated by a recent result of Elberfeld, Jakoby and Tantau showing that
$\mathsf{MSO}$ properties are Logspace computable on graphs of bounded
tree-width, we consider the complexity of computing the determinant of the
adjacency matrix of a bounded tree-width graph and as our main result prove
that it is in Logspace. It is important to notice that the determinant is
neither an $\mathsf{MSO}$-property nor counts the number of solutions of an
$\mathsf{MSO}$-predicate. This technique yields Logspace algorithms for
counting the number of spanning arborescences and directed Euler tours in
bounded tree-width digraphs.
  We demonstrate some linear algebraic applications of the determinant
algorithm by describing Logspace procedures for the characteristic polynomial,
the powers of a weighted bounded tree-width graph and feasibility of a system
of linear equations where the underlying bipartite graph has bounded
tree-width.
  Finally, we complement our upper bounds by proving $\mathsf{L}$-hardness of
the problems of computing the determinant, and of powering a bounded tree-width
matrix. We also show the $\mathsf{GapL}$-hardness of Iterated Matrix
Multiplication where each matrix has bounded tree-width."
"Population protocols have been introduced by Angluin et al. as a model in
which n passively mobile anonymous finite-state agents stably compute a
predicate on the multiset of their inputs via interactions by pairs. The model
has been extended by Guerraoui and Ruppert to yield the community protocol
models where agents have unique identifiers but may only store a finite number
of the identifiers they already heard about. The population protocol models can
only compute semi-linear predicates, whereas in the community protocol model
the whole community of agents provides collectively the power of a Turing
machine with a O(n log n) space. We consider variations on the above models and
we obtain a whole landscape that covers and extends already known results: By
considering the case of homonyms, that is to say the case when several agents
may share the same identifier, we provide a hierarchy that goes from the case
of no identifier (i.e. a single one for all, i.e. the population protocol
model) to the case of unique identifiers (i.e. community protocol model). We
obtain in particular that any Turing Machine on space O(logO(1) n) can be
simulated with at least O(logO(1) n) identifiers, a result filling a gap left
open in all previous studies. Our results also extend and revisit in particular
the hierarchy provided by Chatzigiannakis et al. on population protocols
carrying Turing Machines on limited space, solving the problem of the gap left
by this work between per-agent space o(log log n) (proved to be equivalent to
population protocols) and O(log n) (proved to be equivalent to Turing
machines)."
"We consider the non-preemptive task scheduling problem with release times and
deadlines on a single machine parameterized by the set of task lengths the
tasks can have. The identical task lengths case is known to be solvable in
polynomial time. We prove that the problem with two task lengths is
NP-complete, except for the case in which the short jobs have unit task length,
which was already known to be efficiently solvable."
"We investigate computing models that are presented as families of finite
computing devices with a uniformity condition on the entire family. Examples of
such models include Boolean circuits, membrane systems, DNA computers, chemical
reaction networks and tile assembly systems, and there are many others.
However, in such models there are actually two distinct kinds of uniformity
condition. The first is the most common and well-understood, where each input
length is mapped to a single computing device (e.g. a Boolean circuit) that
computes on the finite set of inputs of that length. The second, called
semi-uniformity, is where each input is mapped to a computing device for that
input (e.g. a circuit with the input encoded as constants). The former notion
is well-known and used in Boolean circuit complexity, while the latter notion
is frequently found in literature on nature-inspired computation from the past
20 years or so.
  Are these two notions distinct? For many models it has been found that these
notions are in fact the same, in the sense that the choice of uniformity or
semi-uniformity leads to characterisations of the same complexity classes. In
other related work, we showed that these notions are actually distinct for
certain classes of Boolean circuits. Here, we give analogous results for
membrane systems by showing that certain classes of uniform membrane systems
are strictly weaker than the analogous semi-uniform classes. This solves a
known open problem in the theory of membrane systems. We then go on to present
results towards characterising the power of these semi-uniform and uniform
membrane models in terms of NL and languages reducible to the unary languages
in NL, respectively."
"The problem of constructing explicit functions which cannot be approximated
by low degree polynomials has been extensively studied in computational
complexity, motivated by applications in circuit lower bounds,
pseudo-randomness, constructions of Ramsey graphs and locally decodable codes.
Still, most of the known lower bounds become trivial for polynomials of
super-logarithmic degree. Here, we suggest a new barrier explaining this
phenomenon. We show that many of the existing lower bound proof techniques
extend to nonclassical polynomials, an extension of classical polynomials which
arose in higher order Fourier analysis. Moreover, these techniques are tight
for nonclassical polynomials of logarithmic degree."
"Variable selection for sparse linear regression is the problem of finding,
given an m x p matrix B and a target vector y, a sparse vector x such that Bx
approximately equals y. Assuming a standard complexity hypothesis, we show that
no polynomial-time algorithm can find a k'-sparse x with ||Bx-y||^2<=h(m,p),
where k'=k*2^{log^{1-delta} p} and h(m,p)<=p^(C_1)*m^(1-C_2), where delta>0,
C_1>0,C_2>0 are arbitrary. This is true even under the promise that there is an
unknown k-sparse vector x^* satisfying Bx^*=y. We prove a similar result for a
statistical version of the problem in which the data are corrupted by noise.
  To the authors' knowledge, these are the first hardness results for sparse
regression that apply when the algorithm simultaneously has k'>k and h(m,p)>0."
"We show new results about the garden-hose model. Our main results include
improved lower bounds based on non-deterministic communication complexity
(leading to the previously unknown $\Theta(n)$ bounds for Inner Product mod 2
and Disjointness), as well as an $O(n\cdot \log^3 n)$ upper bound for the
Distributed Majority function (previously conjectured to have quadratic
complexity). We show an efficient simulation of formulae made of AND, OR, XOR
gates in the garden-hose model, which implies that lower bounds on the
garden-hose complexity $GH(f)$ of the order $\Omega(n^{2+\epsilon})$ will be
hard to obtain for explicit functions. Furthermore we study a time-bounded
variant of the model, in which even modest savings in time can lead to
exponential lower bounds on the size of garden-hose protocols."
"We consider the problem of testing whether an unknown Boolean function $f$ is
monotone versus $\epsilon$-far from every monotone function. The two main
results of this paper are a new lower bound and a new algorithm for this
well-studied problem.
  Lower bound: We prove an $\tilde{\Omega}(n^{1/5})$ lower bound on the query
complexity of any non-adaptive two-sided error algorithm for testing whether an
unknown Boolean function $f$ is monotone versus constant-far from monotone.
This gives an exponential improvement on the previous lower bound of
$\Omega(\log n)$ due to Fischer et al. [FLN+02]. We show that the same lower
bound holds for monotonicity testing of Boolean-valued functions over hypergrid
domains $\{1,\ldots,m\}^n$ for all $m\ge 2$.
  Upper bound: We give an $\tilde{O}(n^{5/6})\text{poly}(1/\epsilon)$-query
algorithm that tests whether an unknown Boolean function $f$ is monotone versus
$\epsilon$-far from monotone. Our algorithm, which is non-adaptive and makes
one-sided error, is a modified version of the algorithm of Chakrabarty and
Seshadhri [CS13a], which makes $\tilde{O}(n^{7/8})\text{poly}(1/\epsilon)$
queries."
"We prove a lower bound of $\Omega(n^{1/2 - c})$, for all $c>0$, on the query
complexity of (two-sided error) non-adaptive algorithms for testing whether an
$n$-variable Boolean function is monotone versus constant-far from monotone.
This improves a $\tilde{\Omega}(n^{1/5})$ lower bound for the same problem that
was recently given in [CST14] and is very close to $\Omega(n^{1/2})$, which we
conjecture is the optimal lower bound for this model."
"In this paper we simplify the definition of the weighted sum Boolean function
which used to be inconvenient to compute and use. We show that the new function
has essentially the same properties as the previous one. In particular, the
bound on the average sensitivity of the weighted sum Boolean function remains
unchanged after the simplification."
"The smoothing parameter $\eta_{\epsilon}(\mathcal{L})$ of a Euclidean lattice
$\mathcal{L}$, introduced by Micciancio and Regev (FOCS'04; SICOMP'07), is
(informally) the smallest amount of Gaussian noise that ""smooths out"" the
discrete structure of $\mathcal{L}$ (up to error $\epsilon$). It plays a
central role in the best known worst-case/average-case reductions for lattice
problems, a wealth of lattice-based cryptographic constructions, and
(implicitly) the tightest known transference theorems for fundamental lattice
quantities.
  In this work we initiate a study of the complexity of approximating the
smoothing parameter to within a factor $\gamma$, denoted $\gamma$-${\rm
GapSPP}$. We show that (for $\epsilon = 1/{\rm poly}(n)$): $(2+o(1))$-${\rm
GapSPP} \in {\rm AM}$, via a Gaussian analogue of the classic
Goldreich-Goldwasser protocol (STOC'98); $(1+o(1))$-${\rm GapSPP} \in {\rm
coAM}$, via a careful application of the Goldwasser-Sipser (STOC'86) set size
lower bound protocol to thin spherical shells; $(2+o(1))$-${\rm GapSPP} \in
{\rm SZK} \subseteq {\rm AM} \cap {\rm coAM}$ (where ${\rm SZK}$ is the class
of problems having statistical zero-knowledge proofs), by constructing a
suitable instance-dependent commitment scheme (for a slightly worse
$o(1)$-term); $(1+o(1))$-${\rm GapSPP}$ can be solved in deterministic
$2^{O(n)} {\rm polylog}(1/\epsilon)$ time and $2^{O(n)}$ space.
  As an application, we demonstrate a tighter worst-case to average-case
reduction for basing cryptography on the worst-case hardness of the ${\rm
GapSPP}$ problem, with $\tilde{O}(\sqrt{n})$ smaller approximation factor than
the ${\rm GapSVP}$ problem.
  Central to our results are two novel, and nearly tight, characterizations of
the magnitude of discrete Gaussian sums."
"The essential variables in a finite function $f$ are defined as variables
which occur in $f$ and weigh with the values of that function.
  The number of essential variables is an important measure of complexity for
discrete functions.
  When replacing some variables in a function with constants the resulting
functions are called subfunctions, and when replacing all essential variables
in a function with constants we obtain an implementation of this function.
  Such an implementation corresponds with a path in an ordered decision diagram
(ODD) of the function which connects the root with a leaf of the diagram. The
sets of essential variables in subfunctions of $f$ are called separable in $f$.
In this paper we study several properties of separable sets of variables in
functions which directly impact on the number of implementations and
subfunctions in these functions.
  We define equivalence relations which classify the functions of $k$-valued
logic into classes with same number of implementations, subfunctions or
separable sets. These relations induce three transformation groups which are
compared with the lattice of all subgroups of restricted affine group (RAG).
This allows us to solve several important computational and combinatorial
problems."
"In this paper we study the complexity of factorization of polynomials in the
free noncommutative ring $\mathbb{F}\langle x_1,x_2,\dots,x_n\rangle$ of
polynomials over the field $\mathbb{F}$ and noncommuting variables
$x_1,x_2,\ldots,x_n$. Our main results are the following.
  Although $\mathbb{F}\langle x_1,x_2,\dots,x_n \rangle$ is not a unique
factorization ring, we note that variable-disjoint factorization in
$\mathbb{F}\langle x_1,x_2,\dots,x_n \rangle$ has the uniqueness property.
Furthermore, we prove that computing the variable-disjoint factorization is
polynomial-time equivalent to Polynomial Identity Testing (both when the input
polynomial is given by an arithmetic circuit or an algebraic branching
program). We also show that variable-disjoint factorization in the black-box
setting can be efficiently computed (where the factors computed will be also
given by black-boxes, analogous to the work [KT91] in the commutative setting).
  As a consequence of the previous result we show that homogeneous
noncommutative polynomials and multilinear noncommutative polynomials have
unique factorizations in the usual sense, which can be efficiently computed.
  Finally, we discuss a polynomial decomposition problem in $\mathbb{F}\langle
x_1,x_2,\dots,x_n\rangle$ which is a natural generalization of homogeneous
polynomial factorization and prove some complexity bounds for it."
"We prove that for every $\epsilon>0$ and predicate $P:\{0,1\}^k\rightarrow
\{0,1\}$ that supports a pairwise independent distribution, there exists an
instance $\mathcal{I}$ of the $\mathsf{Max}P$ constraint satisfaction problem
on $n$ variables such that no assignment can satisfy more than a
$\tfrac{|P^{-1}(1)|}{2^k}+\epsilon$ fraction of $\mathcal{I}$'s constraints but
the degree $\Omega(n)$ Sum of Squares semidefinite programming hierarchy cannot
certify that $\mathcal{I}$ is unsatisfiable. Similar results were previously
only known for weaker hierarchies."
"We study how the adoption of an evaluation mechanism with sharing and
memoization impacts the class of functions which can be computed in polynomial
time. We first show how a natural cost model in which lookup for an already
computed value has no cost is indeed invariant. As a corollary, we then prove
that the most general notion of ramified recurrence is sound for polynomial
time, this way settling an open problem in implicit computational complexity."
"An elegant characterization of the complexity of constraint satisfaction
problems has emerged in the form of the the algebraic dichotomy conjecture of
[BKJ00]. Roughly speaking, the characterization asserts that a CSP {\Lambda} is
tractable if and only if there exist certain non-trivial operations known as
polymorphisms to combine solutions to {\Lambda} to create new ones. In an
entirely separate line of work, the unique games conjecture yields a
characterization of approximability of Max-CSPs. Surprisingly, this
characterization for Max-CSPs can also be reformulated in the language of
polymorphisms.
  In this work, we study whether existence of non-trivial polymorphisms implies
tractability beyond the realm of constraint satisfaction problems, namely in
the value-oracle model. Specifically, given a function f in the value-oracle
model along with an appropriate operation that never increases the value of f ,
we design algorithms to minimize f . In particular, we design a randomized
algorithm to minimize a function f that admits a fractional polymorphism which
is measure preserving and has a transitive symmetry.
  We also reinterpret known results on MaxCSPs and thereby reformulate the
unique games conjecture as a characterization of approximability of max-CSPs in
terms of their approximate polymorphisms."
"The notion of nondeterminism has disappeared from the current definition of
NP, which has led to ambiguities in understanding NP, and caused fundamental
difficulties in studying the relation P versus NP. In this paper, we question
the equivalence of the two definitions of NP, the one defining NP as the class
of problems solvable by a nondeterministic Turing machine in polynomial time,
and the other defining NP as the class of problems verifiable by a
deterministic Turing machine in polynomial time, and reveal cognitive biases in
this equivalence. Inspired from a famous Chinese paradox white horse is not
horse, we further analyze these cognitive biases. The work shows that these
cognitive biases arise from the confusion between different levels of
nondeterminism and determinism, due to the lack of understanding about the
essence of nondeterminism. Therefore, we argue that fundamental difficulties in
understanding P versus NP lie firstly at cognition level, then logic level."
"In this paper, we make a preliminary interpretation of Cook's theorem
presented in [1]. This interpretation reveals cognitive biases in the proof of
Cook's theorem that arise from the attempt of constructing a formula in CNF to
represent a computation of a nondeterministic Turing machine. Such cognitive
biases are due to the lack of understanding about the essence of
nondeterminism, and lead to the confusion between different levels of
nondeterminism and determinism, thus cause the loss of nondeterminism from the
NP-completeness theory. The work shows that Cook's theorem is the origin of the
loss of nondeterminism in terms of the equivalence of the two definitions of
NP, the one defining NP as the class of problems solvable by a nondeterministic
Turing machine in polynomial time, and the other defining NP as the class of
problems verifiable by a deterministic Turing machine in polynomial time.
Therefore, we argue that fundamental difficulties in understanding P versus NP
lie firstly at cognition level, then logic level."
"We show that multiplication can be done in polynomial time on a three counter
machine that receives its input as the contents of two counters. The technique
is generalized to functions of two variables computable by deterministic Turing
machines in linear space."
"We study the complexity of a particular class of board games, which we call
`slide and merge' games. Namely, we consider 2048 and Threes, which are among
the most popular games of their type. In both games, the player is required to
slide all rows or columns of the board in one direction to create a high value
tile by merging pairs of equal tiles into one with the sum of their values.
This combines features from both block pushing and tile matching puzzles, like
Push and Bejeweled, respectively. We define a number of natural decision
problems on a suitable generalization of these games and prove NP-hardness for
2048 by reducing from 3SAT. Finally, we discuss the adaptation of our reduction
to Threes and conjecture a similar result."
"Inspired by computational complexity results for the quantified constraint
satisfaction problem, we study the clones of idempotent polymorphisms of
certain digraph classes. Our first results are two algebraic dichotomy, even
""gap"", theorems. Building on and extending [Martin CP'11], we prove that
partially reflexive paths bequeath a set of idempotent polymorphisms whose
associated clone algebra has: either the polynomially generated powers property
(PGP); or the exponentially generated powers property (EGP). Similarly, we
build on [DaMM ICALP'14] to prove that semicomplete digraphs have the same
property.
  These gap theorems are further motivated by new evidence that PGP could be
the algebraic explanation that a QCSP is in NP even for unbounded alternation.
Along the way we also effect a study of a concrete form of PGP known as
collapsibility, tying together the algebraic and structural threads from [Chen
Sicomp'08], and show that collapsibility is equivalent to its
$\Pi_2$-restriction. We also give a decision procedure for $k$-collapsibility
from a singleton source of a finite structure (a form of collapsibility which
covers all known examples of PGP for finite structures).
  Finally, we present a new QCSP trichotomy result, for partially reflexive
paths with constants. Without constants it is known these QCSPs are either in
NL or Pspace-complete [Martin CP'11], but we prove that with constants they
attain the three complexities NL, NP-complete and Pspace-complete."
"Motivated by a relaxed notion of the celebrated Hamiltonian cycle, this paper
investigates its variant, parity Hamiltonian cycle (PHC): A PHC of a graph is a
closed walk which visits every vertex an odd number of times, where we remark
that the walk may use an edge more than once. First, we give a complete
characterization of the graphs which have PHCs, and give a linear time
algorithm to find a PHC, in which every edge appears at most four times, in
fact. In contrast, we show that finding a PHC is NP-hard if a closed walk is
allowed to use each edge at most z times for each z=1,2,3 (PHCz for short),
even when a given graph is two-edge connected. We then further investigate the
PHC3 problem, and show that the problem is in P when an input graph is
four-edge connected. Finally, we are concerned with three (or two)-edge
connected graphs, and show that the PHC3 is in P for any C_>=5-free or P6-free
graphs. Note that the Hamiltonian cycle problem is known to be NP-hard for
those graph classes."
"We show that the Kth largest subset problem and the Kth largest m-tuple
problem are in PP and hard for PP under polynomial-time Turing reductions.
Several problems from the literature were previously shown NP-hard via
reductions from those two problems, and by our main result they become PP-hard
as well. We also provide complementary PP-upper bounds for some of them."
"Let $\mathcal{F}_{n}^*$ be the set of Boolean functions depending on all $n$
variables. We prove that for any $f\in \mathcal{F}_{n}^*$, $f|_{x_i=0}$ or
$f|_{x_i=1}$ depends on the remaining $n-1$ variables, for some variable $x_i$.
This existent result suggests a possible way to deal with general Boolean
functions via its subfunctions of some restrictions.
  As an application, we consider the degree lower bound of representing
polynomials over finite rings. Let $f\in \mathcal{F}_{n}^*$ and denote the
exact representing degree over the ring $\mathbb{Z}_m$ (with the integer $m>2$)
as $d_m(f)$. Let $m=\Pi_{i=1}^{r}p_i^{e_i}$, where $p_i$'s are distinct primes,
and $r$ and $e_i$'s are positive integers. If $f$ is symmetric, then $m\cdot
d_{p_1^{e_1}}(f)... d_{p_r^{e_r}}(f) > n$. If $f$ is non-symmetric, by the
second moment method we prove almost always $m\cdot d_{p_1^{e_1}}(f)...
d_{p_r^{e_r}}(f) > \lg{n}-1$. In particular, as $m=pq$ where $p$ and $q$ are
arbitrary distinct primes, we have $d_p(f)d_q(f)=\Omega(n)$ for symmetric $f$
and $d_p(f)d_q(f)=\Omega(\lg{n}-1)$ almost always for non-symmetric $f$. Hence
any $n$-variate symmetric Boolean function can have exact representing degree
$o(\sqrt{n})$ in at most one finite field, and for non-symmetric functions,
with $o(\sqrt{\lg{n}})$-degree in at most one finite field."
"A {+,x}-circuit counts a given multivariate polynomial f, if its values on
0-1 inputs are the same as those of f; on other inputs the circuit may output
arbitrary values. Such a circuit counts the number of monomials of f evaluated
to 1 by a given 0-1 input vector (with multiplicities given by their
coefficients). A circuit decides $f$ if it has the same 0-1 roots as f. We
first show that some multilinear polynomials can be exponentially easier to
count than to compute them, and can be exponentially easier to decide than to
count them. Then we give general lower bounds on the size of counting circuits."
"The connection between constraint languages and clone theory has been a
fruitful line of research on the complexity of constraint satisfaction
problems. In a recent result, Cohen et al. [SICOMP'13] have characterised a
Galois connection between valued constraint languages and so-called weighted
clones. In this paper, we study the structure of weighted clones. We extend the
results of Creed and Zivny from [CP'11/SICOMP'13] on types of weightings
necessarily contained in every nontrivial weighted clone. This result has
immediate computational complexity consequences as it provides necessary
conditions for tractability of weighted clones and thus valued constraint
languages. We demonstrate that some of the necessary conditions are also
sufficient for tractability, while others are provably not."
"In this paper was explored well known model k-OBDD. There are proven width
based hierarchy of classes of boolean functions which computed by k-OBDD. The
proof of hierarchy is based on sufficient condition of Boolean function's non
representation as k-OBDD and complexity properties of Boolean function SAF.
This function is modification of known Pointer Jumping (PJ) and Indirect
Storage Access (ISA) functions."
"Powerful skew arithmetic circuits are introduced. These are skew arithmetic
circuits with variables, where input gates can be labelled with powers $x^n$
for binary encoded numbers $n$. It is shown that polynomial identity testing
for powerful skew arithmetic circuits belongs to $\mathsf{coRNC}^2$, which
generalizes a corresponding result for (standard) skew circuits. Two
applications of this result are presented: (i) Equivalence of
higher-dimensional straight-line programs can be tested in $\mathsf{coRNC}^2$;
this result is even new in the one-dimensional case, where the straight-line
programs produce strings. (ii) The compressed word problem (or circuit
evaluation problem) for certain wreath products of finitely generated abelian
groups belongs to $\mathsf{coRNC}^2$."
"We prove an exponential lower bound for general circuits computing the clique
function and hereby confirm that NP != P."
"In this paper we provide an extended formulation for the class of constraint
satisfaction problems and prove that its size is polynomial for instances whose
constraint graph has bounded treewidth. This implies new upper bounds on
extension complexity of several important NP-hard problems on graphs of bounded
treewidth."
"We investigate the complexity of three optimization problems in Boolean
propositional logic related to information theory: Given a conjunctive formula
over a set of relations, find a satisfying assignment with minimal Hamming
distance to a given assignment that satisfies the formula
($\mathsf{NeareastOtherSolution}$, $\mathsf{NOSol}$) or that does not need to
satisfy it ($\mathsf{NearestSolution}$, $\mathsf{NSol}$). The third problem
asks for two satisfying assignments with a minimal Hamming distance among all
such assignments ($\mathsf{MinSolutionDistance}$, $\mathsf{MSD}$).
  For all three problems we give complete classifications with respect to the
relations admitted in the formula. We give polynomial time algorithms for
several classes of constraint languages. For all other cases we prove hardness
or completeness regarding APX, APX, NPO, or equivalence to well-known hard
optimization problems."
"We provide a general framework to remove short advice by formulating the
following computational task for a function $f$: given two oracles at least one
of which is honest (i.e. correctly computes $f$ on all inputs) as well as an
input, the task is to compute $f$ on the input with the help of the oracles by
a probabilistic polynomial-time machine, which we shall call a selector. We
characterize the languages for which short advice can be removed by the notion
of selector: a paddable language has a selector if and only if short advice of
a probabilistic machine that accepts the language can be removed under any
relativized world. Previously, instance checkers have served as a useful tool
to remove short advice of probabilistic computation. We indicate that existence
of instance checkers is a property stronger than that of removing short advice:
although no instance checker for ${\rm EXP}^{\rm NP}$-complete languages exists
unless ${\rm EXP}^{\rm NP} = {\rm NEXP}$, we prove that there exists a selector
for any ${\rm EXP}^{\rm NP}$-complete language, by building on the proof of
${\rm MIP} = {\rm NEXP}$ by Babai, Fortnow, and Lund (1991)."
"In this paper with two equivalent representations of the information
contained by a SAT formula, the reason why string generated by succinct SAT
formula can be greatly compressed is firstly presented based on Kolmogorov
complexity theory. Then what strings can be greatly compressed were classified
and discussed. In this way we discovered the SAT problem was composed of a
basic distinguish problem: distinguish two different distributions induced
under the computer with certain SAT formula ensemble. We then tried to map this
problem into quantum mechanics, or the quantum version basic distinguish
problem: this time two different distributions are induced under quantum
mechanics. Based on the equivalence of statistical distance between probability
space and Hilbert space, in the same time this distance is invariant under all
unitary transformations. The quantum version basic problem cannot be
efficiently solved by any quantum computer. In the worst case, any quantum
computer must perform exponential times measurement in order to solve it. In
the end we proposed the main theorem : The statistical distance in program
space and probability space are identical. We tried to prove it using the
relationship of Kolmogorov complexity and entropy. It showed there is no
difference to solve the basic problem in SAT formula space or probability
space. In the worst case, exponential trials must be performed to solve it.
NP!=P."
"In this paper, we show that the problem of determining whether one player can
force a win in a multiplayer version of the children's card game War is
PSPACE-hard. The same reduction shows that a related problem, asking whether a
player can survive k rounds, is PSPACE-complete when k is polynomial in the
size of the input."
"Comparator circuit model was originally introduced by Mayr and Subramanian
(1992) (and further studied by Cook, Filmus and Le (2012)) to capture problems
which are not known to be P-complete but still not known to admit efficient
parallel algorithms. The class CC is the complexity class of problems many-one
logspace reducible to the Comparator Circuit Value Problem and we know that NL
is contained in CC which is inturn contained in P. Cook, Filmus and Le (2012)
showed that CC is also the class of languages decided by polynomial size
comparator circuits.
  We study generalizations of the comparator circuit model that work over fixed
finite bounded posets. We observe that there are universal comparator circuits
even over arbitrary fixed finite bounded posets. Building on this, we show that
general (resp. skew) comparator circuits of polynomial size over fixed finite
distributive lattices characterizes CC (resp. L). Complementing this, we show
that general comparator circuits of polynomial size over arbitrary fixed finite
lattices exactly characterizes P even when the comparator circuit is skew. In
addition, we show a characterization of the class NP by a family of polynomial
sized comparator circuits over fixed {\em finite bounded posets}. These results
generalize the results by Cook, Filmus and Le (2012) regarding the power of
comparator circuits. As an aside, we consider generalizations of Boolean
formulae over arbitrary lattices. We show that Spira's theorem (1971) can be
extended to this setting as well and show that polynomial sized Boolean
formulae over finite fixed lattices capture exactly NC^1."
"The vertex cover problem is one of the most important and intensively studied
combinatorial optimization problems. Khot and Regev (2003) proved that the
problem is NP-hard to approximate within a factor $2 - \epsilon$, assuming the
Unique Games Conjecture (UGC). This is tight because the problem has an easy
2-approximation algorithm. Without resorting to the UGC, the best
inapproximability result for the problem is due to Dinur and Safra (2002):
vertex cover is NP-hard to approximate within a factor 1.3606. We prove the
following unconditional result about linear programming (LP) relaxations of the
problem: every LP relaxation that approximates vertex cover within a factor
$2-\epsilon$ has super-polynomially many inequalities. As a direct consequence
of our methods, we also establish that LP relaxations (as well as SDP
relaxations) that approximate the independent set problem within any constant
factor have super-polynomial size."
"We systematically study the computational complexity of a broad class of
computational problems in phylogenetic reconstruction. The class contains for
example the rooted triple consistency problem, forbidden subtree problems, the
quartet consistency problem, and many other problems studied in the
bioinformatics literature. The studied problems can be described as constraint
satisfaction problems where the constraints have a first-order definition over
the rooted triple relation. We show that every such phylogeny problem can be
solved in polynomial time or is NP-complete. On the algorithmic side, we
generalize a well-known polynomial-time algorithm of Aho, Sagiv, Szymanski, and
Ullman for the rooted triple consistency problem. Our algorithm repeatedly
solves linear equation systems to construct a solution in polynomial time. We
then show that every phylogeny problem that cannot be solved by our algorithm
is NP-complete. Our classification establishes a dichotomy for a large class of
infinite structures that we believe is of independent interest in universal
algebra, model theory, and topology. The proof of our main result combines
results and techniques from various research areas: a recent classification of
the model-complete cores of the reducts of the homogeneous binary branching
C-relation, Leeb's Ramsey theorem for rooted trees, and universal algebra."
"Sensitivity, block sensitivity and certificate complexity are basic
complexity measures of Boolean functions. The famous sensitivity conjecture
claims that sensitivity is polynomially related to block sensitivity. However,
it has been notoriously hard to obtain even exponential bounds. Since block
sensitivity is known to be polynomially related to certificate complexity, an
equivalent of proving this conjecture would be showing that certificate
complexity is polynomially related to sensitivity. Previously, it has been
shown that $bs(f) \leq C(f) \leq 2^{s(f)-1} s(f) - (s(f)-1)$. In this work, we
give a better upper bound of $bs(f) \leq C(f) \leq
\max\left(2^{s(f)-1}\left(s(f)-\frac 1 3\right), s(f)\right)$ using a recent
theorem limiting the structure of function graphs. We also examine relations
between these measures for functions with small 1-sensitivity $s_1(f)$ and
arbitrary 0-sensitivity $s_0(f)$."
"The permanent vs. determinant problem is one of the most important problems
in theoretical computer science, and is the main target of geometric complexity
theory proposed by Mulmuley and Sohoni. The current best lower bound for the
determinantal complexity of the d by d permanent polynomial is d^2/2, due to
Mignon and Ressayre in 2004. Inspired by their proof method, we introduce a
natural rank concept of polynomials, called the bi-polynomial rank. The
bi-polynomial rank is related to width of an arithmetic branching program. The
bi-polynomial rank of a homogeneous polynomial p of even degree 2k is defined
as the minimum n such that p can be written as a summation of n products of
polynomials of degree k. We prove that the bi-polynomial rank gives a lower
bound of the determinantal complexity. As a consequence, the above Mignon and
Ressayre bound is improved to (d-1)^2 + 1 over the field of reals. We show that
the computation of the bi-polynomial rank is formulated as a rank minimization
problem. Applying the concave minimization technique, we reduce the problem of
lower-bounding determinantal complexity to that of proving the positive
semidefiniteness of matrices, and this is a new approach for the permanent vs.
determinant problem. We propose a computational approach for giving a lower
bound of this rank minimization, via techniques of the concave minimization.
This also yields a new strategy to attack the permanent vs. determinant
problem."
"We introduce the idea of an understanding with respect to a set of clauses as
a satisfying truth assignment explained by the contexts of the literals in the
clauses. Following this idea, we present a mechanical process that obtains, if
it exists, an understanding with respect to a 3-SAT problem instance based on
the contexts of each literal in the instance, otherwise it determines that none
exists. We demonstrate that our process is correct and efficient in solving
3-SAT."
"In this short note, the author shows that the gap problem of some $k$-CSPs
with the support of its predicate the ground of a balanced pairwise independent
distribution can be solved by a modified version of Hast's Algorithm BiLin that
calls Charikar\&Wirth's SDP algorithm for two rounds in polynomial time, when
$k$ is sufficiently large, the support of its predicate is combined by the
grounds of three biased homogeneous distributions and the three biases satisfy
certain conditions. To conclude, the author refutes Unique Game Conjecture,
assuming $P\ne NP$."
"We study the problem of indexing irreducible polynomials over finite fields,
and give the first efficient algorithm for this problem. Specifically, we show
the existence of poly(n, log q)-size circuits that compute a bijection between
{1, ... , |S|} and the set S of all irreducible, monic, univariate polynomials
of degree n over a finite field F_q. This has applications in pseudorandomness,
and answers an open question of Alon, Goldreich, H{\aa}stad and Peralta[AGHP].
  Our approach uses a connection between irreducible polynomials and necklaces
( equivalence classes of strings under cyclic rotation). Along the way, we give
the first efficient algorithm for indexing necklaces of a given length over a
given alphabet, which may be of independent interest."
"We show here that every non-adaptive property testing algorithm making a
constant number of queries, over a fixed alphabet, can be converted to a
sample-based (as per [Goldreich and Ron, 2015]) testing algorithm whose average
number of queries is a fixed, smaller than $1$, power of $n$. Since the query
distribution of the sample-based algorithm is not dependent at all on the
property, or the original algorithm, this has many implications in scenarios
where there are many properties that need to be tested for concurrently, such
as testing (relatively large) unions of properties, or converting a
Merlin-Arthur Proximity proof (as per [Gur and Rothblum, 2013]) to a proper
testing algorithm.
  The proof method involves preparing the original testing algorithm for a
combinatorial analysis, which in turn involves a new result about the existence
of combinatorial structures (essentially generalized sunflowers) that allow the
sample-based tester to replace the original constant query complexity tester."
"Yannakakis showed that the matching problem does not have a small symmetric
linear program. Rothvo{\ss} recently proved that any, not necessarily
symmetric, linear program also has exponential size. It is natural to ask
whether the matching problem can be expressed compactly in a framework such as
semidefinite programming (SDP) that is more powerful than linear programming
but still allows efficient optimization. We answer this question negatively for
symmetric SDPs: any symmetric SDP for the matching problem has exponential
size.
  We also show that an O(k)-round Lasserre SDP relaxation for the metric
traveling salesperson problem yields at least as good an approximation as any
symmetric SDP relaxation of size $n^k$.
  The key technical ingredient underlying both these results is an upper bound
on the degree needed to derive polynomial identities that hold over the space
of matchings or traveling salesperson tours."
"These notes contain, among others, a proof that the average running time of
an easy solution to the satisfiability problem for propositional calculus is,
under some reasonable assumptions, linear (with constant 2) in the size of the
input. Moreover, some suggestions are made about criteria for tractability of
complex algorithms. In particular, it is argued that the distribution of
probability on the whole input space of an algorithm constitutes an
non-negligible factor in estimating whether the algorithm is tractable or not."
"The subcube partition model of computation is at least as powerful as
decision trees but no separation between these models was known. We show that
there exists a function whose deterministic subcube partition complexity is
asymptotically smaller than its randomized decision tree complexity, resolving
an open problem of Friedgut, Kahn, and Wigderson (2002). Our lower bound is
based on the information-theoretic techniques first introduced to lower bound
the randomized decision tree complexity of the recursive majority function.
  We also show that the public-coin partition bound, the best known lower bound
method for randomized decision tree complexity subsuming other general
techniques such as block sensitivity, approximate degree, randomized
certificate complexity, and the classical adversary bound, also lower bounds
randomized subcube partition complexity. This shows that all these lower bound
techniques cannot prove optimal lower bounds for randomized decision tree
complexity, which answers an open question of Jain and Klauck (2010) and Jain,
Lee, and Vishnoi (2014)."
"We exhibit families of $4$-CNF formulas over $n$ variables that have
sums-of-squares (SOS) proofs of unsatisfiability of degree (a.k.a. rank) $d$
but require SOS proofs of size $n^{\Omega(d)}$ for values of $d = d(n)$ from
constant all the way up to $n^{\delta}$ for some universal constant$\delta$.
This shows that the $n^{O(d)}$ running time obtained by using the Lasserre
semidefinite programming relaxations to find degree-$d$ SOS proofs is optimal
up to constant factors in the exponent. We establish this result by combining
$\mathsf{NP}$-reductions expressible as low-degree SOS derivations with the
idea of relativizing CNF formulas in [Kraj\'i\v{c}ek '04] and [Dantchev and
Riis'03], and then applying a restriction argument as in [Atserias, M\""uller,
and Oliva '13] and [Atserias, Lauria, and Nordstr\""om '14]. This yields a
generic method of amplifying SOS degree lower bounds to size lower bounds, and
also generalizes the approach in [ALN14] to obtain size lower bounds for the
proof systems resolution, polynomial calculus, and Sherali-Adams from lower
bounds on width, degree, and rank, respectively."
"We prove an average-case depth hierarchy theorem for Boolean circuits over
the standard basis of $\mathsf{AND}$, $\mathsf{OR}$, and $\mathsf{NOT}$ gates.
Our hierarchy theorem says that for every $d \geq 2$, there is an explicit
$n$-variable Boolean function $f$, computed by a linear-size depth-$d$ formula,
which is such that any depth-$(d-1)$ circuit that agrees with $f$ on $(1/2 +
o_n(1))$ fraction of all inputs must have size $\exp({n^{\Omega(1/d)}}).$ This
answers an open question posed by H{\aa}stad in his Ph.D. thesis.
  Our average-case depth hierarchy theorem implies that the polynomial
hierarchy is infinite relative to a random oracle with probability 1,
confirming a conjecture of H{\aa}stad, Cai, and Babai. We also use our result
to show that there is no ""approximate converse"" to the results of Linial,
Mansour, Nisan and Boppana on the total influence of small-depth circuits, thus
answering a question posed by O'Donnell, Kalai, and Hatami.
  A key ingredient in our proof is a notion of \emph{random projections} which
generalize random restrictions."
"We show that it is quasi-NP-hard to color 2-colorable 8-uniform hypergraphs
with $2^{(\log N)^{1/10-o(1)}}$ colors, where $N$ is the number of vertices.
There has been much focus on hardness of hypergraph coloring recently.
Guruswami, H{\aa}stad, Harsha, Srinivasan and Varma showed that it is
quasi-NP-hard to color 2-colorable 8-uniform hypergraphs with
$2^{2^{\Omega(\sqrt{\log\log N})}}$ colors. Their result is obtained by
composing standard Label Cover with an inner-verifier based on Low Degree Long
Code, using Reed-Muller code testing results by Dinur and Guruswami. Using a
different approach, Khot and Saket constructed a new variant of Label Cover,
and composed it with Quadratic Code to show quasi-NP-hardness of coloring
2-colorable 12-uniform hypergraphs with $2^{(\log N)^c}$ colors, for some $c$
around 1/20. Their construction of Label Cover is based on a new notion of
superposition complexity for CSP instances. The composition with inner-verifier
was subsequently improved by Varma, giving the same hardness result for
8-uniform hypergraphs.
  Our construction uses both Quadratic Code and Low Degree Long Code, and
builds upon the work by Khot and Saket. We present a different approach to
construct CSP instances with superposition hardness by observing that when the
number of assignments is odd, satisfying a constraint in superposition is the
same as ""odd-covering"" the constraint. We employ Low Degree Long Code in order
to keep the construction efficient. In the analysis, we also adapt and
generalize one of the key theorems by Dinur and Guruswami in the context of
analyzing probabilistically checkable proof systems."
"For Boolean functions computed by read-once, depth-$D$ circuits with
unbounded fan-in over the de Morgan basis, we present an explicit pseudorandom
generator with seed length $\tilde{O}(\log^{D+1} n)$. The previous best seed
length known for this model was $\tilde{O}(\log^{D+4} n)$, obtained by Trevisan
and Xue (CCC `13) for all of $AC^0$ (not just read-once). Our work makes use of
Fourier analytic techniques for pseudorandomness introduced by Reingold,
Steinke, and Vadhan (RANDOM `13) to show that the generator of Gopalan et al.
(FOCS `12) fools read-once $AC^0$. To this end, we prove a new Fourier growth
bound for read-once circuits, namely that for every $F: \{0,1\}^n\to\{0,1\}$
computed by a read-once, depth-$D$ circuit,
\begin{equation*}\sum_{s\subseteq[n], |s|=k}|\hat{F}[s]|\le
O(\log^{D-1}n)^k,\end{equation*} where $\hat{F}$ denotes the Fourier transform
of $F$ over $\mathbb{Z}^n_2$."
"A recent result of Moshkovitz \cite{Moshkovitz14} presented an ingenious
method to provide a completely elementary proof of the Parallel Repetition
Theorem for certain projection games via a construction called fortification.
However, the construction used in \cite{Moshkovitz14} to fortify arbitrary
label cover instances using an arbitrary extractor is insufficient to prove
parallel repetition. In this paper, we provide a fix by using a stronger graph
that we call fortifiers. Fortifiers are graphs that have both $\ell_1$ and
$\ell_2$ guarantees on induced distributions from large subsets. We then show
that an expander with sufficient spectral gap, or a bi-regular extractor with
stronger parameters (the latter is also the construction used in an independent
update \cite{Moshkovitz15} of \cite{Moshkovitz14} with an alternate argument),
is a good fortifier. We also show that using a fortifier (in particular
$\ell_2$ guarantees) is necessary for obtaining the robustness required for
fortification."
"In this work, we construct the first locally-correctable codes (LCCs), and
locally-testable codes (LTCs) with constant rate, constant relative distance,
and sub-polynomial query complexity. Specifically, we show that there exist
binary LCCs and LTCs with block length $n$, constant rate (which can even be
taken arbitrarily close to 1), constant relative distance, and query complexity
$\exp(\tilde{O}(\sqrt{\log n}))$. Previously such codes were known to exist
only with $\Omega(n^{\beta})$ query complexity (for constant $\beta > 0$), and
there were several, quite different, constructions known.
  Our codes are based on a general distance-amplification method of Alon and
Luby~\cite{AL96_codes}. We show that this method interacts well with local
correctors and testers, and obtain our main results by applying it to suitably
constructed LCCs and LTCs in the non-standard regime of \emph{sub-constant
relative distance}.
  Along the way, we also construct LCCs and LTCs over large alphabets, with the
same query complexity $\exp(\tilde{O}(\sqrt{\log n}))$, which additionally have
the property of approaching the Singleton bound: they have almost the
best-possible relationship between their rate and distance. This has the
surprising consequence that asking for a large alphabet error-correcting code
to further be an LCC or LTC with $\exp(\tilde{O}(\sqrt{\log n}))$ query
complexity does not require any sacrifice in terms of rate and distance! Such a
result was previously not known for any $o(n)$ query complexity.
  Our results on LCCs also immediately give locally-decodable codes (LDCs) with
the same parameters."
"Given a graph $G$ with a total order defined on its vertices, the Maximum
Pagenumber-$k$ Subgraph Problem asks for a maximum subgraph $G'$ of $G$ such
that $G'$ can be embedded into a $k$-book when the vertices are placed on the
spine according to the specified total order. We show that this problem is
NP-complete for $k \geq 2$."
"We study the complexity of representing polynomials as a sum of products of
polynomials in few variables. More precisely, we study representations of the
form $$P = \sum_{i = 1}^T \prod_{j = 1}^d Q_{ij}$$ such that each $Q_{ij}$ is
an arbitrary polynomial that depends on at most $s$ variables. We prove the
following results.
  1. Over fields of characteristic zero, for every constant $\mu$ such that $0
\leq \mu < 1$, we give an explicit family of polynomials $\{P_{N}\}$, where
$P_{N}$ is of degree $n$ in $N = n^{O(1)}$ variables, such that any
representation of the above type for $P_{N}$ with $s = N^{\mu}$ requires $Td
\geq n^{\Omega(\sqrt{n})}$. This strengthens a recent result of Kayal and Saha
[KS14a] which showed similar lower bounds for the model of sums of products of
linear forms in few variables. It is known that any asymptotic improvement in
the exponent of the lower bounds (even for $s = \sqrt{n}$) would separate VP
and VNP[KS14a].
  2. We obtain a deterministic subexponential time blackbox polynomial identity
testing (PIT) algorithm for circuits computed by the above model when $T$ and
the individual degree of each variable in $P$ are at most $\log^{O(1)} N$ and
$s \leq N^{\mu}$ for any constant $\mu < 1/2$. We get quasipolynomial running
time when $s < \log^{O(1)} N$. The PIT algorithm is obtained by combining our
lower bounds with the hardness-randomness tradeoffs developed in [DSY09, KI04].
To the best of our knowledge, this is the first nontrivial PIT algorithm for
this model (even for the case $s=2$), and the first nontrivial PIT algorithm
obtained from lower bounds for small depth circuits."
"We continue the study of communication cost of computing functions when
inputs are distributed among $k$ processors, each of which is located at one
vertex of a network/graph called a terminal. Every other node of the network
also has a processor, with no input. The communication is point-to-point and
the cost is the total number of bits exchanged by the protocol, in the worst
case, on all edges.
  Chattopadhyay, Radhakrishnan and Rudra (FOCS'14) recently initiated a study
of the effect of topology of the network on the total communication cost using
tools from $L_1$ embeddings. Their techniques provided tight bounds for simple
functions like Element-Distinctness (ED), which depend on the 1-median of the
graph. This work addresses two other kinds of natural functions. We show that
for a large class of natural functions like Set-Disjointness the communication
cost is essentially $n$ times the cost of the optimal Steiner tree connecting
the terminals. Further, we show for natural composed functions like $\text{ED}
\circ \text{XOR}$ and $\text{XOR} \circ \text{ED}$, the naive protocols
suggested by their definition is optimal for general networks. Interestingly,
the bounds for these functions depend on more involved topological parameters
that are a combination of Steiner tree and 1-median costs.
  To obtain our results, we use some new tools in addition to ones used in
Chattopadhyay et. al. These include (i) viewing the communication constraints
via a linear program; (ii) using tools from the theory of tree embeddings to
prove topology sensitive direct sum results that handle the case of composed
functions and (iii) representing the communication constraints of certain
problems as a family of collection of multiway cuts, where each multiway cut
simulates the hardness of computing the function on the star topology."
"Nondeterministic circuits are a nondeterministic computation model in circuit
complexity theory. In this paper, we prove a $3(n-1)$ lower bound for the size
of nondeterministic $U_2$-circuits computing the parity function. It is known
that the minimum size of (deterministic) $U_2$-circuits computing the parity
function exactly equals $3(n-1)$. Thus, our result means that nondeterministic
computation is useless to compute the parity function by $U_2$-circuits and
cannot reduce the size from $3(n-1)$. To the best of our knowledge, this is the
first nontrivial lower bound for the size of nondeterministic circuits
(including formulas, constant depth circuits, and so on) with unlimited
nondeterminism for an explicit Boolean function. We also discuss an approach to
proving lower bounds for the size of deterministic circuits via lower bounds
for the size of nondeterministic restricted circuits."
"Information complexity is the interactive analogue of Shannon's classical
information theory. In recent years this field has emerged as a powerful tool
for proving strong communication lower bounds, and for addressing some of the
major open problems in communication complexity and circuit complexity. A
notable achievement of information complexity is the breakthrough in
understanding of the fundamental direct sum and direct product conjectures,
which aim to quantify the power of parallel computation. This survey provides a
brief introduction to information complexity, and overviews some of the recent
progress on these conjectures and their tight relationship with the fascinating
problem of compressing interactive protocols."
"In this work, we critique two papers, ""A Polynomial-Time Solution to the
Clique Problem"" by Tamta, Pande, and Dhami, and ""A Polynomial-Time Algorithm
For Solving Clique Problems"" by LaPlante. We summarize and analyze both papers,
noting that the algorithms presented in both papers are flawed. We conclude
that neither author has successfully established that P = NP."
"We show that, assuming the (deterministic) Exponential Time Hypothesis,
distinguishing between a graph with an induced $k$-clique and a graph in which
all k-subgraphs have density at most $1-\epsilon$, requires $n^{\tilde
\Omega(log n)}$ time. Our result essentially matches the quasi-polynomial
algorithms of Feige and Seltser [FS97] and Barman [Bar15] for this problem, and
is the first one to rule out an additive PTAS for Densest $k$-Subgraph. We
further strengthen this result by showing that our lower bound continues to
hold when, in the soundness case, even subgraphs smaller by a near-polynomial
factor ($k' = k 2^{-\tilde \Omega (log n)}$) are assumed to be at most
($1-\epsilon$)-dense.
  Our reduction is inspired by recent applications of the ""birthday repetition""
technique [AIM14,BKW15]. Our analysis relies on information theoretical
machinery and is similar in spirit to analyzing a parallel repetition of
two-prover games in which the provers may choose to answer some challenges
multiple times, while completely ignoring other challenges."
"Interactive proofs (IP) model a world where a verifier delegates computation
to an untrustworthy prover, verifying the prover's claims before accepting
them. IP protocols have applications in areas such as verifiable computation
outsourcing, computation delegation, cloud computing. In these applications,
the verifier may pay the prover based on the quality of his work. Rational
interactive proofs (RIP), introduced by Azar and Micali (2012), are an
interactive-proof system with payments, in which the prover is rational rather
than untrustworthy---he may lie, but only to increase his payment. Rational
proofs leverage the provers' rationality to obtain simple and efficient
protocols. Azar and Micali show that RIP=IP(=PSAPCE). They leave the question
of whether multiple provers are more powerful than a single prover for rational
and classical proofs as an open problem.
  In this paper, we introduce multi-prover rational interactive proofs (MRIP).
Here, a verifier cross-checks the provers' answers with each other and pays
them according to the messages exchanged. The provers are cooperative and
maximize their total expected payment if and only if the verifier learns the
correct answer to the problem. We further refine the model of MRIP to
incorporate utility gap, which is the loss in payment suffered by provers who
mislead the verifier to the wrong answer.
  We define the class of MRIP protocols with constant, noticeable and
negligible utility gaps. We give tight characterization for all three MRIP
classes. On the way, we resolve Azar and Micali's open problem---under standard
complexity-theoretic assumptions, MRIP is not only more powerful than RIP, but
also more powerful than MIP; and this is true even the utility gap is required
to be constant. We further show that the full power of each MRIP class can be
achieved using only two provers and three rounds."
"NP complete problem is one of the most challenging issues. The question of
whether all problems in NP are also in P is generally considered one of the
most important open questions in mathematics and theoretical computer science
as it has far-reaching consequences to other problems in mathematics, computer
science, biology, philosophy and cryptography. There are intensive research on
proving `NP not equal to P' and `NP equals to P'. However, none of the `proved'
results is commonly accepted by the research community up to now. In this
paper, instead of proving either one, we aim to provide new perspective:
transforming two typical NP complete problems to exactly solvable P problems in
polynomial time. This approach helps to solve originally NP complete problems
with practical applications. It may shine light on solving other NP complete
problems in similar way."
"We prove that any oblivious algorithm using space $S$ to find the median of a
list of $n$ integers from $\{1,...,2n\}$ requires time $\Omega(n \log\log_S
n)$. This bound also applies to the problem of determining whether the median
is odd or even. It is nearly optimal since Chan, following Munro and Raman, has
shown that there is a (randomized) selection algorithm using only $s$
registers, each of which can store an input value or $O(\log n)$-bit counter,
that makes only $O(\log\log_s n)$ passes over the input. The bound also implies
a size lower bound for read-once branching programs computing the low order bit
of the median and implies the analog of $P \ne NP \cap coNP$ for length $o(n
\log\log n)$ oblivious branching programs."
"We present and study new definitions of universal and programmable universal
unary functions and consider a new simplicity criterion: almost decidability of
the halting set. A set of positive integers S is almost decidable if there
exists a decidable and generic (i.e. a set of natural density one) set whose
intersection with S is decidable. Every decidable set is almost decidable, but
the converse implication is false. We prove the existence of infinitely many
universal functions whose halting sets are generic (negligible, i.e. have
density zero) and (not) almost decidable. One result - namely, the existence of
infinitely many universal functions whose halting sets are generic (negligible)
and not almost decidable - solves an open problem in [9]. We conclude with some
open problems."
"We study average-case complexity of branch-and-bound for maximum independent
set in random graphs under the $\mathcal{G}(n,p)$ distribution. In this model
every pair $(u,v)$ of vertices belongs to $E$ with probability $p$
independently on the existence of any other edge. We make a precise case
analysis, providing phase transitions between subexponential and exponential
complexities depending on the probability $p$ of the random model."
"The hypergraph transversal problem has been intensively studied, from both a
theoretical and a practical point of view. In particular , its incremental
complexity is known to be quasi-polynomial in general and polynomial for
bounded hypergraphs. Recent applications in computational biology however
require to solve a generalization of this problem, that we call bi-objective
transversal problem. The instance is in this case composed of a pair of
hypergraphs (A, B), and the aim is to find minimal sets which hit all the
hyperedges of A while intersecting a minimal set of hyperedges of B. In this
paper, we formalize this problem, link it to a problem on monotone boolean
$\land$ -- $\lor$ formulae of depth 3 and study its incremental complexity."
"We show that every language in NP has a PCP verifier that tosses $O(\log n)$
random coins, has perfect completeness, and a soundness error of at most
$1/\text{poly}(n)$, while making at most $O(\text{poly}\log\log n)$ queries
into a proof over an alphabet of size at most $n^{1/\text{poly}\log\log n}$.
Previous constructions that obtain $1/\text{poly}(n)$ soundness error used
either $\text{poly}\log n $ queries or an exponential sized alphabet, i.e. of
size $2^{n^c}$ for some $c>0$. Our result is an exponential improvement in both
parameters simultaneously.
  Our result can be phrased as a polynomial-gap hardness for approximate CSPs
with arity $\text{poly}\log\log n$ and alphabet size $n^{1/\text{poly}\log n}$.
The ultimate goal, in this direction, would be to prove polynomial hardness for
CSPs with constant arity and polynomial alphabet size (aka the sliding scale
conjecture for inverse polynomial soundness error).
  Our construction is based on a modular generalization of previous PCP
constructions in this parameter regime, which involves a composition theorem
that uses an extra `consistency' query but maintains the inverse polynomial
relation between the soundness error and the alphabet size.
  Our main technical/conceptual contribution is a new notion of soundness,
which we refer to as {\em distributional soundness}, that replaces the previous
notion of ""list decoding soundness"", and that allows us to prove a modular
composition theorem with tighter parameters. This new notion of soundness
allows us to invoke composition a super-constant number of times without
incurring a blow-up in the soundness error."
"Poset games have been the object of mathematical study for over a century,
but little has been written on the computational complexity of determining
important properties of these games. In this introduction we develop the
fundamentals of combinatorial game theory and focus for the most part on poset
games, of which Nim is perhaps the best-known example. We present the
complexity results known to date, some discovered very recently."
"A semilinear relation is a finite union of finite intersections of open and
closed half-spaces over, for instance, the reals, the rationals, or the
integers. Semilinear relations have been studied in connection with algebraic
geometry, automata theory, and spatiotemporal reasoning. We consider semilinear
relations over the rationals and the reals. Under this assumption, the
computational complexity of the constraint satisfaction problem (CSP) is known
for all finite sets containing R+={(x,y,z) | x+y=z}, <=, and {1}. These
problems correspond to expansions of the linear programming feasibility
problem. We generalise this result and fully determine the complexity for all
finite sets of semilinear relations containing R+. This is accomplished in part
by introducing an algorithm, based on computing affine hulls, which solves a
new class of semilinear CSPs in polynomial time. We further analyse the
complexity of linear optimisation over the solution set and the existence of
integer solutions."
"In this paper we study interactive ""one-shot"" analogues of the classical
Slepian-Wolf theorem. Alice receives a value of a random variable $X$, Bob
receives a value of another random variable $Y$ that is jointly distributed
with $X$. Alice's goal is to transmit $X$ to Bob (with some error probability
$\varepsilon$). Instead of one-way transmission, which is studied in the
classical coding theory, we allow them to interact. They may also use shared
randomness.
  We show, that Alice can transmit $X$ to Bob in expected $H(X|Y) +
2\sqrt{H(X|Y)} + O(\log_2\left(\frac{1}{\varepsilon}\right))$ number of bits.
Moreover, we show that every one-round protocol $\pi$ with information
complexity $I$ can be compressed to the (many-round) protocol with expected
communication about $I + 2\sqrt{I}$ bits. This improves a result by Braverman
and Rao \cite{braverman2011information}, where they had $5\sqrt{I}$. Further,
we show how to solve this problem (transmitting $X$) using $3H(X|Y) +
O(\log_2\left(\frac{1}{\varepsilon}\right))$ bits and $4$ rounds on average.
This improves a result of~\cite{brody2013towards}, where they had $4H(X|Y) +
O(\log1/\varepsilon)$ bits and 10 rounds on average.
  In the end of the paper we discuss how many bits Alice and Bob may need to
communicate on average besides $H(X|Y)$. The main question is whether the upper
bounds mentioned above are tight. We provide an example of $(X, Y)$, such that
transmission of $X$ from Alice to Bob with error probability $\varepsilon$
requires $H(X|Y) + \Omega\left(\log_2\left(\frac{1}{\varepsilon}\right)\right)$
bits on average."
"We initiate a study of a relaxed version of the standard Erdos-Renyi random
graph model, where each edge may depend on a few other edges. We call such
graphs ""dependent random graphs"". Our main result in this direction is a
thorough understanding of the clique number of dependent random graphs. We also
obtain bounds for the chromatic number. Surprisingly, many of the standard
properties of random graphs also hold in this relaxed setting. We show that
with high probability, a dependent random graph will contain a clique of size
$\frac{(1-o(1))\log n}{\log(1/p)}$, and the chromatic number will be at most
$\frac{n \log(1/1-p)}{\log n}$.
  As an application and second main result, we give a new communication
protocol for the k-player Multiparty Pointer Jumping (MPJ_k) problem in the
number-on-the-forehead (NOF) model. Multiparty Pointer Jumping is one of the
canonical NOF communication problems, yet even for three players, its
communication complexity is not well understood. Our protocol for MPJ_3 costs
$O(\frac{n\log\log n}{\log n})$ communication, improving on a bound of Brody
and Chakrabarti [BC08]. We extend our protocol to the non-Boolean pointer
jumping problem $\widehat{MPJ}_k$, achieving an upper bound which is o(n) for
any $k >= 4$ players. This is the first o(n) bound for $\widehat{MPJ}_k$ and
improves on a bound of Damm, Jukna, and Sgall [DJS98] which has stood for
almost twenty years."
"We study the design of fixed-parameter algorithms for problems already known
to be solvable in polynomial time. The main motivation is to get more efficient
algorithms for problems with unattractive polynomial running times. Here, we
focus on a fundamental graph problem: Longest Path, that is, given an
undirected graph, find a maximum-length path in $G$. Longest Path is NP-hard in
general but known to be solvable in $O(n^{4})$ time on $n$-vertex interval
graphs. We show how to solve Longest Path on Interval Graphs, parameterized by
vertex deletion number $k$ to proper interval graphs, in $O(k^{9}n)$ time.
Notably, Longest Path is trivially solvable in linear time on proper interval
graphs, and the parameter value $k$ can be approximated up to a factor of 4 in
linear time. From a more general perspective, we believe that using
parameterized complexity analysis may enable a refined understanding of
efficiency aspects for polynomial-time solvable problems similarly to what
classical parameterized complexity analysis does for NP-hard problems."
"In this note, we study the relation between the parity decision tree
complexity of a boolean function $f$, denoted by $\mathrm{D}_{\oplus}(f)$, and
the $k$-party number-in-hand multiparty communication complexity of the XOR
functions $F(x_1,\ldots, x_k)= f(x_1\oplus\cdots\oplus x_k)$, denoted by
$\mathrm{CC}^{(k)}(F)$. It is known that $\mathrm{CC}^{(k)}(F)\leq
k\cdot\mathrm{D}_{\oplus}(f)$ because the players can simulate the parity
decision tree that computes $f$. In this note, we show that
\[\mathrm{D}_{\oplus}(f)\leq O\big(\mathrm{CC}^{(4)}(F)^5\big).\] Our main tool
is a recent result from additive combinatorics due to Sanders. As
$\mathrm{CC}^{(k)}(F)$ is non-decreasing as $k$ grows, the parity decision tree
complexity of $f$ and the communication complexity of the corresponding
$k$-argument XOR functions are polynomially equivalent whenever $k\geq 4$.
  Remark: After the first version of this paper was finished, we discovered
that Hatami and Lovett had already discovered the same result a few years ago,
without writing it up."
"A semilinear relation S is max-closed if it is preserved by taking the
componentwise maximum. The constraint satisfaction problem for max-closed
semilinear constraints is at least as hard as determining the winner in Mean
Payoff Games, a notorious problem of open computational complexity. Mean Payoff
Games are known to be in the intersection of NP and co-NP, which is not known
for max-closed semilinear constraints. Semilinear relations that are max-closed
and additionally closed under translations have been called tropically convex
in the literature. One of our main results is a new duality for open tropically
convex relations, which puts the CSP for tropically convex semilinaer
constraints in general into NP intersected co-NP. This extends the
corresponding complexity result for scheduling under and-or precedence
constraints, or equivalently the max-atoms problem. To this end, we present a
characterization of max-closed semilinear relations in terms of syntactically
restricted first-order logic, and another characterization in terms of a finite
set of relations L that allow primitive positive definitions of all other
relations in the class. We also present a subclass of max-closed constraints
where the CSP is in P; this class generalizes the class of max-closed
constraints over finite domains, and the feasibility problem for max-closed
linear inequalities. Finally, we show that the class of max-closed semilinear
constraints is maximal in the sense that as soon as a single relation that is
not max-closed is added to L, the CSP becomes NP-hard."
"We present a new approach to constructing unconditional pseudorandom
generators against classes of functions that involve computing a linear
function of the inputs. We give an explicit construction of a pseudorandom
generator that fools the discrete Fourier transforms of linear functions with
seed-length that is nearly logarithmic (up to polyloglog factors) in the input
size and the desired error parameter. Our result gives a single pseudorandom
generator that fools several important classes of tests computable in logspace
that have been considered in the literature, including halfspaces (over general
domains), modular tests and combinatorial shapes. For all these classes, our
generator is the first that achieves near logarithmic seed-length in both the
input length and the error parameter. Getting such a seed-length is a natural
challenge in its own right, which needs to be overcome in order to derandomize
RL - a central question in complexity theory.
  Our construction combines ideas from a large body of prior work, ranging from
a classical construction of [NN93] to the recent gradually increasing
independence paradigm of [KMN11, CRSW13, GMRTV12], while also introducing some
novel analytic machinery which might find other applications."
"There are a number of results saying that for certain ""path-following""
algorithms that solve PPAD-complete problems, the solution obtained by the
algorithm is PSPACE-complete to compute. We conjecture that these results are
special cases of a much more general principle, that all such algorithms
compute PSPACE-complete solutions. Such a general result might shed new light
on the complexity class PPAD.
  In this paper we present a new PSPACE-completeness result for an interesting
challenge instance for this conjecture. Chen and Deng~\cite{CD} showed that it
is PPAD-complete to find a trichromatic triangle in a concisely-represented
Sperner triangulation. The proof of Sperner's lemma --- that such a solution
always exists --- identifies one solution in particular, that is found via a
natural ""path-following"" approach. Here we show that it is PSPACE-complete to
compute this specific solution, together with a similar result for the
computation of the path-following solution of two-dimensional discrete Brouwer
functions."
"We prove that deciding whether a given input word contains as subsequence
every possible permutation of integers $\{1,2,\ldots,n\}$ is coNP-complete. The
coNP-completeness holds even when given the guarantee that the input word
contains as subsequences all of length $n-1$ sequences over the same set of
integers. We also show NP-completeness of a related problem of Partially
Non-crossing Perfect Matching in Bipartite Graphs, i.e. to find a perfect
matching in an ordered bipartite graph where edges of the matching incident to
selected vertices (even only from one side) are non-crossing."
"We show that there exists a Boolean function $F$ which observes the following
separations among deterministic query complexity $(D(F))$, randomized zero
error query complexity $(R_0(F))$ and randomized one-sided error query
complexity $(R_1(F))$: $R_1(F) = \widetilde{O}(\sqrt{D(F)})$ and
$R_0(F)=\widetilde{O}(D(F))^{3/4}$. This refutes the conjecture made by Saks
and Wigderson that for any Boolean function $f$,
$R_0(f)=\Omega({D(f)})^{0.753..}$. This also shows widest separation between
$R_1(f)$ and $D(f)$ for any Boolean function. The function $F$ was defined by
G{\""{o}}{\""{o}}s, Pitassi and Watson who studied it for showing a separation
between deterministic decision tree complexity and unambiguous
non-deterministic decision tree complexity. Independently of us, Ambainis et al
proved that different variants of the function $F$ certify optimal (quadratic)
separation between $D(f)$ and $R_0(f)$, and polynomial separation between
$R_0(f)$ and $R_1(f)$. Viewed as separation results, our results are subsumed
by those of Ambainis et al. However, while the functions considerd in the work
of Ambainis et al are different variants of $F$, we work with the original
function $F$ itself."
"We give an $5\cdot n^{\log_{30}5}$ upper bund on the complexity of the
communication game introduced by G. Gilmer, M. Kouck\'y and M. Saks \cite{saks}
to study the Sensitivity Conjecture \cite{linial}, improving on their
$\sqrt{999\over 1000}\sqrt{n}$ bound. We also determine the exact complexity of
the game up to $n\le 9$."
"Constraint Satisfaction Problem (CSP) can be stated as computing a
homomorphism $\mbox{$\bR \rightarrow \bGamma$}$ between two relational
structures, e.g.\ between two directed graphs.
  Recently, the {\em hybrid} setting, where both sides are restricted
simultaneously, attracted some attention. It assumes that the right side
structure $\bGamma$ is fixed and $\bR$ belongs to a class of relational
structures $\mathcal{H}$ (called a {\em structural restriction}) that is,
additionally, {\em closed under inverse homomorphisms}. The key tool that
connects hybrid CSPs with fixed-template CSPs is a construction called a
""lifted language,"" namely a multi-sorted language $\bGamma_{\bR}$ that can be
constructed from an input $\bR$. The tractability of a language $\bGamma_{\bR}$
for any input $\bR\in\mathcal{H}$ is a necessary condition for tractability of
the hybrid problem.
  First we investigate the case when the last property is not only necessary,
but also is sufficient. It turns out that in the latter case, if
Bulatov-Jeavons-Krokhin characterization of tractable constraint languages is
correct, a structural restriction $\mathcal{H}$ is tractable if and only if it
consists of structures that can be homomorphically mapped to some fixed finite
relational structure $\bGamma'$ (that depends only on $\bGamma$).
  In the second part we generalize the construction of $\bGamma'$ and introduce
a finite structure $\bGamma^{\mathfrak{B}}$, indexed by some set of finite
algebras $\mathfrak{B}$. We prove that under some natural conditions on
$\mathfrak{B}$, $\textsc{CSP}(\bGamma)$ is polynomial-time Turing reducible to
$\textsc{CSP}(\bGamma^{\mathfrak{B}})$ and some polymorphisms of $\bGamma$ have
analogs in $\pol(\bGamma^{\mathfrak{B}})$. This construction introduce a new
set of algorithms for fixed-template CSPs and we suggest it as a tool to
approach Feder-Vardi dichotomy conjecture."
"In this paper we are going to solve an open problem about the game tetris. We
are going to give the first results in the complexity of a variant of offline
tetris introduced by Erik Demaine, Susan Hohenberger and David Liben Nowell in
their paper ""Tetris is hard, even to approximate"". In this variant, that
follows a model of movements introduced by John Brzustowsky, we can move and
rotate a piece the number of times we want in the first row. But then, when we
left the piece fall, we cannot move it or rotate it anymore. We are going to
demonstrate that the problem of maximizing the number of cleared lines of this
variant on a particular game board, is NP-hard by reducing the 3-partition
problem to the problem of clearing the board in this variant of tetris"
"In this paper we study combinatorial and algorithmic resp. complexity
questions of upper domination, i.e., the maximum cardinality of a minimal
dominating set in a graph. We give a full classification of the related
maximisation and minimisation problems, as well as the related parameterised
problems, on general graphs and on graphs of bounded degree, and we also study
planar graphs."
"It is known that the extension complexity of the TSP polytope for the
complete graph $K_n$ is exponential in $n$ even if the subtour inequalities are
excluded. In this article we study the polytopes formed by removing other
subsets $\mathcal{H}$ of facet-defining inequalities of the TSP polytope. In
particular, we consider the case when $\mathcal{H}$ is either the set of
blossom inequalities or the simple comb inequalities. These inequalities are
routinely used in cutting plane algorithms for the TSP. We show that the
extension complexity remains exponential even if we exclude these inequalities.
In addition we show that the extension complexity of polytope formed by all
comb inequalities is exponential. For our proofs, we introduce a subclass of
comb inequalities, called $(h,t)$-uniform inequalities, which may be of
independent interest."
"In this paper we give a first set of communication lower bounds for
distributed clustering problems, in particular, for k-center, k-median and
k-means. When the input is distributed across a large number of machines and
the number of clusters k is small, our lower bounds match the current best
upper bounds up to a logarithmic factor. We have designed a new composition
framework in our proofs for multiparty number-in-hand communication complexity
which may be of independent interest."
"In this paper, we show exponential lower bounds for the class of homogeneous
depth-$5$ circuits over all small finite fields. More formally, we show that
there is an explicit family $\{P_d : d \in \mathbb{N}\}$ of polynomials in
$\mathsf{VNP}$, where $P_d$ is of degree $d$ in $n = d^{O(1)}$ variables, such
that over all finite fields $\mathbb{F}_q$, any homogeneous depth-$5$ circuit
which computes $P_d$ must have size at least $\exp(\Omega_q(\sqrt{d}))$.
  To the best of our knowledge, this is the first super-polynomial lower bound
for this class for any field $\mathbb{F}_q \neq \mathbb{F}_2$.
  Our proof builds up on the ideas developed on the way to proving lower bounds
for homogeneous depth-$4$ circuits [GKKS13, FLMS13, KLSS14, KS14] and for
non-homogeneous depth-$3$ circuits over finite fields [GK98, GR00]. Our key
insight is to look at the space of shifted partial derivatives of a polynomial
as a space of functions from $\mathbb{F}_q^n \rightarrow \mathbb{F}_q$ as
opposed to looking at them as a space of formal polynomials and builds over a
tighter analysis of the lower bound of Kumar and Saraf [KS14]."
"In a valued constraint satisfaction problem (VCSP), the goal is to find an
assignment of labels to variables that minimizes a given sum of functions. Each
function in the sum depends on a subset of variables, takes values which are
rational numbers or infinity, and is chosen from a fixed finite set of
functions called a constraint language. The case when all functions take only
values 0 and infinity is known as the constraint satisfaction problem (CSP). It
is known that any CSP with fixed constraint language is polynomial-time
equivalent to one where the constraint language contains a single binary
relation (i.e. a digraph). A recent proof of this by Bulin et al. gives such a
reduction that preserves most of the algebraic properties of the constraint
language that are known to characterize the complexity of the corresponding
CSP. We adapt this proof to the more general setting of VCSP to show that each
VCSP with a fixed finite (valued) constraint language is equivalent to one
where the constraint language consists of one $\{0,\infty\}$-valued binary
function (i.e. a digraph) and one finite-valued unary function, the latter
problem known as the (extended) Minimum Cost Homomorphism Problem for digraphs.
We also show that our reduction preserves some important algebraic properties
of the (valued) constraint language."
"We show a close connection between structural hardness for $k$-partite graphs
and tight inapproximability results for scheduling problems with precedence
constraints. Assuming a natural but nontrivial generalisation of the bipartite
structural hardness result of Bansal and Khot, we obtain a hardness of
$2-\epsilon$ for the problem of minimising the makespan for scheduling
precedence-constrained jobs with preemption on identical parallel machines.
This matches the best approximation guarantee for this problem. Assuming the
same hypothesis, we also obtain a super constant inapproximability result for
the problem of scheduling precedence-constrained jobs on related parallel
machines, making progress towards settling an open question in both lists of
ten open questions by Williamson and Shmoys, and by Schuurman and Woeginger.
  The study of structural hardness of $k$-partite graphs is of independent
interest, as it captures the intrinsic hardness for a large family of
scheduling problems. Other than the ones already mentioned, this generalisation
also implies tight inapproximability to the problem of minimising the weighted
completion time for precedence-constrained jobs on a single machine, and the
problem of minimising the makespan of precedence-constrained jobs on identical
parallel machine, and hence unifying the results of Bansal and Khot, and
Svensson, respectively."
"In this paper we give lower bounds for the representation of real univariate
polynomials as sums of powers of degree 1 polynomials. We present two families
of polynomials of degree d such that the number of powers that are required in
such a representation must be at least of order d. This is clearly optimal up
to a constant factor. Previous lower bounds for this problem were only of order
$\Omega$($\sqrt$ d), and were obtained from arguments based on Wronskian
determinants and ""shifted derivatives."" We obtain this improvement thanks to a
new lower bound method based on Birkhoff interpolation (also known as ""lacunary
polynomial interpolation"")."
We investigate the complexity of a puzzle that turns out to be NL-complete.
"In the study of differential privacy, composition theorems (starting with the
original paper of Dwork, McSherry, Nissim, and Smith (TCC'06)) bound the
degradation of privacy when composing several differentially private
algorithms. Kairouz, Oh, and Viswanath (ICML'15) showed how to compute the
optimal bound for composing $k$ arbitrary $(\epsilon,\delta)$-differentially
private algorithms. We characterize the optimal composition for the more
general case of $k$ arbitrary
$(\epsilon_{1},\delta_{1}),\ldots,(\epsilon_{k},\delta_{k})$-differentially
private algorithms where the privacy parameters may differ for each algorithm
in the composition. We show that computing the optimal composition in general
is $\#$P-complete. Since computing optimal composition exactly is infeasible
(unless FP=$\#$P), we give an approximation algorithm that computes the
composition to arbitrary accuracy in polynomial time. The algorithm is a
modification of Dyer's dynamic programming approach to approximately counting
solutions to knapsack problems (STOC'03)."
"The Polyhedral Escape Problem for continuous linear dynamical systems
consists of deciding, given an affine function $f: \mathbb{R}^{d} \rightarrow
\mathbb{R}^{d}$ and a convex polyhedron $\mathcal{P} \subseteq \mathbb{R}^{d}$,
whether, for some initial point $\boldsymbol{x}_{0}$ in $\mathcal{P}$, the
trajectory of the unique solution to the differential equation
$\dot{\boldsymbol{x}}(t)=f(\boldsymbol{x}(t))$,
$\boldsymbol{x}(0)=\boldsymbol{x}_{0}$, is entirely contained in $\mathcal{P}$.
We show that this problem is decidable, by reducing it in polynomial time to
the decision version of linear programming with real algebraic coefficients,
thus placing it in $\exists \mathbb{R}$, which lies between NP and PSPACE. Our
algorithm makes use of spectral techniques and relies among others on tools
from Diophantine approximation."
"Kernelization is a formalization of efficient preprocessing for NP-hard
problems using the framework of parameterized complexity. Among open problems
in kernelization it has been asked many times whether there are deterministic
polynomial kernelizations for Subset Sum and Knapsack when parameterized by the
number $n$ of items.
  We answer both questions affirmatively by using an algorithm for compressing
numbers due to Frank and Tardos (Combinatorica 1987). This result had been
first used by Marx and V\'egh (ICALP 2013) in the context of kernelization. We
further illustrate its applicability by giving polynomial kernels also for
weighted versions of several well-studied parameterized problems. Furthermore,
when parameterized by the different item sizes we obtain a polynomial
kernelization for Subset Sum and an exponential kernelization for Knapsack.
Finally, we also obtain kernelization results for polynomial integer programs."
"It has long been known, since the classical work of (Arora, Karger,
Karpinski, JCSS~99), that \MC\ admits a PTAS on dense graphs, and more
generally, \kCSP\ admits a PTAS on ""dense"" instances with $\Omega(n^k)$
constraints. In this paper we extend and generalize their exhaustive sampling
approach, presenting a framework for $(1-\eps)$-approximating any \kCSP\
problem in \emph{sub-exponential} time while significantly relaxing the
denseness requirement on the input instance. Specifically, we prove that for
any constants $\delta \in (0, 1]$ and $\eps > 0$, we can approximate \kCSP\
problems with $\Omega(n^{k-1+\delta})$ constraints within a factor of
$(1-\eps)$ in time $2^{O(n^{1-\delta}\ln n /\eps^3)}$. The framework is quite
general and includes classical optimization problems, such as \MC, {\sc
Max}-DICUT, \kSAT, and (with a slight extension) $k$-{\sc Densest Subgraph}, as
special cases. For \MC\ in particular (where $k=2$), it gives an approximation
scheme that runs in time sub-exponential in $n$ even for ""almost-sparse""
instances (graphs with $n^{1+\delta}$ edges). We prove that our results are
essentially best possible, assuming the ETH. First, the density requirement
cannot be relaxed further: there exists a constant $r < 1$ such that for all
$\delta > 0$, \kSAT\ instances with $O(n^{k-1})$ clauses cannot be approximated
within a ratio better than $r$ in time $2^{O(n^{1-\delta})}$. Second, the
running time of our algorithm is almost tight \emph{for all densities}. Even
for \MC\ there exists $r<1$ such that for all $\delta' > \delta >0$, \MC\
instances with $n^{1+\delta}$ edges cannot be approximated within a ratio
better than $r$ in time $2^{n^{1-\delta'}}$."
"This papers considers the problem of maximizing the load that can be served
by a power network. We use the commonly accepted Linear DC power network model
and consider wo configuration options: switching lines and using FACTS devices.
We present the first comprehensive complexity study of this optimization
problem. Our results show hat the problem is NP-complete and that there is no
fully polynomial-time approximation scheme. For switching, these results extend
to planar networks with a aximum-node degree of 3. Additionally, we demonstrate
that the optimization problems are still NP-hard if we restrict the network
structure to cacti with a maximum degree of 3."
"The problem of finding large cliques in random graphs and its ""planted""
variant, where one wants to recover a clique of size $\omega \gg \log{(n)}$
added to an \Erdos-\Renyi graph $G \sim G(n,\frac{1}{2})$, have been intensely
studied. Nevertheless, existing polynomial time algorithms can only recover
planted cliques of size $\omega = \Omega(\sqrt{n})$. By contrast, information
theoretically, one can recover planted cliques so long as $\omega \gg
\log{(n)}$. In this work, we continue the investigation of algorithms from the
sum of squares hierarchy for solving the planted clique problem begun by Meka,
Potechin, and Wigderson (MPW, 2015) and Deshpande and Montanari (DM,2015). Our
main results improve upon both these previous works by showing:
  1. Degree four SoS does not recover the planted clique unless $\omega \gg
\sqrt n poly \log n$, improving upon the bound $\omega \gg n^{1/3}$ due to DM.
A similar result was obtained independently by Raghavendra and Schramm (2015).
  2. For $2 < d = o(\sqrt{\log{(n)}})$, degree $2d$ SoS does not recover the
planted clique unless $\omega \gg n^{1/(d + 1)} /(2^d poly \log n)$, improving
upon the bound due to MPW.
  Our proof for the second result is based on a fine spectral analysis of the
certificate used in the prior works MPW,DM and Feige and Krauthgamer (2003) by
decomposing it along an appropriately chosen basis. Along the way, we develop
combinatorial tools to analyze the spectrum of random matrices with dependent
entries and to understand the symmetries in the eigenspaces of the set
symmetric matrices inspired by work of Grigoriev (2001).
  An argument of Kelner shows that the first result cannot be proved using the
same certificate. Rather, our proof involves constructing and analyzing a new
certificate that yields the nearly tight lower bound by ""correcting"" the
certificate of previous works."
"The subitemset isomorphism problem is really important and there are
excellent practical solutions described in the literature. However, the
computational complexity analysis and classification of the BZ (Bundala and
Zavodny) subitemset isomorphism problem is currently an open problem. In this
paper we prove that checking whether two sorting networks are BZ isomorphic to
each other is GI-Complete; the general GI (Graph Isomorphism) problem is known
to be in NP and LWPP, but widely believed to be neither P nor NP-Complete;
recent research suggests that the problem is in QP. Moreover, we state the BZ
sorting network isomorphism problem as a general isomorphism problem on
itemsets --- because every sorting network is represented by Bundala and
Zavodny as an itemset. The complexity classification presented in this paper
applies sorting networks, as well as the general itemset isomorphism problem.
The main consequence of our work is that currently no polynomial-time algorithm
exists for solving the BZ sorting network subitemset isomorphism problem;
however the CM (Choi and Moon) sorting network isomorphism problem can be
efficiently solved in polynomial time."
"All Colors Shortest Path problem defined on an undirected graph aims at
finding a shortest, possibly non-simple, path where every color occurs at least
once, assuming that each vertex in the graph is associated with a color known
in advance. To the best of our knowledge, this paper is the first to define and
investigate this problem. Even though the problem is computationally similar to
generalized minimum spanning tree, and the generalized traveling salesman
problems, allowing for non-simple paths where a node may be visited multiple
times makes All Colors Shortest Path problem novel and computationally unique.
In this paper we prove that All Colors Shortest Path problem is NP-hard, and
does not lend itself to a constant factor approximation. We also propose
several heuristic solutions for this problem based on LP-relaxation, simulated
annealing, ant colony optimization, and genetic algorithm, and provide
extensive simulations for a comparative analysis of them. The heuristics
presented are not the standard implementations of the well known heuristic
algorithms, but rather sophisticated models tailored for the problem in hand.
This fact is acknowledged by the very promising results reported."
"Given an undirected graph G = (V, E) with n vertices, and a function f : V ->
N, we consider the problem of finding a connected f -factor in G. In this work
we design an algorithm to check for the existence of a connected f -factor, for
the case where f (v) >= n/g(n), for all v in V and g(n) is polylogarithmic in
n. The running time of our algorithm is O(n^{2g(n)}. As a consequence of this
algorithm we conclude that the complexity of connected f -factor for the case
we consider is unlikely to be NP-Complete unless the Exponential Time
Hypothesis (ETH) is false. Secondly, under the assumption of the ETH, we show
that it is also unlikely to be in P for g(n) in O((log n)^{1+eps} ) for any
eps> 0. Therefore, our results show that for all eps> 0, connected f -factor
for f (v) >= n/O(log n)^{1+eps}) is in NP-Intermediate unless the ETH is false.
Further, for any constant c > 0, when g(n) = c, our algorithm for connected f
-factor runs in polynomial time. Finally, we extend our algorithm to compute a
minimum weight connected f -factor in edge weighted graphs in the same
asymptotic time bounds."
"We show that the decision versions of the puzzles Knossos and The Hour-Glass
are complete for NP."