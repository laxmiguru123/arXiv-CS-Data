summary
"This paper computationally obtains optimal bounded-weight, binary,
error-correcting codes for a variety of distance bounds and dimensions. We
compare the sizes of our codes to the sizes of optimal constant-weight, binary,
error-correcting codes, and evaluate the differences."
"Let A(q,n,d) denote the maximum size of a q-ary code of length n and distance
d. We study the minimum asymptotic redundancy \rho(q,n,d)=n-log_q A(q,n,d) as n
grows while q and d are fixed. For any d and q<=d-1, long algebraic codes are
designed that improve on the BCH codes and have the lowest asymptotic
redundancy \rho(q,n,d) <= ((d-3)+1/(d-2)) log_q n known to date. Prior to this
work, codes of fixed distance that asymptotically surpass BCH codes and the
Gilbert-Varshamov bound were designed only for distances 4,5 and 6."
"We give a new lower bound on the expansion coefficient of an edge-vertex
graph of a $d$-regular graph. As a consequence, we obtain an improvement on the
lower bound on relative minimum distance of the expander codes constructed by
Sipser and Spielman. We also derive some improved results on the vertex
expansion of graphs that help us in improving the parameters of the expander
codes of Alon, Bruck, Naor, Naor, and Roth."
"We derive improved bounds on the error and erasure rate for spherical codes
and for binary linear codes under Forney's erasure/list decoding scheme and
prove some related results."
"We address the problem of bounding below the probability of error under
maximum likelihood decoding of a binary code with a known distance distribution
used on a binary symmetric channel. An improved upper bound is given for the
maximum attainable exponent of this probability (the reliability function of
the channel). In particular, we prove that the ``random coding exponent'' is
the true value of the channel reliability for code rate $R$ in some interval
immediately below the critical rate of the channel. An analogous result is
obtained for the Gaussian channel."
"We study codes on graphs combined with an iterative message passing algorithm
for quantization. Specifically, we consider the binary erasure quantization
(BEQ) problem which is the dual of the binary erasure channel (BEC) coding
problem. We show that duals of capacity achieving codes for the BEC yield codes
which approach the minimum possible rate for the BEQ. In contrast, low density
parity check codes cannot achieve the minimum rate unless their density grows
at least logarithmically with block length. Furthermore, we show that duals of
efficient iterative decoding algorithms for the BEC yield efficient encoding
algorithms for the BEQ. Hence our results suggest that graphical models may
yield near optimal codes in source coding as well as in channel coding and that
duality plays a key role in such constructions."
"A variable-length code is a fix-free code if no codeword is a prefix or a
suffix of any other codeword. In a fix-free code any finite sequence of
codewords can be decoded in both directions, which can improve the robustness
to channel noise and speed up the decoding process. In this paper we prove a
new sufficient condition of the existence of fix-free codes and improve the
upper bound on the redundancy of optimal fix-free codes."
"Fundamental results concerning the dynamics of abelian group codes
(behaviors) and their duals are developed. Duals of sequence spaces over
locally compact abelian groups may be defined via Pontryagin duality; dual
group codes are orthogonal subgroups of dual sequence spaces. The dual of a
complete code or system is finite, and the dual of a Laurent code or system is
(anti-)Laurent. If C and C^\perp are dual codes, then the state spaces of C act
as the character groups of the state spaces of C^\perp. The controllability
properties of C are the observability properties of C^\perp. In particular, C
is (strongly) controllable if and only if C^\perp is (strongly) observable, and
the controller memory of C is the observer memory of C^\perp. The controller
granules of C act as the character groups of the observer granules of C^\perp.
Examples of minimal observer-form encoder and syndrome-former constructions are
given. Finally, every observer granule of C is an ""end-around"" controller
granule of C."
"We consider lossy source coding when side information affecting the
distortion measure may be available at the encoder, decoder, both, or neither.
For example, such distortion side information can model reliabilities for noisy
measurements, sensor calibration information, or perceptual effects like
masking and sensitivity to context. When the distortion side information is
statistically independent of the source, we show that in many cases (e.g, for
additive or multiplicative distortion side information) there is no penalty for
knowing the side information only at the encoder, and there is no advantage to
knowing it at the decoder. Furthermore, for quadratic distortion measures
scaled by the distortion side information, we evaluate the penalty for lack of
encoder knowledge and show that it can be arbitrarily large. In this scenario,
we also sketch transform based quantizers constructions which efficiently
exploit encoder side information in the high-resolution limit."
"We continue to discuss why MMSE estimation arises in coding schemes that
approach the capacity of linear Gaussian channels. Here we consider schemes
that involve successive decoding, such as decision-feedback equalization or
successive cancellation."
"We present two sequences of ensembles of non-systematic irregular
repeat-accumulate codes which asymptotically (as their block length tends to
infinity) achieve capacity on the binary erasure channel (BEC) with bounded
complexity per information bit. This is in contrast to all previous
constructions of capacity-achieving sequences of ensembles whose complexity
grows at least like the log of the inverse of the gap (in rate) to capacity.
The new bounded complexity result is achieved by puncturing bits, and allowing
in this way a sufficient number of state nodes in the Tanner graph representing
the codes. We also derive an information-theoretic lower bound on the decoding
complexity of randomly punctured codes on graphs. The bound holds for every
memoryless binary-input output-symmetric channel and is refined for the BEC."
"We present two sequences of ensembles of non-systematic irregular
repeat-accumulate codes which asymptotically (as their block length tends to
infinity) achieve capacity on the binary erasure channel (BEC) with bounded
complexity per information bit. This is in contrast to all previous
constructions of capacity-achieving sequences of ensembles whose complexity
grows at least like the log of the inverse of the gap (in rate) to capacity.
The new bounded complexity result is achieved by puncturing bits, and allowing
in this way a sufficient number of state nodes in the Tanner graph representing
the codes. We also derive an information-theoretic lower bound on the decoding
complexity of randomly punctured codes on graphs. The bound holds for every
memoryless binary-input output-symmetric channel, and is refined for the BEC."
"We discuss why MMSE estimation arises in lattice-based schemes for
approaching the capacity of linear Gaussian channels, and comment on its
properties."
"We compare the elementary theories of Shannon information and Kolmogorov
complexity, the extent to which they have a common purpose, and where they are
fundamentally different. We discuss and relate the basic notions of both
theories: Shannon entropy versus Kolmogorov complexity, the relation of both to
universal coding, Shannon mutual information versus Kolmogorov (`algorithmic')
mutual information, probabilistic sufficient statistic versus algorithmic
sufficient statistic (related to lossy compression in the Shannon theory versus
meaningful information in the Kolmogorov theory), and rate distortion theory
versus Kolmogorov's structure function. Part of the material has appeared in
print before, scattered through various publications, but this is the first
comprehensive systematic comparison. The last mentioned relations are new."
"Capacity formulas and random-coding exponents are derived for a generalized
family of Gel'fand-Pinsker coding problems. These exponents yield asymptotic
upper bounds on the achievable log probability of error. In our model,
information is to be reliably transmitted through a noisy channel with finite
input and output alphabets and random state sequence, and the channel is
selected by a hypothetical adversary. Partial information about the state
sequence is available to the encoder, adversary, and decoder. The design of the
transmitter is subject to a cost constraint. Two families of channels are
considered: 1) compound discrete memoryless channels (CDMC), and 2) channels
with arbitrary memory, subject to an additive cost constraint, or more
generally to a hard constraint on the conditional type of the channel output
given the input. Both problems are closely connected. The random-coding
exponent is achieved using a stacked binning scheme and a maximum penalized
mutual information decoder, which may be thought of as an empirical generalized
Maximum a Posteriori decoder. For channels with arbitrary memory, the
random-coding exponents are larger than their CDMC counterparts. Applications
of this study include watermarking, data hiding, communication in presence of
partially known interferers, and problems such as broadcast channels, all of
which involve the fundamental idea of binning."
"We consider source coding with fixed lag side information at the decoder. We
focus on the special case of perfect side information with unit lag
corresponding to source coding with feedforward (the dual of channel coding
with feedback) introduced by Pradhan. We use this duality to develop a linear
complexity algorithm which achieves the rate-distortion bound for any
memoryless finite alphabet source and distortion measure."
"We propose use of QR factorization with sort and Dijkstra's algorithm for
decreasing the computational complexity of the sphere decoder that is used for
ML detection of signals on the multi-antenna fading channel. QR factorization
with sort decreases the complexity of searching part of the decoder with small
increase in the complexity required for preprocessing part of the decoder.
Dijkstra's algorithm decreases the complexity of searching part of the decoder
with increase in the storage complexity. The computer simulation demonstrates
that the complexity of the decoder is reduced by the proposed methods
significantly."
"In this paper, we analyze the performance of space-time block codes which
enable symbolwise maximum likelihood decoding. We derive an upper bound of
maximum mutual information (MMI) on space-time block codes that enable
symbolwise maximum likelihood decoding for a frequency non-selective
quasi-static fading channel. MMI is an upper bound on how much one can send
information with vanishing error probability by using the target code."
"In this paper, we present two low complexity algorithms that achieve capacity
for the noiseless (d,k) constrained channel when k=2d+1, or when k-d+1 is not
prime. The first algorithm, called symbol sliding, is a generalized version of
the bit flipping algorithm introduced by Aviran et al. [1]. In addition to
achieving capacity for (d,2d+1) constraints, it comes close to capacity in
other cases. The second algorithm is based on interleaving, and is a
generalized version of the bit stuffing algorithm introduced by Bender and Wolf
[2]. This method uses fewer than k-d biased bit streams to achieve capacity for
(d,k) constraints with k-d+1 not prime. In particular, the encoder for
(d,d+2^m-1) constraints, 1\le m<\infty, requires only m biased bit streams."
"Capacity analysis for channels with side information at the receiver has been
an active area of interest. This problem is well investigated for the case of
finite alphabet channels. However, the results are not easily generalizable to
the case of continuous alphabet channels due to analytic difficulties inherent
with continuous alphabets. In the first part of this two-part paper, we address
an analytical framework for capacity analysis of continuous alphabet channels
with side information at the receiver. For this purpose, we establish novel
necessary and sufficient conditions for weak* continuity and strict concavity
of the mutual information. These conditions are used in investigating the
existence and uniqueness of the capacity-achieving measures. Furthermore, we
derive necessary and sufficient conditions that characterize the capacity value
and the capacity-achieving measure for continuous alphabet channels with side
information at the receiver."
"In this part, we consider the capacity analysis for wireless mobile systems
with multiple antenna architectures. We apply the results of the first part to
a commonly known baseband, discrete-time multiple antenna system where both the
transmitter and receiver know the channel's statistical law. We analyze the
capacity for additive white Gaussian noise (AWGN) channels, fading channels
with full channel state information (CSI) at the receiver, fading channels with
no CSI, and fading channels with partial CSI at the receiver. For each type of
channels, we study the capacity value as well as issues such as the existence,
uniqueness, and characterization of the capacity-achieving measures for
different types of moment constraints. The results are applicable to both
Rayleigh and Rician fading channels in the presence of arbitrary line-of-sight
and correlation profiles."
"We examine the structure of families of distortion balls from the perspective
of Kolmogorov complexity. Special attention is paid to the canonical
rate-distortion function of a source word which returns the minimal Kolmogorov
complexity of all distortion balls containing that word subject to a bound on
their cardinality. This canonical rate-distortion function is related to the
more standard algorithmic rate-distortion function for the given distortion
measure. Examples are given of list distortion, Hamming distortion, and
Euclidean distortion. The algorithmic rate-distortion function can behave
differently from Shannon's rate-distortion function. To this end, we show that
the canonical rate-distortion function can and does assume a wide class of
shapes (unlike Shannon's); we relate low algorithmic mutual information to low
Kolmogorov complexity (and consequently suggest that certain aspects of the
mutual information formulation of Shannon's rate-distortion function behave
differently than would an analogous formulation using algorithmic mutual
information); we explore the notion that low Kolmogorov complexity distortion
balls containing a given word capture the interesting properties of that word
(which is hard to formalize in Shannon's theory) and this suggests an approach
to denoising; and, finally, we show that the different behavior of the
rate-distortion curves of individual source words to some extent disappears
after averaging over the source words."
"The feedback capacity of the stationary Gaussian additive noise channel has
been open, except for the case where the noise is white. Here we find the
feedback capacity of the stationary first-order moving average additive
Gaussian noise channel in closed form. Specifically, the channel is given by
$Y_i = X_i + Z_i,$ $i = 1, 2, ...,$ where the input $\{X_i\}$ satisfies a power
constraint and the noise $\{Z_i\}$ is a first-order moving average Gaussian
process defined by $Z_i = \alpha U_{i-1} + U_i,$ $|\alpha| \le 1,$ with white
Gaussian innovations $U_i,$ $i = 0,1,....$
  We show that the feedback capacity of this channel is $-\log x_0,$ where
$x_0$ is the unique positive root of the equation $ \rho x^2 = (1-x^2) (1 -
|\alpha|x)^2,$ and $\rho$ is the ratio of the average input power per
transmission to the variance of the noise innovation $U_i$. The optimal coding
scheme parallels the simple linear signalling scheme by Schalkwijk and Kailath
for the additive white Gaussian noise channel -- the transmitter sends a
real-valued information-bearing signal at the beginning of communication and
subsequently refines the receiver's error by processing the feedback noise
signal through a linear stationary first-order autoregressive filter. The
resulting error probability of the maximum likelihood decoding decays
doubly-exponentially in the duration of the communication. This feedback
capacity of the first-order moving average Gaussian channel is very similar in
form to the best known achievable rate for the first-order
\emph{autoregressive} Gaussian noise channel studied by Butman, Wolfowitz, and
Tiernan, although the optimality of the latter is yet to be established."
"Geographic routing with greedy relaying strategies have been widely studied
as a routing scheme in sensor networks. These schemes assume that the nodes
have perfect information about the location of the destination. When the
distance between the source and destination is normalized to unity, the
asymptotic routing delays in these schemes are $\Theta(\frac{1}{M(n)}),$ where
M(n) is the maximum distance traveled in a single hop (transmission range of a
radio). In this paper, we consider routing scenarios where nodes have location
errors (imprecise GPS), or where only coarse geographic information about the
destination is available, and only a fraction of the nodes have routing
information. We show that even with such imprecise or limited
destination-location information, the routing delays are
$\Theta(\frac{1}{M(n)})$. We also consider the throughput-capacity of networks
with progressive routing strategies that take packets closer to the destination
in every step, but not necessarily along a straight-line. We show that the
throughput-capacity with progressive routing is order-wise the same as the
maximum achievable throughput-capacity."
"We obtain the first term in the high signal-to-noise ratio (SNR) expansion of
the capacity of fading networks where the transmitters and receivers--while
fully cognizant of the fading \emph{law}--have no access to the fading
\emph{realization}. This term is an integer multiple of $\log \log
\textnormal{SNR}$ with the coefficient having a simple combinatorial
characterization."
"The dependence of the Gaussian input information rate on the line-of-sight
(LOS) matrix in multiple-input multiple-output coherent Rician fading channels
is explored. It is proved that the outage probability and the mutual
information induced by a multivariate circularly symmetric Gaussian input with
any covariance matrix are monotonic in the LOS matrix D, or more precisely,
monotonic in D'D in the sense of the Loewner partial order. Conversely, it is
also demonstrated that this ordering on the LOS matrices is a necessary
condition for the uniform monotonicity over all input covariance matrices. This
result is subsequently applied to prove the monotonicity of the isotropic
Gaussian input information rate and channel capacity in the singular values of
the LOS matrix. Extensions to multiple-access channels are also discussed."
"Recently, a quasi-orthogonal space-time block code (QSTBC) capable of
achieving a significant fraction of the outage mutual information of a
multiple-input-multiple output (MIMO) wireless communication system for the
case of four transmit and one receive antennas was proposed. We generalize
these results to $n_T=2^n$ transmit and an arbitrary number of receive antennas
$n_R$. Furthermore, we completely characterize the structure of the equivalent
channel for the general case and show that for all $n_T=2^n$ and $n_R$ the
eigenvectors of the equivalent channel are fixed and independent from the
channel realization. Furthermore, the eigenvalues of the equivalent channel are
independent identically distributed random variables each following a
noncentral chi-square distribution with $4n_R$ degrees of freedom.
  Based on these important insights into the structure of the QSTBC, we derive
an analytical lower bound for the fraction of outage probability achieved with
QSTBC and show that this bound is tight for low signal-to-noise-ratios (SNR)
values and also for increasing number of receive antennas. We also present an
upper bound, which is tight for high SNR values and derive analytical
expressions for the case of four transmit antennas. Finally, by utilizing the
special structure of the QSTBC we propose a new transmit strategy, which
decouples the signals transmitted from different antennas in order to detect
the symbols separately with a linear ML-detector rather than joint detection,
an up to now only known advantage of orthogonal space-time block codes (OSTBC)."
"This paper deals with arbitrarily distributed finite-power input signals
observed through an additive Gaussian noise channel. It shows a new formula
that connects the input-output mutual information and the minimum mean-square
error (MMSE) achievable by optimal estimation of the input given the output.
That is, the derivative of the mutual information (nats) with respect to the
signal-to-noise ratio (SNR) is equal to half the MMSE, regardless of the input
statistics. This relationship holds for both scalar and vector signals, as well
as for discrete-time and continuous-time noncausal MMSE estimation. This
fundamental information-theoretic result has an unexpected consequence in
continuous-time nonlinear estimation: For any input signal with finite power,
the causal filtering MMSE achieved at SNR is equal to the average value of the
noncausal smoothing MMSE achieved with a channel whose signal-to-noise ratio is
chosen uniformly distributed between 0 and SNR."
"A new lower bound on the error probability of maximum likelihood decoding of
a binary code on a binary symmetric channel was proved in Barg and McGregor
(2004, cs.IT/0407011). It was observed in that paper that this bound leads to a
new region of code rates in which the random coding exponent is asymptotically
tight, giving a new region in which the reliability of the BSC is known
exactly. The present paper explains the relation of these results to the union
bound on the error probability."
"We introduce the idea of distortion side information, which does not directly
depend on the source but instead affects the distortion measure. We show that
such distortion side information is not only useful at the encoder, but that
under certain conditions, knowing it at only the encoder is as good as knowing
it at both encoder and decoder, and knowing it at only the decoder is useless.
Thus distortion side information is a natural complement to the signal side
information studied by Wyner and Ziv, which depends on the source but does not
involve the distortion measure. Furthermore, when both types of side
information are present, we characterize the penalty for deviating from the
configuration of encoder-only distortion side information and decoder-only
signal side information, which in many cases is as good as full side
information knowledge."
"We consider transmitting a source across a pair of independent, non-ergodic
channels with random states (e.g., slow fading channels) so as to minimize the
average distortion. The general problem is unsolved. Hence, we focus on
comparing two commonly used source and channel encoding systems which
correspond to exploiting diversity either at the physical layer through
parallel channel coding or at the application layer through multiple
description source coding.
  For on-off channel models, source coding diversity offers better performance.
For channels with a continuous range of reception quality, we show the reverse
is true. Specifically, we introduce a new figure of merit called the distortion
exponent which measures how fast the average distortion decays with SNR. For
continuous-state models such as additive white Gaussian noise channels with
multiplicative Rayleigh fading, optimal channel coding diversity at the
physical layer is more efficient than source coding diversity at the
application layer in that the former achieves a better distortion exponent.
  Finally, we consider a third decoding architecture: multiple description
encoding with a joint source-channel decoding. We show that this architecture
achieves the same distortion exponent as systems with optimal channel coding
diversity for continuous-state channels, and maintains the the advantages of
multiple description systems for on-off channels. Thus, the multiple
description system with joint decoding achieves the best performance, from
among the three architectures considered, on both continuous-state and on-off
channels."
"A simple and natural Gao algorithm for decoding algebraic codes is described.
Its relation to the Welch-Berlekamp and Euclidean algorithms is given."
"In this work we have considered formal power series and partial differential
equations, and their relationship with Coding Theory. We have obtained the
nature of solutions for the partial differential equations for Cycle Poisson
Case. The coefficients for this case have been simulated, and the high tendency
of growth is shown. In the light of Complex Analysis, the Hadamard
Multiplication's Theorem is presented as a new approach to divide the power
sums relating to the error probability, each part of which can be analyzed
later."
"Existing quantum key distribution schemes need the support of classical
authentication scheme to ensure security. This is a conceptual drawback of
quantum cryptography. It is pointed out that quantum cryptosystem does not need
any support of classical cryptosystem to ensure security. No-cloning principal
can alone provide security in communication. Even no-cloning principle itself
can help to authenticate each bit of information. It implies that quantum
password need not to be a secret password."
"We study the repeated use of a monotonic recording medium--such as punched
tape or photographic plate--where marks can be added at any time but never
erased. (For practical purposes, also the electromagnetic ""ether"" falls into
this class.) Our emphasis is on the case where the successive users act
independently and selfishly, but not maliciously; typically, the ""first user""
would be a blind natural process tending to degrade the recording medium, and
the ""second user"" a human trying to make the most of whatever capacity is left.
  To what extent is a length of used tape ""equivalent""--for information
transmission purposes--to a shorter length of virgin tape? Can we characterize
a piece of used tape by an appropriate ""effective length"" and forget all other
details? We identify two equivalence principles. The weak principle is exact,
but only holds for a sequence of infinitesimal usage increments. The strong
principle holds for any amount of incremental usage, but is only approximate;
nonetheless, it is quite accurate even in the worst case and is virtually exact
over most of the range--becoming exact in the limit of heavily used tape.
  The fact that strong equivalence does not hold exactly, but then it does
almost exactly, comes as a bit of a surprise."
"For practical wireless DS-CDMA systems, channel estimation is imperfect due
to noise and interference. In this paper, the impact of channel estimation
errors on multiuser detection (MUD) is analyzed under the framework of the
replica method. System performance is obtained in the large system limit for
optimal MUD, linear MUD and turbo MUD, and is validated by numerical results
for finite systems."
"Communications in dispersive direct-sequence code-division multiple-access
(DS-CDMA) channels suffer from intersymbol and multiple-access interference,
which can significantly impair performance. Joint maximum \textit{a posteriori}
probability (MAP) equalization and multiuser detection with error control
decoding can be used to mitigate this interference and to achieve the optimal
bit error rate. Unfortunately, such optimal detection typically requires
prohibitive computational complexity. This problem is addressed in this paper
through the development of a reduced state trellis search detection algorithm,
based on decision feedback from channel decoders. The performance of this
algorithm is analyzed in the large-system limit. This analysis and simulations
show that this low-complexity algorithm can obtain near-optimal performance
under moderate signal-to-noise ratio and attains larger system load capacity
than parallel interference cancellation."
"In this paper, the performance of a binary phase shift keyed random
time-hopping impulse radio system with pulse-based polarity randomization is
analyzed. Transmission over frequency-selective channels is considered and the
effects of inter-frame interference and multiple access interference on the
performance of a generic Rake receiver are investigated for both synchronous
and asynchronous systems. Closed form (approximate) expressions for the
probability of error that are valid for various Rake combining schemes are
derived. The asynchronous system is modelled as a chip-synchronous system with
uniformly distributed timing jitter for the transmitted pulses of interfering
users. This model allows the analytical technique developed for the synchronous
case to be extended to the asynchronous case. An approximate closed-form
expression for the probability of bit error, expressed in terms of the
autocorrelation function of the transmitted pulse, is derived for the
asynchronous case. Then, transmission over an additive white Gaussian noise
channel is studied as a special case, and the effects of multiple-access
interference is investigated for both synchronous and asynchronous systems. The
analysis shows that the chip-synchronous assumption can result in
over-estimating the error probability, and the degree of over-estimation mainly
depends on the autocorrelation function of the ultra-wideband pulse and the
signal-to-interference-plus-noise-ratio of the system. Simulations studies
support the approximate analysis."
"Sensor networks in which energy is a limited resource so that energy
consumption must be minimized for the intended application are considered. In
this context, an energy-efficient method for the joint estimation of an unknown
analog source under a given distortion constraint is proposed. The approach is
purely analog, in which each sensor simply amplifies and forwards the
noise-corrupted analog bservation to the fusion center for joint estimation.
The total transmission power across all the sensor nodes is minimized while
satisfying a distortion requirement on the joint estimate. The energy
efficiency of this analog approach is compared with previously proposed digital
approaches with and without coding. It is shown in our simulation that the
analog approach is more energy-efficient than the digital system without
coding, and in some cases outperforms the digital system with optimal coding."
"The effect of Rician-ness on the capacity of multiple antenna systems is
investigated under the assumption that channel state information (CSI) is
available only at the receiver. The average-power-constrained capacity of such
systems is considered under two different assumptions on the knowledge about
the fading available at the transmitter: the case in which the transmitter has
no knowledge of fading at all, and the case in which the transmitter has
knowledge of the distribution of the fading process but not the instantaneous
CSI. The exact capacity is given for the former case while capacity bounds are
derived for the latter case. A new signalling scheme is also proposed for the
latter case and it is shown that by exploiting the knowledge of Rician-ness at
the transmitter via this signalling scheme, significant capacity gain can be
achieved. The derived capacity bounds are evaluated explicitly to provide
numerical results in some representative situations."
"The problem of scheduling sensor transmissions for the detection of
correlated random fields using spatially deployed sensors is considered. Using
the large deviations principle, a closed-form expression for the error exponent
of the miss probability is given as a function of the sensor spacing and
signal-to-noise ratio (SNR). It is shown that the error exponent has a distinct
characteristic: at high SNR, the error exponent is monotonically increasing
with respect to sensor spacing, while at low SNR there is an optimal spacing
for scheduled sensors."
"Concavity of the auxiliary function which appears in the random coding
exponent as the lower bound of the quantum reliability function for general
quantum states is proven for s between 0 and 1."
"Estimating the number of sources impinging on an array of sensors is a well
known and well investigated problem. A common approach for solving this problem
is to use an information theoretic criterion, such as Minimum Description
Length (MDL) or the Akaike Information Criterion (AIC). The MDL estimator is
known to be a consistent estimator, robust against deviations from the Gaussian
assumption, and non-robust against deviations from the point source and/or
temporally or spatially white additive noise assumptions. Over the years
several alternative estimation algorithms have been proposed and tested.
Usually, these algorithms are shown, using computer simulations, to have
improved performance over the MDL estimator, and to be robust against
deviations from the assumed spatial model. Nevertheless, these robust
algorithms have high computational complexity, requiring several
multi-dimensional searches.
  In this paper, motivated by real life problems, a systematic approach toward
the problem of robust estimation of the number of sources using information
theoretic criteria is taken. An MDL type estimator that is robust against
deviation from assumption of equal noise level across the array is studied. The
consistency of this estimator, even when deviations from the equal noise level
assumption occur, is proven. A novel low-complexity implementation method
avoiding the need for multi-dimensional searches is presented as well, making
this estimator a favorable choice for practical applications."
"Convex relaxations of the optimal finger selection algorithm are proposed for
a minimum mean square error (MMSE) Rake receiver in an impulse radio
ultra-wideband system. First, the optimal finger selection problem is
formulated as an integer programming problem with a non-convex objective
function. Then, the objective function is approximated by a convex function and
the integer programming problem is solved by means of constraint relaxation
techniques. The proposed algorithms are suboptimal due to the approximate
objective function and the constraint relaxation steps. However, they can be
used in conjunction with the conventional finger selection algorithm, which is
suboptimal on its own since it ignores the correlation between multipath
components, to obtain performances reasonably close to that of the optimal
scheme that cannot be implemented in practice due to its complexity. The
proposed algorithms leverage convexity of the optimization problem
formulations, which is the watershed between `easy' and `difficult'
optimization problems."
"One of the features characterizing almost every multiple access (MA)
communication system is the processing gain. Through the use of spreading
sequences, the processing gain of Random CDMA systems (RCDMA), is devoted to
both bandwidth expansion and orthogonalization of the signals transmitted by
different users. Another type of multiple access system is Impulse Radio (IR).
In many aspects, IR systems are similar to time division multiple access (TDMA)
systems, and the processing gain of IR systems represents the ratio between the
actual transmission time and the total time between two consecutive
ransmissions (on-plus-off to on ratio). While CDMA systems, which constantly
excite the channel, rely on spreading sequences to orthogonalize the signals
transmitted by different users, IR systems transmit a series of short pulses
and the orthogonalization between the signals transmitted by different users is
achieved by the fact that most of the pulses do not collide with each other at
the receiver.
  In this paper, a general class of MA communication systems that use both
types of processing gain is presented, and both IR and RCDMA systems are
demonstrated to be two special cases of this more general class of systems. The
bit error rate (BER) of several receivers as a function of the ratio between
the two types of processing gain is analyzed and compared under the constraint
that the total processing gain of the system is large and fixed. It is
demonstrated that in non inter-symbol interference (ISI) channels there is no
tradeoff between the two types of processing gain. However, in ISI channels a
tradeoff between the two types of processing gain exists. In addition, the
sub-optimality of RCDMA systems in frequency selective channels is established."
"In this work, a non-cooperative power control game for multi-carrier CDMA
systems is proposed. In the proposed game, each user needs to decide how much
power to transmit over each carrier to maximize its overall utility. The
utility function considered here measures the number of reliable bits
transmitted per joule of energy consumed. It is shown that the user's utility
is maximized when the user transmits only on the carrier with the best
""effective channel"". The existence and uniqueness of Nash equilibrium for the
proposed game are investigated and the properties of equilibrium are studied.
Also, an iterative and distributed algorithm for reaching the equilibrium (if
it exists) is presented. It is shown that the proposed approach results in a
significant improvement in the total utility achieved at equilibrium compared
to the case in which each user maximizes its utility over each carrier
independently."
"Transmission of information over a discrete-time memoryless Rician fading
channel is considered where neither the receiver nor the transmitter knows the
fading coefficients. First the structure of the capacity-achieving input
signals is investigated when the input is constrained to have limited
peakedness by imposing either a fourth moment or a peak constraint. When the
input is subject to second and fourth moment limitations, it is shown that the
capacity-achieving input amplitude distribution is discrete with a finite
number of mass points in the low-power regime. A similar discrete structure for
the optimal amplitude is proven over the entire SNR range when there is only a
peak power constraint. The Rician fading with phase-noise channel model, where
there is phase uncertainty in the specular component, is analyzed. For this
model it is shown that, with only an average power constraint, the
capacity-achieving input amplitude is discrete with a finite number of levels.
For the classical average power limited Rician fading channel, it is proven
that the optimal input amplitude distribution has bounded support."
"Transmission of information over a discrete-time memoryless Rician fading
channel is considered where neither the receiver nor the transmitter knows the
fading coefficients. The spectral-efficiency/bit-energy tradeoff in the
low-power regime is examined when the input has limited peakedness. It is shown
that if a fourth moment input constraint is imposed or the input
peak-to-average power ratio is limited, then in contrast to the behavior
observed in average power limited channels, the minimum bit energy is not
always achieved at zero spectral efficiency. The low-power performance is also
characterized when there is a fixed peak limit that does not vary with the
average power. A new signaling scheme that overlays phase-shift keying on
on-off keying is proposed and shown to be optimally efficient in the low-power
regime."
"In this paper, optimal power allocation and capacity regions are derived for
GSIC (groupwise successive interference cancellation) systems operating in
multipath fading channels, under imperfect channel estimation conditions. It is
shown that the impact of channel estimation errors on the system capacity is
two-fold: it affects the receivers' performance within a group of users, as
well as the cancellation performance (through cancellation errors). An
iterative power allocation algorithm is derived, based on which it can be shown
that the total required received power is minimized when the groups are ordered
according to their cancellation errors, and the first detected group has the
smallest cancellation error.
  Performace/complexity tradeoff issues are also discussed by directly
comparing the system capacity for different implementations: GSIC with linear
minimum-mean-square error (LMMSE) receivers within the detection groups, GSIC
with matched filter receivers, multicode LMMSE systems, and simple all matched
filter receivers systems."
"Application of the turbo principle to multiuser decoding results in an
exchange of probability distributions between two sets of constraints. Firstly,
constraints imposed by the multiple-access channel, and secondly, individual
constraints imposed by each users' error control code. A-posteriori probability
computation for the first set of constraints is prohibitively complex for all
but a small number of users. Several lower complexity approaches have been
proposed in the literature. One class of methods is based on linear filtering
(e.g. LMMSE). A more recent approach is to compute approximations to the
posterior probabilities by marginalising over a subset of sequences (list
detection). Most of the list detection methods are restricted to non-singular
systems. In this paper, we introduce a transformation that permits application
of standard tree-search methods to underdetermined systems. We find that the
resulting tree-search based receiver outperforms existing methods."
"We consider the pulse design problem in multicarrier transmission where the
pulse shapes are adapted to the second order statistics of the WSSUS channel.
Even though the problem has been addressed by many authors analytical insights
are rather limited. First we show that the problem is equivalent to the pure
state channel fidelity in quantum information theory. Next we present a new
approach where the original optimization functional is related to an eigenvalue
problem for a pseudo differential operator by utilizing unitary representations
of the Weyl--Heisenberg group.A local approximation of the operator for
underspread channels is derived which implicitly covers the concepts of pulse
scaling and optimal phase space displacement. The problem is reformulated as a
differential equation and the optimal pulses occur as eigenstates of the
harmonic oscillator Hamiltonian. Furthermore this operator--algebraic approach
is extended to provide exact solutions for different classes of scattering
environments."
"A new design method for high rate, fully diverse ('spherical') space
frequency codes for MIMO-OFDM systems is proposed, which works for arbitrary
numbers of antennas and subcarriers. The construction exploits a differential
geometric connection between spherical codes and space time codes. The former
are well studied e.g. in the context of optimal sequence design in CDMA
systems, while the latter serve as basic building blocks for space frequency
codes. In addition a decoding algorithm with moderate complexity is presented.
This is achieved by a lattice based construction of spherical codes, which
permits lattice decoding algorithms and thus offers a substantial reduction of
complexity."
"In this paper have written the results of the information analysis of
structures. The obtained information estimation (IE) are based on an entropy
measure of C. Shannon. Obtained IE is univalent both for the non-isomorphic and
for the isomorphic graphs, algorithmically, it is asymptotically steady and has
vector character. IE can be used for the solution of the problems ranking of
structures by the preference, the evaluation of the structurization of subject
area, the solution of the problems of structural optimization. Information
estimations and method of the information analysis of structures it can be used
in many fields of knowledge (Electrical Systems and Circuit, Image recognition,
Computer technology, Databases and Bases of knowledge, Organic chemistry,
Biology and others) and it can be base for the structure calculus."
"This paper presents a stochastic algorithm for iterative error control
decoding. We show that the stochastic decoding algorithm is an approximation of
the sum-product algorithm. When the code's factor graph is a tree, as with
trellises, the algorithm approaches maximum a-posteriori decoding. We also
demonstrate a stochastic approximations to the alternative update rule known as
successive relaxation. Stochastic decoders have very simple digital
implementations which have almost no RAM requirements. We present example
stochastic decoders for a trellis-based Hamming code, and for a Block Turbo
code constructed from Hamming codes."
"We consider the problem of nonlinear dimensionality reduction: given a
training set of high-dimensional data whose ``intrinsic'' low dimension is
assumed known, find a feature extraction map to low-dimensional space, a
reconstruction map back to high-dimensional space, and a geometric description
of the dimension-reduced data as a smooth manifold. We introduce a
complexity-regularized quantization approach for fitting a Gaussian mixture
model to the training set via a Lloyd algorithm. Complexity regularization
controls the trade-off between adaptation to the local shape of the underlying
manifold and global geometric consistency. The resulting mixture model is used
to design the feature extraction and reconstruction maps and to define a
Riemannian metric on the low-dimensional data. We also sketch a proof of
consistency of our scheme for the purposes of estimating the unknown underlying
pdf of high-dimensional data."
"The Gallager bound is well known in the area of channel coding. However, most
discussions about it mainly focus on its applications to memoryless channels.
We show in this paper that the bounds obtained by Gallager's method are very
tight even for general sources and channels that are defined in the
information-spectrum theory. Our method is mainly based on the estimations of
error exponents in those bounds, and by these estimations we proved the direct
part of the Slepian-Wolf theorem and channel coding theorem for general sources
and channels."
"We show how to construct an algorithm to search for binary idempotents which
may be used to construct binary LDPC codes. The algorithm, which allows control
of the key properties of sparseness, code rate and minimum distance, is
constructed in the Mattson-Solomon domain. Some of the new codes, found by
using this technique, are displayed."
"Cycle codes are a special case of low-density parity-check (LDPC) codes and
as such can be decoded using an iterative message-passing decoding algorithm on
the associated Tanner graph. The existence of pseudo-codewords is known to
cause the decoding algorithm to fail in certain instances. In this paper, we
draw a connection between pseudo-codewords of cycle codes and the so-called
edge zeta function of the associated normal graph and show how the Newton
polyhedron of the zeta function equals the fundamental cone of the code, which
plays a crucial role in characterizing the performance of iterative decoding
algorithms."
"It is shown that some well-known and some new cyclic codes with orthogonal
parity-check equations can be constructed in the finite-field transform domain.
It is also shown that, for some binary linear cyclic codes, the performance of
the iterative decoder can be improved by substituting some of the dual code
codewords in the parity-check matrix with other dual code codewords formed from
linear combinations. This technique can bring the performance of a code closer
to its maximum-likelihood performance, which can be derived from the erroneous
decoded codeword whose euclidean distance with the respect to the received
block is smaller than that of the correct codeword. For (63,37), (93,47) and
(105,53) cyclic codes, the maximum-likelihood performance is realised with this
technique."
"An algorithm of improving the performance of iterative decoding on
perpendicular magnetic recording is presented. This algorithm follows on the
authors' previous works on the parallel and serial concatenated turbo codes and
low-density parity-check codes. The application of this algorithm with
signal-to-noise ratio mismatch technique shows promising results in the
presence of media noise. We also show that, compare to the standard iterative
decoding algorithm, an improvement of within one order of magnitude can be
achieved."
"Based on the ideas of cyclotomic cosets, idempotents and Mattson-Solomon
polynomials, we present a new method to construct GF(2^m), where m>0 cyclic
low-density parity-check codes. The construction method produces the dual code
idempotent which is used to define the parity-check matrix of the low-density
parity-check code. An interesting feature of this construction method is the
ability to increment the code dimension by adding more idempotents and so
steadily decrease the sparseness of the parity-check matrix. We show that the
constructed codes can achieve performance very close to the
sphere-packing-bound constrained for binary transmission."
"We present a unified large system analysis of linear receivers for a class of
random matrix channels. The technique unifies the analysis of both the
minimum-mean-squared-error (MMSE) receiver and the adaptive least-squares (ALS)
receiver, and also uses a common approach for both random i.i.d. and random
orthogonal precoding. We derive expressions for the asymptotic
signal-to-interference-plus-noise (SINR) of the MMSE receiver, and both the
transient and steady-state SINR of the ALS receiver, trained using either
i.i.d. data sequences or orthogonal training sequences. The results are in
terms of key system parameters, and allow for arbitrary distributions of the
power of each of the data streams and the eigenvalues of the channel
correlation matrix. In the case of the ALS receiver, we allow a diagonal
loading constant and an arbitrary data windowing function. For i.i.d. training
sequences and no diagonal loading, we give a fundamental relationship between
the transient/steady-state SINR of the ALS and the MMSE receivers. We
demonstrate that for a particular ratio of receive to transmit dimensions and
window shape, all channels which have the same MMSE SINR have an identical
transient ALS SINR response. We demonstrate several applications of the
results, including an optimization of information throughput with respect to
training sequence length in coded block transmission."
"Generalisations of the bent property of a boolean function are presented, by
proposing spectral analysis with respect to a well-chosen set of local unitary
transforms. Quadratic boolean functions are related to simple graphs and it is
shown that the orbit generated by successive Local Complementations on a graph
can be found within the transform spectra under investigation. The flat spectra
of a quadratic boolean function are related to modified versions of its
associated adjacency matrix."
"In the first part of this paper [16], some results on how to compute the flat
spectra of Boolean constructions w.r.t. the transforms {I,H}^n, {H,N}^n and
{I,H,N}^n were presented, and the relevance of Local Complementation to the
quadratic case was indicated. In this second part, the results are applied to
develop recursive formulae for the numbers of flat spectra of some structural
quadratics. Observations are made as to the generalised Bent properties of
boolean functions of algebraic degree greater than two, and the number of flat
spectra w.r.t. {I,H,N}^n are computed for some of them."
"We present an efficient, low-cost implementation of time-hopping impulse
radio that fulfills the spectral mask mandated by the FCC and is suitable for
high-data-rate, short-range communications. Key features are: (i) all-baseband
implementation that obviates the need for passband components, (ii) symbol-rate
(not chip rate) sampling, A/D conversion, and digital signal processing, (iii)
fast acquisition due to novel search algorithms, (iv) spectral shaping that can
be adapted to accommodate different spectrum regulations and interference
environments. Computer simulations show that this system can provide 110Mbit/s
at 7-10m distance, as well as higher data rates at shorter distances under FCC
emissions limits. Due to the spreading concept of time-hopping impulse radio,
the system can sustain multiple simultaneous users, and can suppress narrowband
interference effectively."
"We present an interleaving scheme that yields quasi-cyclic turbo codes. We
prove that randomly chosen members of this family yield with probability almost
1 turbo codes with asymptotically optimum minimum distance, i.e. growing as a
logarithm of the interleaver size. These interleavers are also very practical
in terms of memory requirements and their decoding error probabilities for
small block lengths compare favorably with previous interleaving schemes."
"In this paper, a class of nonlinear MMSE multiuser detectors are derived
based on a multivariate Gaussian approximation of the multiple access
interference. This approach leads to expressions identical to those describing
the probabilistic data association (PDA) detector, thus providing an
alternative analytical justification for this structure. A simplification to
the PDA detector based on approximating the covariance matrix of the
multivariate Gaussian distribution is suggested, resulting in a soft
interference cancellation scheme. Corresponding multiuser soft-input,
soft-output detectors delivering extrinsic log-likelihood ratios are derived
for application in iterative multiuser decoders. Finally, a large system
performance analysis is conducted for the simplified PDA, showing that the bit
error rate performance of this detector can be accurately predicted and related
to the replica method analysis for the optimal detector. Methods from
statistical neuro-dynamics are shown to provide a closely related alternative
large system prediction. Numerical results demonstrate that for large systems,
the bit error rate is accurately predicted by the analysis and found to be
close to optimal performance."
"The performance of second order statistics (SOS) based semi-blind channel
estimation in long-code DS-CDMA systems is analyzed. The covariance matrix of
SOS estimates is obtained in the large system limit, and is used to analyze the
large-sample performance of two SOS based semi-blind channel estimation
algorithms. A notion of blind estimation efficiency is also defined and is
examined via simulation results."
"The achievable information rate of finite-state input two-dimensional (2-D)
channels with memory is an open problem, which is relevant, e.g., for
inter-symbol-interference (ISI) channels and cellular multiple-access channels.
We propose a method for simulation-based computation of such information rates.
We first draw a connection between the Shannon-theoretic information rate and
the statistical mechanics notion of free energy. Since the free energy of such
systems is intractable, we approximate it using the cluster variation method,
implemented via generalized belief propagation. The derived, fully tractable,
algorithm is shown to provide a practically accurate estimate of the
information rate. In our experimental study we calculate the information rates
of 2-D ISI channels and of hexagonal Wyner cellular networks with binary
inputs, for which formerly only bounds were known."
"We define multilevel codes on bipartite graphs that have properties analogous
to multilevel serial concatenations. A decoding algorithm is described that
corrects a proportion of errors equal to half the Blokh-Zyablov bound on the
minimum distance. The error probability of this algorithm has exponent similar
to that of serially concatenated multilevel codes."
"The problems of sensor configuration and activation for the detection of
correlated random fields using large sensor arrays are considered. Using
results that characterize the large-array performance of sensor networks in
this application, the detection capabilities of different sensor configurations
are analyzed and compared. The dependence of the optimal choice of
configuration on parameters such as sensor signal-to-noise ratio (SNR), field
correlation, etc., is examined, yielding insights into the most effective
choices for sensor selection and activation in various operating regimes."
"Spectral properties and performance of multi-pulse impulse radio
ultra-wideband systems with pulse-based polarity randomization are analyzed.
Instead of a single type of pulse transmitted in each frame, multiple types of
pulses are considered, which is shown to reduce the effects of multiple-access
interference. First, the spectral properties of a multi-pulse impulse radio
system is investigated. It is shown that the power spectral density is the
average of spectral contents of different pulse shapes. Then, approximate
closed-form expressions for bit error probability of a multi-pulse impulse
radio system are derived for RAKE receivers in asynchronous multiuser
environments. The theoretical and simulation results indicate that impulse
radio systems that are more robust against multiple-access interference than a
""classical"" impulse radio system can be designed with multiple types of
ultra-wideband pulses."
"Density evolution (DE) is one of the most powerful analytical tools for
low-density parity-check (LDPC) codes on memoryless
binary-input/symmetric-output channels. The case of non-symmetric channels is
tackled either by the LDPC coset code ensemble (a channel symmetrizing
argument) or by the generalized DE for linear codes on non-symmetric channels.
Existing simulations show that the bit error rate performances of these two
different approaches are nearly identical. This paper explains this phenomenon
by proving that as the minimum check node degree $d_c$ becomes sufficiently
large, the performance discrepancy of the linear and the coset LDPC codes is
theoretically indistinguishable. This typicality of linear codes among the LDPC
coset code ensemble provides insight into the concentration theorem of LDPC
coset codes."
"This paper investigates decoding of binary linear block codes over the binary
erasure channel (BEC). Of the current iterative decoding algorithms on this
channel, we review the Recovery Algorithm and the Guess Algorithm. We then
present a Multi-Guess Algorithm extended from the Guess Algorithm and a new
algorithm -- the In-place Algorithm. The Multi-Guess Algorithm can push the
limit to break the stopping sets. However, the performance of the Guess and the
Multi-Guess Algorithm depend on the parity-check matrix of the code.
Simulations show that we can decrease the frame error rate by several orders of
magnitude using the Guess and the Multi-Guess Algorithms when the parity-check
matrix of the code is sparse. The In-place Algorithm can obtain better
performance even if the parity check matrix is dense. We consider the
application of these algorithms in the implementation of multicast and
broadcast techniques on the Internet. Using these algorithms, a user does not
have to wait until the entire transmission has been received."
"We propose a technique to derive upper bounds on Gallager's cost-constrained
random coding exponent function. Applying this technique to the non-coherent
peak-power or average-power limited discrete time memoryless Ricean fading
channel, we obtain the high signal-to-noise ratio (SNR) expansion of this
channel's cut-off rate. At high SNR the gap between channel capacity and the
cut-off rate approaches a finite limit. This limit is approximately 0.26 nats
per channel-use for zero specular component (Rayleigh) fading and approaches
0.39 nats per channel-use for very large specular components.
  We also compute the asymptotic cut-off rate of a Rayleigh fading channel when
the receiver has access to some partial side information concerning the fading.
It is demonstrated that the cut-off rate does not utilize the side information
as efficiently as capacity, and that the high SNR gap between the two increases
to infinity as the imperfect side information becomes more and more precise."
"A binary linear error correcting codes represented by two code families
Kronecker products sum are considered. The dimension and distance of new code
is investigated. Upper and lower bounds of distance are obtained. Some examples
are given. It is shown that some classic constructions are the private cases of
considered one. The subclass of codes with equal lower and upper distance
bounds is allocated."
"This paper studies a two-tier CDMA system in which the microcell base is
converted into a data access point (DAP), i.e., a limited-range base station
that provides high-speed access to one user at a time. The microcell (or DAP)
user operates on the same frequency as the macrocell users and has the same
chip rate. However, it adapts its spreading factor, and thus its data rate, in
accordance with interference conditions. By contrast, the macrocell serves
multiple simultaneous data users, each with the same fixed rate. The
achieveable throughput for individual microcell users is examined and a simple,
accurate approximation for its probability distribution is presented.
Computations for average throughputs, both per-user and total, are also
presented. The numerical results highlight the impact of a desensitivity
parameter used in the base-selection process."
"This paper examines the effect of soft handoff on the uplink user capacity of
a CDMA system consisting of a single macrocell in which a single hotspot
microcell is embedded. The users of these two base stations operate over the
same frequency band. In the soft handoff scenario studied here, both macrocell
and microcell base stations serve each system user and the two received copies
of a desired user's signal are summed using maximal ratio combining. Exact and
approximate analytical methods are developed to compute uplink user capacity.
Simulation results demonstrate a 20% increase in user capacity compared to hard
handoff. In addition, simple, approximate methods are presented for estimating
soft handoff capacity and are shown to be quite accurate."
"This paper examines the uplink user capacity in a two-tier code division
multiple access (CDMA) system with hotspot microcells when user terminal power
is limited and the wireless channel is finitely-dispersive. A
finitely-dispersive channel causes variable fading of the signal power at the
output of the RAKE receiver. First, a two-cell system composed of one macrocell
and one embedded microcell is studied and analytical methods are developed to
estimate the user capacity as a function of a dimensionless parameter that
depends on the transmit power constraint and cell radius. Next, novel
analytical methods are developed to study the effect of variable fading, both
with and without transmit power constraints. Finally, the analytical methods
are extended to estimate uplink user capacity for multicell CDMA systems,
composed of multiple macrocells and multiple embedded microcells. In all cases,
the analysis-based estimates are compared with and confirmed by simulation
results."
"We consider the capacity problem for wireless networks. Networks are modeled
as random unit-disk graphs, and the capacity problem is formulated as one of
finding the maximum value of a multicommodity flow. In this paper, we develop a
proof technique based on which we are able to obtain a tight characterization
of the solution to the linear program associated with the multiflow problem, to
within constants independent of network size. We also use this proof method to
analyze network capacity for a variety of transmitter/receiver architectures,
for which we obtain some conclusive results. These results contain as a special
case (and strengthen) those of Gupta and Kumar for random networks, for which a
new derivation is provided using only elementary counting and discrete
probability tools."
"This paper studies randomly spread code-division multiple access (CDMA) and
multiuser detection in the large-system limit using the replica method
developed in statistical physics. Arbitrary input distributions and flat fading
are considered. A generic multiuser detector in the form of the posterior mean
estimator is applied before single-user decoding. The generic detector can be
particularized to the matched filter, decorrelator, linear MMSE detector, the
jointly or the individually optimal detector, and others. It is found that the
detection output for each user, although in general asymptotically non-Gaussian
conditioned on the transmitted symbol, converges as the number of users go to
infinity to a deterministic function of a ""hidden"" Gaussian statistic
independent of the interferers. Thus the multiuser channel can be decoupled:
Each user experiences an equivalent single-user Gaussian channel, whose
signal-to-noise ratio suffers a degradation due to the multiple-access
interference. The uncoded error performance (e.g., symbol-error-rate) and the
mutual information can then be fully characterized using the degradation
factor, also known as the multiuser efficiency, which can be obtained by
solving a pair of coupled fixed-point equations identified in this paper. Based
on a general linear vector channel model, the results are also applicable to
MIMO channels such as in multiantenna systems."
"We present a new algorithm for dynamic prefix-free coding, based on Shannon
coding. We give a simple analysis and prove a better upper bound on the length
of the encoding produced than the corresponding bound for dynamic Huffman
coding. We show how our algorithm can be modified for efficient
length-restricted coding, alphabetic coding and coding with unequal letter
costs."
"Several non-asymptotic formulas are established in channel resolvability and
identification capacity, and they are applied to wire-tap channel. By using
these formulas, the $\epsilon$ capacities of the above three problems are
considered in the most general setting, where no structural assumptions such as
the stationary memoryless property are made on a channel. As a result, we solve
an open problem proposed in Han & Verdu and Han. Moreover, we obtain lower
bounds of the exponents of error probability and the wire-tapper's information
in wire-tap channel."
"Second order asymptotics of fixed-length source coding and intrinsic
randomness is discussed with a constant error constraint. There was a
difference between optimal rates of fixed-length source coding and intrinsic
randomness, which never occurred in the first order asymptotics. In addition,
the relation between uniform distribution and compressed data is discussed
based on this fact. These results are valid for general information sources as
well as independent and identical distributions. A universal code attaining the
second order optimal rate is also constructed."
"The multiple description (MD) problem has received considerable attention as
a model of information transmission over unreliable channels. A general
framework for designing efficient multiple description quantization schemes is
proposed in this paper. We provide a systematic treatment of the El Gamal-Cover
(EGC) achievable MD rate-distortion region, and show that any point in the EGC
region can be achieved via a successive quantization scheme along with
quantization splitting. For the quadratic Gaussian case, the proposed scheme
has an intrinsic connection with the Gram-Schmidt orthogonalization, which
implies that the whole Gaussian MD rate-distortion region is achievable with a
sequential dithered lattice-based quantization scheme as the dimension of the
(optimal) lattice quantizers becomes large. Moreover, this scheme is shown to
be universal for all i.i.d. smooth sources with performance no worse than that
for an i.i.d. Gaussian source with the same variance and asymptotically optimal
at high resolution. A class of low-complexity MD scalar quantizers in the
proposed general framework also is constructed and is illustrated
geometrically; the performance is analyzed in the high resolution regime, which
exhibits a noticeable improvement over the existing MD scalar quantization
schemes."
"We address the problem of constructing a fast lossless code in the case when
the source alphabet is large. The main idea of the new scheme may be described
as follows. We group letters with small probabilities in subsets (acting as
super letters) and use time consuming coding for these subsets only, whereas
letters in the subsets have the same code length and therefore can be coded
fast. The described scheme can be applied to sources with known and unknown
statistics."
"We address the problem of detecting deviations of binary sequence from
randomness,which is very important for random number (RNG) and pseudorandom
number generators (PRNG). Namely, we consider a null hypothesis $H_0$ that a
given bit sequence is generated by Bernoulli source with equal probabilities of
0 and 1 and the alternative hypothesis $H_1$ that the sequence is generated by
a stationary and ergodic source which differs from the source under $H_0$. We
show that data compression methods can be used as a basis for such testing and
describe two new tests for randomness, which are based on ideas of universal
coding. Known statistical tests and suggested ones are applied for testing
PRNGs. Those experiments show that the power of the new tests is greater than
of many known algorithms."
"In this paper, the average coset weight distribution (ACWD) of structured
ensembles of LDPC (Low-density Parity-Check) matrix, which is called combined
ensembles, is discussed. A combined ensemble is composed of a set of simpler
ensembles such as a regular bipartite ensemble. Two classes of combined
ensembles have prime importance; a stacked ensemble and a concatenated
ensemble, which consists of set of stacked matrices and concatenated matrices,
respectively. The ACWD formulas of these ensembles is shown in this paper. Such
formulas are key tools to evaluate the ACWD of a complex combined ensemble.
  From the ACWD of an ensemble, we can obtain some detailed properties of a
code (e.g., weight of coset leaders) which is not available from an average
weight distribution. Moreover, it is shown that the analysis based on the ACWD
is indispensable to evaluate the average weight distribution of some classes of
combined ensembles."
"This papers presents a detailed analysis of pseudocodewords of Tanner graphs.
Pseudocodewords arising on the iterative decoder's computation tree are
distinguished from pseudocodewords arising on finite degree lifts. Lower bounds
on the minimum pseudocodeword weight are presented for the BEC, BSC, and AWGN
channel. Some structural properties of pseudocodewords are examined, and
pseudocodewords and graph properties that are potentially problematic with
min-sum iterative decoding are identified. An upper bound on the minimum degree
lift needed to realize a particular irreducible lift-realizable pseudocodeword
is given in terms of its maximal component, and it is shown that all
irreducible lift-realizable pseudocodewords have components upper bounded by a
finite value $t$ that is dependent on the graph structure. Examples and
different Tanner graph representations of individual codes are examined and the
resulting pseudocodeword distributions and iterative decoding performances are
analyzed. The results obtained provide some insights in relating the structure
of the Tanner graph to the pseudocodeword distribution and suggest ways of
designing Tanner graphs with good minimum pseudocodeword weight."
"In this paper, we consider a network communications problem in which multiple
correlated sources must be delivered to a single data collector node, over a
network of noisy independent point-to-point channels. We prove that perfect
reconstruction of all the sources at the sink is possible if and only if, for
all partitions of the network nodes into two subsets S and S^c such that the
sink is always in S^c, we have that H(U_S|U_{S^c}) < \sum_{i\in S,j\in S^c}
C_{ij}. Our main finding is that in this setup a general source/channel
separation theorem holds, and that Shannon information behaves as a classical
network flow, identical in nature to the flow of water in pipes. At first
glance, it might seem surprising that separation holds in a fairly general
network situation like the one we study. A closer look, however, reveals that
the reason for this is that our model allows only for independent
point-to-point channels between pairs of nodes, and not multiple-access and/or
broadcast channels, for which separation is well known not to hold. This
``information as flow'' view provides an algorithmic interpretation for our
results, among which perhaps the most important one is the optimality of
implementing codes using a layered protocol stack."
"This paper presents a method for jointly designing the transmitter-receiver
pair in a block-by-block communication system that employs (intra-block)
decision feedback detection. We provide closed-form expressions for
transmitter-receiver pairs that simultaneously minimize the arithmetic mean
squared error (MSE) at the decision point (assuming perfect feedback), the
geometric MSE, and the bit error rate of a uniformly bit-loaded system at
moderate-to-high signal-to-noise ratios. Separate expressions apply for the
``zero-forcing'' and ``minimum MSE'' (MMSE) decision feedback structures. In
the MMSE case, the proposed design also maximizes the Gaussian mutual
information and suggests that one can approach the capacity of the block
transmission system using (independent instances of) the same (Gaussian) code
for each element of the block. Our simulation studies indicate that the
proposed transceivers perform significantly better than standard transceivers,
and that they retain their performance advantages in the presence of error
propagation."
"In this paper we propose a new soft-input soft-output equalization algorithm,
offering very good performance/complexity tradeoffs. It follows the structure
of the BCJR algorithm, but dynamically constructs a simplified trellis during
the forward recursion. In each trellis section, only the M states with the
strongest forward metric are preserved, similar to the M-BCJR algorithm. Unlike
the M-BCJR, however, the remaining states are not deleted, but rather merged
into the surviving states. The new algorithm compares favorably with the
reduced-state BCJR algorithm, offering better performance and more flexibility,
particularly for systems with higher order modulations."
"The story of the Viterbi algorithm (VA) is told from a personal perspective.
Applications both within and beyond communications are discussed. In brief
summary, the VA has proved to be an extremely important algorithm in a
surprising variety of fields."
"Cooperative optimization is a new way for finding global optima of
complicated functions of many variables. It has some important properties not
possessed by any conventional optimization methods. It has been successfully
applied in solving many large scale optimization problems in image processing,
computer vision, and computational chemistry. This paper shows the application
of this optimization principle in decoding LDPC codes, which is another hard
combinatorial optimization problem. In our experiments, it significantly
out-performed the sum-product algorithm, the best known method for decoding
LDPC codes. Compared to the sum-product algorithm, our algorithm reduced the
error rate further by three fold, improved the speed by six times, and lowered
error floors dramatically in the decoding."
"We show that the Extrinsic Information about the coded bits of any good
(capacity achieving) code operating over a wide class of discrete memoryless
channels (DMC) is zero when channel capacity is below the code rate and
positive constant otherwise, that is, the Extrinsic Information Transfer (EXIT)
chart is a step function of channel quality, for any capacity achieving code.
It follows that, for a common class of iterative receivers where the error
correcting decoder must operate at first iteration at rate above capacity (such
as in turbo equalization, turbo channel estimation, parallel and serial
concatenated coding and the like), classical good codes which achieve capacity
over the DMC are not effective and should be replaced by different new ones.
Another meaning of the results is that a good code operating at rate above
channel capacity falls apart into its individual transmitted symbols in the
sense that all the information about a coded transmitted symbol is contained in
the corresponding received symbol and no information about it can be inferred
from the other received symbols. The binary input additive white Gaussian noise
channel is treated in part 1 of this report. Part 2 extends the results to the
symmetric binary channel and to the binary erasure channel and provides an
heuristic extension to wider class of channel models."
"In this letter, the SNR value at which the error performance curve of a soft
decision maximum likelihood decoder reaches the slope corresponding to the code
minimum distance is determined for a random code. Based on this value, referred
to as the critical point, new insight about soft bounded distance decoding of
random-like codes (and particularly Reed-Solomon codes) is provided."
"We study the entropy rate of pattern sequences of stochastic processes, and
its relationship to the entropy rate of the original process. We give a
complete characterization of this relationship for i.i.d. processes over
arbitrary alphabets, stationary ergodic processes over discrete alphabets, and
a broad family of stationary ergodic processes over uncountable alphabets. For
cases where the entropy rate of the pattern process is infinite, we
characterize the possible growth rate of the block entropy."
"Bounds on the entropy of patterns of sequences generated by independently
identically distributed (i.i.d.) sources are derived. A pattern is a sequence
of indices that contains all consecutive integer indices in increasing order of
first occurrence. If the alphabet of a source that generated a sequence is
unknown, the inevitable cost of coding the unknown alphabet symbols can be
exploited to create the pattern of the sequence. This pattern can in turn be
compressed by itself. The bounds derived here are functions of the i.i.d.
source entropy, alphabet size, and letter probabilities. It is shown that for
large alphabets, the pattern entropy must decrease from the i.i.d. one. The
decrease is in many cases more significant than the universal coding redundancy
bounds derived in prior works. The pattern entropy is confined between two
bounds that depend on the arrangement of the letter probabilities in the
probability space. For very large alphabets whose size may be greater than the
coded pattern length, all low probability letters are packed into one symbol.
The pattern entropy is upper and lower bounded in terms of the i.i.d. entropy
of the new packed alphabet. Correction terms, which are usually negligible, are
provided for both upper and lower bounds."
"The goal of a denoising algorithm is to recover a signal from its
noise-corrupted observations. Perfect recovery is seldom possible and
performance is measured under a given single-letter fidelity criterion. For
discrete signals corrupted by a known discrete memoryless channel, the DUDE was
recently shown to perform this task asymptotically optimally, without knowledge
of the statistical properties of the source. In the present work we address the
scenario where, in addition to the lack of knowledge of the source statistics,
there is also uncertainty in the channel characteristics. We propose a family
of discrete denoisers and establish their asymptotic optimality under a minimax
performance criterion which we argue is appropriate for this setting. As we
show elsewhere, the proposed schemes can also be implemented computationally
efficiently."
"The problem of predicting a sequence $x_1,x_2,...$ generated by a discrete
source with unknown statistics is considered. Each letter $x_{t+1}$ is
predicted using information on the word $x_1x_2... x_t$ only. In fact, this
problem is a classical problem which has received much attention. Its history
can be traced back to Laplace. We address the problem where each $x_i$ belongs
to some large (or even infinite) alphabet. A method is presented for which the
precision is greater than for known algorithms, where precision is estimated by
the Kullback-Leibler divergence. The results can readily be translated to
results about adaptive coding."
"The mutual information of a discrete time memoryless Rayleigh fading channel
is considered, where neither the transmitter nor the receiver has the knowledge
of the channel state information except the fading statistics. We present the
mutual information of this channel in closed form when the input distribution
is complex Gaussian, and derive a lower bound in terms of the capacity of the
corresponding non fading channel and the capacity when the perfect channel
state information is known at the receiver."
"A discrete-time single-user scalar channel with temporally correlated
Rayleigh fading is analyzed. There is no side information at the transmitter or
the receiver. A simple expression is given for the capacity per unit energy, in
the presence of a peak constraint. The simple formula of Verdu for capacity per
unit cost is adapted to a channel with memory, and is used in the proof. In
addition to bounding the capacity of a channel with correlated fading, the
result gives some insight into the relationship between the correlation in the
fading process and the channel capacity. The results are extended to a channel
with side information, showing that the capacity per unit energy is one nat per
Joule, independently of the peak power constraint.
  A continuous-time version of the model is also considered. The capacity per
unit energy subject to a peak constraint (but no bandwidth constraint) is given
by an expression similar to that for discrete time, and is evaluated for
Gauss-Markov and Clarke fading channels."
"A novel, non-trivial, probabilistic upper bound on the entropy of an unknown
one-dimensional distribution, given the support of the distribution and a
sample from that distribution, is presented. No knowledge beyond the support of
the unknown distribution is required, nor is the distribution required to have
a density. Previous distribution-free bounds on the cumulative distribution
function of a random variable given a sample of that variable are used to
construct the bound. A simple, fast, and intuitive algorithm for computing the
entropy bound from a sample is provided."
"This article introduces a new DNA sequence compression algorithm which is
based on LUT and LZ77 algorithm. Combined a LUT-based pre-coding routine and
LZ77 compression routine,this algorithm can approach a compression ratio of
1.9bits \slash base and even lower.The biggest advantage of this algorithm is
fast execution, small memory occupation and easy implementation."
"We enumerate the inequivalent self-dual additive codes over GF(4) of
blocklength n, thereby extending the sequence A090899 in The On-Line
Encyclopedia of Integer Sequences from n = 9 to n = 12. These codes have a
well-known interpretation as quantum codes. They can also be represented by
graphs, where a simple graph operation generates the orbits of equivalent
codes. We highlight the regularity and structure of some graphs that correspond
to codes with high distance. The codes can also be interpreted as quadratic
Boolean functions, where inequivalence takes on a spectral meaning. In this
context we define PAR_IHN, peak-to-average power ratio with respect to the
{I,H,N}^n transform set. We prove that PAR_IHN of a Boolean function is
equivalent to the the size of the maximum independent set over the associated
orbit of graphs. Finally we propose a construction technique to generate
Boolean functions with low PAR_IHN and algebraic degree higher than 2."
"We consider a variation of the Wyner-Ziv problem pertaining to lossy
compression of individual sequences using finite-state encoders and decoders.
There are two main results in this paper. The first characterizes the
relationship between the performance of the best $M$-state encoder-decoder pair
to that of the best block code of size $\ell$ for every input sequence, and
shows that the loss of the latter relative to the former (in terms of both rate
and distortion) never exceeds the order of $(\log M)/\ell$, independently of
the input sequence. Thus, in the limit of large $M$, the best rate-distortion
performance of every infinite source sequence can be approached universally by
a sequence of block codes (which are also implementable by finite-state
machines). While this result assumes an asymptotic regime where the number of
states is fixed, and only the length $n$ of the input sequence grows without
bound, we then consider the case where the number of states $M=M_n$ is allowed
to grow concurrently with $n$. Our second result is then about the critical
growth rate of $M_n$ such that the rate-distortion performance of $M_n$-state
encoder-decoder pairs can still be matched by a universal code. We show that
this critical growth rate of $M_n$ is linear in $n$."
"We consider the Shannon cipher system in a setting where the secret key is
delivered to the legitimate receiver via a channel with limited capacity. For
this setting, we characterize the achievable region in the space of three
figures of merit: the security (measured in terms of the equivocation), the
compressibility of the cryptogram, and the distortion associated with the
reconstruction of the plaintext source. Although lossy reconstruction of the
plaintext does not rule out the option that the (noisy) decryption key would
differ, to a certain extent, from the encryption key, we show, nevertheless,
that the best strategy is to strive for perfect match between the two keys, by
applying reliable channel coding to the key bits, and to control the distortion
solely via rate-distortion coding of the plaintext source before the
encryption. In this sense, our result has a flavor similar to that of the
classical source-channel separation theorem. Some variations and extensions of
this model are discussed as well."
"In this paper we deal with a single-antenna discrete-time flat-fading
channel. The fading process is assumed to be stationary for the duration of a
single data block. From block to block the fading process is allowed to be
non-stationary. The number of scatterers bounds the rank of the channels
covariance matrix. The signal-to-noise ratio (SNR), the user velocity, and the
data block-length define the usable rank of the time-variant channel subspace.
The usable channel subspace grows with the SNR. This growth in dimensionality
must be taken into account for asymptotic capacity results in the high-SNR
regime. Using results from the theory of time-concentrated and band-limited
sequences we are able to define an SNR threshold below which the capacity grows
logarithmically. Above this threshold the capacity grows
double-logarithmically."
"We consider the problem of communicating over the general discrete memoryless
broadcast channel (BC) with partially cooperating receivers. In our setup,
receivers are able to exchange messages over noiseless conference links of
finite capacities, prior to decoding the messages sent from the transmitter. In
this paper we formulate the general problem of broadcast with cooperation. We
first find the capacity region for the case where the BC is physically
degraded. Then, we give achievability results for the general broadcast
channel, for both the two independent messages case and the single common
message case."
"In this correspondence, first-tier indirect (direct) discernible
constellation expansions are defined for generalized orthogonal designs. The
expanded signal constellation, leading to so-called super-orthogonal codes,
allows the achievement of coding gains in addition to diversity gains enabled
by orthogonal designs. Conditions that allow the shape of an expanded
multidimensional constellation to be preserved at the channel output, on an
instantaneous basis, are derived. It is further shown that, for such
constellations, the channel alters neither the relative distances nor the
angles between signal points in the expanded signal constellation."
"A closed form formula of the partition weight enumerator of maximum distance
separable (MDS) codes is derived for an arbitrary number of partitions. Using
this result, some properties of MDS codes are discussed. The results are
extended for the average binary image of MDS codes in finite fields of
characteristic two. As an application, we study the multiuser error probability
of Reed Solomon codes."
"We derive bounds on the asymptotic density of parity-check matrices and the
achievable rates of binary linear block codes transmitted over memoryless
binary-input output-symmetric (MBIOS) channels. The lower bounds on the density
of arbitrary parity-check matrices are expressed in terms of the gap between
the rate of these codes for which reliable communication is achievable and the
channel capacity, and the bounds are valid for every sequence of binary linear
block codes. These bounds address the question, previously considered by Sason
and Urbanke, of how sparse can parity-check matrices of binary linear block
codes be as a function of the gap to capacity. Similarly to a previously
reported bound by Sason and Urbanke, the new lower bounds on the parity-check
density scale like the log of the inverse of the gap to capacity, but their
tightness is improved (except for a binary symmetric/erasure channel, where
they coincide with the previous bound). The new upper bounds on the achievable
rates of binary linear block codes tighten previously reported bounds by
Burshtein et al., and therefore enable to obtain tighter upper bounds on the
thresholds of sequences of binary linear block codes under ML decoding. The
bounds are applied to low-density parity-check (LDPC) codes, and the
improvement in their tightness is exemplified numerically. The upper bounds on
the achievable rates enable to assess the inherent loss in performance of
various iterative decoding algorithms as compared to optimal ML decoding. The
lower bounds on the asymptotic parity-check density are helpful in assessing
the inherent tradeoff between the asymptotic performance of LDPC codes and
their decoding complexity (per iteration) under message-passing decoding."
"The paper introduces new bounds on the asymptotic density of parity-check
matrices and the achievable rates under ML decoding of binary linear block
codes transmitted over memoryless binary-input output-symmetric channels. The
lower bounds on the parity-check density are expressed in terms of the gap
between the channel capacity and the rate of the codes for which reliable
communication is achievable, and are valid for every sequence of binary linear
block codes. The bounds address the question, previously considered by Sason
and Urbanke, of how sparse can parity-check matrices of binary linear block
codes be as a function of the gap to capacity. The new upper bounds on the
achievable rates of binary linear block codes tighten previously reported
bounds by Burshtein et al., and therefore enable to obtain tighter upper bounds
on the thresholds of sequences of binary linear block codes under ML decoding.
The bounds are applied to low-density parity-check (LDPC) codes, and the
improvement in their tightness is exemplified numerically."
"We propose two approximate algorithms for MAP decoding on tail-biting
trellises. The algorithms work on a subset of nodes of the tail-biting trellis,
judiciously selected. We report the results of simulations on an AWGN channel
using the approximate algorithms on tail-biting trellises for the $(24,12)$
Extended Golay Code and a rate 1/2 convolutional code with memory 6."
"A game-theoretic approach for studying power control in multiple-access
networks with transmission delay constraints is proposed. A non-cooperative
power control game is considered in which each user seeks to choose a transmit
power that maximizes its own utility while satisfying the user's delay
requirements. The utility function measures the number of reliable bits
transmitted per joule of energy and the user's delay constraint is modeled as
an upper bound on the delay outage probability. The Nash equilibrium for the
proposed game is derived, and its existence and uniqueness are proved. Using a
large-system analysis, explicit expressions for the utilities achieved at
equilibrium are obtained for the matched filter, decorrelating and minimum mean
square error multiuser detectors. The effects of delay constraints on the
users' utilities (in bits/Joule) and network capacity (i.e., the maximum number
of users that can be supported) are quantified."
"A broad set of sufficient conditions that guarantees the existence of the
maximum entropy (maxent) distribution consistent with specified bounds on
certain generalized moments is derived. Most results in the literature are
either focused on the minimum cross-entropy distribution or apply only to
distributions with a bounded-volume support or address only equality
constraints. The results of this work hold for general moment inequality
constraints for probability distributions with possibly unbounded support, and
the technical conditions are explicitly on the underlying generalized moment
functions. An analytical characterization of the maxent distribution is also
derived using results from the theory of constrained optimization in
infinite-dimensional normed linear spaces. Several auxiliary results of
independent interest pertaining to certain properties of convex coercive
functions are also presented."
"We show how to store good approximations of probability distributions in
small space."
"In this paper, we propose novel cooperative transmission protocols for delay
limited coherent fading channels consisting of N (half-duplex and
single-antenna) partners and one cell site. In our work, we differentiate
between the relay, cooperative broadcast (down-link), and cooperative
multiple-access (up-link) channels. For the relay channel, we investigate two
classes of cooperation schemes; namely, Amplify and Forward (AF) protocols and
Decode and Forward (DF) protocols. For the first class, we establish an upper
bound on the achievable diversity-multiplexing tradeoff with a single relay. We
then construct a new AF protocol that achieves this upper bound. The proposed
algorithm is then extended to the general case with N-1 relays where it is
shown to outperform the space-time coded protocol of Laneman and Worenell
without requiring decoding/encoding at the relays. For the class of DF
protocols, we develop a dynamic decode and forward (DDF) protocol that achieves
the optimal tradeoff for multiplexing gains 0 < r < 1/N. Furthermore, with a
single relay, the DDF protocol is shown to dominate the class of AF protocols
for all multiplexing gains. The superiority of the DDF protocol is shown to be
more significant in the cooperative broadcast channel. The situation is
reversed in the cooperative multiple-access channel where we propose a new AF
protocol that achieves the optimal tradeoff for all multiplexing gains. A
distinguishing feature of the proposed protocols in the three scenarios is that
they do not rely on orthogonal subspaces, allowing for a more efficient use of
resources. In fact, using our results one can argue that the sub-optimality of
previously proposed protocols stems from their use of orthogonal subspaces
rather than the half-duplex constraint."
"In this paper, we adopt a cross layer design approach for analyzing the
throughput-delay tradeoff of the multicast channel in a single cell system. To
illustrate the main ideas, we start with the single group case, i.e., pure
multicast, where a common information stream is requested by all the users. We
consider three classes of scheduling algorithms with progressively increasing
complexity. The first class strives for minimum complexity by resorting to a
static scheduling strategy along with memoryless decoding. Our analysis for
this class of scheduling algorithms reveals the existence of a static
scheduling policy that achieves the optimal scaling law of the throughput at
the expense of a delay that increases exponentially with the number of users.
The second scheduling policy resorts to a higher complexity incremental
redundancy encoding/decoding strategy to achieve a superior throughput-delay
tradeoff. The third, and most complex, scheduling strategy benefits from the
cooperation between the different users to minimize the delay while achieving
the optimal scaling law of the throughput. In particular, the proposed
cooperative multicast strategy is shown to simultaneously achieve the optimal
scaling laws of both throughput and delay. Then, we generalize our scheduling
algorithms to exploit the multi-group diversity available when different
information streams are requested by different subsets of the user population.
Finally, we discuss the effect of the potential gains of equipping the base
station with multi-transmit antennas and present simulation results that
validate our theoretical claims."
"We show how any dynamic instantaneous compression algorithm can be converted
to an asymmetric communication protocol, with which a server with high
bandwidth can help clients with low bandwidth send it messages. Unlike previous
authors, we do not assume the server knows the messages' distribution, and our
protocols are the first to use only one round of communication for each
message."
"The performance of Neyman-Pearson detection of correlated stochastic signals
using noisy observations is investigated via the error exponent for the miss
probability with a fixed level. Using the state-space structure of the signal
and observation model, a closed-form expression for the error exponent is
derived, and the connection between the asymptotic behavior of the optimal
detector and that of the Kalman filter is established. The properties of the
error exponent are investigated for the scalar case. It is shown that the error
exponent has distinct characteristics with respect to correlation strength: for
signal-to-noise ratio (SNR) >1 the error exponent decreases monotonically as
the correlation becomes stronger, whereas for SNR <1 there is an optimal
correlation that maximizes the error exponent for a given SNR."
"We consider receiver design for coded transmission over linear Gaussian
channels. We restrict ourselves to the class of lattice codes and formulate the
joint detection and decoding problem as a closest lattice point search (CLPS).
Here, a tree search framework for solving the CLPS is adopted. In our
framework, the CLPS algorithm decomposes into the preprocessing and tree search
stages. The role of the preprocessing stage is to expose the tree structure in
a form {\em matched} to the search stage. We argue that the minimum mean square
error decision feedback (MMSE-DFE) frontend is instrumental for solving the
joint detection and decoding problem in a single search stage. It is further
shown that MMSE-DFE filtering allows for using lattice reduction methods to
reduce complexity, at the expense of a marginal performance loss, and solving
under-determined linear systems. For the search stage, we present a generic
method, based on the branch and bound (BB) algorithm, and show that it
encompasses all existing sphere decoders as special cases. The proposed generic
algorithm further allows for an interesting classification of tree search
decoders, sheds more light on the structural properties of all known sphere
decoders, and inspires the design of more efficient decoders. In particular, an
efficient decoding algorithm that resembles the well known Fano sequential
decoder is identified. The excellent performance-complexity tradeoff achieved
by the proposed MMSE-Fano decoder is established via simulation results and
analytical arguments in several MIMO and ISI scenarios."
"In this paper we consider the use of variable length non prefix-free codes
for coding constrained sequences of symbols. We suppose to have a Markov source
where some state transitions are impossible, i.e. the stochastic matrix
associated with the Markov chain has some null entries. We show that classic
Kraft inequality is not a necessary condition, in general, for unique
decodability under the above hypothesis and we propose a relaxed necessary
inequality condition. This allows, in some cases, the use of non prefix-free
codes that can give very good performance, both in terms of compression and
computational efficiency. Some considerations are made on the relation between
the proposed approach and other existing coding paradigms."
"In this paper, we investigate the optimal tradeoff between source and channel
coding for channels with bit or packet erasure. Upper and Lower bounds on the
optimal channel coding rate are computed to achieve minimal end-to-end
distortion. The bounds are calculated based on a combination of sphere packing,
straight line and expurgated error exponents and also high rate vector
quantization theory. By modeling a packet erasure channel in terms of an
equivalent bit erasure channel, we obtain bounds on the packet size for a
specified limit on the distortion."
"This paper provides details about experiments in realistic, urban, and
frequency flat channels with space-time coding that specifically examines the
impact of the number of receive antennas and the design criteria for code
selection on the performance. Also the performance characteristics are examined
of the coded modulations in the presence of finite size array geometries. This
paper gives some insight into which of the theories are most useful in
realistic deployments."
"While achieving a compression ratio of 2.0 bits/base, the new algorithm codes
non-N bases in fixed length. It dramatically reduces the time of coding and
decoding than previous DNA compression algorithms and some universal
compression programs."
"We present a construction of LDPC codes that have minimum pseudocodeword
weight equal to the minimum distance, and perform well with iterative decoding.
The construction involves enumerating a d-regular tree for a fixed number of
layers and employing a connection algorithm based on mutually orthogonal Latin
squares to close the tree. Methods are presented for degrees d=p^s and d =
p^s+1, for p a prime, -- one of which includes the well-known
finite-geometry-based LDPC codes."
"We consider the problem of compression of two memoryless binary sources, the
correlation between which is defined by a Hidden Markov Model (HMM). We propose
a Decision Feedback (DF) based scheme which when used with low density parity
check codes results in compression close to the Slepian Wolf limits."
"We give an information flow interpretation for multicasting using network
coding. This generalizes the fluid model used to represent flows to a single
receiver. Using the generalized model, we present a decentralized algorithm to
minimize the number of packets that undergo network coding. We also propose a
decentralized algorithm to construct capacity achieving multicast codes when
the processing at some nodes is restricted to routing. The proposed algorithms
can be coupled with existing decentralized schemes to achieve minimum cost
muticast."
"We consider the problem of compression of two memoryless binary sources, the
correlation between which is defined by a Hidden Markov Model (HMM). We propose
a Decision Feedback (DF) based scheme which when used with low density parity
check codes results in compression close to the Slepian Wolf limits."
"Caire, Taricco and Biglieri presented a detailed analysis of bit interleaved
coded modulation, a simple and popular technique used to improve system
performance, especially in the context of fading channels. They derived an
upper bound to the probability of error, called the expurgated bound. In this
correspondence, the proof of the expurgated bound is shown to be flawed. A new
upper bound is also derived. It is not known whether the original expurgated
bound is valid for the important special case of square QAM with Gray labeling,
but the new bound is very close to, and slightly tighter than, the original
bound for a numerical example."
"Recently, the remarkable potential of a multiple-input multiple-output (MIMO)
wireless communication system was unveiled for its ability to provide spatial
diversity or multiplexing gains. For MIMO diversity schemes, it is already
known that. by the optimal antenna selection maximizing the post-processing
signal-to-noise ratio, the diversity order of the full system can be
maintained. On the other hand, the diversity order achieved by antenna
selection in spatial multiplexing systems, especially those exploiting
practical coding and decoding schemes, has not been rigorously analyzed thus
far. In this paper, from a geometric standpoint, we propose a new framework for
theoretically analyzing the diversity order achieved by transmit antenna
selection for separately encoded spatial multiplexing systems with linear and
decision-feedback receivers. We rigorously show that a diversity order of
(Nt-1)(Nr-1) can be achieved for an Nr by Nt SM system when L=2 antennas are
selected from the transmit side; while for L>2 scenarios, we give bounds for
the achievable diversity order and show that the optimal diversity order is at
least (Nt-L+1)(Nr-L+1) . Furthermore, the same geometrical approach can be used
to evaluate the diversity-multiplexing tradeoff curves for the considered
spatial multiplexing systems with transmit antenna selection."
"We briefly survey some concepts related to empirical entropy -- normal
numbers, de Bruijn sequences and Markov processes -- and investigate how well
it approximates Kolmogorov complexity. Our results suggest $\ell$th-order
empirical entropy stops being a reasonable complexity metric for almost all
strings of length $m$ over alphabets of size $n$ about when $n^\ell$ surpasses
$m$."
"An alternative to extrinsic information transfer (EXIT) charts called mean
squared error (MSE) charts that use a measure related to the MSE instead of
mutual information is proposed. Using the relationship between mutual
information and minimum mean squared error (MMSE), a relationship between the
rate of any code and the area under a plot of MSE versus signal to noise ratio
(SNR) is obtained, when the log likelihood ratios (LLR) can be assumed to be
from a Gaussian channel. Using this result, a theoretical justification is
provided for designing concatenated codes by matching the EXIT charts of the
inner and outer decoders, when the LLRs are Gaussian which is typically assumed
for code design using EXIT charts. Finally, for the special case of AWGN
channel it is shown that any capacity achieving code has an EXIT curve that is
flat. This extends Ashikhmin et als results for erasure channels to the
Gaussian channel."
"A computationally tractable CDMA multiuser detection algorithm is developed
based on survey propagation."
"In this paper, we characterize the decoding region of algebraic soft decoding
(ASD) of Reed-Solomon (RS) codes over erasure channels and binary symmetric
channel (BSC). Optimal multiplicity assignment strategies (MAS) are
investigated and tight bounds are derived to show the ASD can significantly
outperform conventional Berlekamp Massey (BM) decoding over these channels for
a wide code rate range. The analysis technique can also be extended to other
channel models, e.g., RS coded modulation over erasure channels."
"An iterative algorithm is presented for soft-input-soft-output (SISO)
decoding of Reed-Solomon (RS) codes. The proposed iterative algorithm uses the
sum product algorithm (SPA) in conjunction with a binary parity check matrix of
the RS code. The novelty is in reducing a submatrix of the binary parity check
matrix that corresponds to less reliable bits to a sparse nature before the SPA
is applied at each iteration. The proposed algorithm can be geometrically
interpreted as a two-stage gradient descent with an adaptive potential
function. This adaptive procedure is crucial to the convergence behavior of the
gradient descent algorithm and, therefore, significantly improves the
performance. Simulation results show that the proposed decoding algorithm and
its variations provide significant gain over hard decision decoding (HDD) and
compare favorably with other popular soft decision decoding methods."
"The stability of scheduled multiaccess communication with random coding and
independent decoding of messages is investigated. The number of messages that
may be scheduled for simultaneous transmission is limited to a given maximum
value, and the channels from transmitters to receiver are quasi-static, flat,
and have independent fades. Requests for message transmissions are assumed to
arrive according to an i.i.d. arrival process. Then, we show the following: (1)
in the limit of large message alphabet size, the stability region has an
interference limited information-theoretic capacity interpretation, (2)
state-independent scheduling policies achieve this asymptotic stability region,
and (3) in the asymptotic limit corresponding to immediate access, the
stability region for non-idling scheduling policies is shown to be identical
irrespective of received signal powers."
"The problem of decentralized detection in a sensor network subjected to a
total average power constraint and all nodes sharing a common bandwidth is
investigated. The bandwidth constraint is taken into account by assuming
non-orthogonal communication between sensors and the data fusion center via
direct-sequence code-division multiple-access (DS-CDMA). In the case of large
sensor systems and random spreading, the asymptotic decentralized detection
performance is derived assuming independent and identically distributed (iid)
sensor observations via random matrix theory. The results show that, even under
both power and bandwidth constraints, it is better to combine many not-so-good
local decisions rather than relying on one (or a few) very-good local
decisions."
This paper has been withdrawn by the author.
"A new construction is proposed for low density parity check (LDPC) codes
using quadratic permutation polynomials over finite integer rings. The
associated graphs for the new codes have both algebraic and pseudo-random
nature, and the new codes are quasi-cyclic. Graph isomorphisms and
automorphisms are identified and used in an efficient search for good codes.
Graphs with girth as large as 12 were found. Upper bounds on the minimum
Hamming distance are found both analytically and algorithmically. The bounds
indicate that the minimum distance grows with block length. Near-codewords are
one of the causes for error floors in LDPC codes; the new construction provides
a good framework for studying near-codewords in LDPC codes. Nine example codes
are given, and computer simulation results show the excellent error performance
of these codes. Finally, connections are made between this new LDPC
construction and turbo codes using interleavers generated by quadratic
permutation polynomials."
"An interleaver is a critical component for the channel coding performance of
turbo codes. Algebraic constructions are of particular interest because they
admit analytical designs and simple, practical hardware implementation.
Contention-free interleavers have been recently shown to be suitable for
parallel decoding of turbo codes. In this correspondence, it is shown that
permutation polynomials generate maximum contention-free interleavers, i.e.,
every factor of the interleaver length becomes a possible degree of parallel
processing of the decoder. Further, it is shown by computer simulations that
turbo codes using these interleavers perform very well for the 3rd Generation
Partnership Project (3GPP) standard."
"We consider a stationary and ergodic source $p$ generated symbols $x_1 ...
x_t$ from some finite set $A$ and a null hypothesis $H_0$ that $p$ is Markovian
source with memory (or connectivity) not larger than $m, (m >= 0).$ The
alternative hypothesis $H_1$ is that the sequence is generated by a stationary
and ergodic source, which differs from the source under $H_0$. In particular,
if $m= 0$ we have the null hypothesis $H_0$ that the sequence is generated by
Bernoully source (or the hypothesis that $x_1 ...x_t$ are independent.) Some
new tests which are based on universal codes and universal predictors, are
suggested."
"We consider a wireless network composed of three nodes and limited by the
half-duplex and total power constraints. This formulation encompasses many of
the special cases studied in the literature and allows for capturing the common
features shared by them. Here, we focus on three special cases, namely 1) Relay
Channel, 2) Multicast Channel, and 3) Conference Channel. These special cases
are judicially chosen to reflect varying degrees of complexity while
highlighting the common ground shared by the different variants of the three
node wireless network. For the relay channel, we propose a new cooperation
scheme that exploits the wireless feedback gain. This scheme combines the
benefits of decode-and-forward and compress-and-forward strategies and avoids
the idealistic feedback assumption adopted in earlier works. Our analysis of
the achievable rate of this scheme reveals the diminishing feedback gain at
both the low and high signal-to-noise ratio regimes. Inspired by the proposed
feedback strategy, we identify a greedy cooperation framework applicable to
both the multicast and conference channels. Our performance analysis reveals
several nice properties of the proposed greedy approach and the central role of
cooperative source-channel coding in exploiting the receiver side information
in the wireless network setting. Our proofs for the cooperative multicast with
side-information rely on novel nested and independent binning encoders along
with a list decoder."
"Due to a large number of multipath components in a typical ultra wideband
(UWB) system, selective Rake (SRake) receivers, which combine energy from a
subset of multipath components, are commonly employed. In order to optimize
system performance, an optimal selection of multipath components to be employed
at fingers of an SRake receiver needs to be considered. In this paper, this
finger selection problem is investigated for a minimum mean square error (MMSE)
UWB SRake receiver. Since the optimal solution is NP hard, a genetic algorithm
(GA) based iterative scheme is proposed, which can achieve near-optimal
performance after a reasonable number of iterations. Simulation results are
presented to compare the performance of the proposed finger selection algorithm
with those of the conventional and optimal schemes."
"High time resolution of ultra wideband (UWB) signals facilitates very precise
positioning capabilities based on time-of-arrival (TOA) measurements. Although
the theoretical lower bound for TOA estimation can be achieved by the maximum
likelihood principle, it is impractical due to the need for extremely high-rate
sampling and the presence of large number of multipath components. On the other
hand, the conventional correlation-based algorithm, which serially searches
possible signal delays, takes a very long time to estimate the TOA of a
received UWB signal. Moreover, the first signal path does not always have the
strongest correlation output. Therefore, first path detection algorithms need
to be considered. In this paper, a data-aided two-step TOA estimation algorithm
is proposed. In order to speed up the estimation process, the first step
estimates the rough TOA of the received signal based on received signal energy.
Then, in the second step, the arrival time of the first signal path is
estimated by considering a hypothesis testing approach. The proposed scheme
uses low-rate correlation outputs, and is able to perform accurate TOA
estimation in reasonable time intervals. The simulation results are presented
to analyze the performance of the estimator."
"In this work, the cross-layer design problem of joint multiuser detection and
power control is studied using a game-theoretic approach. The uplink of a
direct-sequence code division multiple access (DS-CDMA) data network is
considered and a non-cooperative game is proposed in which users in the network
are allowed to choose their uplink receivers as well as their transmit powers
to maximize their own utilities. The utility function measures the number of
reliable bits transmitted by the user per joule of energy consumed. Focusing on
linear receivers, the Nash equilibrium for the proposed game is derived. It is
shown that the equilibrium is one where the powers are SIR-balanced with the
minimum mean square error (MMSE) detector as the receiver. In addition, this
framework is used to study power control games for the matched filter, the
decorrelator, and the MMSE detector; and the receivers' performance is compared
in terms of the utilities achieved at equilibrium (in bits/Joule). The optimal
cooperative solution is also discussed and compared with the non-cooperative
approach. Extensions of the results to the case of multiple receive antennas
are also presented. In addition, an admission control scheme based on
maximizing the total utility in the network is proposed."
"The performance of Bayesian detection of Gaussian signals using noisy
observations is investigated via the error exponent for the average error
probability. Under unknown signal correlation structure or limited processing
capability it is reasonable to use the simple quadratic detector that is
optimal in the case of an independent and identically distributed (i.i.d.)
signal. Using the large deviations principle, the performance of this detector
(which is suboptimal for non-i.i.d. signals) is compared with that of the
optimal detector for correlated signals via the asymptotic relative efficiency
defined as the ratio between sample sizes of two detectors required for the
same performance in the large-sample-size regime. The effects of SNR on the ARE
are investigated. It is shown that the asymptotic efficiency of the simple
quadratic detector relative to the optimal detector converges to one as the SNR
increases without bound for any bounded spectrum, and that the simple quadratic
detector performs as well as the optimal detector for a wide range of the
correlation values at high SNR."
"The minimum distance is one of the most important combinatorial
characterizations of a code. The maximum likelihood decoding problem is one of
the most important algorithmic problems of a code. While these problems are
known to be hard for general linear codes, the techniques used to prove their
hardness often rely on the construction of artificial codes. In general, much
less is known about the hardness of the specific classes of natural linear
codes. In this paper, we show that both problems are
  NP-hard for algebraic geometry codes. We achieve this by reducing a
well-known NP-complete problem to these problems using a randomized algorithm.
The family of codes in the reductions are based on elliptic curves. They have
positive rates, but the alphabet sizes are exponential in the block lengths."
"The maximum-marginal-a-posteriori success rate of statistical decision under
multivariate Gaussian error distribution on an integer lattice is almost
rigorously calculated by using union-bound approximation and Monte Carlo
integration. These calculations are applied to the revelation of the various
possible realizations of the reliable and short-period integer ambiguity
resolution in precise carrier-phase relative positioning by GPS/GNSS. The
theoretical foundation and efficient methodology are systematically developed,
and two types of the enhancement of union-bound approximation are proposed and
examined.
  The results revealed include an extremely high reliability under the
condition of accurate carrier-phase measurements and a large number of visible
satellites, its heavy degradation caused by the slight amount of differentiated
ionospheric delays due to the nonvanishing baseline length between rover and
reference receivers, and the advantages of the use of the multiple carrier
frequencies. The succeeding initialization of the integer ambiguities is shown
to overcome the disadvantageous condition of the nonvanishing baseline length
effectively due to the reasonably assumed temporal and spatial constancy of
differentiated ionospheric delays."
"We present analytical expressions for optimal entropy-constrained
multiple-description lattice vector quantizers which, under high-resolutions
assumptions, minimize the expected distortion for given packet-loss
probabilities. We consider the asymmetric case where packet-loss probabilities
and side entropies are allowed to be unequal and find optimal quantizers for
any number of descriptions in any dimension. We show that the normalized second
moments of the side-quantizers are given by that of an $L$-dimensional sphere
independent of the choice of lattices. Furthermore, we show that the optimal
bit-distribution among the descriptions is not unique. In fact, within certain
limits, bits can be arbitrarily distributed."
"Relations between the local weight distributions of a binary linear code, its
extended code, and its even weight subcode are presented. In particular, for a
code of which the extended code is transitive invariant and contains only
codewords with weight multiples of four, the local weight distribution can be
obtained from that of the extended code. Using the relations, the local weight
distributions of the $(127,k)$ primitive BCH codes for $k\leq50$, the
$(127,64)$ punctured third-order Reed-Muller, and their even weight subcodes
are obtained from the local weight distribution of the $(128,k)$ extended
primitive BCH codes for $k\leq50$ and the $(128,64)$ third-order Reed-Muller
code. We also show an approach to improve an algorithm for computing the local
weight distribution proposed before."
"Consider data transmission over a binary-input additive white Gaussian noise
channel using a binary low-density parity-check code. We ask the following
question: Given a decoder that takes log-likelihood ratios as input, does it
help to modify the log-likelihood ratios before decoding? If we use an optimal
decoder then it is clear that modifying the log-likelihoods cannot possibly
help the decoder's performance, and so the answer is ""no."" However, for a
suboptimal decoder like the linear programming decoder, the answer might be
""yes"": In this paper we prove that for certain interesting classes of
low-density parity-check codes and large enough SNRs, it is advantageous to
truncate the log-likelihood ratios before passing them to the linear
programming decoder."
"A Wiener filter can be interpreted as a cascade of a whitening- and an
estimation filter. This paper gives a detailed investigates of the properties
of these two filters. Then the practical consequences for the overall Wiener
filter are ascertained. It is shown that if the given spectral densities are
smooth (Hoelder continuous) functions, the resulting Wiener filter will always
be stable and can be approximated arbitrarily well by a finite impulse response
(FIR) filter. Moreover, the smoothness of the spectral densities characterizes
how fast the FIR filter approximates the desired filter characteristic. If on
the other hand the spectral densities are continuous but not smooth enough, the
resulting Wiener filter may not be stable."
"Capacity gain from transmitter and receiver cooperation are compared in a
relay network where the cooperating nodes are close together. When all nodes
have equal average transmit power along with full channel state information
(CSI), it is proved that transmitter cooperation outperforms receiver
cooperation, whereas the opposite is true when power is optimally allocated
among the nodes but only receiver phase CSI is available. In addition, when the
nodes have equal average power with receiver phase CSI only, cooperation is
shown to offer no capacity improvement over a non-cooperative scheme with the
same average network power. When the system is under optimal power allocation
with full CSI, the decode-and-forward transmitter cooperation rate is close to
its cut-set capacity upper bound, and outperforms compress-and-forward receiver
cooperation. Moreover, it is shown that full CSI is essential in transmitter
cooperation, while optimal power allocation is essential in receiver
cooperation."
"In this paper new codes for orthogonal frequency-division multiplexing (OFDM)
with tightly controlled peak-to-mean envelope power ratio (PMEPR) are proposed.
We identify a new family of sequences occuring in complementary sets and show
that such sequences form subsets of a new generalization of the Reed--Muller
codes. Contrarily to previous constructions we present a compact description of
such codes, which makes them suitable even for larger block lengths. We also
show that some previous constructions just occur as special cases in our
construction."
"Signature coding for multiple access OR channel is considered. We prove that
in block asynchronous case the upper bound on the minimum code length
asymptotically is the same as in the case of synchronous access."
"An efficient decoder for the generalized first-order Reed-Muller code
RM_q(1,m) is essential for the decoding of various block-coding schemes for
orthogonal frequency-division multiplexing with reduced peak-to-mean power
ratio. We present an efficient and simple maximum-likelihood decoding algorithm
for RM_q(1,m). It is shown that this algorithm has lower complexity than other
previously known maximum-likelihood decoders for RM_q(1,m)."
"An ensemble of LDPC convolutional codes with parity-check matrices composed
of permutation matrices is considered. The convergence of the iterative belief
propagation based decoder for terminated convolutional codes in the ensemble is
analyzed for binary-input output-symmetric memoryless channels using density
evolution techniques. We observe that the structured irregularity in the Tanner
graph of the codes leads to significantly better thresholds when compared to
corresponding LDPC block codes."
"The cutoff rate $R_0(W)$ of a discrete memoryless channel (DMC) $W$ is often
used as a figure of merit, alongside the channel capacity $C(W)$. Given a
channel $W$ consisting of two possibly correlated subchannels $W_1$, $W_2$, the
capacity function always satisfies $C(W_1)+C(W_2) \le C(W)$, while there are
examples for which $R_0(W_1)+R_0(W_2) > R_0(W)$. This fact that cutoff rate can
be ``created'' by channel splitting was noticed by Massey in his study of an
optical modulation system modeled as a $M$'ary erasure channel. This paper
demonstrates that similar gains in cutoff rate can be achieved for general
DMC's by methods of channel combining and splitting. Relation of the proposed
method to Pinsker's early work on cutoff rate improvement and to Imai-Hirakawa
multi-level coding are also discussed."
"Linear codes for error detection on a q-ary symmetric channel are studied. It
is shown that for given dimension k and minimum distance d, there exists a
value \mu(d,k) such that if C is a code of length n >= \mu(d,k), then neither C
nor its dual are good for error detection. For d >> k or k << d good
approximations for \mu(d,k) are given. A generalization to non-linear codes is
also given."
"In this paper we study the redundancy of Huffman codes. In particular, we
consider sources for which the probability of one of the source symbols is
known. We prove a conjecture of Ye and Yeung regarding the upper bound on the
redundancy of such Huffman codes, which yields in a tight upper bound. We also
derive a tight lower bound for the redundancy under the same assumption.
  We further apply the method introduced in this paper to other related
problems. It is shown that several other previously known bounds with different
constraints follow immediately from our results."
"Capacity of M-ary Amplitude and Phase-Shift Keying(M-APSK) over an Additive
White Gaussian Noise(AWGN) channel that also introduces an unknown carrier
phase rotation is considered. The phase remains constant over a block of L
symbols and it is independent from block to block. Aiming to design codes with
equally probable symbols, uniformly distributed channel inputs are assumed.
Based on results of Peleg and Shamai for M-ary Phase Shift Keying(M-PSK)
modulation, easily computable upper and lower bounds on the effective M-APSK
capacity are derived. For moderate M and L and a broad range of Signal-to-Noise
Ratios(SNR's), the bounds come close together. As in the case of M-PSK
modulation, for large L the coherent capacity is approached."
"Just as the Hamming weight spectrum of a linear block code sheds light on the
performance of a maximum likelihood decoder, the pseudo-weight spectrum
provides insight into the performance of a linear programming decoder. Using
properties of polyhedral cones, we find the pseudo-weight spectrum of some
short codes. We also present two general lower bounds on the minimum
pseudo-weight. The first bound is based on the column weight of the
parity-check matrix. The second bound is computed by solving an optimization
problem. In some cases, this bound is more tractable to compute than previously
known bounds and thus can be applied to longer codes."
"We show that the duality between channel capacity and data compression is
retained when state information is available to the sender, to the receiver, to
both, or to neither. We present a unified theory for eight special cases of
channel capacity and rate distortion with state information, which also extends
existing results to arbitrary pairs of independent and identically distributed
(i.i.d.) correlated state information available at the sender and at the
receiver, respectively. In particular, the resulting general formula for
channel capacity assumes the same form as the generalized Wyner Ziv rate
distortion function."
"Sparse intersymbol-interference (ISI) channels are encountered in a variety
of high-data-rate communication systems. Such channels have a large channel
memory length, but only a small number of significant channel coefficients. In
this paper, trellis-based equalization of sparse ISI channels is revisited. Due
to the large channel memory length, the complexity of maximum-likelihood
detection, e.g., by means of the Viterbi algorithm (VA), is normally
prohibitive. In the first part of the paper, a unified framework based on
factor graphs is presented for complexity reduction without loss of optimality.
In this new context, two known reduced-complexity algorithms for sparse ISI
channels are recapitulated: The multi-trellis VA (M-VA) and the
parallel-trellis VA (P-VA). It is shown that the M-VA, although claimed, does
not lead to a reduced computational complexity. The P-VA, on the other hand,
leads to a significant complexity reduction, but can only be applied for a
certain class of sparse channels. In the second part of the paper, a unified
approach is investigated to tackle general sparse channels: It is shown that
the use of a linear filter at the receiver renders the application of standard
reduced-state trellis-based equalizer algorithms feasible, without significant
loss of optimality. Numerical results verify the efficiency of the proposed
receiver structure."
"This paper computes the sensing capacity of a sensor network, with sensors of
limited range, sensing a two-dimensional Markov random field, by modeling the
sensing operation as an encoder. Sensor observations are dependent across
sensors, and the sensor network output across different states of the
environment is neither identically nor independently distributed. Using a
random coding argument, based on the theory of types, we prove a lower bound on
the sensing capacity of the network, which characterizes the ability of the
sensor network to distinguish among environments with Markov structure, to
within a desired accuracy."
"In 1975, Chaitin introduced his celebrated Omega number, the halting
probability of a universal Chaitin machine, a universal Turing machine with a
prefix-free domain. The Omega number's bits are {\em algorithmically
random}--there is no reason the bits should be the way they are, if we define
``reason'' to be a computable explanation smaller than the data itself. Since
that time, only {\em two} explicit universal Chaitin machines have been
proposed, both by Chaitin himself.
  Concrete algorithmic information theory involves the study of particular
universal Turing machines, about which one can state theorems with specific
numerical bounds, rather than include terms like O(1). We present several new
tiny Chaitin machines (those with a prefix-free domain) suitable for the study
of concrete algorithmic information theory. One of the machines, which we call
Keraia, is a binary encoding of lambda calculus based on a curried lambda
operator. Source code is included in the appendices.
  We also give an algorithm for restricting the domain of blank-endmarker
machines to a prefix-free domain over an alphabet that does not include the
endmarker; this allows one to take many universal Turing machines and construct
universal Chaitin machines from them."
"In this paper, we investigate in detail the performance of turbo codes in
quasi-static fading channels both with and without antenna diversity. First, we
develop a simple and accurate analytic technique to evaluate the performance of
turbo codes in quasi-static fading channels. The proposed analytic technique
relates the frame error rate of a turbo code to the iterative decoder
convergence threshold, rather than to the turbo code distance spectrum.
Subsequently, we compare the performance of various turbo codes in quasi-static
fading channels. We show that, in contrast to the situation in the AWGN
channel, turbo codes with different interleaver sizes or turbo codes based on
RSC codes with different constraint lengths and generator polynomials exhibit
identical performance. Moreover, we also compare the performance of turbo codes
and convolutional codes in quasi-static fading channels under the condition of
identical decoding complexity. In particular, we show that turbo codes do not
outperform convolutional codes in quasi-static fading channels with no antenna
diversity; and that turbo codes only outperform convolutional codes in
quasi-static fading channels with antenna diversity."
"This paper describes a new set of block source codes well suited for data
compression. These codes are defined by sets of productions rules of the form
a.l->b, where a in A represents a value from the source alphabet A and l, b are
-small- sequences of bits. These codes naturally encompass other Variable
Length Codes (VLCs) such as Huffman codes. It is shown that these codes may
have a similar or even a shorter mean description length than Huffman codes for
the same encoding and decoding complexity. A first code design method allowing
to preserve the lexicographic order in the bit domain is described. The
corresponding codes have the same mean description length (mdl) as Huffman
codes from which they are constructed. Therefore, they outperform from a
compression point of view the Hu-Tucker codes designed to offer the
lexicographic property in the bit domain. A second construction method allows
to obtain codes such that the marginal bit probability converges to 0.5 as the
sequence length increases and this is achieved even if the probability
distribution function is not known by the encoder."
"The goal of a denoising algorithm is to reconstruct a signal from its
noise-corrupted observations. Perfect reconstruction is seldom possible and
performance is measured under a given fidelity criterion. In a recent work, the
authors addressed the problem of denoising unknown discrete signals corrupted
by a discrete memoryless channel when the channel, rather than being completely
known, is only known to lie in some uncertainty set of possible channels. A
sequence of denoisers was derived for this case and shown to be asymptotically
optimal with respect to a worst-case criterion argued most relevant to this
setting. In the present work we address the implementation and complexity of
this denoiser for channels parametrized by a scalar, establishing its
practicality. We show that for symmetric channels, the problem can be mapped
into a convex optimization problem, which can be solved efficiently. We also
present empirical results suggesting the potential of these schemes to do well
in practice. A key component of our schemes is an estimator of the subset of
channels in the uncertainty set that are feasible in the sense of being able to
give rise to the noise-corrupted signal statistics for some channel input
distribution. We establish the efficiency of this estimator, both
algorithmically and experimentally. We also present a modification of the
recently developed discrete universal denoiser (DUDE) that assumes a channel
based on the said estimator, and show that, in practice, the resulting scheme
performs well. For concreteness, we focus on the binary alphabet case and
binary symmetric channels, but also discuss the extensions of the algorithms to
general finite alphabets and to general channels parameterized by a scalar."
"The decoding error probability of codes is studied as a function of their
block length. It is shown that the existence of codes with a polynomially small
decoding error probability implies the existence of codes with an exponentially
small decoding error probability. Specifically, it is assumed that there exists
a family of codes of length N and rate R=(1-\epsilon)C (C is a capacity of a
binary symmetric channel), whose decoding probability decreases polynomially in
1/N. It is shown that if the decoding probability decreases sufficiently fast,
but still only polynomially fast in 1/N, then there exists another such family
of codes whose decoding error probability decreases exponentially fast in N.
Moreover, if the decoding time complexity of the assumed family of codes is
polynomial in N and 1/\epsilon, then the decoding time complexity of the
presented family is linear in N and polynomial in 1/\epsilon. These codes are
compared to the recently presented codes of Barg and Zemor, ``Error Exponents
of Expander Codes,'' IEEE Trans. Inform. Theory, 2002, and ``Concatenated
Codes: Serial and Parallel,'' IEEE Trans. Inform. Theory, 2005. It is shown
that the latter families can not be tuned to have exponentially decaying (in N)
error probability, and at the same time to have decoding time complexity linear
in N and polynomial in 1/\epsilon."
"A novel detector for multiple-input multiple-output (MIMO) communications is
presented. The algorithm belongs to the class of the lattice detectors, i.e. it
finds a reduced complexity solution to the problem of finding the closest
vector to the received observations. The algorithm achieves optimal
maximum-likelihood (ML) performance in case of two transmit antennas, at the
same time keeping a complexity much lower than the exhaustive search-based ML
detection technique. Also, differently from the state-of-art lattice detector
(namely sphere decoder), the proposed algorithm is suitable for a highly
parallel hardware architecture and for a reliable bit soft-output information
generation, thus making it a promising option for real-time high-data rate
transmission."
"This paper considers the achievable rates and decoding complexity of
low-density parity-check (LDPC) codes over statistically independent parallel
channels. The paper starts with the derivation of bounds on the conditional
entropy of the transmitted codeword given the received sequence at the output
of the parallel channels; the component channels are considered to be
memoryless, binary-input, and output-symmetric (MBIOS). These results serve for
the derivation of an upper bound on the achievable rates of ensembles of LDPC
codes under optimal maximum-likelihood (ML) decoding when their transmission
takes place over parallel MBIOS channels. The paper relies on the latter bound
for obtaining upper bounds on the achievable rates of ensembles of randomly and
intentionally punctured LDPC codes over MBIOS channels. The paper also provides
a lower bound on the decoding complexity (per iteration) of ensembles of LDPC
codes under message-passing iterative decoding over parallel MBIOS channels;
the bound is given in terms of the gap between the rate of these codes for
which reliable communication is achievable and the channel capacity. The paper
presents a diagram which shows interconnections between the theorems introduced
in this paper and some other previously reported results. The setting which
serves for the derivation of the bounds on the achievable rates and decoding
complexity is general, and the bounds can be applied to other scenarios which
can be treated as different forms of communication over parallel channels."
"Network or graph structures are ubiquitous in the study of complex systems.
Often, we are interested in complexity trends of these system as it evolves
under some dynamic. An example might be looking at the complexity of a food web
as species enter an ecosystem via migration or speciation, and leave via
extinction.
  In this paper, a complexity measure of networks is proposed based on the {\em
complexity is information content} paradigm. To apply this paradigm to any
object, one must fix two things: a representation language, in which strings of
symbols from some alphabet describe, or stand for the objects being considered;
and a means of determining when two such descriptions refer to the same object.
With these two things set, the information content of an object can be computed
in principle from the number of equivalent descriptions describing a particular
object.
  I propose a simple representation language for undirected graphs that can be
encoded as a bitstring, and equivalence is a topological equivalence. I also
present an algorithm for computing the complexity of an arbitrary undirected
network."
"In this paper, we investigate achievable rates for data transmission from
sources to sinks through multiple relay networks. We consider myopic coding, a
constrained communication strategy in which each node has only a local view of
the network, meaning that nodes can only transmit to and decode from
neighboring nodes. We compare this with omniscient coding, in which every node
has a global view of the network and all nodes can cooperate. Using Gaussian
channels as examples, we find that when the nodes transmit at low power, the
rates achievable with two-hop myopic coding are as large as that under
omniscient coding in a five-node multiple relay channel and close to that under
omniscient coding in a six-node multiple relay channel. These results suggest
that we may do local coding and cooperation without compromising much on the
transmission rate. Practically, myopic coding schemes are more robust to
topology changes because encoding and decoding at a node are not affected when
there are changes at remote nodes. Furthermore, myopic coding mitigates the
high computational complexity and large buffer/memory requirements of
omniscient coding."
"This paper presents an algebraic construction of families of unitary matrices
that achieve full diversity. They are obtained as subsets of cyclic division
algebras."
"Adaptive (variable-length) codes associate variable-length codewords to
symbols being encoded depending on the previous symbols in the input data
string. This class of codes has been presented in [Dragos Trinca,
cs.DS/0505007] as a new class of non-standard variable-length codes.
Generalized adaptive codes (GA codes, for short) have been also presented in
[Dragos Trinca, cs.DS/0505007] not only as a new class of non-standard
variable-length codes, but also as a natural generalization of adaptive codes
of any order. This paper is intended to continue developing the theory of
variable-length codes by establishing several interesting connections between
adaptive codes and other classes of codes. The connections are discussed not
only from a theoretical point of view (by proving new results), but also from
an applicative one (by proposing several applications). First, we prove that
adaptive Huffman encodings and Lempel-Ziv encodings are particular cases of
encodings by GA codes. Second, we show that any (n,1,m) convolutional code
satisfying certain conditions can be modelled as an adaptive code of order m.
Third, we describe a cryptographic scheme based on the connection between
adaptive codes and convolutional codes, and present an insightful analysis of
this scheme. Finally, we conclude by generalizing adaptive codes to
(p,q)-adaptive codes, and discussing connections between adaptive codes and
time-varying codes."
"The acquisition, or synchronization, of the multipath profile for an
ultrawideband pulse position modulation (PPM) communication systems is
considered. Synchronization is critical for the proper operation of PPM based
For the multipath channel, it is assumed that channel gains are known, but path
delays are unknown. In the limit of large bandwidth, W, it is assumed that the
number of paths, L, grows. The delay spread of the channel, M, is proportional
to the bandwidth. The rate of growth of L versus M determines whether
synchronization can occur. It is shown that if L/sqrt(M) --> 0, then the
maximum likelihood synchronizer cannot acquire any of the paths and
alternatively if L/M --> 0, the maximum likelihood synchronizer is guaranteed
to miss at least one path."
"We extend Shannon's result on the capacity of channels with state information
to multiple user channels. More specifically, we characterize the capacity
(region) of degraded broadcast channels and physically degraded relay channels
where the channel state information is causally available at the transmitters.
We also obtain inner and outer bounds on the capacity region for multiple
access channels with causal state information at the transmitters."
"A method for constructing sets of sequences with zero-correlation zone (ZCZ
sequences) and sequence sets with low cross correlation is proposed. The method
is to use families of short sequences and complete orthogonal sequence sets to
derive families of long sequences with desired correlation properties. It is a
unification of works of Matsufuji and Torii \emph{et al.}, and there are more
choices of parameters of sets for our method. In particular, ZCZ sequence sets
generated by the method can achieve a related ZCZ bound. Furthermore, the
proposed method can be utilized to derive new ZCZ sets with both longer ZCZ and
larger set size from known ZCZ sets. These sequence sets are applicable in
broadband satellite IP networks."
"Since the publication of Shannon's theory of one terminal source coding, a
number of interesting extensions have been derived by researchers such as
Slepian-Wolf, Wyner, Ahlswede-K\""{o}rner, Wyner-Ziv and Berger-Yeung.
Specifically, the achievable rate or rate-distortion region has been described
by a first order information-theoretic functional of the source statistics in
each of the above cases. At the same time several problems have also remained
unsolved. Notable two terminal examples include the joint distortion problem,
where both sources are reconstructed under a combined distortion criterion, as
well as the partial side information problem, where one source is reconstructed
under a distortion criterion using information about the other (side
information) available at a certain rate (partially). In this paper we solve
both of these open problems. Specifically, we give an infinite order
description of the achievable rate-distortion region in each case. In our
analysis we set the above problems in a general framework and formulate a
unified methodology that solves not only the problems at hand but any two
terminal problem with noncooperative encoding. The key to such unification is
held by a fundamental source coding principle which we derive by extending the
typicality arguments of Shannon and Wyner-Ziv. Finally, we demonstrate the
expansive scope of our technique by re-deriving known coding theorems. We shall
observe that our infinite order descriptions simplify to the expected first
order in the known special cases."
"In the first paper of this two part communication, we solved in a unified
framework a variety of two terminal source coding problems with noncooperative
encoders, thereby consolidating works of Shannon, Slepian-Wolf, Wyner,
Ahlswede-K\""{o}rner, Wyner-Ziv, Berger {\em et al.} and Berger-Yeung. To
achieve such unification we made use of a fundamental principle that
dissociates bulk of the analysis from the distortion criterion at hand (if any)
and extends the typicality arguments of Shannon and Wyner-Ziv. In this second
paper, we generalize the fundamental principle for any number of sources and on
its basis exhaustively solve all multiterminal source coding problems with
noncooperative encoders and one decoder. The distortion criteria, when
applicable, are required to apply to single letters and be bounded. Our
analysis includes cases where side information is, respectively, partially
available, completely available and altogether unavailable at the decoder. As
seen in our first paper, the achievable regions permit infinite order
information-theoretic descriptions. We also show that the entropy-constrained
multiterminal estimation problem can be solved as a special case of our theory."
"This paper investigates the achievable information rate of phase-shift keying
(PSK) over frequency non-selective Rayleigh fading channels without channel
state information (CSI). The fading process exhibits general temporal
correlation characterized by its spectral density function. We consider both
discrete-time and continuous-time channels, and find their asymptotics at low
signal-to-noise ratio (SNR). Compared to known capacity upper bounds under peak
constraints, these asymptotics usually lead to negligible rate loss in the
low-SNR regime for slowly time-varying fading channels. We further specialize
to case studies of Gauss-Markov and Clarke's fading models."
"Motivated by the evident success of context-tree based methods in lossless
data compression, we explore, in this paper, methods of the same spirit in
universal prediction of individual sequences. By context-tree prediction, we
refer to a family of prediction schemes, where at each time instant $t$, after
having observed all outcomes of the data sequence $x_1,...,x_{t-1}$, but not
yet $x_t$, the prediction is based on a ``context'' (or a state) that consists
of the $k$ most recent past outcomes $x_{t-k},...,x_{t-1}$, where the choice of
$k$ may depend on the contents of a possibly longer, though limited, portion of
the observed past, $x_{t-k_{\max}},...x_{t-1}$. This is different from the
study reported in [1], where general finite-state predictors as well as
``Markov'' (finite-memory) predictors of fixed order, were studied in the
regime of individual sequences.
  Another important difference between this study and [1] is the asymptotic
regime. While in [1], the resources of the predictor (i.e., the number of
states or the memory size) were kept fixed regardless of the length $N$ of the
data sequence, here we investigate situations where the number of contexts or
states is allowed to grow concurrently with $N$. We are primarily interested in
the following fundamental question: What is the critical growth rate of the
number of contexts, below which the performance of the best context-tree
predictor is still universally achievable, but above which it is not? We show
that this critical growth rate is linear in $N$. In particular, we propose a
universal context-tree algorithm that essentially achieves optimum performance
as long as the growth rate is sublinear, and show that, on the other hand, this
is impossible in the linear case."
"In this paper, we complement Verd\'{u}'s work on spectral efficiency in the
wideband regime by investigating the fundamental tradeoff between rate and
bandwidth when a constraint is imposed on the error exponent. Specifically, we
consider both AWGN and Rayleigh-fading channels. For the AWGN channel model,
the optimal values of $R_z(0)$ and $\dot{R_z}(0)$ are calculated, where
$R_z(1/B)$ is the maximum rate at which information can be transmitted over a
channel with bandwidth $B/2$ when the error-exponent is constrained to be
greater than or equal to $z.$ Based on this calculation, we say that a sequence
of input distributions is near optimal if both $R_z(0)$ and $\dot{R_z}(0)$ are
achieved. We show that QPSK, a widely-used signaling scheme, is near-optimal
within a large class of input distributions for the AWGN channel. Similar
results are also established for a fading channel where full CSI is available
at the receiver."
"In this work, we extend the non-orthogonal amplify-and-forward (NAF)
cooperative diversity scheme to the MIMO channel. A family of space-time block
codes for a half-duplex MIMO NAF fading cooperative channel with N relays is
constructed. The code construction is based on the non-vanishing determinant
criterion (NVD) and is shown to achieve the optimal diversity-multiplexing
tradeoff (DMT) of the channel. We provide a general explicit algebraic
construction, followed by some examples. In particular, in the single relay
case, it is proved that the Golden code and the 4x4 Perfect code are optimal
for the single-antenna and two-antenna case, respectively. Simulation results
reveal that a significant gain (up to 10dB) can be obtained with the proposed
codes, especially in the single-antenna case."
"Non-data-aided (NDA) parameter estimation is considered for
binary-phase-shift-keying transmission in an additive white Gaussian noise
channel. Cramer-Rao lower bounds (CRLBs) for signal amplitude, noise variance,
channel reliability constant and bit-error rate are derived and it is shown how
these parameters relate to the signal-to-noise ratio (SNR). An alternative
derivation of the iterative maximum likelihood (ML) SNR estimator is presented
together with a novel, low complexity NDA SNR estimator. The performance of the
proposed estimator is compared to previously suggested estimators and the CRLB.
The results show that the proposed estimator performs close to the iterative ML
estimator at significantly lower computational complexity."
"An algorithm that performs joint equalization and decoding for nonlinear
two-dimensional intersymbol interference channels is presented. The algorithm
performs sum-product message-passing on a factor graph that represents the
underlying system. The two-dimensional optical storage (TWODOS) technology is
an example of a system with nonlinear two-dimensional intersymbol interference.
Simulations for the nonlinear channel model of TWODOS show significant
improvement in performance over uncoded performance. Noise tolerance thresholds
for the algorithm for the TWODOS channel, computed using density evolution, are
also presented and accurately predict the limiting performance of the algorithm
as the codeword length increases."
"An algorithm that performs joint equalization and decoding for channels with
nonlinear two-dimensional intersymbol interference is presented. The algorithm
performs sum-product message-passing on a factor graph that represents the
underlying system. The two-dimensional optical storage (TwoDOS) technology is
an example of a system with nonlinear two-dimensional intersymbol interference.
Simulations for the nonlinear channel model of TwoDOS show significant
improvement in performance over uncoded performance. Noise tolerance thresholds
for the TwoDOS channel computed using density evolution are also presented."
"Joint equalization and decoding schemes are described for two-dimensional
intersymbol interference (ISI) channels. Equalization is performed using the
minimum mean-square-error (MMSE) criterion. Low-density parity-check codes are
used for error correction. The MMSE schemes are the extension of those proposed
by Tuechler et al. (2002) for one-dimensional ISI channels. Extrinsic
information transfer charts, density evolution, and bit-error rate versus
signal-to-noise ratio curves are used to study the performance of the schemes."
"Let P and Q be two probability distributions which differ only for values
with non-zero probability. We show that the variational distance between the
n-fold product distributions P^n and Q^n cannot grow faster than the square
root of n."
"Density evolution is one of the most powerful analytical tools for
low-density parity-check (LDPC) codes and graph codes with message passing
decoding algorithms. With channel symmetry as one of its fundamental
assumptions, density evolution (DE) has been widely and successfully applied to
different channels, including binary erasure channels, binary symmetric
channels, binary additive white Gaussian noise channels, etc. This paper
generalizes density evolution for non-symmetric memoryless channels, which in
turn broadens the applications to general memoryless channels, e.g. z-channels,
composite white Gaussian noise channels, etc. The central theorem underpinning
this generalization is the convergence to perfect projection for any fixed size
supporting tree. A new iterative formula of the same complexity is then
presented and the necessary theorems for the performance concentration theorems
are developed. Several properties of the new density evolution method are
explored, including stability results for general asymmetric memoryless
channels. Simulations, code optimizations, and possible new applications
suggested by this new density evolution method are also provided. This result
is also used to prove the typicality of linear LDPC codes among the coset code
ensemble when the minimum check node degree is sufficiently large. It is shown
that the convergence to perfect projection is essential to the belief
propagation algorithm even when only symmetric channels are considered. Hence
the proof of the convergence to perfect projection serves also as a completion
of the theory of classical density evolution for symmetric memoryless channels."
"In this paper, an outage limited MIMO channel is considered. We build on
Zheng and Tse's elegant formulation of the diversity-multiplexing tradeoff to
develop a better understanding of the asymptotic relationship between the
probability of error, transmission rate, and signal-to-noise ratio. In
particular, we identify the limitation imposed by the multiplexing gain notion
and develop a new formulation for the throughput-reliability tradeoff that
avoids this limitation. The new characterization is then used to elucidate the
asymptotic trends exhibited by the outage probability curves of MIMO channels."
"We consider both channel coding and source coding, with perfect past
feedback/feedforward, in the presence of side information. It is first observed
that feedback does not increase the capacity of the Gel'fand-Pinsker channel,
nor does feedforward improve the achievable rate-distortion performance in the
Wyner-Ziv problem. We then focus on the Gaussian case showing that, as in the
absence of side information, feedback/feedforward allows to efficiently attain
the respective performance limits. In particular, we derive schemes via
variations on that of Schalkwijk and Kailath. These variants, which are as
simple as their origins and require no binning, are shown to achieve,
respectively, the capacity of Costa's channel, and the Wyner-Ziv rate
distortion function. Finally, we consider the finite-alphabet setting and
derive schemes for both the channel and the source coding problems that attain
the fundamental limits, using variations on schemes of Ahlswede and Ooi and
Wornell, and of Martinian and Wornell, respectively."
"This paper investigates an efficient and practical information reconciliation
method in the case where two parties have access to correlated continuous
random variables. We show that reconciliation is a special case of channel
coding and that existing coded modulation techniques can be adapted for
reconciliation. We describe an explicit reconciliation method based on LDPC
codes in the case of correlated Gaussian variables. We believe that the
proposed method can improve the efficiency of quantum key distribution
protocols based on continuous-spectrum quantum states."
"In this paper, we define the power region as the set of power allocations for
K users such that everybody meets a minimum signal-to-interference ratio (SIR).
The SIR is modeled in a multiuser CDMA system with fixed linear receiver and
signature sequences. We show that the power region is convex in linear and
logarithmic scale. It furthermore has a componentwise minimal element. Power
constraints are included by the intersection with the set of all viable power
adjustments.
  In this framework, we aim at minimizing the total expended power by
minimizing a componentwise monotone functional. If the feasible power region is
nonempty, the minimum is attained. Otherwise, as a solution to balance
conflicting interests, we suggest the projection of the minimum point in the
power region onto the set of viable power settings. Finally, with an
appropriate utility function, the problem of minimizing the total expended
power can be seen as finding the Nash bargaining solution, which sheds light on
power assignment from a game theoretic point of view. Convexity and
componentwise monotonicity are essential prerequisites for this result."
"The paper introduces ensembles of accumulate-repeat-accumulate (ARA) codes
which asymptotically achieve capacity on the binary erasure channel (BEC) with
{\em bounded complexity} per information bit. It also introduces symmetry
properties which play a central role in the construction of capacity-achieving
ensembles for the BEC. The results here improve on the tradeoff between
performance and complexity provided by the first capacity-achieving ensembles
of irregular repeat-accumulate (IRA) codes with bounded complexity per
information bit; these IRA ensembles were previously constructed by Pfister,
Sason and Urbanke. The superiority of ARA codes with moderate to large block
length is exemplified by computer simulations which compare their performance
with those of previously reported capacity-achieving ensembles of LDPC and IRA
codes. The ARA codes also have the advantage of being systematic."
"We study a game puzzle that has enjoyed recent popularity among
mathematicians, computer scientist, coding theorists and even the mass press.
In the game, $n$ players are fitted with randomly assigned colored hats.
Individual players can see their teammates' hat colors, but not their own.
Based on this information, and without any further communication, each player
must attempt to guess his hat color, or pass. The team wins if there is at
least one correct guess, and no incorrect ones. The goal is to devise guessing
strategies that maximize the team winning probability. We show that for the
case of two hat colors, and for any value of $n$, playing strategies are
equivalent to binary covering codes of radius one. This link, in particular
with Hamming codes, had been observed for values of $n$ of the form $2^m-1$. We
extend the analysis to games with hats of $q$ colors, $q\geq 2$, where
1-coverings are not sufficient to characterize the best strategies. Instead, we
introduce the more appropriate notion of a {\em strong covering}, and show
efficient constructions of these coverings, which achieve winning probabilities
approaching unity. Finally, we briefly discuss results on variants of the
problem, including arbitrary input distributions, randomized playing
strategies, and symmetric strategies."
"An interference-limited noise-free CDMA downlink channel operating under a
complexity constraint on the receiver is introduced. According to this
paradigm, detected bits, obtained by performing hard decisions directly on the
channel's matched filter output, must be the same as the transmitted binary
inputs. This channel setting, allowing the use of the simplest receiver scheme,
seems to be worthless, making reliable communication at any rate impossible. We
prove, by adopting statistical mechanics notion, that in the large-system limit
such a complexity-constrained CDMA channel gives rise to a non-trivial
Shannon-theoretic capacity, rigorously analyzed and corroborated using
finite-size channel simulations."
"A noisy CDMA downlink channel operating under a strict complexity constraint
on the receiver is introduced. According to this constraint, detected bits,
obtained by performing hard decisions directly on the channel's matched filter
output, must be the same as the transmitted binary inputs. This channel
setting, allowing the use of the simplest receiver scheme, seems to be
worthless, making reliable communication at any rate impossible. However,
recently this communication paradigm was shown to yield valuable information
rates in the case of a noiseless channel. This finding calls for the
investigation of this attractive complexity-constrained transmission scheme for
the more practical noisy channel case. By adopting the statistical mechanics
notion of metastable states of the renowned Hopfield model, it is proved that
under a bounded noise assumption such complexity-constrained CDMA channel gives
rise to a non-trivial Shannon-theoretic capacity, rigorously analyzed and
corroborated using finite-size channel simulations. For unbounded noise the
channel's outage capacity is addressed and specifically described for the
popular additive white Gaussian noise."
"We introduce a new family of concatenated codes with an outer low-density
parity-check (LDPC) code and an inner low-density generator matrix (LDGM) code,
and prove that these codes can achieve capacity under any memoryless
binary-input output-symmetric (MBIOS) channel using maximum-likelihood (ML)
decoding with bounded graphical complexity, i.e., the number of edges per
information bit in their graphical representation is bounded. In particular, we
also show that these codes can achieve capacity on the binary erasure channel
(BEC) under belief propagation (BP) decoding with bounded decoding complexity
per information bit per iteration for all erasure probabilities in (0, 1). By
deriving and analyzing the average weight distribution (AWD) and the
corresponding asymptotic growth rate of these codes with a rate-1 inner LDGM
code, we also show that these codes achieve the Gilbert-Varshamov bound with
asymptotically high probability. This result can be attributed to the presence
of the inner rate-1 LDGM code, which is demonstrated to help eliminate high
weight codewords in the LDPC code while maintaining a vanishingly small amount
of low weight codewords."
"For generalized Reed-Solomon codes, it has been proved \cite{GuruswamiVa05}
that the problem of determining if a received word is a deep hole is
co-NP-complete. The reduction relies on the fact that the evaluation set of the
code can be exponential in the length of the code -- a property that practical
codes do not usually possess. In this paper, we first presented a much simpler
proof of the same result. We then consider the problem for standard
Reed-Solomon codes, i.e. the evaluation set consists of all the nonzero
elements in the field. We reduce the problem of identifying deep holes to
deciding whether an absolutely irreducible hypersurface over a finite field
contains a rational point whose coordinates are pairwise distinct and nonzero.
By applying Schmidt and Cafure-Matera estimation of rational points on
algebraic varieties, we prove that the received vector $(f(\alpha))_{\alpha \in
\F_q}$ for Reed-Solomon $[q,k]_q$, $k < q^{1/7 - \epsilon}$, cannot be a deep
hole, whenever $f(x)$ is a polynomial of degree $k+d$ for $1\leq d < q^{3/13
-\epsilon}$."
"Wide band systems operating over multipath channels may spread their power
over bandwidth if they use duty cycle. Channel uncertainty limits the
achievable data rates of power constrained wide band systems; Duty cycle
transmission reduces the channel uncertainty because the receiver has to
estimate the channel only when transmission takes place. The optimal choice of
the fraction of time used for transmission depends on the spectral efficiency
of the signal modulation. The general principle is demonstrated by comparing
the channel conditions that allow different modulations to achieve the capacity
in the limit. Direct sequence spread spectrum and pulse position modulation
systems with duty cycle achieve the channel capacity, if the increase of the
number of channel paths with the bandwidth is not too rapid. The higher
spectral efficiency of the spread spectrum modulation lets it achieve the
channel capacity in the limit, in environments where pulse position modulation
with non-vanishing symbol time cannot be used because of the large number of
channel paths."
"In this paper, we analyze the capacity of multiple-input multiple-output
(MIMO) Rayleigh-fading channels in the presence of spatial fading correlation
at both the transmitter and the receiver, assuming that the channel is unknown
at the transmitter and perfectly known at the receiver. We first derive the
determinant representation for the exact characteristic function of the
capacity, which is then used to determine the trace representations for the
mean, variance, skewness, kurtosis, and other higher-order statistics (HOS).
These results allow us to exactly evaluate two relevant information-theoretic
capacity measures--ergodic capacity and outage capacity--and the HOS of the
capacity for such a MIMO channel. The analytical framework presented in the
paper is valid for arbitrary numbers of antennas and generalizes the previously
known results for independent and identically distributed or one-sided
correlated MIMO channels to the case when fading correlation exists on both
sides. We verify our analytical results by comparing them with Monte Carlo
simulations for a correlation model based on realistic channel measurements as
well as a classical exponential correlation model."
"We investigate the capacity of opportunistic communication in the presence of
dynamic and distributed spectral activity, i.e. when the time varying spectral
holes sensed by the cognitive transmitter are correlated but not identical to
those sensed by the cognitive receiver. Using the information theoretic
framework of communication with causal and non-causal side information at the
transmitter and/or the receiver, we obtain analytical capacity expressions and
the corresponding numerical results. We find that cognitive radio communication
is robust to dynamic spectral environments even when the communication occurs
in bursts of only 3-5 symbols. The value of handshake overhead is investigated
for both lightly loaded and heavily loaded systems. We find that the capacity
benefits of overhead information flow from the transmitter to the receiver is
negligible while feedback information overhead in the opposite direction
significantly improves capacity."
"The capacity of stationary additive Gaussian noise channels with feedback is
characterized as the solution to a variational problem. Toward this end, it is
proved that the optimal feedback coding scheme is stationary. When specialized
to the first-order autoregressive moving-average noise spectrum, this
variational characterization yields a closed-form expression for the feedback
capacity. In particular, this result shows that the celebrated
Schalkwijk--Kailath coding scheme achieves the feedback capacity for the
first-order autoregressive moving-average Gaussian channel, resolving a
long-standing open problem studied by Butman, Schalkwijk--Tiernan, Wolfowitz,
Ozarow, Ordentlich, Yang--Kavcic--Tatikonda, and others."
"Optimal link adaption to the scattering function of wide sense stationary
uncorrelated mobile communication channels is still an unsolved problem despite
its importance for next-generation system design. In multicarrier transmission
such link adaption is performed by pulse shaping, i.e. by properly adjusting
the transmit and receive filters. For example pulse shaped Offset--QAM systems
have been recently shown to have superior performance over standard cyclic
prefix OFDM (while operating at higher spectral efficiency).In this paper we
establish a general mathematical framework for joint transmitter and receiver
pulse shape optimization for so-called Weyl--Heisenberg or Gabor signaling with
respect to the scattering function of the WSSUS channel. In our framework the
pulse shape optimization problem is translated to an optimization problem over
trace class operators which in turn is related to fidelity optimization in
quantum information processing. By convexity relaxation the problem is shown to
be equivalent to a \emph{convex constraint quasi-convex maximization problem}
thereby revealing the non-convex nature of the overall WSSUS pulse design
problem. We present several iterative algorithms for optimization providing
applicable results even for large--scale problem constellations. We show that
with transmitter-side knowledge of the channel statistics a gain of $3 - 6$dB
in $\SINR$ can be expected."
"The encoder and decoder for lossy data compression of binary memoryless
sources are developed on the basis of a specific-type nonmonotonic perceptron.
Statistical mechanical analysis indicates that the potential ability of the
perceptron-based code saturates the theoretically achievable limit in most
cases although exactly performing the compression is computationally difficult.
To resolve this difficulty, we provide a computationally tractable
approximation algorithm using belief propagation (BP), which is a current
standard algorithm of probabilistic inference. Introducing several
approximations and heuristics, the BP-based algorithm exhibits performance that
is close to the achievable limit in a practical time scale in optimal cases."
"Phase noise and frequency offsets are due to their time-variant behavior one
of the most limiting disturbances in practical OFDM designs and therefore
intensively studied by many authors. In this paper we present a generalized
framework for the prediction of uncoded system performance in the presence of
time-variant distortions including the transmitter and receiver pulse shapes as
well as the channel. Therefore, unlike existing studies, our approach can be
employed for more general multicarrier schemes. To show the usefulness of our
approach, we apply the results to OFDM in the context of frequency offset and
Wiener phase noise, yielding improved bounds on the uncoded performance. In
particular, we obtain exact formulas for the averaged performance in AWGN and
time-invariant multipath channels."
"The use of multi-antenna arrays in both transmission and reception has been
shown to dramatically increase the throughput of wireless communication
systems. As a result there has been considerable interest in characterizing the
ergodic average of the mutual information for realistic correlated channels.
Here, an approach is presented that provides analytic expressions not only for
the average, but also the higher cumulant moments of the distribution of the
mutual information for zero-mean Gaussian (multiple-input multiple-output) MIMO
channels with the most general multipath covariance matrices when the channel
is known at the receiver. These channels include multi-tap delay paths, as well
as general channels with covariance matrices that cannot be written as a
Kronecker product, such as dual-polarized antenna arrays with general
correlations at both transmitter and receiver ends. The mathematical methods
are formally valid for large antenna numbers, in which limit it is shown that
all higher cumulant moments of the distribution, other than the first two scale
to zero. Thus, it is confirmed that the distribution of the mutual information
tends to a Gaussian, which enables one to calculate the outage capacity. These
results are quite accurate even in the case of a few antennas, which makes this
approach applicable to realistic situations."
"In this paper, we analyze the frequency-hopping orthogonal frequency-division
multiplexing (OFDM) system known as Multiband OFDM for high-rate wireless
personal area networks (WPANs) based on ultra-wideband (UWB) transmission.
Besides considering the standard, we also propose and study system performance
enhancements through the application of Turbo and Repeat-Accumulate (RA) codes,
as well as OFDM bit-loading. Our methodology consists of (a) a study of the
channel model developed under IEEE 802.15 for UWB from a frequency-domain
perspective suited for OFDM transmission, (b) development and quantification of
appropriate information-theoretic performance measures, (c) comparison of these
measures with simulation results for the Multiband OFDM standard proposal as
well as our proposed extensions, and (d) the consideration of the influence of
practical, imperfect channel estimation on the performance. We find that the
current Multiband OFDM standard sufficiently exploits the frequency selectivity
of the UWB channel, and that the system performs in the vicinity of the channel
cutoff rate. Turbo codes and a reduced-complexity clustered bit-loading
algorithm improve the system power efficiency by over 6 dB at a data rate of
480 Mbps."
"In this paper, we present an iterative soft-decision decoding algorithm for
Reed-Solomon codes offering both complexity and performance advantages over
previously known decoding algorithms. Our algorithm is a list decoding
algorithm which combines two powerful soft decision decoding techniques which
were previously regarded in the literature as competitive, namely, the
Koetter-Vardy algebraic soft-decision decoding algorithm and belief-propagation
based on adaptive parity check matrices, recently proposed by Jiang and
Narayanan. Building on the Jiang-Narayanan algorithm, we present a
belief-propagation based algorithm with a significant reduction in
computational complexity. We introduce the concept of using a
belief-propagation based decoder to enhance the soft-input information prior to
decoding with an algebraic soft-decision decoder. Our algorithm can also be
viewed as an interpolation multiplicity assignment scheme for algebraic
soft-decision decoding of Reed-Solomon codes."
"We explore the SNR-optimal relay functionality in a \emph{memoryless} relay
network, i.e. a network where, during each channel use, the signal transmitted
by a relay depends only on the last received symbol at that relay. We develop a
generalized notion of SNR for the class of memoryless relay functions. The
solution to the generalized SNR optimization problem leads to the novel concept
of minimum mean square uncorrelated error estimation(MMSUEE). For the elemental
case of a single relay, we show that MMSUEE is the SNR-optimal memoryless relay
function regardless of the source and relay transmit power, and the modulation
scheme. This scheme, that we call estimate and forward (EF), is also shown to
be SNR-optimal with PSK modulation in a parallel relay network. We demonstrate
that EF performs better than the best of amplify and forward (AF) and
demodulate and forward (DF), in both parallel and serial relay networks. We
also determine that AF is near-optimal at low transmit power in a parallel
network, while DF is near-optimal at high transmit power in a serial network.
For hybrid networks that contain both serial and parallel elements, and when
robust performance is desired, the advantage of EF over the best of AF and DF
is found to be significant. Error probabilities are provided to substantiate
the performance gain obtained through SNR optimality. We also show that, for
\emph{Gaussian} inputs, AF, DF and EF become identical."
"Linear space-time block codes (STBCs) of unitary rate and full diversity,
systematically constructed over arbitrary constellations for any number of
transmit antennas are introduced. The codes are obtained by generalizing the
existing ABBA STBCs, a.k.a quasi-orthogonal STBCs (QO-STBCs). Furthermore, a
fully orthogonal (symbol-by-symbol) decoder for the new generalized ABBA
(GABBA) codes is provided. This remarkably low-complexity decoder relies on
partition orthogonality properties of the code structure to decompose the
received signal vector into lower-dimension tuples, each dependent only on
certain subsets of the transmitted symbols. Orthogonal decodability results
from the nested application of this technique, with no matrix inversion or
iterative signal processing required. The exact bit-error-rate probability of
GABBA codes over generalized fading channels with maximum likelihood (ML)
decoding is evaluated analytically and compared against simulation results
obtained with the proposed orthogonal decoder. The comparison reveals that the
proposed GABBA solution, despite its very low complexity, achieves nearly the
same performance of the bound corresponding to the ML-decoded system,
especially in systems with large numbers of antennas."
"We present a tree-based construction of LDPC codes that have minimum
pseudocodeword weight equal to or almost equal to the minimum distance, and
perform well with iterative decoding. The construction involves enumerating a
$d$-regular tree for a fixed number of layers and employing a connection
algorithm based on permutations or mutually orthogonal Latin squares to close
the tree. Methods are presented for degrees $d=p^s$ and $d = p^s+1$, for $p$ a
prime. One class corresponds to the well-known finite-geometry and finite
generalized quadrangle LDPC codes; the other codes presented are new. We also
present some bounds on pseudocodeword weight for $p$-ary LDPC codes. Treating
these codes as $p$-ary LDPC codes rather than binary LDPC codes improves their
rates, minimum distances, and pseudocodeword weights, thereby giving a new
importance to the finite geometry LDPC codes where $p > 2$."
"A unified approach to energy-efficient power control, applicable to a large
family of receivers including the matched filter, the decorrelator, the
(linear) minimum-mean-square-error detector (MMSE), and the individually and
jointly optimal multiuser detectors, has recently been proposed for
code-division-multiple-access (CDMA) networks. This unified power control (UPC)
algorithm exploits the linear relationship that has been shown to exist between
the transmit power and the output signal-to-interference-plus-noise ratio (SIR)
in large systems. Based on this principle and by computing the multiuser
efficiency, the UPC algorithm updates the users' transmit powers in an
iterative way to achieve the desired target SIR. In this paper, the convergence
of the UPC algorithm is proved for the matched filter, the decorrelator, and
the MMSE detector. In addition, the performance of the algorithm in finite-size
systems is studied and compared with that of existing power control schemes.
The UPC algorithm is particularly suitable for systems with randomly generated
long spreading sequences (i.e., sequences whose period is longer than one
symbol duration)."
"In this paper we investigate the notion of conditional independence and prove
several information inequalities for conditionally independent random
variables."
"In Multi-Input Multi-Output (MIMO) systems, Maximum-Likelihood (ML) decoding
is equivalent to finding the closest lattice point in an N-dimensional complex
space. In general, this problem is known to be NP hard. In this paper, we
propose a quasi-maximum likelihood algorithm based on Semi-Definite Programming
(SDP). We introduce several SDP relaxation models for MIMO systems, with
increasing complexity. We use interior-point methods for solving the models and
obtain a near-ML performance with polynomial computational complexity. Lattice
basis reduction is applied to further reduce the computational complexity of
solving these models. The proposed relaxation models are also used for soft
output decoding in MIMO systems."
"Suppose we have a signal y which we wish to represent using a linear
combination of a number of basis atoms a_i, y=sum_i x_i a_i = Ax. The problem
of finding the minimum L0 norm representation for y is a hard problem. The
Basis Pursuit (BP) approach proposes to find the minimum L1 norm representation
instead, which corresponds to a linear program (LP) that can be solved using
modern LP techniques, and several recent authors have given conditions for the
BP (minimum L1 norm) and sparse (minimum L0 solutions) representations to be
identical. In this paper, we explore this sparse representation problem} using
the geometry of convex polytopes, as recently introduced into the field by
Donoho. By considering the dual LP we find that the so-called polar polytope P
of the centrally-symmetric polytope P whose vertices are the atom pairs +-a_i
is particularly helpful in providing us with geometrical insight into
optimality conditions given by Fuchs and Tropp for non-unit-norm atom sets. In
exploring this geometry we are able to tighten some of these earlier results,
showing for example that the Fuchs condition is both necessary and sufficient
for L1-unique-optimality, and that there are situations where Orthogonal
Matching Pursuit (OMP) can eventually find all L1-unique-optimal solutions with
m nonzeros even if ERC fails for m, if allowed to run for more than m steps."
"In this paper, we provide a performance analysis of a new class of serial
concatenated convolutional codes (SCCC) where the inner encoder can be
punctured beyond the unitary rate. The puncturing of the inner encoder is not
limited to inner coded bits, but extended to systematic bits. Moreover, it is
split into two different puncturings, in correspondence with inner code
systematic bits and parity bits. We derive the analytical upper bounds to the
error probability of this particular code structure and address suitable design
guidelines for the inner code puncturing patterns. We show that the percentile
of systematic and parity bits to be deleted strongly depends on the SNR region
of interest. In particular, to lower the error floor it is advantageous to put
more puncturing on inner systematic bits. Furthermore, we show that puncturing
of inner systematic bits should be interleaver dependent. Based on these
considerations, we derive design guidelines to obtain well-performing
rate-compatible SCCC families. Throughout the paper, the performance of the
proposed codes are compared with analytical bounds, and with the performance of
PCCC and SCCC proposed in the literature."
"This paper proposes that the mathematical relationship between an entropy
distribution and its limit offers some new insight into system performance.
This relationship is used to quantify variation among the entities of a system,
where variation is defined as tolerance, option, specification or
implementation variation among the entities of a system. Variation has a
significnt and increasing impact on communications system performance. This
paper introduces means to identify, quantify and reduce such performance
variations."
"We apply belief propagation (BP) to multi--user detection in a spread
spectrum system, under the assumption of Gaussian symbols. We prove that BP is
both convergent and allows to estimate the correct conditional expectation of
the input symbols. It is therefore an optimal --minimum mean square error--
detection algorithm. This suggests the possibility of designing BP detection
algorithms for more general systems. As a byproduct we rederive the Tse-Hanly
formula for minimum mean square error without any recourse to random matrix
theory."
"We show that iterative coding systems can not surpass capacity using only
quantities which naturally appear in density evolution. Although the result in
itself is trivial, the method which we apply shows that in order to achieve
capacity the various components in an iterative coding system have to be
perfectly matched. This generalizes the perfect matching condition which was
previously known for the case of transmission over the binary erasure channel
to the general class of binary-input memoryless output-symmetric channels.
Potential applications of this perfect matching condition are the construction
of capacity-achieving degree distributions and the determination of the number
required iterations as a function of the multiplicative gap to capacity."
"In this work, the geometric relation between space time block code design for
the coherent channel and its non-coherent counterpart is exploited to get an
analogue of the information theoretic inequality $I(X;S)\le I((X,H);S)$ in
terms of diversity. It provides a lower bound on the performance of
non-coherent codes when used in coherent scenarios. This leads in turn to a
code design decomposition result splitting coherent code design into two
complexity reduced sub tasks. Moreover a geometrical criterion for high
performance space time code design is derived."
"We explore the available degrees of freedom for various multiuser MIMO
communication scenarios such as the multiple access, broadcast, interference,
relay, X and Z channels. For the two user MIMO interference channel, we find a
general inner bound and a genie-aided outer bound that give us the exact number
of degrees of freedom in many cases. We also study a share-and-transmit scheme
for transmitter cooperation. For the share-and-transmit scheme, we show how the
gains of transmitter cooperation are entirely offset by the cost of enabling
that cooperation so that the available degrees of freedom are not increased."
"Optimal link adaption to the scattering function of wide sense stationary
uncorrelated scattering (WSSUS) mobile communication channels is still an
unsolved problem despite its importance for next-generation system design. In
multicarrier transmission such link adaption is performed by pulse shaping
which in turn is equivalent to precoding with respect to the second order
channel statistics. In the present framework a translation of the precoder
optimization problem into an optimization problem over trace class operators is
used. This problem which is also well-known in the context of quantum
information theory is unsolved in general due to its non-convex nature. However
in very low dimension the problem formulation reveals an additional analytic
structure which again admits the solution to the optimal precoder and
multiplexing scheme. Hence, in this contribution the analytic solution of the
problem for the 2x2 doubly--dispersive WSSUS channel is presented."
"In this paper we consider the computation of channel capacity for ergodic
multiple-input multiple-output channels with additive white Gaussian noise. Two
scenarios are considered. Firstly, a time-varying channel is considered in
which both the transmitter and the receiver have knowledge of the channel
realization. The optimal transmission strategy is water-filling over space and
time. It is shown that this may be achieved in a causal, indeed instantaneous
fashion. In the second scenario, only the receiver has perfect knowledge of the
channel realization, while the transmitter has knowledge of the channel gain
probability law. In this case we determine an optimality condition on the input
covariance for ergodic Gaussian vector channels with arbitrary channel
distribution under the condition that the channel gains are independent of the
transmit signal. Using this optimality condition, we find an iterative
algorithm for numerical computation of optimal input covariance matrices.
Applications to correlated Rayleigh and Ricean channels are given."
"For a given blocklength we determine the number of interleavers which have
spread equal to two. Using this, we find out the probability that a randomly
chosen interleaver has spread two. We show that as blocklength increases, this
probability increases but very quickly converges to the value $1-e^{-2} \approx
0.8647$. Subsequently, we determine a lower bound on the probability of an
interleaver having spread at least $s$. We show that this lower bound converges
to the value $e^{-2(s-2)^{2}}$, as the blocklength increases."
"In an ultra wideband (UWB) impulse radio (IR) system, a number of pulses,
each transmitted in an interval called a ""frame"", is employed to represent one
information symbol. Conventionally, a single type of UWB pulse is used in all
frames of all users. In this paper, IR systems with multiple types of UWB
pulses are considered, where different types of pulses can be used in different
frames by different users. Both stored-reference (SR) and transmitted-reference
(TR) systems are considered. First, the spectral properties of a multi-pulse IR
system with polarity randomization is investigated. It is shown that the
average power spectral density is the average of the spectral contents of
different pulse shapes. Then, approximate closed-form expressions for the bit
error probability of a multi-pulse SR-IR system are derived for RAKE receivers
in asynchronous multiuser environments. The effects of both inter-frame
interference (IFI) and multiple-access interference (MAI) are analyzed. The
theoretical and simulation results indicate that SR-IR systems that are more
robust against IFI and MAI than a ""conventional"" SR-IR system can be designed
with multiple types of ultra-wideband pulses. Finally, extensions to
multi-pulse TR-IR systems are briefly described."
"Cooperative diversity has been recently proposed as a way to form virtual
antenna arrays that provide dramatic gains in slow fading wireless
environments. However most of the proposed solutions require distributed
space-time coding algorithms, the careful design of which is left for future
investigation if there is more than one cooperative relay. We propose a novel
scheme, that alleviates these problems and provides diversity gains on the
order of the number of relays in the network. Our scheme first selects the best
relay from a set of M available relays and then uses this best relay for
cooperation between the source and the destination. We develop and analyze a
distributed method to select the best relay that requires no topology
information and is based on local measurements of the instantaneous channel
conditions. This method also requires no explicit communication among the
relays. The success (or failure) to select the best available path depends on
the statistics of the wireless channel, and a methodology to evaluate
performance for any kind of wireless channel statistics, is provided.
Information theoretic analysis of outage probability shows that our scheme
achieves the same diversity-multiplexing tradeoff as achieved by more complex
protocols, where coordination and distributed space-time coding for M nodes is
required, such as those proposed in [7]. The simplicity of the technique,
allows for immediate implementation in existing radio hardware and its adoption
could provide for improved flexibility, reliability and efficiency in future 4G
wireless systems."
"It is shown that while the mutual information curves for coded modulation
(CM) and bit interleaved coded modulation (BICM) overlap in the case of a
single input single output channel, the same is not true in multiple input
multiple output (MIMO) channels. A method for mitigating fading in the presence
of multiple transmit antennas, named coordinate interleaving (CI), is presented
as a generalization of component interleaving for a single transmit antenna.
The extent of any advantages of CI over BICM, relative to CM, is analyzed from
a mutual information perspective; the analysis is based on an equivalent
parallel channel model for CI. Several expressions for mutual information in
the presence of CI and multiple transmit and receive antennas are derived.
Results show that CI gives higher mutual information compared to that of BICM
if proper signal mappings are used. Effects like constellation rotation in the
presence of CI are also considered and illustrated; it is shown that
constellation rotation can increase the constrained capacity."
"M-ary On-Off Frequency-Shift-Keying (OOFSK) is a digital modulation format in
which M-ary FSK signaling is overlaid on On/Off keying. This paper investigates
the potential of this modulation format in the context of wideband fading
channels. First it is assumed that the receiver uses energy detection for the
reception of OOFSK signals. Capacity expressions are obtained for the cases in
which the receiver has perfect and imperfect fading side information. Power
efficiency is investigated when the transmitter is subject to a peak-to-average
power ratio (PAR) limitation or a peak power limitation. It is shown that under
a PAR limitation, it is extremely power inefficient to operate in the very low
SNR regime. On the other hand, if there is only a peak power limitation, it is
demonstrated that power efficiency improves as one operates with smaller SNR
and vanishing duty factor. Also studied are the capacity improvements that
accrue when the receiver can track phase shifts in the channel or if the
received signal has a specular component. To take advantage of those features,
the phase of the modulation is also allowed to carry information."
"L multiple descriptions of a vector Gaussian source for individual and
central receivers are investigated. The sum rate of the descriptions with
covariance distortion measure constraints, in a positive semidefinite ordering,
is exactly characterized. For two descriptions, the entire rate region is
characterized. Jointly Gaussian descriptions are optimal in achieving the
limiting rates. The key component of the solution is a novel
information-theoretic inequality that is used to lower bound the achievable
multiple description rates."
"Mobile communication channels are often modeled as linear time-varying
filters or, equivalently, as time-frequency integral operators with finite
support in time and frequency. Such a characterization inherently assumes the
signals are narrowband and may not be appropriate for wideband signals. In this
paper time-scale characterizations are examined that are useful in wideband
time-varying channels, for which a time-scale integral operator is physically
justifiable. A review of these time-frequency and time-scale characterizations
is presented. Both the time-frequency and time-scale integral operators have a
two-dimensional discrete characterization which motivates the design of
time-frequency or time-scale rake receivers. These receivers have taps for both
time and frequency (or time and scale) shifts of the transmitted signal. A
general theory of these characterizations which generates, as specific cases,
the discrete time-frequency and time-scale models is presented here. The
interpretation of these models, namely, that they can be seen to arise from
processing assumptions on the transmit and receive waveforms is discussed. Out
of this discussion a third model arises: a frequency-scale continuous channel
model with an associated discrete frequency-scale characterization."
"We determine the rate region of the quadratic Gaussian two-encoder
source-coding problem. This rate region is achieved by a simple architecture
that separates the analog and digital aspects of the compression. Furthermore,
this architecture requires higher rates to send a Gaussian source than it does
to send any other source with the same covariance. Our techniques can also be
used to determine the sum rate of some generalizations of this classical
problem. Our approach involves coupling the problem to a quadratic Gaussian
``CEO problem.''"
"We identify the common underlying form of the capacity expression that is
applicable to both cases where causal or non-causal side information is made
available to the transmitter. Using this common form we find that for the
single user channel, the multiple access channel, the degraded broadcast
channel, and the degraded relay channel, the sum capacity with causal and
non-causal side information are identical when all the transmitter side
information is also made available to all the receivers. A genie-aided
outerbound is developed that states that when a genie provides $n$ bits of side
information to a receiver the resulting capacity improvement can not be more
than $n$ bits. Combining these two results we are able to bound the relative
capacity advantage of non-causal side information over causal side information
for both single user as well as various multiple user communication scenarios.
Applications of these capacity bounds are demonstrated through examples of
random access channels. Interestingly, the capacity results indicate that the
excessive MAC layer overheads common in present wireless systems may be avoided
through coding across multiple access blocks. It is also shown that even one
bit of side information at the transmitter can result in unbounded capacity
improvement. As a side, we obtain the sum capacity for a multiple access
channel when the side information available to the transmitter is causal and
possibly correlated to the side information available to the receiver."
"We provide a counterexample to Cover's conjecture that the feedback capacity
$C_\textrm{FB}$ of an additive Gaussian noise channel under power constraint
$P$ be no greater than the nonfeedback capacity $C$ of the same channel under
power constraint $2P$, i.e., $C_\textrm{FB}(P) \le C(2P)$."
"We consider a communication system in which the outputs of a Markov source
are encoded and decoded in \emph{real-time} by a finite memory receiver, and
the distortion measure does not tolerate delays. The objective is to choose
designs, i.e. real-time encoding, decoding and memory update strategies that
minimize a total expected distortion measure. This is a dynamic team problem
with non-classical information structure [Witsenhausen:1971]. We use the
structural results of [Teneketzis:2004] to develop a sequential decomposition
for the finite and infinite horizon problems. Thus, we obtain a systematic
methodology for the determination of jointly optimal encoding decoding and
memory update strategies for real-time point-to-point communication systems."
"The potential benefits of multiple-antenna systems may be limited by two
types of channel degradations rank deficiency and spatial fading correlation of
the channel. In this paper, we assess the effects of these degradations on the
diversity performance of multiple-input multiple-output (MIMO) systems, with an
emphasis on orthogonal space-time block codes, in terms of the symbol error
probability, the effective fading figure (EFF), and the capacity at low
signal-to-noise ratio (SNR). In particular, we consider a general family of
MIMO channels known as double-scattering channels, which encompasses a variety
of propagation environments from independent and identically distributed
Rayleigh to degenerate keyhole or pinhole cases by embracing both
rank-deficient and spatial correlation effects. It is shown that a MIMO system
with $n_T$ transmit and $n_R$ receive antennas achieves the diversity of order
$\frac{\n_T n_S n_R}{\max(n_T,n_S,n_R)}$ in a double-scattering channel with
$n_S$ effective scatterers. We also quantify the combined effect of the spatial
correlation and the lack of scattering richness on the EFF and the low-SNR
capacity in terms of the correlation figures of transmit, receive, and
scatterer correlation matrices. We further show the monotonicity properties of
these performance measures with respect to the strength of spatial correlation,
characterized by the eigenvalue majorization relations of the correlation
matrices."
"This paper investigates the limits of information transfer over a fast
Rayleigh fading MIMO channel, where neither the transmitter nor the receiver
has the knowledge of the channel state information (CSI) except the fading
statistics. We develop a scalar channel model due to absence of the phase
information in non-coherent Rayleigh fading and derive a capacity supremum with
the number of receive antennas at any signal to noise ratio (SNR) using
Lagrange optimisation. Also, we conceptualise the discrete nature of the
optimal input distribution by posing the optimisation on the channel mutual
information for $N$ discrete inputs. Furthermore, we derive an expression for
the asymptotic capacity when the input power is large, and compare with the
existing capacity results when the receiver is equipped with a large number of
antennas."
"We propose a computationally efficient multilevel coding scheme to achieve
the capacity of an ISI channel using layers of binary inputs. The transmitter
employs multilevel coding with linear mapping. The receiver uses multistage
decoding where each stage performs a separate linear minimum mean square error
(LMMSE) equalization and decoding. The optimality of the scheme is due to the
fact that the LMMSE equalizer is information lossless in an ISI channel when
signal to noise ratio is sufficiently low. The computational complexity is low
and scales linearly with the length of the channel impulse response and the
number of layers. The decoder at each layer sees an equivalent AWGN channel,
which makes coding straightforward."
"This paper introduces a new trellis pruning method which uses nonlinear
convolutional coding for peak-to-average power ratio (PAPR) reduction of
filtered QPSK and 16-QAM modulations. The Nyquist filter is viewed as a
convolutional encoder that controls the analog waveforms of the filter output
directly. Pruning some edges of the encoder trellis can effectively reduce the
PAPR. The only tradeoff is a slightly lower channel capacity and increased
complexity. The paper presents simulation results of the pruning action and the
resulting PAPR, and also discusses the decoding algorithm and the capacity of
the filtered and pruned QPSK and 16-QAM modulations on the AWGN channel.
Simulation results show that the pruning method reduces the PAPR significantly
without much damage to capacity."
"There is a fundamental relationship between belief propagation and maximum a
posteriori decoding. The case of transmission over the binary erasure channel
was investigated in detail in a companion paper. This paper investigates the
extension to general memoryless channels (paying special attention to the
binary case). An area theorem for transmission over general memoryless channels
is introduced and some of its many consequences are discussed. We show that
this area theorem gives rise to an upper-bound on the maximum a posteriori
threshold for sparse graph codes. In situations where this bound is tight, the
extrinsic soft bit estimates delivered by the belief propagation decoder
coincide with the correct a posteriori probabilities above the maximum a
posteriori threshold. More generally, it is conjectured that the fundamental
relationship between the maximum a posteriori and the belief propagation
decoder which was observed for transmission over the binary erasure channel
carries over to the general case. We finally demonstrate that in order for the
design rate of an ensemble to approach the capacity under belief propagation
decoding the component codes have to be perfectly matched, a statement which is
well known for the special case of transmission over the binary erasure
channel."
"We present an analysis, under iterative decoding, of coset LDPC codes over
GF(q), designed for use over arbitrary discrete-memoryless channels
(particularly nonbinary and asymmetric channels). We use a random-coset
analysis to produce an effect that is similar to output-symmetry with binary
channels. We show that the random selection of the nonzero elements of the
GF(q) parity-check matrix induces a permutation-invariance property on the
densities of the decoder messages, which simplifies their analysis and
approximation. We generalize several properties, including symmetry and
stability from the analysis of binary LDPC codes. We show that under a Gaussian
approximation, the entire q-1 dimensional distribution of the vector messages
is described by a single scalar parameter (like the distributions of binary
LDPC messages). We apply this property to develop EXIT charts for our codes. We
use appropriately designed signal constellations to obtain substantial shaping
gains. Simulation results indicate that our codes outperform multilevel codes
at short block lengths. We also present simulation results for the AWGN
channel, including results within 0.56 dB of the unconstrained Shannon limit
(i.e. not restricted to any signal constellation) at a spectral efficiency of 6
bits/s/Hz."
"We consider a model for secrecy generation, with three terminals, by means of
public interterminal communication, and examine the problem of characterizing
all the rates at which all three terminals can generate a ``secret key,'' and
-- simultaneously -- two designated terminals can generate a ``private key''
which is effectively concealed from the remaining terminal; both keys are also
concealed from an eavesdropper that observes the public communication. Inner
and outer bounds for the ``secret key--private key capacity region'' are
derived. Under a certain special condition, these bounds coincide to yield the
(exact) secret key--private key capacity region."
"We are interested in how to best communicate a (usually real valued) source
to a number of destinations (sinks) over a network with capacity constraints in
a collective fidelity metric over all the sinks, a problem which we call joint
network-source coding. Unlike the lossless network coding problem, lossy
reconstruction of the source at the sinks is permitted. We make a first attempt
to characterize the set of all distortions achievable by a set of sinks in a
given network. While the entire region of all achievable distortions remains
largely an open problem, we find a large, non-trivial subset of it using ideas
in multiple description coding. The achievable region is derived over all
balanced multiple-description codes and over all network flows, while the
network nodes are allowed to forward and duplicate data packets."
"This paper uses an incremental matrix expansion approach to derive asymptotic
eigenvalue distributions (a.e.d.'s) of sums and products of large random
matrices. We show that the result can be derived directly as a consequence of
two common assumptions, and matches the results obtained from using R- and
S-transforms in free probability theory. We also give a direct derivation of
the a.e.d. of the sum of certain random matrices which are not free. This is
used to determine the asymptotic signal-to-interference-ratio of a multiuser
CDMA system with a minimum mean-square error linear receiver."
"An interleaver is a critical component for the channel coding performance of
turbo codes. Algebraic constructions are of particular interest because they
admit analytical designs and simple, practical hardware implementation. Sun and
Takeshita have recently shown that the class of quadratic permutation
polynomials over integer rings provides excellent performance for turbo codes.
In this correspondence, a necessary and sufficient condition is proven for the
existence of a quadratic inverse polynomial for a quadratic permutation
polynomial over an integer ring. Further, a simple construction is given for
the quadratic inverse. All but one of the quadratic interleavers proposed
earlier by Sun and Takeshita are found to admit a quadratic inverse, although
none were explicitly designed to do so. An explanation is argued for the
observation that restriction to a quadratic inverse polynomial does not narrow
the pool of good quadratic interleavers for turbo codes."
"We consider multiple-input multiple-output (MIMO) transmit beamforming
systems with maximum ratio combining (MRC) receivers. The operating environment
is Rayleigh-fading with both transmit and receive spatial correlation. We
present exact expressions for the probability density function (p.d.f.) of the
output signal-to-noise ratio (SNR), as well as the system outage probability.
The results are based on explicit closed-form expressions which we derive for
the p.d.f. and c.d.f. of the maximum eigenvalue of double-correlated complex
Wishart matrices. For systems with two antennas at either the transmitter or
the receiver, we also derive exact closed-form expressions for the symbol error
rate (SER). The new expressions are used to prove that MIMO-MRC achieves the
maximum available spatial diversity order, and to demonstrate the effect of
spatial correlation. The analysis is validated through comparison with
Monte-Carlo simulations."
"It is believed that a particle cannot carry more than one bit of information.
It is pointed out that particle or single-particle quantum state can carry more
than one bit of information. It implies that minimum energy cost of
transmitting a bit will be less than the accepted limit KTlog2."
"We present error-correcting codes that achieve the information-theoretically
best possible trade-off between the rate and error-correction radius.
Specifically, for every $0 < R < 1$ and $\eps> 0$, we present an explicit
construction of error-correcting codes of rate $R$ that can be list decoded in
polynomial time up to a fraction $(1-R-\eps)$ of {\em worst-case} errors. At
least theoretically, this meets one of the central challenges in algorithmic
coding theory.
  Our codes are simple to describe: they are {\em folded Reed-Solomon codes},
which are in fact {\em exactly} Reed-Solomon (RS) codes, but viewed as a code
over a larger alphabet by careful bundling of codeword symbols. Given the
ubiquity of RS codes, this is an appealing feature of our result, and in fact
our methods directly yield better decoding algorithms for RS codes when errors
occur in {\em phased bursts}.
  The alphabet size of these folded RS codes is polynomial in the block length.
We are able to reduce this to a constant (depending on $\eps$) using ideas
concerning ``list recovery'' and expander-based codes from
\cite{GI-focs01,GI-ieeejl}. Concatenating the folded RS codes with suitable
inner codes also gives us polynomial time constructible binary codes that can
be efficiently list decoded up to the Zyablov bound, i.e., up to twice the
radius achieved by the standard GMD decoding of concatenated codes."
"By replacing linear averaging in Shannon entropy with Kolmogorov-Nagumo
average (KN-averages) or quasilinear mean and further imposing the additivity
constraint, R\'{e}nyi proposed the first formal generalization of Shannon
entropy. Using this recipe of R\'{e}nyi, one can prepare only two information
measures: Shannon and R\'{e}nyi entropy. Indeed, using this formalism R\'{e}nyi
characterized these additive entropies in terms of axioms of quasilinear mean.
As additivity is a characteristic property of Shannon entropy,
pseudo-additivity of the form $x \oplus_{q} y = x + y + (1-q)x y$ is a
characteristic property of nonextensive (or Tsallis) entropy. One can apply
R\'{e}nyi's recipe in the nonextensive case by replacing the linear averaging
in Tsallis entropy with KN-averages and thereby imposing the constraint of
pseudo-additivity. In this paper we show that nonextensive entropy is unique
under the R\'{e}nyi's recipe, and there by give a characterization."
"A wideband fading channel is considered with causal channel state information
(CSI) at the transmitter and no receiver CSI. A simple orthogonal code with
energy detection rule at the receiver (similar to [6]) is shown to achieve the
capacity of this channel in the limit of large bandwidth. This code transmits
energy only when the channel gain is large enough. In this limit, this capacity
without any receiver CSI is the same as the capacity with full receiver CSI--a
phenomenon also true for dirty paper coding. For Rayleigh fading, this capacity
(per unit time) is proportional to the logarithm of the bandwidth. Our coding
scheme is motivated from the Gel'fand-Pinsker [2,3] coding and dirty paper
coding [4]. Nonetheless, for our case, only causal CSI is required at the
transmitter in contrast with dirty-paper coding and Gel'fand-Pinsker coding,
where non-causal CSI is required.
  Then we consider a general discrete channel with i.i.d. states. Each input
has an associated cost and a zero cost input ""0"" exists. The channel state is
assumed be to be known at the transmitter in a causal manner. Capacity per unit
cost is found for this channel and a simple orthogonal code is shown to achieve
this capacity. Later, a novel orthogonal coding scheme is proposed for the case
of causal transmitter CSI and a condition for equivalence of capacity per unit
cost for causal and non-causal transmitter CSI is derived. Finally, some
connections are made to the case of non-causal transmitter CSI in [8]."
"We investigate energy-efficiency issues and resource allocation policies for
time division multi-access (TDMA) over fading channels in the power-limited
regime. Supposing that the channels are frequency-flat block-fading and
transmitters have full or quantized channel state information (CSI), we first
minimize power under a weighted sum-rate constraint and show that the optimal
rate and time allocation policies can be obtained by water-filling over
realizations of convex envelopes of the minima for cost-reward functions. We
then address a related minimization under individual rate constraints and
derive the optimal allocation policies via greedy water-filling. Using
water-filling across frequencies and fading states, we also extend our results
to frequency-selective channels. Our approaches not only provide fundamental
power limits when each user can support an infinite number of
capacity-achieving codebooks, but also yield guidelines for practical designs
where users can only support a finite number of adaptive modulation and coding
(AMC) modes with prescribed symbol error probabilities, and also for systems
where only discrete-time allocations are allowed."
"In the cryptanalysis of stream ciphers and pseudorandom sequences, the
notions of linear, jump, and 2-adic complexity arise naturally to measure the
(non)randomness of a given string. We define an isometry K on F_q^\infty that
is the precise equivalent to Euclid's algorithm over the reals to calculate the
continued fraction expansion of a formal power series. The continued fraction
expansion allows to deduce the linear and jump complexity profiles of the input
sequence. Since K is an isometry, the resulting F_q^\infty-sequence is i.i.d.
for i.i.d. input. Hence the linear and jump complexity profiles may be modelled
via Bernoulli experiments (for F_2: coin tossing), and we can apply the very
precise bounds as collected by Revesz, among others the Law of the Iterated
Logarithm.
  The second topic is the 2-adic span and complexity, as defined by Goresky and
Klapper. We derive again an isometry, this time on the dyadic integers Z_2
which induces an isometry A on F_2}^\infty. The corresponding jump complexity
behaves on average exactly like coin tossing.
  Index terms:
  Formal power series, isometry, linear complexity, jump complexity, 2-adic
complexity, 2-adic span, law of the iterated logarithm, Levy classes, stream
ciphers, pseudorandom sequences"
"A generalization of the problem of writing on dirty paper is considered in
which one transmitter sends a common message to multiple receivers. Each
receiver experiences on its link an additive interference (in addition to the
additive noise), which is known noncausally to the transmitter but not to any
of the receivers. Applications range from wireless multi-antenna multicasting
to robust dirty paper coding.
  We develop results for memoryless channels in Gaussian and binary special
cases. In most cases, we observe that the availability of side information at
the transmitter increases capacity relative to systems without such side
information, and that the lack of side information at the receivers decreases
capacity relative to systems with such side information.
  For the noiseless binary case, we establish the capacity when there are two
receivers. When there are many receivers, we show that the transmitter side
information provides a vanishingly small benefit. When the interference is
large and independent across the users, we show that time sharing is optimal.
  For the Gaussian case we present a coding scheme and establish its optimality
in the high signal-to-interference-plus-noise limit when there are two
receivers. When the interference is large and independent across users we show
that time-sharing is again optimal. Connections to the problem of robust dirty
paper coding are also discussed."
"The capacity region of the multiple access channel with arbitrarily
correlated sources remains an open problem. Cover, El Gamal and Salehi gave an
achievable region in the form of single-letter entropy and mutual information
expressions, without a single-letter converse. Cover, El Gamal and Salehi also
gave a converse in terms of some n-letter mutual informations, which are
incomputable. In this paper, we derive an upper bound for the sum rate of this
channel in a single-letter expression by using spectrum analysis. The
incomputability of the sum rate of Cover, El Gamal and Salehi scheme comes from
the difficulty of characterizing the possible joint distributions for the
n-letter channel inputs. Here we introduce a new data processing inequality,
which leads to a single-letter necessary condition for these possible joint
distributions. We develop a single-letter upper bound for the sum rate by using
this single-letter necessary condition on the possible joint distributions."
"We derive the density evolution equations for non-binary low-density
parity-check (LDPC) ensembles when transmission takes place over the binary
erasure channel. We introduce ensembles defined with respect to the general
linear group over the binary field. For these ensembles the density evolution
equations can be written compactly. The density evolution for the general
linear group helps us in understanding the density evolution for codes defined
with respect to finite fields. We compute thresholds for different alphabet
sizes for various LDPC ensembles. Surprisingly, the threshold is not a
monotonic function of the alphabet size. We state the stability condition for
non-binary LDPC ensembles over any binary memoryless symmetric channel. We also
give upper bounds on the MAP thresholds for various non-binary ensembles based
on EXIT curves and the area theorem."
"We prove a new outer bound on the rate-distortion region for the
multiterminal source-coding problem. This bound subsumes the best outer bound
in the literature and improves upon it strictly in some cases. The improved
bound enables us to obtain a new, conclusive result for the binary erasure
version of the ""CEO problem."" The bound recovers many of the converse results
that have been established for special cases of the problem, including the
recent one for the Gaussian version of the CEO problem."
"An upper bound to the information capacity of a wavelength-division multi-
plexed optical fiber communication system is derived in a model incorporating
the nonlinear propagation effects of cross-phase modulation (XPM). This work is
based on the paper by Mitra et al., finding lower bounds to the channel
capacity, in which physical models for propagation are used to calculate
statistical properties of the conditional probability distribution relating
input and output in a single WDM channel. In this paper we present a tractable
channel model incorporating the effects of cross phase modulation. Using this
model we find an upper bound to the information capacity of the fiber optical
communication channel at high SNR. The results provide physical insight into
the manner in which nonlinearities degrade the information capacity."
"The paper introduces ensembles of accumulate-repeat-accumulate (ARA) codes
which asymptotically achieve capacity on the binary erasure channel (BEC) with
{\em bounded complexity}, per information bit, of encoding and decoding. It
also introduces symmetry properties which play a central role in the
construction of capacity-achieving ensembles for the BEC with bounded
complexity. The results here improve on the tradeoff between performance and
complexity provided by previous constructions of capacity-achieving ensembles
of codes defined on graphs. The superiority of ARA codes with moderate to large
block length is exemplified by computer simulations which compare their
performance with those of previously reported capacity-achieving ensembles of
LDPC and IRA codes. The ARA codes also have the advantage of being systematic."
"We adopt a game theoretic approach for the design and analysis of distributed
resource allocation algorithms in fading multiple access channels. The users
are assumed to be selfish, rational, and limited by average power constraints.
We show that the sum-rate optimal point on the boundary of the multipleaccess
channel capacity region is the unique Nash Equilibrium of the corresponding
water-filling game. This result sheds a new light on the opportunistic
communication principle and argues for the fairness of the sum-rate optimal
point, at least from a game theoretic perspective. The base-station is then
introduced as a player interested in maximizing a weighted sum of the
individual rates. We propose a Stackelberg formulation in which the
base-station is the designated game leader. In this set-up, the base-station
announces first its strategy defined as the decoding order of the different
users, in the successive cancellation receiver, as a function of the channel
state. In the second stage, the users compete conditioned on this particular
decoding strategy. We show that this formulation allows for achieving all the
corner points of the capacity region, in addition to the sum-rate optimal
point. On the negative side, we prove the non-existence of a base-station
strategy in this formulation that achieves the rest of the boundary points. To
overcome this limitation, we present a repeated game approach which achieves
the capacity region of the fading multiple access channel. Finally, we extend
our study to vector channels highlighting interesting differences between this
scenario and the scalar channel case."
"A game-theoretic model for studying power control in multi-carrier CDMA
systems is proposed. Power control is modeled as a non-cooperative game in
which each user decides how much power to transmit over each carrier to
maximize its own utility. The utility function considered here measures the
number of reliable bits transmitted over all the carriers per Joule of energy
consumed and is particularly suitable for networks where energy efficiency is
important. The multi-dimensional nature of users' strategies and the
non-quasiconcavity of the utility function make the multi-carrier problem much
more challenging than the single-carrier or throughput-based-utility case. It
is shown that, for all linear receivers including the matched filter, the
decorrelator, and the minimum-mean-square-error (MMSE) detector, a user's
utility is maximized when the user transmits only on its ""best"" carrier. This
is the carrier that requires the least amount of power to achieve a particular
target signal-to-interference-plus-noise ratio (SINR) at the output of the
receiver. The existence and uniqueness of Nash equilibrium for the proposed
power control game are studied. In particular, conditions are given that must
be satisfied by the channel gains for a Nash equilibrium to exist, and the
distribution of the users among the carriers at equilibrium is also
characterized. In addition, an iterative and distributed algorithm for reaching
the equilibrium (when it exists) is presented. It is shown that the proposed
approach results in significant improvements in the total utility achieved at
equilibrium compared to a single-carrier system and also to a multi-carrier
system in which each user maximizes its utility over each carrier
independently."
"Performance of reliable communication over a coherent slow fading channel at
high SNR is succinctly captured as a fundamental tradeoff between diversity and
multiplexing gains. We study the problem of designing codes that optimally
tradeoff the diversity and multiplexing gains. Our main contribution is a
precise characterization of codes that are universally tradeoff-optimal, i.e.,
they optimally tradeoff the diversity and multiplexing gains for every
statistical characterization of the fading channel. We denote this
characterization as one of approximate universality where the approximation is
in the connection between error probability and outage capacity with diversity
and multiplexing gains, respectively. The characterization of approximate
universality is then used to construct new coding schemes as well as to show
optimality of several schemes proposed in the space-time coding literature."
"We are interested in how to best communicate a real valued source to a number
of destinations (sinks) over a network with capacity constraints in a
collective fidelity metric over all the sinks, a problem which we call joint
network-source coding. It is demonstrated that multiple description codes along
with proper diversity routing provide a powerful solution to joint
network-source coding. A systematic optimization approach is proposed. It
consists of optimizing the network routing given a multiple description code
and designing optimal multiple description code for the corresponding optimized
routes."
"Perfect space-time codes were first introduced by Oggier et. al. to be the
space-time codes that have full rate, full diversity-gain, non-vanishing
determinant for increasing spectral efficiency, uniform average transmitted
energy per antenna and good shaping of the constellation. These defining
conditions jointly correspond to optimality with respect to the Zheng-Tse D-MG
tradeoff, independent of channel statistics, as well as to near optimality in
maximizing mutual information. All the above traits endow the code with error
performance that is currently unmatched. Yet perfect space-time codes have been
constructed only for 2,3,4 and 6 transmit antennas. We construct minimum and
non-minimum delay perfect codes for all channel dimensions."
"This paper consists of two parts. In the first part, we develop a new
information theory, in which it is not a coincidence that information and
physical entropy share the same mathematical formula. It is an adaptation of
mind to help search for resources. We then show that psychological patterns
either reflect the constraints of physical laws or are evolutionary adaptations
to efficiently process information and to increase the chance of survival in
the environment of our evolutionary past. In the second part, we demonstrate
that the new information theory provides the foundation to understand market
behavior. One fundamental result from the information theory is that
information is costly. In general, information with higher value is more
costly. Another fundamental result from the information theory is that the
amount of information one can receive is the amount of information generated
minus equivocation. The level of equivocation, which is the measure of
information asymmetry, is determined by the correlation between the source of
information and the receiver of information. In general, how much information
one can receive depends on the background knowledge of the receiver. The
difference in cost different investors are willing to pay for information and
the difference in background knowledge about a particular information causes
the heterogeneity in information processing by the investment public, which is
the main reason of the price and volume patterns observed in the market. Many
assumptions in some of the recent models on behavioral finance can be derived
naturally from this theory."
"In this work we explicitly provide the first ever optimal, with respect to
the Zheng-Tse diversity multiplexing gain (D-MG) tradeoff, cooperative
diversity schemes for wireless relay networks. The schemes are based on
variants of perfect space-time codes and are optimal for any number of users
and all statistically symmetric (and in some cases, asymmetric) fading
distributions.
  We deduce that, with respect to the D-MG tradeoff, channel knowledge at the
intermediate relays and infinite delay are unnecessary. We also show that the
non-dynamic selection decode and forward strategy, the non-dynamic amplify and
forward, the non-dynamic receive and forward, the dynamic amplify and forward
and the dynamic receive and forward cooperative diversity strategies allow for
exactly the same D-MG optimal performance."
"We present a new model for LT codes which simplifies the analysis of the
error probability of decoding by belief propagation. For any given degree
distribution, we provide the first rigorous expression for the limiting error
probability as the length of the code goes to infinity via recent results in
random hypergraphs [Darling-Norris 2005]. For a code of finite length, we
provide an algorithm for computing the probability of error of the decoder.
This algorithm improves the one of [Karp-Luby-Shokrollahi 2004] by a linear
factor."
"Uncertainty principles for concentration of signals into truncated subspaces
are considered. The ``classic'' uncertainty principle is explored as a special
case of a more general operator framework. The time-bandwidth concentration
problem is shown as a similar special case. A spatial concentration of radio
signals example is provided, and it is shown that an uncertainty principle
exists for concentration of single-frequency signals for regions in space. We
show that the uncertainty is related to the volumes of the spatial regions."
"In this paper, we introduce the novel use of linear spatial precoding based
on fixed and known parameters of multiple-input multiple-output (MIMO) channels
to improve the performance of space-time coded MIMO systems. We derive linear
spatial precoding schemes for both coherent (channel is known at the receiver)
and non-coherent (channel is un-known at the receiver) space-time coded MIMO
systems. Antenna spacing and antenna placement (geometry) are considered as
fixed parameters of MIMO channels, which are readily known at the transmitter.
These precoding schemes exploit the antenna placement information at both ends
of the MIMO channel to ameliorate the effect of non-ideal antenna placement on
the performance of space-time coded systems. In these schemes, the precoder is
fixed for given transmit and receive antenna configurations and transmitter
does not require any feedback of channel state information (partial or full)
from the receiver. Closed form solutions for both precoding schemes are
presented for systems with up to three receiver antennas. A generalized method
is proposed for more than three receiver antennas. We use the coherent
space-time block codes (STBC) and differential space-time block codes to
analyze the performance of proposed precoding schemes. Simulation results show
that at low SNRs, both precoders give significant performance improvement over
a non-precoded system for small antenna aperture sizes."
"We estimate the variance of weight and stopping set distribution of regular
LDPC ensembles. Using this estimate and the second moment method we obtain
bounds on the probability that a randomly chosen code from regular LDPC
ensemble has its weight distribution and stopping set distribution close to
respective ensemble averages. We are able to show that a large fraction of
total number of codes have their weight and stopping set distribution close to
the average."
"Upper and lower bounds on the error probability of linear codes under
maximum-likelihood (ML) decoding are shortly surveyed and applied to ensembles
of codes on graphs. For upper bounds, focus is put on Gallager bounding
techniques and their relation to a variety of other reported bounds. Within the
class of lower bounds, we address de Caen's based bounds and their
improvements, sphere-packing bounds, and information-theoretic bounds on the
bit error probability of codes defined on graphs. A comprehensive overview is
provided in a monograph by the authors which is currently in preparation."
"The paper is focused on the tradeoff between performance and decoding
complexity per iteration for LDPC codes in terms of their gap (in rate) to
capacity. The study of this tradeoff is done via information-theoretic bounds
which also enable to get an indication on the sub-optimality of message-passing
iterative decoding algorithms (as compared to optimal ML decoding). The bounds
are generalized for parallel channels, and are applied to ensembles of
punctured LDPC codes where both intentional and random puncturing are
addressed. This work suggests an improvement in the tightness of some
information-theoretic bounds which were previously derived by Burshtein et al.
and by Sason and Urbanke."
"The paper presents bounds on the achievable rates and the decoding complexity
of low-density parity-check (LDPC) codes. It is assumed that the communication
of these codes takes place over statistically independent parallel channels
where these channels are memoryless, binary-input and output-symmetric (MBIOS).
The bounds are applied to punctured LDPC codes. A diagram concludes our
discussion by showing interconnections between the theorems in this paper and
some previously reported results."
"The goal of the present paper is the derivation of a framework for the
finite-length analysis of message-passing iterative decoding of low-density
parity-check codes. To this end we introduce the concept of graph-cover
decoding. Whereas in maximum-likelihood decoding all codewords in a code are
competing to be the best explanation of the received vector, under graph-cover
decoding all codewords in all finite covers of a Tanner graph representation of
the code are competing to be the best explanation. We are interested in
graph-cover decoding because it is a theoretical tool that can be used to show
connections between linear programming decoding and message-passing iterative
decoding. Namely, on the one hand it turns out that graph-cover decoding is
essentially equivalent to linear programming decoding. On the other hand,
because iterative, locally operating decoding algorithms like message-passing
iterative decoding cannot distinguish the underlying Tanner graph from any
covering graph, graph-cover decoding can serve as a model to explain the
behavior of message-passing iterative decoding. Understanding the behavior of
graph-cover decoding is tantamount to understanding the so-called fundamental
polytope. Therefore, we give some characterizations of this polytope and
explain its relation to earlier concepts that were introduced to understand the
behavior of message-passing iterative decoding for finite-length codes."
"We develop a code length principle which is invariant to the choice of
parameterization on the model distributions. An invariant approximation formula
for easy computation of the marginal distribution is provided for gaussian
likelihood models. We provide invariant estimators of the model parameters and
formulate conditions under which these estimators are essentially posteriori
unbiased for gaussian models. An upper bound on the coarseness of
discretization on the model parameters is deduced. We introduce a
discrimination measure between probability distributions and use it to
construct probability distributions on model classes. The total code length is
shown to equal the NML code length of Rissanen to within an additive constant
when choosing Jeffreys prior distribution on the model parameters together with
a particular choice of prior distribution on the model classes. Our model
selection principle is applied to a gaussian estimation problem for data in a
wavelet representation and its performance is tested and compared to
alternative wavelet-based estimation methods in numerical experiments"
"In this letter we present a new construction of interleavers for turbo codes
from 3-regular Hamiltonian graphs. The interleavers can be generated using a
few parameters, which can be selected in such a way that the girth of the
interleaver graph (IG) becomes large, inducing a high summary distance. The
size of the search space for these parameters is derived. The proposed
interleavers themselves work as their de-interleavers."
"Gaussian channels with memory and with noiseless feedback have been widely
studied in the information theory literature. However, a coding scheme to
achieve the feedback capacity is not available. In this paper, a coding scheme
is proposed to achieve the feedback capacity for Gaussian channels. The coding
scheme essentially implements the celebrated Kalman filter algorithm, and is
equivalent to an estimation system over the same channel without feedback. It
reveals that the achievable information rate of the feedback communication
system can be alternatively given by the decay rate of the Cramer-Rao bound of
the associated estimation system. Thus, combined with the control theoretic
characterizations of feedback communication (proposed by Elia), this implies
that the fundamental limitations in feedback communication, estimation, and
control coincide. This leads to a unifying perspective that integrates
information, estimation, and control. We also establish the optimality of the
Kalman filtering in the sense of information transmission, a supplement to the
optimality of Kalman filtering in the sense of information processing proposed
by Mitter and Newton. In addition, the proposed coding scheme generalizes the
Schalkwijk-Kailath codes and reduces the coding complexity and coding delay.
The construction of the coding scheme amounts to solving a finite-dimensional
optimization problem. A simplification to the optimal stationary input
distribution developed by Yang, Kavcic, and Tatikonda is also obtained. The
results are verified in a numerical example."
"We investigate the computation of Csiszar's bounds for the joint
source-channel coding (JSCC) error exponent, E_J, of a communication system
consisting of a discrete memoryless source and a discrete memoryless channel.
We provide equivalent expressions for these bounds and derive explicit formulas
for the rates where the bounds are attained. These equivalent representations
can be readily computed for arbitrary source-channel pairs via Arimoto's
algorithm. When the channel's distribution satisfies a symmetry property, the
bounds admit closed-form parametric expressions. We then use our results to
provide a systematic comparison between the JSCC error exponent E_J and the
tandem coding error exponent E_T, which applies if the source and channel are
separately coded. It is shown that E_T <= E_J <= 2E_T. We establish conditions
for which E_J > E_T and for which E_J = 2E_T. Numerical examples indicate that
E_J is close to 2E_T for many source-channel pairs. This gain translates into a
power saving larger than 2 dB for a binary source transmitted over additive
white Gaussian noise channels and Rayleigh fading channels with finite output
quantization. Finally, we study the computation of the lossy JSCC error
exponent under the Hamming distortion measure."
"We review how Shannon's classical notion of capacity is not enough to
characterize a noisy communication channel if the channel is intended to be
used as part of a feedback loop to stabilize an unstable scalar linear system.
While classical capacity is not enough, another sense of capacity (parametrized
by reliability) called ``anytime capacity'' is shown to be necessary for the
stabilization of an unstable process. The required rate is given by the log of
the unstable system gain and the required reliability comes from the sense of
stability desired. A consequence of this necessity result is a sequential
generalization of the Schalkwijk/Kailath scheme for communication over the AWGN
channel with feedback.
  In cases of sufficiently rich information patterns between the encoder and
decoder, adequate anytime capacity is also shown to be sufficient for there to
exist a stabilizing controller. These sufficiency results are then generalized
to cases with noisy observations, delayed control actions, and without any
explicit feedback between the observer and the controller. Both necessary and
sufficient conditions are extended to continuous time systems as well. We close
with comments discussing a hierarchy of difficulty for communication problems
and how these results establish where stabilization problems sit in that
hierarchy."
"The capacity of peak-power limited, single-antenna, non-coherent, flat-fading
channels with memory is considered. The emphasis is on the capacity pre-log,
i.e., on the limiting ratio of channel capacity to the logarithm of the
signal-to-noise ratio (SNR), as the SNR tends to infinity. It is shown that,
among all stationary and ergodic fading processes of a given spectral
distribution function whose law has no mass point at zero, the Gaussian process
gives rise to the smallest pre-log."
"We provide a tight approximate characterization of the $n$-dimensional
product multicommodity flow (PMF) region for a wireless network of $n$ nodes.
Separate characterizations in terms of the spectral properties of appropriate
network graphs are obtained in both an information theoretic sense and for a
combinatorial interference model (e.g., Protocol model). These provide an inner
approximation to the $n^2$ dimensional capacity region. These results answer
the following questions which arise naturally from previous work: (a) What is
the significance of $1/\sqrt{n}$ in the scaling laws for the Protocol
interference model obtained by Gupta and Kumar (2000)? (b) Can we obtain a
tight approximation to the ""maximum supportable flow"" for node distributions
more general than the geometric random distribution, traffic models other than
randomly chosen source-destination pairs, and under very general assumptions on
the channel fading model?
  We first establish that the random source-destination model is essentially a
one-dimensional approximation to the capacity region, and a special case of
product multi-commodity flow. Building on previous results, for a combinatorial
interference model given by a network and a conflict graph, we relate the
product multicommodity flow to the spectral properties of the underlying graphs
resulting in computational upper and lower bounds. For the more interesting
random fading model with additive white Gaussian noise (AWGN), we show that the
scaling laws for PMF can again be tightly characterized by the spectral
properties of appropriately defined graphs. As an implication, we obtain
computationally efficient upper and lower bounds on the PMF for any wireless
network with a guaranteed approximation factor."
"We derive new upper and lower bounds on the fading number of multiple-input
single-output (MISO) fading channels of general (not necessarily Gaussian)
regular law with spatial and temporal memory. The fading number is the second
term, after the double-logarithmic term, of the high signal-to-noise ratio
(SNR) expansion of channel capacity.
  In case of an isotropically distributed fading vector it is proven that the
upper and lower bound coincide, i.e., the general MISO fading number with
memory is known precisely.
  The upper and lower bounds show that a type of beam-forming is asymptotically
optimal."
"An algorithm for exact maximum likelihood(ML) decoding on tail-biting
trellises is presented, which exhibits very good average case behavior. An
approximate variant is proposed, whose simulated performance is observed to be
virtually indistinguishable from the exact one at all values of signal to noise
ratio, and which effectively performs computations equivalent to at most two
rounds on the tail-biting trellis. The approximate algorithm is analyzed, and
the conditions under which its output is different from the ML output are
deduced. The results of simulations on an AWGN channel for the exact and
approximate algorithms on the 16 state tail-biting trellis for the (24,12)
Extended Golay Code, and tail-biting trellises for two rate 1/2 convolutional
codes with memories of 4 and 6 respectively, are reported. An advantage of our
algorithms is that they do not suffer from the effects of limit cycles or the
presence of pseudocodewords."
"We propose to send a Gaussian source over an average-power limited additive
white Gaussian noise channel by transmitting a linear combination of the source
sequence and the result of its quantization using a high dimensional Gaussian
vector quantizer. We show that, irrespective of the rate of the vector
quantizer (assumed to be fixed and smaller than the channel's capacity), this
transmission scheme is asymptotically optimal (as the quantizer's dimension
tends to infinity) under the mean squared-error fidelity criterion. This
generalizes the classical result of Goblick about the optimality of scaled
uncoded transmission, which corresponds to choosing the rate of the vector
quantizer as zero, and the classical source-channel separation approach, which
corresponds to choosing the rate of the vector quantizer arbitrarily close to
the capacity of the channel."
"We consider a problem where a memoryless bi-variate Gaussian source is to be
transmitted over an additive white Gaussian multiple-access channel with two
transmitting terminals and one receiving terminal. The first transmitter only
sees the first source component and the second transmitter only sees the second
source component. We are interested in the pair of mean squared-error
distortions at which the receiving terminal can reproduce each of the source
components.
  It is demonstrated that in the symmetric case, below a certain
signal-to-noise ratio (SNR) threshold, which is determined by the source
correlation, uncoded communication is optimal. For SNRs above this threshold we
present outer and inner bounds on the achievable distortions."
"We consider questions related to the computation of the capacity of codes
that avoid forbidden difference patterns. The maximal number of $n$-bit
sequences whose pairwise differences do not contain some given forbidden
difference patterns increases exponentially with $n$. The exponent is the
capacity of the forbidden patterns, which is given by the logarithm of the
joint spectral radius of a set of matrices constructed from the forbidden
difference patterns. We provide a new family of bounds that allows for the
approximation, in exponential time, of the capacity with arbitrary high degree
of accuracy. We also provide a polynomial time algorithm for the problem of
determining if the capacity of a set is positive, but we prove that the same
problem becomes NP-hard when the sets of forbidden patterns are defined over an
extended set of symbols. Finally, we prove the existence of extremal norms for
the sets of matrices arising in the capacity computation. This result makes it
possible to apply a specific (even though non polynomial) approximation
algorithm. We illustrate this fact by computing exactly the capacity of codes
that were only known approximately."
"Let C = {x_1,...,x_N} \subset {0,1}^n be an [n,N] binary error correcting
code (not necessarily linear). Let e \in {0,1}^n be an error vector. A codeword
x in C is said to be ""disturbed"" by the error e if the closest codeword to x +
e is no longer x. Let A_e be the subset of codewords in C that are disturbed by
e. In this work we study the size of A_e in random codes C (i.e. codes in which
each codeword x_i is chosen uniformly and independently at random from
{0,1}^n). Using recent results of Vu [Random Structures and Algorithms 20(3)]
on the concentration of non-Lipschitz functions, we show that |A_e| is strongly
concentrated for a wide range of values of N and ||e||.
  We apply this result in the study of communication channels we refer to as
""oblivious"". Roughly speaking, a channel W(y|x) is said to be oblivious if the
error distribution imposed by the channel is independent of the transmitted
codeword x. For example, the well studied Binary Symmetric Channel is an
oblivious channel.
  In this work, we define oblivious and partially oblivious channels and
present lower bounds on their capacity. The oblivious channels we define have
connections to Arbitrarily Varying Channels with state constraints."
"Recently a powerful class of rate-compatible serially concatenated
convolutional codes (SCCCs) have been proposed based on minimizing analytical
upper bounds on the error probability in the error floor region. Here this
class of codes is further investigated by combining analytical upper bounds
with extrinsic information transfer charts analysis. Following this approach,
we construct a family of rate-compatible SCCCs with good performance in both
the error floor and the waterfall regions over a broad range of code rates."
"Compression algorithms reduce the redundancy in data representation to
decrease the storage required for that data. Data compression offers an
attractive approach to reducing communication costs by using available
bandwidth effectively. Over the last decade there has been an unprecedented
explosion in the amount of digital data transmitted via the Internet,
representing text, images, video, sound, computer programs, etc. With this
trend expected to continue, it makes sense to pursue research on developing
algorithms that can most effectively use available network bandwidth by
maximally compressing data. This research paper is focused on addressing this
problem of lossless compression of text files. Lossless compression researchers
have developed highly sophisticated approaches, such as Huffman encoding,
arithmetic encoding, the Lempel-Ziv family, Dynamic Markov Compression (DMC),
Prediction by Partial Matching (PPM), and Burrows-Wheeler Transform (BWT) based
algorithms. However, none of these methods has been able to reach the
theoretical best-case compression ratio consistently, which suggests that
better algorithms may be possible. One approach for trying to attain better
compression ratios is to develop new compression algorithms. An alternative
approach, however, is to develop intelligent, reversible transformations that
can be applied to a source text that improve an existing, or backend,
algorithm's ability to compress. The latter strategy has been explored here."
"Though Shannon entropy of a probability measure $P$, defined as $- \int_{X}
\frac{\ud P}{\ud \mu} \ln \frac{\ud P}{\ud\mu} \ud \mu$ on a measure space $(X,
\mathfrak{M},\mu)$, does not qualify itself as an information measure (it is
not a natural extension of the discrete case), maximum entropy (ME)
prescriptions in the measure-theoretic case are consistent with that of
discrete case. In this paper, we study the measure-theoretic definitions of
generalized information measures and discuss the ME prescriptions. We present
two results in this regard: (i) we prove that, as in the case of classical
relative-entropy, the measure-theoretic definitions of generalized
relative-entropies, R\'{e}nyi and Tsallis, are natural extensions of their
respective discrete cases, (ii) we show that, ME prescriptions of
measure-theoretic Tsallis entropy are consistent with the discrete case."
"We consider coding schemes for channels with non-uniform inputs (NUI), where
standard linear block codes can not be applied directly. We show that
multilevel coding (MLC) with a set of linear codes and a deterministic mapper
can achieve the information rate of the channel with NUI. The mapper, however,
does not have to be one-to-one. As an application of the proposed MLC scheme,
we present a rateless transmission scheme over the binary symmetric channel
(BSC)."
"A novel code construction algorithm is presented to find all the possible
code families for code reconfiguration in an OCDMA system. The algorithm is
developed through searching all the complete subgraphs of a constructed graph.
The proposed algorithm is flexible and practical for constructing optical
orthogonal codes (OOCs) of arbitrary requirement. Simulation results show that
one should choose an appropriate code length in order to obtain sufficient
number of code families for code reconfiguration with reasonable cost."
"A construction of expander codes is presented with the following three
properties:
  (i) the codes lie close to the Singleton bound, (ii) they can be encoded in
time complexity that is linear in their code length, and (iii) they have a
linear-time bounded-distance decoder.
  By using a version of the decoder that corrects also erasures, the codes can
replace MDS outer codes in concatenated constructions, thus resulting in
linear-time encodable and decodable codes that approach the Zyablov bound or
the capacity of memoryless channels. The presented construction improves on an
earlier result by Guruswami and Indyk in that any rate and relative minimum
distance that lies below the Singleton bound is attainable for a significantly
smaller alphabet size."
"A simple scheme for communication over MIMO broadcast channels is introduced
which adopts the lattice reduction technique to improve the naive channel
inversion method. Lattice basis reduction helps us to reduce the average
transmitted energy by modifying the region which includes the constellation
points. Simulation results show that the proposed scheme performs well, and as
compared to the more complex methods (such as the perturbation method) has a
negligible loss. Moreover, the proposed method is extended to the case of
different rates for different users. The asymptotic behavior of the symbol
error rate of the proposed method and the perturbation technique, and also the
outage probability for the case of fixed-rate users is analyzed. It is shown
that the proposed method, based on LLL lattice reduction, achieves the optimum
asymptotic slope of symbol-error-rate (called the precoding diversity). Also,
the outage probability for the case of fixed sum-rate is analyzed."
"Diversity order is an important measure for the performance of communication
systems over MIMO fading channels. In this paper, we prove that in MIMO
multiple access systems (or MIMO point-to-point systems with V-BLAST
transmission), lattice-reduction-aided decoding achieves the maximum receive
diversity (which is equal to the number of receive antennas). Also, we prove
that the naive lattice decoding (which discards the out-of-region decoded
points) achieves the maximum diversity."
"We consider stability of scheduled multiaccess message communication with
random coding and joint maximum-likehood decoding of messages. The framework we
consider here models both the random message arrivals and the subsequent
reliable communication by suitably combining techniques from queueing theory
and information theory. The number of messages that may be scheduled for
simultaneous transmission is limited to a given maximum value, and the channels
from transmitters to receiver are quasi-static, flat, and have independent
fades. Requests for message transmissions are assumed to arrive according to an
i.i.d. arrival process. Then, (i) we derive an outer bound to the region of
message arrival rate vectors achievable by the class of stationary scheduling
policies, (ii) we show for any message arrival rate vector that satisfies the
outerbound, that there exists a stationary state-independent policy that
results in a stable system for the corresponding message arrival process, and
(iii) in the limit of large message lengths, we show that the stability region
of message nat arrival rate vectors has information-theoretic capacity region
interpretation."
"We consider scheduled message communication over a discrete memoryless
degraded broadcast channel. The framework we consider here models both the
random message arrivals and the subsequent reliable communication by suitably
combining techniques from queueing theory and information theory. The channel
from the transmitter to each of the receivers is quasi-static, flat, and with
independent fades across the receivers. Requests for message transmissions are
assumed to arrive according to an i.i.d. arrival process. Then, (i) we derive
an outer bound to the region of message arrival vectors achievable by the class
of stationary scheduling policies, (ii) we show for any message arrival vector
that satisfies the outerbound, that there exists a stationary
``state-independent'' policy that results in a stable system for the
corresponding message arrival process, and (iii) under two asymptotic regimes,
we show that the stability region of nat arrival rate vectors has
information-theoretic capacity region interpretation."
"Product codes are widely used in data-storage, optical and wireless
applications. Their analytical performance evaluation usually relies on the
truncated union bound, which provides a low error rate approximation based on
the minimum distance term only. In fact, the complete weight enumerator of most
product codes remains unknown. In this paper, concatenated representations are
introduced and applied to compute the complete average enumerators of arbitrary
product codes over a field Fq. The split weight enumerators of some important
constituent codes (Hamming, Reed-Solomon) are studied and used in the analysis.
The average binary weight enumerators of Reed Solomon product codes are also
derived. Numerical results showing the enumerator behavior are presented. By
using the complete enumerators, Poltyrev bounds on the maximum likelihood
performance, holding at both high and low error rates, are finally shown and
compared against truncated union bounds and simulation results."
"The energy-delay tradeoffs in wireless networks are studied using a
game-theoretic framework. A multi-class multiple-access network is considered
in which users choose their transmit powers, and possibly transmission rates,
in a distributed manner to maximize their own utilities while satisfying their
delay quality-of-service (QoS) requirements. The utility function considered
here measures the number of reliable bits transmitted per Joule of energy
consumed and is particularly useful for energy-constrained networks. The Nash
equilibrium solution for the proposed non-cooperative game is presented and
closed-form expressions for the users' utilities at equilibrium are obtained.
Based on this, the losses in energy efficiency and network capacity due to
presence of delay-sensitive users are quantified. The analysis is extended to
the scenario where the QoS requirements include both the average source rate
and a bound on the average total delay (including queuing delay). It is shown
that the incoming traffic rate and the delay constraint of a user translate
into a ""size"" for the user, which is an indication of the amount of resources
consumed by the user. Using this framework, the tradeoffs among throughput,
delay, network capacity and energy efficiency are also quantified."
"Detectability of failures of linear programming (LP) decoding and its
potential for improvement by adding new constraints motivate the use of an
adaptive approach in selecting the constraints for the LP problem. In this
paper, we make a first step in studying this method, and show that it can
significantly reduce the complexity of the problem, which was originally
exponential in the maximum check-node degree. We further show that adaptively
adding new constraints, e.g. by combining parity checks, can provide large
gains in the performance."
"We describe the structure of optimal Input covariance matrices for single
user multiple-input/multiple-output (MIMO) communication system with covariance
feedback and for general correlated fading. Our approach is based on the novel
concept of right commutant and recovers previously derived results for the
Kronecker product models. Conditions are derived which allow a significant
simplification of the optimization problem."
"We investigate the behavior of two users and one jammer in an AWGN channel
with and without fading when they participate in a non-cooperative zero-sum
game, with the channel's input/output mutual information as the objective
function. We assume that the jammer can eavesdrop the channel and can use the
information obtained to perform correlated jamming. Under various assumptions
on the channel characteristics, and the extent of information available at the
users and the jammer, we show the existence, or otherwise non-existence of a
simultaneously optimal set of strategies for the users and the jammer. In all
the cases where the channel is non-fading, we show that the game has a
solution, and the optimal strategies are Gaussian signalling for the users and
linear jamming for the jammer. In fading channels, we envision each player's
strategy as a power allocation function over the channel states, together with
the signalling strategies at each channel state. We reduce the game solution to
a set of power allocation functions for the players and show that when the
jammer is uncorrelated, the game has a solution, but when the jammer is
correlated, a set of simultaneously optimal power allocation functions for the
users and the jammer does not always exist. In this case, we characterize the
max-min user power allocation strategies and the corresponding jammer power
allocation strategy."
"The minimum mean-square error of the estimation of a signal where observed
from the additive white Gaussian noise (WGN) channel's output, is analyzed. It
is assumed that the channel input's signal is composed of a (normalized) sum of
N narrowband, mutually independent waves. It is shown that if N goes to
infinity, then for any fixed signal energy to noise energy ratio (no mater how
big) both the causal minimum mean-square error CMMSE and the non-causal minimum
mean-square error MMSE converge to the signal energy at a rate which is
proportional to 1/N."
"We propose a new construction for low-density source codes with multiple
parameters that can be tuned to optimize the performance of the code. In
addition, we introduce a set of analysis techniques for deriving upper bounds
for the expected distortion of our construction, as well as more general
low-density constructions. We show that (with an optimal encoding algorithm)
our codes achieve the rate-distortion bound for a binary symmetric source and
Hamming distortion. Our methods also provide rigorous upper bounds on the
minimum distortion achievable by previously proposed low-density constructions."
"For a fading Gaussian multiple access channel with user cooperation, we
obtain the optimal power allocation policies that maximize the rates achievable
by block Markov superposition coding. The optimal policies result in a coding
scheme that is simpler than the one for a general multiple access channel with
generalized feedback. This simpler coding scheme also leads to the possibility
of formulating an otherwise non-concave optimization problem as a concave one.
Using the channel state information at the transmitters to adapt the powers, we
demonstrate significant gains over the achievable rates for existing
cooperative systems."
"A linear time approximate maximum likelihood decoding algorithm on
tail-biting trellises is prsented, that requires exactly two rounds on the
trellis. This is an adaptation of an algorithm proposed earlier with the
advantage that it reduces the time complexity from O(mlogm) to O(m) where m is
the number of nodes in the tail-biting trellis. A necessary condition for the
output of the algorithm to differ from the output of the ideal ML decoder is
reduced and simulation results on an AWGN channel using tail-biting rrellises
for two rate 1/2 convoluational codes with memory 4 and 6 respectively are
reported"
"In this paper we use game theoretic techniques to study the value of
cooperation in distributed spectrum management problems. We show that the
celebrated iterative water-filling algorithm is subject to the prisoner's
dilemma and therefore can lead to severe degradation of the achievable rate
region in an interference channel environment. We also provide thorough
analysis of a simple two bands near-far situation where we are able to provide
closed form tight bounds on the rate region of both fixed margin iterative
water filling (FM-IWF) and dynamic frequency division multiplexing (DFDM)
methods. This is the only case where such analytic expressions are known and
all previous studies included only simulated results of the rate region. We
then propose an alternative algorithm that alleviates some of the drawbacks of
the IWF algorithm in near-far scenarios relevant to DSL access networks. We
also provide experimental analysis based on measured DSL channels of both
algorithms as well as the centralized optimum spectrum management."
"In this paper, we analyze the asymptotic performance of multiple antenna
channels where the transmitter has either perfect or finite bit channel state
information. Using the diversity-multiplexing tradeoff to characterize the
system performance, we demonstrate that channel feedback can fundamentally
change the system behavior. Even one-bit of information can increase the
diversity order of the system compared to the system with no transmitter
information. In addition, as the amount of channel information at the
transmitter increases, the diversity order for each multiplexing gain increases
and goes to infinity for perfect transmitter information. The major reason for
diversity order gain is a ""location-dependent"" temporal power control, which
adapts the power control strategy based on the average channel conditions of
the channel."
"The structure and size of the interleaver used in a turbo code critically
affect the distance spectrum and the covariance property of a component
decoder's information input and soft output. This paper introduces a new class
of interleavers, the inter-block permutation (IBP) interleavers, that can be
build on any existing ""good"" block-wise interleaver by simply adding an IBP
stage. The IBP interleavers reduce the above-mentioned correlation and increase
the effective interleaving size. The increased effective interleaving size
improves the distance spectrum while the reduced covariance enhances the
iterative decoder's performance. Moreover, the structure of the
IBP(-interleaved) turbo codes (IBPTC) is naturally fit for high rate
applications that necessitate parallel decoding.
  We present some useful bounds and constraints associated with the IBPTC that
can be used as design guidelines. The corresponding codeword weight upper
bounds for weight-2 and weight-4 input sequences are derived. Based on some of
the design guidelines, we propose a simple IBP algorithm and show that the
associated IBPTC yields 0.3 to 1.2 dB performance gain, or equivalently, an
IBPTC renders the same performance with a much reduced interleaving delay. The
EXIT and covariance behaviors provide another numerical proof of the
superiority of the proposed IBPTC."
"A communication theory for a transmitter broadcasting to many receivers is
presented. In this case energetic considerations cannot be neglected as in
Shannon theory. It is shown that, when energy is assigned to the information
bit, information theory complies with classical thermodynamic and is part of
it. To provide a thermodynamic theory of communication it is necessary to
define equilibrium for informatics systems that are not in thermal equilibrium
and to calculate temperature, heat, and entropy with accordance to Clausius
inequality. It is shown that for a binary file the temperature is proportional
to the bit energy and that information is thermodynamic entropy. Equilibrium
exists in random files that cannot be compressed. Thermodynamic bounds on the
computing power of a physical device, and the maximum information that an
antenna can broadcast are calculated."
"We consider the CDMA (code-division multiple-access) multi-user detection
problem for binary signals and additive white gaussian noise. We propose a
spreading sequences scheme based on random sparse signatures, and a detection
algorithm based on belief propagation (BP) with linear time complexity. In the
new scheme, each user conveys its power onto a finite number of chips l, in the
large system limit.
  We analyze the performances of BP detection and prove that they coincide with
the ones of optimal (symbol MAP) detection in the l->\infty limit. In the same
limit, we prove that the information capacity of the system converges to
Tanaka's formula for random `dense' signatures, thus providing the first
rigorous justification of this formula. Apart from being computationally
convenient, the new scheme allows for optimization in close analogy with
irregular low density parity check code ensembles."
"Space-Time block codes (STBC) from Orthogonal Designs (OD) and Co-ordinate
Interleaved Orthogonal Designs (CIOD) have been attracting wider attention due
to their amenability for fast (single-symbol) ML decoding, and full-rate with
full-rank over quasi-static fading channels. However, these codes are instances
of single-symbol decodable codes and it is natural to ask, if there exist codes
other than STBCs form ODs and CIODs that allow single-symbol coding?
  In this paper, the above question is answered in the affirmative by
characterizing all linear STBCs, that allow single-symbol ML decoding (not
necessarily full-diversity) over quasi-static fading channels-calling them
single-symbol decodable designs (SDD). The class SDD includes ODs and CIODs as
proper subclasses. Further, among the SDD, a class of those that offer
full-diversity, called Full-rank SDD (FSDD) are characterized and classified."
"In this paper we derive analytical expressions for the central and side
quantizers which, under high-resolutions assumptions, minimize the expected
distortion of a symmetric multiple-description lattice vector quantization
(MD-LVQ) system subject to entropy constraints on the side descriptions for
given packet-loss probabilities.
  We consider a special case of the general n-channel symmetric
multiple-description problem where only a single parameter controls the
redundancy tradeoffs between the central and the side distortions. Previous
work on two-channel MD-LVQ showed that the distortions of the side quantizers
can be expressed through the normalized second moment of a sphere. We show here
that this is also the case for three-channel MD-LVQ. Furthermore, we conjecture
that this is true for the general n-channel MD-LVQ.
  For given source, target rate and packet-loss probabilities we find the
optimal number of descriptions and construct the MD-LVQ system that minimizes
the expected distortion. We verify theoretical expressions by numerical
simulations and show in a practical setup that significant performance
improvements can be achieved over state-of-the-art two-channel MD-LVQ by using
three-channel MD-LVQ."
"Recent work has suggested that low-density generator matrix (LDGM) codes are
likely to be effective for lossy source coding problems. We derive rigorous
upper bounds on the effective rate-distortion function of LDGM codes for the
binary symmetric source, showing that they quickly approach the rate-distortion
function as the degree increases. We also compare and contrast the standard
LDGM construction with a compound LDPC/LDGM construction introduced in our
previous work, which provably saturates the rate-distortion bound with finite
degrees. Moreover, this compound construction can be used to generate nested
codes that are simultaneously good as source and channel codes, and are hence
well-suited to source/channel coding with side information. The sparse and
high-girth graphical structure of our constructions render them well-suited to
message-passing encoding."
"The performance of the automatic repeat request-dynamic decode and forward
(ARQ-DDF) cooperation protocol is analyzed in two distinct scenarios. The first
scenario is the multiple access relay (MAR) channel where a single relay is
dedicated to simultaneously help several multiple access users. For this setup,
it is shown that the ARQ-DDF protocol achieves the optimal diversity
multiplexing tradeoff (DMT) of the channel. The second scenario is the
cooperative vector multiple access (CVMA) channel where the users cooperate in
delivering their messages to a destination equipped with multiple receiving
antennas. For this setup, we develop a new variant of the ARQ-DDF protocol
where the users are purposefully instructed not to cooperate in the first round
of transmission. Lower and upper bounds on the achievable DMT are then derived.
These bounds are shown to converge to the optimal tradeoff as the number of
transmission rounds increases."
"A novel lattice coding framework is proposed for outage-limited cooperative
channels. This framework provides practical implementations for the optimal
cooperation protocols proposed by Azarian et al. In particular, for the relay
channel we implement a variant of the dynamic decode and forward protocol,
which uses orthogonal constellations to reduce the channel seen by the
destination to a single-input single-output time-selective one, while
inheriting the same diversity-multiplexing tradeoff. This simplification allows
for building the receiver using traditional belief propagation or tree search
architectures. Our framework also generalizes the coding scheme of Yang and
Belfiore in the context of amplify and forward cooperation. For the cooperative
multiple access channel, a tree coding approach, matched to the optimal linear
cooperation protocol of Azarain et al, is developed. For this scenario, the
MMSE-DFE Fano decoder is shown to enjoy an excellent tradeoff between
performance and complexity. Finally, the utility of the proposed schemes is
established via a comprehensive simulation study."
"In slow fading scenarios, cooperation between nodes can increase the amount
of diversity for communication. We study the performance limit in such
scenarios by analyzing the outage capacity of slow fading relay channels. Our
focus is on the low SNR and low outage probability regime, where the adverse
impact of fading is greatest but so are the potential gains from cooperation.
We showed that while the standard Amplify-Forward protocol performs very poorly
in this regime, a modified version we called the Bursty Amplify-Forward
protocol is optimal and achieves the outage capacity of the network. Moreover,
this performance can be achieved without a priori channel knowledge at the
receivers. In contrast, the Decode-Forward protocol is strictly sub-optimal in
this regime. Our results directly yield the outage capacity per unit energy of
fading relay channels."
"A recent result of Zheng and Tse states that over a quasi-static channel,
there exists a fundamental tradeoff, referred to as the diversity-multiplexing
gain (D-MG) tradeoff, between the spatial multiplexing gain and the diversity
gain that can be simultaneously achieved by a space-time (ST) block code. This
tradeoff is precisely known in the case of i.i.d. Rayleigh-fading, for T>=
n_t+n_r-1 where T is the number of time slots over which coding takes place and
n_t,n_r are the number of transmit and receive antennas respectively. For T <
n_t+n_r-1, only upper and lower bounds on the D-MG tradeoff are available.
  In this paper, we present a complete solution to the problem of explicitly
constructing D-MG optimal ST codes, i.e., codes that achieve the D-MG tradeoff
for any number of receive antennas. We do this by showing that for the square
minimum-delay case when T=n_t=n, cyclic-division-algebra (CDA) based ST codes
having the non-vanishing determinant property are D-MG optimal. While
constructions of such codes were previously known for restricted values of n,
we provide here a construction for such codes that is valid for all n.
  For the rectangular, T > n_t case, we present two general techniques for
building D-MG-optimal rectangular ST codes from their square counterparts. A
byproduct of our results establishes that the D-MG tradeoff for all T>= n_t is
the same as that previously known to hold for T >= n_t + n_r -1."
"We study an incremental redundancy (IR) cooperative coding scheme for
wireless networks. To exploit the spatial diversity benefit we propose a
cluster-based collaborating strategy for a quasi-static Rayleigh fading channel
model and based on a network geometric distance profile. Our scheme enhances
the network performance by embedding an IR cooperative coding scheme into an
existing noncooperative route. More precisely, for each hop, we form a
collaborating cluster of M-1 nodes between the (hop) sender and the (hop)
destination. The transmitted message is encoded using a mother code and
partitioned into M blocks corresponding to the each of M slots. In the first
slot, the (hop) sender broadcasts its information by transmitting the first
block, and its helpers attempt to relay this message. In the remaining slots,
the each of left-over M-1 blocks is sent either through a helper which has
successfully decoded the message or directly by the (hop) sender where a
dynamic schedule is based on the ACK-based feedback from the cluster. By
employing powerful good codes (e.g., turbo codes, LDPC codes, and raptor codes)
whose performance is characterized by a threshold behavior, our approach
improves the reliability of a multi-hop routing through not only cooperation
diversity benefit but also a coding advantage. The study of the diversity and
the coding gain of the proposed scheme is based on a new simple threshold bound
on the frame-error rate (FER) of maximum likelihood decoding. A average FER
upper bound and its asymptotic (in large SNR) version are derived as a function
of the average fading channel SNRs and the code threshold."
"Gossip algorithms for aggregation have recently received significant
attention for sensor network applications because of their simplicity and
robustness in noisy and uncertain environments. However, gossip algorithms can
waste significant energy by essentially passing around redundant information
multiple times. For realistic sensor network model topologies like grids and
random geometric graphs, the inefficiency of gossip schemes is caused by slow
mixing times of random walks on those graphs. We propose and analyze an
alternative gossiping scheme that exploits geographic information. By utilizing
a simple resampling method, we can demonstrate substantial gains over
previously proposed gossip protocols. In particular, for random geometric
graphs, our algorithm computes the true average to accuracy $1/n^a$ using
$O(n^{1.5}\sqrt{\log n})$ radio transmissions, which reduces the energy
consumption by a $\sqrt{\frac{n}{\log n}}$ factor over standard gossip
algorithms."
"This paper is devoted to the finite-length analysis of turbo decoding over
the binary erasure channel (BEC). The performance of iterative
belief-propagation (BP) decoding of low-density parity-check (LDPC) codes over
the BEC can be characterized in terms of stopping sets. We describe turbo
decoding on the BEC which is simpler than turbo decoding on other channels. We
then adapt the concept of stopping sets to turbo decoding and state an exact
condition for decoding failure. Apply turbo decoding until the transmitted
codeword has been recovered, or the decoder fails to progress further. Then the
set of erased positions that will remain when the decoder stops is equal to the
unique maximum-size turbo stopping set which is also a subset of the set of
erased positions. Furthermore, we present some improvements of the basic turbo
decoding algorithm on the BEC. The proposed improved turbo decoding algorithm
has substantially better error performance as illustrated by the given
simulation results. Finally, we give an expression for the turbo stopping set
size enumerating function under the uniform interleaver assumption, and an
efficient enumeration algorithm of small-size turbo stopping sets for a
particular interleaver. The solution is based on the algorithm proposed by
Garello et al. in 2001 to compute an exhaustive list of all low-weight
codewords in a turbo code."
"This note has been withdrawn by the author as the more complete result was
recently proved by A.Quas and Y.Peres"
"Low-Density Parity-Check (LDPC) codes received much attention recently due to
their capacity-approaching performance. The iterative message-passing algorithm
is a widely adopted decoding algorithm for LDPC codes \cite{Kschischang01}. An
important design issue for LDPC codes is designing codes with fast decoding
speed while maintaining capacity-approaching performance. In another words, it
is desirable that the code can be successfully decoded in few number of
decoding iterations, at the same time, achieves a significant portion of the
channel capacity. Despite of its importance, this design issue received little
attention so far. In this paper, we address this design issue for the case of
binary erasure channel.
  We prove that density-efficient capacity-approaching LDPC codes satisfy a so
called ""flatness condition"". We show an asymptotic approximation to the number
of decoding iterations. Based on these facts, we propose an approximated
optimization approach to finding the codes with good decoding speed. We further
show that the optimal codes in the sense of decoding speed are
""right-concentrated"". That is, the degrees of check nodes concentrate around
the average right degree."
"We suggest a new approach to hypothesis testing for ergodic and stationary
processes. In contrast to standard methods, the suggested approach gives a
possibility to make tests, based on any lossless data compression method even
if the distribution law of the codeword lengths is not known. We apply this
approach to the following four problems: goodness-of-fit testing (or identity
testing), testing for independence, testing of serial independence and
homogeneity testing and suggest nonparametric statistical tests for these
problems. It is important to note that practically used so-called archivers can
be used for suggested testing."
"In his thesis, Wiberg showed the existence of thresholds for families of
regular low-density parity-check codes under min-sum algorithm decoding. He
also derived analytic bounds on these thresholds. In this paper, we formulate
similar results for linear programming decoding of regular low-density
parity-check codes."
"Whereas many results are known about thresholds for ensembles of low-density
parity-check codes under message-passing iterative decoding, this is not the
case for linear programming decoding. Towards closing this knowledge gap, this
paper presents some bounds on the thresholds of low-density parity-check code
ensembles under linear programming decoding."
"We consider linear-programming (LP) decoding of low-density parity-check
(LDPC) codes. While it is clear that one can use any general-purpose LP solver
to solve the LP that appears in the decoding problem, we argue in this paper
that the LP at hand is equipped with a lot of structure that one should take
advantage of. Towards this goal, we study the dual LP and show how
coordinate-ascent methods lead to very simple update rules that are tightly
connected to the min-sum algorithm. Moreover, replacing minima in the formula
of the dual LP with soft-minima one obtains update rules that are tightly
connected to the sum-product algorithm. This shows that LP solvers with
complexity similar to the min-sum algorithm and the sum-product algorithm are
feasible. Finally, we also discuss some sub-gradient-based methods."
"The feedback capacity of additive stationary Gaussian noise channels is
characterized as the solution to a variational problem. Toward this end, it is
proved that the optimal feedback coding scheme is stationary. When specialized
to the first-order autoregressive moving average noise spectrum, this
variational characterization yields a closed-form expression for the feedback
capacity. In particular, this result shows that the celebrated
Schalkwijk-Kailath coding scheme achieves the feedback capacity for the
first-order autoregressive moving average Gaussian channel, positively
answering a long-standing open problem studied by Butman, Schalkwijk-Tiernan,
Wolfowitz, Ozarow, Ordentlich, Yang-Kavcic-Tatikonda, and others. More
generally, it is shown that a k-dimensional generalization of the
Schalkwijk-Kailath coding scheme achieves the feedback capacity for any
autoregressive moving average noise spectrum of order k. Simply put, the
optimal transmitter iteratively refines the receiver's knowledge of the
intended message."
"Stopping sets, and in particular their numbers and sizes, play an important
role in determining the performance of iterative decoders of linear codes over
binary erasure channels. In the 2004 Shannon Lecture, McEliece presented an
expression for the number of stopping sets of size three for a full-rank
parity-check matrix of the Hamming code. In this correspondence, we derive an
expression for the number of stopping sets of any given size for the same
parity-check matrix."
"See cs.IT/0605135: R. Dabora, S. D. Servetto; On the Role of
Estimate-and-Forward with Time-Sharing in Cooperative Communications."
"Hoholdt, van Lint and Pellikaan used order functions to construct codes by
means of Linear Algebra and Semigroup Theory only. However, Geometric Goppa
codes that can be represented by this method are mainly those based on just one
point. In this paper we introduce the concept of near order function with the
aim of generalize this approach in such a way that a of wider family of
Geometric Goppa codes can be studied on a more elementary setting."
"We consider a multiple-input, multiple-output (MIMO) wideband Rayleigh block
fading channel where the channel state is unknown to both the transmitter and
the receiver and there is only an average power constraint on the input. We
compute the capacity and analyze its dependence on coherence length, number of
antennas and receive signal-to-noise ratio (SNR) per degree of freedom. We
establish conditions on the coherence length and number of antennas for the
non-coherent channel to have a ""near coherent"" performance in the wideband
regime. We also propose a signaling scheme that is near-capacity achieving in
this regime.
  We compute the error probability for this wideband non-coherent MIMO channel
and study its dependence on SNR, number of transmit and receive antennas and
coherence length. We show that error probability decays inversely with
coherence length and exponentially with the product of the number of transmit
and receive antennas. Moreover, channel outage dominates error probability in
the wideband regime. We also show that the critical as well as cut-off rates
are much smaller than channel capacity in this regime."
"We examine the issue of separation and code design for networks that operate
over finite fields. We demonstrate that source-channel (or source-network)
separation holds for several canonical network examples like the noisy multiple
access channel and the erasure degraded broadcast channel, when the whole
network operates over a common finite field. This robustness of separation is
predicated on the fact that noise and inputs are independent, and we examine
the failure of separation when noise is dependent on inputs in multiple access
channels.
  Our approach is based on the sufficiency of linear codes. Using a simple and
unifying framework, we not only re-establish with economy the optimality of
linear codes for single-transmitter, single-receiver channels and for
Slepian-Wolf source coding, but also establish the optimality of linear codes
for multiple access and for erasure degraded broadcast channels. The linearity
allows us to obtain simple optimal code constructions and to study capacity
regions of the noisy multiple access and the degraded broadcast channel. The
linearity of both source and network coding blurs the delineation between
source and network codes. While our results point to the fact that separation
of source coding and channel coding is optimal in some canonical networks, we
show that decomposing networks into canonical subnetworks may not be effective.
Thus, we argue that it may be the lack of decomposability of a network into
canonical network modules, rather than the lack of separation between source
and channel coding, that presents major challenges for coding over networks."
"In this paper, the second-order statistics of the instantaneous mutual
information are studied, in time-varying Rayleigh fading channels, assuming
general non-isotropic scattering environments. Specifically, first the
autocorrelation function, correlation coefficient, level crossing rate, and the
average outage duration of the instantaneous mutual information are
investigated in single-input single-output (SISO) systems. Closed-form exact
expressions are derived, as well as accurate approximations in low- and
high-SNR regimes. Then, the results are extended to multiple-input
single-output and single-input multiple-output systems, as well as
multiple-input multiple-output systems with orthogonal space-time block code
transmission. Monte Carlo simulations are provided to verify the accuracy of
the analytical results. The results shed more light on the dynamic behavior of
the instantaneous mutual information in mobile fading channels."
"This paper considers the quantization problem on the Grassmann manifold
\mathcal{G}_{n,p}, the set of all p-dimensional planes (through the origin) in
the n-dimensional Euclidean space. The chief result is a closed-form formula
for the volume of a metric ball in the Grassmann manifold when the radius is
sufficiently small. This volume formula holds for Grassmann manifolds with
arbitrary dimension n and p, while previous results pertained only to p=1, or a
fixed p with asymptotically large n. Based on this result, several quantization
bounds are derived for sphere packing and rate distortion tradeoff. We
establish asymptotically equivalent lower and upper bounds for the rate
distortion tradeoff. Since the upper bound is derived by constructing random
codes, this result implies that the random codes are asymptotically optimal.
The above results are also extended to the more general case, in which
\mathcal{G}_{n,q} is quantized through a code in \mathcal{G}_{n,p}, where p and
q are not necessarily the same. Finally, we discuss some applications of the
derived results to multi-antenna communication systems."
"It is well known that Multiple-Input Multiple-Output (MIMO) systems have high
spectral efficiency, especially when channel state information at the
transmitter (CSIT) is available. When CSIT is obtained by feedback, it is
practical to assume that the channel state feedback rate is finite and the CSIT
is not perfect. For such a system, we consider beamforming and power on/off
strategy for its simplicity and near optimality, where power on/off means that
a beamforming vector (beam) is either turned on with a constant power or turned
off. The main contribution of this paper is to accurately evaluate the
information rate as a function of the channel state feedback rate. Name a beam
turned on as an on-beam and the minimum number of the transmit and receive
antennas as the dimension of a MIMO system. We prove that the ratio of the
optimal number of on-beams and the system dimension converges to a constant for
a given signal-to-noise ratio (SNR) when the numbers of transmit and receive
antennas approach infinity simultaneously and when beamforming is perfect.
Asymptotic formulas are derived to evaluate this ratio and the corresponding
information rate per dimension. The asymptotic results can be accurately
applied to finite dimensional systems and suggest a power on/off strategy with
a constant number of on-beams. For this suboptimal strategy, we take a novel
approach to introduce power efficiency factor, which is a function of the
feedback rate, to quantify the effect of imperfect beamforming. By combining
power efficiency factor and the asymptotic formulas for perfect beamforming
case, the information rate of the power on/off strategy with a constant number
of on-beams is accurately characterized."
"This article considers the question of the teleportation protocol from an
engineering perspective. The protocol ideally requires an authority that
ensures that the two communicating parties have a perfectly entangled pair of
particles available to them. But this cannot be unconditionally established to
the satisfaction of the parties due to the fact that an unknown quantum state
cannot be copied. This supports the view that quantum information cannot be
treated on the same basis as classical information."
"In this paper, we consider a quasi-orthogonal (QO) space-time block code
(STBC) with minimum decoding complexity (MDC-QO-STBC). We formulate its
algebraic structure and propose a systematic method for its construction. We
show that a maximum-likelihood (ML) decoder for this MDC-QOSTBC, for any number
of transmit antennas, only requires the joint detection of two real symbols.
Assuming the use of a square or rectangular quadratic-amplitude modulation
(QAM) or multiple phase-shift keying (MPSK) modulation for this MDC-QOSTBC, we
also obtain the optimum constellation rotation angle, in order to achieve full
diversity and optimum coding gain. We show that the maximum achievable code
rate of these MDC-QOSTBC is 1 for three and four antennas and 3/4 for five to
eight antennas. We also show that the proposed MDC-QOSTBC has several desirable
properties, such as a more even power distribution among antennas and better
scalability in adjusting the number of transmit antennas, compared with the
coordinate interleaved orthogonal design (CIOD) and asymmetric CIOD (ACIOD)
codes. For the case of an odd number of transmit antennas, MDC-QO-STBC also has
better decoding performance than CIOD."
"This paper considers the problem of guessing the realization of a finite
alphabet source when some side information is provided. The only knowledge the
guesser has about the source and the correlated side information is that the
joint source is one among a family. A notion of redundancy is first defined and
a new divergence quantity that measures this redundancy is identified. This
divergence quantity shares the Pythagorean property with the Kullback-Leibler
divergence. Good guessing strategies that minimize the supremum redundancy
(over the family) are then identified. The min-sup value measures the richness
of the uncertainty set. The min-sup redundancies for two examples - the
families of discrete memoryless sources and finite-state arbitrarily varying
sources - are then determined."
"Multiple transmit antennas in a downlink channel can provide tremendous
capacity (i.e. multiplexing) gains, even when receivers have only single
antennas. However, receiver and transmitter channel state information is
generally required. In this paper, a system where each receiver has perfect
channel knowledge, but the transmitter only receives quantized information
regarding the channel instantiation is analyzed. The well known zero forcing
transmission technique is considered, and simple expressions for the throughput
degradation due to finite rate feedback are derived. A key finding is that the
feedback rate per mobile must be increased linearly with the SNR (in dB) in
order to achieve the full multiplexing gain, which is in sharp contrast to
point-to-point MIMO systems in which it is not necessary to increase the
feedback rate as a function of the SNR."
"A multiple antenna broadcast channel with perfect channel state information
at the receivers is considered. If each receiver quantizes its channel
knowledge to a finite number of bits which are fed back to the transmitter, the
large capacity benefits of the downlink channel can be realized. However, the
required number of feedback bits per mobile must be scaled with both the number
of transmit antennas and the system SNR, and thus can be quite large in even
moderately sized systems. It is shown that a small number of antennas can be
used at each receiver to improve the quality of the channel estimate provided
to the transmitter. As a result, the required feedback rate per mobile can be
significantly decreased."
"Universal compression of patterns of sequences generated by independently
identically distributed (i.i.d.) sources with unknown, possibly large,
alphabets is investigated. A pattern is a sequence of indices that contains all
consecutive indices in increasing order of first occurrence. If the alphabet of
a source that generated a sequence is unknown, the inevitable cost of coding
the unknown alphabet symbols can be exploited to create the pattern of the
sequence. This pattern can in turn be compressed by itself. It is shown that if
the alphabet size $k$ is essentially small, then the average minimax and
maximin redundancies as well as the redundancy of every code for almost every
source, when compressing a pattern, consist of at least 0.5 log(n/k^3) bits per
each unknown probability parameter, and if all alphabet letters are likely to
occur, there exist codes whose redundancy is at most 0.5 log(n/k^2) bits per
each unknown probability parameter, where n is the length of the data
sequences. Otherwise, if the alphabet is large, these redundancies are
essentially at least O(n^{-2/3}) bits per symbol, and there exist codes that
achieve redundancy of essentially O(n^{-1/2}) bits per symbol. Two sub-optimal
low-complexity sequential algorithms for compression of patterns are presented
and their description lengths analyzed, also pointing out that the pattern
average universal description length can decrease below the underlying i.i.d.\
entropy for large enough alphabets."
"A simple feedback control algorithm is presented for distributed beamforming
in a wireless network. A network of wireless sensors that seek to cooperatively
transmit a common message signal to a Base Station (BS) is considered. In this
case, it is well-known that substantial energy efficiencies are possible by
using distributed beamforming. The feedback algorithm is shown to achieve the
carrier phase coherence required for beamforming in a scalable and distributed
manner. In the proposed algorithm, each sensor independently makes a random
adjustment to its carrier phase. Assuming that the BS is able to broadcast one
bit of feedback each timeslot about the change in received signal to noise
ratio (SNR), the sensors are able to keep the favorable phase adjustments and
discard the unfavorable ones, asymptotically achieving perfect phase coherence.
A novel analytical model is derived that accurately predicts the convergence
rate. The analytical model is used to optimize the algorithm for fast
convergence and to establish the scalability of the algorithm."
"We derive the maximum entropy of a flow (information utility) which conforms
to traffic constraints imposed by a generalized token bucket regulator, by
taking into account the covert information present in the randomness of packet
lengths. Under equality constraints of aggregate tokens and aggregate bucket
depth, a generalized token bucket regulator can achieve higher information
utility than a standard token bucket regulator. The optimal generalized token
bucket regulator has a near-uniform bucket depth sequence and a decreasing
token increment sequence."
"We determine the capacity-achieving input covariance matrices for coherent
block-fading correlated MIMO Rician channels. In contrast with the Rayleigh and
uncorrelated Rician cases, no closed-form expressions for the eigenvectors of
the optimum input covariance matrix are available. Both the eigenvectors and
eigenvalues have to be evaluated by using numerical techniques. As the
corresponding optimization algorithms are not very attractive, we evaluate the
limit of the average mutual information when the number of transmit and receive
antennas converge to infinity at the same rate. If the channel is
semi-correlated, we propose an attractive optimization algorithm of the large
system approximant, and establish some convergence results. Simulation results
show that our approach provide reliable results even for a quite moderate
number of transmit and receive antennas."
"Conventional turbo codes (CTCs) usually employ a block-oriented interleaving
so that each block is separately encoded and decoded. As interleaving and
de-interleaving are performed within a block, the message-passing process
associated with an iterative decoder is limited to proceed within the
corresponding range. This paper presents a new turbo coding scheme that uses a
special interleaver structure and a multiple-round early termination test
involving both sign check and a CRC code. The new interleaver structure is
naturally suited for high speed parallel processing and the resulting coding
system offers new design options and tradeoffs that are not available to CTCs.
In particular, it becomes possible for the decoder to employ an efficient
inter-block collaborative decoding algorithm, passing the information obtained
from termination test proved blocks to other unproved blocks. It also becomes
important to have a proper decoding schedule. The combined effect is improved
performance and reduction in the average decoding delay (whence the required
computing power). A memory (storage) management mechanism is included as a
critical part of the decoder so as to provide additional design tradeoff
between performance and memory size. It is shown that the latter has a
modular-like effect in that additional memory units render enhanced performance
due not only to less forced early terminations but to possible increases of the
interleaving depth. Depending on the decoding schedule, the degree of
parallelism and other decoding resources available, the proposed scheme admits
a variety of decoder architectures that meet a large range of throughput and
performance demands."
"In multiple-input multiple-output (MIMO) fading channels maximum likelihood
(ML) detection is desirable to achieve high performance, but its complexity
grows exponentially with the spectral efficiency. The current state of the art
in MIMO detection is list decoding and lattice decoding. This paper proposes a
new class of lattice detectors that combines some of the principles of both
list and lattice decoding, thus resulting in an efficient parallelizable
implementation and near optimal soft-ouput ML performance. The novel detector
is called layered orthogonal lattice detector (LORD), because it adopts a new
lattice formulation and relies on a channel orthogonalization process. It
should be noted that the algorithm achieves optimal hard-output ML performance
in case of two transmit antennas. For two transmit antennas max-log bit
soft-output information can be generated and for greater than two antennas
approximate max-log detection is achieved. Simulation results show that LORD,
in MIMO system employing orthogonal frequency division multiplexing (OFDM) and
bit interleaved coded modulation (BICM) is able to achieve very high
signal-to-noise ratio (SNR) gains compared to practical soft-output detectors
such as minimum-mean square error (MMSE), in either linear or nonlinear
iterative scheme. Besides, the performance comparison with hard-output decoded
algebraic space time codes shows the fundamental importance of soft-output
generation capability for practical wireless applications."
"We study conditions on $f$ under which an $f$-divergence $D_f$ will satisfy
$D_f \geq c_f V^2$ or $D_f \geq c_{2,f} V^2 + c_{4,f} V^4$, where $V$ denotes
variational distance and the coefficients $c_f$, $c_{2,f}$ and $c_{4,f}$ are
{\em best possible}. As a consequence, we obtain lower bounds in terms of $V$
for many well known distance and divergence measures. For instance, let
$D_{(\alpha)} (P,Q) = [\alpha (\alpha-1)]^{-1} [\int q^{\alpha} p^{1-\alpha} d
\mu -1]$ and ${\cal I}_\alpha (P,Q) = (\alpha -1)^{-1} \log [\int p^\alpha
q^{1-\alpha} d \mu]$ be respectively the {\em relative information of type}
($1-\alpha$) and {\em R\'{e}nyi's information gain of order} $\alpha$. We show
that $D_{(\alpha)} \geq {1/2} V^2 + {1/72} (\alpha+1)(2-\alpha) V^4$ whenever
$-1 \leq \alpha \leq 2$, $\alpha \not= 0,1$ and that ${\cal I}_{\alpha} =
\frac{\alpha}{2} V^2 + {1/36} \alpha (1 + 5 \alpha - 5 \alpha^2) V^4$ for $0 <
\alpha < 1$. Pinsker's inequality $D \geq {1/2}
  V^2$ and its extension $D \geq {1/2} V^2 + {1/36} V^4$ are special cases of
each one of these."
"The concept of a fiber aided wireless network architecture (FAWNA) is
introduced in [Ray et al., Allerton Conference 2005], which allows high-speed
mobile connectivity by leveraging the speed of optical networks. In this paper,
we consider a single-input, multiple-output (SIMO) FAWNA, which consists of a
SIMO wireless channel and an optical fiber channel, connected through
wireless-optical interfaces. We propose a scheme where the received wireless
signal at each interface is quantized and sent over the fiber. Though our
architecture is similar to that of the classical CEO problem, our problem is
different from it. We show that the capacity of our scheme approaches the
capacity of the architecture, exponentially with fiber capacity. We also show
that for a given fiber capacity, there is an optimal operating wireless
bandwidth and an optimal number of wireless-optical interfaces. The
wireless-optical interfaces of our scheme have low complexity and do not
require knowledge of the transmitter code book. They are also extendable to
FAWNAs with large number of transmitters and interfaces and, offer adaptability
to variable rates, changing channel conditions and node positions."
"In this paper we analyze the interference channel as a conflict situation.
This viewpoint implies that certain points in the rate region are unreasonable
to one of the players. Therefore these points cannot be considered achievable
based on game theoretic considerations. We then propose to use Nash bargaining
solution as a tool that provides preferred points on the boundary of the game
theoretic rate region. We provide analysis for the 2x2 intereference channel
using the FDM achievable rate region. We also outline how to generalize our
results to other achievable rate regions for the interference channel as well
as the multiple access channel.
  Keywords: Spectrum optimization, distributed coordination, game theory,
interference channel, multiple access channel."
"This submission is being withdrawn due to serious errors in the achievability
proofs. The reviewers of the journal I had submitted to had found errors back
in 2006. I had forgotten about this paper until I saw the CFP for a JSAC issue
on in-network computation.
http://www.jsac.ucsd.edu/Calls/in-networkcomputationcfp.pdf."
"In a slow fading channel, how to find a cooperative diversity scheme that
achieves the transmit diversity bound is still an open problem. In fact, all
previously proposed amplify-and-forward (AF) and decode-and-forward (DF)
schemes do not improve with the number of relays in terms of the diversity
multiplexing tradeoff (DMT) for multiplexing gains r higher than 0.5. In this
work, we study the class of slotted amplify-and-forward (SAF) schemes. We first
establish an upper bound on the DMT for any SAF scheme with an arbitrary number
of relays N and number of slots M. Then, we propose a sequential SAF scheme
that can exploit the potential diversity gain in the high multiplexing gain
regime. More precisely, in certain conditions, the sequential SAF scheme
achieves the proposed DMT upper bound which tends to the transmit diversity
bound when M goes to infinity. In particular, for the two-relay case, the
three-slot sequential SAF scheme achieves the proposed upper bound and
outperforms the two-relay non-orthorgonal amplify-and-forward (NAF) scheme of
Azarian et al. for multiplexing gains r < 2/3. Numerical results reveal a
significant gain of our scheme over the previously proposed AF schemes,
especially in high spectral efficiency and large network size regime."
"It is well known that the presence of double scattering degrades the
performance of a MIMO channel, in terms of both the multiplexing gain and the
diversity gain. In this paper, a closed-form expression of the
diversity-multiplexing tradeoff (DMT) of double scattering MIMO channels is
obtained. It is shown that, for a channel with nT transmit antennas, nR receive
antennas and nS scatterers, the DMT only depends on the ordered version of the
triple (nT,nS,nR), for arbitrary nT, nS and nR. The condition under which the
double scattering channel has the same DMT as the single scattering channel is
also established."
"Golay sequences are well suited for the use as codewords in orthogonal
frequency-division multiplexing (OFDM), since their peak-to-mean envelope power
ratio (PMEPR) in q-ary phase-shift keying (PSK) modulation is at most 2. It is
known that a family of polyphase Golay sequences of length 2^m organizes in
m!/2 cosets of a q-ary generalization of the first-order Reed-Muller code,
RM_q(1,m). In this paper a more general construction technique for cosets of
RM_q(1,m) with low PMEPR is established. These cosets contain so-called
near-complementary sequences. The application of this theory is then
illustrated by providing some construction examples. First, it is shown that
the m!/2 cosets of RM_q(1,m) comprised of Golay sequences just arise as a
special case. Second, further families of cosets of RM_q(1,m) with maximum
PMEPR between 2 and 4 are presented, showing that some previously unexplained
phenomena can now be understood within a unified framework. A lower bound on
the PMEPR of cosets of RM_q(1,m) is proved as well, and it is demonstrated that
the upper bound on the PMEPR is tight in many cases. Finally it is shown that
all upper bounds on the PMEPR of cosets of RM_q(1,m) also hold for the
peak-to-average power ratio (PAPR) under the Walsh-Hadamard transform."
"This paper presents two methods for approximating the performance of coded
multicarrier systems operating over frequency-selective, quasi-static fading
channels with non-ideal interleaving. The first method is based on
approximating the performance of the system over each realization of the
channel, and is suitable for obtaining the outage performance of this type of
system. The second method is based on knowledge of the correlation matrix of
the frequency-domain channel gains and can be used to directly obtain the
average performance. Both of the methods are applicable for
convolutionally-coded interleaved systems employing Quadrature Amplitude
Modulation (QAM). As examples, both methods are used to study the performance
of the Multiband Orthogonal Frequency Division Multiplexing (OFDM) proposal for
high data-rate Ultra-Wideband (UWB) communication."
"In this first part, a computable outer bound is proved for the multiterminal
source coding problem, for a setup with two encoders, discrete memoryless
sources, and bounded distortion measures."
"This paper is concerned with the scaling of the number of hops in a large
scale wireless ad-hoc network (WANET), a quantity we call network latency. A
large network latency affects all aspects of data communication in a WANET,
including an increase in delay, packet loss, required processing power and
memory. We consider network management and data routing challenges in WANETs
with scalable network latency. On the physical side, reducing network latency
imposes a significantly higher power and bandwidth demand on nodes, as is
reflected in a set of new bounds. On the protocol front, designing distributed
routing protocols that can guarantee the delivery of data packets within
scalable number of hops is a challenging task. To solve this, we introduce
multi-resolution randomized hierarchy (MRRH), a novel power and bandwidth
efficient WANET protocol with scalable network latency. MRRH uses a randomized
algorithm for building and maintaining a random hierarchical network topology,
which together with the proposed routing algorithm can guarantee efficient
delivery of data packets in the wireless network. For a network of size $N$,
MRRH can provide an average latency of only $O(\log^{3} N)$. The power and
bandwidth consumption of MRRH are shown to be \emph{nearly} optimal for the
latency it provides. Therefore, MRRH, is a provably efficient candidate for
truly large scale wireless ad-hoc networking."
"We prove a new extremal inequality, motivated by the vector Gaussian
broadcast channel and the distributed source coding with a single quadratic
distortion constraint problems. As a corollary, this inequality yields a
generalization of the classical entropy-power inequality (EPI). As another
corollary, this inequality sheds insight into maximizing the differential
entropy of the sum of two dependent random variables."
"Two new proofs of the Fisher information inequality (FII) using data
processing inequalities for mutual information and conditional variance are
presented."
"The predominate traffic patterns in a wireless sensor network are many-to-one
and one-to-many communication. Hence, the performance of wireless sensor
networks is characterized by the rate at which data can be disseminated from or
aggregated to a data sink. In this paper, we consider the data aggregation
problem. We demonstrate that a data aggregation rate of O(log(n)/n) is optimal
and that this rate can be achieved in wireless sensor networks using a
generalization of cooperative beamforming called cooperative time-reversal
communication."
"We consider the problem of adaptive modulation for wideband DS-CDMA Rayleigh
fading channels with imperfect channel state information (CSI). We assume a
multidimensional signal subspace spanned by a collection of random spreading
codes (multicoding) and study the effects of both the subspace dimension and
the probability distribution of the transmitted symbols on the mutual
information between the channel input and output in the presence of uncertainty
regarding the true state of the channel. We develop approximations for the
mutual information as well as both upper and lower bounds on the mutual
information that are stated explicitly in terms of the dimension of the signal
constellation, the number of resolvable fading paths on the channel, the
current estimate of channel state, and the mean-squared-error of the channel
estimate. We analyze these approximations and bounds in order to quantify the
impact of signal dimension and symbol distribution on system performance."
"The capacity of non-coherent stationary Gaussian fading channels with memory
under a peak-power constraint is studied in the asymptotic weak-signal regime.
It is assumed that the fading law is known to both transmitter and receiver but
that neither is cognizant of the fading realization. A connection is
demonstrated between the asymptotic behavior of channel capacity in this regime
and the asymptotic behavior of the prediction error incurred in predicting the
fading process from very noisy observations of its past. This connection can be
viewed as the low signal-to-noise ratio (SNR) analog of recent results by
Lapidoth & Moser and by Lapidoth demonstrating connections between the high SNR
capacity growth and the noiseless or almost-noiseless prediction error. We
distinguish between two families of fading laws: the ``slowly forgetting'' and
the ``quickly forgetting''. For channels in the former category the low SNR
capacity is achieved by IID inputs, whereas in the latter such inputs are
typically sub-optimal. Instead, the asymptotic capacity can be approached by
inputs with IID phase but block-constant magnitude."
"In this paper, we study two important metrics in multiple-input
multiple-output (MIMO) time-varying Rayleigh flat fading channels. One is the
eigen-mode, and the other is the instantaneous mutual information (IMI). Their
second-order statistics, such as the correlation coefficient, level crossing
rate (LCR), and average fade/outage duration, are investigated, assuming a
general nonisotropic scattering environment. Exact closed-form expressions are
derived and Monte Carlo simulations are provided to verify the accuracy of the
analytical results. For the eigen-modes, we found they tend to be
spatio-temporally uncorrelated in large MIMO systems. For the IMI, the results
show that its correlation coefficient can be well approximated by the squared
amplitude of the correlation coefficient of the channel, under certain
conditions. Moreover, we also found the LCR of IMI is much more sensitive to
the scattering environment than that of each eigen-mode."
"We investigate the optimal performance of dense sensor networks by studying
the joint source-channel coding problem. The overall goal of the sensor network
is to take measurements from an underlying random process, code and transmit
those measurement samples to a collector node in a cooperative multiple access
channel with feedback, and reconstruct the entire random process at the
collector node. We provide lower and upper bounds for the minimum achievable
expected distortion when the underlying random process is stationary and
Gaussian. In the case where the random process is also Markovian, we evaluate
the lower and upper bounds explicitly and show that they are of the same order
for a wide range of sum power constraints. Thus, for a Gauss-Markov random
process, under these sum power constraints, we determine the achievability
scheme that is order-optimal, and express the minimum achievable expected
distortion as a function of the sum power constraint."
"Flat-fading channels that are correlated in time are considered under peak
and average power constraints. For discrete-time channels, a new upper bound on
the capacity per unit time is derived. A low SNR analysis of a full-scattering
vector channel is used to derive a complimentary lower bound. Together, these
bounds allow us to identify the exact scaling of channel capacity for a fixed
peak to average ratio, as the average power converges to zero. The upper bound
is also asymptotically tight as the average power converges to zero for a fixed
peak power.
  For a continuous time infinite bandwidth channel, Viterbi identified the
capacity for M-FSK modulation. Recently, Zhang and Laneman showed that the
capacity can be achieved with non-bursty signaling (QPSK). An additional
contribution of this paper is to obtain similar results under peak and average
power constraints."
"In this note, a new method for deriving the volume of hypersphere is proposed
by using probability theory. The explicit expression of the multiple times
convolution of the probability density functions we should use is very
complicated. But in here, we don't need its whole explicit expression. We just
need the only a part of information and this fact make it possible to derive
the general expression of the voulume of hypersphere. We also comments about
the paradox in the hypersphere which was introduced by R.W.Hamming."
"This paper introduces a new class of efficient inter connection networks
called as M-graphs for large multi-processor systems.The concept of M-matrix
and M-graph is an extension of Mn-matrices and Mn-graphs.We analyze these
M-graphs regarding their suitability for large multi-processor systems. An(p,N)
M-graph consists of N nodes, where p is the degree of each node.The topology is
found to be having many attractive features prominent among them is the
capability of maximal fault-tolerance, high density and constant diameter.It is
found that these combinatorial structures exibit some properties like
symmetry,and an inter-relation with the nodes, and degree of the concerned
graph, which can be utilized for the purposes of inter connected networks.But
many of the properties of these mathematical and graphical structures still
remained unexplored and the present aim of the paper is to study and analyze
some of the properties of these M-graphs and explore their application in
networks and multi-processor systems."
"In this paper, we present a concatenated coding scheme for a high rate
$2\times 2$ multiple-input multiple-output (MIMO) system over slow fading
channels. The inner code is the Golden code \cite{Golden05} and the outer code
is a trellis code. Set partitioning of the Golden code is designed specifically
to increase the minimum determinant. The branches of the outer trellis code are
labeled with these partitions. Viterbi algorithm is applied for trellis
decoding. In order to compute the branch metrics a lattice sphere decoder is
used. The general framework for code optimization is given. The performance of
the proposed concatenated scheme is evaluated by simulation. It is shown that
the proposed scheme achieves significant performance gains over uncoded Golden
code."
"Motivated by applications of rateless coding, decision feedback, and ARQ, we
study the problem of universal decoding for unknown channels, in the presence
of an erasure option. Specifically, we harness the competitive minimax
methodology developed in earlier studies, in order to derive a universal
version of Forney's classical erasure/list decoder, which in the erasure case,
optimally trades off between the probability of erasure and the probability of
undetected error. The proposed universal erasure decoder guarantees universal
achievability of a certain fraction $\xi$ of the optimum error exponents of
these probabilities (in a sense to be made precise in the sequel). A
single--letter expression for $\xi$, which depends solely on the coding rate
and the threshold, is provided. The example of the binary symmetric channel is
studied in full detail, and some conclusions are drawn."
"Rodrigo de Miguel et al 2007 J. Phys. A: Math. Theor. 40 5241-5260: A noisy
vector channel operating under a strict complexity constraint at the receiver
is introduced. According to this constraint, detected bits, obtained by
performing hard decisions directly on the channel's matched filter output, must
be the same as the transmitted binary inputs. An asymptotic analysis is carried
out using mathematical tools imported from the study of neural networks, and it
is shown that, under a bounded noise assumption, such complexity-constrained
channel exhibits a non-trivial Shannon-theoretic capacity. It is found that
performance relies on rigorous interference-based multiuser cooperation at the
transmitter and that this cooperation is best served when all transmitters use
the same amplitude."
"We introduce a distributed source coding scheme called successive Wyner-Ziv
coding. We show that any point in the rate region of the quadratic Gaussian CEO
problem can be achieved via the successive Wyner-Ziv coding. The concept of
successive refinement in the single source coding is generalized to the
distributed source coding scenario, which we refer to as distributed successive
refinement. For the quadratic Gaussian CEO problem, we establish a necessary
and sufficient condition for distributed successive refinement, where the
successive Wyner-Ziv coding scheme plays an important role."
"A game-theoretic model is proposed to study the cross-layer problem of joint
power and rate control with quality of service (QoS) constraints in
multiple-access networks. In the proposed game, each user seeks to choose its
transmit power and rate in a distributed manner in order to maximize its own
utility and at the same time satisfy its QoS requirements. The user's QoS
constraints are specified in terms of the average source rate and average
delay. The utility function considered here measures energy efficiency and the
delay includes both transmission and queueing delays. The Nash equilibrium
solution for the proposed non-cooperative game is derived and a closed-form
expression for the utility achieved at equilibrium is obtained. It is shown
that the QoS requirements of a user translate into a ""size"" for the user which
is an indication of the amount of network resources consumed by the user. Using
this framework, the tradeoffs among throughput, delay, network capacity and
energy efficiency are also studied."
"A $K$-user pseudo-orthogonal (PO) randomly spread CDMA system, equivalent to
transmission over a subset of $K'\leq K$ single-user Gaussian channels, is
introduced. The high signal-to-noise ratio performance of the PO-CDMA is
analyzed by rigorously deriving its asymptotic multiuser efficiency (AME) in
the large system limit. Interestingly, the $K'$-optimized PO-CDMA transceiver
scheme yields an AME which is practically equal to 1 for system loads smaller
than 0.1 and lower bounded by 1/4 for increasing loads. As opposed to the
vanishing efficiency of linear multiuser detectors, the derived efficiency is
comparable to the ultimate CDMA efficiency achieved for the intractable optimal
multiuser detector."
"This note considers Kak's observer-reference model of quantum information,
where it is shown that qubits carry information that is sqrt n / ln n times
classical information, where n is the number of components in the measurement
system, to analyze information processing in quantum gates. The obverse side of
this exponential nature of quantum information is that the computational
complexity of implementing unconditionally reliable quantum gates is also
exponential."
"We consider a distributed source coding system in which several observations
are communicated to the decoder using limited transmission rate. The
observations must be separately coded. We introduce a robust distributed coding
scheme which flexibly trades off between system robustness and compression
efficiency. The optimality of this coding scheme is proved for various special
cases."
"The problem of choosing the optimal multipath components to be employed at a
minimum mean square error (MMSE) selective Rake receiver is considered for an
impulse radio ultra-wideband system. First, the optimal finger selection
problem is formulated as an integer programming problem with a non-convex
objective function. Then, the objective function is approximated by a convex
function and the integer programming problem is solved by means of constraint
relaxation techniques. The proposed algorithms are suboptimal due to the
approximate objective function and the constraint relaxation steps. However,
they perform better than the conventional finger selection algorithm, which is
suboptimal since it ignores the correlation between multipath components, and
they can get quite close to the optimal scheme that cannot be implemented in
practice due to its complexity. In addition to the convex relaxation
techniques, a genetic algorithm (GA) based approach is proposed, which does not
need any approximations or integer relaxations. This iterative algorithm is
based on the direct evaluation of the objective function, and can achieve
near-optimal performance with a reasonable number of iterations. Simulation
results are presented to compare the performance of the proposed finger
selection algorithms with that of the conventional and the optimal schemes."
"In this paper, we introduce the notion of perfect space-time block codes
(STBC). These codes have full rate, full diversity, non-vanishing constant
minimum determinant for increasing spectral efficiency, uniform average
transmitted energy per antenna and good shaping. We present algebraic
constructions of perfect STBCs for 2, 3, 4 and 6 antennas."
"In this paper, we investigate achievable rates on the multiple access channel
with feedback and correlated sources (MACFCS). The motivation for studying the
MACFCS stems from the fact that in a sensor network, sensors collect and
transmit correlated data to a common sink. We derive two achievable rate
regions for the three-node MACFCS."
"We investigate the achievable rate of data transmission from sources to sinks
through a multiple-relay network. We study achievable rates for omniscient
coding, in which all nodes are considered in the coding design at each node. We
find that, when maximizing the achievable rate, not all nodes need to
``cooperate'' with all other nodes in terms of coding and decoding. This leads
us to suggest a constrained network, whereby each node only considers a few
neighboring nodes during encoding and decoding. We term this myopic coding and
calculate achievable rates for myopic coding. We show by examples that, when
nodes transmit at low SNR, these rates are close to that achievable by
omniscient coding, when the network is unconstrained . This suggests that a
myopic view of the network might be as good as a global view. In addition,
myopic coding has the practical advantage of being more robust to topology
changes. It also mitigates the high computational complexity and large
buffer/memory requirements of omniscient coding schemes."
"We address the problem of delay in an arithmetic coding system. Due to the
nature of the arithmetic coding process, source sequences causing arbitrarily
large encoding or decoding delays exist. This phenomena raises the question of
just how large is the expected input to output delay in these systems, i.e.,
once a source sequence has been encoded, what is the expected number of source
letters that should be further encoded to allow full decoding of that sequence.
In this paper, we derive several new upper bounds on the expected delay for a
memoryless source, which improve upon a known bound due to Gallager. The bounds
provided are uniform in the sense of being independent of the sequence's
history. In addition, we give a sufficient condition for a source to admit a
bounded expected delay, which holds for a stationary ergodic Markov source of
any order."
"Cognitive radios have been proposed as a means to implement efficient reuse
of the licensed spectrum. The key feature of a cognitive radio is its ability
to recognize the primary (licensed) user and adapt its communication strategy
to minimize the interference that it generates. We consider a communication
scenario in which the primary and the cognitive user wish to communicate to
different receivers, subject to mutual interference. Modeling the cognitive
radio as a transmitter with side-information about the primary transmission, we
characterize the largest rate at which the cognitive radio can reliably
communicate under the constraint that (i) no interference is created for the
primary user, and (ii) the primary encoder-decoder pair is oblivious to the
presence of the cognitive radio."
"We provide a complete characterization of the rate-distortion region for the
multistage successive refinement of the Wyner-Ziv source coding problem with
degraded side informations at the decoder. Necessary and sufficient conditions
for a source to be successively refinable along a distortion vector are
subsequently derived. A source-channel separation theorem is provided when the
descriptions are sent over independent channels for the multistage case.
Furthermore, we introduce the notion of generalized successive refinability
with multiple degraded side informations. This notion captures whether
progressive encoding to satisfy multiple distortion constraints for different
side informations is as good as encoding without progressive requirement.
Necessary and sufficient conditions for generalized successive refinability are
given. It is shown that the following two sources are generalized successively
refinable: (1) the Gaussian source with degraded Gaussian side informations,
(2) the doubly symmetric binary source when the worse side information is a
constant. Thus for both cases, the failure of being successively refinable is
only due to the inherent uncertainty on which side information will occur at
the decoder, but not the progressive encoding requirement."
"A multiple-access channel is considered in which messages from one encoder
are confidential. Confidential messages are to be transmitted with perfect
secrecy, as measured by equivocation at the other encoder. The upper bounds and
the achievable rates for this communication situation are determined."
"An information-spectrum approach is applied to solve the multiterminal source
coding problem for correlated general sources, where sources may be
nonstationary and/or nonergodic, and the distortion measure is arbitrary and
may be nonadditive. A general formula for the rate-distortion region of the
multiterminal source coding problem with the maximum distortion criterion under
fixed-length coding is shown in this correspondence."
"Motivated by the problem of reducing the peak to average power ratio (PAPR)
of transmitted signals, we consider a design of complementary set matrices
whose column sequences satisfy a correlation constraint. The design algorithm
recursively builds a collection of $2^{t+1}$ mutually orthogonal (MO)
complementary set matrices starting from a companion pair of sequences. We
relate correlation properties of column sequences to that of the companion pair
and illustrate how to select an appropriate companion pair to ensure that a
given column correlation constraint is satisfied. For $t=0$, companion pair
properties directly determine matrix column correlation properties. For $t\geq
1$, reducing correlation merits of the companion pair may lead to improved
column correlation properties. However, further decrease of the maximum
out-off-phase aperiodic autocorrelation of column sequences is not possible
once the companion pair correlation merit is less than a threshold determined
by $t$. We also reveal a design of the companion pair which leads to
complementary set matrices with Golay column sequences. Exhaustive search for
companion pairs satisfying a column correlation constraint is infeasible for
medium and long sequences. We instead search for two shorter length sequences
by minimizing a cost function in terms of their autocorrelation and
crosscorrelation merits. Furthermore, an improved cost function which helps in
reducing the maximum out-off-phase column correlation is derived based on the
properties of the companion pair. By exploiting the well-known Welch bound,
sufficient conditions for the existence of companion pairs which satisfy a set
of column correlation constraints are also given."
"A discrete memoryless generalized multiple access channel (GMAC) with
confidential messages is studied, where two users attempt to transmit common
information to a destination and each user also has private (confidential)
information intended for the destination. The two users are allowed to receive
channel outputs, and hence may obtain the confidential information sent by each
other from channel outputs they receive. However, each user views the other
user as a wire-tapper, and wishes to keep its confidential information as
secret as possible from the other user. The level of secrecy of the
confidential information is measured by the equivocation rate, i.e., the
entropy rate of the confidential information conditioned on channel outputs at
the wire-tapper. The performance measure of interest for the GMAC with
confidential messages is the rate-equivocation tuple that includes the common
rate, two private rates and two equivocation rates as components. The set that
includes all these achievable rate-equivocation tuples is referred to as the
capacity-equivocation region. The GMAC with one confidential message set is
first studied, where only one user (user 1) has private (confidential)
information for the destination. Inner and outer bounds on the
capacity-equivocation region are derived, and the capacity-equivocation are
established for some classes of channels including the Gaussian GMAC.
Furthermore, the secrecy capacity region is established, which is the set of
all achievable rates with user 2 being perfectly ignorant of confidential
messages of user 1. For the GMAC with two confidential message sets, where both
users have confidential messages for the destination, an inner bound on the
capacity-equivocation region is obtained."
"The capacity regions are investigated for two relay broadcast channels
(RBCs), where relay links are incorporated into standard two-user broadcast
channels to support user cooperation. In the first channel, the Partially
Cooperative Relay Broadcast Channel, only one user in the system can act as a
relay and transmit to the other user through a relay link. An achievable rate
region is derived based on the relay using the decode-and-forward scheme. An
outer bound on the capacity region is derived and is shown to be tighter than
the cut-set bound. For the special case where the Partially Cooperative RBC is
degraded, the achievable rate region is shown to be tight and provides the
capacity region. Gaussian Partially Cooperative RBCs and Partially Cooperative
RBCs with feedback are further studied. In the second channel model being
studied in the paper, the Fully Cooperative Relay Broadcast Channel, both users
can act as relay nodes and transmit to each other through relay links. This is
a more general model than the Partially Cooperative RBC. All the results for
Partially Cooperative RBCs are correspondingly generalized to the Fully
Cooperative RBCs. It is further shown that the AWGN Fully Cooperative RBC has a
larger achievable rate region than the AWGN Partially Cooperative RBC. The
results illustrate that relaying and user cooperation are powerful techniques
in improving the capacity of broadcast channels."
"We consider the multiple-access communication problem in a distributed
setting for both the additive white Gaussian noise channel and the discrete
memoryless channel. We propose a scheme called Distributed Rate Splitting to
achieve the optimal rates allowed by information theory in a distributed
manner. In this scheme, each real user creates a number of virtual users via a
power/rate splitting mechanism in the M-user Gaussian channel or via a random
switching mechanism in the M-user discrete memoryless channel. At the receiver,
all virtual users are successively decoded. Compared with other multiple-access
techniques, Distributed Rate Splitting can be implemented with lower complexity
and less coordination. Furthermore, in a symmetric setting, we show that the
rate tuple achieved by this scheme converges to the maximum equal rate point
allowed by the information-theoretic bound as the number of virtual users per
real user tends to infinity. When the capacity regions are asymmetric, we show
that a point on the dominant face can be achieved asymptotically. Finally, when
there is an unequal number of virtual users per real user, we show that
differential user rate requirements can be accommodated in a distributed
fashion."
"The problem of finding the shortest linear shift-register capable of
generating t finite length sequences over some field F is considered. A similar
problem was already addressed by Feng and Tzeng. They presented an iterative
algorithm for solving this multi-sequence shift-register synthesis problem,
which can be considered as generalization of the well known Berlekamp-Massey
algorithm. The Feng-Tzeng algorithm works indeed, if all t sequences have the
same length. This paper focuses on multi-sequence shift-register synthesis for
generating sequences of varying length. It is exposed, that the Feng-Tzeng
algorithm does not always give the correct solution in this case. A modified
algorithm is proposed and formally proved, which overcomes this problem."
"Tight bounds on the block entropy of patterns of sequences generated by
independent and identically distributed (i.i.d.) sources are derived. A pattern
of a sequence is a sequence of integer indices with each index representing the
order of first occurrence of the respective symbol in the original sequence.
Since a pattern is the result of data processing on the original sequence, its
entropy cannot be larger. Bounds derived here describe the pattern entropy as
function of the original i.i.d. source entropy, the alphabet size, the symbol
probabilities, and their arrangement in the probability space. Matching upper
and lower bounds derived provide a useful tool for very accurate approximations
of pattern block entropies for various distributions, and for assessing the
decrease of the pattern entropy from that of the original i.i.d. sequence."
"This paper outlines a three-step procedure for determining the low bit error
rate performance curve of a wide class of LDPC codes of moderate length. The
traditional method to estimate code performance in the higher SNR region is to
use a sum of the contributions of the most dominant error events to the
probability of error. These dominant error events will be both code and decoder
dependent, consisting of low-weight codewords as well as non-codeword events if
ML decoding is not used. For even moderate length codes, it is not feasible to
find all of these dominant error events with a brute force search. The proposed
method provides a convenient way to evaluate very low bit error rate
performance of an LDPC code without requiring knowledge of the complete error
event weight spectrum or resorting to a Monte Carlo simulation. This new method
can be applied to various types of decoding such as the full belief propagation
version of the message passing algorithm or the commonly used min-sum
approximation to belief propagation. The proposed method allows one to
efficiently see error performance at bit error rates that were previously out
of reach of Monte Carlo methods. This result will provide a solid foundation
for the analysis and design of LDPC codes and decoders that are required to
provide a guaranteed very low bit error rate performance at certain SNRs."
"A sequential updating scheme (SUS) for belief propagation (BP) decoding of
LDPC codes over Galois fields, $GF(q)$, and correlated Markov sources is
proposed, and compared with the standard parallel updating scheme (PUS). A
thorough experimental study of various transmission settings indicates that the
convergence rate, in iterations, of the BP algorithm (and subsequently its
complexity) for the SUS is about one half of that for the PUS, independent of
the finite field size $q$. Moreover, this 1/2 factor appears regardless of the
correlations of the source and the channel's noise model, while the error
correction performance remains unchanged. These results may imply on the
'universality' of the one half convergence speed-up of SUS decoding."
"This paper is motivated by a sensor network on a correlated field where
nearby sensors share information, and can thus assist rather than interfere
with one another. A special class of two-user Gaussian interference channels
(IFCs) is considered where one of the two transmitters knows both the messages
to be conveyed to the two receivers (called the IFC with degraded message
sets). Both achievability and converse arguments are provided for this scenario
for a class of discrete memoryless channels with weak interference. For the
case of the Gaussian weak interference channel with degraded message sets,
optimality of Gaussian inputs is also shown, resulting in the capacity region
of this channel."
"This paper is motivated by a sensor network on a correlated field where
nearby sensors share information, and can thus assist rather than interfere
with one another. We consider a special class of two-user Gaussian interference
channels (IFCs) where one of the two transmitters knows both the messages to be
conveyed to the two receivers. Both achievability and converse arguments are
provided for a channel with Gaussian inputs and Gaussian noise when the
interference is weaker than the direct link (a so called weak IFC). In general,
this region serves as an outer bound on the capacity of weak IFCs with no
shared knowledge between transmitters."
"The memoryless noncoherent single-input single-output (SISO) Rayleigh-fading
channel is considered. Closed-form expressions for the mutual information
between the output and the input of this channel when the input magnitude
distribution is discrete and restricted to having two mass points are derived,
and it is subsequently shown how these expressions can be used to obtain
closed-form expressions for the capacity of this channel for signal to noise
ratio (SNR) values of up to approximately 0 dB, and a tight capacity lower
bound for SNR values between 0 dB and 10 dB. The expressions for the channel
capacity and its lower bound are given as functions of a parameter which can be
obtained via numerical root-finding algorithms."
"The problem of discrete universal filtering, in which the components of a
discrete signal emitted by an unknown source and corrupted by a known DMC are
to be causally estimated, is considered. A family of filters are derived, and
are shown to be universally asymptotically optimal in the sense of achieving
the optimum filtering performance when the clean signal is stationary, ergodic,
and satisfies an additional mild positivity condition. Our schemes are
comprised of approximating the noisy signal using a hidden Markov process (HMP)
via maximum-likelihood (ML) estimation, followed by the use of the forward
recursions for HMP state estimation. It is shown that as the data length
increases, and as the number of states in the HMP approximation increases, our
family of filters attain the performance of the optimal distribution-dependent
filter."
"A fading broadcast channel is considered where the transmitter employs two
antennas and each of the two receivers employs a single receive antenna. It is
demonstrated that even if the realization of the fading is precisely known to
the receivers, the high signal-to-noise (SNR) throughput is greatly reduced if,
rather than knowing the fading realization \emph{precisely}, the trasmitter
only knows the fading realization \emph{approximately}. The results are general
and are not limited to memoryless Gaussian fading."
"A discrete memoryless generalized multiple access channel (GMAC) with
confidential messages is studied, where two users attempt to transmit common
information to a destination and each user also has private (confidential)
information intended for the destination. This channel generalizes the multiple
access channel (MAC) in that the two users also receive channel outputs. It is
assumed that each user views the other user as a wire-tapper, and wishes to
keep its confidential information as secret as possible from the other user.
The level of secrecy of the confidential information is measured by the
equivocation rate. The performance measure of interest is the rate-equivocation
tuple that includes the common rate, two private rates and two equivocation
rates as components. The set that includes all achievable rate-equivocation
tuples is referred to as the capacity-equivocation region. For the GMAC with
one confidential message set, where only one user (user 1) has private
(confidential) information for the destination, inner and outer bounds on the
capacity-equivocation region are derived. The secrecy capacity region is
established, which is the set of all achievable rates with user 2 being
perfectly ignorant of confidential messages of user 1. Furthermore, the
capacity-equivocation region and the secrecy capacity region are established
for the degraded GMAC with one confidential message set. For the GMAC with two
confidential message sets, where both users have confidential messages for the
destination, inner bounds on the capacity-equivocation region and the secrecy
capacity region are obtained."
"Assuming iterative decoding for binary erasure channels (BECs), a novel
tree-based technique for upper bounding the bit error rates (BERs) of
arbitrary, finite low-density parity-check (LDPC) codes is provided and the
resulting bound can be evaluated for all operating erasure probabilities,
including both the waterfall and the error floor regions. This upper bound can
also be viewed as a narrowing search of stopping sets, which is an approach
different from the stopping set enumeration used for lower bounding the error
floor. When combined with optimal leaf-finding modules, this upper bound is
guaranteed to be tight in terms of the asymptotic order. The Boolean framework
proposed herein further admits a composite search for even tighter results. For
comparison, a refinement of the algorithm is capable of exhausting all stopping
sets of size <14 for irregular LDPC codes of length n=500, which requires
approximately 1.67*10^25 trials if a brute force approach is taken. These
experiments indicate that this upper bound can be used both as an analytical
tool and as a deterministic worst-performance (error floor) guarantee, the
latter of which is crucial to optimizing LDPC codes for extremely low BER
applications, e.g., optical/satellite communications."
"In this paper, random coding error exponents and cutoff rate are studied for
noncoherent Rician fading channels, where neither the receiver nor the
transmitter has channel side information. First, it is assumed that the input
is subject only to an average power constraint. In this case, a lower bound to
the random coding error exponent is considered and the optimal input achieving
this lower bound is shown to have a discrete amplitude and uniform phase. If
the input is subject to both average and peak power constraints, it is proven
that the optimal input achieving the random coding error exponent has again a
discrete nature. Finally, the cutoff rate is analyzed, and the optimality of
the single-mass input amplitude distribution in the low-power regime is
discussed."
"We describe and analyze sparse graphical code constructions for the problems
of source coding with decoder side information (the Wyner-Ziv problem), and
channel coding with encoder side information (the Gelfand-Pinsker problem). Our
approach relies on a combination of low-density parity check (LDPC) codes and
low-density generator matrix (LDGM) codes, and produces sparse constructions
that are simultaneously good as both source and channel codes. In particular,
we prove that under maximum likelihood encoding/decoding, there exist
low-density codes (i.e., with finite degrees) from our constructions that can
saturate both the Wyner-Ziv and Gelfand-Pinsker bounds."
"In this paper, we investigate communication strategies for the multiple
access channel with feedback and correlated sources (MACFCS). The MACFCS models
a wireless sensor network scenario in which sensors distributed throughout an
arbitrary random field collect correlated measurements and transmit them to a
common sink. We derive achievable rate regions for the three-node MACFCS.
First, we study the strategy when source coding and channel coding are
combined, which we term full decoding at sources. Second, we look at several
strategies when source coding and channel coding are separated, which we term
full decoding at destination. From numerical computations on Gaussian channels,
we see that different strategies perform better under certain source
correlations and channel setups."
"In this paper, we derive the capacity of a special class of mesh networks. A
mesh network is defined as a heterogeneous wireless network in which the
transmission among power limited nodes is assisted by powerful relays, which
use the same wireless medium. We find the capacity of the mesh network when
there is one source, one destination, and multiple relays. We call this channel
the single source multiple relay single destination (SSMRSD) mesh network. Our
approach is as follows. We first look at an upper bound on the information
theoretic capacity of these networks in the Gaussian setting. We then show that
the bound is achievable asymptotically using the compress-forward strategy for
the multiple relay channel. Theoretically, the results indicate the value of
cooperation and the utility of carefully deployed relays in wireless ad-hoc and
sensor networks. The capacity characterization quantifies how the relays can be
used to either conserve node energy or to increase transmission rate."
"We present a novel differential space-time modulation (DSTM) scheme that is
single-symbol decodable and can provide full transmit diversity. It is the
first known singlesymbol- decodable DSTM scheme not based on Orthogonal STBC
(O-STBC), and it is constructed based on the recently proposed
Minimum-Decoding-Complexity Quasi-Orthogonal Space-Time Block Code
(MDC-QOSTBC). We derive the code design criteria and present systematic
methodology to find the solution sets. The proposed DSTM scheme can provide
higher code rate than DSTM schemes based on O-STBC. Its decoding complexity is
also considerably lower than DSTM schemes based on Sp(2) and
double-symbol-decodable QOSTBC, with negligible or slight trade-off in decoding
error probability performance."
"A game-theoretic analysis is used to study the effects of receiver choice on
the energy efficiency of multi-hop networks in which the nodes communicate
using Direct-Sequence Code Division Multiple Access (DS-CDMA). A Nash
equilibrium of the game in which the network nodes can choose their receivers
as well as their transmit powers to maximize the total number of bits they
transmit per unit of energy is derived. The energy efficiencies resulting from
the use of different linear multiuser receivers in this context are compared,
looking at both the non-cooperative game and the Pareto optimal solution. For
analytical ease, particular attention is paid to asymptotically large networks.
Significant gains in energy efficiency are observed when multiuser receivers,
particularly the linear minimum mean-square error (MMSE) receiver, are used
instead of conventional matched filter receivers."
"The recovery of network structure from experimental data is a basic and
fundamental problem. Unfortunately, experimental data often do not directly
reveal structure due to inherent limitations such as imprecision in timing or
other observation mechanisms. We consider the problem of inferring network
structure in the form of a directed graph from co-occurrence observations. Each
observation arises from a transmission made over the network and indicates
which vertices carry the transmission without explicitly conveying their order
in the path. Without order information, there are an exponential number of
feasible graphs which agree with the observed data equally well. Yet, the basic
physical principles underlying most networks strongly suggest that all feasible
graphs are not equally likely. In particular, vertices that co-occur in many
observations are probably closely connected. Previous approaches to this
problem are based on ad hoc heuristics. We model the experimental observations
as independent realizations of a random walk on the underlying graph, subjected
to a random permutation which accounts for the lack of order information.
Treating the permutations as missing data, we derive an exact
expectation-maximization (EM) algorithm for estimating the random walk
parameters. For long transmission paths the exact E-step may be computationally
intractable, so we also describe an efficient Monte Carlo EM (MCEM) algorithm
and derive conditions which ensure convergence of the MCEM algorithm with high
probability. Simulations and experiments with Internet measurements demonstrate
the promise of this approach."
"An outer bound to the capacity region of the two-receiver discrete memoryless
broadcast channel is given. The outer bound is tight for all cases where the
capacity region is known. When specialized to the case of no common
information, this outer bound is contained in the Korner-Marton outer bound.
This containment is shown to be strict for the binary skew-symmetric broadcast
channel. Thus, this outer bound is in general tighter than all other known
outer bounds on the discrete memoryless broadcast channel."
"We investigate the optimal performance of dense sensor networks by studying
the joint source-channel coding problem. The overall goal of the sensor network
is to take measurements from an underlying random process, code and transmit
those measurement samples to a collector node in a cooperative multiple access
channel with imperfect feedback, and reconstruct the entire random process at
the collector node. We provide lower and upper bounds for the minimum
achievable expected distortion when the underlying random process is Gaussian.
In the case where the random process satisfies some general conditions, we
evaluate the lower and upper bounds explicitly and show that they are of the
same order for a wide range of sum power constraints. Thus, for these random
processes, under these sum power constraints, we determine the achievability
scheme that is order-optimal, and express the minimum achievable expected
distortion as a function of the sum power constraint."
"Precoding with block diagonalization is an attractive scheme for approaching
sum capacity in multiuser multiple input multiple output (MIMO) broadcast
channels. This method requires either global channel state information at every
receiver or an additional training phase, which demands additional system
planning. In this paper we propose a lattice based multi-user precoder that
uses block diagonalization combined with pre-equalization and perturbation for
the multiuser MIMO broadcast channel. An achievable sum rate of the proposed
scheme is derived and used to show that the proposed technique approaches the
achievable sum rate of block diagonalization with water-filling but does not
require the additional information at the receiver. Monte Carlo simulations
with equal power allocation show that the proposed method provides better bit
error rate and diversity performance than block diagonalization with a
zero-forcing receiver. Additionally, the proposed method shows similar
performance to the maximum likelihood receiver but with much lower receiver
complexity."
"Pseudocodewords of q-ary LDPC codes are examined and the weight of a
pseudocodeword on the q-ary symmetric channel is defined. The weight definition
of a pseudocodeword on the AWGN channel is also extended to two-dimensional
q-ary modulation such as q-PAM and q-PSK. The tree-based lower bounds on the
minimum pseudocodeword weight are shown to also hold for q-ary LDPC codes on
these channels."
"The multi-terminal rate-distortion problem has been studied extensively.
Notably, among these, Tung and Housewright have provided the best known inner
and outer bounds for the rate region under certain distortion constraints. In
this paper, we first propose an outer bound for the rate region, and show that
it is tighter than the outer bound of Tung and Housewright. Our outer bound
involves some $n$-letter Markov chain constraints, which cause computational
difficulties. We utilize a necessary condition for the Markov chain constraints
to obtain another outer bound, which is represented in terms of some
single-letter mutual information expressions evaluated over probability
distributions that satisfy some single-letter conditions."
"In this work we focus on the general relay channel. We investigate the
application of estimate-and-forward (EAF) to different scenarios. Specifically,
we consider assignments of the auxiliary random variables that always satisfy
the feasibility constraints. We first consider the multiple relay channel and
obtain an achievable rate without decoding at the relays. We demonstrate the
benefits of this result via an explicit discrete memoryless multiple relay
scenario where multi-relay EAF is superior to multi-relay decode-and-forward
(DAF). We then consider the Gaussian relay channel with coded modulation, where
we show that a three-level quantization outperforms the Gaussian quantization
commonly used to evaluate the achievable rates in this scenario. Finally we
consider the cooperative general broadcast scenario with a multi-step
conference. We apply estimate-and-forward to obtain a general multi-step
achievable rate region. We then give an explicit assignment of the auxiliary
random variables, and use this result to obtain an explicit expression for the
single common message broadcast scenario with a two-step conference."
"We consider a peak-power-limited single-antenna block-stationary Gaussian
fading channel where neither the transmitter nor the receiver knows the channel
state information, but both know the channel statistics. This model subsumes
most previously studied Gaussian fading models. We first compute the asymptotic
channel capacity in the high SNR regime and show that the behavior of channel
capacity depends critically on the channel model. For the special case where
the fading process is symbol-by-symbol stationary, we also reveal a fundamental
interplay between the codeword length, communication rate, and decoding error
probability. Specifically, we show that the codeword length must scale with SNR
in order to guarantee that the communication rate can grow logarithmically with
SNR with bounded decoding error probability, and we find a necessary condition
for the growth rate of the codeword length. We also derive an expression for
the capacity per unit energy. Furthermore, we show that the capacity per unit
energy is achievable using temporal ON-OFF signaling with optimally allocated
ON symbols, where the optimal ON-symbol allocation scheme may depend on the
peak power constraint."
"In this paper, the multiple access channel (MAC) with channel state is
analyzed in a scenario where a) the channel state is known non-causally to the
transmitters and b) there is perfect causal feedback from the receiver to the
transmitters. An achievable region and an outer bound are found for a discrete
memoryless MAC that extend existing results, bringing together ideas from the
two separate domains of MAC with state and MAC with feedback. Although this
achievable region does not match the outer bound in general, special cases
where they meet are identified.
  In the case of a Gaussian MAC, a specialized achievable region is found by
using a combination of dirty paper coding and a generalization of the
Schalkwijk-Kailath, Ozarow and Merhav-Weissman schemes, and this region is
found to be capacity achieving. Specifically, it is shown that additive
Gaussian interference that is known non-causally to the transmitter causes no
loss in capacity for the Gaussian MAC with feedback."
"The sum capacity on a symbol-synchronous CDMA system having processing gain
$N$ and supporting $K$ power constrained users is achieved by employing at most
$2N-1$ sequences. Analogously, the minimum received power (energy-per-chip) on
the symbol-synchronous CDMA system supporting $K$ users that demand specified
data rates is attained by employing at most $2N-1$ sequences. If there are $L$
oversized users in the system, at most $2N-L-1$ sequences are needed. $2N-1$ is
the minimum number of sequences needed to guarantee optimal allocation for
single dimensional signaling. $N$ orthogonal sequences are sufficient if a few
users (at most $N-1$) are allowed to signal in multiple dimensions. If there
are no oversized users, these split users need to signal only in two dimensions
each. The above results are shown by proving a converse to a well-known result
of Weyl on the interlacing eigenvalues of the sum of two Hermitian matrices,
one of which is of rank 1. The converse is analogous to Mirsky's converse to
the interlacing eigenvalues theorem for bordering matrices."
"This paper examines the performance of decision feedback based iterative
channel estimation and multiuser detection in channel coded aperiodic DS-CDMA
systems operating over multipath fading channels. First, explicit expressions
describing the performance of channel estimation and parallel interference
cancellation based multiuser detection are developed. These results are then
combined to characterize the evolution of the performance of a system that
iterates among channel estimation, multiuser detection and channel decoding.
Sufficient conditions for convergence of this system to a unique fixed point
are developed."
"Feedback of quantized channel state information (CSI), called limited
feedback, enables transmit beamforming in multiple-input-multiple-output (MIMO)
wireless systems with a small amount of overhead. Due to its efficiency,
beamforming with limited feedback has been adopted in several wireless
communication standards. Prior work on limited feedback commonly adopts the
block fading channel model where temporal correlation in wireless channels is
neglected. This paper considers temporally-correlated channels and designs
single-user transmit beamforming with limited feedback. Analytical results
concerning CSI feedback are derived by modeling quantized CSI as a first-order
finite-state Markov chain. These results include the source bit rate generated
by time-varying quantized CSI, the required bit rate for a CSI feedback
channel, and the effect of feedback delay. In particular, based on the theory
of Markov chain convergence rate, feedback delay is proved to reduce the
throughput gain due to CSI feedback at least exponentially. Furthermore, an
algorithm is proposed for CSI feedback compression in time. Combining the
results in this work leads to a new method for designing limited feedback
beamforming as demonstrated by a design example."
"A generic $(r,m)$-erasure correcting set is a collection of vectors in
$\bF_2^r$ which can be used to generate, for each binary linear code of
codimension $r$, a collection of parity check equations that enables iterative
decoding of all correctable erasure patterns of size at most $m$.
  That is to say, the only stopping sets of size at most $m$ for the generated
parity check equations are the erasure patterns for which there is more than
one manner to fill in theerasures to obtain a codeword.
  We give an explicit construction of generic $(r,m)$-erasure correcting sets
of cardinality $\sum_{i=0}^{m-1} {r-1\choose i}$. Using a random-coding-like
argument, we show that for fixed $m$, the minimum size of a generic
$(r,m)$-erasure correcting set is linear in $r$.
  Keywords: iterative decoding, binary erasure channel, stopping set"
"We propose an improved algorithm for finding roots of polynomials over finite
fields. This makes possible significant speedup of the decoding process of
Bose-Chaudhuri-Hocquenghem, Reed-Solomon, and some other error-correcting
codes."
"In this correspondence, we study the minimum pseudo-weight and minimum
pseudo-codewords of low-density parity-check (LDPC) codes under linear
programming (LP) decoding. First, we show that the lower bound of Kelly,
Sridhara, Xu and Rosenthal on the pseudo-weight of a pseudo-codeword of an LDPC
code with girth greater than 4 is tight if and only if this pseudo-codeword is
a real multiple of a codeword. Then, we show that the lower bound of Kashyap
and Vardy on the stopping distance of an LDPC code is also a lower bound on the
pseudo-weight of a pseudo-codeword of this LDPC code with girth 4, and this
lower bound is tight if and only if this pseudo-codeword is a real multiple of
a codeword. Using these results we further show that for some LDPC codes, there
are no other minimum pseudo-codewords except the real multiples of minimum
codewords. This means that the LP decoding for these LDPC codes is
asymptotically optimal in the sense that the ratio of the probabilities of
decoding errors of LP decoding and maximum-likelihood decoding approaches to 1
as the signal-to-noise ratio leads to infinity. Finally, some LDPC codes are
listed to illustrate these results."
"Let $N$ local decision makers in a sensor network communicate with their
neighbors to reach a decision \emph{consensus}. Communication is local, among
neighboring sensors only, through noiseless or noisy links. We study the design
of the network topology that optimizes the rate of convergence of the iterative
decision consensus algorithm. We reformulate the topology design problem as a
spectral graph design problem, namely, maximizing the eigenratio~$\gamma$ of
two eigenvalues of the graph Laplacian~$L$, a matrix that is naturally
associated with the interconnectivity pattern of the network. This
reformulation avoids costly Monte Carlo simulations and leads to the class of
non-bipartite Ramanujan graphs for which we find a lower bound on~$\gamma$. For
Ramanujan topologies and noiseless links, the local probability of error
converges much faster to the overall global probability of error than for
structured graphs, random graphs, or graphs exhibiting small-world
characteristics. With noisy links, we determine the optimal number of
iterations before calling a decision. Finally, we introduce a new class of
random graphs that are easy to construct, can be designed with arbitrary number
of sensors, and whose spectral and convergence properties make them practically
equivalent to Ramanujan topologies."
"In this paper, a downlink scenario in which a single-antenna base station
communicates with K single antenna users, over a time-correlated fading
channel, is considered. It is assumed that channel state information is
perfectly known at each receiver, while the statistical characteristics of the
fading process and the fading gain at the beginning of each frame are known to
the transmitter. By evaluating the random coding error exponent of the
time-correlated fading channel, it is shown that there is an optimal codeword
length which maximizes the throughput. The throughput of the conventional
scheduling that transmits to the user with the maximum signal to noise ratio is
examined using both fixed length codewords and variable length codewords.
Although optimizing the codeword length improves the performance, it is shown
that using the conventional scheduling, the gap between the achievable
throughput and the maximum possible throughput of the system tends to infinity
as K goes to infinity. A simple scheduling that considers both the signal to
noise ratio and the channel time variation is proposed. It is shown that by
using this scheduling, the gap between the achievable throughput and the
maximum throughput of the system approaches zero."
"A partially cooperative relay broadcast channel (RBC) is a three-node network
with one source node and two destination nodes (destinations 1 and 2) where
destination 1 can act as a relay to assist destination 2. Inner and outer
bounds on the capacity region of the discrete memoryless partially cooperative
RBC are obtained. When the relay function is disabled, the inner and outer
bounds reduce to new bounds on the capacity region of broadcast channels. Four
classes of RBCs are studied in detail. For the partially cooperative RBC with
degraded message sets, inner and outer bounds are obtained. For the
semideterministic partially cooperative RBC and the orthogonal partially
cooperative RBC, the capacity regions are established. For the parallel
partially cooperative RBC with unmatched degraded subchannels, the capacity
region is established for the case of degraded message sets. The capacity is
also established when the source node has only a private message for
destination 2, i.e., the channel reduces to a parallel relay channel with
unmatched degraded subchannels."
"We consider the detection of binary (antipodal) signals transmitted in a
spatially multiplexed fashion over a fading multiple-input multiple-output
(MIMO) channel and where the detection is done by means of semidefinite
relaxation (SDR). The SDR detector is an attractive alternative to maximum
likelihood (ML) detection since the complexity is polynomial rather than
exponential. Assuming that the channel matrix is drawn with i.i.d. real valued
Gaussian entries, we study the receiver diversity and prove that the SDR
detector achieves the maximum possible diversity. Thus, the error probability
of the receiver tends to zero at the same rate as the optimal maximum
likelihood (ML) receiver in the high signal to noise ratio (SNR) limit. This
significantly strengthens previous performance guarantees available for the
semidefinite relaxation detector. Additionally, it proves that full diversity
detection is in certain scenarios also possible when using a non-combinatorial
receiver structure."
"Several recent standards such as IEEE 802.11a/g, IEEE 802.16, and ECMA
Multiband Orthogonal Frequency Division Multiplexing (MB-OFDM) for high
data-rate Ultra-Wideband (UWB), employ bit-interleaved convolutionally-coded
multicarrier modulation over quasi-static fading channels. Motivated by the
lack of appropriate error rate analysis techniques for this popular type of
system and channel model, we present two novel analytical methods for bit error
rate (BER) estimation of coded multicarrier systems operating over
frequency-selective quasi-static channels with non-ideal interleaving. In the
first method, the approximate performance of the system is calculated for each
realization of the channel, which is suitable for obtaining the outage BER
performance (a common performance measure for e.g. MB-OFDM systems). The second
method assumes Rayleigh distributed frequency-domain subcarrier channel gains
and knowledge of their correlation matrix, and can be used to directly obtain
the average BER performance. Both methods are applicable to
convolutionally-coded interleaved multicarrier systems employing Quadrature
Amplitude Modulation (QAM), and are also able to account for narrowband
interference (modeled as a sum of tone interferers). To illustrate the
application of the proposed analysis, both methods are used to study the
performance of a tone-interference-impaired MB-OFDM system."
"For a wide class of multi-user systems, a subset of capacity region which
includes the corner points and the sum-capacity facet has a special structure
known as polymatroid. Multiaccess channels with fixed input distributions and
multiple-antenna broadcast channels are examples of such systems. Any interior
point of the sum-capacity facet can be achieved by time-sharing among corner
points or by an alternative method known as rate-splitting. The main purpose of
this paper is to find a point on the sum-capacity facet which satisfies a
notion of fairness among active users. This problem is addressed in two cases:
(i) where the complexity of achieving interior points is not feasible, and (ii)
where the complexity of achieving interior points is feasible. For the first
case, the corner point for which the minimum rate of the active users is
maximized (max-min corner point) is desired for signaling. A simple greedy
algorithm is introduced to find the optimum max-min corner point. For the
second case, the polymatroid properties are exploited to locate a rate-vector
on the sum-capacity facet which is optimally fair in the sense that the minimum
rate among all users is maximized (max-min rate). In the case that the rate of
some users can not increase further (attain the max-min value), the algorithm
recursively maximizes the minimum rate among the rest of the users. It is shown
that the problems of deriving the time-sharing coefficients or rate-spitting
scheme can be solved by decomposing the problem to some lower-dimensional
subproblems. In addition, a fast algorithm to compute the time-sharing
coefficients to attain a general point on the sum-capacity facet is proposed."
"In this paper we show a some new look at large deviation theorems from the
viewpoint of the information-spectrum (IS) methods, which has been first
exploited in information theory, and also demonstrate a new basic formula for
the large deviation rate function in general, which is a pair of the lower and
upper IS rate functions. In particular, we are interested in establishing the
general large deviation rate functions that can be derivable as the
Fenchel-Legendre transform of the cumulant generating function. The final goal
is to show a necessary and sufficient condition for the rate function to be of
Cram\'er-G\""artner-Ellis type."
"A code over GF$(q^m)$ can be imaged or expanded into a code over GF$(q)$
using a basis for the extension field over the base field. The properties of
such an image depend on the original code and the basis chosen for imaging.
Problems relating the properties of a code and its image with respect to a
basis have been of great interest in the field of coding theory. In this work,
a generalized version of the problem of self-orthogonality of the $q$-ary image
of a $q^m$-ary code has been considered. Given an inner product (more
generally, a biadditive form), necessary and sufficient conditions have been
derived for a code over a field extension and an expansion basis so that an
image of that code is self-orthogonal. The conditions require that the original
code be self-orthogonal with respect to several related biadditive forms
whenever certain power sums of the dual basis elements do not vanish. Numerous
interesting corollaries have been derived by specializing the general
conditions. An interesting result for the canonical or regular inner product in
fields of characteristic two is that only self-orthogonal codes result in
self-orthogonal images. Another result is that image of a code is
self-orthogonal for all bases if and only if trace of the code is
self-orthogonal, except for the case of binary images of 4-ary codes. The
conditions are particularly simple to state and apply for cyclic codes. To
illustrate a possible application, new quantum error-correcting codes have been
constructed with larger minimum distance than previously known."
"We consider a pair of correlated processes {Z_n} and {S_n} (two sided), where
the former is observable and the later is hidden. The uncertainty in the
estimation of Z_n upon its finite past history is H(Z_n|Z_0^{n-1}), and for
estimation of S_n upon this observation is H(S_n|Z_0^{n-1}), which are both
sequences of n. The limits of these sequences (and their existence) are of
practical and theoretical interest. The first limit, if exists, is the entropy
rate. We call the second limit the estimation entropy. An example of a process
jointly correlated to another one is the hidden Markov process. It is the
memoryless observation of the Markov state process where state transitions are
independent of past observations. We consider a new representation of hidden
Markov process using iterated function system. In this representation the state
transitions are deterministically related to the process. This representation
provides a unified framework for the analysis of the two limiting entropies for
this process, resulting in integral expressions for the limits. This analysis
shows that under mild conditions the limits exist and provides a simple method
for calculating the elements of the corresponding sequences."
"In this paper multi-user detection techniques, such as Parallel and Serial
Interference Cancellations (PIC & SIC), General Minimum Mean Square Error
(GMMSE) and polynomial MMSE, for the downlink of a broadband Multi-Carrier Code
Division Multiple Access (MCCDMA) system are investigated. The Bit Error Rate
(BER) and Frame Error Rate (FER) results are evaluated, and compared with
single-user detection (MMSEC, EGC) approaches, as well. The performance
evaluation takes into account the system load, channel coding and modulation
schemes."
"On the multi-antenna broadcast channel, the spatial degrees of freedom
support simultaneous transmission to multiple users. The optimal multiuser
transmission, known as dirty paper coding, is not directly realizable.
Moreover, close-to-optimal solutions such as Tomlinson-Harashima precoding are
sensitive to CSI inaccuracy. This paper considers a more practical design
called per user unitary and rate control (PU2RC), which has been proposed for
emerging cellular standards. PU2RC supports multiuser simultaneous
transmission, enables limited feedback, and is capable of exploiting multiuser
diversity. Its key feature is an orthogonal beamforming (or precoding)
constraint, where each user selects a beamformer (or precoder) from a codebook
of multiple orthonormal bases. In this paper, the asymptotic throughput scaling
laws for PU2RC with a large user pool are derived for different regimes of the
signal-to-noise ratio (SNR). In the multiuser-interference-limited regime, the
throughput of PU2RC is shown to scale logarithmically with the number of users.
In the normal SNR and noise-limited regimes, the throughput is found to scale
double logarithmically with the number of users and also linearly with the
number of antennas at the base station. In addition, numerical results show
that PU2RC achieves higher throughput and is more robust against CSI
quantization errors than the popular alternative of zero-forcing beamforming if
the number of users is sufficiently large."
"This paper is focused on the performance analysis of binary linear block
codes (or ensembles) whose transmission takes place over independent and
memoryless parallel channels. New upper bounds on the maximum-likelihood (ML)
decoding error probability are derived. These bounds are applied to various
ensembles of turbo-like codes, focusing especially on repeat-accumulate codes
and their recent variations which possess low encoding and decoding complexity
and exhibit remarkable performance under iterative decoding. The framework of
the second version of the Duman and Salehi (DS2) bounds is generalized to the
case of parallel channels, along with the derivation of their optimized tilting
measures. The connection between the generalized DS2 and the 1961 Gallager
bounds, addressed by Divsalar and by Sason and Shamai for a single channel, is
explored in the case of an arbitrary number of independent parallel channels.
The generalization of the DS2 bound for parallel channels enables to re-derive
specific bounds which were originally derived by Liu et al. as special cases of
the Gallager bound. In the asymptotic case where we let the block length tend
to infinity, the new bounds are used to obtain improved inner bounds on the
attainable channel regions under ML decoding. The tightness of the new bounds
for independent parallel channels is exemplified for structured ensembles of
turbo-like codes. The improved bounds with their optimized tilting measures
show, irrespectively of the block length of the codes, an improvement over the
union bound and other previously reported bounds for independent parallel
channels; this improvement is especially pronounced for moderate to large block
lengths."
"The performance of maximum-likelihood (ML) decoded binary linear block codes
is addressed via the derivation of tightened upper bounds on their decoding
error probability. The upper bounds on the block and bit error probabilities
are valid for any memoryless, binary-input and output-symmetric communication
channel, and their effectiveness is exemplified for various ensembles of
turbo-like codes over the AWGN channel. An expurgation of the distance spectrum
of binary linear block codes further tightens the resulting upper bounds."
"The performance of maximum-likelihood (ML) decoded binary linear block codes
over the AWGN channel is addressed via the tangential-sphere bound (TSB) and
two of its recent improved versions. The paper is focused on the derivation of
the error exponents of these bounds. Although it was exemplified that some
recent improvements of the TSB tighten this bound for finite-length codes, it
is demonstrated in this paper that their error exponents coincide. For an
arbitrary ensemble of binary linear block codes, the common value of these
error exponents is explicitly expressed in terms of the asymptotic growth rate
of the average distance spectrum."
"Broadly speaking Information theory (IT) assumes no structure of the
underlying states. But what about contexts where states do have a clear
structure - how should IT cope with such situations? And if such coping is at
all possible then - how should structure be expressed so that it can be coped
with? A possible answer to these questions is presented here. Noting that IT
can cope well with a structure expressed as an accurate clustering (by shifting
to the implied reduced alphabet), a generalization is suggested in which
structure is expressed as a measure on reduced alphabets. Given such structure
an extension of IT is presented where the reduced alphabets are treated
simultaneously. This structure-sensitive IT, called ITs, extends traditional IT
in the sense that: a)there are structure-sensitive analogs to the notions of
traditional IT and b)translating a theorem in IT by replacing its notions with
their structure-sensitive counterparts, yields a (provable) theorem of ITs.
Seemingly paradoxically, ITs extends IT but it's completely within the
framework of IT. The richness of the suggested structures is demonstrated by
two disparate families studied in more detail: the family of hierarchical
structures and the family of linear structures. The formal findings extend the
scope of cases to which a rigorous application of IT can be applied (with
implications on quantization, for example). The implications on the foundations
of IT are that the assumption regarding no underlying structure of states is
not mandatory and that there is a framework for expressing such underlying
structure."
"We consider the problem of estimating the total probability of all symbols
that appear with a given frequency in a string of i.i.d. random variables with
unknown distribution. We focus on the regime in which the block length is large
yet no symbol appears frequently in the string. This is accomplished by
allowing the distribution to change with the block length. Under a natural
convergence assumption on the sequence of underlying distributions, we show
that the total probabilities converge to a deterministic limit, which we
characterize. We then show that the Good-Turing total probability estimator is
strongly consistent."
"The paper deals with orthogonal space-time block coded MC-CDMA systems in
outdoor realistic downlink scenarios with up to two transmit and receive
antennas. Assuming no channel state information at the transmitter, we compare
several linear single-user detection and spreading schemes, with or without
channel coding, achieving a spectral efficiency of 1-2 bits/s/Hz. The different
results obtained demonstrate that spatial diversity significantly improves the
performance of MC-CDMA systems, and allows different chip-mapping without
notably decreasing performance. Moreover, the global system exhibits a good
trade-off between complexity at mobile stations and performance. Then,
Alamouti's STBC MC-CDMA schemes derive full benefit from the frequency and
spatial diversities and can be considered as a very realistic and promising
candidate for the air interface downlink of the 4/sup th/ generation mobile
radio systems."
"The asymptotic iterative decoding performances of low-density parity-check
(LDPC) codes using min-sum (MS) and sum-product (SP) decoding algorithms on
memoryless binary-input output-symmetric (MBIOS) channels are analyzed in this
paper. For MS decoding, the analysis is done by upper bounding the bit error
probability of the root bit of a tree code by the sequence error probability of
a subcode of the tree code assuming the transmission of the all-zero codeword.
The result is a recursive upper bound on the bit error probability after each
iteration. For SP decoding, we derive a recursively determined lower bound on
the bit error probability after each iteration. This recursive lower bound
recovers the density evolution equation of LDPC codes on the binary erasure
channel (BEC) with inequalities satisfied with equalities. A significant
implication of this result is that the performance of LDPC codes under SP
decoding on the BEC is an upper bound of the performance on all MBIOS channels
with the same uncoded bit error probability. All results hold for the more
general multi-edge type LDPC codes."
"We consider Slepian-Wolf code design based on LDPC (low-density parity-check)
coset codes for memoryless source-side information pairs. A density evolution
formula, equipped with a concentration theorem, is derived for Slepian- Wolf
coding based on LDPC coset codes. As a consequence, an intimate connection
between Slepian-Wolf coding and channel coding is established. Specifically we
show that, under density evolution, design of binary LDPC coset codes for
Slepian-Wolf coding of an arbitrary memoryless source-side information pair
reduces to design of binary LDPC codes for binary-input output-symmetric
channels without loss of optimality. With this connection, many classic results
in channel coding can be easily translated into the Slepian-Wolf setting."
"The performance of iterative decoding techniques for linear block codes
correcting erasures depends very much on the sizes of the stopping sets
associated with the underlying Tanner graph, or, equivalently, the parity-check
matrix representing the code. In this paper, we introduce the notion of
dead-end sets to explicitly demonstrate this dependency. The choice of the
parity-check matrix entails a trade-off between performance and complexity. We
give bounds on the complexity of iterative decoders achieving optimal
performance in terms of the sizes of the underlying parity-check matrices.
Further, we fully characterize codes for which the optimal stopping set
enumerator equals the weight enumerator."
"Elaborating on prior work by Minka, we formulate a general computation rule
for lossy messages. An important special case (with many applications in
communications) is the conversion of ""soft-bit"" messages to Gaussian messages.
By this method, the performance of a Kalman equalizer is improved, both for
uncoded and coded transmission."
This paper has been withdrawn
"We model the development of the linear complexity of multisequences by a
stochastic infinite state machine, the Battery-Discharge-Model, BDM. The states
s in S of the BDM have asymptotic probabilities or mass Pr(s)=1/(P(q,M)
q^K(s)), where K(s) in N_0 is the class of the state s, and P(q,M)=\sum_(K
in\N0) P_M(K)q^(-K)=\prod_(i=1..M) q^i/(q^i-1) is the generating function of
the number of partitions into at most M parts. We have (for each timestep
modulo M+1) just P_M(K) states of class K \.
  We obtain a closed formula for the asymptotic probability for the linear
complexity deviation d(n) := L(n)-\lceil n\cdot M/(M+1)\rceil with
Pr(d)=O(q^(-|d|(M+1))), for M in N, for d in Z. The precise formula is given in
the text. It has been verified numerically for M=1..8, and is conjectured to
hold for all M in N.
  From the asymptotic growth (proven for all M in N), we infer the Law of the
Logarithm for the linear complexity deviation, -liminf_{n\to\infty} d_a(n) /
log n = 1 /((M+1)log q) = limsup_{n\to\infty} d_a(n) / log n, which immediately
yields L_a(n)/n \to M/(M+1) with measure one, for all M in N, a result recently
shown already by Niederreiter and Wang. Keywords: Linear complexity, linear
complexity deviation, multisequence, Battery Discharge Model, isometry."
"We explain how to optimize finite-length LDPC codes for transmission over the
binary erasure channel. Our approach relies on an analytic approximation of the
erasure probability. This is in turn based on a finite-length scaling result to
model large scale erasures and a union bound involving minimal stopping sets to
take into account small error events. We show that the performances of
optimized ensembles as observed in simulations are well described by our
approximation. Although we only address the case of transmission over the
binary erasure channel, our method should be applicable to a more general
setting."
"Two product array codes are used to construct the (24, 12, 8) binary Golay
code through the direct sum operation. This construction provides a systematic
way to find proper (8, 4, 4) linear block component codes for generating the
Golay code, and it generates and extends previously existing methods that use a
similar construction framework. The code constructed is simple to decode."
"Let $X$ be a discrete random variable with support $S$ and $f : S \to
S^\prime$ be a bijection. Then it is well-known that the entropy of $X$ is the
same as the entropy of $f(X)$. This entropy preservation property has been
well-utilized to establish non-trivial properties of discrete stochastic
processes, e.g. queuing process \cite{prg03}. Entropy as well as entropy
preservation is well-defined only in the context of purely discrete or
continuous random variables. However for a mixture of discrete and continuous
random variables, which arise in many interesting situations, the notions of
entropy and entropy preservation have not been well understood.
  In this paper, we extend the notion of entropy in a natural manner for a
mixed-pair random variable, a pair of random variables with one discrete and
the other continuous. Our extensions are consistent with the existing
definitions of entropy in the sense that there exist natural injections from
discrete or continuous random variables into mixed-pair random variables such
that their entropy remains the same. This extension of entropy allows us to
obtain sufficient conditions for entropy preservation in mixtures of discrete
and continuous random variables under bijections.
  The extended definition of entropy leads to an entropy rate for continuous
time Markov chains. As an application, we recover a known probabilistic result
related to Poisson process. We strongly believe that the frame-work developed
in this paper can be useful in establishing probabilistic properties of complex
processes, such as load balancing systems, queuing network, caching algorithms."
"The problem of cooperative fusion in the presence of Byzantine sensors is
considered. An information theoretic formulation is used to characterize the
Shannon capacity of sensor fusion. It is shown that when less than half of the
sensors are Byzantine, the effect of Byzantine attack can be entirely
mitigated, and the fusion capacity is identical to that when all sensors are
honest. But when at least half of the sensors are Byzantine, they can
completely defeat the sensor fusion so that no information can be transmitted
reliably. A capacity achieving transmit-then-verify strategy is proposed for
the case that less than half of the sensors are Byzantine, and its error
probability and coding rate is analyzed by using a Markov decision process
modeling of the transmission protocol."
"In this paper, we derive Gallager's random coding error exponent for
multiple-input multiple-output (MIMO) channels, assuming no channel-state
information (CSI) at the transmitter and perfect CSI at the receiver. This
measure gives insight into a fundamental tradeoff between the communication
reliability and information rate of MIMO channels, enabling to determine the
required codeword length to achieve a prescribed error probability at a given
rate below the channel capacity. We quantify the effects of the number of
antennas, channel coherence time, and spatial fading correlation on the MIMO
exponent. In addition, general formulae for the ergodic capacity and the cutoff
rate in the presence of spatial correlation are deduced from the exponent
expressions. These formulae are applicable to arbitrary structures of transmit
and receive correlation, encompassing all the previously known results as
special cases of our expressions."
"We provide achievability as well as converse results for the degrees of
freedom region of a MIMO $X$ channel, i.e., a system with two transmitters, two
receivers, each equipped with multiple antennas, where independent messages
need to be conveyed over fixed channels from each transmitter to each receiver.
With M=1 antennas at each node, we find that the total (sum rate) degrees of
freedom are bounded above and below as $1 \leq\eta_X^\star \leq {4/3}$. If
$M>1$ and channel matrices are non-degenerate then the precise degrees of
freedom $\eta_X^\star = {4/3}M$. Simple zero forcing without dirty paper
encoding or successive decoding, suffices to achieve the ${4/3}M$ degrees of
freedom. With equal number of antennas at all nodes, we explore the increase in
degrees of freedom when some of the messages are made available to a
transmitter or receiver in the manner of cognitive radio. With a cognitive
transmitter we show that the number of degrees of freedom $\eta = {3/2}M$ (for
$M>1$) on the MIMO $X$ channel. The same degrees of freedom are obtained on the
MIMO $X$ channel with a cognitive receiver as well. In contrast to the $X$
channel result, we show that for the MIMO \emph{interference} channel, the
degrees of freedom are not increased even if both the transmitter and the
receiver of one user know the other user's message. However, the interference
channel can achieve the full $2M$ degrees of freedom if \emph{each} user has
either a cognitive transmitter or a cognitive receiver. Lastly, if the channels
vary with time/frequency then the $X$ channel with single antennas $(M=1)$ at
all nodes has exactly 4/3 degrees of freedom with no shared messages and
exactly 3/2 degrees of freedom with a cognitive transmitter or a cognitive
receiver."
"We consider a state-dependent multiaccess channel (MAC) with state
non-causally known to some encoders. We derive an inner bound for the capacity
region in the general discrete memoryless case and specialize to a binary
noiseless case. In the case of maximum entropy channel state, we obtain the
capacity region for binary noiseless MAC with one informed encoder by deriving
a non-trivial outer bound for this case. For a Gaussian state-dependent MAC
with one encoder being informed of the channel state, we present an inner bound
by applying a slightly generalized dirty paper coding (GDPC) at the informed
encoder that allows for partial state cancellation, and a trivial outer bound
by providing channel state to the decoder also. The uninformed encoders benefit
from the state cancellation in terms of achievable rates, however, appears that
GDPC cannot completely eliminate the effect of the channel state on the
achievable rate region, in contrast to the case of all encoders being informed.
In the case of infinite state variance, we analyze how the uninformed encoder
benefits from the informed encoder's actions using the inner bound and also
provide a non-trivial outer bound for this case which is better than the
trivial outer bound."
"In this paper, we present a method of estimating the volatility of a signal
that displays stochastic noise (such as a risky asset traded on an open market)
utilizing Linear Predictive Coding. The main purpose is to associate volatility
with a series of statistical properties that can lead us, through further
investigation, toward a better understanding of structural volatility as well
as to improve the quality of our current estimates."
"The decoding of Low-Density Parity-Check codes by the Belief Propagation (BP)
algorithm is revisited. We check the iterative algorithm for its convergence to
a codeword (termination), we run Monte Carlo simulations to find the
probability distribution function of the termination time, n_it. Tested on an
example [155, 64, 20] code, this termination curve shows a maximum and an
extended algebraic tail at the highest values of n_it. Aiming to reduce the
tail of the termination curve we consider a family of iterative algorithms
modifying the standard BP by means of a simple relaxation. The relaxation
parameter controls the convergence of the modified BP algorithm to a minimum of
the Bethe free energy. The improvement is experimentally demonstrated for
Additive-White-Gaussian-Noise channel in some range of the signal-to-noise
ratios. We also discuss the trade-off between the relaxation parameter of the
improved iterative scheme and the number of iterations."
"We consider codes over the alphabet Q={0,1,..,q-1}intended for the control of
unidirectional errors of level l. That is, the transmission channel is such
that the received word cannot contain both a component larger than the
transmitted one and a component smaller than the transmitted one. Moreover, the
absolute value of the difference between a transmitted component and its
received version is at most l.
  We introduce and study q-ary codes capable of correcting all unidirectional
errors of level l. Lower and upper bounds for the maximal size of those codes
are presented.
  We also study codes for this aim that are defined by a single equation on the
codeword coordinates(similar to the Varshamov-Tenengolts codes for correcting
binary asymmetric errors). We finally consider the problem of detecting all
unidirectional errors of level l."
"In this paper we consider the communication problem that involves
transmission of correlated sources over broadcast channels. We consider a
graph-based framework for this information transmission problem. The system
involves a source coding module and a channel coding module. In the source
coding module, the sources are efficiently mapped into a nearly semi-regular
bipartite graph, and in the channel coding module, the edges of this graph are
reliably transmitted over a broadcast channel. We consider nearly semi-regular
bipartite graphs as discrete interface between source coding and channel coding
in this multiterminal setting. We provide an information-theoretic
characterization of (1) the rate of exponential growth (as a function of the
number of channel uses) of the size of the bipartite graphs whose edges can be
reliably transmitted over a broadcast channel and (2) the rate of exponential
growth (as a function of the number of source samples) of the size of the
bipartite graphs which can reliably represent a pair of correlated sources to
be transmitted over a broadcast channel."
"Given a probability distribution P, what is the minimum amount of bits needed
to store a value x sampled according to P, such that x can later be recovered
(except with some small probability)? Or, what is the maximum amount of uniform
randomness that can be extracted from x? Answering these and similar
information-theoretic questions typically boils down to computing so-called
smooth entropies. In this paper, we derive explicit and almost tight bounds on
the smooth entropies of n-fold product distributions."
"New approach for analysis and decoding MIMO signaling is developed for usual
model of nongaussion noise consists of background and impulsive noise named
epsilon - noise. It is shown that non-gaussion noise performance significantly
worse than gaussion ones. Stimulation results strengthen out theory. Robust in
statistical sense detection rule is suggested for such kind of noise features
much best robust detector performance than detector designed for Gaussian noise
in impulsive environment and modest margin in background noise. Proposed
algorithms performance are comparable with developed potential bound. Proposed
tool, is crucial issue for MIMO communication system design, since real noise
environment has impulsive character that contradict with wide used Gaussian
approach, so real MIMO performance much different for Gaussian a non-Gaussian
noise model."
"In this paper we provide the answer to the following question: Given a noisy
channel and epsilon>0, how many bits can be transmitted with an error of at
most epsilon by a single use of the channel?"
"In this paper, a unifying framework for orthogonal frequency division
multiplexing (OFDM) multiuser resource allocation is presented. The isolated
seeming problems of maximizing a weighted sum of rates for a given power budget
$\bar{P}$ and minimizing sum power for given rate requirements
$\mathbf{\bar{R}}$ can be interpreted jointly in this framework. To this end we
embed the problems in a higher dimensional space. Based on these results, we
subsequently consider the combined problem of maximizing a weighted sum of
rates under given rate requirements $\mathbf{\bar{R}}$ and a fixed power budget
$\bar{P}$. This new problem is challenging, since the additional constraints do
not allow to use the hitherto existing approaches. Interestingly, the optimal
decoding orders turn out to be the ordering of the Lagrangian factors in all
problems."
"A new approach for decoding binary linear codes by solving a linear program
(LP) over a relaxed codeword polytope was recently proposed by Feldman et al.
In this paper we investigate the structure of the polytope used in the LP
relaxation decoding. We begin by showing that for expander codes, every
fractional pseudocodeword always has at least a constant fraction of
non-integral bits. We then prove that for expander codes, the active set of any
fractional pseudocodeword is smaller by a constant fraction than the active set
of any codeword. We exploit this fact to devise a decoding algorithm that
provably outperforms the LP decoder for finite blocklengths. It proceeds by
guessing facets of the polytope, and resolving the linear program on these
facets. While the LP decoder succeeds only if the ML codeword has the highest
likelihood over all pseudocodewords, we prove that for expander codes the
proposed algorithm succeeds even with a constant number of pseudocodewords of
higher likelihood. Moreover, the complexity of the proposed algorithm is only a
constant factor larger than that of the LP decoder."
"This paper derives an improved sphere-packing (ISP) bound for finite-length
codes whose transmission takes place over symmetric memoryless channels. We
first review classical results, i.e., the 1959 sphere-packing (SP59) bound of
Shannon for the Gaussian channel, and the 1967 sphere-packing (SP67) bound of
Shannon et al. for discrete memoryless channels. A recent improvement on the
SP67 bound, as suggested by Valembois and Fossorier, is also discussed. These
concepts are used for the derivation of a new lower bound on the decoding error
probability (referred to as the ISP bound) which is uniformly tighter than the
SP67 bound and its recent improved version. The ISP bound is applicable to
symmetric memoryless channels, and some of its applications are exemplified.
Its tightness is studied by comparing it with bounds on the ML decoding error
probability, and computer simulations of iteratively decoded turbo-like codes.
The paper also presents a technique which performs the entire calculation of
the SP59 bound in the logarithmic domain, thus facilitating the exact
calculation of this bound for moderate to large block lengths without the need
for the asymptotic approximations provided by Shannon."
This submission has been withdrawn by the author.
"We consider multi-hop multiple access (MAC) and broadcast channels (BC) where
communication takes place with the assistance of relays that amplify and
forward (AF) their received signals. For a two hop parallel AF relay MAC,
assuming a sum power constraint across all relays we characterize optimal relay
amplification factors and the resulting capacity regions. We find that the
parallel AF relay MAC with total transmit power of the two users $P_1+P_2=P$
and total relay power $P_R$ is the dual of the parallel AF relay BC where the
MAC source nodes become the BC destination nodes, the MAC destination node
becomes the BC source node, the dual BC source transmit power is $P_R$ and the
total transmit power of the AF relays is $P$. The duality means that the
capacity region of the AF relay MAC with a sum power constraint $P$ on the
transmitters is the same as that of the dual BC. The duality relationship is
found to be useful in characterizing the capacity region of the AF relay BC as
the union of MAC capacity regions. The duality extends to distributed relays
with multiple antennas and more than 2 hops as well."
"We consider capacity of discrete-time channels with feedback for the general
case where the feedback is a time-invariant deterministic function of the
output samples. Under the assumption that the channel states take values in a
finite alphabet, we find an achievable rate and an upper bound on the capacity.
We further show that when the channel is indecomposable, and has no intersymbol
interference (ISI), its capacity is given by the limit of the maximum of the
(normalized) directed information between the input $X^N$ and the output $Y^N$,
i.e. $C = \lim_{N \to \infty} \frac{1}{N} \max I(X^N \to Y^N)$, where the
maximization is taken over the causal conditioning probability
$Q(x^N||z^{N-1})$ defined in this paper. The capacity result is used to show
that the source-channel separation theorem holds for time-invariant determinist
feedback. We also show that if the state of the channel is known both at the
encoder and the decoder then feedback does not increase capacity."
"This work considers the problem of communication from a single transmitter,
over a network with colocated users, through an independent block Rayleigh
fading channel. The colocation nature of the users allows cooperation, which
increases the overall achievable rate, from the transmitter to the destined
user. The transmitter is ignorant of the fading coefficients, while receivers
have access to perfect channel state information (CSI). This gives rise to the
multi-layer broadcast approach used by the transmitter. The broadcast approach
allows, in our network setting, to improve the cooperation between the
colocated users. That is due to the nature of broadcasting, where the better
the channel quality, the more layers that can be decoded. The cooperation
between the users is performed over an additive white Gaussian channels (AWGN),
with a relaying power constraint, and unlimited bandwidth. Three commonly used
cooperation techniques are studied: amplify-forward (AF), compress-forward
(CF), and decode-forward (DF). These methods are extended using the broadcast
approach, for the case of relaxed decoding delay constraint. For this case a
separated processing of the layers, which includes multi-session cooperation is
shown to be beneficial. Further, closed form expressions for infinitely many AF
sessions and recursive expressions for the more complex CF are given. Numerical
results for the various cooperation strategies demonstrate the efficiency of
multi-session cooperation."
"This paper considers the Linear Minimum Variance recursive state estimation
for the linear discrete time dynamic system with random state transition and
measurement matrices, i.e., random parameter matrices Kalman filtering. It is
shown that such system can be converted to a linear dynamic system with
deterministic parameter matrices but state-dependent process and measurement
noises. It is proved that under mild conditions, the recursive state estimation
of this system is still of the form of a modified Kalman filtering. More
importantly, this result can be applied to Kalman filtering with intermittent
and partial observations as well as randomly variant dynamic systems."
"This paper deals with throughput scaling laws for random ad-hoc wireless
networks in a rich scattering environment. We develop schemes to optimize the
ratio, $\rho(n)$ of achievable network sum capacity to the sum of the
point-to-point capacities of source-destinations pairs operating in isolation.
For fixed SNR networks, i.e., where the worst case SNR over the
source-destination pairs is fixed independent of $n$, we show that
collaborative strategies yield a scaling law of $\rho(n) = {\cal
O}(\frac{1}{n^{1/3}})$ in contrast to multi-hop strategies which yield a
scaling law of $\rho(n) = {\cal O}(\frac{1}{\sqrt{n}})$. While, networks where
worst case SNR goes to zero, do not preclude the possibility of collaboration,
multi-hop strategies achieve optimal throughput. The plausible reason is that
the gains due to collaboration cannot offset the effect of vanishing receive
SNR. This suggests that for fixed SNR networks, a network designer should look
for network protocols that exploit collaboration. The fact that most current
networks operate in a fixed SNR interference limited environment provides
further motivation for considering this regime."
"Code annealing, a new method of designing good codes of short block length,
is proposed, which is then concatenated with cyclic lifting to create finite
codes of low frame error rate (FER) error floors without performance outliers.
The stopping set analysis is performed on the cyclically lifted code ensemble
assuming uniformly random lifting sequences, and the suppressing effect/weight
of the cyclic lifting is identified for the first time, based on which the
ensemble FER error floor can be analytically determined and a scaling law is
derived. Both the first-order and high-order suppressing effects are discussed
and quantified by different methods including the explicit expression, an
algorithmic upper bound, and an algebraic lower bound.
  The mismatch between the suppressing weight and the stopping distances
explains the dramatic performance discrepancy among different cyclically lifted
codes when the underlying base codes have degree 2 variable nodes or not. For
the former case, a degree augmentation method is further introduced to mitigate
this metric mismatch, and a systematic method of constructing irregular codes
of low FER error floors is presented. Both regular and irregular codes of very
low FER error floors are reported, for which the improvement factor ranges from
10^6-10^4 when compared to the classic graph-based code ensembles."
"We apply two variations of the principle of Minimum Cross Entropy (the
Kullback information measure) to fit parameterized probability density models
to observed data densities. For an array beamforming problem with P incident
narrowband point sources, N > P sensors, and colored noise, both approaches
yield eigenvector fitting methods similar to that of the MUSIC algorithm[1].
Furthermore, the corresponding cross-entropies are related to the MDL model
order selection criterion[2]."
"This paper has been withdrawn by the author(s), due a crucial sign error in
Thm. 11."
"As a generalization of cyclic codes, quasi-cyclic (QC) codes contain many
good linear codes. But quasi-cyclic codes studied so far are mainly limited to
one generator (1-generator) QC codes. In this correspondence, 2-generator and
3-generator QC codes are studied, and many good, new QC codes are constructed
from simplex codes. Some new binary QC codes or related codes, that improve the
bounds on maximum minimum distance for binary linear codes are constructed.
They are 5-generator QC [93, 17, 34] and [254, 23, 102] codes, and related [96,
17, 36], [256, 23, 104] codes."
"This paper describes an efficient implementation of binning for the relay
channel using low-density parity-check (LDPC) codes. We devise bilayer LDPC
codes to approach the theoretically promised rate of the decode-and-forward
relaying strategy by incorporating relay-generated information bits in
specially designed bilayer graphical code structures. While conventional LDPC
codes are sensitively tuned to operate efficiently at a certain channel
parameter, the proposed bilayer LDPC codes are capable of working at two
different channel parameters and two different rates: that at the relay and at
the destination. To analyze the performance of bilayer LDPC codes, bilayer
density evolution is devised as an extension of the standard density evolution
algorithm. Based on bilayer density evolution, a design methodology is
developed for the bilayer codes in which the degree distribution is iteratively
improved using linear programming. Further, in order to approach the
theoretical decode-and-forward rate for a wide range of channel parameters,
this paper proposes two different forms bilayer codes, the bilayer-expurgated
and bilayer-lengthened codes. It is demonstrated that a properly designed
bilayer LDPC code can achieve an asymptotic infinite-length threshold within
0.24 dB gap to the Shannon limits of two different channels simultaneously for
a wide range of channel parameters. By practical code construction,
finite-length bilayer codes are shown to be able to approach within a 0.6 dB
gap to the theoretical decode-and-forward rate of the relay channel at a block
length of $10^5$ and a bit-error probability (BER) of $10^{-4}$. Finally, it is
demonstrated that a generalized version of the proposed bilayer code
construction is applicable to relay networks with multiple relays."
"Expressions for (EPI Shannon type) Divergence-Power Inequalities (DPI) in two
cases (time-discrete and band-limited time-continuous) of stationary random
processes are given. The new expressions connect the divergence rate of the sum
of independent processes, the individual divergence rate of each process, and
their power spectral densities. All divergences are between a process and a
Gaussian process with same second order statistics, and are assumed to be
finite. A new proof of the Shannon entropy-power inequality EPI, based on the
relationship between divergence and causal minimum mean-square error (CMMSE) in
Gaussian channels with large signal-to-noise ratio, is also shown."
"In this paper we address the problem of closed-form spectral evaluation of
CPM. We show that the multi-h CPM signal can be conveniently generated by a PTI
SM. The output is governed by a Markov chain with the unusual peculiarity of
being cyclostationary and reducible; this holds also in the single-h context.
Judicious reinterpretation of the result leads to a formalization through a
stationary and irreducible Markov chain, whose spectral evaluation is known in
closed-form from the literature. Two are the major outcomes of this paper.
First, unlike the literature, we obtain a PSD in true closed-form. Second, we
give novel insights into the CPM format."
"High data-rate Distributed Orthogonal Space-Time Block Codes (DOSTBCs) which
achieve the single-symbol decodability and full diversity order are proposed in
this paper. An upper bound of the data-rate of the DOSTBC is derived and it is
approximately twice larger than that of the conventional repetition-based
cooperative strategy. In order to facilitate the systematic constructions of
the DOSTBCs achieving the upper bound of the data-rate, some special DOSTBCs,
which have diagonal noise covariance matrices at the destination terminal, are
investigated. These codes are referred to as the row-monomial DOSTBCs. An upper
bound of the data-rate of the row-monomial DOSTBC is derived and it is equal to
or slightly smaller than that of the DOSTBC. Lastly, the systematic
construction methods of the row-monomial DOSTBCs achieving the upper bound of
the data-rate are presented."
"We present simple coding strategies, which are variants of the
Schalkwijk-Kailath scheme, for communicating reliably over additive white noise
channels in the presence of corrupted feedback. More specifically, we consider
a framework comprising an additive white forward channel and a backward link
which is used for feedback. We consider two types of corruption mechanisms in
the backward link. The first is quantization noise, i.e., the encoder receives
the quantized values of the past outputs of the forward channel. The
quantization is uniform, memoryless and time invariant (that is,
symbol-by-symbol scalar quantization), with bounded quantization error. The
second corruption mechanism is an arbitrarily distributed additive bounded
noise in the backward link. Here we allow symbol-by-symbol encoding at the
input to the backward channel. We propose simple explicit schemes that
guarantee positive information rate, in bits per channel use, with positive
error exponent. If the forward channel is additive white Gaussian then our
schemes achieve capacity, in the limit of diminishing amplitude of the noise
components at the backward link, while guaranteeing that the probability of
error converges to zero as a doubly exponential function of the block length.
Furthermore, if the forward channel is additive white Gaussian and the backward
link consists of an additive bounded noise channel, with signal-to-noise ratio
(SNR) constrained symbol-by-symbol encoding, then our schemes are also
capacity-achieving in the limit of high SNR."
This paper has been withdrawn by the author
"Asymptotic spectral distribution (ASD) of the crosscorrelation matrix is
investigated for a random spreading short/long-code asynchronous direct
sequence-code division multiple access (DS-CDMA) system. The discrete-time
decision statistics are obtained as the output samples of a bank of symbol
matched filters of all users. The crosscorrelation matrix is studied when the
number of symbols transmitted by each user tends to infinity. Two levels of
asynchronism are considered. One is symbol-asynchronous but chip-synchronous,
and the other is chip-asynchronous. The existence of a nonrandom ASD is proved
by moment convergence theorem, where the focus is on the derivation of
asymptotic eigenvalue moments (AEM) of the crosscorrelation matrix. A
combinatorics approach based on noncrossing partition of set partition theory
is adopted for AEM computation. The spectral efficiency and the minimum
mean-square-error (MMSE) achievable by a linear receiver of asynchronous CDMA
are plotted by AEM using a numerical method."
"The normalized min-sum algorithm can achieve near-optimal performance at
decoding LDPC codes. However, it is a critical question to understand the
mathematical principle underlying the algorithm. Traditionally, people thought
that the normalized min-sum algorithm is a good approximation to the
sum-product algorithm, the best known algorithm for decoding LDPC codes and
Turbo codes. This paper offers an alternative approach to understand the
normalized min-sum algorithm. The algorithm is derived directly from
cooperative optimization, a newly discovered general method for
global/combinatorial optimization. This approach provides us another
theoretical basis for the algorithm and offers new insights on its power and
limitation. It also gives us a general framework for designing new decoding
algorithms."
"In this paper, we present a fast min-sum algorithm for decoding LDPC codes
over GF(q). Our algorithm is different from the one presented by David Declercq
and Marc Fossorier in ISIT 05 only at the way of speeding up the horizontal
scan in the min-sum algorithm. The Declercq and Fossorier's algorithm speeds up
the computation by reducing the number of configurations, while our algorithm
uses the dynamic programming instead. Compared with the configuration reduction
algorithm, the dynamic programming one is simpler at the design stage because
it has less parameters to tune. Furthermore, it does not have the performance
degradation problem caused by the configuration reduction because it searches
the whole configuration space efficiently through dynamic programming. Both
algorithms have the same level of complexity and use simple operations which
are suitable for hardware implementations."
"Many implementations for decoding LDPC codes are based on the
(normalized/offset) min-sum algorithm due to its satisfactory performance and
simplicity in operations. Usually, each iteration of the min-sum algorithm
contains two scans, the horizontal scan and the vertical scan. This paper
presents a single-scan version of the min-sum algorithm to speed up the
decoding process. It can also reduce memory usage or wiring because it only
needs the addressing from check nodes to variable nodes while the original
min-sum algorithm requires that addressing plus the addressing from variable
nodes to check nodes. To cut down memory usage or wiring further, another
version of the single-scan min-sum algorithm is presented where the messages of
the algorithm are represented by single bit values instead of using fixed point
ones. The software implementation has shown that the single-scan min-sum
algorithm is more than twice as fast as the original min-sum algorithm."
"This paper derives an improved sphere-packing (ISP) bound targeting codes of
short to moderate block lengths. We first review the 1967 sphere-packing (SP67)
bound for discrete memoryless channels, and a recent improvement by Valembois
and Fossorier. These concepts are used for the derivation of a new lower bound
on the decoding error probability (referred to as the ISP bound) which is
uniformly tighter than the SP67 bound and its recent improved version. Under a
mild condition, the ISP bound is applicable to general memoryless channels, and
some of its applications are exemplified. Its tightness is studied by comparing
it with bounds on the ML decoding error probability. It is exemplified that the
ISP bound suggests an interesting alternative to the 1959 sphere-packing (SP59)
bound of Shannon for the Gaussian channel, especially for digital modulations
of high spectral efficiency."
"This paper is focused on the performance analysis of binary linear block
codes (or ensembles) whose transmission takes place over independent and
memoryless parallel channels. New upper bounds on the maximum-likelihood (ML)
decoding error probability are derived. The framework of the second version of
the Duman and Salehi (DS2) bounds is generalized to the case of parallel
channels, along with the derivation of optimized tilting measures. The
connection between the generalized DS2 and the 1961 Gallager bounds, known
previously for a single channel, is revisited for the case of parallel
channels. The new bounds are used to obtain improved inner bounds on the
attainable channel regions under ML decoding. These improved bounds are applied
to ensembles of turbo-like codes, focusing on repeat-accumulate codes and their
recent variations."
"We propose a new low-density parity-check code construction scheme based on
2-lifts. The proposed codes have an advantage of admitting efficient hardware
implementations. With the motivation of designing codes with low error floors,
we present an analysis of the low-weight stopping set distributions of the
proposed codes. Based on this analysis, we propose design criteria for
designing codes with low error floors. Numerical results show that the
resulting codes have low error probabilities over binary erasure channels."
"Classical rate-distortion theory requires knowledge of an elusive source
distribution. Instead, we analyze rate-distortion properties of individual
objects using the recently developed algorithmic rate-distortion theory. The
latter is based on the noncomputable notion of Kolmogorov complexity. To apply
the theory we approximate the Kolmogorov complexity by standard data
compression techniques, and perform a number of experiments with lossy
compression and denoising of objects from different domains. We also introduce
a natural generalization to lossy compression with side information. To
maintain full generality we need to address a difficult searching problem.
While our solutions are therefore not time efficient, we do observe good
denoising and compression performance."
"We consider a general multiple antenna network with multiple sources,
multiple destinations and multiple relays in terms of the
diversity-multiplexing tradeoff (DMT). We examine several subcases of this most
general problem taking into account the processing capability of the relays
(half-duplex or full-duplex), and the network geometry (clustered or
non-clustered). We first study the multiple antenna relay channel with a
full-duplex relay to understand the effect of increased degrees of freedom in
the direct link. We find DMT upper bounds and investigate the achievable
performance of decode-and-forward (DF), and compress-and-forward (CF)
protocols. Our results suggest that while DF is DMT optimal when all terminals
have one antenna each, it may not maintain its good performance when the
degrees of freedom in the direct link is increased, whereas CF continues to
perform optimally. We also study the multiple antenna relay channel with a
half-duplex relay. We show that the half-duplex DMT behavior can significantly
be different from the full-duplex case. We find that CF is DMT optimal for
half-duplex relaying as well, and is the first protocol known to achieve the
half-duplex relay DMT. We next study the multiple-access relay channel (MARC)
DMT. Finally, we investigate a system with a single source-destination pair and
multiple relays, each node with a single antenna, and show that even under the
idealistic assumption of full-duplex relays and a clustered network, this
virtual multi-input multi-output (MIMO) system can never fully mimic a real
MIMO DMT. For cooperative systems with multiple sources and multiple
destinations the same limitation remains to be in effect."
"In the design of multiple description lattice vector quantizers (MDLVQ),
index assignment plays a critical role. In addition, one also needs to choose
the Voronoi cell size of the central lattice v, the sublattice index N, and the
number of side descriptions K to minimize the expected MDLVQ distortion, given
the total entropy rate of all side descriptions Rt and description loss
probability p. In this paper we propose a linear-time MDLVQ index assignment
algorithm for any K >= 2 balanced descriptions in any dimensions, based on a
new construction of so-called K-fraction lattice. The algorithm is greedy in
nature but is proven to be asymptotically (N -> infinity) optimal for any K >=
2 balanced descriptions in any dimensions, given Rt and p. The result is
stronger when K = 2: the optimality holds for finite N as well, under some mild
conditions. For K > 2, a local adjustment algorithm is developed to augment the
greedy index assignment, and conjectured to be optimal for finite N.
  Our algorithmic study also leads to better understanding of v, N and K in
optimal MDLVQ design. For K = 2 we derive, for the first time, a
non-asymptotical closed form expression of the expected distortion of optimal
MDLVQ in p, Rt, N. For K > 2, we tighten the current asymptotic formula of the
expected distortion, relating the optimal values of N and K to p and Rt more
precisely."
"We refine and extend an earlier MDL denoising criterion for wavelet-based
denoising. We start by showing that the denoising problem can be reformulated
as a clustering problem, where the goal is to obtain separate clusters for
informative and non-informative wavelet coefficients, respectively. This
suggests two refinements, adding a code-length for the model index, and
extending the model in order to account for subband-dependent coefficient
distributions. A third refinement is derivation of soft thresholding inspired
by predictive universal coding with weighted mixtures. We propose a practical
method incorporating all three refinements, which is shown to achieve good
performance and robustness in denoising both artificial and natural signals."
"We introduce a general framework for treating channels with memory and
feedback. First, we generalize Massey's concept of directed information and use
it to characterize the feedback capacity of general channels. Second, we
present coding results for Markov channels. This requires determining
appropriate sufficient statistics at the encoder and decoder. Third, a dynamic
programming framework for computing the capacity of Markov channels is
presented. Fourth, it is shown that the average cost optimality equation (ACOE)
can be viewed as an implicit single-letter characterization of the capacity.
Fifth, scenarios with simple sufficient statistics are described."
"An elementary combinatorial Tanner graph construction for a family of
near-regular low density parity check codes achieving high girth is presented.
The construction allows flexibility in the choice of design parameters like
rate, average degree, girth and block length of the code and yields an
asymptotic family. The complexity of constructing codes in the family grows
only quadratically with the block length."
"Message-passing iterative decoders for low-density parity-check (LDPC) block
codes are known to be subject to decoding failures due to so-called
pseudo-codewords. These failures can cause the large signal-to-noise ratio
performance of message-passing iterative decoding to be worse than that
predicted by the maximum-likelihood decoding union bound. In this paper we
address the pseudo-codeword problem from the convolutional-code perspective. In
particular, we compare the performance of LDPC convolutional codes with that of
their ``wrapped'' quasi-cyclic block versions and we show that the minimum
pseudo-weight of an LDPC convolutional code is at least as large as the minimum
pseudo-weight of an underlying quasi-cyclic code. This result, which parallels
a well-known relationship between the minimum Hamming weight of convolutional
codes and the minimum Hamming weight of their quasi-cyclic counterparts, is due
to the fact that every pseudo-codeword in the convolutional code induces a
pseudo-codeword in the block code with pseudo-weight no larger than that of the
convolutional code's pseudo-codeword. This difference in the weight spectra
leads to improved performance at low-to-moderate signal-to-noise ratios for the
convolutional code, a conclusion supported by simulation results."
"We present a novel iterative algorithm for detection of binary Markov random
fields (MRFs) corrupted by two-dimensional (2D) intersymbol interference (ISI)
and additive white Gaussian noise (AWGN). We assume a first-order binary MRF as
a simple model for correlated images. We assume a 2D digital storage channel,
where the MRF is interleaved before being written and then read by a 2D
transducer; such channels occur in recently proposed optical disk storage
systems. The detection algorithm is a concatenation of two
soft-input/soft-output (SISO) detectors: an iterative row-column soft-decision
feedback (IRCSDF) ISI detector, and a MRF detector. The MRF detector is a SISO
version of the stochastic relaxation algorithm by Geman and Geman in IEEE
Trans. Pattern Anal. and Mach. Intell., Nov. 1984. On the 2 x 2 averaging-mask
ISI channel, at a bit error rate (BER) of 10^{-5}, the concatenated algorithm
achieves SNR savings of between 0.5 and 2.0 dB over the IRCSDF detector alone;
the savings increase as the MRFs become more correlated, or as the SNR
decreases. The algorithm is also fairly robust to mismatches between the
assumed and actual MRF parameters."
"Construction of signal sets with low correlation property is of interest to
designers of CDMA systems. One of the preferred ways of constructing such sets
is the interleaved construction which uses two sequences a and b with 2-level
autocorrelation and a shift sequence e. The shift sequence has to satisfy
certain conditions for the resulting signal set to have low correlation
properties. This article shows that the conditions reported in literature are
too strong and gives a version which results in more number of shift sequences.
An open problem on the existence of shift sequences for attaining an
interleaved set with maximum correlation value bounded by v+2 is also taken up
and solved."
"We consider transmission over the ergodic fading multi-antenna broadcast
(MIMO-BC) channel with partial channel state information at the transmitter and
full information at the receiver. Over the equivalent {\it non}-fading channel,
capacity has recently been shown to be achievable using transmission schemes
that were designed for the ``dirty paper'' channel. We focus on a similar
``fading paper'' model. The evaluation of the fading paper capacity is
difficult to obtain. We confine ourselves to the {\it linear-assignment}
capacity, which we define, and use convex analysis methods to prove that its
maximizing distribution is Gaussian. We compare our fading-paper transmission
to an application of dirty paper coding that ignores the partial state
information and assumes the channel is fixed at the average fade. We show that
a gain is easily achieved by appropriately exploiting the information. We also
consider a cooperative upper bound on the sum-rate capacity as suggested by
Sato. We present a numeric example that indicates that our scheme is capable of
realizing much of this upper bound."
"This paper presents the first concerted look at low correlation sequence
families over QAM constellations of size M^2=4^m and their potential
applicability as spreading sequences in a CDMA setting.
  Five constructions are presented, and it is shown how such sequence families
have the ability to transport a larger amount of data as well as enable
variable-rate signalling on the reverse link.
  Canonical family CQ has period N, normalized maximum-correlation parameter
theta_max bounded above by A sqrt(N), where 'A' ranges from 1.8 in the 16-QAM
case to 3.0 for large M. In a CDMA setting, each user is enabled to transfer 2m
bits of data per period of the spreading sequence which can be increased to 3m
bits of data by halving the size of the sequence family. The technique used to
construct CQ is easily extended to produce larger sequence families and an
example is provided.
  Selected family SQ has a lower value of theta_max but permits only (m+1)-bit
data modulation. The interleaved 16-QAM sequence family IQ has theta_max <=
sqrt(2) sqrt(N) and supports 3-bit data modulation.
  The remaining two families are over a quadrature-PAM (Q-PAM) subset of size
2M of the M^2-QAM constellation. Family P has a lower value of theta_max in
comparison with Family SQ, while still permitting (m+1)-bit data modulation.
Interleaved family IP, over the 8-ary Q-PAM constellation, permits 3-bit data
modulation and interestingly, achieves the Welch lower bound on theta_max."
"We provide a single-letter characterization for the capacity region of a
class of discrete degraded interference channels (DDICs). The class of DDICs
considered includes the discrete additive degraded interference channel (DADIC)
studied by Benzel. We show that for the class of DDICs studied, encoder
cooperation does not increase the capacity region, and therefore, the capacity
region of the class of DDICs is the same as the capacity region of the
corresponding degraded broadcast channel."
"We establish that the feedback capacity of the trapdoor channel is the
logarithm of the golden ratio and provide a simple communication scheme that
achieves capacity. As part of the analysis, we formulate a class of dynamic
programs that characterize capacities of unifilar finite-state channels. The
trapdoor channel is an instance that admits a simple analytic solution."
"This paper focuses on finite-dimensional upper and lower bounds on decodable
thresholds of Zm and binary low-density parity-check (LDPC) codes, assuming
belief propagation decoding on memoryless channels. A concrete framework is
presented, admitting systematic searches for new bounds. Two noise measures are
considered: the Bhattacharyya noise parameter and the soft bit value for a
maximum a posteriori probability (MAP) decoder on the uncoded channel. For Zm
LDPC codes, an iterative m-dimensional bound is derived for
m-ary-input/symmetric-output channels, which gives a sufficient stability
condition for Zm LDPC codes and is complemented by a matched necessary
stability condition introduced herein. Applications to coded modulation and to
codes with non-equiprobable distributed codewords are also discussed.
  For binary codes, two new lower bounds are provided for symmetric channels,
including a two-dimensional iterative bound and a one-dimensional non-iterative
bound, the latter of which is the best known bound that is tight for binary
symmetric channels (BSCs), and is a strict improvement over the bound derived
by the channel degradation argument. By adopting the reverse channel
perspective, upper and lower bounds on the decodable Bhattacharyya noise
parameter are derived for non-symmetric channels, which coincides with the
existing bound for symmetric channels."
"In this work, the delay limited capacity (DLC) of orthogonal frequency
division multiplexing (OFDM) systems is investigated. The analysis is organized
into two parts. In the first part, the impact of system parameters on the OFDM
DLC is analyzed in a general setting. The main results are that under weak
assumptions the maximum achievable single user DLC is almost independent of the
distribution of the path attenuations in the low signal-to-noise (SNR) region
but depends strongly on the delay spread. In the high SNR region the roles are
exchanged. Here, the impact of delay spread is negligible while the impact of
the distribution becomes dominant. The relevant asymptotic quantities are
derived without employing simplifying assumptions on the OFDM correlation
structure. Moreover, for both cases it is shown that the DLC is maximized if
the total channel energy is uniformly spread, i.e. the power delay profile is
uniform. It is worth pointing out that since universal bounds are obtained the
results can also be used for other classes of parallel channels with block
fading characteristic. The second part extends the setting to the broadcast
channel and studies the corresponding OFDM DLC BC region. An algorithm for
computing the OFDM BC DLC region is presented. To derive simple but smart
resource allocation strategies, the principle of rate water-filling employing
order statistics is introduced. This yields analytical lower bounds on the OFDM
DLC region based on orthogonal frequency division multiple access (OFDMA) and
ordinal channel state information (CSI). Finally, the schemes are compared to
an algorithm using full CSI."
"Interleaved Reed-Solomon codes are applied in numerous data processing, data
transmission, and data storage systems. They are generated by interleaving
several codewords of ordinary Reed-Solomon codes. Usually, these codewords are
decoded independently by classical algebraic decoding methods. However, by
collaborative algebraic decoding approaches, such interleaved schemes allow the
correction of error patterns beyond half the minimum distance, provided that
the errors in the received signal occur in bursts. In this work, collaborative
decoding of interleaved Reed-Solomon codes by multi-sequence shift-register
synthesis is considered and analyzed. Based on the framework of interleaved
Reed-Solomon codes, concatenated code designs are investigated, which are
obtained by interleaving several Reed-Solomon codes, and concatenating them
with an inner block code."
"Block diagonalization is a linear precoding technique for the multiple
antenna broadcast (downlink) channel that involves transmission of multiple
data streams to each receiver such that no multi-user interference is
experienced at any of the receivers. This low-complexity scheme operates only a
few dB away from capacity but does require very accurate channel knowledge at
the transmitter, which can be very difficult to obtain in fading scenarios. We
consider a limited feedback system where each receiver knows its channel
perfectly, but the transmitter is only provided with a finite number of channel
feedback bits from each receiver. Using a random vector quantization argument,
we quantify the throughput loss due to imperfect channel knowledge as a
function of the feedback level. The quality of channel knowledge must improve
proportional to the SNR in order to prevent interference-limitations, and we
show that scaling the number of feedback bits linearly with the system SNR is
sufficient to maintain a bounded rate loss. Finally, we investigate a simple
scalar quantization scheme that is seen to achieve the same scaling behavior as
vector quantization."
"An enhanced covering lemma for a Markov chain is proved in this paper, and
then the distributed source coding problem of correlated general sources with
one average distortion criterion under fixed-length coding is investigated.
Based on the enhanced lemma, a sufficient and necessary condition for
determining the achievability of rate-distortion triples is given."
"Suggested the decision of the network cranback protocols performance
analyzing problem from Eyal Felstine, Reuven Cohen and Ofer Hadar, "" Crankback
Prediction in Hierarchical ATM networks"", Journal of Network and Systems
Management, Vol. 10, No. 3, September 2002. It show that the false alarm
probability and probability of successful way crossing can be calculated. The
main optimization equations are developed for cranback protocol parameters by
using analytical expressions for statistical protocol characteristics."
"It is impossible to provide an effective utilization of communication
networks without the analysis of the quantitative characteristics of the
traffic in real time. The constant supervision of all channels of the data
practically is impracticable because requires transfer of the significant
additional information on a network and large resources expenses for devices of
the control. Thus, the task on traffic estimation with small expenses in real
time is the urgent."
"In this paper, we study properties of rank metric codes in general and
maximum rank distance (MRD) codes in particular. For codes with the rank
metric, we first establish Gilbert and sphere-packing bounds, and then obtain
the asymptotic forms of these two bounds and the Singleton bound. Based on the
asymptotic bounds, we observe that asymptotically Gilbert-Varsharmov bound is
exceeded by MRD codes and sphere-packing bound cannot be attained. We also
establish bounds on the rank covering radius of maximal codes, and show that
all MRD codes are maximal codes and all the MRD codes known so far achieve the
maximum rank covering radius."
"It is believed that quantum communication is not possible with a pure
ensemble of states because quantum entropy of pure state is zero. This is
indeed possible due to geometric consequence of entanglement."
"We consider the secure transmission of information over an ergodic fading
channel in the presence of an eavesdropper. Our eavesdropper can be viewed as
the wireless counterpart of Wyner's wiretapper. The secrecy capacity of such a
system is characterized under the assumption of asymptotically long coherence
intervals. We first consider the full Channel State Information (CSI) case,
where the transmitter has access to the channel gains of the legitimate
receiver and the eavesdropper. The secrecy capacity under this full CSI
assumption serves as an upper bound for the secrecy capacity when only the CSI
of the legitimate receiver is known at the transmitter, which is characterized
next. In each scenario, the perfect secrecy capacity is obtained along with the
optimal power and rate allocation strategies. We then propose a low-complexity
on/off power allocation strategy that achieves near-optimal performance with
only the main channel CSI. More specifically, this scheme is shown to be
asymptotically optimal as the average SNR goes to infinity, and interestingly,
is shown to attain the secrecy capacity under the full CSI assumption.
Remarkably, our results reveal the positive impact of fading on the secrecy
capacity and establish the critical role of rate adaptation, based on the main
channel CSI, in facilitating secure communications over slow fading channels."
"A cross-layer optimization approach is adopted for the design of symmetric
random access wireless systems. Instead of the traditional collision model, a
more realistic physical layer model is considered. Based on this model, an
Incremental Redundancy Automatic Repeat reQuest (IR-ARQ) scheme, tailored to
jointly combat the effects of collisions, multi-path fading, and additive
noise, is developed. The Diversity-Multiplexing-Delay tradeoff (DMDT) of the
proposed scheme is analyzed for fully-loaded queues, and compared with that of
Gallager tree algorithm for collision resolution and the network-assisted
diversity multiple access (NDMA) protocol of Tsatsanis et al.. The fully-loaded
queue model is then replaced by one with random arrivals, under which these
protocols are compared in terms of the stability region, average delay and
diversity gain. Overall, our analytical and numerical results establish the
superiority of the proposed IR-ARQ scheme and reveal some important insights.
For example, it turns out that the performance is optimized, for a given total
throughput, by maximizing the probability that a certain user sends a new
packet and minimizing the transmission rate employed by each user."
"We consider communication over Automatic Repeat reQuest (ARQ) memoryless
channels with deadlines. In particular, an upper bound L is imposed on the
maximum number of ARQ transmission rounds. In this setup, it is shown that
incremental redundancy ARQ outperforms Forney's memoryless decoding in terms of
the achievable error exponents."
"In this paper, we consider the discrete memoryless interference channel with
common information, in which two senders need deliver not only private messages
but also certain common messages to their corresponding receivers. We derive an
achievable rate region for such a channel by exploiting a random coding
strategy, namely cascaded superposition coding. We reveal that the derived
achievable rate region generalizes some important existing results for the
interference channels with or without common information. Furthermore, we
specialize to a class of deterministic interference channels with common
information, and show that the derived achievable rate region is indeed the
capacity region for this class of channels."
"A general lossless joint source-channel coding scheme based on linear codes
is proposed and then analyzed in this paper. It is shown that a linear code
with good joint spectrum can be used to establish limit-approaching joint
source-channel coding schemes for arbitrary sources and channels, where the
joint spectrum of the code is a generalization of the input-output weight
distribution."
"An achievable rate region for the Gaussian interference channel is derived
using Sato's modified frequency division multiplexing idea and a special case
of Han and Kobayashi's rate region (denoted by $\Gmat^\prime$). We show that
the new inner bound includes $\Gmat^\prime$, Sason's rate region $\Dmat$, as
well as the achievable region via TDM/FDM, as its subsets. The advantage of
this improved inner bound over $\Gmat^\prime$ arises due to its inherent
ability to utilize the whole transmit power range on the real line without
violating the power constraint. We also provide analysis to examine the
conditions for the new achievable region to strictly extend $\Gmat^\prime$."
"Random coding, expurgated and sphere packing bounds are derived by method of
types and method of graph decomposition for $E$-capacity of discrete memoryless
channel (DMC). Three decoding rules are considered, the random coding bound is
attainable by each of the three rules, but the expurgated bound is achievable
only by maximum-likelihood decoding. Sphere packing bound is obtained by very
simple combinatorial reasonings of the method of types. The paper joins and
reviews the results of previous hard achievable publications."
"For output-symmetric DMCs at even moderately high rates, fixed-block-length
communication systems show no improvements in their error exponents with
feedback. In this paper, we study systems with fixed end-to-end delay and show
that feedback generally provides dramatic gains in the error exponents.
  A new upper bound (the uncertainty-focusing bound) is given on the
probability of symbol error in a fixed-delay communication system with
feedback. This bound turns out to have a similar form to Viterbi's bound used
for the block error probability of convolutional codes as a function of the
fixed constraint length. The uncertainty-focusing bound is shown to be
asymptotically achievable with noiseless feedback for erasure channels as well
as any output-symmetric DMC that has strictly positive zero-error capacity.
Furthermore, it can be achieved in a delay-universal (anytime) fashion even if
the feedback itself is delayed by a small amount. Finally, it is shown that for
end-to-end delay, it is generally possible at high rates to beat the
sphere-packing bound for general DMCs -- thereby providing a counterexample to
a conjecture of Pinsker."
"The sphere-packing bound $E_{sp}(R)$ bounds the reliability function for
fixed-length block-codes. For symmetric channels, it remains a valid bound even
when strictly causal noiseless feedback is allowed from the decoder to the
encoder. To beat the bound, the problem must be changed. While it has long been
known that variable-length block codes can do better when trading-off error
probability with expected block-length, this correspondence shows that the {\em
fixed-delay} setting also presents such an opportunity for generic channels.
  While $E_{sp}(R)$ continues to bound the tradeoff between bit error and fixed
end-to-end latency for symmetric channels used {\em without} feedback, a new
bound called the ``focusing bound'' gives the limits on what can be done with
feedback. If low-rate reliable flow-control is free (ie. the noisy channel has
strictly positive zero-error capacity), then the focusing bound can be
asymptotically achieved. Even when the channel has no zero-error capacity, it
is possible to substantially beat the sphere-packing bound by synthesizing an
appropriately reliable channel to carry the flow-control information."
"When designing a distributed control system, the system designer has a choice
in how to connect the different units through communication channels. In
practice, noiseless and noisy channels may coexist. Using the standard toy
example of scalar stabilization, this paper shows how a small amount of
noiseless feedback can perform a ``supervisory'' role and thereby boost the
effectiveness of noisy feedback."
"Shannon proved that if we can transmit bits reliably at rates larger than the
rate distortion function $R(D)$, then we can transmit this source to within a
distortion $D$. We answer the converse question ``If we can transmit a source
to within a distortion $D$, can we transmit bits reliably at rates less than
the rate distortion function?'' in the affirmative. This can be viewed as a
direct converse of the rate distortion theorem."
"Our understanding of information in systems has been based on the foundation
of memoryless processes. Extensions to stable Markov and auto-regressive
processes are classical. Berger proved a source coding theorem for the
marginally unstable Wiener process, but the infinite-horizon exponentially
unstable case has been open since Gray's 1970 paper. There were also no
theorems showing what is needed to communicate such processes across noisy
channels.
  In this work, we give a fixed-rate source-coding theorem for the
infinite-horizon problem of coding an exponentially unstable Markov process.
The encoding naturally results in two distinct bitstreams that have
qualitatively different QoS requirements for communicating over a noisy medium.
The first stream captures the information that is accumulating within the
nonstationary process and requires sufficient anytime reliability from the
channel used to communicate the process. The second stream captures the
historical information that dissipates within the process and is essentially
classical. This historical information can also be identified with a natural
stable counterpart to the unstable process. A converse demonstrating the
fundamentally layered nature of unstable sources is given by means of
information-embedding ideas."
"Distributed source coding is traditionally viewed in the block coding context
-- all the source symbols are known in advance at the encoders. This paper
instead considers a streaming setting in which iid source symbol pairs are
revealed to the separate encoders in real time and need to be reconstructed at
the decoder with some tolerable end-to-end delay using finite rate noiseless
channels. A sequential random binning argument is used to derive a lower bound
on the error exponent with delay and show that both ML decoding and universal
decoding achieve the same positive error exponents inside the traditional
Slepian-Wolf rate region. The error events are different from the block-coding
error events and give rise to slightly different exponents. Because the
sequential random binning scheme is also universal over delays, the resulting
code eventually reconstructs every source symbol correctly with probability 1."
"In a remarkable paper published in 1976, Burnashev determined the reliability
function of variable-length block codes over discrete memoryless channels with
feedback. Subsequently, an alternative achievability proof was obtained by
Yamamoto and Itoh via a particularly simple and instructive scheme. Their idea
is to alternate between a communication and a confirmation phase until the
receiver detects the codeword used by the sender to acknowledge that the
message is correct. We provide a converse that parallels the Yamamoto-Itoh
achievability construction. Besides being simpler than the original, the
proposed converse suggests that a communication and a confirmation phase are
implicit in any scheme for which the probability of error decreases with the
largest possible exponent. The proposed converse also makes it intuitively
clear why the terms that appear in Burnashev's exponent are necessary."
"In part I, we reviewed how Shannon's classical notion of capacity is not
sufficient to characterize a noisy communication channel if the channel is
intended to be used as part of a feedback loop to stabilize an unstable scalar
linear system. While classical capacity is not enough, a sense of capacity
(parametrized by reliability) called ""anytime capacity"" is both necessary and
sufficient for channel evaluation in this context. The rate required is the log
of the open-loop system gain and the required reliability comes from the
desired sense of stability. Sufficiency is maintained even in cases with noisy
observations and without any explicit feedback between the observer and the
controller. This established the asymptotic equivalence between scalar
stabilization problems and delay-universal communication problems with
feedback.
  Here in part II, the vector-state generalizations are established and it is
the magnitudes of the unstable eigenvalues that play an essential role. To deal
with such systems, the concept of the anytime rate-region is introduced. This
is the region of rates that the channel can support while still meeting
potentially different anytime reliability targets for parallel message streams.
All the scalar results generalize on an eigenvalue by eigenvalue basis. When
there is no explicit feedback of the noisy channel outputs, the intrinsic delay
of the unstable system tells us what the feedback delay needs to be while
evaluating the anytime-rate-region for the channel. An example involving a
binary erasure channel is used to illustrate how differentiated service is
required in any separation-based control architecture."
"In this paper, we first introduce the concept of elementary linear subspace,
which has similar properties to those of a set of coordinates. Using this new
concept, we derive properties of maximum rank distance (MRD) codes that
parallel those of maximum distance separable (MDS) codes. Using these
properties, we show that the decoder error probability of MRD codes with error
correction capability t decreases exponentially with t^2 based on the
assumption that all errors with the same rank are equally likely. We argue that
the channel based on this assumption is an approximation of a channel corrupted
by crisscross errors."
"The problem of many hypotheses logarithmically asymptotically optimal (LAO)
testing for a model consisting of three or more independent objects is solved.
It is supposed that $M$ probability distributions are known and each object
independently of others follows to one of them. The matrix of asymptotic
interdependencies (reliability--reliability functions) of all possible pairs of
the error probability exponents (reliabilities) in optimal testing for this
model is studied.
  This problem was introduced (and solved for the case of two objects and two
given probability distributions) by Ahlswede and Haroutunian. The model with
two independent objects with $M$ hypotheses was explored by Haroutunian and
Hakobyan."
"It is well known that orthogonal coding can be used to approach the Shannon
capacity of the power-constrained AWGN channel without a bandwidth constraint.
This correspondence describes a semi-orthogonal variation of pulse position
modulation that is sequential in nature -- bits can be ``streamed across''
without having to buffer up blocks of bits at the transmitter. ML decoding
results in an exponentially small probability of error as a function of
tolerated receiver delay and thus eventually a zero probability of error on
every transmitted bit. In the high-rate regime, a matching upper bound is given
on the delay error exponent. We close with some comments on the case with
feedback and the connections to the capacity per unit cost problem."
"Since many real-world problems arising in the fields of compiler
optimisation, automated software engineering, formal proof systems, and so
forth are equivalent to the Halting Problem--the most notorious undecidable
problem--there is a growing interest, not only academically, in understanding
the problem better and in providing alternative solutions. Halting computations
can be recognised by simply running them; the main difficulty is to detect
non-halting programs. Our approach is to have the probability space extend over
both space and time and to consider the probability that a random $N$-bit
program has halted by a random time. We postulate an a priori computable
probability distribution on all possible runtimes and we prove that given an
integer k>0, we can effectively compute a time bound T such that the
probability that an N-bit program will eventually halt given that it has not
halted by T is smaller than 2^{-k}. We also show that the set of halting
programs (which is computably enumerable, but not computable) can be written as
a disjoint union of a computable set and a set of effectively vanishing
probability. Finally, we show that ``long'' runtimes are effectively rare. More
formally, the set of times at which an N-bit program can stop after the time
2^{N+constant} has effectively zero density."
"This is the second part of a two-part series of papers. In this paper, for
the generalized non-orthogonal amplify and forward (GNAF) protocol presented in
Part-I, a construction of a new family of distributed space-time codes based on
Co-ordinate Interleaved Orthogonal Designs (CIOD) which result in reduced
Maximum Likelihood (ML) decoding complexity at the destination is proposed.
Further, it is established that the recently proposed Toeplitz space-time codes
as well as space-time block codes (STBCs) from cyclic division algebras can be
used in GNAF protocol. Finally, a lower bound on the optimal
Diversity-Multiplexing Gain (DM-G) tradeoff for the GNAF protocol is
established and it is shown that this bound approaches the transmit diversity
bound asymptotically as the number of relays and the number of channels uses
increases."
"In this two-part series of papers, a generalized non-orthogonal amplify and
forward (GNAF) protocol which generalizes several known cooperative diversity
protocols is proposed. Transmission in the GNAF protocol comprises of two
phases - the broadcast phase and the cooperation phase. In the broadcast phase,
the source broadcasts its information to the relays as well as the destination.
In the cooperation phase, the source and the relays together transmit a
space-time code in a distributed fashion. The GNAF protocol relaxes the
constraints imposed by the protocol of Jing and Hassibi on the code structure.
In Part-I of this paper, a code design criteria is obtained and it is shown
that the GNAF protocol is delay efficient and coding gain efficient as well.
Moreover GNAF protocol enables the use of sphere decoders at the destination
with a non-exponential Maximum likelihood (ML) decoding complexity. In Part-II,
several low decoding complexity code constructions are studied and a lower
bound on the Diversity-Multiplexing Gain tradeoff of the GNAF protocol is
obtained."
"A Space-Time Block Code (STBC) in $K$ symbols (variables) is called $g$-group
decodable STBC if its maximum-likelihood decoding metric can be written as a
sum of $g$ terms such that each term is a function of a subset of the $K$
variables and each variable appears in only one term. In this paper we provide
a general structure of the weight matrices of multi-group decodable codes using
Clifford algebras. Without assuming that the number of variables in each group
to be the same, a method of explicitly constructing the weight matrices of
full-diversity, delay-optimal $g$-group decodable codes is presented for
arbitrary number of antennas. For the special case of $N_t=2^a$ we construct
two subclass of codes: (i) A class of $2a$-group decodable codes with rate
$\frac{a}{2^{(a-1)}}$, which is, equivalently, a class of Single-Symbol
Decodable codes, (ii) A class of $(2a-2)$-group decodable with rate
$\frac{(a-1)}{2^{(a-2)}}$, i.e., a class of Double-Symbol Decodable codes.
Simulation results show that the DSD codes of this paper perform better than
previously known Quasi-Orthogonal Designs."
"In this paper, a downlink communication system, in which a Base Station (BS)
equipped with $M$ antennas communicates with $N$ users each equipped with $K$
receive antennas, is considered. An efficient suboptimum algorithm is proposed
for selecting a set of users in order to maximize the sum-rate throughput of
the system. For the asymptotic case when $N$ tends to infinity, the necessary
and sufficient conditions in order to achieve the maximum sum-rate throughput,
such that the difference between the achievable sum-rate and the maximum value
approaches zero, is derived. The complexity of our algorithm is investigated in
terms of the required amount of feedback from the users to the base station, as
well as the number of searches required for selecting the users. It is shown
that the proposed method is capable of achieving a large portion of the
sum-rate capacity, with a very low complexity."
"We consider the problem of rate/distortion with side information available
only at the decoder. For the case of jointly-Gaussian source X and side
information Y, and mean-squared error distortion, Wyner proved in 1976 that the
rate/distortion function for this problem is identical to the conditional
rate/distortion function R_{X|Y}, assuming the side information Y is available
at the encoder. In this paper we construct a structured class of asymptotically
optimal quantizers for this problem: under the assumption of high correlation
between source X and side information Y, we show there exist quantizers within
our class whose performance comes arbitrarily close to Wyner's bound. As an
application illustrating the relevance of the high-correlation asymptotics, we
also explore the use of these quantizers in the context of a problem of data
compression for sensor networks, in a setup involving a large number of devices
collecting highly correlated measurements within a confined area. An important
feature of our formulation is that, although the per-node throughput of the
network tends to zero as network size increases, so does the amount of
information generated by each transmitter. This is a situation likely to be
encountered often in practice, which allows us to cast under new--and more
``optimistic''--light some negative results on the transport capacity of
large-scale wireless networks."
"This paper analyzes MIMO systems with multichannel beamforming in Ricean
fading. Our results apply to a wide class of multichannel systems which
transmit on the eigenmodes of the MIMO channel. We first present new
closed-form expressions for the marginal ordered eigenvalue distributions of
complex noncentral Wishart matrices. These are used to characterize the
statistics of the signal to noise ratio (SNR) on each eigenmode. Based on this,
we present exact symbol error rate (SER) expressions. We also derive
closed-form expressions for the diversity order, array gain, and outage
probability. We show that the global SER performance is dominated by the
subchannel corresponding to the minimum channel singular value. We also show
that, at low outage levels, the outage probability varies inversely with the
Ricean K-factor for cases where transmission is only on the most dominant
subchannel (i.e. a singlechannel beamforming system). Numerical results are
presented to validate the theoretical analysis."
"Toric codes are obtained by evaluating rational functions of a nonsingular
toric variety at the algebraic torus. One can extend toric codes to the so
called generalized toric codes. This extension consists on evaluating elements
of an arbitrary polynomial algebra at the algebraic torus instead of a linear
combination of monomials whose exponents are rational points of a convex
polytope. We study their multicyclic and metric structure, and we use them to
express their dual and to estimate their minimum distance."
"This letter derives the asymptotic symbol error rate (SER) and outage
probability of multiple-input multiple-output (MIMO) maximum ratio combining
(MRC) systems. We consider Rayleigh fading channels with both transmit and
receive spatial correlation. Our results are based on new asymptotic
expressions which we derive for the p.d.f. and c.d.f. of the maximum eigenvalue
of positive-definite quadratic forms in complex Gaussian matrices. We prove
that spatial correlation does not affect the diversity order, but that it
reduces the array gain and hence increases the SER in the high SNR regime."
"The capacity region of a channel consists of all achievable rate vectors.
Picking a particular point in the capacity region is synonymous with rate
allocation. The issue of fairness in rate allocation is addressed in this
paper. We review several notions of fairness, including max-min fairness,
proportional fairness and Nash bargaining solution. Their efficiencies for
general multiuser channels are discussed. We apply these ideas to the Gaussian
multiple access channel (MAC) and the Gaussian broadcast channel (BC). We show
that in the Gaussian MAC, max-min fairness and proportional fairness coincide.
For both Gaussian MAC and BC, we devise efficient algorithms that locate the
fair point in the capacity region. Some elementary properties of fair rate
allocations are proved."
"In the distributed coding of correlated sources, the problem of
characterizing the joint probability distribution of a pair of random variables
satisfying an n-letter Markov chain arises. The exact solution of this problem
is intractable. In this paper, we seek a single-letter necessary condition for
this n-letter Markov chain. To this end, we propose a new data processing
inequality on a new measure of correlation by means of spectrum analysis. Based
on this new data processing inequality, we provide a single-letter necessary
condition for the required joint probability distribution. We apply our results
to two specific examples involving the distributed coding of correlated
sources: multi-terminal rate-distortion region and multiple access channel with
correlated sources, and propose new necessary conditions for these two
problems."
"Kullback-Leibler relative-entropy, in cases involving distributions resulting
from relative-entropy minimization, has a celebrated property reminiscent of
squared Euclidean distance: it satisfies an analogue of the Pythagoras'
theorem. And hence, this property is referred to as Pythagoras' theorem of
relative-entropy minimization or triangle equality and plays a fundamental role
in geometrical approaches of statistical estimation theory like information
geometry. Equvalent of Pythagoras' theorem in the generalized nonextensive
formalism is established in (Dukkipati at el., Physica A, 361 (2006) 124-138).
In this paper we give a detailed account of it."
"A new concept named nonsymmetric entropy which generalizes the concepts of
Boltzman's entropy and shannon's entropy, was introduced. Maximal nonsymmetric
entropy principle was proven. Some important distribution laws were derived
naturally from maximal nonsymmetric entropy principle."
"A simple proof is given for the convexity of log det (I+K X^{-1}) in the
positive definite matrix variable X with a given positive semidefinite K."
"The capacity of a class of deterministic relay channels with the transmitter
input X, the receiver output Y, the relay output Y_1 = f(X, Y), and a separate
communication link from the relay to the receiver with capacity R_0, is shown
to be
  C(R_0) = \max_{p(x)} \min \{I(X;Y)+R_0, I(X;Y, Y_1) \}.
  Thus every bit from the relay is worth exactly one bit to the receiver. Two
alternative coding schemes are presented that achieve this capacity. The first
scheme, ``hash-and-forward'', is based on a simple yet novel use of random
binning on the space of relay outputs, while the second scheme uses the usual
``compress-and-forward''. In fact, these two schemes can be combined together
to give a class of optimal coding schemes. As a corollary, this relay capacity
result confirms a conjecture by Ahlswede and Han on the capacity of a channel
with rate-limited state information at the decoder in the special case when the
channel state is recoverable from the channel input and the output."
"We show that one can do away with the cyclic prefix (CP) for SC-FDE and OFDM
at the cost of a moderate increase in the complexity of a DFT-based receiver.
Such an approach effectively deals with the decrease in the number of channel
uses due to the introduction of the CP. It is shown that the SINR for SC-FDE
remains the same asymptotically with the proposed receiver without CP as that
of the conventional receiver with CP. The results are shown for $N_t$ transmit
antennas and $N_r$ receive antennas where $N_r \geq N_t$."
"n source and destination pairs randomly located in an area want to
communicate with each other. Signals transmitted from one user to another at
distance r apart are subject to a power loss of r^{-alpha}, as well as a random
phase. We identify the scaling laws of the information theoretic capacity of
the network. In the case of dense networks, where the area is fixed and the
density of nodes increasing, we show that the total capacity of the network
scales linearly with n. This improves on the best known achievability result of
n^{2/3} of Aeron and Saligrama, 2006. In the case of extended networks, where
the density of nodes is fixed and the area increasing linearly with n, we show
that this capacity scales as n^{2-alpha/2} for 2<alpha<3 and sqrt{n} for
alpha>3. The best known earlier result (Xie and Kumar 2006) identified the
scaling law for alpha > 4. Thus, much better scaling than multihop can be
achieved in dense networks, as well as in extended networks with low
attenuation. The performance gain is achieved by intelligent node cooperation
and distributed MIMO communication. The key ingredient is a hierarchical and
digital architecture for nodal exchange of information for realizing the
cooperation."
"In prefix coding over an infinite alphabet, methods that consider specific
distributions generally consider those that decline more quickly than a power
law (e.g., Golomb coding). Particular power-law distributions, however, model
many random variables encountered in practice. For such random variables,
compression performance is judged via estimates of expected bits per input
symbol. This correspondence introduces a family of prefix codes with an eye
towards near-optimal coding of known distributions. Compression performance is
precisely estimated for well-known probability distributions using these codes
and using previously known prefix codes. One application of these near-optimal
codes is an improved representation of rational numbers."
"Two broad classes of graphical modeling problems for codes can be identified
in the literature: constructive and extractive problems. The former class of
problems concern the construction of a graphical model in order to define a new
code. The latter class of problems concern the extraction of a graphical model
for a (fixed) given code. The design of a new low-density parity-check code for
some given criteria (e.g. target block length and code rate) is an example of a
constructive problem. The determination of a graphical model for a classical
linear block code which implies a decoding algorithm with desired performance
and complexity characteristics is an example of an extractive problem. This
work focuses on extractive graphical model problems and aims to lay out some of
the foundations of the theory of such problems for linear codes.
  The primary focus of this work is a study of the space of all graphical
models for a (fixed) given code. The tradeoff between cyclic topology and
complexity in this space is characterized via the introduction of a new bound:
the tree-inducing cut-set bound. The proposed bound provides a more precise
characterization of this tradeoff than that which can be obtained using
existing tools (e.g. the Cut-Set Bound) and can be viewed as a generalization
of the square-root bound for tail-biting trellises to graphical models with
arbitrary cyclic topologies. Searching the space of graphical models for a
given code is then enabled by introducing a set of basic graphical model
transformation operations which are shown to span this space. Finally,
heuristics for extracting novel graphical models for linear block codes using
these transformations are investigated."
"The performance of algebraic soft-decision decoding of Reed-Solomon codes
using bit-level soft information is investigated. Optimal multiplicity
assignment strategies of algebraic soft-decision decoding with infinite cost
are first studied over erasure channels and the binary symmetric channel. The
corresponding decoding radii are calculated in closed forms and tight bounds on
the error probability are derived. The multiplicity assignment strategy and the
corresponding performance analysis are then generalized to characterize the
decoding region of algebraic softdecision decoding over a mixed error and
bit-level erasure channel. The bit-level decoding region of the proposed
multiplicity assignment strategy is shown to be significantly larger than that
of conventional Berlekamp-Massey decoding. As an application, a bit-level
generalized minimum distance decoding algorithm is proposed. The proposed
decoding compares favorably with many other Reed-Solomon soft-decision decoding
algorithms over various channels. Moreover, owing to the simplicity of the
proposed bit-level generalized minimum distance decoding, its performance can
be tightly bounded using order statistics."
"We investigate the optimal performance of dense sensor networks by studying
the joint source-channel coding problem. The overall goal of the sensor network
is to take measurements from an underlying random process, code and transmit
those measurement samples to a collector node in a cooperative multiple access
channel with potential feedback, and reconstruct the entire random process at
the collector node. We provide lower and upper bounds for the minimum
achievable expected distortion when the underlying random process is Gaussian.
When the Gaussian random process satisfies some general conditions, we evaluate
the lower and upper bounds explicitly, and show that they are of the same order
for a wide range of power constraints. Thus, for these random processes, under
these power constraints, we express the minimum achievable expected distortion
as a function of the power constraint. Further, we show that the achievability
scheme that achieves the lower bound on the distortion is a separation-based
scheme that is composed of multi-terminal rate-distortion coding and
amplify-and-forward channel coding. Therefore, we conclude that separation is
order-optimal for the dense Gaussian sensor network scenario under
consideration, when the underlying random process satisfies some general
conditions."
"New bounds on the rate distortion function of certain non-Gaussian sources,
with a proportional-weighted mean-square error (MSE) distortion measure, are
given. The growth, g, of the rate distortion function, as a result of changing
from a non-weighted MSE distortion measure to a proportional-weighted
distortion criterion is analyzed. It is shown that for a small distortion, d,
the growth, g, and the difference between the rate distortion functions of a
Gaussian memoryless source and a source with memory, both with the same
marginal statistics and MSE distortion measure, share the same lower bound.
Several examples and applications are also given."
"Generalized Tanner graphs have been implicitly studied by a number of authors
under the rubric of generalized parity-check matrices. This work considers the
conditioning of binary hidden variables in such models in order to break all
cycles and thus derive optimal soft-in soft-out (SISO) decoding algorithms.
Conditionally cycle-free generalized Tanner graphs are shown to imply optimal
SISO decoding algorithms for the first order Reed-Muller codes and their duals
- the extended Hamming codes - which are substantially less complex than
conventional bit-level trellis decoding. The study of low-complexity optimal
SISO decoding algorithms for the family of extended Hamming codes is
practically motivated. Specifically, it is shown that exended Hamming codes
offer an attractive alternative to high-rate convolutional codes in terms of
both performance and complexity for use in very high-rate, very low-floor,
serially concatenated coding schemes."
"We study how much memory one-pass compression algorithms need to compete with
the best multi-pass algorithms. We call a one-pass algorithm an (f (n,
\ell))-footprint compressor if, given $n$, $\ell$ and an $n$-ary string $S$, it
stores $S$ in ((\rule{0ex}{2ex} O (H_\ell (S)) + o (\log n)) |S| + O (n^{\ell +
1} \log n)) bits -- where (H_\ell (S)) is the $\ell$th-order empirical entropy
of $S$ -- while using at most (f (n, \ell)) bits of memory. We prove that, for
any (\epsilon > 0) and some (f (n, \ell) \in O (n^{\ell + \epsilon} \log n)),
there is an (f (n, \ell))-footprint compressor; on the other hand, there is no
(f (n, \ell))-footprint compressor for (f (n, \ell) \in o (n^\ell \log n))."
"In this paper, both non-mixing and mixing local minima of the entropy are
analyzed from the viewpoint of blind source separation (BSS); they correspond
respectively to acceptable and spurious solutions of the BSS problem. The
contribution of this work is twofold. First, a Taylor development is used to
show that the \textit{exact} output entropy cost function has a non-mixing
minimum when this output is proportional to \textit{any} of the non-Gaussian
sources, and not only when the output is proportional to the lowest entropic
source. Second, in order to prove that mixing entropy minima exist when the
source densities are strongly multimodal, an entropy approximator is proposed.
The latter has the major advantage that an error bound can be provided. Even if
this approximator (and the associated bound) is used here in the BSS context,
it can be applied for estimating the entropy of any random variable with
multimodal density."
"Starting from Shannon's celebrated 1948 channel coding theorem, we trace the
evolution of channel coding from Hamming codes to capacity-approaching codes.
We focus on the contributions that have led to the most significant
improvements in performance vs. complexity for practical applications,
particularly on the additive white Gaussian noise (AWGN) channel. We discuss
algebraic block codes, and why they did not prove to be the way to get to the
Shannon limit. We trace the antecedents of today's capacity-approaching codes:
convolutional codes, concatenated codes, and other probabilistic coding
schemes. Finally, we sketch some of the practical applications of these codes."
"In this two-part paper, we consider the transmission of confidential data
over wireless wiretap channels. The first part presents an
information-theoretic problem formulation in which two legitimate partners
communicate over a quasi-static fading channel and an eavesdropper observes
their transmissions through another independent quasi-static fading channel. We
define the secrecy capacity in terms of outage probability and provide a
complete characterization of the maximum transmission rate at which the
eavesdropper is unable to decode any information. In sharp contrast with known
results for Gaussian wiretap channels (without feedback), our contribution
shows that in the presence of fading information-theoretic security is
achievable even when the eavesdropper has a better average signal-to-noise
ratio (SNR) than the legitimate receiver - fading thus turns out to be a friend
and not a foe. The issue of imperfect channel state information is also
addressed. Practical schemes for wireless information-theoretic security are
presented in Part II, which in some cases comes close to the secrecy capacity
limits given in this paper."
"In Part I of this two-part paper on confidential communication over wireless
channels, we studied the fundamental security limits of quasi-static fading
channels from the point of view of outage secrecy capacity with perfect and
imperfect channel state information. In Part II, we develop a practical secret
key agreement protocol for Gaussian and quasi-static fading wiretap channels.
The protocol uses a four-step procedure to secure communications: establish
common randomness via an opportunistic transmission, perform message
reconciliation, establish a common key via privacy amplification, and use of
the key. We introduce a new reconciliation procedure that uses multilevel
coding and optimized low density parity check codes which in some cases comes
close to achieving the secrecy capacity limits established in Part I. Finally,
we develop new metrics for assessing average secure key generation rates and
show that our protocol is effective in secure key renewal."
"We consider a relay channel where a relay helps the transmission of messages
from one sender to one receiver. The relay is considered not only as a sender
that helps the message transmission but as a wire-tapper who can obtain some
knowledge about the transmitted messages. In this paper we study the coding
problem of the relay channel under the situation that some of transmitted
messages are confidential to the relay. A security of such confidential
messages is measured by the conditional entropy. The rate region is defined by
the set of transmission rates for which messages are reliably transmitted and
the security of confidential messages is larger than a prescribed level. In
this paper we give two definition of the rate region. We first define the rate
region in the case of deterministic encoder and call it the deterministic rate
region. Next, we define the rate region in the case of stochastic encoder and
call it the stochastic rate region. We derive explicit inner and outer bounds
for the above two rate regions and present a class of relay channels where two
bounds match. Furthermore, we show that stochastic encoder can enlarge the rate
region. We also evaluate the deterministic rate region of the Gaussian relay
channel with confidential messages."
"Shannon's secrecy system is studied in a setting, where both the legitimate
decoder and the wiretapper have access to side information sequences correlated
to the source, but the wiretapper receives both the coded information and the
side information via channels that are more noisy than the respective channels
of the legitmate decoder, which in turn, also shares a secret key with the
encoder. A single--letter characterization is provided for the achievable
region in the space of five figures of merit: the equivocation at the
wiretapper, the key rate, the distortion of the source reconstruction at the
legitimate receiver, the bandwidth expansion factor of the coded channels, and
the average transmission cost (generalized power). Beyond the fact that this is
an extension of earlier studies, it also provides a framework for studying
fundamental performance limits of systematic codes in the presence of a wiretap
channel. The best achievable performance of systematic codes is then compared
to that of a general code in several respects, and a few examples are given."
"A general lossless joint source-channel coding (JSCC) scheme based on linear
codes and random interleavers for multiple-access channels (MACs) is presented
and then analyzed in this paper. By the information-spectrum approach and the
code-spectrum approach, it is shown that a linear code with a good joint
spectrum can be used to establish limit-approaching lossless JSCC schemes for
correlated general sources and general MACs, where the joint spectrum is a
generalization of the input-output weight distribution. Some properties of
linear codes with good joint spectra are investigated. A formula on the
""distance"" property of linear codes with good joint spectra is derived, based
on which, it is further proved that, the rate of any systematic codes with good
joint spectra cannot be larger than the reciprocal of the corresponding
alphabet cardinality, and any sparse generator matrices cannot yield linear
codes with good joint spectra. The problem of designing arbitrary rate coding
schemes is also discussed. A novel idea called ""generalized puncturing"" is
proposed, which makes it possible that one good low-rate linear code is enough
for the design of coding schemes with multiple rates. Finally, various coding
problems of MACs are reviewed in a unified framework established by the
code-spectrum approach, under which, criteria and candidates of good linear
codes in terms of spectrum requirements for such problems are clearly
presented."
"The performance of codes defined from graphs depends on the expansion
property of the underlying graph in a crucial way. Graph products, such as the
zig-zag product and replacement product provide new infinite families of
constant degree expander graphs. The paper investigates the use of zig-zag and
replacement product graphs for the construction of codes on graphs. A
modification of the zig-zag product is also introduced, which can operate on
two unbalanced biregular bipartite graphs."
"We consider cooperative relay communication in a fading channel environment
under the Orthogonal Amplify and Forward (OAF) and Orthogonal and
Non-Orthogonal Selection Decode and Forward (OSDF and NSDF) protocols. For all
these protocols, we compute the Diversity-Multiplexing Gain Tradeoff (DMT). We
construct DMT optimal codes for the protocols which are sphere decodable and,
in certain cases, incur minimum possible delay. Our results establish that the
DMT of the OAF protocol is identical to the DMT of the Non-Orthogonal Amplify
and Forward (NAF) protocol. Two variants of the NSDF protocol are considered:
fixed-NSDF and variable-NSDF protocol. In the variable-NSDF protocol, the
fraction of time duration for which the source alone transmits is allowed to
vary with the rate of communication. Among the class of static
amplify-and-forward and decode-and-forward protocols, the variable-NSDF
protocol is shown to have the best known DMT for any number of relays apart
from the two-relay case. When there are two relays, the variable-NSDF protocol
is shown to improve on the DMT of the best previously-known protocol for higher
values of the multiplexing gain. Our results also establish that the fixed-NSDF
protocol has a better DMT than the NAF protocol for any number of relays.
Finally, we present a DMT optimal code construction for the NAF protocol."
"The use of error-correcting codes for tight control of the peak-to-mean
envelope power ratio (PMEPR) in orthogonal frequency-division multiplexing
(OFDM) transmission is considered in this correspondence. By generalizing a
result by Paterson, it is shown that each q-phase (q is even) sequence of
length 2^m lies in a complementary set of size 2^{k+1}, where k is a
nonnegative integer that can be easily determined from the generalized Boolean
function associated with the sequence. For small k this result provides a
reasonably tight bound for the PMEPR of q-phase sequences of length 2^m. A new
2^h-ary generalization of the classical Reed-Muller code is then used together
with the result on complementary sets to derive flexible OFDM coding schemes
with low PMEPR. These codes include the codes developed by Davis and Jedwab as
a special case. In certain situations the codes in the present correspondence
are similar to Paterson's code constructions and often outperform them."
"The peak-to-mean envelope power ratio (PMEPR) of a code employed in
orthogonal frequency-division multiplexing (OFDM) systems can be reduced by
permuting its coordinates and by rotating each coordinate by a fixed phase
shift. Motivated by some previous designs of phase shifts using suboptimal
methods, the following question is considered in this paper. For a given binary
code, how much PMEPR reduction can be achieved when the phase shifts are taken
from a 2^h-ary phase-shift keying (2^h-PSK) constellation? A lower bound on the
achievable PMEPR is established, which is related to the covering radius of the
binary code. Generally speaking, the achievable region of the PMEPR shrinks as
the covering radius of the binary code decreases. The bound is then applied to
some well understood codes, including nonredundant BPSK signaling, BCH codes
and their duals, Reed-Muller codes, and convolutional codes. It is demonstrated
that most (presumably not optimal) phase-shift designs from the literature
attain or approach our bound."
"A constant-amplitude code is a code that reduces the peak-to-average power
ratio (PAPR) in multicode code-division multiple access (MC-CDMA) systems to
the favorable value 1. In this paper quaternary constant-amplitude codes (codes
over Z_4) of length 2^m with error-correction capabilities are studied. These
codes exist for every positive integer m, while binary constant-amplitude codes
cannot exist if m is odd. Every word of such a code corresponds to a function
from the binary m-tuples to Z_4 having the bent property, i.e., its Fourier
transform has magnitudes 2^{m/2}. Several constructions of such functions are
presented, which are exploited in connection with algebraic codes over Z_4 (in
particular quaternary Reed-Muller, Kerdock, and Delsarte-Goethals codes) to
construct families of quaternary constant-amplitude codes. Mappings from binary
to quaternary constant-amplitude codes are presented as well."
"We study the MIMO broadcast channel and compare the achievable throughput for
the optimal strategy of dirty paper coding to that achieved with sub-optimal
and lower complexity linear precoding (e.g., zero-forcing and block
diagonalization) transmission. Both strategies utilize all available spatial
dimensions and therefore have the same multiplexing gain, but an absolute
difference in terms of throughput does exist. The sum rate difference between
the two strategies is analytically computed at asymptotically high SNR, and it
is seen that this asymptotic statistic provides an accurate characterization at
even moderate SNR levels. Furthermore, the difference is not affected by
asymmetric channel behavior when each user a has different average SNR.
Weighted sum rate maximization is also considered, and a similar quantification
of the throughput difference between the two strategies is performed. In the
process, it is shown that allocating user powers in direct proportion to user
weights asymptotically maximizes weighted sum rate. For multiple antenna users,
uniform power allocation across the receive antennas is applied after
distributing power proportional to the user weight."
"A method for estimating the performance of low-density parity-check (LDPC)
codes decoded by hard-decision iterative decoding algorithms on binary
symmetric channels (BSC) is proposed. Based on the enumeration of the smallest
weight error patterns that can not be all corrected by the decoder, this method
estimates both the frame error rate (FER) and the bit error rate (BER) of a
given LDPC code with very good precision for all crossover probabilities of
practical interest. Through a number of examples, we show that the proposed
method can be effectively applied to both regular and irregular LDPC codes and
to a variety of hard-decision iterative decoding algorithms. Compared with the
conventional Monte Carlo simulation, the proposed method has a much smaller
computational complexity, particularly for lower error rates."
"The intersection problem for additive (extended and non-extended) perfect
codes, i.e. which are the possibilities for the number of codewords in the
intersection of two additive codes C1 and C2 of the same length, is
investigated. Lower and upper bounds for the intersection number are computed
and, for any value between these bounds, codes which have this given
intersection value are constructed.
  For all these codes the abelian group structure of the intersection is
characterized. The parameters of this abelian group structure corresponding to
the intersection codes are computed and lower and upper bounds for these
parameters are established. Finally, constructions of codes the intersection of
which fits any parameters between these bounds are given."
"Consider the case where consecutive blocks of N letters of a semi-infinite
individual sequence X over a finite-alphabet are being compressed into binary
sequences by some one-to-one mapping. No a-priori information about X is
available at the encoder, which must therefore adopt a universal
data-compression algorithm. It is known that if the universal LZ77 data
compression algorithm is successively applied to N-blocks then the best
error-free compression for the particular individual sequence X is achieved, as
$N$ tends to infinity. The best possible compression that may be achieved by
any universal data compression algorithm for finite N-blocks is discussed. It
is demonstrated that context tree coding essentially achieves it. Next,
consider a device called classifier (or discriminator) that observes an
individual training sequence X. The classifier's task is to examine individual
test sequences of length N and decide whether the test N-sequence has the same
features as those that are captured by the training sequence X, or is
sufficiently different, according to some appropriatecriterion. Here again, it
is demonstrated that a particular universal context classifier with a
storage-space complexity that is linear in N, is essentially optimal. This may
contribute a theoretical ""individual sequence"" justification for the
Probabilistic Suffix Tree (PST) approach in learning theory and in
computational biology."
"We consider the communication scenario where multiple cognitive users wish to
communicate to the same receiver, in the presence of primary transmission. The
cognitive transmitters are assumed to have the side information about the
primary transmission. The capacity region of cognitive users is formulated
under the constraint that the capacity of primary transmission is not changed
as if no cognitive users exist. Moreover, the maximum sum-rate point of the
capacity region is characterized, by optimally allocating the power of each
cognitive user to transmit its own information."
"A new approach for upper bounding the channel reliability function using the
code spectrum is described. It allows to treat in a unified way both a low and
a high rate cases. In particular, the earlier known upper bounds are improved,
and a new derivation of the sphere-packing bound is presented."
"This paper establishes the utility of user cooperation in facilitating secure
wireless communications. In particular, the four-terminal relay-eavesdropper
channel is introduced and an outer-bound on the optimal rate-equivocation
region is derived. Several cooperation strategies are then devised and the
corresponding achievable rate-equivocation region are characterized. Of
particular interest is the novel Noise-Forwarding (NF) strategy, where the
relay node sends codewords independent of the source message to confuse the
eavesdropper. This strategy is used to illustrate the deaf helper phenomenon,
where the relay is able to facilitate secure communications while being totally
ignorant of the transmitted messages. Furthermore, NF is shown to increase the
secrecy capacity in the reversely degraded scenario, where the relay node fails
to offer performance gains in the classical setting. The gain offered by the
proposed cooperation strategies is then proved theoretically and validated
numerically in the additive White Gaussian Noise (AWGN) channel."
This paper has been withdrawn by the author.
"In this paper, we first introduce the concept of elementary linear subspace,
which has similar properties to those of a set of coordinates. We then use
elementary linear subspaces to derive properties of maximum rank distance (MRD)
codes that parallel those of maximum distance separable codes. Using these
properties, we show that, for MRD codes with error correction capability t, the
decoder error probability of bounded rank distance decoders decreases
exponentially with t^2 based on the assumption that all errors with the same
rank are equally likely."
"The belief propagation algorithm has been recognized in the information
theory community as a soft-decision iterative decoding algorithm. It is the
most powerful algorithm found so far for attacking hard optimization problems
in channel decoding. Quantum mechanics is the foundation of modern physics with
the time-independent Schrodinger equation being one of the most important
equations. This paper shows that the equation can be derived from a generalized
belief propagation algorithm. Such a connection on a mathematical basis might
shed new insights into the foundations of quantum mechanics and quantum
computing."
"A transform that enables generator-matrix-based Reed-Solomon (RS) coded data
to be recovered under interpolation-based list decoding is presented. The
transform matrix needs to be computed only once and the transformation of an
element from the output list to the desired RS coded data block incurs $k^{2}$
field multiplications, given a code of dimension $k$."
"Rateless/fountain codes are designed so that all input symbols can be
recovered from a slightly larger number of coded symbols, with high probability
using an iterative decoder. In this paper we investigate the number of input
symbols that can be recovered by the same decoder, but when the number of coded
symbols available is less than the total number of input symbols. Of course
recovery of all inputs is not possible, and the fraction that can be recovered
will depend on the output degree distribution of the code.
  In this paper we (a) outer bound the fraction of inputs that can be recovered
for any output degree distribution of the code, and (b) design degree
distributions which meet/perform close to this bound. Our results are of
interest for real-time systems using rateless codes, and for Raptor-type
two-stage designs."
"This paper presents an algebraic theory of linear signal processing. At the
core of algebraic signal processing is the concept of a linear signal model
defined as a triple (A, M, phi), where familiar concepts like the filter space
and the signal space are cast as an algebra A and a module M, respectively, and
phi generalizes the concept of the z-transform to bijective linear mappings
from a vector space of, e.g., signal samples, into the module M. A signal model
provides the structure for a particular linear signal processing application,
such as infinite and finite discrete time, or infinite or finite discrete
space, or the various forms of multidimensional linear signal processing. As
soon as a signal model is chosen, basic ingredients follow, including the
associated notions of filtering, spectrum, and Fourier transform. The shift
operator is a key concept in the algebraic theory: it is the generator of the
algebra of filters A. Once the shift is chosen, a well-defined methodology
leads to the associated signal model. Different shifts correspond to infinite
and finite time models with associated infinite and finite z-transforms, and to
infinite and finite space models with associated infinite and finite
C-transforms (that we introduce). In particular, we show that the 16 discrete
cosine and sine transforms are Fourier transforms for the finite space models.
Other definitions of the shift naturally lead to new signal models and to new
transforms as associated Fourier transforms in one and higher dimensions,
separable and non-separable. We explain in algebraic terms shift-invariance
(the algebra of filters A is commutative), the role of boundary conditions and
signal extensions, the connections between linear transforms and linear finite
Gauss-Markov fields, and several other concepts and connections."
"We analyze the effect of finite rate feedback on CDMA (code-division multiple
access) signature optimization and MIMO (multi-input-multi-output) beamforming
vector selection. In CDMA signature optimization, for a particular user, the
receiver selects a signature vector from a codebook to best avoid interference
from other users, and then feeds the corresponding index back to the specified
user. For MIMO beamforming vector selection, the receiver chooses a beamforming
vector from a given codebook to maximize throughput, and feeds back the
corresponding index to the transmitter. These two problems are dual: both can
be modeled as selecting a unit norm vector from a finite size codebook to
""match"" a randomly generated Gaussian matrix. In signature optimization, the
least match is required while the maximum match is preferred for beamforming
selection.
  Assuming that the feedback link is rate limited, our main result is an exact
asymptotic performance formulae where the length of the signature/beamforming
vector, the dimensions of interference/channel matrix, and the feedback rate
approach infinity with constant ratios. The proof rests on a large deviation
principle over a random matrix ensemble. Further, we show that random codebooks
generated from the isotropic distritution are asymptotically optimal not only
on average, but also with probability one."
"Several proofs of the monotonicity of the non-Gaussianness (divergence with
respect to a Gaussian random variable with identical second order statistics)
of the sum of n independent and identically distributed (i.i.d.) random
variables were published. We give an upper bound on the decrease rate of the
non-Gaussianness which is proportional to the inverse of n, for large n. The
proof is based on the relationship between non-Gaussianness and minimum
mean-square error (MMSE) and causal minimum mean-square error (CMMSE) in the
time-continuous Gaussian channel."
"Variable-length block-coding schemes are investigated for discrete memoryless
channels with ideal feedback under cost constraints. Upper and lower bounds are
found for the minimum achievable probability of decoding error $P_{e,\min}$ as
a function of constraints $R, \AV$, and $\bar \tau$ on the transmission rate,
average cost, and average block length respectively. For given $R$ and $\AV$,
the lower and upper bounds to the exponent $-(\ln P_{e,\min})/\bar \tau$ are
asymptotically equal as $\bar \tau \to \infty$. The resulting reliability
function, $\lim_{\bar \tau\to \infty} (-\ln P_{e,\min})/\bar \tau$, as a
function of $R$ and $\AV$, is concave in the pair $(R, \AV)$ and generalizes
the linear reliability function of Burnashev to include cost constraints. The
results are generalized to a class of discrete-time memoryless channels with
arbitrary alphabets, including additive Gaussian noise channels with amplitude
and power constraints."
"In this contribution, models of wireless channels are derived from the
maximum entropy principle, for several cases where only limited information
about the propagation environment is available. First, analytical models are
derived for the cases where certain parameters (channel energy, average energy,
spatial correlation matrix) are known deterministically. Frequently, these
parameters are unknown (typically because the received energy or the spatial
correlation varies with the user position), but still known to represent
meaningful system characteristics. In these cases, analytical channel models
are derived by assigning entropy-maximizing distributions to these parameters,
and marginalizing them out. For the MIMO case with spatial correlation, we show
that the distribution of the covariance matrices is conveniently handled
through its eigenvalues. The entropy-maximizing distribution of the covariance
matrix is shown to be a Wishart distribution. Furthermore, the corresponding
probability density function of the channel matrix is shown to be described
analytically by a function of the channel Frobenius norm. This technique can
provide channel models incorporating the effect of shadow fading and spatial
correlation between antennas without the need to assume explicit values for
these parameters. The results are compared in terms of mutual information to
the classical i.i.d. Gaussian model."
"In this correspondence the cumulants of the mutual information of the flat
Rayleigh fading amplify-and-forward MIMO relay channel without direct link
between source and destination are derived in the large array limit. The
analysis is based on the replica trick and covers both spatially independent
and correlated fading in the first and the second hop, while beamforming at all
terminals is restricted to deterministic weight matrices. Expressions for mean
and variance of the mutual information are obtained. Their parameters are
determined by a nonlinear equation system. All higher cumulants are shown to
vanish as the number of antennas n goes to infinity. In conclusion the
distribution of the mutual information I becomes Gaussian in the large n limit
and is completely characterized by the expressions obtained for mean and
variance of I. Comparisons with simulation results show that the asymptotic
results serve as excellent approximations for systems with only few antennas at
each node. The derivation of the results follows the technique formalized by
Moustakas et al. in [1]. Although the evaluations are more involved for the
MIMO relay channel compared to point-to-point MIMO channels, the structure of
the results is surprisingly simple again. In particular an elegant formula for
the mean of the mutual information is obtained, i.e., the ergodic capacity of
the two-hop amplify-and-forward MIMO relay channel without direct link."
"Based on data from a large-scale experiment with human subjects, we conclude
that the logarithm of probability to guess a word in context (unpredictability)
depends linearly on the word length. This result holds both for poetry and
prose, even though with prose, the subjects don't know the length of the
omitted word. We hypothesize that this effect reflects a tendency of natural
language to have an even information rate."
"This paper considers the design of relay assisted F/TDMA ad hoc networks with
multiple relay nodes each of which assists the transmission of a predefined
subset of source nodes to their respective destinations. Considering the sum
capacity as the performance metric, we solve the problem of optimally
allocating the total power of each relay node between the transmissions it is
assisting. We consider four different relay transmission strategies, namely
regenerative decode-and-forward (RDF), nonregenerative decode-and-forward
(NDF), amplify-and-forward (AF) and compress-and-forward (CF). We first obtain
the optimum power allocation policies for the relay nodes that employ a uniform
relaying strategy for all nodes. We show that the optimum power allocation for
the RDF and NDF cases are modified water-filling solutions. We observe that for
a given relay transmit power, NDF always outperforms RDF whereas CF always
provides higher sum capacity than AF. When CF and NDF are compared, it is
observed that either of CF or NDF may outperform the other in different
scenarios. This observation suggests that the sum capacity can be further
improved by having each relay adopt its relaying strategy in helping different
source nodes. We investigate this problem next and determine the optimum power
allocation and relaying strategy for each source node that relay nodes assist.
We observe that optimum power allocation for relay nodes with hybrid relaying
strategies provides higher sum capacity than pure RDF, NDF, AF or CF relaying
strategies."
"We generalize the notion of the stopping redundancy in order to study the
smallest size of a trapping set in Tanner graphs of linear block codes. In this
context, we introduce the notion of the trapping redundancy of a code, which
quantifies the relationship between the number of redundant rows in any
parity-check matrix of a given code and the size of its smallest trapping set.
Trapping sets with certain parameter sizes are known to cause error-floors in
the performance curves of iterative belief propagation decoders, and it is
therefore important to identify decoding matrices that avoid such sets. Bounds
on the trapping redundancy are obtained using probabilistic and constructive
methods, and the analysis covers both general and elementary trapping sets.
Numerical values for these bounds are computed for the [2640,1320] Margulis
code and the class of projective geometry codes, and compared with some new
code-specific trapping set size estimates."
"A unification of thermodynamics and information theory is proposed. It is
argued that similarly to the randomness due to collisions in thermal systems,
the quenched randomness that exists in data files in informatics systems
contributes to entropy. Therefore, it is possible to define equilibrium and to
calculate temperature for informatics systems. The obtained temperature yields
correctly the Shannon information balance in informatics systems and is
consistent with the Clausius inequality and the Carnot cycle."
"In this paper, a game-theoretic model for studying power control for wireless
data networks in frequency-selective multipath environments is analyzed. The
uplink of an impulse-radio ultrawideband system is considered. The effects of
self-interference and multiple-access interference on the performance of
generic Rake receivers are investigated for synchronous systems. Focusing on
energy efficiency, a noncooperative game is proposed in which users in the
network are allowed to choose their transmit powers to maximize their own
utilities, and the Nash equilibrium for the proposed game is derived. It is
shown that, due to the frequency selective multipath, the noncooperative
solution is achieved at different signal-to-interference-plus-noise ratios,
depending on the channel realization and the type of Rake receiver employed. A
large-system analysis is performed to derive explicit expressions for the
achieved utilities. The Pareto-optimal (cooperative) solution is also discussed
and compared with the noncooperative approach."
"We investigate the decoding region for Algebraic Soft-Decision Decoding (ASD)
of Reed-Solomon codes in a discrete, memoryless, additive-noise channel. An
expression is derived for the error correction radius within which the
soft-decision decoder produces a list that contains the transmitted codeword.
The error radius for ASD is shown to be larger than that of Guruswami-Sudan
hard-decision decoding for a subset of low-rate codes. These results are also
extended to multivariable interpolation in the sense of Parvaresh and Vardy. An
upper bound is then presented for ASD's probability of error, where an error is
defined as the event that the decoder selects an erroneous codeword from its
list. This new definition gives a more accurate bound on the probability of
error of ASD than the results available in the literature."
"This paper describes an approach for half-duplex cooperative transmission in
a classical three-node relay channel. Assuming availability of channel state
information at nodes, the approach makes use of this information to optimize
distinct flows through the direct link from the source to the destination and
the path via the relay, respectively. It is shown that such a design can
effectively harness diversity advantage of the relay channel in both high-rate
and low-rate scenarios. When the rate requirement is low, the proposed design
gives a second-order outage diversity performance approaching that of
full-duplex relaying. When the rate requirement becomes asymptotically large,
the design still gives a close-to-second-order outage diversity performance.
The design also achieves the best diversity-multiplexing tradeoff possible for
the relay channel. With optimal long-term power control over the fading relay
channel, the proposed design achieves a delay-limited rate performance that is
only 3.0dB (5.4dB) worse than the capacity performance of the additive white
Gaussian channel in low- (high-) rate scenarios."
"Situations in many fields of research, such as digital communications,
nuclear physics and mathematical finance, can be modelled with random matrices.
When the matrices get large, free probability theory is an invaluable tool for
describing the asymptotic behaviour of many systems. It will be shown how free
probability can be used to aid in source detection for certain systems. Sample
covariance matrices for systems with noise are the starting point in our source
detection problem. Multiplicative free deconvolution is shown to be a method
which can aid in expressing limit eigenvalue distributions for sample
covariance matrices, and to simplify estimators for eigenvalue distributions of
covariance matrices."
"his study presents a novel technique to estimate the computational complexity
of sequential decoding using the Berry-Esseen theorem. Unlike the theoretical
bounds determined by the conventional central limit theorem argument, which
often holds only for sufficiently large codeword length, the new bound obtained
from the Berry-Esseen theorem is valid for any blocklength. The accuracy of the
new bound is then examined for two sequential decoding algorithms, an
ordering-free variant of the generalized Dijkstra's algorithm (GDA)(or
simplified GDA) and the maximum-likelihood sequential decoding algorithm
(MLSDA). Empirically investigating codes of small blocklength reveals that the
theoretical upper bound for the simplified GDA almost matches the simulation
results as the signal-to-noise ratio (SNR) per information bit ($\gamma_b$) is
greater than or equal to 8 dB. However, the theoretical bound may become
markedly higher than the simulated average complexity when $\gamma_b$ is small.
For the MLSDA, the theoretical upper bound is quite close to the simulation
results for both high SNR ($\gamma_b\geq 6$ dB) and low SNR ($\gamma_b\leq 2$
dB). Even for moderate SNR, the simulation results and the theoretical bound
differ by at most \makeblue{0.8} on a $\log_{10}$ scale."
"Berger's paper `The Source Coding Game', IEEE Trans. Inform. Theory, 1971,
considers the problem of finding the rate-distortion function for an
adversarial source comprised of multiple known IID sources. The adversary,
called the `switcher', was allowed only causal access to the source
realizations and the rate-distortion function was obtained through the use of a
type covering lemma. In this paper, the rate-distortion function of the
adversarial source is described, under the assumption that the switcher has
non-causal access to all source realizations. The proof utilizes the type
covering lemma and simple conditional, random `switching' rules. The
rate-distortion function is once again the maximization of the R(D) function
for a region of attainable IID distributions."
"Based on cyclic simplex codes, a new construction of a family of 2-generator
quasi-cyclic two-weight codes is given. New optimal binary quasi-cyclic [195,
8, 96], [210, 8, 104] and [240, 8, 120] codes, good QC ternary [195, 6, 126],
[208, 6, 135], [221, 6, 144] codes are thus obtained. Furthermre, binary
quasi-cyclic self-complementary codes are also constructed."
"This paper studies the performance of partial-Rake (PRake) receivers in
impulse-radio ultrawideband wireless networks when an energy-efficient power
control scheme is adopted. Due to the large bandwidth of the system, the
multipath channel is assumed to be frequency-selective. By making use of
noncooperative game-theoretic models and large-system analysis tools, explicit
expressions are derived in terms of network parameters to measure the effects
of self-interference and multiple-access interference at a receiving access
point. Performance of the PRake receivers is thus compared in terms of achieved
utilities and loss to that of the all-Rake receiver. Simulation results are
provided to validate the analysis."
"We address the problem of nonparametric estimation of characteristics for
stationary and ergodic time series. We consider finite-alphabet time series and
real-valued ones and the following four problems: i) estimation of the
(limiting) probability (or estimation of the density for real-valued time
series), ii) on-line prediction, iii) regression and iv) classification (or
so-called problems with side information). We show that so-called archivers (or
data compressors) can be used as a tool for solving these problems. In
particular, firstly, it is proven that any so-called universal code (or
universal data compressor) can be used as a basis for constructing
asymptotically optimal methods for the above problems. (By definition, a
universal code can ""compress"" any sequence generated by a stationary and
ergodic source asymptotically till the Shannon entropy of the source.) And,
secondly, we show experimentally that estimates, which are based on practically
used methods of data compression, have a reasonable precision."
"In this article we obtain estimates on the approximate eigenstructure of
channels with a spreading function supported only on a set of finite measure
$|U|$.Because in typical application like wireless communication the spreading
function is a random process corresponding to a random Hilbert--Schmidt channel
operator $\BH$ we measure this approximation in terms of the ratio of the
$p$--norm of the deviation from variants of the Weyl symbol calculus to the
$a$--norm of the spreading function itself. This generalizes recent results
obtained for the case $p=2$ and $a=1$. We provide a general approach to this
topic and consider then operators with $|U|<\infty$ in more detail. We show the
relation to pulse shaping and weighted norms of ambiguity functions. Finally we
derive several necessary conditions on $|U|$, such that the approximation error
is below certain levels."
"A coding theorem is proved for a class of stationary channels with feedback
in which the output Y_n = f(X_{n-m}^n, Z_{n-m}^n) is the function of the
current and past m symbols from the channel input X_n and the stationary
ergodic channel noise Z_n. In particular, it is shown that the feedback
capacity is equal to $$ \limp_{n\to\infty} \sup_{p(x^n||y^{n-1})} \frac{1}{n}
I(X^n \to Y^n), $$ where I(X^n \to Y^n) = \sum_{i=1}^n I(X^i; Y_i|Y^{i-1})
denotes the Massey directed information from the channel input to the output,
and the supremum is taken over all causally conditioned distributions
p(x^n||y^{n-1}) = \prod_{i=1}^n p(x_i|x^{i-1},y^{i-1}). The main ideas of the
proof are the Shannon strategy for coding with side information and a new
elementary coding technique for the given channel model without feedback, which
is in a sense dual to Gallager's lossy coding of stationary ergodic sources. A
similar approach gives a simple alternative proof of coding theorems for finite
state channels by Yang-Kavcic-Tatikonda, Chen-Berger, and
Permuter-Weissman-Goldsmith."
"We consider the problem of transmitting a bivariate Gaussian source over a
two-user additive Gaussian multiple-access channel with feedback. Each of the
transmitters observes one of the source components and tries to describe it to
the common receiver. We are interested in the minimal mean squared error at
which the receiver can reconstruct each of the source components.
  In the ``symmetric case'' we show that, below a certain signal-to-noise ratio
threshold which is determined by the source correlation, feedback is useless
and the minimal distortion is achieved by uncoded transmission. For the general
case we give necessary conditions for the achievability of a distortion pair."
"In this work, we are concerned with maximizing the lifetime of a cluster of
sensors engaged in single-hop communication with a base-station. In a
data-gathering network, the spatio-temporal correlation in sensor data induces
data-redundancy. Also, the interaction between two communicating parties is
well-known to reduce the communication complexity. This paper proposes a
formalism that exploits these two opportunities to reduce the number of bits
transmitted by a sensor node in a cluster, hence enhancing its lifetime. We
argue that our approach has several inherent advantages in scenarios where the
sensor nodes are acutely energy and computing-power constrained, but the
base-station is not so. This provides us an opportunity to develop
communication protocols, where most of the computing and communication is done
by the base-station.
  The proposed framework casts the sensor nodes and base-station communication
problem as the problem of multiple informants with correlated information
communicating with a recipient and attempts to extend extant work on
interactive communication between an informant-recipient pair to such
scenarios. Our work makes four major contributions. Firstly, we explicitly show
that in such scenarios interaction can help in reducing the communication
complexity. Secondly, we show that the order in which the informants
communicate with the recipient may determine the communication complexity.
Thirdly, we provide the framework to compute the $m$-message communication
complexity in such scenarios. Lastly, we prove that in a typical sensor network
scenario, the proposed formalism significantly reduces the communication and
computational complexities."
"While most useful information theoretic inequalities can be deduced from the
basic properties of entropy or mutual information, Shannon's entropy power
inequality (EPI) seems to be an exception: available information theoretic
proofs of the EPI hinge on integral representations of differential entropy
using either Fisher's information (FI) or minimum mean-square error (MMSE). In
this paper, we first present a unified view of proofs via FI and MMSE, showing
that they are essentially dual versions of the same proof, and then fill the
gap by providing a new, simple proof of the EPI, which is solely based on the
properties of mutual information and sidesteps both FI or MMSE representations."
"We consider a single-hop data gathering sensor cluster consisting of a set of
sensors that need to transmit data periodically to a base-station. We are
interested in maximizing the lifetime of this network. Even though the setting
of our problem is very simple, it turns out that the solution is far from easy.
The complexity arises from several competing system-level opportunities
available to reduce the energy consumed in radio transmission. First, sensor
data is spatially and temporally correlated. Recent advances in distributed
source-coding allow us to take advantage of these correlations to reduce the
number of transmitted bits, with concomitant savings in energy. Second, it is
also well-known that channel-coding can be used to reduce transmission energy
by increasing transmission time. Finally, sensor nodes are cooperative, unlike
nodes in an ad hoc network that are often modeled as competitive, allowing us
to take full advantage of the first two opportunities for the purpose of
maximizing cluster lifetime. In this paper, we pose the problem of maximizing
lifetime as a max-min optimization problem subject to the constraint of
successful data collection and limited energy supply at each node. By
introducing the notion of instantaneous decoding, we are able to simplify this
optimization problem into a joint scheduling and time allocation problem. We
show that even with our ample simplification, the problem remains NP-hard. We
provide some algorithms, heuristics and insight for various scenarios. Our
chief contribution is to illustrate both the challenges and gains provided by
joint source-channel coding and scheduling."
"This paper demonstrates the significant gains that multi-access users can
achieve from sharing a single amplify-forward relay in slow fading
environments. The proposed protocol, namely the multi-access relay
amplify-forward, allows for a low-complexity relay and achieves the optimal
diversity-multiplexing trade-off at high multiplexing gains. Analysis of the
protocol reveals that it uniformly dominates the compress-forward strategy and
further outperforms the dynamic decode-forward protocol at high multiplexing
gains. An interesting feature of the proposed protocol is that, at high
multiplexing gains, it resembles a multiple-input single-output system, and at
low multiplexing gains, it provides each user with the same
diversity-multiplexing trade-off as if there is no contention for the relay
from the other users."
"We bound the number of electromagnetic signals which may be observed over a
frequency range $2W$ for a time $T$ within a region of space enclosed by a
radius $R$. Our result implies that broadband fields in space cannot be
arbitrarily complex: there is a finite amount of information which may be
extracted from a region of space via electromagnetic radiation.
  Three-dimensional space allows a trade-off between large carrier frequency
and bandwidth. We demonstrate applications in super-resolution and broadband
communication."
"We bound the number of electromagnetic signals which may be observed over a
frequency range $[F-W,F+W]$ a time interval $[0,T]$ within a sphere of radius
$R$. We show that the such constrained signals may be represented by a series
expansion whose terms are bounded exponentially to zero beyond a threshold. Our
result implies there is a finite amount of information which may be extracted
from a region of space via electromagnetic radiation."
"In this paper, we investigate the minimum average transmit energy that can be
obtained in multiple antenna broadcast systems with channel inversion
technique. The achievable gain can be significantly higher than the
conventional gains that are mentioned in methods like perturbation technique of
Peel, et al. In order to obtain this gain, we introduce a Selective Mapping
(SLM) technique (based on random coding arguments). We propose to implement the
SLM method by using nested lattice codes in a trellis precoding framework."
"We are concerned with maximizing the lifetime of a data-gathering wireless
sensor network consisting of set of nodes directly communicating with a
base-station. We model this scenario as the m-message interactive communication
between multiple correlated informants (sensor nodes) and a recipient
(base-station). With this framework, we show that m-message interactive
communication can indeed enhance network lifetime. Both worst-case and
average-case performances are considered."
"It has been shown recently that the dirty-paper coding is the optimal
strategy for maximizing the sum rate of multiple-input multiple-output Gaussian
broadcast channels (MIMO BC). Moreover, by the channel duality, the nonconvex
MIMO BC sum rate problem can be transformed to the convex dual MIMO
multiple-access channel (MIMO MAC) problem with a sum power constraint. In this
paper, we design an efficient algorithm based on conjugate gradient projection
(CGP) to solve the MIMO BC maximum sum rate problem. Our proposed CGP algorithm
solves the dual sum power MAC problem by utilizing the powerful concept of
Hessian conjugacy. We also develop a rigorous algorithm to solve the projection
problem. We show that CGP enjoys provable convergence, nice scalability, and
great efficiency for large MIMO BC systems."
"In recent years, network coding has been investigated as a method to obtain
improvements in wireless networks. A typical assumption of previous work is
that relay nodes performing network coding can decode the messages from sources
perfectly. On a simple relay network, we design a scheme to obtain network
coding gain even when the relay node cannot perfectly decode its received
messages. In our scheme, the operation at the relay node resembles message
passing in belief propagation, sending the logarithm likelihood ratio (LLR) of
the network coded message to the destination. Simulation results demonstrate
the gain obtained over different channel conditions. The goal of this paper is
not to give a theoretical result, but to point to possible interaction of
network coding with user cooperation in noisy scenario. The extrinsic
information transfer (EXIT) chart is shown to be a useful engineering tool to
analyze the performance of joint channel coding and network coding in the
network."
"We analyze an uplink of a fast flat fading MIMO-CDMA channel in the case
where the data symbol vector for each user follows an arbitrary distribution.
The spectral efficiency of the channel with CSI at the receiver is evaluated
analytically with the replica method. The main result is that the hierarchical
decoupling principle holds in the MIMO-CDMA channel, i.e., the MIMO-CDMA
channel is decoupled into a bank of single-user MIMO channels in the many-user
limit, and each single-user MIMO channel is further decoupled into a bank of
scalar Gaussian channels in the many-antenna limit for a fading model with a
limited number of scatterers."
"In this paper we concentrate on rate-1/3 systematic parallel concatenated
convolutional codes and their rate-1/2 punctured child codes. Assuming
maximum-likelihood decoding over an additive white Gaussian channel, we
demonstrate that a rate-1/2 non-systematic child code can exhibit a lower error
floor than that of its rate-1/3 parent code, if a particular condition is met.
However, assuming iterative decoding, convergence of the non-systematic code
towards low bit-error rates is problematic. To alleviate this problem, we
propose rate-1/2 partially-systematic codes that can still achieve a lower
error floor than that of their rate-1/3 parent codes. Results obtained from
extrinsic information transfer charts and simulations support our conclusion."
"In this paper, we propose to study and optimize a very general class of LDPC
codes whose variable nodes belong to finite sets with different orders. We
named this class of codes Hybrid LDPC codes. Although efficient optimization
techniques exist for binary LDPC codes and more recently for non-binary LDPC
codes, they both exhibit drawbacks due to different reasons. Our goal is to
capitalize on the advantages of both families by building codes with binary (or
small finite set order) and non-binary parts in their factor graph
representation. The class of Hybrid LDPC codes is obviously larger than
existing types of codes, which gives more degrees of freedom to find good codes
where the existing codes show their limits. We give two examples where hybrid
LDPC codes show their interest."
"A construction of a new family of distributed space time codes (DSTCs) having
full diversity and low Maximum Likelihood (ML) decoding complexity is provided
for the two phase based cooperative diversity protocols of Jing-Hassibi and the
recently proposed Generalized Non-orthogonal Amplify and Forward (GNAF)
protocol of Rajan et al. The salient feature of the proposed DSTCs is that they
satisfy the extra constraints imposed by the protocols and are also four-group
ML decodable which leads to significant reduction in ML decoding complexity
compared to all existing DSTC constructions. Moreover these codes have uniform
distribution of power among the relays as well as in time. Also, simulations
results indicate that these codes perform better in comparison with the only
known DSTC with the same rate and decoding complexity, namely the Coordinate
Interleaved Orthogonal Design (CIOD). Furthermore, they perform very close to
DSTCs from field extensions which have same rate but higher decoding
complexity."
"Design criteria and full-diversity Distributed Space Time Codes (DSTCs) for
the two phase transmission based cooperative diversity protocol of Jing-Hassibi
and the Generalized Nonorthogonal Amplify and Forward (GNAF) protocol are
reported, when the relay nodes are assumed to have knowledge of the phase
component of the source to relay channel gains. It is shown that this under
this partial channel state information (CSI), several well known space time
codes for the colocated MIMO (Multiple Input Multiple Output) channel become
amenable for use as DSTCs. In particular, the well known complex orthogonal
designs, generalized coordinate interleaved orthogonal designs (GCIODs) and
unitary weight single symbol decodable (UW-SSD) codes are shown to satisfy the
required design constraints for DSTCs. Exploiting the relaxed code design
constraints, we propose DSTCs obtained from Clifford Algebras which have low ML
decoding complexity."
"We adress the problem of the algebraic decoding of any cyclic code up to the
true minimum distance. For this, we use the classical formulation of the
problem, which is to find the error locator polynomial in terms of the syndroms
of the received word. This is usually done with the Berlekamp-Massey algorithm
in the case of BCH codes and related codes, but for the general case, there is
no generic algorithm to decode cyclic codes. Even in the case of the quadratic
residue codes, which are good codes with a very strong algebraic structure,
there is no available general decoding algorithm. For this particular case of
quadratic residue codes, several authors have worked out, by hand, formulas for
the coefficients of the locator polynomial in terms of the syndroms, using the
Newton identities. This work has to be done for each particular quadratic
residue code, and is more and more difficult as the length is growing.
Furthermore, it is error-prone. We propose to automate these computations,
using elimination theory and Grbner bases. We prove that, by computing
appropriate Grbner bases, one automatically recovers formulas for the
coefficients of the locator polynomial, in terms of the syndroms."
"Discrete-time Rayleigh fading multiple-input multiple-output (MIMO) channels
are considered, with no channel state information at the transmitter and
receiver. The fading is assumed to be correlated in time and independent from
antenna to antenna. Peak and average transmit power constraints are imposed,
either on the sum over antennas, or on each individual antenna. In both cases,
an upper bound and an asymptotic lower bound, as the signal-to-noise ratio
approaches zero, on the channel capacity are presented. The limit of normalized
capacity is identified under the sum power constraints, and, for a subclass of
channels, for individual power constraints. These results carry over to a SISO
channel with delay spread (i.e. frequency selective fading)."
"A common problem on sequential-type decoding is that at the signal-to-noise
ratio (SNR) below the one corresponding to the cutoff rate, the average
decoding complexity per information bit and the required stack size grow
rapidly with the information length. In order to alleviate the problem in the
maximum-likelihood sequential decoding algorithm (MLSDA), we propose to
directly eliminate the top path whose end node is $\Delta$-trellis-level prior
to the farthest one among all nodes that have been expanded thus far by the
sequential search. Following random coding argument, we analyze the
early-elimination window $\Delta$ that results in negligible performance
degradation for the MLSDA. Our analytical results indicate that the required
early elimination window for negligible performance degradation is just twice
of the constraint length for rate one-half convolutional codes. For rate
one-third convolutional codes, the required early-elimination window even
reduces to the constraint length. The suggestive theoretical level thresholds
almost coincide with the simulation results. As a consequence of the small
early-elimination window required for near maximum-likelihood performance, the
MLSDA with early-elimination modification rules out considerable computational
burdens, as well as memory requirement, by directly eliminating a big number of
the top paths, which makes the MLSDA with early elimination very suitable for
applications that dictate a low-complexity software implementation with near
maximum-likelihood performance."
"We study the ergodic capacity of a frequency-selective Rayleigh fading
channel with correlated scattering, which finds application in the area of UWB.
Under an average power constraint, we consider a single-user, single-antenna
transmission. Coherent reception is assumed with full CSI at the receiver and
no CSI at the transmitter. We distinguish between a continuous- and a
discrete-time channel, modeled either as random process or random vector with
generic covariance. As a practically relevant example, we examine an
exponentially attenuated Ornstein-Uhlenbeck process in detail. Finally, we give
numerical results, discuss the relation between the continuous- and the
discrete-time channel model and show the significant impact of correlated
scattering."
"In this paper we study the impact of the processing order of nodes of a
bipartite graph, on the performance of an iterative message-passing decoding.
To this end, we introduce the concept of neighborhood reliabilities of graph's
nodes. Nodes reliabilities are calculated at each iteration and then are used
to obtain a processing order within a serial or serial/parallel scheduling. The
basic idea is that by processing first the most reliable data, the decoder is
reinforced before processing the less reliable one. Using neighborhood
reliabilities, the Min-Sum decoder of LDPC codes approaches the performance of
the Sum-Product decoder."
"In this paper, we obtain the scaling laws of the sum-rate capacity of a MIMO
X-channel, a 2 independent sender, 2 independent receiver channel with messages
from each transmitter to each receiver, at high signal to noise ratios (SNR).
The X-channel has sparked recent interest in the context of cooperative
networks and it encompasses the interference, multiple access, and broadcast
channels as special cases. Here, we consider the case with partially
cooperative transmitters in which only partial and asymmetric side-information
is available at one of the transmitters. It is proved that when there are M
antennas at all four nodes, the sum-rate scales like 2Mlog(SNR) which is in
sharp contrast to [\lfloor 4M/3 \rfloor,4M/3]log(SNR) for non-cooperative
X-channels \cite{maddah-ali,jafar_degrees}. This further proves that, in terms
of sum-rate scaling at high SNR, partial side-information at one of the
transmitters and full side-information at both transmitters are equivalent in
the MIMO X-channel."
"A network of $n$ wireless communication links is considered. Fading is
assumed to be the dominant factor affecting the strength of the channels
between nodes. The objective is to analyze the achievable throughput of the
network when power allocation is allowed. By proposing a decentralized on-off
power allocation strategy, a lower bound on the achievable throughput is
obtained for a general fading model. In particular, under Rayleigh fading
conditions the achieved sum-rate is of order $\log n$, which is, by a constant
factor, larger than what is obtained with a centralized scheme in the work of
Gowaikar et al. Similar to most of previous works on large networks, the
proposed scheme assigns a vanishingly small rate for each link. However, it is
shown that by allowing the sum-rate to decrease by a factor $\alpha<1$, this
scheme is capable of providing non-zero rate-per-links of order $\Theta(1)$. To
obtain larger non-zero rate-per-links, the proposed scheme is modified to a
centralized version. It turns out that for the same number of active links the
centralized scheme achieves a much larger rate-per-link. Moreover, at large
values of rate-per-link, it achieves a sum-rate close to $\log n$, i.e., the
maximum achieved by the decentralized scheme."
"This paper investigates the relationship between the rank weight distribution
of a linear code and that of its dual code. The main result of this paper is
that, similar to the MacWilliams identity for the Hamming metric, the rank
weight distribution of any linear code can be expressed as an analytical
expression of that of its dual code. Remarkably, our new identity has a similar
form to the MacWilliams identity for the Hamming metric. Our new identity
provides a significant analytical tool to the rank weight distribution analysis
of linear codes. We use a linear space based approach in the proof for our new
identity, and adapt this approach to provide an alternative proof of the
MacWilliams identity for the Hamming metric. Finally, we determine the
relationship between moments of the rank distribution of a linear code and
those of its dual code, and provide an alternative derivation of the rank
weight distribution of maximum rank distance codes."
"This paper investigates packing and covering properties of codes with the
rank metric. First, we investigate packing properties of rank metric codes.
Then, we study sphere covering properties of rank metric codes, derive bounds
on their parameters, and investigate their asymptotic covering properties."
"For a stationary additive Gaussian-noise channel with a rational noise power
spectrum of a finite-order $L$, we derive two new results for the feedback
capacity under an average channel input power constraint. First, we show that a
very simple feedback-dependent Gauss-Markov source achieves the feedback
capacity, and that Kalman-Bucy filtering is optimal for processing the
feedback. Based on these results, we develop a new method for optimizing the
channel inputs for achieving the Cover-Pombra block-length-$n$ feedback
capacity by using a dynamic programming approach that decomposes the
computation into $n$ sequentially identical optimization problems where each
stage involves optimizing $O(L^2)$ variables. Second, we derive the explicit
maximal information rate for stationary feedback-dependent sources. In general,
evaluating the maximal information rate for stationary sources requires solving
only a few equations by simple non-linear programming. For first-order
autoregressive and/or moving average (ARMA) noise channels, this optimization
admits a closed form maximal information rate formula. The maximal information
rate for stationary sources is a lower bound on the feedback capacity, and it
equals the feedback capacity if the long-standing conjecture, that stationary
sources achieve the feedback capacity, holds."
"We consider a linear Gaussian noise channel used with delayed feedback. The
channel noise is assumed to be a ARMA (autoregressive and/or moving average)
process. We reformulate the Gaussian noise channel into an intersymbol
interference channel with white noise, and show that the delayed-feedback of
the original channel is equivalent to the instantaneous-feedback of the derived
channel. By generalizing results previously developed for Gaussian channels
with instantaneous feedback and applying them to the derived intersymbol
interference channel, we show that conditioned on the delayed feedback, a
conditional Gauss-Markov source achieves the feedback capacity and its Markov
memory length is determined by the noise spectral order and the feedback delay.
A Kalman-Bucy filter is shown to be optimal for processing the feedback. The
maximal information rate for stationary sources is derived in terms of channel
input power constraint and the steady state solution of the Riccati equation of
the Kalman-Bucy filter used in the feedback loop."
"In this paper, we present an analytical analysis of the convergence of raptor
codes under joint decoding over the binary input additive white noise channel
(BIAWGNC), and derive an optimization method. We use Information Content
evolution under Gaussian approximation, and focus on a new decoding scheme that
proves to be more efficient: the joint decoding of the two code components of
the raptor code. In our general model, the classical tandem decoding scheme
appears to be a subcase, and thus, the design of LT codes is also possible."
"Capacity gains from transmitter and receiver cooperation are compared in a
relay network where the cooperating nodes are close together. Under
quasi-static phase fading, when all nodes have equal average transmit power
along with full channel state information (CSI), it is shown that transmitter
cooperation outperforms receiver cooperation, whereas the opposite is true when
power is optimally allocated among the cooperating nodes but only CSI at the
receiver (CSIR) is available. When the nodes have equal power with CSIR only,
cooperative schemes are shown to offer no capacity improvement over
non-cooperation under the same network power constraint. When the system is
under optimal power allocation with full CSI, the decode-and-forward
transmitter cooperation rate is close to its cut-set capacity upper bound, and
outperforms compress-and-forward receiver cooperation. Under fast Rayleigh
fading in the high SNR regime, similar conclusions follow. Cooperative systems
provide resilience to fading in channel magnitudes; however, capacity becomes
more sensitive to power allocation, and the cooperating nodes need to be closer
together for the decode-and-forward scheme to be capacity-achieving. Moreover,
to realize capacity improvement, full CSI is necessary in transmitter
cooperation, while in receiver cooperation optimal power allocation is
essential."
"A $K$-user memoryless interference channel is considered where each receiver
sequentially decodes the data of a subset of transmitters before it decodes the
data of the designated transmitter. Therefore, the data rate of each
transmitter depends on (i) the subset of receivers which decode the data of
that transmitter, (ii) the decoding order, employed at each of these receivers.
In this paper, a greedy algorithm is developed to find the users which are
decoded at each receiver and the corresponding decoding order such that the
minimum rate of the users is maximized. It is proven that the proposed
algorithm is optimal."
"In this paper, we consider an automatic-repeat-request (ARQ) retransmission
protocol signaling over a block-fading multiple-input, multiple-output (MIMO)
channel. Unlike previous work, we allow for multiple fading blocks within each
transmission (ARQ round), and we constrain the transmitter to fixed rate codes
constructed over complex signal constellations. In particular, we examine the
general case of average input-power-constrained constellations as well as the
practically important case of finite discrete constellations. This scenario is
a suitable model for practical wireless communications systems employing
orthogonal frequency division multiplexing techniques over a MIMO ARQ channel.
Two cases of fading dynamics are considered, namely short-term static fading
where channel fading gains change randomly for each ARQ round, and long-term
static fading where channel fading gains remain constant over all ARQ rounds
pertaining to a given message. As our main result, we prove that for the
block-fading MIMO ARQ channel with discrete input signal constellation
satisfying a short-term power constraint, the optimal signal-to-noise ratio
(SNR) exponent is given by a modified Singleton bound, relating all the system
parameters. To demonstrate the practical significance of the theoretical
analysis, we present numerical results showing that practical
Singleton-bound-achieving maximum distance separable codes achieve the optimal
SNR exponent."
"We construct a class of linear space-time block codes for any number of
transmit antennas that have controllable ML decoding complexity with a maximum
rate of 1 symbol per channel use. The decoding complexity for $M$ transmit
antennas can be varied from ML decoding of $2^{\lceil \log_2M \rceil -1}$
symbols together to single symbol ML decoding. For ML decoding of $2^{\lceil
\log_2M \rceil - n}$ ($n=1,2,...$) symbols together, a diversity of
$\min(M,2^{\lceil \log_2M \rceil-n+1})$ can be achieved. Numerical results show
that the performance of the constructed code when $2^{\lceil \log_2M \rceil-1}$
symbols are decoded together is quite close to the performance of ideal rate-1
orthogonal codes (that are non-existent for more than 2 transmit antennas)."
"It is known at a qualitative level that directional antennas can be used to
boost the capacity of wireless ad hoc networks. Lacking is a measure to
quantify this advantage and to compare directional antennas of different
footprint patterns. This paper introduces the concept of the effective beam
width (and the effective null width as its dual counterpart) as a measure which
quantitatively captures the capacity-boosting capability of directional
antennas. Beam width is commonly defined to be the directional angle spread
within which the main-lobe beam power is above a certain threshold. In
contrast, our effective beam width definition lumps the effects of the (i)
antenna pattern, (ii) active-node distribution, and (iii) channel
characteristics, on network capacity into a single quantitative measure. We
investigate the mathematical properties of the effective beam width and show
how the convenience afforded by these properties can be used to analyze the
effectiveness of complex directional antenna patterns in boosting network
capacity, with fading and multi-user interference taken into account. In
particular, we derive the extent to which network capacity can be scaled with
the use of phased array antennas. We show that a phased array antenna with N
elements can boost transport capacity of an Aloha-like network by a factor of
order N^1.620."
"List decoding for arbitrarily varying channels (AVCs) under state constraints
is investigated. It is shown that rates within $\epsilon$ of the randomized
coding capacity of AVCs with input-dependent state can be achieved under
maximal error with list decoding using lists of size $O(1/\epsilon)$. Under
average error an achievable rate region and converse bound are given for lists
of size $L$. These bounds are based on two different notions of
symmetrizability and do not coincide in general. An example is given that shows
that for list size $L$ the capacity may be positive but strictly smaller than
the randomized coding capacity. This behavior is different than the situation
without state constraints."
"We consider a dense fading multi-user network with multiple active
multi-antenna source-destination pair terminals communicating simultaneously
through a large common set of $K$ multi-antenna relay terminals in the full
spatial multiplexing mode. We use Shannon-theoretic tools to analyze the
tradeoff between energy efficiency and spectral efficiency (known as the power-
bandwidth tradeoff) in meaningful asymptotic regimes of signal-to-noise ratio
(SNR) and network size. We design linear distributed multi-antenna relay
beamforming (LDMRB) schemes that exploit the spatial signature of multi-user
interference and characterize their power-bandwidth tradeoff under a system
wide power constraint on source and relay transmissions. The impact of multiple
users, multiple relays and multiple antennas on the key performance measures of
the high and low SNR regimes is investigated in order to shed new light on the
possible reduction in power and bandwidth requirements through the usage of
such practical relay cooperation techniques. Our results indicate that
point-to-point coded multi-user networks supported by distributed relay
beamforming techniques yield enhanced energy efficiency and spectral
efficiency, and with appropriate signaling and sufficient antenna degrees of
freedom, can achieve asymptotically optimal power-bandwidth tradeoff with the
best possible (i.e., as in the cutset bound) energy scaling of $K^{-1}$ and the
best possible spectral efficiency slope at any SNR for large number of relay
terminals."
"In this paper, an $n$-user Gaussian interference channel, where the power of
the transmitters are subject to some upper-bounds is studied. We obtain a
closed-form expression for the rate region of such a channel based on the
Perron-Frobenius theorem. While the boundary of the rate region for the case of
unconstrained power is a well-established result, this is the first result for
the case of constrained power. We extend this result to the time-varying
channels and obtain a closed-form solution for the rate region of such
channels."
"We consider joint transceiver design for general Multiple-Input
Multiple-Output communication systems that implement interference
(pre-)subtraction, such as those based on Decision Feedback Equalization (DFE)
or Tomlinson-Harashima precoding (THP). We develop a unified framework for
joint transceiver design by considering design criteria that are expressed as
functions of the Mean Square Error (MSE) of the individual data streams. By
deriving two inequalities that involve the logarithms of the individual MSEs,
we obtain optimal designs for two classes of communication objectives, namely
those that are Schur-convex and Schur-concave functions of these logarithms.
For Schur-convex objectives, the optimal design results in data streams with
equal MSEs. This design simultaneously minimizes the total MSE and maximizes
the mutual information for the DFE-based model. For Schur-concave objectives,
the optimal DFE design results in linear equalization and the optimal THP
design results in linear precoding. The proposed framework embraces a wide
range of design objectives and can be regarded as a counterpart of the existing
framework of linear transceiver design."
"We consider a multi-object detection problem over a sensor network (SNET)
with limited range sensors. This problem complements the widely considered
decentralized detection problem where all sensors observe the same object.
While the necessity for global collaboration is clear in the decentralized
detection problem, the benefits of collaboration with limited range sensors is
unclear and has not been widely explored. In this paper we develop a
distributed detection approach based on recent development of the false
discovery rate (FDR). We first extend the FDR procedure and develop a
transformation that exploits complete or partial knowledge of either the
observed distributions at each sensor or the ensemble (mixture) distribution
across all sensors. We then show that this transformation applies to
multi-dimensional observations, thus extending FDR to multi-dimensional
settings. We also extend FDR theory to cases where distributions under both
null and positive hypotheses are uncertain. We then propose a robust
distributed algorithm to perform detection. We further demonstrate scalability
to large SNETs by showing that the upper bound on the communication complexity
scales linearly with the number of sensors that are in the vicinity of objects
and is independent of the total number of sensors. Finally, we deal with
situations where the sensing model may be uncertain and establish robustness of
our techniques to such uncertainties."
"A much simpler proof of Theorem 1 from M.Burnashev ""Code spectrum and
reliability function: Binary symmetric channel"" is presented."
"This paper formulates and studies a general distributed field reconstruction
problem using a dense network of noisy one-bit randomized scalar quantizers in
the presence of additive observation noise of unknown distribution. A
constructive quantization, coding, and field reconstruction scheme is developed
and an upper-bound to the associated mean squared error (MSE) at any point and
any snapshot is derived in terms of the local spatio-temporal smoothness
properties of the underlying field. It is shown that when the noise, sensor
placement pattern, and the sensor schedule satisfy certain weak technical
requirements, it is possible to drive the MSE to zero with increasing sensor
density at points of field continuity while ensuring that the per-sensor
bitrate and sensing-related network overhead rate simultaneously go to zero.
The proposed scheme achieves the order-optimal MSE versus sensor density
scaling behavior for the class of spatially constant spatio-temporal fields."
"Motivated by video coding applications, the problem of sequential coding of
correlated sources with encoding and/or decoding frame-delays is studied. The
fundamental tradeoffs between individual frame rates, individual frame
distortions, and encoding/decoding frame-delays are derived in terms of a
single-letter information-theoretic characterization of the rate-distortion
region for general inter-frame source correlations and certain types of
potentially frame specific and coupled single-letter fidelity criteria. The
sum-rate-distortion region is characterized in terms of generalized directed
information measures highlighting their role in delayed sequential source
coding problems. For video sources which are spatially stationary memoryless
and temporally Gauss-Markov, MSE frame distortions, and a sum-rate constraint,
our results expose the optimality of idealized differential predictive coding
among all causal sequential coders, when the encoder uses a positive rate to
describe each frame. Somewhat surprisingly, causal sequential encoding with
one-frame-delayed noncausal sequential decoding can exactly match the
sum-rate-MSE performance of joint coding for all nontrivial MSE-tuples
satisfying certain positive semi-definiteness conditions. Thus, even a single
frame-delay holds potential for yielding significant performance improvements.
Generalizations to higher order Markov sources are also presented and
discussed. A rate-distortion performance equivalence between, causal sequential
encoding with delayed noncausal sequential decoding, and, delayed noncausal
sequential encoding with causal sequential decoding, is also established."
"A power optimal scheduling algorithm that guarantees desired throughput and
bounded delay to each user is developed for fading multi-access multi-band
systems. The optimization is over the joint space of all rate allocation and
coding strategies. The proposed scheduling assigns rates on each band based
only on the current system state, and subsequently uses optimal multi-user
signaling to achieve these rates. The scheduling is computationally simple, and
hence scalable. Due to uplink-downlink duality, all the results extend in
straightforward fashion to the broadcast channels."
"Design of Space-Time Block Codes (STBCs) for Maximum Likelihood (ML)
reception has been predominantly the main focus of researchers. However, the ML
decoding complexity of STBCs becomes prohibitive large as the number of
transmit and receive antennas increase. Hence it is natural to resort to a
suboptimal reception technique like linear Minimum Mean Squared Error (MMSE)
receiver. Barbarossa et al and Liu et al have independently derived necessary
and sufficient conditions for a full rate linear STBC to be MMSE optimal, i.e
achieve least Symbol Error Rate (SER). Motivated by this problem, certain
existing high rate STBC constructions from crossed product algebras are
identified to be MMSE optimal. Also, it is shown that a certain class of codes
from cyclic division algebras which are special cases of crossed product
algebras are MMSE optimal. Hence, these STBCs achieve least SER when MMSE
reception is employed and are fully diverse when ML reception is employed."
"We study the problem of computing the rate-distortion function for sources
with feed-forward and the capacity for channels with feedback. The formulas
(involving directed information) for the optimal rate-distortion function with
feed-forward and channel capacity with feedback are multi-letter expressions
and cannot be computed easily in general. In this work, we derive conditions
under which these can be computed for a large class of sources/channels with
memory and distortion/cost measures. Illustrative examples are also provided."
"In a recent paper [1] an improved analysis concerning the analysis of List
Decoding was presented. The event that the correct codeword is excluded from
the list is central. For the additive white Gaussian noise (AWGN) channel an
important quantity is the in [1] called effective Euclidean distance. This was
earlier considered in [2] under the name Vector Euclidean Distance, where also
a simple mathematical expression for this quantity was easily derived for any
list size. In [1], a geometrical analysis gives this when the list size is 1, 2
or 3."
"A definition of atomic codeword for a group code is presented. Some
properties of atomic codewords of group codes are investigated. Using these
properties, it is shown that every minimal tail-biting trellis for a group code
over a finite abelian group can be constructed from its characteristic
generators, which extends the work of Koetter and Vardy who treated the case of
a linear code over a field. We also present an efficient algorithm for
constructing the minimal tail-biting trellis of a group code over a finite
abelian group, given a generator matrix."
"It is well known that Space-Time Block Codes (STBCs) obtained from Orthogonal
Designs (ODs) are single-symbol-decodable (SSD) and from Quasi-Orthogonal
Designs (QODs) are double-symbol decodable. However, there are SSD codes that
are not obtainable from ODs and DSD codes that are not obtainable from QODs. In
this paper a method of constructing $g$-symbol decodable ($g$-SD) STBCs using
representations of Clifford algebras are presented which when specialized to
$g=1,2$ gives SSD and DSD codes respectively. For the number of transmit
antennas $2^a$ the rate (in complex symbols per channel use) of the $g$-SD
codes presented in this paper is $\frac{a+1-g}{2^{a-g}}$. The maximum rate of
the DSD STBCs from QODs reported in the literature is $\frac{a}{2^{a-1}}$ which
is smaller than the rate $\frac{a-1}{2^{a-2}}$ of the DSD codes of this paper,
for $2^a$ transmit antennas. In particular, the reported DSD codes for 8 and 16
transmit antennas offer rates 1 and 3/4 respectively whereas the known STBCs
from QODs offer only 3/4 and 1/2 respectively. The construction of this paper
is applicable for any number of transmit antennas."
"Belief Propagation (BP) and Linear Programming (LP) decodings of Low Density
Parity Check (LDPC) codes are discussed. We summarize results of
instanton/pseudo-codeword approach developed for analysis of the error-floor
domain of the codes. Instantons are special, code and decoding specific,
configurations of the channel noise contributing most to the Frame-Error-Rate
(FER). Instantons are decoded into pseudo-codewords. Instanton/pseudo-codeword
with the lowest weight describes the largest Signal-to-Noise-Ratio (SNR)
asymptotic of FER, while the whole spectra of the low weight instantons is
descriptive of the FER vs SNR profile in the extended error-floor domain.
First, we describe a general optimization method that allows to find the
instantons for any coding/decoding. Second, we introduce LP-specific
pseudo-codeword search algorithm that allows efficient calculations of the
pseudo-codeword spectra. Finally, we discuss results of combined BP/LP
error-floor exploration experiments for two model codes."
"This paper addresses the following question, which is of interest in the
design and deployment of a multiuser decentralized network. Given a total
system bandwidth of W Hz and a fixed data rate constraint of R bps for each
transmission, how many frequency slots N of size W/N should the band be
partitioned into to maximize the number of simultaneous transmissions in the
network? In an interference-limited ad-hoc network, dividing the available
spectrum results in two competing effects: on the positive side, it reduces the
number of users on each band and therefore decreases the interference level
which leads to an increased SINR, while on the negative side the SINR
requirement for each transmission is increased because the same information
rate must be achieved over a smaller bandwidth. Exploring this tradeoff between
bandwidth and SINR and determining the optimum value of N in terms of the
system parameters is the focus of the paper. Using stochastic geometry, we
analytically derive the optimal SINR threshold (which directly corresponds to
the optimal spectral efficiency) on this tradeoff curve and show that it is a
function of only the path loss exponent. Furthermore, the optimal SINR point
lies between the low-SINR (power-limited) and high-SINR (bandwidth-limited)
regimes. In order to operate at this optimal point, the number of frequency
bands (i.e., the reuse factor) should be increased until the threshold SINR,
which is an increasing function of the reuse factor, is equal to the optimal
value."
"We consider a MIMO fading broadcast channel and compare the achievable
ergodic rates when the channel state information at the transmitter is provided
by analog noisy feedback or by quantized (digital) feedback. The superiority of
digital feedback is shown, with perfect or imperfect CSIR, whenever the number
of feedback channel uses per channel coefficient is larger than 1. Also, we
show that by proper design of the digital feedback link, errors in the feedback
have a minor effect even by using very simple uncoded modulation. Finally, we
show that analog feedback achieves a fraction 1 - 2F of the optimal
multiplexing gain even in the presence of a feedback delay, when the fading
belongs to the class of Doppler processes with normalized maximum Doppler
frequency shift 0 <= F <= 1/2."
"In this paper, we propose two new models of spatial correlations in sensor
data in a data-gathering sensor network. A particular property of these models
is that if a sensor node knows in \textit{how many} bits it needs to transmit
its data, then it also knows \textit{which} bits of its data it needs to
transmit."
"This paper derives the outage probability and transmission capacity of ad hoc
wireless networks with nodes employing multiple antenna diversity techniques,
for a general class of signal distributions. This analysis allows system
performance to be quantified for fading or non-fading environments. The
transmission capacity is given for interference-limited uniformly random
networks on the entire plane with path loss exponent $\alpha>2$ in which nodes
use: (1) static beamforming through $M$ sectorized antennas, for which the
increase in transmission capacity is shown to be $\Theta(M^2)$ if the antennas
are without sidelobes, but less in the event of a nonzero sidelobe level; (2)
dynamic eigen-beamforming (maximal ratio transmission/combining), in which the
increase is shown to be $\Theta(M^{\frac{2}{\alpha}})$; (3) various transmit
antenna selection and receive antenna selection combining schemes, which give
appreciable but rapidly diminishing gains; and (4) orthogonal space-time block
coding, for which there is only a small gain due to channel hardening,
equivalent to Nakagami-$m$ fading for increasing $m$. It is concluded that in
ad hoc networks, static and dynamic beamforming perform best, selection
combining performs well but with rapidly diminishing returns with added
antennas, and that space-time block coding offers only marginal gains."
"The capacity of the two-user Gaussian interference channel has been open for
thirty years. The understanding on this problem has been limited. The best
known achievable region is due to Han-Kobayashi but its characterization is
very complicated. It is also not known how tight the existing outer bounds are.
In this work, we show that the existing outer bounds can in fact be arbitrarily
loose in some parameter ranges, and by deriving new outer bounds, we show that
a simplified Han-Kobayashi type scheme can achieve to within a single bit the
capacity for all values of the channel parameters. We also show that the scheme
is asymptotically optimal at certain high SNR regimes. Using our results, we
provide a natural generalization of the point-to-point classical notion of
degrees of freedom to interference-limited scenarios."
"We investigate the stopping redundancy hierarchy of linear block codes and
its connection to permutation decoding techniques. An element in the ordered
list of stopping redundancy values represents the smallest number of possibly
linearly dependent rows in any parity-check matrix of a code that avoids
stopping sets of a given size. Redundant parity-check equations can be shown to
have a similar effect on decoding performance as permuting the coordinates of
the received codeword according to a selected set of automorphisms of the code.
Based on this finding we develop new decoding strategies for data transmission
over the binary erasure channel that combine iterative message passing and
permutation decoding in order to avoid errors confined to stopping sets. We
also introduce the notion of s-SAD sets, containing the smallest number of
automorphisms of a code with the property that they move any set of not more
than s erasures into positions that do not correspond to stopping sets within a
judiciously chosen parity-check matrix."
"Random linear network coding is a particularly decentralized approach to the
multicast problem. Use of random network codes introduces a non-zero
probability however that some sinks will not be able to successfully decode the
required sources. One of the main theoretical motivations for random network
codes stems from the lower bound on the probability of successful decoding
reported by Ho et. al. (2003). This result demonstrates that all sinks in a
linearly solvable network can successfully decode all sources provided that the
random code field size is large enough. This paper develops a new bound on the
probability of successful decoding."
"This paper presents new lower and upper bounds for the compression rate of
binary prefix codes optimized over memoryless sources according to various
nonlinear codeword length objectives. Like the most well-known redundancy
bounds for minimum average redundancy coding - Huffman coding - these are in
terms of a form of entropy and/or the probability of an input symbol, often the
most probable one. The bounds here, some of which are tight, improve on known
bounds of the form L in [H,H+1), where H is some form of entropy in bits (or,
in the case of redundancy objectives, 0) and L is the length objective, also in
bits. The objectives explored here include exponential-average length, maximum
pointwise redundancy, and exponential-average pointwise redundancy (also called
dth exponential redundancy). The first of these relates to various problems
involving queueing, uncertainty, and lossless communications; the second
relates to problems involving Shannon coding and universal modeling. For these
two objectives we also explore the related problem of the necessary and
sufficient conditions for the shortest codeword of a code being a specific
length."
"This paper studies properties of entropy functions that are induced by groups
and subgroups. We showed that many information theoretic properties of those
group induced entropy functions also have corresponding group theoretic
interpretations. Then we propose an extension method to find outer bound for
these group induced entropy functions."
"This work is devoted to practical joint source channel coding. Although the
proposed approach has more general scope, for the sake of clarity we focus on a
specific application example, namely, the transmission of digital images over
noisy binary-input output-symmetric channels. The basic building blocks of most
state-of the art source coders are: 1) a linear transformation; 2) scalar
quantization of the transform coefficients; 3) probability modeling of the
sequence of quantization indices; 4) an entropy coding stage. We identify the
weakness of the conventional separated source-channel coding approach in the
catastrophic behavior of the entropy coding stage. Hence, we replace this stage
with linear coding, that maps directly the sequence of redundant quantizer
output symbols into a channel codeword. We show that this approach does not
entail any loss of optimality in the asymptotic regime of large block length.
However, in the practical regime of finite block length and low decoding
complexity our approach yields very significant improvements. Furthermore, our
scheme allows to retain the transform, quantization and probability modeling of
current state-of the art source coders, that are carefully matched to the
features of specific classes of sources. In our working example, we make use of
``bit-planes'' and ``contexts'' model defined by the JPEG2000 standard and we
re-interpret the underlying probability model as a sequence of conditionally
Markov sources. The Markov structure allows to derive a simple successive
coding and decoding scheme, where the latter is based on iterative Belief
Propagation. We provide a construction example of the proposed scheme based on
punctured Turbo Codes and we demonstrate the gain over a conventional separated
scheme by running extensive numerical experiments on test images."
"Recently, Tarokh and others have raised the possibility that a cognitive
radio might know the interference signal being transmitted by a strong primary
user in a non-causal way, and use this knowledge to increase its data rates.
However, there is a subtle difference between knowing the signal transmitted by
the primary and the actual interference at our receiver since there is a
wireless channel between these two points. We show that even an unknown phase
results in a substantial decrease in the data rates that can be achieved, and
thus there is a need to feedback interference channel estimates to the
cognitive transmitter. We then consider the case of fading channels. We derive
an upper bound on the rate for given outage error probability for faded dirt.
We give a scheme that uses appropriate ""training"" to obtain such estimates and
quantify this scheme's required overhead as a function of the relevant
coherence time and interference power."
"This paper investigates general properties of codes with the rank metric. We
first investigate asymptotic packing properties of rank metric codes. Then, we
study sphere covering properties of rank metric codes, derive bounds on their
parameters, and investigate their asymptotic covering properties. Finally, we
establish several identities that relate the rank weight distribution of a
linear code to that of its dual code. One of our identities is the counterpart
of the MacWilliams identity for the Hamming metric, and it has a different form
from the identity by Delsarte."
"We analyze the performance of coherent impulsive-radio (IR) ultra-wideband
(UWB) channel in presence of the interference generated by concurrent
transmissions of the systems with the same impulsive radio. We derive a novel
algorithm, using Monte-Carlo method, to calculate a lower bound on the rate
that can be achieved using maximum-likelihood estimator. Using this bound we
show that such a channel is very robust to interference, in contrast to the
nearest-neighbor detector."
"Wyner's wiretap channel is extended to parallel broadcast channels and fading
channels with multiple receivers. In the first part of the paper, we consider
the setup of parallel broadcast channels with one sender, multiple intended
receivers, and one eavesdropper. We study the situations where the sender
broadcasts either a common message or independent messages to the intended
receivers. We derive upper and lower bounds on the common-message-secrecy
capacity, which coincide when the users are reversely degraded. For the case of
independent messages we establish the secrecy sum-capacity when the users are
reversely degraded.
  In the second part of the paper we apply our results to fading channels:
perfect channel state information of all intended receivers is known globally,
whereas the eavesdropper channel is known only to her. For the common message
case, a somewhat surprising result is proven: a positive rate can be achieved
independently of the number of intended receivers. For independent messages, an
opportunistic transmission scheme is presented that achieves the secrecy
sum-capacity in the limit of large number of receivers. Our results are stated
for a fast fading channel model. Extensions to the block fading model are also
discussed."
"We study information-theoretic security for discrete memoryless interference
and broadcast channels with independent confidential messages sent to two
receivers. Confidential messages are transmitted to their respective receivers
with information-theoretic secrecy. That is, each receiver is kept in total
ignorance with respect to the message intended for the other receiver. The
secrecy level is measured by the equivocation rate at the eavesdropping
receiver. In this paper, we present inner and outer bounds on secrecy capacity
regions for these two communication systems. The derived outer bounds have an
identical mutual information expression that applies to both channel models.
The difference is in the input distributions over which the expression is
optimized. The inner bound rate regions are achieved by random binning
techniques. For the broadcast channel, a double-binning coding scheme allows
for both joint encoding and preserving of confidentiality. Furthermore, we show
that, for a special case of the interference channel, referred to as the switch
channel, the two bound bounds meet. Finally, we describe several transmission
schemes for Gaussian interference channels and derive their achievable rate
regions while ensuring mutual information-theoretic secrecy. An encoding scheme
in which transmitters dedicate some of their power to create artificial noise
is proposed and shown to outperform both time-sharing and simple multiplexed
transmission of the confidential messages."
"This paper presents a class of multi-channel cosine-modulated filter banks
satisfying the perfect reconstruction (PR) property using an IIR prototype
filter. By imposing a suitable structure on the polyphase filter coefficients,
we show that it is possible to greatly simplify the PR condition, while
preserving the causality and stability of the system. We derive closed-form
expressions for the synthesis filters and also study the numerical stability of
the filter bank using frame theoretic bounds. Further, we show that it is
possible to implement this filter bank with much lower number of arithmetic
operations when compared to FIR filter banks with comparable performance. The
filter bank's modular structure also lends itself to efficient VLSI
implementation."
"An identity between two versions of the Chernoff bound on the probability a
certain large deviations event, is established. This identity has an
interpretation in statistical physics, namely, an isothermal equilibrium of a
composite system that consists of multiple subsystems of particles. Several
information--theoretic application examples, where the analysis of this large
deviations probability naturally arises, are then described from the viewpoint
of this statistical mechanical interpretation. This results in several
relationships between information theory and statistical physics, which we
hope, the reader will find insightful."
"In this paper, we present a simple technique to approximate the performance
union bound of a punctured turbo code. The bound approximation exploits only
those terms of the transfer function that have a major impact on the overall
performance. We revisit the structure of the constituent convolutional encoder
and we develop a rapid method to calculate the most significant terms of the
transfer function of a turbo encoder. We demonstrate that, for a large
interleaver size, this approximation is very accurate. Furthermore, we apply
our proposed method to a family of punctured turbo codes, which we call
pseudo-randomly punctured codes. We conclude by emphasizing the benefits of our
approach compared to those employed previously. We also highlight the
advantages of pseudo-random puncturing over other puncturing schemes."
"Consider a d*n matrix A, with d<n. The problem of solving for x in y=Ax is
underdetermined, and has infinitely many solutions (if there are any). Given y,
the minimum Kolmogorov complexity solution (MKCS) of the input x is defined to
be an input z (out of many) with minimum Kolmogorov-complexity that satisfies
y=Az. One expects that if the actual input is simple enough, then MKCS will
recover the input exactly. This paper presents a preliminary study of the
existence and value of the complexity level up to which such a complexity-based
recovery is possible. It is shown that for the set of all d*n binary matrices
(with entries 0 or 1 and d<n), MKCS exactly recovers the input for an
overwhelming fraction of the matrices provided the Kolmogorov complexity of the
input is O(d). A weak converse that is loose by a log n factor is also
established for this case. Finally, we investigate the difficulty of finding a
matrix that has the property of recovering inputs with complexity of O(d) using
MKCS."
"Orthogonal coding schemes, known to asymptotically achieve the capacity per
unit cost (CPUC) for single-user ergodic memoryless channels with a zero-cost
input symbol, are investigated for single-user compound memoryless channels,
which exhibit uncertainties in their input-output statistical relationships. A
minimax formulation is adopted to attain robustness. First, a class of
achievable rates per unit cost (ARPUC) is derived, and its utility is
demonstrated through several representative case studies. Second, when the
uncertainty set of channel transition statistics satisfies a convexity
property, optimization is performed over the class of ARPUC through utilizing
results of minimax robustness. The resulting CPUC lower bound indicates the
ultimate performance of the orthogonal coding scheme, and coincides with the
CPUC under certain restrictive conditions. Finally, still under the convexity
property, it is shown that the CPUC can generally be achieved, through
utilizing a so-called mixed strategy in which an orthogonal code contains an
appropriate composition of different nonzero-cost input symbols."
"Low-Density Parity-Check (LDPC) codes are usually decoded by running an
iterative belief-propagation, or message-passing, algorithm over the factor
graph of the code. The traditional message-passing schedule consists of
updating all the variable nodes in the graph, using the same pre-update
information, followed by updating all the check nodes of the graph, again,
using the same pre-update information. Recently several studies show that
sequential scheduling, in which messages are generated using the latest
available information, significantly improves the convergence speed in terms of
number of iterations. Sequential scheduling raises the problem of finding the
best sequence of message updates. This paper presents practical scheduling
strategies that use the value of the messages in the graph to find the next
message to be updated. Simulation results show that these informed update
sequences require significantly fewer iterations than standard sequential
schedules. Furthermore, the paper shows that informed scheduling solves some
standard trapping set errors. Therefore, it also outperforms traditional
scheduling for a large numbers of iterations. Complexity and implementability
issues are also addressed."
"We formulate the classical decoding algorithm of alternant codes afresh based
on interpolation as in Sudan's list decoding of Reed-Solomon codes, and thus
get rid of the key equation and the linear recurring sequences in the theory.
The result is a streamlined exposition of the decoding algorithm using a bit of
the theory of Groebner bases of modules."
"The downlink of a multiple-input multiple output (MIMO) broadcast channel
(BC) is considered, where each receiver is equipped with a single antenna and
the transmitter performs nonlinear Dirty-Paper Coding (DPC). We present an
efficient algorithm that finds the optimum transmit filters and power
allocation as well as the optimum precoding order(s) possibly affording
time-sharing between individual DPC orders. Subsequently necessary and
sufficient conditions for the optimality of an arbitrary precoding order are
derived. Based on these we propose a suboptimal algorithm showing excellent
performance and having low complexity."
"In this paper, a new approach for decoding low-rate Reed-Solomon codes beyond
half the minimum distance is considered and analyzed. Unlike the Sudan
algorithm published in 1997, this new approach is based on multi-sequence
shift-register synthesis, which makes it easy to understand and simple to
implement. The computational complexity of this shift-register based algorithm
is of the same order as the complexity of the well-known Berlekamp-Massey
algorithm. Moreover, the error correcting radius coincides with the error
correcting radius of the original Sudan algorithm, and the practical decoding
performance observed on a q-ary symmetric channel (QSC) is virtually identical
to the decoding performance of the Sudan algorithm. Bounds for the failure and
error probability as well as for the QSC decoding performance of the new
algorithm are derived, and the performance is illustrated by means of examples."
"Zhang et. al. recently derived upper and lower bounds on the achievable
diversity of an N_R x N_T i.i.d. Rayleigh fading multiple antenna system using
transmit antenna selection, spatial multiplexing and a linear receiver
structure. For the case of L = 2 transmitting (out of N_T available) antennas
the bounds are tight and therefore specify the maximal diversity order. For the
general case with L <= min(N_R,N_T) transmitting antennas it was conjectured
that the maximal diversity is (N_T-L+1)(N_R-L+1) which coincides with the lower
bound. Herein, we prove this conjecture for the zero forcing and zero forcing
decision feedback (with optimal detection ordering) receiver structures."
"We address the problem,`Is a local tree structure sufficient for the local
optimality of message passing algorithm in low density parity check codes?'.It
is shown that the answer is negative. Using this observation, we pinpoint a
flaw in the proof of Theorem 1 in the paper `The Capacity of Low-Density
Parity-Check Codes Under Message-Passing Decoding' by Thomas J. Richardson and
R\""udiger L.Urbanke\cite{RUCapacity}. We further provide a new proof of that
theorem based on a different argument."
"Since the classical work of Berlekamp, McEliece and van Tilborg, it is well
known that the problem of exact maximum-likelihood (ML) decoding of general
linear codes is NP-hard. In this paper, we show that exact ML decoding of a
classs of asymptotically good error correcting codes--expander codes, a special
case of low density parity check (LDPC) codes--over binary symmetric channels
(BSCs) is possible with an expected polynomial complexity. More precisely, for
any bit-flipping probability, $p$, in a nontrivial range, there exists a rate
region of non-zero support and a family of asymptotically good codes, whose
error probability decays exponentially in coding length $n$, for which ML
decoding is feasible in expected polynomial time. Furthermore, as $p$
approaches zero, this rate region approaches the channel capacity region. The
result is based on the existence of polynomial-time suboptimal decoding
algorithms that provide an ML certificate and the ability to compute the
probability that the suboptimal decoder yields the ML solution. One such ML
certificate decoder is the LP decoder of Feldman; we also propose a more
efficient $O(n^2)$ algorithm based on the work of Sipser and Spielman and the
Ford-Fulkerson algorithm. The results can be extended to AWGN channels and
suggest that it may be feasible to eliminate the error floor phenomenon
associated with message-passage decoding of LDPC codes in the high SNR regime.
Finally, we observe that the argument of Berlekamp, McEliece and van Tilborg
can be used to show that ML decoding of the considered class of codes
constructed from LDPC codes with regular left degree, of which the considered
expander codes are a special case, remains NP-hard; thus giving an interesting
contrast between the worst-case and expected complexities."
"Source coding theorems and Shannon rate-distortion functions were studied for
the discrete-time Wiener process by Berger and generalized to nonstationary
Gaussian autoregressive processes by Gray and by Hashimoto and Arimoto.
Hashimoto and Arimoto provided an example apparently contradicting the methods
used in Gray, implied that Gray's rate-distortion evaluation was not correct in
the nonstationary case, and derived a new formula that agreed with previous
results for the stationary case and held in the nonstationary case. In this
correspondence it is shown that the rate-distortion formulas of Gray and
Hashimoto and Arimoto are in fact consistent and that the example of of
Hashimoto and Arimoto does not form a counter example to the methods or results
of the earlier paper. Their results do provide an alternative, but equivalent,
formula for the rate-distortion function in the nonstationary case and they
provide a concrete example that the classic Kolmogorov formula differs from the
autoregressive formula when the autoregressive source is not stationary. Some
observations are offered on the different versions of the Toeplitz asymptotic
eigenvalue distribution theorem used in the two papers to emphasize how a
slight modification of the classic theorem avoids the problems with certain
singularities."
"We consider the problem of transmitting data at rate R over a state dependent
channel p(y|x,s) with the state information available at the sender and at the
same time conveying the information about the channel state itself to the
receiver. The amount of state information that can be learned at the receiver
is captured by the mutual information I(S^n; Y^n) between the state sequence
S^n and the channel output Y^n. The optimal tradeoff is characterized between
the information transmission rate R and the state uncertainty reduction rate
\Delta, when the state information is either causally or noncausally available
at the sender. This result is closely related and in a sense dual to a recent
study by Merhav and Shamai, which solves the problem of masking the state
information from the receiver rather than conveying it."
"We investigate the effect of feedback delay on the outage probability of
multiple-input single-output (MISO) fading channels. Channel state information
at the transmitter (CSIT) is a delayed version of the channel state information
available at the receiver (CSIR). We consider two cases of CSIR: (a) perfect
CSIR and (b) CSI estimated at the receiver using training symbols. With perfect
CSIR, under a short-term power constraint, we determine: (a) the outage
probability for beamforming with imperfect CSIT (BF-IC) analytically, and (b)
the optimal spatial power allocation (OSPA) scheme that minimizes outage
numerically. Results show that, for delayed CSIT, BF-IC is close to optimal for
low SNR and uniform spatial power allocation (USPA) is close to optimal at high
SNR. Similarly, under a long-term power constraint, we show that BF-IC is close
to optimal for low SNR and USPA is close to optimal at high SNR. With imperfect
CSIR, we obtain an upper bound on the outage probability with USPA and BF-IC.
Results show that the loss in performance due to imperfection in CSIR is not
significant, if the training power is chosen appropriately."
"In coded bi-directional cooperation, two nodes wish to exchange messages over
a shared half-duplex channel with the help of a relay. In this paper, we derive
performance bounds for this problem for each of three protocols.
  The first protocol is a two phase protocol were both users simultaneously
transmit during the first phase and the relay alone transmits during the
second. In this protocol, our bounds are tight and a multiple-access channel
transmission from the two users to the relay followed by a coded broadcast-type
transmission from the relay to the users achieves all points in the two-phase
capacity region.
  The second protocol considers sequential transmissions from the two users
followed by a transmission from the relay while the third protocol is a hybrid
of the first two protocols and has four phases. In the latter two protocols the
inner and outer bounds are not identical, and differ in a manner similar to the
inner and outer bounds of Cover's relay channel. Numerical evaluation shows
that at least in some cases of interest our bounds do not differ significantly.
  Finally, in the Gaussian case with path loss, we derive achievable rates and
compare the relative merits of each protocol in various regimes. This case is
of interest in cellular systems. Surprisingly, we find that in some cases, the
achievable rate region of the four phase protocol sometimes contains points
that are outside the outer bounds of the other protocols."
"For a multiple antenna system, we compute the asymptotic distribution of
antenna selection gain when the transmitter selects the transmit antenna with
the strongest channel. We use this to asymptotically estimate the underlying
channel capacity distributions, and demonstrate that unlike
multiple-input/multiple-output (MIMO) systems, the channel for antenna
selection systems hardens at a slower rate, and thus a significant multiuser
scheduling gain can exist - O(1/ log m) for channel selection as opposed to
O(1/ sqrt{m}) for MIMO, where m is the number of transmit antennas.
Additionally, even without this scheduling gain, it is demonstrated that
transmit antenna selection systems outperform open loop MIMO systems in low
signal-to-interference-plus-noise ratio (SINR) regimes, particularly for a
small number of receive antennas. This may have some implications on wireless
system design, because most of the users in modern wireless systems have low
SINRs"
"In this paper, the problem of communicating using chemical messages
propagating using Brownian motion, rather than electromagnetic messages
propagating as waves in free space or along a wire, is considered. This problem
is motivated by nanotechnological and biotechnological applications, where the
energy cost of electromagnetic communication might be prohibitive. Models are
given for communication using particles that propagate with Brownian motion,
and achievable capacity results are given. Under conservative assumptions, it
is shown that rates exceeding one bit per particle are achievable."
"We consider the problem of joint source-channel coding for transmitting K
samples of a complex Gaussian source over T = bK uses of a block-fading
multiple input multiple output (MIMO) channel with M transmit and N receive
antennas. We consider the case when we are allowed to code over L blocks. The
channel gain is assumed to be constant over a block and channel gains for
different blocks are assumed to be independent. The performance measure of
interest is the rate of decay of the expected mean squared error with the
signal-to-noise ratio (SNR), called the distortion SNR exponent. We first show
that using a broadcast strategy of Gunduz and Erkip, but with a different power
and rate allocation policy, the optimal distortion SNR exponent can be achieved
for bandwidth efficiencies 0 < b < (|N-M|+1)/min(M,N). This is the first time
the optimal exponent is characterized for 1/min(M,N) < b < (|N-M |+ 1)/ min(M,
N). Also, for b > MNL^2, we show that the broadcast scheme achieves the optimal
exponent of MNL. Special cases of this result have been derived for the L=1
case and for M=N=1 by Gunduz and Erkip. We then propose a digital layered
transmission scheme that uses both time layering and superposition. This
includes many previously known schemes as special cases. The proposed scheme is
at least as good as the currently best known schemes for the entire range of
bandwidth efficiencies, whereas at least for some M, N, and b, it is strictly
better than the currently best known schemes."
"In this article an explicit method (relying on representation theory) to
construct packings in Grassmannian space is presented. Infinite families of
configurations having only one non-trivial set of principal angles are found
using 2-transitive groups. These packings are proved to reach the simplex bound
and are therefore optimal w.r.t. the chordal distance. The construction is
illustrated by an example on the symmetric group. Then some natural extends and
consequences of this situation are given."
"In this paper a scheduling policy is presented which minimizes the average
delay of the users. The scheduling scheme is investigated both by analysis and
simulations carried out in the context of Orthogonal Frequency Division
Multiplexing (OFDM) broadcast channels (BC). First the delay optimality is
obtained for a static scenario providing solutions for specific subproblems,
then the analysis is carried over to the dynamic scheme. Furthermore auxiliary
tools are given for proving throughput optimality. Finally simulations show the
superior performance of the presented scheme."
"We consider approximations of signals by the elements of a frame in a complex
vector space of dimension $N$ and formulate both the noiseless and the noisy
sparse representation problems. The noiseless representation problem is to find
sparse representations of a signal $\mathbf{r}$ given that such representations
exist. In this case, we explicitly construct a frame, referred to as the
Vandermonde frame, for which the noiseless sparse representation problem can be
solved uniquely using $O(N^2)$ operations, as long as the number of non-zero
coefficients in the sparse representation of $\mathbf{r}$ is $\epsilon N$ for
some $0 \le \epsilon \le 0.5$, thus improving on a result of Candes and Tao
\cite{Candes-Tao}. We also show that $\epsilon \le 0.5$ cannot be relaxed
without violating uniqueness.
  The noisy sparse representation problem is to find sparse representations of
a signal $\mathbf{r}$ satisfying a distortion criterion. In this case, we
establish a lower bound on the trade-off between the sparsity of the
representation, the underlying distortion and the redundancy of any given
frame."
"In distributed detection systems with wireless sensor networks, the
communication between sensors and a fusion center is not perfect due to
interference and limited transmitter power at the sensors to combat noise at
the fusion center's receiver. The problem of optimizing detection performance
with such imperfect communication brings a new challenge to distributed
detection. In this paper, sensors are assumed to have independent but
nonidentically distributed observations, and a multi-input/multi-output (MIMO)
channel model is included to account for imperfect communication between the
sensors and the fusion center. The J-divergence between the distributions of
the detection statistic under different hypotheses is used as a performance
criterion in order to provide a tractable analysis. Optimizing the performance
(in terms of the J-divergence) with individual and total transmitter power
constraints on the sensors is studied, and the corresponding power allocation
scheme is provided. It is interesting to see that the proposed power allocation
is a tradeoff between two factors, the communication channel quality and the
local decision quality. For the case with orthogonal channels under certain
conditions, the power allocation can be solved by a weighted water-filling
algorithm. Simulations show that, to achieve the same performance, the proposed
power allocation in certain cases only consumes as little as 25 percent of the
total power used by an equal power allocation scheme."
"$M$-ary signal transmission over AWGN channel with additive $Q$-ary
interference where the sequence of i.i.d. interference symbols is known
causally at the transmitter is considered. Shannon's theorem for channels with
side information at the transmitter is used to formulate the capacity of the
channel. It is shown that by using at most $MQ-Q+1$ out of $M^Q$ input symbols
of the \emph{associated} channel, the capacity is achievable. For the special
case where the Gaussian noise power is zero, a sufficient condition, which is
independent of interference, is given for the capacity to be $\log_2 M$ bits
per channel use. The problem of maximization of the transmission rate under the
constraint that the channel input given any current interference symbol is
uniformly distributed over the channel input alphabet is investigated. For this
setting, the general structure of a communication system with optimal precoding
is proposed. The extension of the proposed precoding scheme to continuous
channel input alphabet is also investigated."
"In this paper, two-dimensional percolation lattices are applied to describe
wireless propagation environment, and stochastic rays are employed to model the
trajectories of radio waves. We first derive the probability that a stochastic
ray undergoes certain number of collisions at a specific spatial location.
Three classes of stochastic rays with different constraint conditions are
considered: stochastic rays of random walks, and generic stochastic rays with
two different anomalous levels. Subsequently, we obtain the closed-form
formulation of mean received power of radio waves under non line-of-sight
conditions for each class of stochastic ray. Specifically, the determination of
model parameters and the effects of lattice structures on the path loss are
investigated. The theoretical results are validated by comparison with
experimental data."
"Overheads incurred by routing protocols diminish the capacity available for
relaying useful data in a mobile wireless ad hoc network. Discovering lower
bounds on the amount of protocol overhead incurred for routing data packets is
important for the development of efficient routing protocols, and for
characterizing the actual (effective) capacity available for network users.
This paper presents an information-theoretic framework for characterizing the
minimum routing overheads of geographic routing in a network with mobile nodes.
specifically, the minimum overhead problem is formulated as a rate-distortion
problem. The formulation may be applied to networks with arbitrary traffic
arrival and location service schemes. Lower bounds are derived for the minimum
overheads incurred for maintaining the location of destination nodes and
consistent neighborhood information in terms of node mobility and packet
arrival process. This leads to a characterization of the deficit caused by the
routing overheads on the overall transport capacity."
"It is shown why the discriminant of a maximal order within a cyclic division
algebra must be minimized in order to get the densest possible matrix lattices
with a prescribed nonvanishing minimum determinant. Using results from class
field theory a lower bound to the minimum discriminant of a maximal order with
a given center and index (= the number of Tx/Rx antennas) is derived. Also
numerous examples of division algebras achieving our bound are given. E.g. we
construct a matrix lattice with QAM coefficients that has 2.5 times as many
codewords as the celebrated Golden code of the same minimum determinant. We
describe a general algorithm due to Ivanyos and Ronyai for finding maximal
orders within a cyclic division algebra and discuss our enhancements to this
algorithm. We also consider general methods for finding cyclic division
algebras of a prescribed index achieving our lower bound."
"We describe a method of constructing a sequence of phase coded waveforms with
perfect autocorrelation in the presence of Doppler shift. The constituent
waveforms are Golay complementary pairs which have perfect autocorrelation at
zero Doppler but are sensitive to nonzero Doppler shifts. We extend this
construction to multiple dimensions, in particular to radar polarimetry, where
the two dimensions are realized by orthogonal polarizations. Here we determine
a sequence of two-by-two Alamouti matrices where the entries involve Golay
pairs and for which the sum of the matrix-valued ambiguity functions vanish at
small Doppler shifts. The Prouhet-Thue-Morse sequence plays a key role in the
construction of Doppler resilient sequences of Golay pairs."
"In a three-node network a half-duplex relay node enables bidirectional
communication between two nodes with a spectral efficient two phase protocol.
In the first phase, two nodes transmit their message to the relay node, which
decodes the messages and broadcast a re-encoded composition in the second
phase. In this work we determine the capacity region of the broadcast phase. In
this scenario each receiving node has perfect information about the message
that is intended for the other node. The resulting set of achievable rates of
the two-phase bidirectional relaying includes the region which can be achieved
by applying XOR on the decoded messages at the relay node. We also prove the
strong converse for the maximum error probability and show that this implies
that the $[\eps_1,\eps_2]$-capacity region defined with respect to the average
error probability is constant for small values of error parameters $\eps_1$,
$\eps_2$."
"Orthogonal Frequency Division Multiplexing (OFDM) is a multi-carrier
modulation scheme that provides efficient bandwidth utilization and robustness
against time dispersive channels. This paper deals with the basic system model
for OFDM based systems and with self-interference, or the corruption of desired
signal by itself in OFDM systems. A simple transceiver based on OFDM modulation
is presented. Important impairments in OFDM systems are mathematically analyzed"
"We propose a novel encoding scheme for algebraic codes such as codes on
algebraic curves, multidimensional cyclic codes, and hyperbolic cascaded
Reed-Solomon codes and present numerical examples. We employ the recurrence
from the Gr\""obner basis of the locator ideal for a set of rational points and
the two-dimensional inverse discrete Fourier transform. We generalize the
functioning of the generator polynomial for Reed-Solomon codes and develop
systematic encoding for various algebraic codes."
"Recently, researchers showed that dirty paper coding (DPC) is the optimal
transmission strategy for multiple-input multiple-output broadcast channels
(MIMO-BC). In this paper, we study how to determine the maximum weighted sum of
DPC rates through solving the maximum weighted sum rate problem of the dual
MIMO multiple access channel (MIMO-MAC) with a sum power constraint. We first
simplify the maximum weighted sum rate problem such that enumerating all
possible decoding orders in the dual MIMO-MAC is unnecessary. We then design an
efficient algorithm based on conjugate gradient projection (CGP) to solve the
maximum weighted sum rate problem. Our proposed CGP method utilizes the
powerful concept of Hessian conjugacy. We also develop a rigorous algorithm to
solve the projection problem. We show that CGP enjoys provable convergence,
nice scalability, and great efficiency for large MIMO-BC systems."
"The problem of lossless fixed-rate streaming coding of discrete memoryless
sources with side information at the decoder is studied. A random time-varying
tree-code is used to sequentially bin strings and a Stack Algorithm with a
variable bias uses the side information to give a delay-universal coding system
for lossless source coding with side information. The scheme is shown to give
exponentially decaying probability of error with delay, with exponent equal to
Gallager's random coding exponent for sources with side information. The mean
of the random variable of computation for the stack decoder is bounded, and
conditions on the bias are given to guarantee a finite $\rho^{th}$ moment for
$0 \leq \rho \leq 1$.
  Further, the problem is also studied in the case where there is a discrete
memoryless channel between encoder and decoder. The same scheme is slightly
modified to give a joint-source channel encoder and Stack Algorithm-based
sequential decoder using side information. Again, by a suitable choice of bias,
the probability of error decays exponentially with delay and the random
variable of computation has a finite mean. Simulation results for several
examples are given."
"Detectability of failures of linear programming (LP) decoding and the
potential for improvement by adding new constraints motivate the use of an
adaptive approach in selecting the constraints for the underlying LP problem.
In this paper, we make a first step in studying this method, and show that it
can significantly reduce the complexity of the problem, which was originally
exponential in the maximum check-node degree. We further show that adaptively
adding new constraints, e.g. by combining parity checks, can provide large
gains in the performance."
"The existing ARQ schemes (including a hybrid ARQ) have a throughput depending
on packet error probability. In this paper we describe a strategy for delay
tolerant applications which provide a constant throughput until the algorithm
robustness criterion is not failed. The algorithm robustness criterion is
applied to find the optimum size of the retransmission block in the assumption
of the small changes of coding rate within the rate compatible codes family."
"A conjugate code pair is defined as a pair of linear codes either of which
contains the dual of the other. A conjugate code pair represents the essential
structure of the corresponding Calderbank-Shor-Steane (CSS) quantum
error-correcting code. It is known that conjugate code pairs are applicable to
quantum cryptography. In this work, a polynomial construction of conjugate code
pairs is presented. The constructed pairs achieve the highest known achievable
rate on additive channels, and are decodable with algorithms of polynomial
complexity."
"A pragmatic approach for the construction of space-time codes over block
fading channels is investigated. The approach consists in using common
convolutional encoders and Viterbi decoders with suitable generators and rates,
thus greatly simplifying the implementation of space-time codes. For the design
of pragmatic space-time codes a methodology is proposed and applied, based on
the extension of the concept of generalized transfer function for convolutional
codes over block fading channels. Our search algorithm produces the
convolutional encoder generators of pragmatic space-time codes for various
number of states, number of antennas and fading rate. Finally it is shown that,
for the investigated cases, the performance of pragmatic space-time codes is
better than that of previously known space-time codes, confirming that they are
a valuable choice in terms of both implementation complexity and performance."
"In this paper, a downlink communication system, in which a Base Station (BS)
equipped with M antennas communicates with N users each equipped with K receive
antennas ($K \leq M$), is considered. It is assumed that the receivers have
perfect Channel State Information (CSI), while the BS only knows the partial
CSI, provided by the receivers via feedback. The minimum amount of feedback
required at the BS, to achieve the maximum sum-rate capacity in the asymptotic
case of $N \to \infty$ and different ranges of SNR is studied. In the fixed and
low SNR regimes, it is demonstrated that to achieve the maximum sum-rate, an
infinite amount of feedback is required. Moreover, in order to reduce the gap
to the optimum sum-rate to zero, in the fixed SNR regime, the minimum amount of
feedback scales as $\theta(\ln \ln \ln N)$, which is achievable by the Random
Beam-Forming scheme proposed in [14]. In the high SNR regime, two cases are
considered; in the case of $K < M$, it is proved that the minimum amount of
feedback bits to reduce the gap between the achievable sum-rate and the maximum
sum-rate to zero grows logaritmically with SNR, which is achievable by the
""Generalized Random Beam-Forming"" scheme, proposed in [18]. In the case of $K =
M$, it is shown that by using the Random Beam-Forming scheme and the total
amount of feedback not growing with SNR, the maximum sum-rate capacity is
achieved."
"The capacity of time-varying channels with periodic feedback at the
transmitter is evaluated. It is assumed that the channel state information is
perfectly known at the receiver and is fed back to the transmitter at the
regular time-intervals. The system capacity is investigated in two cases: i)
finite state Markov channel, and ii) additive white Gaussian noise channel with
time-correlated fading. In the first case, it is shown that the capacity is
achievable by multiplexing multiple codebooks across the channel. In the second
case, the channel capacity and the optimal adaptive coding is obtained. It is
shown that the optimal adaptation can be achieved by a single Gaussian
codebook, while adaptively allocating the total power based on the side
information at the transmitter."
"This paper considers the setup of a parallel MIMO relay network in which $K$
relays, each equipped with $N$ antennas, assist the transmitter and the
receiver, each equipped with $M$ antennas, in the half-duplex mode, under the
assumption that $N\geq{M}$. This setup has been studied in the literature like
in \cite{nabar}, \cite{nabar2}, and \cite{qr}. In this paper, a simple scheme,
the so-called Incremental Cooperative Beamforming, is introduced and shown to
achieve the capacity of the network in the asymptotic case of $K\to{\infty}$
with a gap no more than $O(\frac{1}{\log(K)})$. This result is shown to hold,
as long as the power of the relays scales as $\omega(\frac{\log^9(K)}{K})$.
Finally, the asymptotic SNR behavior is studied and it is proved that the
proposed scheme achieves the full multiplexing gain, regardless of the number
of relays."
"This paper studies on-chip communication with non-ideal heat sinks. A channel
model is proposed where the variance of the additive noise depends on the
weighted sum of the past channel input powers. It is shown that, depending on
the weights, the capacity can be either bounded or unbounded in the input
power. A necessary condition and a sufficient condition for the capacity to be
bounded are presented."
"Sparse Code Division Multiple Access (CDMA), a variation on the standard CDMA
method in which the spreading (signature) matrix contains only a relatively
small number of non-zero elements, is presented and analysed using methods of
statistical physics. The analysis provides results on the performance of
maximum likelihood decoding for sparse spreading codes in the large system
limit. We present results for both cases of regular and irregular spreading
matrices for the binary additive white Gaussian noise channel (BIAWGN) with a
comparison to the canonical (dense) random spreading code."
"Given a multiple-input multiple-output (MIMO) channel, feedback from the
receiver can be used to specify a transmit precoding matrix, which selectively
activates the strongest channel modes. Here we analyze the performance of
Random Vector Quantization (RVQ), in which the precoding matrix is selected
from a random codebook containing independent, isotropically distributed
entries. We assume that channel elements are i.i.d. and known to the receiver,
which relays the optimal (rate-maximizing) precoder codebook index to the
transmitter using B bits. We first derive the large system capacity of
beamforming (rank-one precoding matrix) as a function of B, where large system
refers to the limit as B and the number of transmit and receive antennas all go
to infinity with fixed ratios. With beamforming RVQ is asymptotically optimal,
i.e., no other quantization scheme can achieve a larger asymptotic rate. The
performance of RVQ is also compared with that of a simpler reduced-rank scalar
quantization scheme in which the beamformer is constrained to lie in a random
subspace. We subsequently consider a precoding matrix with arbitrary rank, and
approximate the asymptotic RVQ performance with optimal and linear receivers
(matched filter and Minimum Mean Squared Error (MMSE)). Numerical examples show
that these approximations accurately predict the performance of finite-size
systems of interest. Given a target spectral efficiency, numerical examples
show that the amount of feedback required by the linear MMSE receiver is only
slightly more than that required by the optimal receiver, whereas the matched
filter can require significantly more feedback."
"It has been observed that particular rate-1/2 partially systematic parallel
concatenated convolutional codes (PCCCs) can achieve a lower error floor than
that of their rate-1/3 parent codes. Nevertheless, good puncturing patterns can
only be identified by means of an exhaustive search, whilst convergence towards
low bit error probabilities can be problematic when the systematic output of a
rate-1/2 partially systematic PCCC is heavily punctured. In this paper, we
present and study a family of rate-1/2 partially systematic PCCCs, which we
call pseudo-randomly punctured codes. We evaluate their bit error rate
performance and we show that they always yield a lower error floor than that of
their rate-1/3 parent codes. Furthermore, we compare analytic results to
simulations and we demonstrate that their performance converges towards the
error floor region, owning to the moderate puncturing of their systematic
output. Consequently, we propose pseudo-random puncturing as a means of
improving the bandwidth efficiency of a PCCC and simultaneously lowering its
error floor."
"We investigate cooperative wireless relay networks in which the nodes can
help each other in data transmission. We study different coding strategies in
the single-source single-destination network with many relay nodes. Given the
myriad of ways in which nodes can cooperate, there is a natural routing
problem, i.e., determining an ordered set of nodes to relay the data from the
source to the destination. We find that for a given route, the
decode-and-forward strategy, which is an information theoretic cooperative
coding strategy, achieves rates significantly higher than that achievable by
the usual multi-hop coding strategy, which is a point-to-point non-cooperative
coding strategy. We construct an algorithm to find an optimal route (in terms
of rate maximizing) for the decode-and-forward strategy. Since the algorithm
runs in factorial time in the worst case, we propose a heuristic algorithm that
runs in polynomial time. The heuristic algorithm outputs an optimal route when
the nodes transmit independent codewords. We implement these coding strategies
using practical low density parity check codes to compare the performance of
the strategies on different routes."
"The interference channel with degraded message sets (IC-DMS) refers to a
communication model in which two senders attempt to communicate with their
respective receivers simultaneously through a common medium, and one of the
senders has complete and a priori (non-causal) knowledge about the message
being transmitted by the other. A coding scheme that collectively has
advantages of cooperative coding, collaborative coding, and dirty paper coding,
is developed for such a channel. With resorting to this coding scheme,
achievable rate regions of the IC-DMS in both discrete memoryless and Gaussian
cases are derived, which, in general, include several previously known rate
regions. Numerical examples for the Gaussian case demonstrate that in the
high-interference-gain regime, the derived achievable rate regions offer
considerable improvements over these existing results."
"We present an algorithm for systematic encoding of Hermitian codes. For a
Hermitian code defined over GF(q^2), the proposed algorithm achieves a run time
complexity of O(q^2) and is suitable for VLSI implementation. The encoder
architecture uses as main blocks q varying-rate Reed-Solomon encoders and
achieves a space complexity of O(q^2) in terms of finite field multipliers and
memory elements."
"This paper develops a contention-based opportunistic feedback technique
towards relay selection in a dense wireless network. This technique enables the
forwarding of additional parity information from the selected relay to the
destination. For a given network, the effects of varying key parameters such as
the feedback probability are presented and discussed. A primary advantage of
the proposed technique is that relay selection can be performed in a
distributed way. Simulation results find its performance to closely match that
of centralized schemes that utilize GPS information, unlike the proposed
method. The proposed relay selection method is also found to achieve throughput
gains over a point-to-point transmission strategy."
"It has been shown that a decentralized relay selection protocol based on
opportunistic feedback from the relays yields good throughput performance in
dense wireless networks. This selection strategy supports a hybrid-ARQ
transmission approach where relays forward parity information to the
destination in the event of a decoding error. Such an approach, however,
suffers a loss compared to centralized strategies that select relays with the
best channel gain to the destination. This paper closes the performance gap by
adding another level of channel feedback to the decentralized relay selection
problem. It is demonstrated that only one additional bit of feedback is
necessary for good throughput performance. The performance impact of varying
key parameters such as the number of relays and the channel feedback threshold
is discussed. An accompanying bit error rate analysis demonstrates the
importance of relay selection."
"We assess the practicality of random network coding by illuminating the issue
of overhead and considering it in conjunction with increasingly long packets
sent over the erasure channel. We show that the transmission of increasingly
long packets, consisting of either of an increasing number of symbols per
packet or an increasing symbol alphabet size, results in a data rate
approaching zero over the erasure channel. This result is due to an erasure
probability that increases with packet length. Numerical results for a
particular modulation scheme demonstrate a data rate of approximately zero for
a large, but finite-length packet. Our results suggest a reduction in the
performance gains offered by random network coding."
"We study universal compression of sequences generated by monotonic
distributions. We show that for a monotonic distribution over an alphabet of
size $k$, each probability parameter costs essentially $0.5 \log (n/k^3)$ bits,
where $n$ is the coded sequence length, as long as $k = o(n^{1/3})$. Otherwise,
for $k = O(n)$, the total average sequence redundancy is $O(n^{1/3+\epsilon})$
bits overall. We then show that there exists a sub-class of monotonic
distributions over infinite alphabets for which redundancy of
$O(n^{1/3+\epsilon})$ bits overall is still achievable. This class contains
fast decaying distributions, including many distributions over the integers and
geometric distributions. For some slower decays, including other distributions
over the integers, redundancy of $o(n)$ bits overall is achievable, where a
method to compute specific redundancy rates for such distributions is derived.
The results are specifically true for finite entropy monotonic distributions.
Finally, we study individual sequence redundancy behavior assuming a sequence
is governed by a monotonic distribution. We show that for sequences whose
empirical distributions are monotonic, individual redundancy bounds similar to
those in the average case can be obtained. However, even if the monotonicity in
the empirical distribution is violated, diminishing per symbol individual
sequence redundancies with respect to the monotonic maximum likelihood
description length may still be achievable."
"A multiple antenna downlink channel where limited channel feedback is
available to the transmitter is considered. In a vector downlink channel
(single antenna at each receiver), the transmit antenna array can be used to
transmit separate data streams to multiple receivers only if the transmitter
has very accurate channel knowledge, i.e., if there is high-rate channel
feedback from each receiver. In this work it is shown that channel feedback
requirements can be significantly reduced if each receiver has a small number
of antennas and appropriately combines its antenna outputs. A combining method
that minimizes channel quantization error at each receiver, and thereby
minimizes multi-user interference, is proposed and analyzed. This technique is
shown to outperform traditional techniques such as maximum-ratio combining
because minimization of interference power is more critical than maximization
of signal power in the multiple antenna downlink. Analysis is provided to
quantify the feedback savings, and the technique is seen to work well with user
selection and is also robust to receiver estimation error."
"Low density lattice codes (LDLC) are novel lattice codes that can be decoded
efficiently and approach the capacity of the additive white Gaussian noise
(AWGN) channel. In LDLC a codeword x is generated directly at the n-dimensional
Euclidean space as a linear transformation of a corresponding integer message
vector b, i.e., x = Gb, where H, the inverse of G, is restricted to be sparse.
The fact that H is sparse is utilized to develop a linear-time iterative
decoding scheme which attains, as demonstrated by simulations, good error
performance within ~0.5dB from capacity at block length of n = 100,000 symbols.
The paper also discusses convergence results and implementation considerations."
"Most design approaches for trellis-coded quantization take advantage of the
duality of trellis-coded quantization with trellis-coded modulation, and use
the same empirically-found convolutional codes to label the trellis branches.
This letter presents an alternative approach that instead takes advantage of
maximum-Hamming-distance convolutional codes. The proposed source codes are
shown to be competitive with the best in the literature for the same
computational complexity."
"We consider the problem of estimating the probability of an observed string
drawn i.i.d. from an unknown distribution. The key feature of our study is that
the length of the observed string is assumed to be of the same order as the
size of the underlying alphabet. In this setting, many letters are unseen and
the empirical distribution tends to overestimate the probability of the
observed letters. To overcome this problem, the traditional approach to
probability estimation is to use the classical Good-Turing estimator. We
introduce a natural scaling model and use it to show that the Good-Turing
sequence probability estimator is not consistent. We then introduce a novel
sequence probability estimator that is indeed consistent under the natural
scaling model."
"This paper presents new low-complexity lattice-decoding algorithms for
noncoherent block detection of QAM and PAM signals over complex-valued fading
channels. The algorithms are optimal in terms of the generalized likelihood
ratio test (GLRT). The computational complexity is polynomial in the block
length; making GLRT-optimal noncoherent detection feasible for implementation.
We also provide even lower complexity suboptimal algorithms. Simulations show
that the suboptimal algorithms have performance indistinguishable from the
optimal algorithms. Finally, we consider block based transmission, and propose
to use noncoherent detection as an alternative to pilot assisted transmission
(PAT). The new technique is shown to outperform PAT."
"While most useful information theoretic inequalities can be deduced from the
basic properties of entropy or mutual information, up to now Shannon's entropy
power inequality (EPI) is an exception: Existing information theoretic proofs
of the EPI hinge on representations of differential entropy using either Fisher
information or minimum mean-square error (MMSE), which are derived from de
Bruijn's identity. In this paper, we first present an unified view of these
proofs, showing that they share two essential ingredients: 1) a data processing
argument applied to a covariance-preserving linear transformation; 2) an
integration over a path of a continuous Gaussian perturbation. Using these
ingredients, we develop a new and brief proof of the EPI through a mutual
information inequality, which replaces Stam and Blachman's Fisher information
inequality (FII) and an inequality for MMSE by Guo, Shamai and Verd\'u used in
earlier proofs. The result has the advantage of being very simple in that it
relies only on the basic properties of mutual information. These ideas are then
generalized to various extended versions of the EPI: Zamir and Feder's
generalized EPI for linear transformations of the random variables, Takano and
Johnson's EPI for dependent variables, Liu and Viswanath's
covariance-constrained EPI, and Costa's concavity inequality for the entropy
power."
"We describe and analyze the joint source/channel coding properties of a class
of sparse graphical codes based on compounding a low-density generator matrix
(LDGM) code with a low-density parity check (LDPC) code. Our first pair of
theorems establish that there exist codes from this ensemble, with all degrees
remaining bounded independently of block length, that are simultaneously
optimal as both source and channel codes when encoding and decoding are
performed optimally. More precisely, in the context of lossy compression, we
prove that finite degree constructions can achieve any pair $(R, D)$ on the
rate-distortion curve of the binary symmetric source. In the context of channel
coding, we prove that finite degree codes can achieve any pair $(C, p)$ on the
capacity-noise curve of the binary symmetric channel. Next, we show that our
compound construction has a nested structure that can be exploited to achieve
the Wyner-Ziv bound for source coding with side information (SCSI), as well as
the Gelfand-Pinsker bound for channel coding with side information (CCSI).
Although the current results are based on optimal encoding and decoding, the
proposed graphical codes have sparse structure and high girth that renders them
well-suited to message-passing and other efficient decoding procedures."
"In this paper, we propose an achievable rate region for discrete memoryless
interference channels with conferencing at the transmitter side. We employ
superposition block Markov encoding, combined with simultaneous superposition
coding, dirty paper coding, and random binning to obtain the achievable rate
region. We show that, under respective conditions, the proposed achievable
region reduces to Han and Kobayashi achievable region for interference
channels, the capacity region for degraded relay channels, and the capacity
region for the Gaussian vector broadcast channel. Numerical examples for the
Gaussian case are given."
"In spatially distributed multiuser antenna systems, the received signal
contains multiple carrier-frequency offsets (CFOs) arising from mismatch
between the oscillators of transmitters and receivers. This results in a
time-varying rotation of the data constellation, which needs to be compensated
at the receiver before symbol recovery. In this paper, a new approach for blind
CFO estimation and symbol recovery is proposed. The received base-band signal
is over-sampled, and its polyphase components are used to formulate a virtual
Multiple-Input Multiple-Output (MIMO) problem. By applying blind MIMO system
estimation techniques, the system response can be estimated and decoupled
versions of the user symbols can be recovered, each one of which contains a
distinct CFO. By applying a decision feedback Phase Lock Loop (PLL), the CFO
can be mitigated and the transmitted symbols can be recovered. The estimated
MIMO system response provides information about the CFOs that can be used to
initialize the PLL, speed up its convergence, and avoid ambiguities usually
linked with PLL."
"We prove that approximating the size of stopping and trapping sets in Tanner
graphs of linear block codes, and more restrictively, the class of low-density
parity-check (LDPC) codes, is NP-hard. The ramifications of our findings are
that methods used for estimating the height of the error-floor of moderate- and
long-length LDPC codes based on stopping and trapping set enumeration cannot
provide accurate worst-case performance predictions."
"We consider a cognitive network consisting of n random pairs of cognitive
transmitters and receivers communicating simultaneously in the presence of
multiple primary users. Of interest is how the maximum throughput achieved by
the cognitive users scales with n. Furthermore, how far these users must be
from a primary user to guarantee a given primary outage. Two scenarios are
considered for the network scaling law: (i) when each cognitive transmitter
uses constant power to communicate with a cognitive receiver at a bounded
distance away, and (ii) when each cognitive transmitter scales its power
according to the distance to a considered primary user, allowing the cognitive
transmitter-receiver distances to grow. Using single-hop transmission, suitable
for cognitive devices of opportunistic nature, we show that, in both scenarios,
with path loss larger than 2, the cognitive network throughput scales linearly
with the number of cognitive users. We then explore the radius of a primary
exclusive region void of cognitive transmitters. We obtain bounds on this
radius for a given primary outage constraint. These bounds can help in the
design of a primary network with exclusive regions, outside of which cognitive
users may transmit freely. Our results show that opportunistic secondary
spectrum access using single-hop transmission is promising."
"Power control is a fundamental task accomplished in any wireless cellular
network; its aim is to set the transmit power of any mobile terminal, so that
each user is able to achieve its own target SINR. While conventional power
control algorithms require knowledge of a number of parameters of the signal of
interest and of the multiaccess interference, in this paper it is shown that in
a large CDMA system much of this information can be dispensed with, and
effective distributed power control algorithms may be implemented with very
little information on the user of interest. An uplink CDMA system subject to
flat fading is considered with a focus on the cases in which a linear MMSE
receiver and a non-linear MMSE serial interference cancellation receiver are
adopted; for the latter case new formulas are also given for the system SINR in
the large system asymptote. Experimental results show an excellent agreement
between the performance and the power profile of the proposed distributed
algorithms and that of conventional ones that require much greater prior
knowledge."
"This paper is focused on the cross-layer design problem of joint multiuser
detection and power control for energy-efficiency optimization in a wireless
data network through a game-theoretic approach. Building on work of Meshkati,
et al., wherein the tools of game-theory are used in order to achieve
energy-efficiency in a simple synchronous code division multiple access system,
system asynchronism, the use of bandlimited chip-pulses, and the multipath
distortion induced by the wireless channel are explicitly incorporated into the
analysis. Several non-cooperative games are proposed wherein users may vary
their transmit power and their uplink receiver in order to maximize their
utility, which is defined here as the ratio of data throughput to transmit
power. In particular, the case in which a linear multiuser detector is adopted
at the receiver is considered first, and then, the more challenging case in
which non-linear decision feedback multiuser detectors are employed is
considered. The proposed games are shown to admit a unique Nash equilibrium
point, while simulation results show the effectiveness of the proposed
solutions, as well as that the use of a decision-feedback multiuser receiver
brings remarkable performance improvements."
"On a fading channel with no channel state information at the receiver,
calculating true log-likelihood ratios (LLR) is complicated. Existing work
assume that the power of the additive noise is known and use the expected value
of the fading gain in a linear function of the channel output to find
approximate LLRs. In this work, we first assume that the power of the additive
noise is known and we find the optimum linear approximation of LLRs in the
sense of maximum achievable transmission rate on the channel. The maximum
achievable rate under this linear LLR calculation is almost equal to the
maximum achievable rate under true LLR calculation. We also observe that this
method appears to be the optimum in the sense of bit error rate performance
too. These results are then extended to the case that the noise power is
unknown at the receiver and a performance almost identical to the case that the
noise power is perfectly known is obtained."
"A main distinguishing feature of a wireless network compared with a wired
network is its broadcast nature, in which the signal transmitted by a node may
reach several other nodes, and a node may receive signals from several other
nodes simultaneously. Rather than a blessing, this feature is treated more as
an interference-inducing nuisance in most wireless networks today (e.g., IEEE
802.11). This paper shows that the concept of network coding can be applied at
the physical layer to turn the broadcast property into a capacity-boosting
advantage in wireless ad hoc networks. Specifically, we propose a
physical-layer network coding (PNC) scheme to coordinate transmissions among
nodes. In contrast to straightforward network coding which performs coding
arithmetic on digital bit streams after they have been received, PNC makes use
of the additive nature of simultaneously arriving electromagnetic (EM) waves
for equivalent coding operation. And in doing so, PNC can potentially achieve
100% and 50% throughput increases compared with traditional transmission and
straightforward network coding, respectively, in multi-hop networks. More
specifically, the information-theoretic capacity of PNC is almost double that
of traditional transmission in the SNR region of practical interest (higher
than 0dB). We believe this is a first paper that ventures into EM-wave-based
network coding at the physical layer and demonstrates its potential for
boosting network capacity."
"The problem of designing high rate, full diversity noncoherent space-time
block codes (STBCs) with low encoding and decoding complexity is addressed.
First, the notion of $g$-group encodable and $g$-group decodable linear STBCs
is introduced. Then for a known class of rate-1 linear designs, an explicit
construction of fully-diverse signal sets that lead to four-group encodable and
four-group decodable differential scaled unitary STBCs for any power of two
number of antennas is provided. Previous works on differential STBCs either
sacrifice decoding complexity for higher rate or sacrifice rate for lower
decoding complexity."
"The differential encoding/decoding setup introduced by Kiran et al, Oggier et
al and Jing et al for wireless relay networks that use codebooks consisting of
unitary matrices is extended to allow codebooks consisting of scaled unitary
matrices. For such codebooks to be used in the Jing-Hassibi protocol for
cooperative diversity, the conditions that need to be satisfied by the relay
matrices and the codebook are identified. A class of previously known rate one,
full diversity, four-group encodable and four-group decodable Differential
Space-Time Codes (DSTCs) is proposed for use as Distributed DSTCs (DDSTCs) in
the proposed set up. To the best of our knowledge, this is the first known low
decoding complexity DDSTC scheme for cooperative wireless networks."
"The Extended BP (EBP) Generalized EXIT (GEXIT) function introduced in
\cite{MMRU05} plays a fundamental role in the asymptotic analysis of sparse
graph codes. For transmission over the binary erasure channel (BEC) the
analytic properties of the EBP GEXIT function are relatively simple and well
understood. The general case is much harder and even the existence of the curve
is not known in general. We introduce some tools from non-linear analysis which
can be useful to prove the existence of EXIT like curves in some cases. The
main tool is the Krasnoselskii-Rabinowitz (KR) bifurcation theorem."
"The problem of resource allocation is studied for a two-user fading
orthogonal multiaccess relay channel (MARC) where both users (sources)
communicate with a destination in the presence of a relay. A half-duplex relay
is considered that transmits on a channel orthogonal to that used by the
sources. The instantaneous fading state between every transmit-receive pair in
this network is assumed to be known at both the transmitter and receiver. Under
an average power constraint at each source and the relay, the sum-rate for the
achievable strategy of decode-and-forward (DF) is maximized over all power
allocations (policies) at the sources and relay. It is shown that the sum-rate
maximizing policy exploits the multiuser fading diversity to reveal the
optimality of opportunistic channel use by each user. A geometric
interpretation of the optimal power policy is also presented."
"A transmitter without channel state information (CSI) wishes to send a
delay-limited Gaussian source over a slowly fading channel. The source is coded
in superimposed layers, with each layer successively refining the description
in the previous one. The receiver decodes the layers that are supported by the
channel realization and reconstructs the source up to a distortion. In the
limit of a continuum of infinite layers, the optimal power distribution that
minimizes the expected distortion is given by the solution to a set of linear
differential equations in terms of the density of the fading distribution. In
the optimal power distribution, as SNR increases, the allocation over the
higher layers remains unchanged; rather the extra power is allocated towards
the lower layers. On the other hand, as the bandwidth ratio b (channel uses per
source symbol) tends to zero, the power distribution that minimizes expected
distortion converges to the power distribution that maximizes expected
capacity. While expected distortion can be improved by acquiring CSI at the
transmitter (CSIT) or by increasing diversity from the realization of
independent fading paths, at high SNR the performance benefit from diversity
exceeds that from CSIT, especially when b is large."
"Motivated by on-chip communication, a channel model is proposed where the
variance of the additive noise depends on the weighted sum of the past channel
input powers. For this channel, an expression for the capacity per unit cost is
derived, and it is shown that the expression holds also in the presence of
feedback."
"A wireless network in which packets are broadcast to a group of receivers
through use of a random access protocol is considered in this work. The
relation to previous work on networks of interacting queues is discussed and
subsequently, the stability and throughput regions of the system are analyzed
and presented. A simple network of two source nodes and two destination nodes
is considered first. The broadcast service process is analyzed assuming a
channel that allows for packet capture and multipacket reception. In this small
network, the stability and throughput regions are observed to coincide. The
same problem for a network with N sources and M destinations is considered
next. The channel model is simplified in that multipacket reception is no
longer permitted. Bounds on the stability region are developed using the
concept of stability rank and the throughput region of the system is compared
to the bounds. Our results show that as the number of destination nodes
increases, the stability and throughput regions diminish. Additionally, a
previous conjecture that the stability and throughput regions coincide for a
network of arbitrarily many sources is supported for a broadcast scenario by
the results presented in this work."
"This paper studies a variant of the classical problem of ``writing on dirty
paper'' in which the sum of the input and the interference, or dirt, is
multiplied by a random variable that models resizing, known to the decoder but
not to the encoder. The achievable rate of Costa's dirty paper coding (DPC)
scheme is calculated and compared to the case of the decoder's also knowing the
dirt. In the ergodic case, the corresponding rate loss vanishes asymptotically
in the limits of both high and low signal-to-noise ratio (SNR), and is small at
all finite SNR for typical distributions like Rayleigh, Rician, and Nakagami.
In the quasi-static case, the DPC scheme is lossless at all SNR in terms of
outage probability. Quasi-static fading broadcast channels (BC) without
transmit channel state information (CSI) are investigated as an application of
the robustness properties. It is shown that the DPC scheme leads to an outage
achievable rate region that strictly dominates that of time division."
"In wireless ad hoc networks, distributed nodes can collaboratively form an
antenna array for long-distance communications to achieve high energy
efficiency. In recent work, Ochiai, et al., have shown that such collaborative
beamforming can achieve a statistically nice beampattern with a narrow main
lobe and low sidelobes. However, the process of collaboration introduces
significant delay, since all collaborating nodes need access to the same
information. In this paper, a technique that significantly reduces the
collaboration overhead is proposed. It consists of two phases. In the first
phase, nodes transmit locally in a random access fashion. Collisions, when they
occur, are viewed as linear mixtures of the collided packets. In the second
phase, a set of cooperating nodes acts as a distributed antenna system and
beamform the received analog waveform to one or more faraway destinations. This
step requires multiplication of the received analog waveform by a complex
number, which is independently computed by each cooperating node, and which
enables separation of the collided packets based on their final destination.
The scheme requires that each node has global knowledge of the network
coordinates. The proposed scheme can achieve high throughput, which in certain
cases exceeds one."
"In this paper, we study a routing problem on the Gaussian multiple relay
channel, in which nodes employ a decode-and-forward coding strategy. We are
interested in routes for the information flow through the relays that achieve
the highest DF rate. We first construct an algorithm that provably finds
optimal DF routes. As the algorithm runs in factorial time in the worst case,
we propose a polynomial time heuristic algorithm that finds an optimal route
with high probability. We demonstrate that that the optimal (and near optimal)
DF routes are good in practice by simulating a distributed DF coding scheme
using low density parity check codes with puncturing and incremental
redundancy."
"In this paper non-group permutation modulated sequences for the Gaussian
channel are considered. Without the restriction to group codes rather than
subsets of group codes, arbitrary rates are achievable. The code construction
utilizes the known optimal group constellations to ensure at least the same
performance but exploit the Gray code ordering structure of multiset
permutations as a selection criterion at the decoder. The decoder achieves near
maximum likelihood performance at low computational cost and low additional
memory requirements at the receiver."
"This work examines the problem of sequential detection of a change in the
drift of a Brownian motion in the case of two-sided alternatives. Applications
to real life situations in which two-sided changes can occur are discussed.
Traditionally, 2-CUSUM stopping rules have been used for this problem due to
their asymptotically optimal character as the mean time between false alarms
tends to $\infty$. In particular, attention has focused on 2-CUSUM harmonic
mean rules due to the simplicity in calculating their first moments. In this
paper, we derive closed-form expressions for the first moment of a general
2-CUSUM stopping rule. We use these expressions to obtain explicit upper and
lower bounds for it. Moreover, we derive an expression for the rate of change
of this first moment as one of the threshold parameters changes. Based on these
expressions we obtain explicit upper and lower bounds to this rate of change.
Using these expressions we are able to find the best 2-CUSUM stopping rule with
respect to the extended Lorden criterion. In fact, we demonstrate not only the
existence but also the uniqueness of the best 2-CUSUM stopping both in the case
of a symmetric change and in the case of a non-symmetric case. Furthermore, we
discuss the existence of a modification of the 2-CUSUM stopping rule that has a
strictly better performance than its classical 2-CUSUM counterpart for small
values of the mean time between false alarms. We conclude with a discussion on
the open problem of strict optimality in the case of two-sided alternatives."
"A new class of space time codes with high performance is presented. The code
design utilizes tailor-made permutation codes, which are known to have large
minimal distances as spherical codes. A geometric connection between spherical
and space time codes has been used to translate them into the final space time
codes. Simulations demonstrate that the performance increases with the block
lengths, a result that has been conjectured already in previous work. Further,
the connection to permutation codes allows for moderate complex en-/decoding
algorithms."
"In this paper, the stability condition for low-density parity-check (LDPC)
codes on the binary erasure channel (BEC) is extended to generalized LDPC
(GLDPC) codes and doublygeneralized LDPC (D-GLDPC) codes. It is proved that, in
both cases, the stability condition only involves the component codes with
minimum distance 2. The stability condition for GLDPC codes is always expressed
as an upper bound to the decoding threshold. This is not possible for D-GLDPC
codes, unless all the generalized variable nodes have minimum distance at least
3. Furthermore, a condition called derivative matching is defined in the paper.
This condition is sufficient for a GLDPC or DGLDPC code to achieve the
stability condition with equality. If this condition is satisfied, the
threshold of D-GLDPC codes (whose generalized variable nodes have all minimum
distance at least 3) and GLDPC codes can be expressed in closed form."
"We examine the problem of determining which nodes are neighbors of a given
one in a wireless network. We consider an unsupervised network operating on a
frequency-flat Gaussian channel, where $K+1$ nodes associate their identities
to nonorthogonal signatures, transmitted at random times, synchronously, and
independently. A number of neighbor-discovery algorithms, based on different
optimization criteria, are introduced and analyzed. Numerical results show how
reduced-complexity algorithms can achieve a satisfactory performance."
"The detection and estimation of signals in noisy, limited data is a problem
of interest to many scientific and engineering communities. We present a
computationally simple, sample eigenvalue based procedure for estimating the
number of high-dimensional signals in white noise when there are relatively few
samples. We highlight a fundamental asymptotic limit of sample eigenvalue based
detection of weak high-dimensional signals from a limited sample size and
discuss its implication for the detection of two closely spaced signals.
  This motivates our heuristic definition of the 'effective number of
identifiable signals.' Numerical simulations are used to demonstrate the
consistency of the algorithm with respect to the effective number of signals
and the superior performance of the algorithm with respect to Wax and Kailath's
""asymptotically consistent"" MDL based estimator."
"In wireless packet-forwarding networks with selfish nodes, applications of a
repeated game can induce the nodes to forward each others' packets, so that the
network performance can be improved. However, the nodes on the boundary of such
networks cannot benefit from this strategy, as the other nodes do not depend on
them. This problem is sometimes known as the curse of the boundary nodes. To
overcome this problem, an approach based on coalition games is proposed, in
which the boundary nodes can use cooperative transmission to help the backbone
nodes in the middle of the network. In return, the backbone nodes are willing
to forward the boundary nodes' packets. The stability of the coalitions is
studied using the concept of a core. Then two types of fairness, namely, the
min-max fairness using nucleolus and the average fairness using the Shapley
function are investigated. Finally, a protocol is designed using both repeated
games and coalition games. Simulation results show how boundary nodes and
backbone nodes form coalitions together according to different fairness
criteria. The proposed protocol can improve the network connectivity by about
50%, compared with pure repeated game schemes."
"Collaborative beamforming (CB) and cooperative transmission (CT) have
recently emerged as communication techniques that can make effective use of
collaborative/cooperative nodes to create a virtual
multiple-input/multiple-output (MIMO) system. Extending the lifetime of
networks composed of battery-operated nodes is a key issue in the design and
operation of wireless sensor networks. This paper considers the effects on
network lifetime of allowing closely located nodes to use CB/CT to reduce the
load or even to avoid packet-forwarding requests to nodes that have critical
battery life. First, the effectiveness of CB/CT in improving the signal
strength at a faraway destination using energy in nearby nodes is studied.
Then, the performance improvement obtained by this technique is analyzed for a
special 2D disk case. Further, for general networks in which
information-generation rates are fixed, a new routing problem is formulated as
a linear programming problem, while for other general networks, the cost for
routing is dynamically adjusted according to the amount of energy remaining and
the effectiveness of CB/CT. From the analysis and the simulation results, it is
seen that the proposed method can reduce the payloads of energy-depleting nodes
by about 90% in the special case network considered and improve the lifetimes
of general networks by about 10%, compared with existing techniques."
"Extending network lifetime of battery-operated devices is a key design issue
that allows uninterrupted information exchange among distributive nodes in
wireless sensor networks. Collaborative beamforming (CB) and cooperative
transmission (CT) have recently emerged as new communication techniques that
enable and leverage effective resource sharing among collaborative/cooperative
nodes. In this paper, we seek to maximize the lifetime of sensor networks by
using the new idea that closely located nodes can use CB/CT to reduce the load
or even avoid packet forwarding requests to nodes that have critical battery
life. First, we study the effectiveness of CB/CT to improve the signal strength
at a faraway destination using energy in nearby nodes. Then, a 2D disk case is
analyzed to assess the resulting performance improvement. For general networks,
if information-generation rates are fixed, the new routing problem is
formulated as a linear programming problem; otherwise, the cost for routing is
dynamically adjusted according to the amount of energy remaining and the
effectiveness of CB/CT. From the analysis and simulation results, it is seen
that the proposed schemes can improve the lifetime by about 90% in the 2D disk
network and by about 10% in the general networks, compared to existing schemes."
"Cooperative transmission is an emerging communication technique that takes
advantages of the broadcast nature of wireless channels. However, due to low
spectral efficiency and the requirement of orthogonal channels, its potential
for use in future wireless networks is limited. In this paper, by making use of
multiuser detection (MUD) and network coding, cooperative transmission
protocols with high spectral efficiency, diversity order, and coding gain are
developed. Compared with the traditional cooperative transmission protocols
with single-user detection, in which the diversity gain is only for one source
user, the proposed MUD cooperative transmission protocols have the merits that
the improvement of one user's link can also benefit the other users. In
addition, using MUD at the relay provides an environment in which network
coding can be employed. The coding gain and high diversity order can be
obtained by fully utilizing the link between the relay and the destination.
  From the analysis and simulation results, it is seen that the proposed
protocols achieve higher diversity gain, better asymptotic efficiency, and
lower bit error rate, compared to traditional MUD and to existing cooperative
transmission protocols."
"We establish the optimal diversity-multiplexing (DM) tradeoff of coherent
time, frequency and time-frequency selective-fading MIMO channels and provide a
code design criterion for DM-tradeoff optimality. Our results are based on the
analysis of the ""Jensen channel"" associated to a given selective-fading MIMO
channel. While the original problem seems analytically intractable due to the
mutual information being a sum of correlated random variables, the Jensen
channel is equivalent to the original channel in the sense of the DM-tradeoff
and lends itself nicely to analytical treatment. Finally, as a consequence of
our results, we find that the classical rank criterion for space-time code
design (in selective-fading MIMO channels) ensures optimality in the sense of
the DM-tradeoff."
"Distributed estimation based on measurements from multiple wireless sensors
is investigated. It is assumed that a group of sensors observe the same
quantity in independent additive observation noises with possibly different
variances. The observations are transmitted using amplify-and-forward (analog)
transmissions over non-ideal fading wireless channels from the sensors to a
fusion center, where they are combined to generate an estimate of the observed
quantity. Assuming that the Best Linear Unbiased Estimator (BLUE) is used by
the fusion center, the equal-power transmission strategy is first discussed,
where the system performance is analyzed by introducing the concept of
estimation outage and estimation diversity, and it is shown that there is an
achievable diversity gain on the order of the number of sensors. The optimal
power allocation strategies are then considered for two cases: minimum
distortion under power constraints; and minimum power under distortion
constraints. In the first case, it is shown that by turning off bad sensors,
i.e., sensors with bad channels and bad observation quality, adaptive power
gain can be achieved without sacrificing diversity gain. Here, the adaptive
power gain is similar to the array gain achieved in Multiple-Input
Single-Output (MISO) multi-antenna systems when channel conditions are known to
the transmitter. In the second case, the sum power is minimized under
zero-outage estimation distortion constraint, and some related energy
efficiency issues in sensor networks are discussed."
"In time hopping impulse radio, $N_f$ pulses of duration $T_c$ are transmitted
for each information symbol. This gives rise to two types of processing gain:
(i) pulse combining gain, which is a factor $N_f$, and (ii) pulse spreading
gain, which is $N_c=T_f/T_c$, where $T_f$ is the mean interval between two
subsequent pulses. This paper investigates the trade-off between these two
types of processing gain in the presence of timing jitter. First, an additive
white Gaussian noise (AWGN) channel is considered and approximate closed form
expressions for bit error probability are derived for impulse radio systems
with and without pulse-based polarity randomization. Both symbol-synchronous
and chip-synchronous scenarios are considered. The effects of multiple-access
interference and timing jitter on the selection of optimal system parameters
are explained through theoretical analysis. Finally, a multipath scenario is
considered and the trade-off between processing gains of a synchronous impulse
radio system with pulse-based polarity randomization is analyzed. The effects
of the timing jitter, multiple-access interference and inter-frame interference
are investigated. Simulation studies support the theoretical results."
"In this paper we address the problem of finding the sensing capacity of
sensor networks for a class of linear observation models and a fixed SNR
regime. Sensing capacity is defined as the maximum number of signal dimensions
reliably identified per sensor observation. In this context sparsity of the
phenomena is a key feature that determines sensing capacity. Precluding the SNR
of the environment the effect of sparsity on the number of measurements
required for accurate reconstruction of a sparse phenomena has been widely
dealt with under compressed sensing. Nevertheless the development there was
motivated from an algorithmic perspective. In this paper our aim is to derive
these bounds in an information theoretic set-up and thus provide algorithm
independent conditions for reliable reconstruction of sparse signals. In this
direction we first generalize the Fano's inequality and provide lower bounds to
the probability of error in reconstruction subject to an arbitrary distortion
criteria. Using these lower bounds to the probability of error, we derive upper
bounds to sensing capacity and show that for fixed SNR regime sensing capacity
goes down to zero as sparsity goes down to zero. This means that
disproportionately more sensors are required to monitor very sparse events. Our
next main contribution is that we show the effect of sensing diversity on
sensing capacity, an effect that has not been considered before. Sensing
diversity is related to the effective \emph{coverage} of a sensor with respect
to the field. In this direction we show the following results (a) Sensing
capacity goes down as sensing diversity per sensor goes down; (b) Random
sampling (coverage) of the field by sensors is better than contiguous location
sampling (coverage)."
"We introduce the concept of $\delta$-sequence. A $\delta$-sequence $\Delta$
generates a well-ordered semigroup $S$ in $\mathbb{Z}^2$ or $\mathbb{R}$. We
show how to construct (and compute parameters) for the dual code of any
evaluation code associated with a weight function defined by $\Delta$ from the
polynomial ring in two indeterminates to a semigroup $S$ as above. We prove
that this is a simple procedure which can be understood by considering a
particular class of valuations of function fields of surfaces, called plane
valuations at infinity. We also give algorithms to construct an unlimited
number of $\delta$-sequences of the different existing types, and so this paper
provides the tools to know and use a new large set of codes."
"In this paper, a hierarchical cross-layer design approach is proposed to
increase energy efficiency in ad hoc networks through joint adaptation of
nodes' transmitting powers and route selection. The design maintains the
advantages of the classic OSI model, while accounting for the cross-coupling
between layers, through information sharing. The proposed joint power control
and routing algorithm is shown to increase significantly the overall energy
efficiency of the network, at the expense of a moderate increase in complexity.
Performance enhancement of the joint design using multiuser detection is also
investigated, and it is shown that the use of multiuser detection can increase
the capacity of the ad hoc network significantly for a given level of energy
consumption."
"This paper characterizes the capacity of a class of modulo additive noise
relay channels, in which the relay observes a corrupted version of the noise
and has a separate channel to the destination. The capacity is shown to be
strictly below the cut-set bound in general and achievable using a
quantize-and-forward strategy at the relay. This result confirms a conjecture
by Ahlswede and Han about the capacity of channels with rate limited state
information at the destination for this particular class of channels."
"Capacity improvement from transmitter and receiver cooperation is
investigated in a two-transmitter, two-receiver network with phase fading and
full channel state information available at all terminals. The transmitters
cooperate by first exchanging messages over an orthogonal transmitter
cooperation channel, then encoding jointly with dirty paper coding. The
receivers cooperate by using Wyner-Ziv compress-and-forward over an analogous
orthogonal receiver cooperation channel. To account for the cost of
cooperation, the allocation of network power and bandwidth among the data and
cooperation channels is studied. It is shown that transmitter cooperation
outperforms receiver cooperation and improves capacity over non-cooperative
transmission under most operating conditions when the cooperation channel is
strong. However, a weak cooperation channel limits the transmitter cooperation
rate; in this case receiver cooperation is more advantageous.
Transmitter-and-receiver cooperation offers sizable additional capacity gain
over transmitter-only cooperation at low SNR, whereas at high SNR transmitter
cooperation alone captures most of the cooperative capacity improvement."
"A game-theoretic model is proposed to study the cross-layer problem of joint
power and rate control with quality of service (QoS) constraints in
multiple-access networks. In the proposed game, each user seeks to choose its
transmit power and rate in a distributed manner in order to maximize its own
utility while satisfying its QoS requirements. The user's QoS constraints are
specified in terms of the average source rate and an upper bound on the average
delay where the delay includes both transmission and queuing delays. The
utility function considered here measures energy efficiency and is particularly
suitable for wireless networks with energy constraints. The Nash equilibrium
solution for the proposed non-cooperative game is derived and a closed-form
expression for the utility achieved at equilibrium is obtained. It is shown
that the QoS requirements of a user translate into a ""size"" for the user which
is an indication of the amount of network resources consumed by the user. Using
this competitive multiuser framework, the tradeoffs among throughput, delay,
network capacity and energy efficiency are studied. In addition, analytical
expressions are given for users' delay profiles and the delay performance of
the users at Nash equilibrium is quantified."
"A unified approach to energy-efficient power control is proposed for
code-division multiple access (CDMA) networks. The approach is applicable to a
large family of multiuser receivers including the matched filter, the
decorrelator, the linear minimum mean-square error (MMSE) receiver, and the
(nonlinear) optimal detectors. It exploits the linear relationship that has
been shown to exist between the transmit power and the output
signal-to-interference-plus-noise ratio (SIR) in the large-system limit. It is
shown that, for this family of receivers, when users seek to selfishly maximize
their own energy efficiency, the Nash equilibrium is SIR-balanced. In addition,
a unified power control (UPC) algorithm for reaching the Nash equilibrium is
proposed. The algorithm adjusts the user's transmit powers by iteratively
computing the large-system multiuser efficiency, which is independent of
instantaneous spreading sequences. The convergence of the algorithm is proved
for the matched filter, the decorrelator, and the MMSE receiver, and is
demonstrated by means of simulation for an optimal detector. Moreover, the
performance of the algorithm in finite-size systems is studied and compared
with that of a conventional power control scheme, in which user powers depend
on the instantaneous spreading sequences."
"In this two-part paper, we consider the multiantenna multihop relay channels
in which the source signal arrives at the destination through N independent
relaying hops in series. The main concern of this work is to design relaying
strategies that utilize efficiently the relays in such a way that the diversity
is maximized. In part I, we focus on the amplify-and-forward (AF) strategy with
which the relays simply scale the received signal and retransmit it. More
specifically, we characterize the diversity-multiplexing tradeoff (DMT) of the
AF scheme in a general multihop channel with arbitrary number of antennas and
arbitrary number of hops. The DMT is in closed-form expression as a function of
the number of antennas at each node. First, we provide some basic results on
the DMT of the general Rayleigh product channels. It turns out that these
results have very simple and intuitive interpretation. Then, the results are
applied to the AF multihop channels which is shown to be equivalent to the
Rayleigh product channel, in the DMT sense. Finally, the project-and-forward
(PF) scheme, a variant of the AF scheme, is proposed. We show that the PF
scheme has the same DMT as the AF scheme, while the PF can have significant
power gain over the AF scheme in some cases. In part II, we will derive the
upper bound on the diversity of the multihop channels and show that it can be
achieved by partitioning the multihop channel into AF subchannels."
"This paper examines the joint problem of detection and identification of a
sudden and unobservable change in the probability distribution function (pdf)
of a sequence of independent and identically distributed (i.i.d.) random
variables to one of finitely many alternative pdf's. The objective is quick
detection of the change and accurate inference of the ensuing pdf. Following a
Bayesian approach, a new sequential decision strategy for this problem is
revealed and is proven optimal. Geometrical properties of this strategy are
demonstrated via numerical examples."
"In this paper, memories built from components subject to transient faults are
considered. A fault-tolerant memory architecture based on low-density
parity-check codes is proposed and the existence of reliable memories for the
adversarial failure model is proved. The proof relies on the expansion property
of the underlying Tanner graph of the code. An equivalence between the
Taylor-Kuznetsov (TK) scheme and Gallager B algorithm is established and the
results are extended to the independent failure model. It is also shown that
the proposed memory architecture has lower redundancy compared to the TK
scheme. The results are illustrated with specific numerical examples."
"This paper introduces a new combinatorial construction for q-ary
constant-weight codes which yields several families of optimal codes and
asymptotically optimal codes. The construction reveals intimate connection
between q-ary constant-weight codes and sets of pairwise disjoint combinatorial
designs of various types."
"We give an algorithm for finding network encoding and decoding equations for
error-free multicasting networks with multiple sources and sinks. The algorithm
given is efficient (polynomial complexity) and works on any kind of network
(acyclic, link cyclic, flow cyclic, or even in the presence of knots). The key
idea will be the appropriate use of the delay (both natural and additional)
during the encoding. The resulting code will always work with finite delay with
binary encoding coefficients."
"In this paper, the bit energy requirements of training-based transmission
over block Rayleigh fading channels are studied. Pilot signals are employed to
obtain the minimum mean-square-error (MMSE) estimate of the channel fading
coefficients. Energy efficiency is analyzed in the worst case scenario where
the channel estimate is assumed to be perfect and the error in the estimate is
considered as another source of additive Gaussian noise. It is shown that bit
energy requirement grows without bound as the snr goes to zero, and the minimum
bit energy is achieved at a nonzero snr value below which one should not
operate. The effect of the block length on both the minimum bit energy and the
snr value at which the minimum is achieved is investigated. Flash training
schemes are analyzed and shown to improve the energy efficiency in the low-snr
regime. Energy efficiency analysis is also carried out when peak power
constraints are imposed on pilot signals."
"The low-snr capacity of M-ary PSK transmission over both the additive white
Gaussian noise (AWGN) and fading channels is analyzed when hard-decision
detection is employed at the receiver. Closed-form expressions for the first
and second derivatives of the capacity at zero snr are obtained. The
spectral-efficiency/bit-energy tradeoff in the low-snr regime is analyzed by
finding the wideband slope and the bit energy required at zero spectral
efficiency. Practical design guidelines are drawn from the
information-theoretic analysis. The fading channel analysis is conducted for
both coherent and noncoherent cases, and the performance penalty in the
low-power regime for not knowing the channel is identified."
"In this paper, pilot-assisted transmission over Gauss-Markov Rayleigh fading
channels is considered. A simple scenario, where a single pilot signal is
transmitted every T symbols and T-1 data symbols are transmitted in between the
pilots, is studied. First, it is assumed that binary phase-shift keying (BPSK)
modulation is employed at the transmitter. With this assumption, the training
period, and data and training power allocation are jointly optimized by
maximizing an achievable rate expression. Achievable rates and energy-per-bit
requirements are computed using the optimal training parameters. Secondly, a
capacity lower bound is obtained by considering the error in the estimate as
another source of additive Gaussian noise, and the training parameters are
optimized by maximizing this lower bound."
"In this paper, the error performance of on-off frequency shift keying (OOFSK)
modulation over fading channels is analyzed when the receiver is equipped with
multiple antennas. The analysis is conducted in two cases: the coherent
scenario where the fading is perfectly known at the receiver, and the
noncoherent scenario where neither the receiver nor the transmitter knows the
fading coefficients. For both cases, the maximum a posteriori probability (MAP)
detection rule is derived and analytical probability of error expressions are
obtained. The effect of fading correlation among the receiver antennas is also
studied. Simulation results indicate that for sufficiently low duty cycle
values, lower probability of error values with respect to FSK signaling are
achieved. Equivalently, when compared to FSK modulation, OOFSK with low duty
cycle requires less energy to achieve the same probability of error, which
renders this modulation a more energy efficient transmission technique."
"In this paper, the performance of signaling strategies with high
peak-to-average power ratio is analyzed in both coherent and noncoherent fading
channels. Two recently proposed modulation schemes, namely on-off binary
phase-shift keying and on-off quaternary phase-shift keying, are considered.
For these modulation formats, the optimal decision rules used at the detector
are identified and analytical expressions for the error probabilities are
obtained. Numerical techniques are employed to compute the error probabilities.
It is concluded that increasing the peakedness of the signals results in
reduced error rates for a given power level and hence improve the energy
efficiency."
"We consider power allocation algorithms for fixed-rate transmission over
Nakagami-m non-ergodic block-fading channels with perfect transmitter and
receiver channel state information and discrete input signal constellations
under both short- and long-term power constraints. Optimal power allocation
schemes are shown to be direct applications of previous results in the
literature. We show that the SNR exponent of the optimal short-term scheme is
given by the Singleton bound. We also illustrate the significant gains
available by employing long-term power constraints. Due to the nature of the
expressions involved, the complexity of optimal schemes may be prohibitive for
system implementation. We propose simple sub-optimal power allocation schemes
whose outage probability performance is very close to the minimum outage
probability obtained by optimal schemes."
"This paper proposes a novel algorithm for finding error-locators of
algebraic-geometric codes that can eliminate the division-calculations of
finite fields from the Berlekamp-Massey-Sakata algorithm. This inverse-free
algorithm provides full performance in correcting a certain class of errors,
generic errors, which includes most errors, and can decode codes on algebraic
curves without the determination of unknown syndromes. Moreover, we propose
three different kinds of architectures that our algorithm can be applied to,
and we represent the control operation of shift-registers and switches at each
clock-timing with numerical simulations. We estimate the performance in
comparison of the total running time and the numbers of multipliers and
shift-registers in three architectures with those of the conventional ones for
codes on algebraic curves."
"A key idea in coding for the broadcast channel (BC) is binning, in which the
transmitter encode information by selecting a codeword from an appropriate bin
(the messages are thus the bin indexes). This selection is normally done by
solving an appropriate (possibly difficult) combinatorial problem. Recently it
has been shown that binning for the Blackwell channel --a particular BC-- can
be done by iterative schemes based on Survey Propagation (SP). This method uses
decimation for SP and suffers a complexity of O(n^2). In this paper we propose
a new variation of the Belief Propagation (BP) algorithm, named Reinforced BP
algorithm, that turns BP into a solver. Our simulations show that this new
algorithm has complexity O(n log n). Using this new algorithm together with a
non-linear coding scheme, we can efficiently achieve rates close to the border
of the capacity region of the Blackwell channel."
"We present a new class of irregular low-density parity-check (LDPC) codes for
moderate block lengths (up to a few thousand bits) that are well-suited for
rate-compatible puncturing. The proposed codes show good performance under
puncturing over a wide range of rates and are suitable for usage in incremental
redundancy hybrid-automatic repeat request (ARQ) systems. In addition, these
codes are linear-time encodable with simple shift-register circuits. For a
block length of 1200 bits the codes outperform optimized irregular LDPC codes
and extended irregular repeat-accumulate (eIRA) codes for all puncturing rates
0.6~0.9 (base code performance is almost the same) and are particularly good at
high puncturing rates where good puncturing performance has been previously
difficult to achieve."
"This paper considers the multi-input multi-output (MIMO) relay channel where
multiple antennas are employed by each terminal. Compared to single-input
single-output (SISO) relay channels, MIMO relay channels introduce additional
degrees of freedom, making the design and analysis of optimal cooperative
strategies more complex. In this paper, a partial cooperation strategy that
combines transmit-side message splitting and block-Markov encoding is
presented. Lower bounds on capacity that improve on a previously proposed
non-cooperative lower bound are derived for Gaussian MIMO relay channels."
"The distributed source coding problem is considered when the sensors, or
encoders, are under Byzantine attack; that is, an unknown number of sensors
have been reprogrammed by a malicious intruder to undermine the reconstruction
at the fusion center. Three different forms of the problem are considered. The
first is a variable-rate setup, in which the decoder adaptively chooses the
rates at which the sensors transmit. An explicit characterization of the
variable-rate minimum achievable sum rate is stated, given by the maximum
entropy over the set of distributions indistinguishable from the true source
distribution by the decoder. In addition, two forms of the fixed-rate problem
are considered, one with deterministic coding and one with randomized coding.
The achievable rate regions are given for both these problems, with a larger
region achievable using randomized coding, though both are suboptimal compared
to variable-rate coding."
"In this paper the benefits provided by multi-cell processing of signals
transmitted by mobile terminals which are received via dedicated relay
terminals (RTs) are assessed. Unlike previous works, each RT is assumed here to
be capable of full-duplex operation and receives the transmission of adjacent
relay terminals. Focusing on intra-cell TDMA and non-fading channels, a
simplified uplink cellular model introduced by Wyner is considered. This
framework facilitates analytical derivation of the per-cell sum-rate of
multi-cell and conventional single-cell receivers. In particular, the analysis
is based on the observation that the signal received at the base stations can
be interpreted as the outcome of a two-dimensional linear time invariant
system. Numerical results are provided as well in order to provide further
insight into the performance benefits of multi-cell processing with relaying."
"In this paper, achievable rates of imperfectly-known fading relay channels
are studied. It is assumed that communication starts with the network training
phase in which the receivers estimate the fading coefficients of their
respective channels. In the data transmission phase, amplify-and-forward and
decode-and-forward relaying schemes are considered, and the corresponding
achievable rate expressions are obtained. The achievable rate expressions are
then employed to identify the optimal resource allocation strategies."
"In previous work, an ordering result was given for the symbolwise probability
of error using general Markov channels, under iterative decoding of LDPC codes.
In this paper, the ordering result is extended to mutual information, under the
assumption of an iid input distribution. For certain channels, in which the
capacity-achieving input distribution is iid, this allows ordering of the
channels by capacity. The complexity of analyzing general Markov channels is
mitigated by this ordering, since it is possible to immediately determine that
a wide class of channels, with different numbers of states, has a smaller
mutual information than a given channel."
"In wireless data networks, communication is particularly susceptible to
eavesdropping due to its broadcast nature. Security and privacy systems have
become critical for wireless providers and enterprise networks. This paper
considers the problem of secret communication over the Gaussian broadcast
channel, where a multi-antenna transmitter sends independent confidential
messages to two users with perfect secrecy. That is, each user would like to
obtain its own message reliably and confidentially. First, a computable
Sato-type outer bound on the secrecy capacity region is provided for a
multi-antenna broadcast channel with confidential messages. Next, a dirty-paper
secure coding scheme and its simplified version are described. For each case,
the corresponding achievable rate region is derived under the perfect secrecy
requirement. Finally, two numerical examples demonstrate that the Sato-type
outer bound is consistent with the boundary of the simplified dirty-paper
coding secrecy rate region."
"Convexity/concavity properties of symbol error rates (SER) of the maximum
likelihood detector operating in the AWGN channel (non-fading and fading) are
studied. Generic conditions are identified under which the SER is a
convex/concave function of the SNR. Universal bounds for the SER 1st and 2nd
derivatives are obtained, which hold for arbitrary constellations and are tight
for some of them. Applications of the results are discussed, which include
optimum power allocation in spatial multiplexing systems, optimum power/time
sharing to decrease or increase (jamming problem) error rate, and implication
for fading channels."
"Diversity-multiplexing tradeoff (DMT) presents a compact framework to compare
various MIMO systems and channels in terms of the two main advantages they
provide (i.e. high data rate and/or low error rate). This tradeoff was
characterized asymptotically (SNR-> infinity) for i.i.d. Rayleigh fading
channel by Zheng and Tse [1]. The asymptotic DMT overestimates the finite-SNR
one [2]. In this paper, using the recent results on the asymptotic (in the
number of antennas) outage capacity distribution, we derive and analyze the
finite-SNR DMT for a broad class of channels (not necessarily Rayleigh fading).
Based on this, we give the convergence conditions for the asymptotic DMT to be
approached by the finite-SNR one. The multiplexing gain definition is shown to
affect critically the convergence point: when the multiplexing gain is defined
via the mean (ergodic) capacity, the convergence takes place at realistic SNR
values. Furthermore, in this case the diversity gain can also be used to
estimate the outage probability with reasonable accuracy. The multiplexing gain
definition via the high-SNR asymptote of the mean capacity (as in [1]) results
in very slow convergence for moderate to large systems (as 1/ln(SNR)^2) and,
hence, the asymptotic DMT cannot be used at realistic SNR values. For this
definition, the high-SNR threshold increases exponentially in the number of
antennas and in the multiplexing gain. For correlated keyhole channel, the
diversity gain is shown to decrease with correlation and power imbalance of the
channel. While the SNR-asymptotic DMT of Zheng and Tse does not capture this
effect, the size-asymptotic DMT does."
"A unified analytical framework for optimum power allocation in the unordered
V-BLAST algorithm and its comparative performance analysis are presented.
Compact closed-form approximations for the optimum power allocation are
derived, based on average total and block error rates. The choice of the
criterion has little impact on the power allocation and, overall, the optimum
strategy is to allocate more power to lower step transmitters and less to
higher ones. High-SNR approximations for optimized average block and total
error rates are given. The SNR gain of optimization is rigorously defined and
studied using analytical tools, including lower and upper bounds, high and low
SNR approximations. The gain is upper bounded by the number of transmitters,
for any modulation format and type of fading channel. While the average
optimization is less complex than the instantaneous one, its performance is
almost as good at high SNR. A measure of robustness of the optimized algorithm
is introduced and evaluated. The optimized algorithm is shown to be robust to
perturbations in individual and total transmit powers. Based on the algorithm
robustness, a pre-set power allocation is suggested as a low-complexity
alternative to the other optimization strategies, which exhibits only a minor
loss in performance over the practical SNR range."
"The min-sum (MS) algorithm is arguably the second most fundamental algorithm
in the realm of message passing due to its optimality (for a tree code) with
respect to the {\em block error} probability \cite{Wiberg}. There also seems to
be a fundamental relationship of MS decoding with the linear programming
decoder \cite{Koetter}. Despite its importance, its fundamental properties have
not nearly been studied as well as those of the sum-product (also known as BP)
algorithm.
  We address two questions related to the MS rule. First, we characterize the
stability condition under MS decoding. It turns out to be essentially the same
condition as under BP decoding. Second, we perform a degree distribution
optimization. Contrary to the case of BP decoding, under MS decoding the
thresholds of the best degree distributions for standard irregular LDPC
ensembles are significantly bounded away from the Shannon threshold. More
precisely, on the AWGN channel, for the best codes that we find, the gap to
capacity is 1dB for a rate 0.3 code and it is 0.4dB when the rate is 0.9 (the
gap decreases monotonically as we increase the rate).
  We also used the optimization procedure to design codes for modified MS
algorithm where the output of the check node is scaled by a constant
$1/\alpha$. For $\alpha = 1.25$, we observed that the gap to capacity was
lesser for the modified MS algorithm when compared with the MS algorithm.
However, it was still quite large, varying from 0.75 dB to 0.2 dB for rates
between 0.3 and 0.9.
  We conclude by posing what we consider to be the most important open
questions related to the MS algorithm."
"In this paper, a family of low-density parity-check (LDPC) degree
distributions, whose decoding threshold on the binary erasure channel (BEC)
admits a simple closed form, is presented. These degree distributions are a
subset of the check regular distributions (i.e. all the check nodes have the
same degree), and are referred to as $p$-positive distributions. It is given
proof that the threshold for a $p$-positive distribution is simply expressed by
$[\lambda'(0)\rho'(1)]^{-1}$. Besides this closed form threshold expression,
the $p$-positive distributions exhibit three additional properties. First, for
given code rate, check degree and maximum variable degree, they are in some
cases characterized by a threshold which is extremely close to that of the best
known check regular distributions, under the same set of constraints. Second,
the threshold optimization problem within the $p$-positive class can be solved
in some cases with analytic methods, without using any numerical optimization
tool. Third, these distributions can achieve the BEC capacity. The last
property is shown by proving that the well-known binomial degree distributions
belong to the $p$-positive family."
"We characterize the capacity of the general class of noncoherent underspread
wide-sense stationary uncorrelated scattering (WSSUS) time-frequency-selective
Rayleigh fading channels, under peak constraints in time and frequency and in
time only. Capacity upper and lower bounds are found which are explicit in the
channel's scattering function and allow to identify the capacity-maximizing
bandwidth for a given scattering function and a given peak-to-average power
ratio."
"We analyze fading interference relay networks where M single-antenna
source-destination terminal pairs communicate concurrently and in the same
frequency band through a set of K single-antenna relays using half-duplex
two-hop relaying. Assuming that the relays have channel state information
(CSI), it is shown that in the large-M limit, provided K grows fast enough as a
function of M, the network ""decouples"" in the sense that the individual
source-destination terminal pair capacities are strictly positive. The
corresponding required rate of growth of K as a function of M is found to be
sufficient to also make the individual source-destination fading links converge
to nonfading links. We say that the network ""crystallizes"" as it breaks up into
a set of effectively isolated ""wires in the air"". A large-deviations analysis
is performed to characterize the ""crystallization"" rate, i.e., the rate (as a
function of M,K) at which the decoupled links converge to nonfading links. In
the course of this analysis, we develop a new technique for characterizing the
large-deviations behavior of certain sums of dependent random variables. For
the case of no CSI at the relay level, assuming amplify-and-forward relaying,
we compute the per source-destination terminal pair capacity for M,K converging
to infinity, with K/M staying fixed, using tools from large random matrix
theory."
"This paper addresses the performance of bit-interleaved coded multiple
beamforming (BICMB) [1], [2] with imperfect knowledge of beamforming vectors.
Most studies for limited-rate channel state information at the transmitter
(CSIT) assume that the precoding matrix has an invariance property under an
arbitrary unitary transform. In BICMB, this property does not hold. On the
other hand, the optimum precoder and detector for BICMB are invariant under a
diagonal unitary transform. In order to design a limited-rate CSIT system for
BICMB, we propose a new distortion measure optimum under this invariance. Based
on this new distortion measure, we introduce a new set of centroids and employ
the generalized Lloyd algorithm for codebook design. We provide simulation
results demonstrating the performance improvement achieved with the proposed
distortion measure and the codebook design for various receivers with linear
detectors. We show that although these receivers have the same performance for
perfect CSIT, their performance varies under imperfect CSIT."
"This paper characterizes the effect of finite rate channel state feedback on
the sum rate of a multi-access multiple-input multiple-output (MIMO) system. We
propose to control the users jointly, specifically, we first choose the users
jointly and then select the corresponding beamforming vectors jointly. To
quantify the sum rate, this paper introduces the composite Grassmann manifold
and the composite Grassmann matrix. By characterizing the distortion rate
function on the composite Grassmann manifold and calculating the logdet
function of a random composite Grassmann matrix, a good sum rate approximation
is derived. According to the distortion rate function on the composite
Grassmann manifold, the loss due to finite beamforming decreases exponentially
as the feedback bits on beamforming increases."
"This paper considers the quantization problem on the Grassmann manifold with
dimension n and p. The unique contribution is the derivation of a closed-form
formula for the volume of a metric ball in the Grassmann manifold when the
radius is sufficiently small. This volume formula holds for Grassmann manifolds
with arbitrary dimension n and p, while previous results are only valid for
either p=1 or a fixed p with asymptotically large n. Based on the volume
formula, the Gilbert-Varshamov and Hamming bounds for sphere packings are
obtained. Assuming a uniformly distributed source and a distortion metric based
on the squared chordal distance, tight lower and upper bounds are established
for the distortion rate tradeoff. Simulation results match the derived results.
As an application of the derived quantization bounds, the information rate of a
Multiple-Input Multiple-Output (MIMO) system with finite-rate channel-state
feedback is accurately quantified for arbitrary finite number of antennas,
while previous results are only valid for either Multiple-Input Single-Output
(MISO) systems or those with asymptotically large number of transmit antennas
but fixed number of receive antennas."
"This paper quantifies the information rate of multiple-input multiple-output
(MIMO) systems with finite rate channel state feedback and power on/off
strategy. In power on/off strategy, a beamforming vector (beam) is either
turned on (denoted by on-beam) with a constant power or turned off. We prove
that the ratio of the optimal number of on-beams and the number of antennas
converges to a constant for a given signal-to-noise ratio (SNR) when the number
of transmit and receive antennas approaches infinity simultaneously and when
beamforming is perfect. Based on this result, a near optimal strategy, i.e.,
power on/off strategy with a constant number of on-beams, is discussed. For
such a strategy, we propose the power efficiency factor to quantify the effect
of imperfect beamforming. A formula is proposed to compute the maximum power
efficiency factor achievable given a feedback rate. The information rate of the
overall MIMO system can be approximated by combining the asymptotic results and
the formula for power efficiency factor. Simulations show that this
approximation is accurate for all SNR regimes."
"This paper considers broadcast channels with L antennas at the base station
and m single-antenna users, where each user has perfect channel knowledge and
the base station obtains channel information through a finite rate feedback.
The key observation of this paper is that the optimal number of on-users (users
turned on), say s, is a function of signal-to-noise ratio (SNR) and other
system parameters. Towards this observation, we use asymptotic analysis to
guide the design of feedback and transmission strategies. As L, m and the
feedback rates approach infinity linearly, we derive the asymptotic optimal
feedback strategy and a realistic criterion to decide which users should be
turned on. Define the corresponding asymptotic throughput per antenna as the
spatial efficiency. It is a function of the number of on-users s, and
therefore, s should be appropriately chosen. Based on the above asymptotic
results, we also develop a scheme for a system with finite many antennas and
users. Compared with other works where s is presumed constant, our scheme
achieves a significant gain by choosing the appropriate s. Furthermore, our
analysis and scheme is valid for heterogeneous systems where different users
may have different path loss coefficients and feedback rates."
"The Grassmann manifold G_{n,p}(L) is the set of all p-dimensional planes
(through the origin) in the n-dimensional Euclidean space L^{n}, where L is
either R or C. This paper considers an unequal dimensional quantization in
which a source in G_{n,p}(L) is quantized through a code in G_{n,q}(L), where p
and q are not necessarily the same. It is different from most works in
literature where p\equiv q. The analysis for unequal dimensional quantization
is based on the volume of a metric ball in G_{n,p}(L) whose center is in
G_{n,q}(L). Our chief result is a closed-form formula for the volume of a
metric ball when the radius is sufficiently small. This volume formula holds
for Grassmann manifolds with arbitrary n, p, q and L, while previous results
pertained only to some special cases. Based on this volume formula, several
bounds are derived for the rate distortion tradeoff assuming the quantization
rate is sufficiently high. The lower and upper bounds on the distortion rate
function are asymptotically identical, and so precisely quantify the asymptotic
rate distortion tradeoff. We also show that random codes are asymptotically
optimal in the sense that they achieve the minimum achievable distortion with
probability one as n and the code rate approach infinity linearly. Finally, we
discuss some applications of the derived results to communication theory. A
geometric interpretation in the Grassmann manifold is developed for capacity
calculation of additive white Gaussian noise channel. Further, the derived
distortion rate function is beneficial to characterizing the effect of
beamforming matrix selection in multi-antenna communications."
"Sphere decoding (SD) is a low complexity maximum likelihood (ML) detection
algorithm, which has been adapted for different linear channels in digital
communications. The complexity of the SD has been shown to be exponential in
some cases, and polynomial in others and under certain assumptions. The sphere
radius and the number of nodes visited throughout the tree traversal search are
the decisive factors for the complexity of the algorithm. The radius problem
has been addressed and treated widely in the literature. In this paper, we
propose a new structure for SD, which drastically reduces the overall
complexity. The complexity is measured in terms of the floating point
operations per second (FLOPS) and the number of nodes visited throughout the
algorithm tree search. This reduction in the complexity is due to the ability
of decoding the real and imaginary parts of each jointly detected symbol
independently of each other, making use of the new lattice representation. We
further show by simulations that the new approach achieves 80% reduction in the
overall complexity compared to the conventional SD for a 2x2 system, and almost
50% reduction for the 4x4 and 6x6 cases, thus relaxing the requirements for
hardware implementation."
"Based on Planck's blackbody equation it is argued that a single mode light
pulse, with a large number of photons, carries one entropy unit. Similarly, an
empty radiation mode carries no entropy. In this case, the calculated entropy
that a coded sequence of light pulses is carrying is simply the Gibbs mixing
entropy, which is identical to the logical Shannon information. This approach
is supported by a demonstration that information transmission and
amplification, by a sequence of light pulses in an optical fiber, is a classic
Carnot machine comprising of two isothermals and two adiabatic. Therefore it is
concluded that entropy under certain conditions is information."
"This paper studies the ergodic capacity of time- and frequency-selective
multipath fading channels in the ultrawideband (UWB) regime when training
signals are used for channel estimation at the receiver. Motivated by recent
measurement results on UWB channels, we propose a model for sparse multipath
channels. A key implication of sparsity is that the independent degrees of
freedom (DoF) in the channel scale sub-linearly with the signal space dimension
(product of signaling duration and bandwidth). Sparsity is captured by the
number of resolvable paths in delay and Doppler. Our analysis is based on a
training and communication scheme that employs signaling over orthogonal
short-time Fourier (STF) basis functions. STF signaling naturally relates
sparsity in delay-Doppler to coherence in time-frequency. We study the impact
of multipath sparsity on two fundamental metrics of spectral efficiency in the
wideband/low-SNR limit introduced by Verdu: first- and second-order optimality
conditions. Recent results by Zheng et. al. have underscored the large gap in
spectral efficiency between coherent and non-coherent extremes and the
importance of channel learning in bridging the gap. Building on these results,
our results lead to the following implications of multipath sparsity: 1) The
coherence requirements are shared in both time and frequency, thereby
significantly relaxing the required scaling in coherence time with SNR; 2)
Sparse multipath channels are asymptotically coherent -- for a given but large
bandwidth, the channel can be learned perfectly and the coherence requirements
for first- and second-order optimality met through sufficiently large signaling
duration; and 3) The requirement of peaky signals in attaining capacity is
eliminated or relaxed in sparse environments."
"In contrast to the prevalent assumption of rich multipath in information
theoretic analysis of wireless channels, physical channels exhibit sparse
multipath, especially at large bandwidths. We propose a model for sparse
multipath fading channels and present results on the impact of sparsity on
non-coherent capacity and reliability in the wideband regime. A key implication
of sparsity is that the statistically independent degrees of freedom in the
channel, that represent the delay-Doppler diversity afforded by multipath,
scale at a sub-linear rate with the signal space dimension (time-bandwidth
product). Our analysis is based on a training-based communication scheme that
uses short-time Fourier (STF) signaling waveforms. Sparsity in delay-Doppler
manifests itself as time-frequency coherence in the STF domain. From a capacity
perspective, sparse channels are asymptotically coherent: the gap between
coherent and non-coherent extremes vanishes in the limit of large signal space
dimension without the need for peaky signaling. From a reliability viewpoint,
there is a fundamental tradeoff between channel diversity and learnability that
can be optimized to maximize the error exponent at any rate by appropriately
choosing the signaling duration as a function of bandwidth."
"We study and compare the Shannon capacity region and the stable throughput
region for a random access system in which source nodes multicast their
messages to multiple destination nodes. Under an erasure channel model which
accounts for interference and allows for multipacket reception, we first
characterize the Shannon capacity region. We then consider a queueing-theoretic
formulation and characterize the stable throughput region for two different
transmission policies: a retransmission policy and random linear coding. Our
results indicate that for large blocklengths, the random linear coding policy
provides a higher stable throughput than the retransmission policy.
Furthermore, our results provide an example of a transmission policy for which
the Shannon capacity region strictly outer bounds the stable throughput region,
which contradicts an unproven conjecture that the Shannon capacity and stable
throughput coincide for random access systems."
"A transmitter without channel state information (CSI) wishes to send a
delay-limited Gaussian source over a slowly fading channel. The source is coded
in superimposed layers, with each layer successively refining the description
in the previous one. The receiver decodes the layers that are supported by the
channel realization and reconstructs the source up to a distortion. The
expected distortion is minimized by optimally allocating the transmit power
among the source layers. For two source layers, the allocation is optimal when
power is first assigned to the higher layer up to a power ceiling that depends
only on the channel fading distribution; all remaining power, if any, is
allocated to the lower layer. For convex distortion cost functions with convex
constraints, the minimization is formulated as a convex optimization problem.
In the limit of a continuum of infinite layers, the minimum expected distortion
is given by the solution to a set of linear differential equations in terms of
the density of the fading distribution. As the bandwidth ratio b (channel uses
per source symbol) tends to zero, the power distribution that minimizes
expected distortion converges to the one that maximizes expected capacity.
While expected distortion can be improved by acquiring CSI at the transmitter
(CSIT) or by increasing diversity from the realization of independent fading
paths, at high SNR the performance benefit from diversity exceeds that from
CSIT, especially when b is large."
"In this paper, a transmission protocol is studied for a two relay wireless
network in which simple repetition coding is applied at the relays.
Information-theoretic achievable rates for this transmission scheme are given,
and a space-time V-BLAST signalling and detection method that can approach them
is developed. It is shown through the diversity multiplexing tradeoff analysis
that this transmission scheme can recover the multiplexing loss of the
half-duplex relay network, while retaining some diversity gain. This scheme is
also compared with conventional transmission protocols that exploit only the
diversity of the network at the cost of a multiplexing loss. It is shown that
the new transmission protocol offers significant performance advantages over
conventional protocols, especially when the interference between the two relays
is sufficiently strong."
"In random-access communication systems, the number of active users varies
with time, and has considerable bearing on receiver's performance. Thus,
techniques aimed at identifying not only the information transmitted, but also
that number, play a central role in those systems. An example of application of
these techniques can be found in multiuser detection (MUD). In typical MUD
analyses, receivers are based on the assumption that the number of active users
is constant and known at the receiver, and coincides with the maximum number of
users entitled to access the system. This assumption is often overly
pessimistic, since many users might be inactive at any given time, and
detection under the assumption of a number of users larger than the real one
may impair performance.
  The main goal of this paper is to introduce a general approach to the problem
of identifying active users and estimating their parameters and data in a
random-access system where users are continuously entering and leaving the
system. The tool whose use we advocate is Random-Set Theory: applying this, we
derive optimum receivers in an environment where the set of transmitters
comprises an unknown number of elements. In addition, we can derive
Bayesian-filter equations which describe the evolution with time of the a
posteriori probability density of the unknown user parameters, and use this
density to derive optimum detectors. In this paper we restrict ourselves to
interferer identification and data detection, while in a companion paper we
shall examine the more complex problem of estimating users' parameters."
"We study the problem of constructing coded modulation schemes over
multidimensional signal sets in Nakagami-$m$ block-fading channels. In
particular, we consider the optimal diversity reliability exponent of the error
probability when the multidimensional constellation is obtained as the rotation
of classical complex-plane signal constellations. We show that multidimensional
rotations of full dimension achieve the optimal diversity reliability exponent,
also achieved by Gaussian constellations. Multidimensional rotations of full
dimension induce a large decoding complexity, and in some cases it might be
beneficial to use multiple rotations of smaller dimension. We also study the
diversity reliability exponent in this case, which yields the optimal
rate-diversity-complexity tradeoff in block-fading channels with discrete
inputs."
"We analyze fading relay networks, where a single-antenna source-destination
terminal pair communicates through a set of half-duplex single-antenna relays
using a two-hop protocol with linear processing at the relay level. A family of
relaying schemes is presented which achieves the entire optimal
diversity-multiplexing (DM) tradeoff curve. As a byproduct of our analysis, it
follows that delay diversity and phase-rolling at the relay level are optimal
with respect to the entire DM-tradeoff curve, provided the delays and the
modulation frequencies, respectively, are chosen appropriately."
"In this paper, a novel decoding algorithm for low-density parity-check (LDPC)
codes based on convex optimization is presented. The decoding algorithm, called
interior point decoding, is designed for linear vector channels. The linear
vector channels include many practically important channels such as inter
symbol interference channels and partial response channels. It is shown that
the maximum likelihood decoding (MLD) rule for a linear vector channel can be
relaxed to a convex optimization problem, which is called a relaxed MLD
problem. The proposed decoding algorithm is based on a numerical optimization
technique so called interior point method with barrier function. Approximate
variations of the gradient descent and the Newton methods are used to solve the
convex optimization problem. In a decoding process of the proposed algorithm, a
search point always lies in the fundamental polytope defined based on a
low-density parity-check matrix. Compared with a convectional joint message
passing decoder, the proposed decoding algorithm achieves better BER
performance with less complexity in the case of partial response channels in
many cases."
"In this paper, redundant random matrix ensembles (abbreviated as redundant
random ensembles) are defined and their stopping set (SS) weight distributions
are analyzed. A redundant random ensemble consists of a set of binary matrices
with linearly dependent rows. These linearly dependent rows (redundant rows)
significantly reduce the number of stopping sets of small size. An upper and
lower bound on the average SS weight distribution of the redundant random
ensembles are shown. From these bounds, the trade-off between the number of
redundant rows (corresponding to decoding complexity of BP on BEC) and the
critical exponent of the asymptotic growth rate of SS weight distribution
(corresponding to decoding performance) can be derived. It is shown that, in
some cases, a dense matrix with linearly dependent rows yields asymptotically
(i.e., in the regime of small erasure probability) better performance than
regular LDPC matrices with comparable parameters."
"In this paper, an analysis of the undetected error probability of ensembles
of binary matrices is presented. The ensemble called the Bernoulli ensemble
whose members are considered as matrices generated from i.i.d. Bernoulli source
is mainly considered here. The main contributions of this work are (i)
derivation of the error exponent of the average undetected error probability
and (ii) closed form expressions for the variance of the undetected error
probability. It is shown that the behavior of the exponent for a sparse
ensemble is somewhat different from that for a dense ensemble. Furthermore, as
a byproduct of the proof of the variance formula, simple covariance formula of
the weight distribution is derived."
"This paper demonstrates that basic statistics (mean, variance) of the
logarithm of the variate itself can be used in the calculation of differential
entropy among random variables known to be multiples and powers of a common
underlying variate. For the same set of distributions, the variance of the
differential self-information is shown also to be a function of statistics of
the logarithmic variate. Then entropy and its ""variance"" can be estimated using
only statistics of the logarithmic variate plus constants, without reference to
the traditional parameters of the variate."
"In this paper, we study the end-to-end distortion/delay tradeoff for a
analogue source transmitted over a fading channel. The analogue source is
quantized and stored in a buffer until it is transmitted. There are two extreme
cases as far as buffer delay is concerned: no delay and infinite delay. We
observe that there is a significant power gain by introducing a buffer delay.
Our goal is to investigate the situation between these two extremes. Using
recently proposed \emph{effective capacity} concept, we derive a closed-form
formula for this tradeoff. For SISO case, an asymptotically tight upper bound
for our distortion-delay curve is derived, which approaches to the infinite
delay lower bound as $\mathcal{D}_\infty \exp(\frac{C}{\tau_n})$, with $\tau_n$
is the normalized delay, $C$ is a constant. For more general MIMO channel, we
computed the distortion SNR exponent -- the exponential decay rate of the
expected distortion in the high SNR regime. Numerical results demonstrate that
introduction of a small amount delay can save significant transmission power."
"A new approach for upper bounding the channel reliability function using the
code spectrum is described. It allows to treat both low and high rate cases in
a unified way. In particular, the earlier known upper bounds are improved, and
a new derivation of the sphere-packing bound is presented."
"The reconstruction of a deterministic data field from binary-quantized noisy
observations of sensors which are randomly deployed over the field domain is
studied. The study focuses on the extremes of lack of deterministic control in
the sensor deployment, lack of knowledge of the noise distribution, and lack of
sensing precision and reliability. Such adverse conditions are motivated by
possible real-world scenarios where a large collection of low-cost, crudely
manufactured sensors are mass-deployed in an environment where little can be
assumed about the ambient noise. A simple estimator that reconstructs the
entire data field from these unreliable, binary-quantized, noisy observations
is proposed. Technical conditions for the almost sure and integrated mean
squared error (MSE) convergence of the estimate to the data field, as the
number of sensors tends to infinity, are derived and their implications are
discussed. For finite-dimensional, bounded-variation, and
Sobolev-differentiable function classes, specific integrated MSE decay rates
are derived. For the first and third function classes these rates are found to
be minimax order optimal with respect to infinite precision sensing and known
noise distribution."
"We consider the following problem of decentralized statistical inference:
given i.i.d. samples from an unknown distribution, estimate an arbitrary
quantile subject to limits on the number of bits exchanged. We analyze a
standard fusion-based architecture, in which each of $m$ sensors transmits a
single bit to the fusion center, which in turn is permitted to send some number
$k$ bits of feedback. Supposing that each of $\nodenum$ sensors receives $n$
observations, the optimal centralized protocol yields mean-squared error
decaying as $\order(1/[n m])$. We develop and analyze the performance of
various decentralized protocols in comparison to this centralized
gold-standard. First, we describe a decentralized protocol based on $k =
\log(\nodenum)$ bits of feedback that is strongly consistent, and achieves the
same asymptotic MSE as the centralized optimum. Second, we describe and analyze
a decentralized protocol based on only a single bit ($k=1$) of feedback. For
step sizes independent of $m$, it achieves an asymptotic MSE of order
$\order[1/(n \sqrt{m})]$, whereas for step sizes decaying as $1/\sqrt{m}$, it
achieves the same $\order(1/[n m])$ decay in MSE as the centralized optimum.
Our theoretical results are complemented by simulations, illustrating the
tradeoffs between these different protocols."
"We characterize stability regions of two-user fading Gaussian multiple access
(MAC) and broadcast (BC) networks with centralized scheduling. The data to be
transmitted to the users is encoded into codewords of fixed length. The rates
of the codewords used are restricted to a fixed set of finite cardinality. With
successive decoding and interference cancellation at the receivers, we find the
set of arrival rates that can be stabilized over the MAC and BC networks. In
MAC and BC networks with average power constraints, we observe that the duality
property that relates the MAC and BC information theoretic capacity regions
extend to their stability regions as well. In MAC and BC networks with peak
power constraints, the union of stability regions of dual MAC networks is found
to be strictly contained in the BC stability region."
"The problem of hypothesis testing against independence for a Gauss-Markov
random field (GMRF) is analyzed. Assuming an acyclic dependency graph, an
expression for the log-likelihood ratio of detection is derived. Assuming
random placement of nodes over a large region according to the Poisson or
uniform distribution and nearest-neighbor dependency graph, the error exponent
of the Neyman-Pearson detector is derived using large-deviations theory. The
error exponent is expressed as a dependency-graph functional and the limit is
evaluated through a special law of large numbers for stabilizing graph
functionals. The exponent is analyzed for different values of the variance
ratio and correlation. It is found that a more correlated GMRF has a higher
exponent at low values of the variance ratio whereas the situation is reversed
at high values of the variance ratio."
"In this paper we give a short theoretical description of the general
predictive adaptive arithmetic coding technique. The links between this
technique and the works of J. Rissanen in the 80's, in particular the BIC
information criterion used in parametrical model selection problems, are
established. We also design lossless and lossy coding techniques of images. The
lossless technique uses a mix between fixed-length coding and arithmetic coding
and provides better compression results than those separate methods. That
technique is also seen to have an interesting application in the domain of
statistics since it gives a data-driven procedure for the non-parametrical
histogram selection problem. The lossy technique uses only predictive adaptive
arithmetic codes and shows how a good choice of the order of prediction might
lead to better results in terms of compression. We illustrate those coding
techniques on a raw grayscale image."
"This paper is devoted to the use of hybrid Petri nets (PNs) for modeling and
control of hybrid dynamic systems (HDS). Modeling, analysis and control of HDS
attract ever more of researchers' attention and several works have been devoted
to these topics. We consider in this paper the extensions of the PN formalism
(initially conceived for modeling and analysis of discrete event systems) in
the direction of hybrid modeling. We present, first, the continuous PN models.
These models are obtained from discrete PNs by the fluidification of the
markings. They constitute the first steps in the extension of PNs toward hybrid
modeling. Then, we present two hybrid PN models, which differ in the class of
HDS they can deal with. The first one is used for deterministic HDS modeling,
whereas the second one can deal with HDS with nondeterministic behavior.
Keywords: Hybrid dynamic systems; D-elementary hybrid Petri nets; Hybrid
automata; Controller synthesis"
"The MacWilliams identity, which relates the weight distribution of a code to
the weight distribution of its dual code, is useful in determining the weight
distribution of codes. In this paper, we derive the MacWilliams identity for
linear codes with the rank metric, and our identity has a different form than
that by Delsarte. Using our MacWilliams identity, we also derive related
identities for rank metric codes. These identities parallel the binomial and
power moment identities derived for codes with the Hamming metric."
"We consider power allocation algorithms for fixed-rate transmission over
Nakagami-m non-ergodic block-fading channels with perfect transmitter and
receiver channel state information and discrete input signal constellations,
under both short- and long-term power constraints. Optimal power allocation
schemes are shown to be direct applications of previous results in the
literature. We show that the SNR exponent of the optimal short-term scheme is
given by m times the Singleton bound. We also illustrate the significant gains
available by employing long-term power constraints. In particular, we analyze
the optimal long-term solution, showing that zero outage can be achieved
provided that the corresponding short-term SNR exponent with the same system
parameters is strictly greater than one. Conversely, if the short-term SNR
exponent is smaller than one, we show that zero outage cannot be achieved. In
this case, we derive the corresponding long-term SNR exponent as a function of
the Singleton bound. Due to the nature of the expressions involved, the
complexity of optimal schemes may be prohibitive for system implementation. We
therefore propose simple sub-optimal power allocation schemes whose outage
probability performance is very close to the minimum outage probability
obtained by optimal schemes. We also show the applicability of these techniques
to practical systems employing orthogonal frequency division multiplexing."
"The space-time bit-interleaved coded modulation (ST-BICM) is an efficient
technique to obtain high diversity and coding gain on a block-fading MIMO
channel. Its maximum-likelihood (ML) performance is computed under ideal
interleaving conditions, which enables a global optimization taking into
account channel coding. Thanks to a diversity upperbound derived from the
Singleton bound, an appropriate choice of the time dimension of the space-time
coding is possible, which maximizes diversity while minimizing complexity.
Based on the analysis, an optimized interleaver and a set of linear precoders,
called dispersive nucleo algebraic (DNA) precoders are proposed. The proposed
precoders have good performance with respect to the state of the art and exist
for any number of transmit antennas and any time dimension. With turbo codes,
they exhibit a frame error rate which does not increase with frame length."
"In the analysis of large random wireless networks, the underlying node
distribution is almost ubiquitously assumed to be the homogeneous Poisson point
process. In this paper, the node locations are assumed to form a Poisson
clustered process on the plane. We derive the distributional properties of the
interference and provide upper and lower bounds for its CCDF. We consider the
probability of successful transmission in an interference limited channel when
fading is modeled as Rayleigh. We provide a numerically integrable expression
for the outage probability and closed-form upper and lower bounds.We show that
when the transmitter-receiver distance is large, the success probability is
greater than that of a Poisson arrangement. These results characterize the
performance of the system under geographical or MAC-induced clustering. We
obtain the maximum intensity of transmitting nodes for a given outage
constraint, i.e., the transmission capacity (of this spatial arrangement) and
show that it is equal to that of a Poisson arrangement of nodes. For the
analysis, techniques from stochastic geometry are used, in particular the
probability generating functional of Poisson cluster processes, the Palm
characterization of Poisson cluster processes and the Campbell-Mecke theorem."
"In this paper, we examine the effects of imperfect channel estimation at the
receiver and no channel knowledge at the transmitter on the capacity of the
fading Costa's channel with channel state information non-causally known at the
transmitter. We derive the optimal Dirty-paper coding (DPC) scheme and its
corresponding achievable rates with the assumption of Gaussian inputs. Our
results, for uncorrelated Rayleigh fading, provide intuitive insights on the
impact of the channel estimate and the channel characteristics (e.g. SNR,
fading process, channel training) on the achievable rates. These are useful in
practical scenarios of multiuser wireless communications (e.g. Broadcast
Channels) and information embedding applications (e.g. robust watermarking). We
also studied optimal training design adapted to each application. We provide
numerical results for a single-user fading Costa's channel with
maximum-likehood (ML) channel estimation. These illustrate an interesting
practical trade-off between the amount of training and its impact to the
interference cancellation performance using DPC scheme."
"The optimal decoder achieving the outage capacity under imperfect channel
estimation is investigated. First, by searching into the family of nearest
neighbor decoders, which can be easily implemented on most practical coded
modulation systems, we derive a decoding metric that minimizes the average of
the transmission error probability over all channel estimation errors. This
metric, for arbitrary memoryless channels, achieves the capacity of a composite
(more noisy) channel. Next, according to the notion of estimation-induced
outage capacity (EIO capacity) introduced in our previous work, we characterize
maximal achievable information rates associated to the proposed decoder. The
performance of the proposed decoding metric over uncorrelated Rayleigh fading
MIMO channels is compared to both the classical mismatched maximum-likelihood
(ML) decoder and the theoretical limits given by the EIO capacity (i.e. the
best decoder in presence of channel estimation errors). Numerical results show
that the derived metric provides significant gains, in terms of achievable
information rates and bit error rate (BER), in a bit interleaved coded
modulation (BICM) framework, without introducing any additional decoding
complexity."
"A multiple input multiple output (MIMO) two-way relay channel is considered,
where two sources want to exchange messages with each other using multiple
relay nodes, and both the sources and relay nodes are equipped with multiple
antennas. Both the sources are assumed to have equal number of antennas and
have perfect channel state information (CSI) for all the channels of the MIMO
two-way relay channel, whereas, each relay node is either assumed to have CSI
for its transmit and receive channel (the coherent case) or no CSI for any of
the channels (the non-coherent case). The main results in this paper are on the
scaling behavior of the capacity region of the MIMO two-way relay channel with
increasing number of relay nodes. In the coherent case, the capacity region of
the MIMO two-way relay channel is shown to scale linearly with the number of
antennas at source nodes and logarithmically with the number of relay nodes. In
the non-coherent case, the capacity region is shown to scale linearly with the
number of antennas at the source nodes and logarithmically with the signal to
noise ratio."
"We discuss how the loop calculus approach of [Chertkov, Chernyak '06],
enhanced by the pseudo-codeword search algorithm of [Chertkov, Stepanov '06]
and the facet-guessing idea from [Dimakis, Wainwright '06], improves decoding
of graph based codes in the error-floor domain. The utility of the new, Linear
Programming based, decoding is demonstrated via analysis and simulations of the
model $[155,64,20]$ code."
"Classically, communication systems are designed assuming perfect channel
state information at the receiver and/or transmitter. However, in many
practical situations, only an estimate of the channel is available that differs
from the true channel. We address this channel mismatch scenario by using the
notion of estimation-induced outage capacity, for which we provide an
associated coding theorem and its strong converse, assuming a discrete
memoryless channel. We illustrate our ideas via numerical simulations for
transmissions over Ricean fading channels under a quality of service (QoS)
constraint using rate-limited feedback channel and maximum likelihood (ML)
channel estimation. Our results provide intuitive insights on the impact of the
channel estimate and the channel characteristics (SNR, Ricean K-factor,
training sequence length, feedback rate, etc.) on the mean outage capacity."
"We analyze a MIMO DS-CDMA channel with a general multiuser detector including
a nonlinear multiuser detector, using the replica method. In the many-user,
limit the MIMO DS-CDMA channel with the multiuser detector is decoupled into a
bank of single-user SIMO Gaussian channels if a spatial spreading scheme is
employed. On the other hand, it is decoupled into a bank of single-user MIMO
Gaussian channels if a spatial spreading scheme is not employed. The spectral
efficiency of the MIMO DS-CDMA channel with the spatial spreading scheme is
comparable with that of the MIMO DS-CDMA channel using an optimal space-time
block code without the spatial spreading scheme. In the case of the QPSK data
modulation scheme the spectral efficiency of the MIMO DS-CDMA channel with the
MMSE detector shows {\it waterfall} behavior and is very close to the
corresponding sum capacity when the system load is just below the transition
point of the {\it waterfall} behavior. Our result implies that the performance
of a multiuser detector taking the data modulation scheme into consideration
can be far superior to that of linear multiuser detectors."
"This paper presents a decentralized relay selection protocol for a dense
wireless network and describes channel feedback strategies that improve its
performance. The proposed selection protocol supports hybrid
automatic-repeat-request transmission where relays forward parity information
to the destination in the event of a decoding error. Channel feedback is
employed for refining the relay selection process and for selecting an
appropriate transmission mode in a proposed adaptive modulation transmission
framework. An approximation of the throughput of the proposed adaptive
modulation strategy is presented, and the dependence of the throughput on
system parameters such as the relay contention probability and the adaptive
modulation switching point is illustrated via maximization of this
approximation. Simulations show that the throughput of the proposed selection
strategy is comparable to that yielded by a centralized selection approach that
relies on geographic information."
"In this paper we consider the class of anti-uniform Huffman codes and derive
tight lower and upper bounds on the average length, entropy, and redundancy of
such codes in terms of the alphabet size of the source. The Fibonacci
distributions are introduced which play a fundamental role in AUH codes. It is
shown that such distributions maximize the average length and the entropy of
the code for a given alphabet size. Another previously known bound on the
entropy for given average length follows immediately from our results."
"Reliable communication over the discrete-input/continuous-output noncoherent
multiple-input multiple-output (MIMO) Rayleigh block fading channel is
considered when the signal-to-noise ratio (SNR) per degree of freedom is low.
Two key problems are posed and solved to obtain the optimum discrete input. In
both problems, the average and peak power per space-time slot of the input
constellation are constrained. In the first one, the peak power to average
power ratio (PPAPR) of the input constellation is held fixed, while in the
second problem, the peak power is fixed independently of the average power. In
the first PPAPR-constrained problem, the mutual information, which grows as
O(SNR^2), is maximized up to second order in SNR. In the second
peak-constrained problem, where the mutual information behaves as O(SNR), the
structure of constellations that are optimal up to first order, or
equivalently, that minimize energy/bit, are explicitly characterized.
Furthermore, among constellations that are first-order optimal, those that
maximize the mutual information up to second order, or equivalently, the
wideband slope, are characterized. In both PPAPR-constrained and
peak-constrained problems, the optimal constellations are obtained in
closed-form as solutions to non-convex optimizations, and interestingly, they
are found to be identical. Due to its special structure, the common solution is
referred to as Space Time Orthogonal Rank one Modulation, or STORM. In both
problems, it is seen that STORM provides a sharp characterization of the
behavior of noncoherent MIMO capacity."
"This paper considers the problem of secret communication over a multiple
access channel with generalized feedback. Two trusted users send independent
confidential messages to an intended receiver, in the presence of a passive
eavesdropper. In this setting, an active cooperation between two trusted users
is enabled through using channel feedback in order to improve the communication
efficiency. Based on rate-splitting and decode-and-forward strategies,
achievable secrecy rate regions are derived for both discrete memoryless and
Gaussian channels. Results show that channel feedback improves the achievable
secrecy rates."
"We consider a wireless sensors network scenario where two nodes detect
correlated sources and deliver them to a central collector via a wireless link.
Differently from the Slepian-Wolf approach to distributed source coding, in the
proposed scenario the sensing nodes do not perform any pre-compression of the
sensed data. Original data are instead independently encoded by means of
low-complexity convolutional codes. The decoder performs joint decoding with
the aim of exploiting the inherent correlation between the transmitted sources.
Complexity at the decoder is kept low thanks to the use of an iterative joint
decoding scheme, where the output of each decoder is fed to the other decoder's
input as a-priori information. For such scheme, we derive a novel analytical
framework for evaluating an upper bound of joint-detection packet error
probability and for deriving the optimum coding scheme. Experimental results
confirm the validity of the analytical framework, and show that recursive codes
allow a noticeable performance gain with respect to non-recursive coding
schemes. Moreover, the proposed recursive coding scheme allows to approach the
ideal Slepian-Wolf scheme performance in AWGN channel, and to clearly
outperform it over fading channels on account of diversity gain due to
correlation of information."
"Opportunistic scheduling and beamforming schemes are proposed for multiuser
MIMO-SDMA downlink systems with linear combining in this work. Signals received
from all antennas of each mobile terminal (MT) are linearly combined to improve
the {\em effective} signal-to-noise-interference ratios (SINRs). By exploiting
limited feedback on the effective SINRs, the base station (BS) schedules
simultaneous data transmission on multiple beams to the MTs with the largest
effective SINRs. Utilizing the extreme value theory, we derive the asymptotic
system throughputs and scaling laws for the proposed scheduling and beamforming
schemes with different linear combining techniques. Computer simulations
confirm that the proposed schemes can substantially improve the system
throughput."
"Performance of cooperative diversity schemes at Low Signal to Noise Ratios
(LSNR) was recently studied by Avestimehr et. al. [1] who emphasized the
importance of diversity gain over multiplexing gain at low SNRs. It has also
been pointed out that continuous energy transfer to the channel is necessary
for achieving the max-flow min-cut bound at LSNR. Motivated by this we propose
the use of Selection Decode and Forward (SDF) at LSNR and analyze its
performance in terms of the outage probability. We also propose an energy
optimization scheme which further brings down the outage probability."
"A generalized sampling theorem for frequency localized signals is presented.
The generalization in the proposed model of sampling is twofold: (1) It applies
to various prefilters effecting a ""soft"" bandlimitation, (2) an approximate
reconstruction from sample values rather than a perfect one is obtained (though
the former might be ""practically perfect"" in many cases). For an arbitrary
finite-energy signal the frequency localization is performed by a prefilter
realizing a crosscorrelation with a function of prescribed properties. The
range of the filter, the so-called localization space, is described in some
detail. Regular sampling is applied and a reconstruction formula is given. For
the reconstruction error a general error estimate is derived and connections
between a critical sampling interval and notions of ""soft bandwidth"" for the
prefilter are indicated. Examples based on the sinc-function, Gaussian
functions and B-splines are discussed."
"While the best known outerbound for the K user interference channel states
that there cannot be more than K/2 degrees of freedom, it has been conjectured
that in general the constant interference channel with any number of users has
only one degree of freedom. In this paper, we explore the spatial degrees of
freedom per orthogonal time and frequency dimension for the K user wireless
interference channel where the channel coefficients take distinct values across
frequency slots but are fixed in time. We answer five closely related
questions. First, we show that K/2 degrees of freedom can be achieved by
channel design, i.e. if the nodes are allowed to choose the best constant,
finite and nonzero channel coefficient values. Second, we show that if channel
coefficients can not be controlled by the nodes but are selected by nature,
i.e., randomly drawn from a continuous distribution, the total number of
spatial degrees of freedom for the K user interference channel is almost surely
K/2 per orthogonal time and frequency dimension. Thus, only half the spatial
degrees of freedom are lost due to distributed processing of transmitted and
received signals on the interference channel. Third, we show that interference
alignment and zero forcing suffice to achieve all the degrees of freedom in all
cases. Fourth, we show that the degrees of freedom $D$ directly lead to an
$\mathcal{O}(1)$ capacity characterization of the form
$C(SNR)=D\log(1+SNR)+\mathcal{O}(1)$ for the multiple access channel, the
broadcast channel, the 2 user interference channel, the 2 user MIMO X channel
and the 3 user interference channel with M>1 antennas at each node. Fifth, we
characterize the degree of freedom benefits from cognitive sharing of messages
on the 3 user interference channel."
"Without assuming any knowledge on source's codebook and its output signals,
we formulate a Gaussian jamming problem in block fading channels as a
two-player zero sum game. The outage probability is adopted as an objective
function, over which transmitter aims at minimization and jammer aims at
maximization by selecting their power control strategies. Optimal power control
strategies for each player are obtained under both short-term and long-term
power constraints. For the latter case, we first prove the non-existence of a
Nash equilibrium, and then provide a complete solution for both maxmin and
minimax problems. Numerical results demonstrate a sharp difference between the
outage probabilities of the minimax and maxmin solutions."
"Multiple carrier-frequency offsets (CFO) arise in a distributed antenna
system, where data are transmitted simultaneously from multiple antennas. In
such systems the received signal contains multiple CFOs due to mismatch between
the local oscillators of transmitters and receiver. This results in a
time-varying rotation of the data constellation, which needs to be compensated
for at the receiver before symbol recovery. This paper proposes a new approach
for blind CFO estimation and symbol recovery. The received base-band signal is
over-sampled, and its polyphase components are used to formulate a virtual
Multiple-Input Multiple-Output (MIMO) problem. By applying blind MIMO system
estimation techniques, the system response is estimated and used to
subsequently transform the multiple CFOs estimation problem into many
independent single CFO estimation problems. Furthermore, an initial estimate of
the CFO is obtained from the phase of the MIMO system response. The Cramer-Rao
Lower bound is also derived, and the large sample performance of the proposed
estimator is compared to the bound."
"We consider a new approach to power control in decentralized wireless
networks, termed fractional power control (FPC). Transmission power is chosen
as the current channel quality raised to an exponent -s, where s is a constant
between 0 and 1. The choices s = 1 and s = 0 correspond to the familiar cases
of channel inversion and constant power transmission, respectively. Choosing s
in (0,1) allows all intermediate policies between these two extremes to be
evaluated, and we see that usually neither extreme is ideal. We derive
closed-form approximations for the outage probability relative to a target SINR
in a decentralized (ad hoc or unlicensed) network as well as for the resulting
transmission capacity, which is the number of users/m^2 that can achieve this
SINR on average. Using these approximations, which are quite accurate over
typical system parameter values, we prove that using an exponent of 1/2
minimizes the outage probability, meaning that the inverse square root of the
channel strength is a sensible transmit power scaling for networks with a
relatively low density of interferers. We also show numerically that this
choice of s is robust to a wide range of variations in the network parameters.
Intuitively, s=1/2 balances between helping disadvantaged users while making
sure they do not flood the network with interference."
"For a state-dependent DMC with input alphabet $\mathcal{X}$ and state
alphabet $\mathcal{S}$ where the i.i.d. state sequence is known causally at the
transmitter, it is shown that by using at most
$|\mathcal{X}||\mathcal{S}|-|\mathcal{S}|+1$ out of
$|\mathcal{X}|^{|\mathcal{S}|}$ input symbols of the Shannon's
\emph{associated} channel, the capacity is achievable. As an example of
state-dependent channels with side information at the transmitter, $M$-ary
signal transmission over AWGN channel with additive $Q$-ary interference where
the sequence of i.i.d. interference symbols is known causally at the
transmitter is considered. For the special case where the Gaussian noise power
is zero, a sufficient condition, which is independent of interference, is given
for the capacity to be $\log_2 M$ bits per channel use. The problem of
maximization of the transmission rate under the constraint that the channel
input given any current interference symbol is uniformly distributed over the
channel input alphabet is investigated. For this setting, the general structure
of a communication system with optimal precoding is proposed."
"Existing works on distributed consensus explore linear iterations based on
reversible Markov chains, which contribute to the slow convergence of the
algorithms. It has been observed that by overcoming the diffusive behavior of
reversible chains, certain nonreversible chains lifted from reversible ones mix
substantially faster than the original chains. In this paper, we investigate
the idea of accelerating distributed consensus via lifting Markov chains, and
propose a class of Location-Aided Distributed Averaging (LADA) algorithms for
wireless networks, where nodes' coarse location information is used to
construct nonreversible chains that facilitate distributed computing and
cooperative processing. First, two general pseudo-algorithms are presented to
illustrate the notion of distributed averaging through chain-lifting. These
pseudo-algorithms are then respectively instantiated through one LADA algorithm
on grid networks, and one on general wireless networks. For a $k\times k$ grid
network, the proposed LADA algorithm achieves an $\epsilon$-averaging time of
$O(k\log(\epsilon^{-1}))$. Based on this algorithm, in a wireless network with
transmission range $r$, an $\epsilon$-averaging time of
$O(r^{-1}\log(\epsilon^{-1}))$ can be attained through a centralized algorithm.
Subsequently, we present a fully-distributed LADA algorithm for wireless
networks, which utilizes only the direction information of neighbors to
construct nonreversible chains. It is shown that this distributed LADA
algorithm achieves the same scaling law in averaging time as the centralized
scheme. Finally, we propose a cluster-based LADA (C-LADA) algorithm, which,
requiring no central coordination, provides the additional benefit of reduced
message complexity compared with the distributed LADA algorithm."
"We study the error probability performance of rotated lattice constellations
in frequency-flat Nakagami-$m$ block-fading channels. In particular, we use the
sphere lower bound on the underlying infinite lattice as a performance
benchmark. We show that the sphere lower bound has full diversity. We observe
that optimally rotated lattices with largest known minimum product distance
perform very close to the lower bound, while the ensemble of random rotations
is shown to lack diversity and perform far from it."
"In this paper we propose a new design criterion and a new class of unitary
signal constellations for differential space-time modulation for
multiple-antenna systems over Rayleigh flat-fading channels with unknown fading
coefficients. Extensive simulations show that the new codes have significantly
better performance than existing codes. We have compared the performance of our
codes with differential detection schemes using orthogonal design, Cayley
differential codes, fixed-point-free group codes and product of groups and for
the same bit error rate, our codes allow smaller signal to noise ratio by as
much as 10 dB. The design of the new codes is accomplished in a systematic way
through the optimization of a performance index that closely describes the bit
error rate as a function of the signal to noise ratio. The new performance
index is computationally simple and we have derived analytical expressions for
its gradient with respect to constellation parameters. Decoding of the proposed
constellations is reduced to a set of one-dimensional closest point problems
that we solve using parallel sphere decoder algorithms. This decoding strategy
can also improve efficiency of existing codes."
"Cognitive radios sense the radio spectrum in order to find unused frequency
bands and use them in an agile manner. Transmission by the primary user must be
detected reliably even in the low signal-to-noise ratio (SNR) regime and in the
face of shadowing and fading. Communication signals are typically
cyclostationary, and have many periodic statistical properties related to the
symbol rate, the coding and modulation schemes as well as the guard periods,
for example. These properties can be exploited in designing a detector, and for
distinguishing between the primary and secondary users' signals. In this paper,
a generalized likelihood ratio test (GLRT) for detecting the presence of
cyclostationarity using multiple cyclic frequencies is proposed. Distributed
decision making is employed by combining the quantized local test statistics
from many secondary users. User cooperation allows for mitigating the effects
of shadowing and provides a larger footprint for the cognitive radio system.
Simulation examples demonstrate the resulting performance gains in the low SNR
regime and the benefits of cooperative detection."
"As a basic information-theoretic model for fading relay channels, the
parallel relay channel is first studied, for which lower and upper bounds on
the capacity are derived. For the parallel relay channel with degraded
subchannels, the capacity is established, and is further demonstrated via the
Gaussian case, for which the synchronized and asynchronized capacities are
obtained. The capacity achieving power allocation at the source and relay nodes
among the subchannels is characterized. The fading relay channel is then
studied, for which resource allocations that maximize the achievable rates are
obtained for both the full-duplex and half-duplex cases. Capacities are
established for fading relay channels that satisfy certain conditions."
"We develop and analyze new cooperative strategies for ad hoc networks that
are more spectrally efficient than classical DF cooperative protocols. Using
analog network coding, our strategies preserve the practical half-duplex
assumption but relax the orthogonality constraint. The introduction of
interference due to non-orthogonality is mitigated thanks to precoding, in
particular Dirty Paper coding. Combined with smart power allocation, our
cooperation strategies allow to save time and lead to more efficient use of
bandwidth and to improved network throughput with respect to classical RDF/PDF."
"We study the high-power asymptotic behavior of the sum-rate capacity of
multi-user interference networks with an equal number of transmitters and
receivers. We assume that each transmitter is cognizant of the message it
wishes to convey to its corresponding receiver and also of the messages that a
subset of the other transmitters wish to send. The receivers are assumed not to
be able to cooperate in any way so that they must base their decision on the
signal they receive only. We focus on the network's pre-log, which is defined
as the limiting ratio of the sum-rate capacity to half the logarithm of the
transmitted power. We present both upper and lower bounds on the network's
pre-log. The lower bounds are based on a linear partial-cancellation scheme
which entails linearly transforming Gaussian codebooks so as to eliminate the
interference in a subset of the receivers. Inter alias, the bounds give a
complete characterization of the networks and side-information settings that
result in a full pre-log, i.e., in a pre-log that is equal to the number of
transmitters (and receivers) as well as a complete characterization of networks
whose pre-log is equal to the full pre-log minus one. They also fully
characterize networks where the full pre-log can only be achieved if each
transmitter knows the messages of all users, i.e., when the side-information is
""full""."
"The new method for Reed-Solomon codes decoding is introduced. The method is
based on the star trellis decoding of the binary image of Reed-Solomon codes."
"The problem of sampling a discrete-time sequence of spatially bandlimited
fields with a bounded dynamic range, in a distributed,
communication-constrained, processing environment is addressed. A central unit,
having access to the data gathered by a dense network of fixed-precision
sensors, operating under stringent inter-node communication constraints, is
required to reconstruct the field snapshots to maximum accuracy. Both
deterministic and stochastic field models are considered. For stochastic
fields, results are established in the almost-sure sense. The feasibility of
having a flexible tradeoff between the oversampling rate (sensor density) and
the analog-to-digital converter (ADC) precision, while achieving an exponential
accuracy in the number of bits per Nyquist-interval per snapshot is
demonstrated. This exposes an underlying ``conservation of bits'' principle:
the bit-budget per Nyquist-interval per snapshot (the rate) can be distributed
along the amplitude axis (sensor-precision) and space (sensor density) in an
almost arbitrary discrete-valued manner, while retaining the same (exponential)
distortion-rate characteristics. Achievable information scaling laws for field
reconstruction over a bounded region are also derived: With N one-bit sensors
per Nyquist-interval, $\Theta(\log N)$ Nyquist-intervals, and total network
bitrate $R_{net} = \Theta((\log N)^2)$ (per-sensor bitrate $\Theta((\log
N)/N)$), the maximum pointwise distortion goes to zero as $D = O((\log N)^2/N)$
or $D = O(R_{net} 2^{-\beta \sqrt{R_{net}}})$. This is shown to be possible
with only nearest-neighbor communication, distributed coding, and appropriate
interpolation algorithms. For a fixed, nonzero target distortion, the number of
fixed-precision sensors and the network rate needed is always finite."
"In wireless relay networks, noise at the relays can be correlated possibly
due to common interference or noise propagation from preceding hops. In this
work we consider a parallel relay network with noise correlation. For the relay
strategy of amplify-and-forward (AF), we determine the optimal rate maximizing
relay gains when correlation knowledge is available at the relays. The effect
of correlation on the performance of the relay networks is analyzed for the
cases where full knowledge of correlation is available at the relays and when
there is no knowledge about the correlation structure. Interestingly we find
that, on the average, noise correlation is beneficial regardless of whether the
relays know the noise covariance matrix or not. However, the knowledge of
correlation can greatly improve the performance. Typically, the performance
improvement from correlation knowledge increases with the relay power and the
number of relays. With perfect correlation knowledge the system is capable of
canceling interference if the number of interferers is less than the number of
relays.
  For a dual-hop multiple access parallel network, we obtain closed form
expressions for the maximum sum-rate and the optimal relay strategy. The relay
optimization for networks with three hops is also considered. For any relay
gains for the first stage relays, this represents a parallel relay network with
correlated noise. Based on the result of two hop networks with noise
correlation, we propose an algorithm for solving the relay optimization problem
for three-hop networks."
"We are concerned with the problem of maximizing the worst-case lifetime of a
data-gathering wireless sensor network consisting of a set of sensor nodes
directly communicating with a base-station.We propose to solve this problem by
modeling sensor node and base-station communication as the interactive
communication between multiple correlated informants (sensor nodes) and a
recipient (base-station). We provide practical and scalable interactive
communication protocols for data gathering in sensor networks and demonstrate
their efficiency compared to traditional approaches.
  In this paper, we first develop a formalism to address the problem of
worst-case interactive communication between a set of multiple correlated
informants and a recipient. We realize that there can be different objectives
to achieve in such a communication scenario and compute the optimal number of
messages and bits exchanged to realize these objectives. Then, we propose to
adapt these results in the context of single-hop data-gathering sensor
networks. Finally, based on this proposed formalism, we propose a clustering
based communication protocol for large sensor networks and demonstrate its
superiority over a traditional clustering protocol."
"We propose an approximation of maximum-likelihood detection in ISI channels
based on linear programming or message passing. We convert the detection
problem into a binary decoding problem, which can be easily combined with LDPC
decoding. We show that, for a certain class of channels and in the absence of
coding, the proposed technique provides the exact ML solution without an
exponential complexity in the size of channel memory, while for some other
channels, this method has a non-diminishing probability of failure as SNR
increases. Some analysis is provided for the error events of the proposed
technique under linear programming."
"The fading broadcast channel with confidential messages (BCC) is
investigated, where a source node has common information for two receivers
(receivers 1 and 2), and has confidential information intended only for
receiver 1. The confidential information needs to be kept as secret as possible
from receiver 2. The channel state information (CSI) is assumed to be known at
both the transmitter and the receivers. The secrecy capacity region is first
established for the parallel Gaussian BCC, and the optimal source power
allocations that achieve the boundary of the secrecy capacity region are
derived. In particular, the secrecy capacity region is established for the
Gaussian case of the Csiszar-Korner BCC model. The secrecy capacity results are
then applied to give the ergodic secrecy capacity region for the fading BCC."
"In this correspondence, we propose a tight lower bound to the outage
probability of discrete-input Nakagami-m block-fading channels. The approach
permits an efficient method for numerical evaluation of the bound, providing an
additional tool for system design. The optimal rate-diversity trade-off for the
Nakagami-m block-fading channel is also derived and a tight upper bound is
obtained for the optimal coding gain constant."
"In a frequency selective slow-fading channel in a MIMO system, the channel
matrix is of the form of a block matrix. We propose a method to calculate the
limit of the eigenvalue distribution of block matrices if the size of the
blocks tends to infinity. We will also calculate the asymptotic eigenvalue
distribution of $HH^*$, where the entries of $H$ are jointly Gaussian, with a
correlation of the form $E[h_{pj}\bar h_{qk}]= \sum_{s=1}^t
\Psi^{(s)}_{jk}\hat\Psi^{(s)}_{pq}$ (where $t$ is fixed and does not increase
with the size of the matrix). We will use an operator-valued free probability
approach to achieve this goal. Using this method, we derive a system of
equations, which can be solved numerically to compute the desired eigenvalue
distribution."
"After receiving useful peer comments, we would like to withdraw this paper."
"A network of n communication links, operating over a shared wireless channel,
is considered. Fading is assumed to be the dominant factor affecting the
strength of the channels between transmitter and receiver terminals. It is
assumed that each link can be active and transmit with a constant power P or
remain silent. The objective is to maximize the throughput over the selection
of active links. By deriving an upper bound and a lower bound, it is shown that
in the case of Rayleigh fading (i) the maximum throughput scales like $\log n$
(ii) the maximum throughput is achievable in a distributed fashion. The upper
bound is obtained using probabilistic methods, where the key point is to upper
bound the throughput of any random set of active links by a chi-squared random
variable. To obtain the lower bound, a decentralized link activation strategy
is proposed and analyzed."
"The error exponent of Markov channels with feedback is studied in the
variable-length block-coding setting. Burnashev's classic result is extended
and a single letter characterization for the reliability function of
finite-state Markov channels is presented, under the assumption that the
channel state is causally observed both at the transmitter and at the receiver
side. Tools from stochastic control theory are used in order to treat channels
with intersymbol interference. In particular the convex analytical approach to
Markov decision processes is adopted to handle problems with stopping time
horizons arising from variable-length coding schemes."
"Unitary space-time modulation is known to be an efficient means to
communicate over non-coherent Multiple Input Multiple Output (MIMO) channels.
In this letter, differential unitary space-time coding and non-coherent
space-time coding for the training based approach of Kim and Tarokh are
addressed. For this approach, necessary and sufficient conditions for
multi-group decodability are derived in a simple way assuming a Generalized
Likelihood Ratio Test receiver and a unitary codebook. Extending Kim and
Tarokh's approach for colocated MIMO systems, a novel training based approach
to distributed non-coherent space-time coding for wireless relay networks is
proposed. An explicit construction of two-group decodable distributed
non-coherent space-time codes achieving full cooperative diversity for all even
number of relays is provided."
"This paper re-examines the well-known fundamental tradeoffs between rate and
reliability for the multi-antenna, block Rayleigh fading channel in the high
signal to noise ratio (SNR) regime when (i) the transmitter has access to
(noiseless) one bit per coherence-interval of causal channel state information
(CSI) and (ii) soft decoding delays together with worst-case delay guarantees
are acceptable. A key finding of this work is that substantial improvements in
reliability can be realized with a very short expected delay and a slightly
longer (but bounded) worst-case decoding delay guarantee in communication
systems where the transmitter has access to even one bit per coherence interval
of causal CSI. While similar in spirit to the recent work on communication
systems based on automatic repeat requests (ARQ) where decoding failure is
known at the transmitter and leads to re-transmission, here transmit
side-information is purely based on CSI. The findings reported here also lend
further support to an emerging understanding that decoding delay (related to
throughput) and codeword blocklength (related to coding complexity and delays)
are distinctly different design parameters which can be tuned to control
reliability."
"We focus on the statistics of word occurrences and of the waiting times
between such occurrences in Blogs. Due to the heterogeneity of words'
frequencies, the empirical analysis is performed by studying classes of
""frequently-equivalent"" words, i.e. by grouping words depending on their
frequencies. Two limiting cases are considered: the dilute limit, i.e. for
those words that are used less than once a day, and the dense limit for
frequent words. In both cases, extreme events occur more frequently than
expected from the Poisson hypothesis. These deviations from Poisson statistics
reveal non-trivial time correlations between events that are associated with
bursts of activities. The distribution of waiting times is shown to behave like
a stretched exponential and to have the same shape for different sets of words
sharing a common frequency, thereby revealing universal features."
"In this thesis, we construct and analyze multiple-description codes based on
lattice vector quantization."
"Link adaptation, in particular adaptive coded modulation (ACM), is a
promising tool for bandwidth-efficient transmission in a fading environment.
The main motivation behind employing ACM schemes is to improve the spectral
efficiency of wireless communication systems. In this paper, using a finite
number of capacity achieving component codes, we propose new transmission
schemes employing constant power transmission, as well as discrete and
continuous power adaptation, for slowly varying flat-fading channels.
  We show that the proposed transmission schemes can achieve throughputs close
to the Shannon limits of flat-fading channels using only a small number of
codes. Specifically, using a fully discrete scheme with just four codes, each
associated with four power levels, we achieve a spectral efficiency within 1 dB
of the continuous-rate continuous-power Shannon capacity. Furthermore, when
restricted to a fixed number of codes, the introduction of power adaptation has
significant gains with respect to ASE and probability of no transmission
compared to a constant power scheme."
"The main focus of space-time coding design and analysis for MIMO systems has
been so far focused on single-user systems. For single-user systems, transmit
diversity schemes suffer a loss in spectral efficiency if the receiver is
equipped with more than one antenna, making them unsuitable for high rate
transmission. One such transmit diversity scheme is the cyclic delay diversity
code (CDD). The advantage of CDD over other diversity schemes such as
orthogonal space-time block codes (OSTBC) is that a code rate of one and delay
optimality are achieved independent of the number of transmit antennas. In this
work we analyze the ergodic rate of a multi-user multiple access channel (MAC)
with each user applying such a cyclic delay diversity (CDD) code. We derive
closed form expressions for the ergodic sum-rate of multi-user CDD and compare
it with the sum-capacity. We study the ergodic rate region and show that in
contrast to what is conventionally known regarding the single-user case,
transmit diversity schemes are viable candidates for high rate transmission in
multi-user systems. Finally, our theoretical findings are illustrated by
numerical simulation results."
"This paper considers the design of a minimax test for two hypotheses where
the actual probability densities of the observations are located in
neighborhoods obtained by placing a bound on the relative entropy between
actual and nominal densities. The minimax problem admits a saddle point which
is characterized. The robust test applies a nonlinear transformation which
flattens the nominal likelihood ratio in the vicinity of one. Results are
illustrated by considering the transmission of binary data in the presence of
additive noise."
"In this paper, we present the results from over-the-air experiments of a
complete implementation of an amplify and forward cooperative communications
system. Our custom OFDM-based physical layer uses a distributed version of the
Alamouti block code, where the relay sends one branch of Alamouti encoded
symbols. First we show analytically and experimentally that amplify and forward
protocols are unaffected by carrier frequency offsets at the relay. This result
allows us to use a conventional Alamouti receiver without change for the
distributed relay system. Our full system implementation shows gains up to
5.5dB in peak power constrained networks. Thus, we can conclusively state that
even the simplest form of relaying can lead to significant gains in practical
implementations."
"In many channel measurement applications, one needs to estimate some
characteristics of the channels based on a limited set of measurements. This is
mainly due to the highly time varying characteristics of the channel. In this
contribution, it will be shown how free probability can be used for channel
capacity estimation in MIMO systems. Free probability has already been applied
in various application fields such as digital communications, nuclear physics
and mathematical finance, and has been shown to be an invaluable tool for
describing the asymptotic behaviour of many large-dimensional systems. In
particular, using the concept of free deconvolution, we provide an
asymptotically (w.r.t. the number of observations) unbiased capacity estimator
for MIMO channels impaired with noise called the free probability based
estimator. Another estimator, called the Gaussian matrix mean based estimator,
is also introduced by slightly modifying the free probability based estimator.
This estimator is shown to give unbiased estimation of the moments of the
channel matrix for any number of observations. Also, the estimator has this
property when we extend to MIMO channels with phase off-set and frequency
drift, for which no estimator has been provided so far in the literature. It is
also shown that both the free probability based and the Gaussian matrix mean
based estimator are asymptotically unbiased capacity estimators as the number
of transmit antennas go to infinity, regardless of whether phase off-set and
frequency drift are present. The limitations in the two estimators are also
explained. Simulations are run to assess the performance of the estimators for
a low number of antennas and samples to confirm the usefulness of the
asymptotic results."
"It is well known, that the Alamouti scheme is the only space-time code from
orthogonal design achieving the capacity of a multiple-input multiple-output
(MIMO) wireless communication system with n_T=2 transmit antennas and n_R=1
receive antenna. In this work, we propose the n-times stacked Alamouti scheme
for n_T=2n transmit antennas and show that this scheme achieves the capacity in
the case of n_R=1 receive antenna. This result may regarded as an extension of
the Alamouti case. For the more general case of more than one receive antenna,
we show that if the number of transmit antennas is higher than the number of
receive antennas we achieve a high portion of the capacity with this scheme.
Further, we show that the MIMO capacity is at most twice the rate achieved with
the proposed scheme for all SNR. We derive lower and upper bounds for the rate
achieved with this scheme and compare it with upper and lower bounds for the
capacity. In addition to the capacity analysis based on the assumption of a
coherent channel, we analyze the error rate performance of the stacked OSTBC
with the optimal ML detector and with the suboptimal lattice-reduction (LR)
aided zero-forcing detector. We compare the error rate performance of the
stacked OSTBC with spatial multiplexing (SM) and full-diversity achieving
schemes. Finally, we illustrate the theoretical results by numerical
simulations."
"The problem of decentralized sequential detection with conditionally
independent observations is studied. The sensors form a star topology with a
central node called fusion center as the hub. The sensors make noisy
observations of a parameter that changes from an initial state to a final state
at a random time where the random change time has a geometric distribution. The
sensors amplify and forward the observations over a wireless Gaussian multiple
access channel and operate under either a power constraint or an energy
constraint. The optimal transmission strategy at each stage is shown to be the
one that maximizes a certain Ali-Silvey distance between the distributions for
the hypotheses before and after the change. Simulations demonstrate that the
proposed analog technique has lower detection delays when compared with
existing schemes. Simulations further demonstrate that the energy-constrained
formulation enables better use of the total available energy than the
power-constrained formulation in the change detection problem."
"Consider a pair of correlated Gaussian sources (X1,X2). Two separate encoders
observe the two components and communicate compressed versions of their
observations to a common decoder. The decoder is interested in reconstructing a
linear combination of X1 and X2 to within a mean-square distortion of D. We
obtain an inner bound to the optimal rate-distortion region for this problem. A
portion of this inner bound is achieved by a scheme that reconstructs the
linear function directly rather than reconstructing the individual components
X1 and X2 first. This results in a better rate region for certain parameter
values. Our coding scheme relies on lattice coding techniques in contrast to
more prevalent random coding arguments used to demonstrate achievable rate
regions in information theory. We then consider the case of linear
reconstruction of K sources and provide an inner bound to the optimal
rate-distortion region. Some parts of the inner bound are achieved using the
following coding structure: lattice vector quantization followed by
""correlated"" lattice-structured binning."
"This paper addresses three issues in the field of ad hoc network capacity:
the impact of i)channel fading, ii) channel inversion power control, and iii)
threshold-based scheduling on capacity. Channel inversion and threshold
scheduling may be viewed as simple ways to exploit channel state information
(CSI) without requiring cooperation across transmitters. We use the
transmission capacity (TC) as our metric, defined as the maximum spatial
intensity of successful simultaneous transmissions subject to a constraint on
the outage probability (OP). By assuming the nodes are located on the infinite
plane according to a Poisson process, we are able to employ tools from
stochastic geometry to obtain asymptotically tight bounds on the distribution
of the signal-to-interference (SIR) level, yielding in turn tight bounds on the
OP (relative to a given SIR threshold) and the TC. We demonstrate that in the
absence of CSI, fading can significantly reduce the TC and somewhat
surprisingly, channel inversion only makes matters worse. We develop a
threshold-based transmission rule where transmitters are active only if the
channel to their receiver is acceptably strong, obtain expressions for the
optimal threshold, and show that this simple, fully distributed scheme can
significantly reduce the effect of fading."
"Given two binary codes of length n, using Plotkin construction we obtain a
code of length 2n. The construction works for linear and nonlinear codes. For
the linear case, it is straightforward to see that the dimension of the final
code is the sum of the dimensions of the starting codes. For nonlinear codes,
the rank and the dimension of the kernel are standard mesures of linearity. In
this report, we prove that both parameters are also the sum of the
corresponding ones of the starting codes."
"Bliss schemes of a run length limited (RLL) codec in combination with an LDPC
codec, generate LDPC parity bits over a systematic sequence of RLL channel bits
that are inherently redundant as they satisfy e.g. a $d=1$ minimum run length
constraint. That is the subsequences consisting of runs of length $d=1$, viz.
$...010...$ and $...101...$, cannot occur. We propose to use this redundancy
during LDPC decoding in a Bliss scheme by introducing additional $d$-constraint
nodes in the factor graph used by the LDPC decoder. The messages sent from
these new nodes to the variable or codeword bit nodes exert a ``force'' on the
resulting soft-bit vector coming out of the LDPC decoding that give it a
tendency to comply with the $d$-constraints. This way, we can significantly
reduce the probability of decoding error."
"Two new rate-one full-diversity space-time block codes (STBC) are proposed.
They are characterized by the \emph{lowest decoding complexity} among the known
rate-one STBC, arising due to the complete separability of the transmitted
symbols into four groups for maximum likelihood detection. The first and the
second codes are delay-optimal if the number of transmit antennas is a power of
2 and even, respectively. The exact pair-wise error probability is derived to
allow for the performance optimization of the two codes. Compared with existing
low-decoding complexity STBC, the two new codes offer several advantages such
as higher code rate, lower encoding/decoding delay and complexity, lower
peak-to-average power ratio, and better performance."
"It is shown that subclasses of separable binary Goppa codes, $\Gamma(L,G)$ -
codes, with $L=\{\alpha \in GF(2^{2l}):G(\alpha)\neq 0 \}$ and special Goppa
polynomials G(x) can be presented as a chain of embedded codes. The true
minimal distance has been obtained for all codes of the chain."
"We consider the multiuser successive refinement (MSR) problem, where the
users are connected to a central server via links with different noiseless
capacities, and each user wishes to reconstruct in a successive-refinement
fashion. An achievable region is given for the two-user two-layer case and it
provides the complete rate-distortion region for the Gaussian source under the
MSE distortion measure. The key observation is that this problem includes the
multiple description (MD) problem (with two descriptions) as a subsystem, and
the techniques useful in the MD problem can be extended to this case. We show
that the coding scheme based on the universality of random binning is
sub-optimal, because multiple Gaussian side informations only at the decoders
do incur performance loss, in contrast to the case of single side information
at the decoder. We further show that unlike the single user case, when there
are multiple users, the loss of performance by a multistage coding approach can
be unbounded for the Gaussian source. The result suggests that in such a
setting, the benefit of using successive refinement is not likely to justify
the accompanying performance loss. The MSR problem is also related to the
source coding problem where each decoder has its individual side information,
while the encoder has the complete set of the side informations. The MSR
problem further includes several variations of the MD problem, for which the
specialization of the general result is investigated and the implication is
discussed."
"Designs for transmit alphabet constrained space-time codes naturally lead to
questions about the design of rank distance codes. Recently, diversity embedded
multi-level space-time codes for flat fading channels have been designed from
sets of binary matrices with rank distance guarantees over the binary field by
mapping them onto QAM and PSK constellations. In this paper we demonstrate that
diversity embedded space-time codes for fading Inter-Symbol Interference (ISI)
channels can be designed with provable rank distance guarantees. As a corollary
we obtain an asymptotic characterization of the fixed transmit alphabet
rate-diversity trade-off for multiple antenna fading ISI channels. The key idea
is to construct and analyze properties of binary matrices with a particular
structure induced by ISI channels."
"We develop a framework for linear-programming (LP) decoding of non-binary
linear codes over rings. We prove that the resulting LP decoder has the
`maximum likelihood certificate' property, and we show that the decoder output
is the lowest cost pseudocodeword. Equivalence between pseudocodewords of the
linear program and pseudocodewords of graph covers is proved. LP decoding
performance is illustrated for the (11,6,5) ternary Golay code with ternary PSK
modulation over AWGN, and in this case it is shown that the LP decoder
performance is comparable to codeword-error-rate-optimum hard-decision based
decoding."
"Universally achievable error exponents pertaining to certain families of
channels (most notably, discrete memoryless channels (DMC's)), and various
ensembles of random codes, are studied by combining the competitive minimax
approach, proposed by Feder and Merhav, with Chernoff bound and Gallager's
techniques for the analysis of error exponents. In particular, we derive a
single--letter expression for the largest, universally achievable fraction
$\xi$ of the optimum error exponent pertaining to the optimum ML decoding.
Moreover, a simpler single--letter expression for a lower bound to $\xi$ is
presented. To demonstrate the tightness of this lower bound, we use it to show
that $\xi=1$, for the binary symmetric channel (BSC), when the random coding
distribution is uniform over: (i) all codes (of a given rate), and (ii) all
linear codes, in agreement with well--known results. We also show that $\xi=1$
for the uniform ensemble of systematic linear codes, and for that of
time--varying convolutional codes in the bit-error--rate sense. For the latter
case, we also show how the corresponding universal decoder can be efficiently
implemented using a slightly modified version of the Viterbi algorithm which em
employs two trellises."
"This paper derives a lower bound to the per-node throughput achievable by a
wireless network when n source-destination pairs are randomly distributed
throughout a disk of radius $n^\gamma$, $ \gamma \geq 0$, propagation is
modeled by attenuation of the form $1/(1+d)^\alpha$, $\alpha >2$, and
successful transmission occurs at a fixed rate W when received signal to noise
and interference ratio is greater than some threshold $\beta$, and at rate 0
otherwise. The lower bound has the form $n^{1-\gamma}$ when $\gamma < 1/2$, and
$(n \ln n)^{-1/2}$ when $\gamma \geq 1/2$. The methods are similar to, but
somewhat simpler than, those in the seminal paper by Gupta and Kumar."
"The problem of side-information scalable (SI-scalable) source coding is
considered in this work, where the encoder constructs a progressive
description, such that the receiver with high quality side information will be
able to truncate the bitstream and reconstruct in the rate distortion sense,
while the receiver with low quality side information will have to receive
further data in order to decode. We provide inner and outer bounds for general
discrete memoryless sources. The achievable region is shown to be tight for the
case that either of the decoders requires a lossless reconstruction, as well as
the case with degraded deterministic distortion measures. Furthermore we show
that the gap between the achievable region and the outer bounds can be bounded
by a constant when square error distortion measure is used. The notion of
perfectly scalable coding is introduced as both the stages operate on the
Wyner-Ziv bound, and necessary and sufficient conditions are given for sources
satisfying a mild support condition. Using SI-scalable coding and successive
refinement Wyner-Ziv coding as basic building blocks, a complete
characterization is provided for the important quadratic Gaussian source with
multiple jointly Gaussian side-informations, where the side information quality
does not have to be monotonic along the scalable coding order. Partial result
is provided for the doubly symmetric binary source with Hamming distortion when
the worse side information is a constant, for which one of the outer bound is
strictly tighter than the other one."
"We consider asynchronous communication over point-to-point discrete
memoryless channels. The transmitter starts sending one block codeword at an
instant that is uniformly distributed within a certain time period, which
represents the level of asynchronism. The receiver, by means of a sequential
decoder, must isolate the message without knowing when the codeword
transmission starts but being cognizant of the asynchronism level A. We are
interested in how quickly can the receiver isolate the sent message,
particularly in the regime where A is exponentially larger than the codeword
length N, which we refer to as `strong asynchronism.'
  This model of sparse communication may represent the situation of a sensor
that remains idle most of the time and, only occasionally, transmits
information to a remote base station which needs to quickly take action.
  The first result shows that vanishing error probability can be guaranteed as
N tends to infinity while A grows as Exp(N*k) if and only if k does not exceed
the `synchronization threshold,' a constant that admits a simple closed form
expression, and is at least as large as the capacity of the synchronized
channel. The second result is the characterization of a set of achievable
strictly positive rates in the regime where A is exponential in N, and where
the rate is defined with respect to the expected delay between the time
information starts being emitted until the time the receiver makes a decision.
  As an application of the first result we consider antipodal signaling over a
Gaussian channel and derive a simple necessary condition between A, N, and SNR
for achieving reliable communication."
"This paper derives a \emph{distributed} Kalman filter to estimate a sparsely
connected, large-scale, $n-$dimensional, dynamical system monitored by a
network of $N$ sensors. Local Kalman filters are implemented on the
($n_l-$dimensional, where $n_l\ll n$) sub-systems that are obtained after
spatially decomposing the large-scale system. The resulting sub-systems
overlap, which along with an assimilation procedure on the local Kalman
filters, preserve an $L$th order Gauss-Markovian structure of the centralized
error processes. The information loss due to the $L$th order Gauss-Markovian
approximation is controllable as it can be characterized by a divergence that
decreases as $L\uparrow$. The order of the approximation, $L$, leads to a lower
bound on the dimension of the sub-systems, hence, providing a criterion for
sub-system selection. The assimilation procedure is carried out on the local
error covariances with a distributed iterate collapse inversion (DICI)
algorithm that we introduce. The DICI algorithm computes the (approximated)
centralized Riccati and Lyapunov equations iteratively with only local
communication and low-order computation. We fuse the observations that are
common among the local Kalman filters using bipartite fusion graphs and
consensus averaging algorithms. The proposed algorithm achieves full
distribution of the Kalman filter that is coherent with the centralized Kalman
filter with an $L$th order Gaussian-Markovian structure on the centralized
error processes. Nowhere storage, communication, or computation of
$n-$dimensional vectors and matrices is needed; only $n_l \ll n$ dimensional
vectors and matrices are communicated or used in the computation at the
sensors."
"The capacity region of the Finite-State Multiple Access Channel (FS-MAC) with
feedback that may be an arbitrary time-invariant function of the channel output
samples is considered. We characterize both an inner and an outer bound for
this region, using Masseys's directed information. These bounds are shown to
coincide, and hence yield the capacity region, of FS-MACs where the state
process is stationary and ergodic and not affected by the inputs.
  Though `multi-letter' in general, our results yield explicit conclusions when
applied to specific scenarios of interest. E.g., our results allow us to:
  - Identify a large class of FS-MACs, that includes the additive mod-2 noise
MAC where the noise may have memory, for which feedback does not enlarge the
capacity region.
  - Deduce that, for a general FS-MAC with states that are not affected by the
input, if the capacity (region) without feedback is zero, then so is the
capacity (region) with feedback.
  - Deduce that the capacity region of a MAC that can be decomposed into a
`multiplexer' concatenated by a point-to-point channel (with, without, or with
partial feedback), the capacity region is given by $\sum_{m} R_m \leq C$, where
C is the capacity of the point to point channel and m indexes the encoders.
Moreover, we show that for this family of channels source-channel coding
separation holds."
"We consider slow fading relay channels with a single multi-antenna
source-destination terminal pair. The source signal arrives at the destination
via N hops through N-1 layers of relays. We analyze the diversity of such
channels with fixed network size at high SNR. In the clustered case where the
relays within the same layer can have full cooperation, the cooperative
decode-and-forward (DF) scheme is shown to be optimal in terms of the
diversity-multiplexing tradeoff (DMT). The upper bound on the DMT, the cut-set
bound, is attained. In the non-clustered case, we show that the naive
amplify-and-forward (AF) scheme has the maximum multiplexing gain of the
channel but is suboptimal in diversity, as compared to the cut-set bound. To
improve the diversity, space-time relay processing is introduced through the
parallel partition of the multihop channel. The idea is to let the source
signal go through K different ""AF paths"" in the multihop channel. This parallel
AF scheme creates a parallel channel in the time domain and has the maximum
diversity if the partition is properly designed. Since this scheme does not
achieve the maximum multiplexing gain in general, we propose a flip-and-forward
(FF) scheme that is built from the parallel AF scheme. It is shown that the FF
scheme achieves both the maximum diversity and multiplexing gains in a
distributed multihop channel of arbitrary size. In order to realize the DMT
promised by the relaying strategies, approximately universal coding schemes are
also proposed."
"Cooperative transmission can greatly improve communication system performance
by taking advantage of the broadcast nature of wireless channels. Most previous
work on resource allocation for cooperation transmission is based on
centralized control. In this paper, we propose two share auction mechanisms,
the SNR auction and the power auction, to distributively coordinate the
resource allocation among users. We prove the existence, uniqueness and
effectiveness of the auction results. In particular, the SNR auction leads to a
fair resource allocation among users, and the power auction achieves a solution
that is close to the efficient allocation."
"Via collaborative beamforming, nodes in a wireless network are able to
transmit a common message over long distances in an energy efficient fashion.
However, the process of making available the same message to all collaborating
nodes introduces delays. In this paper, a MAC-PHY cross-layer scheme is
proposed that enables collaborative beamforming at significantly reduced
collaboration overhead. It consists of two phases. In the first phase, nodes
transmit locally in a random access time-slotted fashion. Simultaneous
transmissions from multiple source nodes are viewed as linear mixtures of all
transmitted packets. In the second phase, a set of collaborating nodes, acting
as a distributed antenna system, beamform the received analog waveform to one
or more faraway destinations. This step requires multiplication of the received
analog waveform by a complex weight, which is independently computed by each
cooperating node, and which allows packets bound to the same destination to add
coherently at the destination node. Assuming that each node has access to
location information, the proposed scheme can achieve high throughput, which in
certain cases exceeds one. An analysis of the symbol error probability
corresponding to the proposed scheme is provided."
"The partition function pertaining to finite--temperature decoding of a
(typical) randomly chosen code is known to have three types of behavior,
corresponding to three phases in the plane of rate vs. temperature: the {\it
ferromagnetic phase}, corresponding to correct decoding, the {\it paramagnetic
phase}, of complete disorder, which is dominated by exponentially many
incorrect codewords, and the {\it glassy phase} (or the condensed phase), where
the system is frozen at minimum energy and dominated by subexponentially many
incorrect codewords. We show that the statistical physics associated with the
two latter phases are intimately related to random coding exponents. In
particular, the exponent associated with the probability of correct decoding at
rates above capacity is directly related to the free energy in the glassy
phase, and the exponent associated with probability of error (the error
exponent) at rates below capacity, is strongly related to the free energy in
the paramagnetic phase. In fact, we derive alternative expressions of these
exponents in terms of the corresponding free energies, and make an attempt to
obtain some insights from these expressions. Finally, as a side result, we also
compare the phase diagram associated with a simple finite-temperature universal
decoder for discrete memoryless channels, to that of the finite--temperature
decoder that is aware of the channel statistics."
"We introduce the notion of the stopping redundancy hierarchy of a linear
block code as a measure of the trade-off between performance and complexity of
iterative decoding for the binary erasure channel. We derive lower and upper
bounds for the stopping redundancy hierarchy via Lovasz's Local Lemma and
Bonferroni-type inequalities, and specialize them for codes with cyclic
parity-check matrices. Based on the observed properties of parity-check
matrices with good stopping redundancy characteristics, we develop a novel
decoding technique, termed automorphism group decoding, that combines iterative
message passing and permutation decoding. We also present bounds on the
smallest number of permutations of an automorphism group decoder needed to
correct any set of erasures up to a prescribed size. Simulation results
demonstrate that for a large number of algebraic codes, the performance of the
new decoding method is close to that of maximum likelihood decoding."
"The necessary and sufficient condition of the channel capacity is rigorously
formulated for the N-user discrete memoryless multiple-access channel (MAC).
The essence of the formulation is to invoke an {\em elementary} MAC where sizes
of input alphabets are not greater than the size of output alphabet. The main
objective is to demonstrate that the channel capacity of an MAC is achieved by
an elementary MAC included in the original MAC. The proof is quite
straightforward by the very definition of the elementary MAC. Moreover it is
proved that the Kuhn-Tucker conditions of the elementary MAC are strictly
sufficient and obviously necessary for the channel capacity. The latter proof
requires some steps such that for the elementary MAC every solution of the
Kuhn-Tucker conditions reveals itself as local maximum on the domain of all
possible input probability distributions and then it achieves the channel
capacity. As a result, in respect of the channel capacity, the MAC in general
can be regarded as an aggregate of a finite number of elementary MAC's."
"Recently, Roth and Skachek proposed two methods for constructing nearly
maximum-distance separable (MDS) expander codes. We show that through the
simple modification of using mixed-alphabet codes derived from MDS codes as
constituent codes in their code designs, one can obtain nearly MDS codes of
significantly smaller alphabet size, albeit at the expense of a (very slight)
reduction in code rate."
"Synchronization of relay nodes is an important and critical issue in
exploiting cooperative diversity in wireless networks. In this paper, two
asynchronous cooperative diversity schemes are proposed, namely, distributed
delay diversity and asynchronous space-time coded cooperative diversity
schemes. In terms of the overall diversity-multiplexing (DM) tradeoff function,
we show that the proposed independent coding based distributed delay diversity
and asynchronous space-time coded cooperative diversity schemes achieve the
same performance as the synchronous space-time coded approach which requires an
accurate symbol-level timing synchronization to ensure signals arriving at the
destination from different relay nodes are perfectly synchronized. This
demonstrates diversity order is maintained even at the presence of asynchronism
between relay node. Moreover, when all relay nodes succeed in decoding the
source information, the asynchronous space-time coded approach is capable of
achieving better DM-tradeoff than synchronous schemes and performs equivalently
to transmitting information through a parallel fading channel as far as the
DM-tradeoff is concerned. Our results suggest the benefits of fully exploiting
the space-time degrees of freedom in multiple antenna systems by employing
asynchronous space-time codes even in a frequency flat fading channel. In
addition, it is shown asynchronous space-time coded systems are able to achieve
higher mutual information than synchronous space-time coded systems for any
finite signal-to-noise-ratio (SNR) when properly selected baseband waveforms
are employed."
"In this paper, we describe a procedure for constructing $q$--ary
$[N,3,N-2]$--MDS codes, of length $N\leq q+1$ (for $q$ odd) or $N\leq q+2$ (for
$q$ even), using a set of non--degenerate Hermitian forms in $PG(2,q^2)$."
"We address the connection between the multiple-description (MD) problem and
Delta-Sigma quantization. The inherent redundancy due to oversampling in
Delta-Sigma quantization, and the simple linear-additive noise model resulting
from dithered lattice quantization, allow us to construct a symmetric and
time-invariant MD coding scheme. We show that the use of a noise shaping filter
makes it possible to trade off central distortion for side distortion.
Asymptotically as the dimension of the lattice vector quantizer and order of
the noise shaping filter approach infinity, the entropy rate of the dithered
Delta-Sigma quantization scheme approaches the symmetric two-channel MD
rate-distortion function for a memoryless Gaussian source and MSE fidelity
criterion, at any side-to-central distortion ratio and any resolution. In the
optimal scheme, the infinite-order noise shaping filter must be minimum phase
and have a piece-wise flat power spectrum with a single jump discontinuity. An
important advantage of the proposed design is that it is symmetric in rate and
distortion by construction, so the coding rates of the descriptions are
identical and there is therefore no need for source splitting."
"Let $s$ be a string of length $n$ over an alphabet of constant size $\sigma$
and let $c$ and $\epsilon$ be constants with (1 \geq c \geq 0) and (\epsilon >
0). Using (O (n)) time, (O (n^c)) bits of memory and one pass we can always
encode $s$ in (n H_k (s) + O (\sigma^k n^{1 - c + \epsilon})) bits for all
integers (k \geq 0) simultaneously. On the other hand, even with unlimited
time, using (O (n^c)) bits of memory and one pass we cannot always encode $s$
in (O (n H_k (s) + \sigma^k n^{1 - c - \epsilon})) bits for, e.g., (k = \lceil
(c + \epsilon / 2) \log_\sigma n \rceil)."
"In this letter we determine the derivative of the mutual information
corresponding to bit-interleaved coded modulation systems. The derivative
follows as a linear combination of minimum-mean-squared error functions of
coded modulation sets. The result finds applications to the analysis of
communications systems in the wideband regime and to the design of power
allocation over parallel channels."
"We trace the history of empirical entropy, touching briefly on its relation
to Markov processes, normal numbers, Shannon entropy, the Chomsky hierarchy,
Kolmogorov complexity, Ziv-Lempel compression, de Bruijn sequences and
stochastic complexity."
"A discrete memoryless half-duplex relay channel is constructed from a
broadcast channel from the source to the relay and destination and a multiple
access channel from the source and relay to the destination. When the relay
listens, the channel operates in the broadcast mode. The channel switches to
the multiple access mode when the relay transmits. If the broadcast component
channel is physically degraded, the half-duplex relay channel will also be
referred to as physically degraded. The capacity of this degraded half-duplex
relay channel is examined. It is shown that the block Markov coding suggested
in the seminal paper by Cover and El Gamal can be modified to achieve capacity
for the degraded half-duplex relay channel. In the code construction, the
listen-transmit schedule of the relay is made to depend on the message to be
sent and hence the schedule carries information itself. If the schedule is
restricted to be deterministic, it is shown that the capacity can be achieved
by a simple management of information flows across the broadcast and multiple
access component channels."
"In order to understand the key merits of multiuser diversity techniques in
relay-assisted cellular multihop networks, this paper analyzes the spectral
efficiency of opportunistic (i.e., channel-aware) scheduling algorithms over a
fading multiuser relay channel with $K$ users in the asymptotic regime of large
(but finite) number of users. Using tools from extreme-value theory, we
characterize the limiting distribution of spectral efficiency focusing on Type
I convergence and utilize it in investigating the large system behavior of the
multiuser relay channel as a function of the number of users and physical
channel signal-to-noise ratios (SNRs). Our analysis results in very accurate
formulas in the large (but finite) $K$ regime, provides insights on the
potential performance enhancements from multihop routing and spectrum reuse
policies in the presence of multiuser diversity gains from opportunistic
scheduling and helps to identify the regimes and conditions in which
relay-assisted multiuser communication provides a clear advantage over direct
multiuser communication."
"The order of letters is not always relevant in a communication task. This
paper discusses the implications of order irrelevance on source coding,
presenting results in several major branches of source coding theory: lossless
coding, universal lossless coding, rate-distortion, high-rate quantization, and
universal lossy coding. The main conclusions demonstrate that there is a
significant rate savings when order is irrelevant. In particular, lossless
coding of n letters from a finite alphabet requires Theta(log n) bits and
universal lossless coding requires n + o(n) bits for many countable alphabet
sources. However, there are no universal schemes that can drive a strong
redundancy measure to zero. Results for lossy coding include distribution-free
expressions for the rate savings from order irrelevance in various high-rate
quantization schemes. Rate-distortion bounds are given, and it is shown that
the analogue of the Shannon lower bound is loose at all finite rates."
"Four different ways of obtaining low-density parity-check codes from expander
graphs are considered. For each case, lower bounds on the minimum stopping set
size and the minimum pseudocodeword weight of expander (LDPC) codes are
derived. These bounds are compared with the known eigenvalue-based lower bounds
on the minimum distance of expander codes. Furthermore, Tanner's
parity-oriented eigenvalue lower bound on the minimum distance is generalized
to yield a new lower bound on the minimum pseudocodeword weight. These bounds
are useful in predicting the performance of LDPC codes under graph-based
iterative decoding and linear programming decoding."
"We introduce S-DUDE, a new algorithm for denoising DMC-corrupted data. The
algorithm, which generalizes the recently introduced DUDE (Discrete Universal
DEnoiser) of Weissman et al., aims to compete with a genie that has access, in
addition to the noisy data, also to the underlying clean data, and can choose
to switch, up to $m$ times, between sliding window denoisers in a way that
minimizes the overall loss. When the underlying data form an individual
sequence, we show that the S-DUDE performs essentially as well as this genie,
provided that $m$ is sub-linear in the size of the data. When the clean data is
emitted by a piecewise stationary process, we show that the S-DUDE achieves the
optimum distribution-dependent performance, provided that the same
sub-linearity condition is imposed on the number of switches. To further
substantiate the universal optimality of the S-DUDE, we show that when the
number of switches is allowed to grow linearly with the size of the data,
\emph{any} (sequence of) scheme(s) fails to compete in the above senses. Using
dynamic programming, we derive an efficient implementation of the S-DUDE, which
has complexity (time and memory) growing only linearly with the data size and
the number of switches $m$. Preliminary experimental results are presented,
suggesting that S-DUDE has the capacity to significantly improve on the
performance attained by the original DUDE in applications where the nature of
the data abruptly changes in time (or space), as is often the case in practice."
"A rateless code-i.e., a rate-compatible family of codes-has the property that
codewords of the higher rate codes are prefixes of those of the lower rate
ones. A perfect family of such codes is one in which each of the codes in the
family is capacity-achieving. We show by construction that perfect rateless
codes with low-complexity decoding algorithms exist for additive white Gaussian
noise channels. Our construction involves the use of layered encoding and
successive decoding, together with repetition using time-varying layer weights.
As an illustration of our framework, we design a practical three-rate code
family. We further construct rich sets of near-perfect rateless codes within
our architecture that require either significantly fewer layers or lower
complexity than their perfect counterparts. Variations of the basic
construction are also developed, including one for time-varying channels in
which there is no a priori stochastic model."
"A generalized multiple access channel (GMAC) with one confidential message
set is studied, where two users (users 1 and 2) attempt to transmit common
information to a destination, and user 1 also has confidential information
intended for the destination. Moreover, user 1 wishes to keep its confidential
information as secret as possible from user 2. A deterministic GMAC is first
studied, and the capacity-equivocation region and the secrecy capacity region
are obtained. Two main classes of the GMAC are then studied: the binary GMAC
and the Gaussian GMAC. For both channels, the capacity-equivocation region and
the secrecy capacity region are established."
"The fading wire-tap channel is investigated, where the source-to-destination
channel and the source-to-wire-tapper channel are corrupted by multiplicative
fading gain coefficients in addition to additive Gaussian noise terms. The
channel state information is assumed to be known at both the transmitter and
the receiver. The parallel wire-tap channel with independent subchannels is
first studied, which serves as an information-theoretic model for the fading
wire-tap channel. The secrecy capacity of the parallel wire-tap channel is
established. This result is then specialized to give the secrecy capacity of
the fading wire-tap channel, which is achieved with the source node dynamically
changing the power allocation according to the channel state realization. An
optimal source power allocation is obtained to achieve the secrecy capacity."
"We focus on full-rate, fast-decodable space-time block codes (STBCs) for 2x2
and 4x2 multiple-input multiple-output (MIMO) transmission. We first derive
conditions for reduced-complexity maximum-likelihood decoding, and apply them
to a unified analysis of two families of 2x2 STBCs that were recently proposed.
In particular, we describe a reduced-complexity sphere decoding algorithm
suitable for QAM signal constellations. Next, we derive a novel
reduced-complexity 4x2 STBC, and show that it outperforms all previously known
codes with certain constellations."
"A novel fast recursive coding technique is proposed. It operates with only
integer values not longer 8 bits and is multiplication free. Recursion the
algorithm is based on indirectly provides rather effective coding of symbols
for very large alphabets. The code length for the proposed technique can be up
to 20-30% less than for arithmetic coding and, in the worst case it is only by
1-3% larger."
"We consider the `one-shot frame synchronization problem' where a decoder
wants to locate a sync pattern at the output of a channel on the basis of
sequential observations. We assume that the sync pattern of length N starts
being emitted at a random time within some interval of size A, that
characterizes the asynchronism level between the transmitter and the receiver.
We show that a sequential decoder can optimally locate the sync pattern, i.e.,
exactly, without delay, and with probability approaching one as N tends to
infinity, if and only if the asynchronism level grows as O(exp(N*k)), with k
below the `synchronization threshold,' a constant that admits a simple
expression depending on the channel. This constant is the same as the one that
characterizes the limit for reliable asynchronous communication, as was
recently reported by the authors. If k exceeds the synchronization threshold,
any decoder, sequential or non-sequential, locates the sync pattern with an
error that tends to one as N tends to infinity. Hence, a sequential decoder can
locate a sync pattern as well as the (non-sequential) maximum likelihood
decoder that operates on the basis of output sequences of maximum length A+N-1,
but with much fewer observations."
"Space-time block codes (STBCs) that are single-symbol decodable (SSD) in a
co-located multiple antenna setting need not be SSD in a distributed
cooperative communication setting. A relay network with N relays and a single
source-destination pair is called a partially-coherent relay channel (PCRC) if
the destination has perfect channel state information (CSI) of all the channels
and the relays have only the phase information of the source-to-relay channels.
In this paper, first, a new set of necessary and sufficient conditions for a
STBC to be SSD for co-located multiple antenna communication is obtained. Then,
this is extended to a set of necessary and sufficient conditions for a
distributed STBC (DSTBC) to be SSD for a PCRC, by identifying the additional
conditions. Using this, several SSD DSTBCs for PCRC are identified among the
known classes of STBCs. It is proved that even if a SSD STBC for a co-located
MIMO channel does not satisfy the additional conditions for the code to be SSD
for a PCRC, single-symbol decoding of it in a PCRC gives full-diversity and
only coding gain is lost. It is shown that when a DSTBC is SSD for a PCRC, then
arbitrary coordinate interleaving of the in-phase and quadrature-phase
components of the variables does not disturb its SSD property for PCRC.
Finally, it is shown that the possibility of {\em channel phase compensation}
operation at the relay nodes using partial CSI at the relays increases the
possible rate of SSD DSTBCs from $\frac{2}{N}$ when the relays do not have CSI
to 1/2, which is independent of N."
"In this paper, we study network coding capacity for random wireless networks.
Previous work on network coding capacity for wired and wireless networks have
focused on the case where the capacities of links in the network are
independent. In this paper, we consider a more realistic model, where wireless
networks are modeled by random geometric graphs with interference and noise. In
this model, the capacities of links are not independent. We consider two
scenarios, single source multiple destinations and multiple sources multiple
destinations. In the first scenario, employing coupling and martingale methods,
we show that the network coding capacity for random wireless networks still
exhibits a concentration behavior around the mean value of the minimum cut
under some mild conditions. Furthermore, we establish upper and lower bounds on
the network coding capacity for dependent and independent nodes. In the second
one, we also show that the network coding capacity still follows a
concentration behavior. Our simulation results confirm our theoretical
predictions."
"The problems of conditional entropy's definition and the formula to compute
conditional entropy are analyzed from various perspectives, and the corrected
computing formula is presented. Examples are given to prove the conclusion that
conditional entropy never be increased is not absolute, thus the representation
that information is to decrease uncertainty in the definition of information is
not absolutely correct."
"Very recently, we proposed the row-monomial distributed orthogonal space-time
block codes (DOSTBCs) and showed that the row-monomial DOSTBCs achieved
approximately twice higher bandwidth efficiency than the repetitionbased
cooperative strategy [1]. However, we imposed two limitations on the
row-monomial DOSTBCs. The first one was that the associated matrices of the
codes must be row-monomial. The other was the assumption that the relays did
not have any channel state information (CSI) of the channels from the source to
the relays, although this CSI could be readily obtained at the relays without
any additional pilot signals or any feedback overhead. In this paper, we first
remove the row-monomial limitation; but keep the CSI limitation. In this case,
we derive an upper bound of the data-rate of the DOSTBC and it is larger than
that of the row-monomial DOSTBCs in [1]. Secondly, we abandon the CSI
limitation; but keep the row-monomial limitation. Specifically, we propose the
row-monomial DOSTBCs with channel phase information (DOSTBCs-CPI) and derive an
upper bound of the data-rate of those codes. The rowmonomial DOSTBCs-CPI have
higher data-rate than the DOSTBCs and the row-monomial DOSTBCs. Furthermore, we
find the actual row-monomial DOSTBCs-CPI which achieve the upper bound of the
data-rate."
"We present a joint source-channel multiple description (JSC-MD) framework for
resource-constrained network communications (e.g., sensor networks), in which
one or many deprived encoders communicate a Markov source against bit errors
and erasure errors to many heterogeneous decoders, some powerful and some
deprived. To keep the encoder complexity at minimum, the source is coded into K
descriptions by a simple multiple description quantizer (MDQ) with neither
entropy nor channel coding. The code diversity of MDQ and the path diversity of
the network are exploited by decoders to correct transmission errors and
improve coding efficiency. A key design objective is resource scalability:
powerful nodes in the network can perform JSC-MD distributed
estimation/decoding under the criteria of maximum a posteriori probability
(MAP) or minimum mean-square error (MMSE), while primitive nodes resort to
simpler MD decoding, all working with the same MDQ code. The application of
JSC-MD to distributed estimation of hidden Markov models in a sensor network is
demonstrated. The proposed JSC-MD MAP estimator is an algorithm of the longest
path in a weighted directed acyclic graph, while the JSC-MD MMSE decoder is an
extension of the well-known forward-backward algorithm to multiple
descriptions. Both algorithms simultaneously exploit the source memory, the
redundancy of the fixed-rate MDQ, and the inter-description correlations. They
outperform the existing hard-decision MDQ decoders by large margins (up to
8dB). For Gaussian Markov sources, the complexity of JSC-MD distributed MAP
sequence estimation can be made as low as that of typical single description
Viterbi-type algorithms."
"Consider a wireless MIMO multi-hop channel with n_s non-cooperating source
antennas and n_d fully cooperating destination antennas, as well as L clusters
containing k non-cooperating relay antennas each. The source signal traverses
all L clusters of relay antennas, before it reaches the destination. When relay
antennas within the same cluster scale their received signals by the same
constant before the retransmission, the equivalent channel matrix H relating
the input signals at the source antennas to the output signals at the
destination antennas is proportional to the product of channel matrices H_l,
l=1,...,L+1, corresponding to the individual hops. We perform an asymptotic
capacity analysis for this channel as follows: In a first instance we take the
limits n_s->infty, n_d->infty and k->infty, but keep both n_s/n_d and k/n_d
fixed. Then, we take the limits L->infty and k/n_d->infty. Requiring that the
H_l's satisfy the conditions needed for the Marcenko-Pastur law, we prove that
the capacity scales linearly in min{n_s,n_d}, as long as the ratio k/n_d scales
at least linearly in L. Moreover, we show that up to a noise penalty and a
pre-log factor the capacity of a point-to-point MIMO channel is approached,
when this scaling is slightly faster than linear. Conversely, almost all
spatial degrees of freedom vanish for less than linear scaling."
"The optimal decoder achieving the outage capacity under imperfect channel
estimation is investigated. First, by searching into the family of nearest
neighbor decoders, which can be easily implemented on most practical coded
modulation systems, we derive a decoding metric that minimizes the average of
the transmission error probability over all channel estimation errors. Next, we
specialize our general expression to obtain the corresponding decoding metric
for fading MIMO channels. According to the notion of estimation-induced outage
(EIO) capacity introduced in our previous work and assuming no channel state
information (CSI) at the transmitter, we characterize maximal achievable
information rates, using Gaussian codebooks, associated to the proposed
decoder. In the case of uncorrelated Rayleigh fading, these achievable rates
are compared to the rates achieved by the classical mismatched
maximum-likelihood (ML) decoder and the ultimate limits given by the EIO
capacity. Numerical results show that the derived metric provides significant
gains for the considered scenario, in terms of achievable information rates and
bit error rate (BER), in a bit interleaved coded modulation (BICM) framework,
without introducing any additional decoding complexity."
This paper has been withdrawn by the author
"The Gilbert-Varshamov bound states that the maximum size A_2(n,d) of a binary
code of length n and minimum distance d satisfies A_2(n,d) >= 2^n/V(n,d-1)
where V(n,d) stands for the volume of a Hamming ball of radius d. Recently
Jiang and Vardy showed that for binary non-linear codes this bound can be
improved to A_2(n,d) >= cn2^n/V(n,d-1) for c a constant and d/n <= 0.499. In
this paper we show that certain asymptotic families of linear binary [n,n/2]
random double circulant codes satisfy the same improved Gilbert-Varshamov
bound."
"Distributed Orthogonal Space-Time Block Codes (DOSTBCs) achieving full
diversity order and single-symbol ML decodability have been introduced recently
for cooperative networks and an upper-bound on the maximal rate of such codes
along with code constructions has been presented. In this report, we introduce
a new class of Distributed STBCs called Semi-orthogonal Precoded Distributed
Single-Symbol Decodable STBCs (S-PDSSDC) wherein, the source performs
co-ordinate interleaving of information symbols appropriately before
transmitting it to all the relays. It is shown that DOSTBCs are a special case
of S-PDSSDCs. A special class of S-PDSSDCs having diagonal covariance matrix at
the destination is studied and an upper bound on the maximal rate of such codes
is derived. The bounds obtained are approximately twice larger than that of the
DOSTBCs. A systematic construction of S-PDSSDCs is presented when the number of
relays $K \geq 4$. The constructed codes are shown to achieve the upper-bound
on the rate when $K$ is of the form 0 modulo 4 or 3 modulo 4. For the rest of
the values of $K$, the constructed codes are shown to have rates higher than
that of DOSTBCs. It is also shown that S-PDSSDCs cannot be constructed with any
form of linear processing at the relays when the source doesn't perform
co-ordinate interleaving of the information symbols."
"The role of multiple antennas for secure communication is investigated within
the framework of Wyner's wiretap channel. We characterize the secrecy capacity
in terms of generalized eigenvalues when the sender and eavesdropper have
multiple antennas, the intended receiver has a single antenna, and the channel
matrices are fixed and known to all the terminals, and show that a beamforming
strategy is capacity-achieving. In addition, we show that in the high
signal-to-noise (SNR) ratio regime the penalty for not knowing eavesdropper's
channel is small--a simple ``secure space-time code'' that can be thought of as
masked beamforming and radiates power isotropically attains near-optimal
performance. In the limit of large number of antennas, we obtain a
realization-independent characterization of the secrecy capacity as a function
of the number $\beta$: the number of eavesdropper antennas per sender antenna.
We show that the eavesdropper is comparatively ineffective when $\beta<1$, but
that for $\beta\ge2$ the eavesdropper can drive the secrecy capacity to zero,
thereby blocking secure communication to the intended receiver. Extensions to
ergodic fading channels are also provided."
"This paper provides a new duality between entropy functions and network
codes. Given a function $g\geq 0$ defined on all proper subsets of $N$ random
variables, we provide a construction for a network multicast problem which is
solvable if and only if $g$ is entropic. The underlying network topology is
fixed and the multicast problem depends on $g$ only through edge capacities and
source rates. Relaxing the requirement that the domain of $g$ be subsets of
random variables, we obtain a similar duality between polymatroids and the
linear programming bound. These duality results provide an alternative proof of
the insufficiency of linear (and abelian) network codes, and demonstrate the
utility of non-Shannon inequalities to tighten outer bounds on network coding
capacity regions."
"In this paper we investigate the achievable rate of a system that includes a
nomadic transmitter with several antennas, which is received by multiple
agents, exhibiting independent channel gains and additive circular-symmetric
complex Gaussian noise. In the nomadic regime, we assume that the agents do not
have any decoding ability. These agents process their channel observations and
forward them to the final destination through lossless links with a fixed
capacity. We propose new achievable rates based on elementary compression and
also on a Wyner-Ziv (CEO-like) processing, for both fast fading and block
fading channels, as well as for general discrete channels. The simpler two
agents scheme is solved, up to an implicit equation with a single variable.
Limiting the nomadic transmitter to a circular-symmetric complex Gaussian
signalling, new upper bounds are derived for both fast and block fading, based
on the vector version of the entropy power inequality. These bounds are then
compared to the achievable rates in several extreme scenarios. The asymptotic
setting with numbers of agents and transmitter's antennas taken to infinity is
analyzed. In addition, the upper bounds are analytically shown to be tight in
several examples, while numerical calculations reveal a rather small gap in a
finite $2\times2$ setting. The advantage of the Wyner-Ziv approach over
elementary compression is shown where only the former can achieve the full
diversity-multiplexing tradeoff. We also consider the non-nomadic setting, with
agents that can decode. Here we give an achievable rate, over fast fading
channel, which combines broadcast with dirty paper coding and the decentralized
reception, which was introduced for the nomadic setting."
"The McEliece cryptosystem is a public-key cryptosystem based on coding theory
that has successfully resisted cryptanalysis for thirty years. The original
version, based on Goppa codes, is able to guarantee a high level of security,
and is faster than competing solutions, like RSA. Despite this, it has been
rarely considered in practical applications, due to two major drawbacks: i)
large size of the public key and ii) low transmission rate. Low-Density
Parity-Check (LDPC) codes are state-of-art forward error correcting codes that
permit to approach the Shannon limit while ensuring limited complexity.
Quasi-Cyclic (QC) LDPC codes are a particular class of LDPC codes, able to join
low complexity encoding of QC codes with high-performing and low-complexity
decoding of LDPC codes. In a previous work it has been proposed to adopt a
particular family of QC-LDPC codes in the McEliece cryptosystem to reduce the
key size and increase the transmission rate. Recently, however, new attacks
have been found that are able to exploit a flaw in the transformation from the
private key to the public one. Such attacks can be effectively countered by
changing the form of some constituent matrices, without altering the system
parameters. This work gives an overview of the QC-LDPC codes-based McEliece
cryptosystem and its cryptanalysis. Two recent versions are considered, and
their ability to counter all the currently known attacks is discussed. A third
version able to reach a higher security level is also proposed. Finally, it is
shown that the new QC-LDPC codes-based cryptosystem scales favorably with the
key length."
"We propose a new algorithm for binary quantization based on the Belief
Propagation algorithm with decimation over factor graphs of Low Density
Generator Matrix (LDGM) codes. This algorithm, which we call Bias Propagation
(BiP), can be considered as a special case of the Survey Propagation algorithm
proposed for binary quantization by Wainwright et al. [8]. It achieves the same
near-optimal rate-distortion performance with a substantially simpler framework
and 10-100 times faster implementation. We thus challenge the widespread belief
that binary quantization based on sparse linear codes cannot be solved by
simple Belief Propagation algorithms. Finally, we give examples of suitably
irregular LDGM codes that work with the BiP algorithm and show their
performance."
"For every $n = 2^k > 8$ there exist exactly $[(k+1)/2]$ mutually
nonequivalent $Z_4$-linear extended perfect codes with distance 4. All these
codes have different ranks."
"If $N=2^k > 8$ then there exist exactly $[(k-1)/2]$ pairwise nonequivalent
$Z_4$-linear Hadamard $(N,2N,N/2)$-codes and $[(k+1)/2]$ pairwise nonequivalent
$Z_4$-linear extended perfect $(N,2^N/2N,4)$-codes. A recurrent construction of
$Z_4$-linear Hadamard codes is given."
"This paper investigates point-to-point information transmission over a
wideband slow-fading channel, modeled as an (asymptotically) large number of
independent identically distributed parallel channels, with the random channel
fading realizations remaining constant over the entire coding block. On the one
hand, in the wideband limit the minimum achievable energy per nat required for
reliable transmission, as a random variable, converges in probability to
certain deterministic quantity. On the other hand, the exponential decay rate
of the outage probability, termed as the wideband outage exponent,
characterizes how the number of parallel channels, {\it i.e.}, the
``bandwidth'', should asymptotically scale in order to achieve a targeted
outage probability at a targeted energy per nat. We examine two scenarios: when
the transmitter has no channel state information and adopts uniform transmit
power allocation among parallel channels; and when the transmitter is endowed
with an one-bit channel state feedback for each parallel channel and
accordingly allocates its transmit power. For both scenarios, we evaluate the
wideband minimum energy per nat and the wideband outage exponent, and discuss
their implication for system performance."
"This paper introduces a new counting code. Its design was motivated by
distributed video coding where, for decoding, error correction methods are
applied to improve predictions. Those error corrections sometimes fail which
results in decoded values worse than the initial prediction. Our code exploits
the fact that bit errors are relatively unlikely events: more than a few bit
errors in a decoded pixel value are rare. With a carefully designed counting
code combined with a prediction those bit errors can be corrected and sometimes
the original pixel value recovered. The error correction improves
significantly. Our new code not only maximizes the Hamming distance between
adjacent (or ""near 1"") codewords but also between nearby (for example ""near 2"")
codewords. This is why our code is significantly different from the well-known
maximal counting sequences which have maximal average Hamming distance.
Fortunately, the new counting code can be derived from Gray Codes for every
code word length (i.e. bit depth)."
"`Tree pruning' (TP) is an algorithm for probabilistic inference on binary
Markov random fields. It has been recently derived by Dror Weitz and used to
construct the first fully polynomial approximation scheme for counting
independent sets up to the `tree uniqueness threshold.' It can be regarded as a
clever method for pruning the belief propagation computation tree, in such a
way to exactly account for the effect of loops.
  In this paper we generalize the original algorithm to make it suitable for
decoding linear codes, and discuss various schemes for pruning the computation
tree. Further, we present the outcomes of numerical simulations on several
linear codes, showing that tree pruning allows to interpolate continuously
between belief propagation and maximum a posteriori decoding. Finally, we
discuss theoretical implications of the new method."
"In this paper, we propose a new coding scheme for the general relay channel.
This coding scheme is in the form of a block Markov code. The transmitter uses
a superposition Markov code. The relay compresses the received signal and maps
the compressed version of the received signal into a codeword conditioned on
the codeword of the previous block. The receiver performs joint decoding after
it has received all of the B blocks. We show that this coding scheme can be
viewed as a generalization of the well-known Compress-And-Forward (CAF) scheme
proposed by Cover and El Gamal. Our coding scheme provides options for
preserving the correlation between the channel inputs of the transmitter and
the relay, which is not possible in the CAF scheme. Thus, our proposed scheme
may potentially yield a larger achievable rate than the CAF scheme."
"This report is devoted to introduction in multichannel algorithm based on
generalized numeration notations (GPN). The internal, external and mixed
account are entered. The concept of the GPN and its classification as
decomposition of an integer on composed of integers is discussed. Realization
of multichannel algorithm on the basis of GPN is introduced. In particular,
some properties of Fibonacci multichannel algorithm are discussed."
"We solve the problem of designing powerful low-density parity-check (LDPC)
codes with iterative decoding for the block-fading channel. We first study the
case of maximum-likelihood decoding, and show that the design criterion is
rather straightforward. Unfortunately, optimal constructions for
maximum-likelihood decoding do not perform well under iterative decoding. To
overcome this limitation, we then introduce a new family of full-diversity LDPC
codes that exhibit near-outage-limit performance under iterative decoding for
all block-lengths. This family competes with multiplexed parallel turbo codes
suitable for nonergodic channels and recently reported in the literature."
"In this paper we formalize the notions of information elements and
information lattices, first proposed by Shannon. Exploiting this formalization,
we identify a comprehensive parallelism between information lattices and
subgroup lattices. Qualitatively, we demonstrate isomorphisms between
information lattices and subgroup lattices. Quantitatively, we establish a
decisive approximation relation between the entropy structures of information
lattices and the log-index structures of the corresponding subgroup lattices.
This approximation extends the approximation for joint entropies carried out
previously by Chan and Yeung. As a consequence of our approximation result, we
show that any continuous law holds in general for the entropies of information
elements if and only if the same law holds in general for the log-indices of
subgroups. As an application, by constructing subgroup counterexamples we find
surprisingly that common information, unlike joint information, obeys neither
the submodularity nor the supermodularity law. We emphasize that the notion of
information elements is conceptually significant--formalizing it helps to
reveal the deep connection between information theory and group theory. The
parallelism established in this paper admits an appealing group-action
explanation and provides useful insights into the intrinsic structure among
information elements from a group-theoretic perspective."
"Convergence properties of Shannon Entropy are studied. In the differential
setting, it is shown that weak convergence of probability measures, or
convergence in distribution, is not enough for convergence of the associated
differential entropies. A general result for the desired differential entropy
convergence is provided, taking into account both compactly and uncompactly
supported densities. Convergence of differential entropy is also characterized
in terms of the Kullback-Liebler discriminant for densities with fairly general
supports, and it is shown that convergence in variation of probability measures
guarantees such convergence under an appropriate boundedness condition on the
densities involved. Results for the discrete setting are also provided,
allowing for infinitely supported probability measures, by taking advantage of
the equivalence between weak convergence and convergence in variation in this
setting."
"We consider a general stochastic input-output dynamical system with output
evolving in time as the solution to a functional coefficients, It\^{o}'s
stochastic differential equation, excited by an input process. This general
class of stochastic systems encompasses not only the classical communication
channel models, but also a wide variety of engineering systems appearing
through a whole range of applications. For this general setting we find
analogous of known relationships linking input-output mutual information and
minimum mean causal and non-causal square errors, previously established in the
context of additive Gaussian noise communication channels. Relationships are
not only established in terms of time-averaged quantities, but also their
time-instantaneous, dynamical counterparts are presented. The problem of
appropriately introducing in this general framework a signal-to-noise ratio
notion expressed through a signal-to-noise ratio parameter is also taken into
account, identifying conditions for a proper and meaningful interpretation."
"The MIMOME channel is a Gaussian wiretap channel in which the sender,
receiver, and eavesdropper all have multiple antennas. We characterize the
secrecy capacity as the saddle-value of a minimax problem. Among other
implications, our result establishes that a Gaussian distribution maximizes the
secrecy capacity characterization of Csisz{\'a}r and K{\""o}rner when applied to
the MIMOME channel. We also determine a necessary and sufficient condition for
the secrecy capacity to be zero. Large antenna array analysis of this condition
reveals several useful insights into the conditions under which secure
communication is possible."
"A multiple transmit antenna, single receive antenna (per receiver) downlink
channel with limited channel feedback is considered. Given a constraint on the
total system-wide channel feedback, the following question is considered: is it
preferable to get low-rate feedback from a large number of receivers or to
receive high-rate/high-quality feedback from a smaller number of (randomly
selected) receivers? Acquiring feedback from many users allows multi-user
diversity to be exploited, while high-rate feedback allows for very precise
selection of beamforming directions. It is shown that systems in which a
limited number of users feedback high-rate channel information significantly
outperform low-rate/many user systems. While capacity increases only double
logarithmically with the number of users, the marginal benefit of channel
feedback is very significant up to the point where the CSI is essentially
perfect."
"A clear understanding the behavior of the error probability (EP) as a
function of signal-to-noise ratio (SNR) and other system parameters is
fundamental for assessing the design of digital wireless communication
systems.We propose an analytical framework based on the log-concavity property
of the EP which we prove for a wide family of multidimensional modulation
formats in the presence of Gaussian disturbances and fading. Based on this
property, we construct a class of local bounds for the EP that improve known
generic bounds in a given region of the SNR and are invertible, as well as
easily tractable for further analysis. This concept is motivated by the fact
that communication systems often operate with performance in a certain region
of interest (ROI) and, thus, it may be advantageous to have tighter bounds
within this region instead of generic bounds valid for all SNRs. We present a
possible application of these local bounds, but their relevance is beyond the
example made in this paper."
"We analyze a slow-fading interference network with MN non-cooperating
single-antenna sources and M non-cooperating single-antenna destinations. In
particular, we assume that the sources are divided into M mutually exclusive
groups of N sources each, every group is dedicated to transmit a common message
to a unique destination, all transmissions occur concurrently and in the same
frequency band and a dedicated 1-bit broadcast feedback channel from each
destination to its corresponding group of sources exists. We provide a
feedback-based iterative distributed (multi-user) beamforming algorithm, which
""learns"" the channels between each group of sources and its assigned
destination. This algorithm is a straightforward generalization, to the
multi-user case, of the feedback-based iterative distributed beamforming
algorithm proposed recently by Mudumbai et al., in IEEE Trans. Inf. Th.
(submitted) for networks with a single group of sources and a single
destination. Putting the algorithm into a Markov chain context, we provide a
simple convergence proof. We then show that, for M finite and N approaching
infinity, spatial multiplexing based on the beamforming weights produced by the
algorithm achieves full spatial multiplexing gain of M and full per-stream
array gain of N, provided the time spent ""learning'' the channels scales
linearly in N. The network is furthermore shown to ""crystallize''. Finally, we
characterize the corresponding crystallization rate."
"Despite the NP hardness of acquiring minimum distance $d_m$ for linear codes
theoretically, in this paper we propose one experimental method of finding
minimum-weight codewords, the weight of which is equal to $d_m$ for LDPC codes.
One existing syndrome decoding method, called serial belief propagation (BP)
with ordered statistic decoding (OSD), is adapted to serve our purpose. We hold
the conjecture that among many candidate error patterns in OSD reprocessing,
modulo 2 addition of the lightest error pattern with one of the left error
patterns may generate a light codeword. When the decoding syndrome changes to
all-zero state, the lightest error pattern reduces to all-zero, the lightest
non-zero error pattern is a valid codeword to update lightest codeword list.
  Given sufficient codewords sending, the survived lightest codewords are
likely to be the target. Compared with existing techniques, our method
demonstrates its efficiency in the simulation of several interested LDPC codes."
"This paper studies the performance of transmission schemes that have rate
that increases with average SNR while maintaining a fixed outage probability.
This is in contrast to the classical Zheng-Tse diversity-multiplexing tradeoff
(DMT) that focuses on increasing rate and decreasing outage probability. Three
different systems are explored: antenna diversity systems, time/frequency
diversity systems, and automatic repeat request (ARQ) systems. In order to
accurately study performance in the fixed outage setting, it is necesary to go
beyond the coarse, asymptotic multiplexing gain metric. In the case of antenna
diversity and time/frequency diversity, an affine approximation to high SNR
outage capacity (i.e., multiplexing gain plus a power/rate offset) accurately
describes performance and shows the very significant benefits of diversity. ARQ
is also seen to provide a significant performance advantage, but even an affine
approximation to outage capacity is unable to capture this advantage and outage
capacity must be directly studied in the non-asymptotic regime."
"Franceschetti et al. have recently shown that per-node throughput in an
extended, ad hoc wireless network with $\Theta(n)$ randomly distributed nodes
and multihop routing can be increased from the $\Omega({1 \over \sqrt{n} \log
n})$ scaling demonstrated in the seminal paper of Gupta and Kumar to $\Omega({1
\over \sqrt{n}})$. The goal of the present paper is to understand the
dependence of this interesting result on the principal new features it
introduced relative to Gupta-Kumar: (1) a capacity-based formula for link
transmission bit-rates in terms of received signal-to-interference-and-noise
ratio (SINR); (2) hierarchical routing from sources to destinations through a
system of communal highways; and (3) cell-based routes constructed by
percolation. The conclusion of the present paper is that the improved
throughput scaling is principally due to the percolation-based routing, which
enables shorter hops and, consequently, less interference. This is established
by showing that throughput $\Omega({1 \over \sqrt{n}})$ can be attained by a
system that does not employ highways, but instead uses percolation to
establish, for each source-destination pair, a set of $\Theta(\log n)$ routes
within a narrow routing corridor running from source to destination. As a
result, highways are not essential. In addition, it is shown that throughput
$\Omega({1 \over \sqrt{n}})$ can be attained with the original threshold
transmission bit-rate model, provided that node transmission powers are
permitted to grow with $n$. Thus, the benefit of the capacity bit-rate model is
simply to permit the power to remain bounded, even as the network expands."
"The word error rate (WER) of soft-decision-decoded binary block codes rarely
has closed-form. Bounding techniques are widely used to evaluate the
performance of maximum-likelihood decoding algorithm. But the existing bounds
are not tight enough especially for low signal-to-noise ratios and become
looser when a suboptimum decoding algorithm is used. This paper proposes a new
concept named square radius probability density function (SR-PDF) of decision
region to evaluate the WER. Based on the SR-PDF, The WER of binary block codes
can be calculated precisely for ML and suboptimum decoders. Furthermore, for a
long binary block code, SR-PDF can be approximated by Gamma distribution with
only two parameters that can be measured easily. Using this property, two
closed-form approximative expressions are proposed which are very close to the
simulation results of the WER of interesting."
"This paper addresses the problem of coding a continuous random source
correlated with another source which is only available at the decoder. The
proposed approach is based on the extension of the channel coding concept of
syndrome from the discrete into the continuous domain. If the correlation
between the sources can be described by an additive Gaussian backward channel
and capacity-achieving linear codes are employed, it is shown that the
performance of the system is asymptotically close to the Wyner-Ziv bound. Even
if such an additive channel is not Gaussian, the design procedure can fit the
desired correlation and transmission rate. Experiments based on trellis-coded
quantization show that the proposed system achieves a performance within 3-4 dB
of the theoretical bound in the 0.5-3 bit/sample rate range for any Gaussian
correlation, with a reasonable computational complexity."
"The cognitive interference channel with confidential messages is studied.
Similarly to the classical two-user interference channel, the cognitive
interference channel consists of two transmitters whose signals interfere at
the two receivers. It is assumed that there is a common message source (message
1) known to both transmitters, and an additional independent message source
(message 2) known only to the cognitive transmitter (transmitter 2). The
cognitive receiver (receiver 2) needs to decode both messages, while the
non-cognitive receiver (receiver 1) should decode only the common message.
Furthermore, message 2 is assumed to be a confidential message which needs to
be kept as secret as possible from receiver 1, which is viewed as an
eavesdropper with regard to message 2. The level of secrecy is measured by the
equivocation rate. A single-letter expression for the capacity-equivocation
region of the discrete memoryless cognitive interference channel is established
and is further explicitly derived for the Gaussian case. Moreover,
particularizing the capacity-equivocation region to the case without a secrecy
constraint, establishes a new capacity theorem for a class of interference
channels, by providing a converse theorem."
"A linear mesh network is considered in which a single user per cell
communicates to a local base station via a dedicated relay (two-hop
communication). Exploiting the possibly relevant inter-cell channel gains, rate
splitting with successive cancellation in both hops is investigated as a
promising solution to improve the rate of basic single-rate communications.
Then, an alternative solution is proposed that attempts to improve the
performance of the second hop (from the relays to base stations) by cooperative
transmission among the relay stations. The cooperative scheme leverages the
common information obtained by the relays as a by-product of the use of rate
splitting in the first hop. Numerical results bring insight into the conditions
(network topology and power constraints) under which rate splitting, with
possible relay cooperation, is beneficial. Multi-cell processing (joint
decoding at the base stations) is also considered for reference."
"Inner and outer bounds are established on the capacity region of two-sender,
two-receiver interference channels where one transmitter knows both messages.
The transmitter with extra knowledge is referred to as being cognitive. The
inner bound is based on strategies that generalize prior work, and include
rate-splitting, Gel'fand-Pinsker coding and cooperative transmission. A general
outer bound is based on the Nair-El Gamal outer bound for broadcast channels. A
simpler bound is presented for the case in which one of the decoders can decode
both messages. The bounds are evaluated and compared for Gaussian channels."
"In this paper, we investigate the error correction capability of
column-weight-three LDPC codes when decoded using the Gallager A algorithm. We
prove that the necessary condition for a code to correct $k \geq 5$ errors is
to avoid cycles of length up to $2k$ in its Tanner graph. As a consequence of
this result, we show that given any $\alpha>0, \exists N $ such that $\forall
n>N$, no code in the ensemble of column-weight-three codes can correct all
$\alpha n$ or fewer errors. We extend these results to the bit flipping
algorithm."
"The problem of channel shortening equalization for optimal detection in ISI
channels is considered. The problem is to choose a linear equalizer and a
partial response target filter such that the combination produces the best
detection performance. Instead of using the traditional approach of MMSE
equalization, we directly seek all equalizer and target pairs that yield
optimal detection performance in terms of the sequence or symbol error rate.
This leads to a new notion of a posteriori equivalence between the equalized
and target channels with a simple characterization in terms of their underlying
probability distributions. Using this characterization we show the surprising
existence an infinite family of equalizer and target pairs for which any
maximum a posteriori (MAP) based detector designed for the target channel is
simultaneously MAP optimal for the equalized channel. For channels whose input
symbols have equal energy, such as q-PSK, the MMSE equalizer designed with a
monic target constraint yields a solution belonging to this optimal family of
designs. Although, these designs produce IIR target filters, the ideas are
extended to design good FIR targets. For an arbitrary choice of target and
equalizer, we derive an expression for the probability of sequence detection
error. This expression is used to design optimal FIR targets and IIR equalizers
and to quantify the FIR approximation penalty."
"In this paper will be presented methodology of encoding information in
valuations of discrete lattice with some translational invariant constrains in
asymptotically optimal way. The method is based on finding statistical
description of such valuations and changing it into statistical algorithm,
which allows to construct deterministically valuation with given statistics.
Optimal statistics allow to generate valuations with uniform distribution - we
get maximum information capacity this way. It will be shown that we can reach
the optimum for one-dimensional models using maximal entropy random walk and
that for the general case we can practically get as close to the capacity of
the model as we want (found numerically: lost 10^{-10} bit/node for Hard
Square). There will be also presented simpler alternative to arithmetic coding
method which can be used as cryptosystem and data correction method too."
"Cooperative technology is expected to have a great impact on the performance
of cellular or, more generally, infrastructure networks. Both multicell
processing (cooperation among base stations) and relaying (cooperation at the
user level) are currently being investigated. In this presentation, recent
results regarding the performance of multicell processing and user cooperation
under the assumption of limited-capacity interbase station and inter-user
links, respectively, are reviewed. The survey focuses on related results
derived for non-fading uplink and downlink channels of simple cellular system
models. The analytical treatment, facilitated by these simple setups, enhances
the insight into the limitations imposed by limited-capacity constraints on the
gains achievable by cooperative techniques."
"We study the problem of the reconstruction of a Gaussian field defined in
[0,1] using N sensors deployed at regular intervals. The goal is to quantify
the total data rate required for the reconstruction of the field with a given
mean square distortion. We consider a class of two-stage mechanisms which a)
send information to allow the reconstruction of the sensor's samples within
sufficient accuracy, and then b) use these reconstructions to estimate the
entire field. To implement the first stage, the heavy correlation between the
sensor samples suggests the use of distributed coding schemes to reduce the
total rate. We demonstrate the existence of a distributed block coding scheme
that achieves, for a given fidelity criterion for the reconstruction of the
field, a total information rate that is bounded by a constant, independent of
the number $N$ of sensors. The constant in general depends on the
autocorrelation function of the field and the desired distortion criterion for
the sensor samples. We then describe a scheme which can be implemented using
only scalar quantizers at the sensors, without any use of distributed source
coding, and which also achieves a total information rate that is a constant,
independent of the number of sensors. While this scheme operates at a rate that
is greater than the rate achievable through distributed coding and entails
greater delay in reconstruction, its simplicity makes it attractive for
implementation in sensor networks."
"The wideband regime of bit-interleaved coded modulation (BICM) in Gaussian
channels is studied. The Taylor expansion of the coded modulation capacity for
generic signal constellations at low signal-to-noise ratio (SNR) is derived and
used to determine the corresponding expansion for the BICM capacity. Simple
formulas for the minimum energy per bit and the wideband slope are given. BICM
is found to be suboptimal in the sense that its minimum energy per bit can be
larger than the corresponding value for coded modulation schemes. The minimum
energy per bit using standard Gray mapping on M-PAM or M^2-QAM is given by a
simple formula and shown to approach -0.34 dB as M increases. Using the low SNR
expansion, a general trade-off between power and bandwidth in the wideband
regime is used to show how a power loss can be traded off against a bandwidth
gain."
"Recently, the secrecy capacity of the multi-antenna wiretap channel was
characterized by Khisti and Wornell [1] using a Sato-like argument. This note
presents an alternative characterization using a channel enhancement argument.
This characterization relies on an extremal entropy inequality recently proved
in the context of multi-antenna broadcast channels, and is directly built on
the physical intuition regarding to the optimal transmission strategy in this
communication scenario."
"This paper deals with a universal coding problem for a certain kind of
multiterminal source coding system that we call the complementary delivery
coding system. In this system, messages from two correlated sources are jointly
encoded, and each decoder has access to one of the two messages to enable it to
reproduce the other message. Both fixed-to-fixed length and fixed-to-variable
length lossless coding schemes are considered. Explicit constructions of
universal codes and bounds of the error probabilities are clarified via
type-theoretical and graph-theoretical analyses. [[Keywords]] multiterminal
source coding, complementary delivery, universal coding, types of sequences,
bipartite graphs"
"In their landmark paper Cover and El Gamal proposed different coding
strategies for the relay channel with a single relay supporting a communication
pair. These strategies are the decode-and-forward and compress-and-forward
approach, as well as a general lower bound on the capacity of a relay network
which relies on the mixed application of the previous two strategies. So far,
only parts of their work - the decode-and-forward and the compress-and-forward
strategy - have been applied to networks with multiple relays.
  This paper derives a mixed strategy for multiple relay networks using a
combined approach of partial decode-and-forward with N +1 levels and the ideas
of successive refinement with different side information at the receivers.
After describing the protocol structure, we present the achievable rates for
the discrete memoryless relay channel as well as Gaussian multiple relay
networks. Using these results we compare the mixed strategy with some special
cases, e. g., multilevel decode-and-forward, distributed compress-and-forward
and a mixed approach where one relay node operates in decode-and-forward and
the other in compress-and-forward mode."
"Single Event Upsets (SEU) as well as permanent faults can significantly
affect the correct on-line operation of digital systems, such as memories and
microprocessors; a memory can be made resilient to permanent and transient
faults by using modular redundancy and coding. In this paper, different memory
systems are compared: these systems utilize simplex and duplex arrangements
with a combination of Reed Solomon coding and scrubbing. The memory systems and
their operations are analyzed by novel Markov chains to characterize
performance for dynamic reconfiguration as well as error detection and
correction under the occurrence of permanent and transient faults. For a
specific Reed Solomon code, the duplex arrangement allows to efficiently cope
with the occurrence of permanent faults, while the use of scrubbing allows to
cope with transient faults."
"The problem of security against timing based traffic analysis in wireless
networks is considered in this work. An analytical measure of anonymity in
eavesdropped networks is proposed using the information theoretic concept of
equivocation. For a physical layer with orthogonal transmitter directed
signaling, scheduling and relaying techniques are designed to maximize
achievable network performance for any given level of anonymity. The network
performance is measured by the achievable relay rates from the sources to
destinations under latency and medium access constraints. In particular,
analytical results are presented for two scenarios:
  For a two-hop network with maximum anonymity, achievable rate regions for a
general m x 1 relay are characterized when nodes generate independent Poisson
transmission schedules. The rate regions are presented for both strict and
average delay constraints on traffic flow through the relay.
  For a multihop network with an arbitrary anonymity requirement, the problem
of maximizing the sum-rate of flows (network throughput) is considered. A
selective independent scheduling strategy is designed for this purpose, and
using the analytical results for the two-hop network, the achievable throughput
is characterized as a function of the anonymity level. The throughput-anonymity
relation for the proposed strategy is shown to be equivalent to an information
theoretic rate-distortion function."
"The distributed source coding problem is considered when the sensors, or
encoders, are under Byzantine attack; that is, an unknown group of sensors have
been reprogrammed by a malicious intruder to undermine the reconstruction at
the fusion center. Three different forms of the problem are considered. The
first is a variable-rate setup, in which the decoder adaptively chooses the
rates at which the sensors transmit. An explicit characterization of the
variable-rate achievable sum rates is given for any number of sensors and any
groups of traitors. The converse is proved constructively by letting the
traitors simulate a fake distribution and report the generated values as the
true ones. This fake distribution is chosen so that the decoder cannot
determine which sensors are traitors while maximizing the required rate to
decode every value. Achievability is proved using a scheme in which the decoder
receives small packets of information from a sensor until its message can be
decoded, before moving on to the next sensor. The sensors use randomization to
choose from a set of coding functions, which makes it probabilistically
impossible for the traitors to cause the decoder to make an error. Two forms of
the fixed-rate problem are considered, one with deterministic coding and one
with randomized coding. The achievable rate regions are given for both these
problems, and it is shown that lower rates can be achieved with randomized
coding."
"This paper deals with a universal coding problem for a certain kind of
multiterminal source coding network called a generalized complementary delivery
network. In this network, messages from multiple correlated sources are jointly
encoded, and each decoder has access to some of the messages to enable it to
reproduce the other messages. Both fixed-to-fixed length and fixed-to-variable
length lossless coding schemes are considered. Explicit constructions of
universal codes and the bounds of the error probabilities are clarified by
using methods of types and graph-theoretical analysis."
"A network of $n$ wireless communication links is considered in a Rayleigh
fading environment. It is assumed that each link can be active and transmit
with a constant power $P$ or remain silent. The objective is to maximize the
number of active links such that each active link can transmit with a constant
rate $\lambda$. An upper bound is derived that shows the number of active links
scales at most like $\frac{1}{\lambda} \log n$. To obtain a lower bound, a
decentralized link activation strategy is described and analyzed. It is shown
that for small values of $\lambda$, the number of supported links by this
strategy meets the upper bound; however, as $\lambda$ grows, this number
becomes far below the upper bound. To shrink the gap between the upper bound
and the achievability result, a modified link activation strategy is proposed
and analyzed based on some results from random graph theory. It is shown that
this modified strategy performs very close to the optimum. Specifically, this
strategy is \emph{asymptotically almost surely} optimum when $\lambda$
approaches $\infty$ or 0. It turns out the optimality results are obtained in
an interference-limited regime. It is demonstrated that, by proper selection of
the algorithm parameters, the proposed scheme also allows the network to
operate in a noise-limited regime in which the transmission rates can be
adjusted by the transmission powers. The price for this flexibility is a
decrease in the throughput scaling law by a multiplicative factor of $\log \log
n$."
"Aiming at bridging the gap between the maximum likelihood decoding (MLD) and
the suboptimal iterative decodings for short or medium length LDPC codes, we
present a generalized ordered statistic decoding (OSD) in the form of syndrome
decoding, to cascade with the belief propagation (BP) or enhanced min-sum
decoding. The OSD is invoked only when the decoding failures are obtained for
the preceded iterative decoding method. With respect to the existing OSD which
is based on the accumulated log-likelihood ratio (LLR) metric, we extend the
accumulative metric to the situation where the BP decoding is in the
probability domain. Moreover, after generalizing the accumulative metric to the
context of the normalized or offset min-sum decoding, the OSD shows appealing
tradeoff between performance and complexity. In the OSD implementation, when
deciding the true error pattern among many candidates, an alternative proposed
proves to be effective to reduce the number of real additions without
performance loss. Simulation results demonstrate that the cascade connection of
enhanced min-sum and OSD decodings outperforms the BP alone significantly, in
terms of either performance or complexity."
"We consider the transmission of a memoryless bivariate Gaussian source over
an average-power-constrained one-to-two Gaussian broadcast channel. The
transmitter observes the source and describes it to the two receivers by means
of an average-power-constrained signal. Each receiver observes the transmitted
signal corrupted by a different additive white Gaussian noise and wishes to
estimate the source component intended for it. That is, Receiver~1 wishes to
estimate the first source component and Receiver~2 wishes to estimate the
second source component. Our interest is in the pairs of expected squared-error
distortions that are simultaneously achievable at the two receivers.
  We prove that an uncoded transmission scheme that sends a linear combination
of the source components achieves the optimal power-versus-distortion trade-off
whenever the signal-to-noise ratio is below a certain threshold. The threshold
is a function of the source correlation and the distortion at the receiver with
the weaker noise."
"This article proposes a novel iterative algorithm based on Low Density Parity
Check (LDPC) codes for compression of correlated sources at rates approaching
the Slepian-Wolf bound. The setup considered in the article looks at the
problem of compressing one source at a rate determined based on the knowledge
of the mean source correlation at the encoder, and employing the other
correlated source as side information at the decoder which decompresses the
first source based on the estimates of the actual correlation. We demonstrate
that depending on the extent of the actual source correlation estimated through
an iterative paradigm, significant compression can be obtained relative to the
case the decoder does not use the implicit knowledge of the existence of
correlation."
"In this paper, we derive the optimal transmitter/ receiver beamforming
vectors and relay weighting matrix for the multiple-input multiple-output
amplify-and-forward relay channel. The analysis is accomplished in two steps.
In the first step, the direct link between the transmitter (Tx) and receiver
(Rx) is ignored and we show that the transmitter and the relay should map their
signals to the strongest right singular vectors of the Tx-relay and relay-Rx
channels. Based on the distributions of these vectors for independent
identically distributed (i.i.d.) Rayleigh channels, the Grassmannian codebooks
are used for quantizing and sending back the channel information to the
transmitter and the relay. The simulation results show that even a few number
of bits can considerably increase the link reliability in terms of bit error
rate. For the second step, the direct link is considered in the problem model
and we derive the optimization problem that identifies the optimal Tx
beamforming vector. For the i.i.d Rayleigh channels, we show that the solution
to this problem is uniformly distributed on the unit sphere and we justify the
appropriateness of the Grassmannian codebook (for determining the optimal
beamforming vector), both analytically and by simulation. Finally, a modified
quantizing scheme is presented which introduces a negligible degradation in the
system performance but significantly reduces the required number of feedback
bits."
"We describe and present a new construction method for codes using encodings
from group rings. They consist primarily of two types: zero-divisor and
unit-derived codes. Previous codes from group rings focused on ideals; for
example cyclic codes are ideals in the group ring over a cyclic group. The
fresh focus is on the encodings themselves, which only under very limited
conditions result in ideals. We use the result that a group ring is isomorphic
to a certain well-defined ring of matrices, and thus every group ring element
has an associated matrix. This allows matrix algebra to be used as needed in
the study and production of codes, enabling the creation of standard generator
and check matrices. Group rings are a fruitful source of units and
zero-divisors from which new codes result. Many code properties, such as being
LDPC or self-dual, may be expressed as properties within the group ring thus
enabling the construction of codes with these properties. The methods are
general enabling the construction of codes with many types of group rings.
There is no restriction on the ring and thus codes over the integers, over
matrix rings or even over group rings themselves are possible and fruitful."
"The utility of limited feedback for coding over an individual sequence of
DMCs is investigated. This study complements recent results showing how limited
or noisy feedback can boost the reliability of communication. A strategy with
fixed input distribution $P$ is given that asymptotically achieves rates
arbitrarily close to the mutual information induced by $P$ and the
state-averaged channel. When the capacity achieving input distribution is the
same over all channel states, this achieves rates at least as large as the
capacity of the state averaged channel, sometimes called the empirical
capacity."
"A novel class of bit-flipping (BF) algorithms for decoding low-density
parity-check (LDPC) codes is presented. The proposed algorithms, which are
called gradient descent bit flipping (GDBF) algorithms, can be regarded as
simplified gradient descent algorithms. Based on gradient descent formulation,
the proposed algorithms are naturally derived from a simple non-linear
objective function."
"This paper addresses the following question, which is of interest in the
design of a multiuser decentralized network. Given a total system bandwidth of
W Hz and a fixed data rate constraint of R bps for each transmission, how many
frequency slots N of size W/N should the band be partitioned into in order to
maximize the number of simultaneous links in the network? Dividing the
available spectrum results in two competing effects. On the positive side, a
larger N allows for more parallel, noninterfering communications to take place
in the same area. On the negative side, a larger N increases the SINR
requirement for each link because the same information rate must be achieved
over less bandwidth. Exploring this tradeoff and determining the optimum value
of N in terms of the system parameters is the focus of the paper. Using
stochastic geometry, the optimal SINR threshold - which directly corresponds to
the optimal spectral efficiency - is derived for both the low SNR
(power-limited) and high SNR (interference-limited) regimes. This leads to the
optimum choice of the number of frequency bands N in terms of the path loss
exponent, power and noise spectral density, desired rate, and total bandwidth."
"In this paper, we study the number of measurements required to recover a
sparse signal in ${\mathbb C}^M$ with $L$ non-zero coefficients from compressed
samples in the presence of noise. For a number of different recovery criteria,
we prove that $O(L)$ (an asymptotically linear multiple of $L$) measurements
are necessary and sufficient if $L$ grows linearly as a function of $M$. This
improves on the existing literature that is mostly focused on variants of a
specific recovery algorithm based on convex programming, for which
$O(L\log(M-L))$ measurements are required. We also show that $O(L\log(M-L))$
measurements are required in the sublinear regime ($L = o(M)$)."
"A codebook based limited feedback strategy is a practical way to obtain
partial channel state information at the transmitter in a precoded
multiple-input multiple-output (MIMO) wireless system. Conventional codebook
designs use Grassmannian packing, equiangular frames, vector quantization, or
Fourier based constructions. While the capacity and error rate performance of
conventional codebook constructions have been extensively investigated,
constructing these codebooks is notoriously difficult relying on techniques
such as nonlinear search or iterative algorithms. Further, the resulting
codebooks may not have a systematic structure to facilitate storage of the
codebook and low search complexity. In this paper, we propose a new systematic
codebook design based on Kerdock codes and mutually unbiased bases. The
proposed Kerdock codebook consists of multiple mutually unbiased unitary bases
matrices with quaternary entries and the identity matrix. We propose to derive
the beamforming and precoding codebooks from this base codebook, eliminating
the requirement to store multiple codebooks. The propose structure requires
little memory to store and, as we show, the quaternary structure facilitates
codeword search. We derive the chordal distance for two antenna and four
antenna codebooks, showing that the proposed codebooks compare favorably with
prior designs. Monte Carlo simulations are used to compare achievable rates and
error rates for different codebooks sizes."
"In this work we find the capacity of a compound finite-state channel with
time-invariant deterministic feedback. The model we consider involves the use
of fixed length block codes. Our achievability result includes a proof of the
existence of a universal decoder for the family of finite-state channels with
feedback. As a consequence of our capacity result, we show that feedback does
not increase the capacity of the compound Gilbert-Elliot channel. Additionally,
we show that for a stationary and uniformly ergodic Markovian channel, if the
compound channel capacity is zero without feedback then it is zero with
feedback. Finally, we use our result on the finite-state channel to show that
the feedback capacity of the memoryless compound channel is given by
$\inf_{\theta} \max_{Q_X} I(X;Y|\theta)$."
"The problem of error control in random linear network coding is addressed
from a matrix perspective that is closely related to the subspace perspective
of K\""otter and Kschischang. A large class of constant-dimension subspace codes
is investigated. It is shown that codes in this class can be easily constructed
from rank-metric codes, while preserving their distance properties. Moreover,
it is shown that minimum distance decoding of such subspace codes can be
reformulated as a generalized decoding problem for rank-metric codes where
partial information about the error is available. This partial information may
be in the form of erasures (knowledge of an error location but not its value)
and deviations (knowledge of an error value but not its location). Taking
erasures and deviations into account (when they occur) strictly increases the
error correction capability of a code: if $\mu$ erasures and $\delta$
deviations occur, then errors of rank $t$ can always be corrected provided that
$2t \leq d - 1 + \mu + \delta$, where $d$ is the minimum rank distance of the
code. For Gabidulin codes, an important family of maximum rank distance codes,
an efficient decoding algorithm is proposed that can properly exploit erasures
and deviations. In a network coding application where $n$ packets of length $M$
over $F_q$ are transmitted, the complexity of the decoding algorithm is given
by $O(dM)$ operations in an extension field $F_{q^n}$."
"This paper provides simple lower bounds on the number of iterations which is
required for successful message-passing decoding of some important families of
graph-based code ensembles (including low-density parity-check codes and
variations of repeat-accumulate codes). The transmission of the code ensembles
is assumed to take place over a binary erasure channel, and the bounds refer to
the asymptotic case where we let the block length tend to infinity. The
simplicity of the bounds derived in this paper stems from the fact that they
are easily evaluated and are expressed in terms of some basic parameters of the
ensemble which include the fraction of degree-2 variable nodes, the target bit
erasure probability and the gap between the channel capacity and the design
rate of the ensemble. This paper demonstrates that the number of iterations
which is required for successful message-passing decoding scales at least like
the inverse of the gap (in rate) to capacity, provided that the fraction of
degree-2 variable nodes of these turbo-like ensembles does not vanish (hence,
the number of iterations becomes unbounded as the gap to capacity vanishes)."
"We consider transmission of a continuous amplitude source over an L-block
Rayleigh fading $M_t \times M_r$ MIMO channel when the channel state
information is only available at the receiver. Since the channel is not
ergodic, Shannon's source-channel separation theorem becomes obsolete and the
optimal performance requires a joint source -channel approach. Our goal is to
minimize the expected end-to-end distortion, particularly in the high SNR
regime. The figure of merit is the distortion exponent, defined as the
exponential decay rate of the expected distortion with increasing SNR. We
provide an upper bound and lower bounds for the distortion exponent with
respect to the bandwidth ratio among the channel and source bandwidths. For the
lower bounds, we analyze three different strategies based on layered source
coding concatenated with progressive, superposition or hybrid digital/analog
transmission. In each case, by adjusting the system parameters we optimize the
distortion exponent as a function of the bandwidth ratio. We prove that the
distortion exponent upper bound can be achieved when the channel has only one
degree of freedom, that is L=1, and $\min\{M_t,M_r\}=1$. When we have more
degrees of freedom, our achievable distortion exponents meet the upper bound
for only certain ranges of the bandwidth ratio. We demonstrate that our
results, which were derived for a complex Gaussian source, can be extended to
more general source distributions as well."
"The Golden space-time trellis coded modulation (GST-TCM) scheme was proposed
in \cite{Hong06} for a high rate $2\times 2$ multiple-input multiple-output
(MIMO) system over slow fading channels. In this letter, we present the
performance analysis of GST-TCM over block fading channels, where the channel
matrix is constant over a fraction of the codeword length and varies from one
fraction to another, independently. In practice, it is not useful to design
such codes for specific block fading channel parameters and a robust solution
is preferable. We then show both analytically and by simulation that the
GST-TCM designed for slow fading channels are indeed robust to all block fading
channel conditions."
"A tree decomposition of the coordinates of a code is a mapping from the
coordinate set to the set of vertices of a tree. A tree decomposition can be
extended to a tree realization, i.e., a cycle-free realization of the code on
the underlying tree, by specifying a state space at each edge of the tree, and
a local constraint code at each vertex of the tree. The constraint complexity
of a tree realization is the maximum dimension of any of its local constraint
codes. A measure of the complexity of maximum-likelihood decoding for a code is
its treewidth, which is the least constraint complexity of any of its tree
realizations.
  It is known that among all tree realizations of a code that extends a given
tree decomposition, there exists a unique minimal realization that minimizes
the state space dimension at each vertex of the underlying tree. In this paper,
we give two new constructions of these minimal realizations. As a by-product of
the first construction, a generalization of the state-merging procedure for
trellis realizations, we obtain the fact that the minimal tree realization also
minimizes the local constraint code dimension at each vertex of the underlying
tree. The second construction relies on certain code decomposition techniques
that we develop. We further observe that the treewidth of a code is related to
a measure of graph complexity, also called treewidth. We exploit this
connection to resolve a conjecture of Forney's regarding the gap between the
minimum trellis constraint complexity and the treewidth of a code. We present a
family of codes for which this gap can be arbitrarily large."
"The problem of channel code design for the $M$-ary input AWGN channel with
additive $Q$-ary interference where the sequence of i.i.d. interference symbols
is known causally at the encoder is considered. The code design criterion at
high SNR is derived by defining a new distance measure between the input
symbols of the Shannon's \emph{associated} channel. For the case of
binary-input channel, i.e., M=2, it is shown that it is sufficient to use only
two (out of $2^Q$) input symbols of the \emph{associated} channel in the
encoding as far as the distance spectrum of code is concerned. This reduces the
problem of channel code design for the binary-input AWGN channel with known
interference at the encoder to design of binary codes for the binary symmetric
channel where the Hamming distance among codewords is the major factor in the
performance of the code."
"This paper investigates downlink transmission over a quasi-static fading
Gaussian broadcast channel (BC), to model delay-sensitive applications over
slowly time-varying fading channels. System performance is characterized by
outage achievable rate regions. In contrast to most previous work, here the
problem is studied under the key assumption that the transmitter only knows the
probability distributions of the fading coefficients, but not their
realizations. For scalar-input channels, two coding schemes are proposed. The
first scheme is called blind dirty paper coding (B-DPC), which utilizes a
robustness property of dirty paper coding to perform precoding at the
transmitter. The second scheme is called statistical superposition coding
(S-SC), in which each receiver adaptively performs successive decoding with the
process statistically governed by the realized fading. Both B-DPC and S-SC
schemes lead to the same outage achievable rate region, which always dominates
that of time-sharing, irrespective of the particular fading distributions. The
S-SC scheme can be extended to BCs with multiple transmit antennas."
"We consider the effects of Rayleigh fading and lognormal shadowing in the
physical interference model for all the successful transmissions of traffic
across the network. New bounds are derived for the capacity of a given random
ad hoc wireless network that reflect packet drop or capture probability of the
transmission links. These bounds are based on a simplified network topology
termed as honey-comb topology under a given routing and scheduling scheme."
"The ""water-filling"" solution for the quadratic rate-distortion function of a
stationary Gaussian source is given in terms of its power spectrum. This
formula naturally lends itself to a frequency domain ""test-channel""
realization. We provide an alternative time-domain realization for the
rate-distortion function, based on linear prediction. This solution has some
interesting implications, including the optimality at all distortion levels of
pre/post filtered vector-quantized differential pulse code modulation (DPCM),
and a duality relationship with decision-feedback equalization (DFE) for
inter-symbol interference (ISI) channels."
"In wireless networks with random node distribution, the underlying point
process model and the channel fading process are usually considered separately.
A unified framework is introduced that permits the geometric characterization
of fading by incorporating the fading process into the point process model.
Concretely, assuming nodes are distributed in a stationary Poisson point
process in $\R^d$, the properties of the point processes that describe the path
loss with fading are analyzed. The main applications are connectivity and
broadcasting."
"In this article we focus on the problem of channel decoding in presence of
a-priori information. In particular, assuming that the a-priori information
reliability is not perfectly estimated at the receiver, we derive a novel
analytical framework for evaluating the decoder's performance. It is derived
the important result that a ""good code"", i.e., a code which allows to fully
exploit the potential benefit of a-priori information, must associate
information sequences with high Hamming weights to codewords with low Hamming
weights. Basing on the proposed analysis, we analyze the performance of
convolutional codes, random codes, and turbo codes. Moreover, we consider the
transmission of correlated binary sources from independent nodes, a problem
which has several practical applications, e.g. in the case of sensor networks.
In this context, we propose a very simple joint source-channel turbo decoding
scheme where each decoder works by exploiting a-priori information given by the
other decoder. In the case of block fading channels, it is shown that the
inherent correlation between information signals provide a form of
non-cooperative diversity, thus allowing joint source-channel decoding to
outperform separation-based schemes."
"We characterize the affine-invariant maximal extended cyclic codes. Then by
the CSS construction, we derive from these codes a family of pure quantum
codes. Also for ordnq even, a new family of degenerate quantum stabilizer codes
is derived from the classical duadic codes. This answer an open problem asked
by Aly et al."
"A pattern of a sequence is a sequence of integer indices with each index
describing the order of first occurrence of the respective symbol in the
original sequence. In a recent paper, tight general bounds on the block entropy
of patterns of sequences generated by independent and identically distributed
(i.i.d.) sources were derived. In this paper, precise approximations are
provided for the pattern block entropies for patterns of sequences generated by
i.i.d. uniform and monotonic distributions, including distributions over the
integers, and the geometric distribution. Numerical bounds on the pattern block
entropies of these distributions are provided even for very short blocks. Tight
bounds are obtained even for distributions that have infinite i.i.d. entropy
rates. The approximations are obtained using general bounds and their
derivation techniques. Conditional index entropy is also studied for
distributions over smaller alphabets."
"The analysis of random coding error exponents pertaining to erasure/list
decoding, due to Forney, is revisited. Instead of using Jensen's inequality as
well as some other inequalities in the derivation, we demonstrate that an
exponentially tight analysis can be carried out by assessing the relevant
moments of a certain distance enumerator. The resulting bound has the following
advantages: (i) it is at least as tight as Forney's bound, (ii) under certain
symmetry conditions associated with the channel and the random coding
distribution, it is simpler than Forney's bound in the sense that it involves
an optimization over one parameter only (rather than two), and (iii) in certain
special cases, like the binary symmetric channel (BSC), the optimum value of
this parameter can be found in closed form, and so, there is no need to conduct
a numerical search. We have not found yet, however, a numerical example where
this new bound is strictly better than Forney's bound. This may provide an
additional evidence to support Forney's conjecture that his bound is tight for
the average code. We believe that the technique we suggest in this paper can be
useful in simplifying, and hopefully also improving, exponential error bounds
in other problem settings as well."
"An interference alignment example is constructed for the deterministic
channel model of the $K$ user interference channel. The deterministic channel
example is then translated into the Gaussian setting, creating the first known
example of a fully connected Gaussian $K$ user interference network with single
antenna nodes, real, non-zero and contant channel coefficients, and no
propagation delays where the degrees of freedom outerbound is achieved. An
analogy is drawn between the propagation delay based interference alignment
examples and the deterministic channel model which also allows similar
constructions for the 2 user $X$ channel as well."
"We consider a MIMO fading broadcast channel and compute achievable ergodic
rates when channel state information is acquired at the receivers via downlink
training and it is provided to the transmitter by channel state feedback.
Unquantized (analog) and quantized (digital) channel state feedback schemes are
analyzed and compared under various assumptions. Digital feedback is shown to
be potentially superior when the feedback channel uses per channel state
coefficient is larger than 1. Also, we show that by proper design of the
digital feedback link, errors in the feedback have a minor effect even if
simple uncoded modulation is used on the feedback channel. We discuss first the
case of an unfaded AWGN feedback channel with orthogonal access and then the
case of fading MIMO multi-access (MIMO-MAC). We show that by exploiting the
MIMO-MAC nature of the uplink channel, a much better scaling of the feedback
channel resource with the number of base station antennas can be achieved.
Finally, for the case of delayed feedback, we show that in the realistic case
where the fading process has (normalized) maximum Doppler frequency shift 0 < F
< 1/2, a fraction 1 - 2F of the optimal multiplexing gain is achievable. The
general conclusion of this work is that very significant downlink throughput is
achievable with simple and efficient channel state feedback, provided that the
feedback link is properly designed."
"Suppose a string $X_1^n=(X_1,X_2,...,X_n)$ generated by a memoryless source
$(X_n)_{n\geq 1}$ with distribution $P$ is to be compressed with distortion no
greater than $D\geq 0$, using a memoryless random codebook with distribution
$Q$. The compression performance is determined by the ``generalized asymptotic
equipartition property'' (AEP), which states that the probability of finding a
$D$-close match between $X_1^n$ and any given codeword $Y_1^n$, is
approximately $2^{-n R(P,Q,D)}$, where the rate function $R(P,Q,D)$ can be
expressed as an infimum of relative entropies. The main purpose here is to
remove various restrictive assumptions on the validity of this result that have
appeared in the recent literature. Necessary and sufficient conditions for the
generalized AEP are provided in the general setting of abstract alphabets and
unbounded distortion measures. All possible distortion levels $D\geq 0$ are
considered; the source $(X_n)_{n\geq 1}$ can be stationary and ergodic; and the
codebook distribution can have memory. Moreover, the behavior of the matching
probability is precisely characterized, even when the generalized AEP is not
valid. Natural characterizations of the rate function $R(P,Q,D)$ are
established under equally general conditions."
"This paper proposes a relaying strategy for the multiple-relay network in
which each relay decodes a selection of transmitted messages by other
transmitting terminals, and forwards parities of the decoded codewords. This
protocol improves the previously known achievable rate of the
decode-and-forward (DF) strategy for multirelay networks by allowing relays to
decode only a selection of messages from relays with strong links to it. Hence,
each relay may have several choices as to which messages to decode, and for a
given network many different parity forwarding protocols may exist. A tree
structure is devised to characterize a class of parity forwarding protocols for
an arbitrary multirelay network. Based on this tree structure, closed-form
expressions for the achievable rates of these DF schemes are derived. It is
shown that parity forwarding is capacity achieving for new forms of degraded
relay networks."
"In recent work, Ozgur, Leveque, and Tse (2007) obtained a complete scaling
characterization of throughput scaling for random extended wireless networks
(i.e., $n$ nodes are placed uniformly at random in a square region of area
$n$). They showed that for small path-loss exponents $\alpha\in(2,3]$
cooperative communication is order optimal, and for large path-loss exponents
$\alpha > 3$ multi-hop communication is order optimal. However, their results
(both the communication scheme and the proof technique) are strongly dependent
on the regularity induced with high probability by the random node placement.
  In this paper, we consider the problem of characterizing the throughput
scaling in extended wireless networks with arbitrary node placement. As a main
result, we propose a more general novel cooperative communication scheme that
works for arbitrarily placed nodes. For small path-loss exponents $\alpha \in
(2,3]$, we show that our scheme is order optimal for all node placements, and
achieves exactly the same throughput scaling as in Ozgur et al. This shows that
the regularity of the node placement does not affect the scaling of the
achievable rates for $\alpha\in (2,3]$. The situation is, however, markedly
different for large path-loss exponents $\alpha >3$. We show that in this
regime the scaling of the achievable per-node rates depends crucially on the
regularity of the node placement. We then present a family of schemes that
smoothly ""interpolate"" between multi-hop and cooperative communication,
depending upon the level of regularity in the node placement. We establish
order optimality of these schemes under adversarial node placement for $\alpha
> 3$."
"Information embedding (IE) is the transmission of information within a host
signal subject to a distortion constraint. There are two types of embedding
methods, namely irreversible IE and reversible IE, depending upon whether or
not the host, as well as the message, is recovered at the decoder. In
irreversible IE, only the embedded message is recovered at the decoder, and in
reversible IE, both the message and the host are recovered at the decoder. This
paper considers combinations of irreversible and reversible IE in multiple
access channels (MAC) and physically degraded broadcast channels (BC)."
"We explore the degrees of freedom of $M\times N$ user wireless $X$ networks,
i.e. networks of $M$ transmitters and $N$ receivers where every transmitter has
an independent message for every receiver. We derive a general outerbound on
the degrees of freedom \emph{region} of these networks. When all nodes have a
single antenna and all channel coefficients vary in time or frequency, we show
that the \emph{total} number of degrees of freedom of the $X$ network is equal
to $\frac{MN}{M+N-1}$ per orthogonal time and frequency dimension.
Achievability is proved by constructing interference alignment schemes for $X$
networks that can come arbitrarily close to the outerbound on degrees of
freedom. For the case where either M=2 or N=2 we find that the outerbound is
exactly achievable. While $X$ networks have significant degrees of freedom
benefits over interference networks when the number of users is small, our
results show that as the number of users increases, this advantage disappears.
Thus, for large $K$, the $K\times K$ user wireless $X$ network loses half the
degrees of freedom relative to the $K\times K$ MIMO outerbound achievable
through full cooperation. Interestingly, when there are few transmitters
sending to many receivers ($N\gg M$) or many transmitters sending to few
receivers ($M\gg N$), $X$ networks are able to approach the $\min(M,N)$ degrees
of freedom possible with full cooperation on the $M\times N$ MIMO channel.
Similar to the interference channel, we also construct an example of a 2 user
$X$ channel with propagation delays where the outerbound on degrees of freedom
is achieved through interference alignment based on a simple TDMA strategy."
"For a certain class of functions, the distribution of the function values can
be calculated in the trellis or a sub-trellis. The forward/backward recursion
known from the BCJR algorithm is generalized to compute the moments of these
distributions. In analogy to the symbol probabilities, by introducing a
constraint at a certain depth in the trellis we obtain symbol moments. These
moments are required for an efficient implementation of the discriminated
belief propagation algorithm in [2], and can furthermore be utilized to compute
conditional entropies in the trellis.
  The moment computation algorithm has the same asymptotic complexity as the
BCJR algorithm. It is applicable to any commutative semi-ring, thus actually
providing a generalization of the Viterbi algorithm."
"The capacity of discrete-time, non-coherent, multipath fading channels is
considered. It is shown that if the delay spread is large in the sense that the
variances of the path gains do not decay faster than geometrically, then
capacity is bounded in the signal-to-noise ratio."
"We address single-user data transmission over a channel where the received
signal incurs interference from a finite number of users (interfering users)
that use single codebooks for transmitting their own messages. The receiver,
however, is allowed to decode interfering users' messages. This means the
signal transmitted from any interfering user is either decoded or considered as
noise at the receiver side. We propose the following method to obtain an
achievable rate for this channel. Assuming its own data is decoded
successfully, the receiver partitions the set of interfering users into two
disjoint subsets, namely the set of decodable users and the set of
non-decodable users. Then the transmitter's rate is chosen such that the
intended signal can be jointly decoded with the set of decodable users. To show
the strength of this method, we prove that for the additive Gaussian channel
with Gaussian interfering users, the Gaussian distribution is optimal and the
achievable rate is the capacity of this channel. To obtain the maximum
achievable rate, one needs to find the maximum decodable subset of interfering
users. Due to the large number of possible choices, having efficient algorithms
that find the set of decodable users with maximum cardinality is desired. To
this end, we propose an algorithm that enables the receiver to accomplish this
task in polynomial time."
"This paper considers the problem of selecting a subset of nodes in a two-hop
wireless network to act as relays in aiding the communication between the
source-destination pair. Optimal relay subset selection with the objective of
maximizing the overall throughput is a difficult problem that depends on
multiple factors including node locations, queue lengths and power consumption.
A partial decode-and-forward strategy is applied in this paper to improve the
tractability of the relay selection problem and performance of the overall
network.
  Note that the number of relays selected ultimately determines the performance
of the network. This paper benchmarks this performance by determining the net
diversity achieved using the relays selected and the partial decode-and-forward
strategy. This framework is subsequently used to further transform relay
selection into a simpler relay placement problem, and two proximity-based
approximation algorithms are developed to determine the appropriate set of
relays to be selected in the network. Other selection strategies such as random
relay selection and a greedy algorithm that relies on channel state information
are also presented. This paper concludes by showing that the proposed
proximity-based relay selection strategies yield near-optimal expected rates
for a small number of selected relays."
"Block diagonalization is a linear precoding technique for the multiple
antenna broadcast (downlink) channel that involves transmission of multiple
data streams to each receiver such that no multi-user interference is
experienced at any of the receivers. This low-complexity scheme operates only a
few dB away from capacity but requires very accurate channel knowledge at the
transmitter. We consider a limited feedback system where each receiver knows
its channel perfectly, but the transmitter is only provided with a finite
number of channel feedback bits from each receiver. Using a random quantization
argument, we quantify the throughput loss due to imperfect channel knowledge as
a function of the feedback level. The quality of channel knowledge must improve
proportional to the SNR in order to prevent interference-limitations, and we
show that scaling the number of feedback bits linearly with the system SNR is
sufficient to maintain a bounded rate loss. Finally, we compare our
quantization strategy to an analog feedback scheme and show the superiority of
quantized feedback."
"Compression algorithms and streaming algorithms are both powerful tools for
dealing with massive data sets, but many of the best compression algorithms --
e.g., those based on the Burrows-Wheeler Transform -- at first seem
incompatible with streaming. In this paper we consider several popular
streaming models and ask in which, if any, we can compress as well as we can
with the BWT. We first prove a nearly tight tradeoff between memory and
redundancy for the Standard, Multipass and W-Streams models, demonstrating a
bound that is achievable with the BWT but unachievable in those models. We then
show we can compute the related Schindler Transform in the StreamSort model and
the BWT in the Read-Write model and, thus, achieve that bound."
"Space-time codes leverage the availability of multiple antennas to enhance
the reliability of communication over wireless channels. While space-time codes
have initially been designed with a focus on open-loop systems, recent
technological advances have enabled the possibility of low-rate feedback from
the receiver to the transmitter. The focus of this paper is on the implications
of this feedback in a single-user multi-antenna system with a general model for
spatial correlation. We assume a limited feedback model, that is, a coherent
receiver and statistics along with B bits of quantized channel information at
the transmitter. We study space-time coding with a family of linear dispersion
(LD) codes that meet an additional orthogonality constraint so as to ensure
low-complexity decoding. Our results show that, when the number of bits of
feedback (B) is small, a space-time coding scheme that is equivalent to
beamforming and does not code across time is optimal in a weak sense in that it
maximizes the average received SNR. As B increases, this weak optimality
transitions to optimality in a strong sense which is characterized by the
maximization of the average mutual information. Thus, from a system designer's
perspective, our work suggests that beamforming may not only be attractive from
a low-complexity viewpoint, but also from an information-theoretic viewpoint."
"Since Tse and Verdu proved that the global maximum likelihood (GML) detector
achieves unit asymptotic multiuser efficiency (AME) in the limit of large
random spreading (LRS) CDMA, no suboptimal detector has been found to achieve
unit AME. In this letter, we obtain that the WSLAS detector with a linear
per-bit complexity achieves unit AME in the LRS-CDMA with a channel load < 1/2
- 1/(4ln2) bits/s/Hz. For a practical system with any user number, a quasi
LRS-CDMA is then proposed to approach the single-user performance in the high
SNR regime."
"In this paper, the bit error performance of a family of likelihood ascent
search (LAS) multiuser detectors is analyzed. An upper bound on the BER of any
LAS detector is obtained by bounding the fixed point region with the worst
initial detector. The concept of indecomposable errors developed by Verdu is
applied to tighten the upper bound. In a special instance, the upper bound is
reduced to that for all the local maximum likelihood detectors. The upper bound
is comparable with that of the optimum detector obtained by Verdu. A lower
bound on the asymptotic multiuser efficiency (AME) is then obtained. It is
shown that there are nontrivial CDMA channels such that a LAS detector can
achieve unit AME regardless of user number. The AME lower bound provides a
means for further seeking a good set of spreading sequences and power
distribution for spectral and power efficient CDMA."
"The arbitrarily varying channel (AVC) is a channel model whose state is
selected maliciously by an adversary. Fixed-blocklength coding assumes a
worst-case bound on the adversary's capabilities, which leads to pessimistic
results. This paper defines a variable-length perspective on this problem, for
which achievable rates are shown that depend on the realized actions of the
adversary. Specifically, rateless codes are constructed which require a limited
amount of common randomness. These codes are constructed for two kinds of AVC
models. In the first the channel state cannot depend on the channel input, and
in the second it can. As a byproduct, the randomized coding capacity of the AVC
with state depending on the transmitted codeword is found and shown to be
achievable with a small amount of common randomness. The results for this model
are proved using a randomized strategy based on list decoding."
"In this paper we apply different techniques of information distortion on a
set of classical books written in English. We study the impact that these
distortions have upon the Kolmogorov complexity and the clustering by
compression technique (the latter based on Normalized Compression Distance,
NCD). We show how to decrease the complexity of the considered books
introducing several modifications in them. We measure how the information
contained in each book is maintained using a clustering error measure. We find
experimentally that the best way to keep the clustering error is by means of
modifications in the most frequent words. We explain the details of these
information distortions and we compare with other kinds of modifications like
random word distortions and unfrequent word distortions. Finally, some
phenomenological explanations from the different empirical results that have
been carried out are presented."
"Code Division Multiple Access (CDMA) in which the signature code assignment
to users contains a random element has recently become a cornerstone of CDMA
research. The random element in the construction is particularly attractive in
that it provides robustness and flexibility in application, whilst not making
significant sacrifices in terms of multiuser efficiency. We present results for
sparse random codes of two types, with and without modulation. Simple
microscopic consideration on system samples would suggest differences in the
phase space of the two models, but we demonstrate that the thermodynamic
results and metastable states are equivalent in the minimum bit error rate
detector. We analyse marginal properties of interactions and also make
analogies to constraint satisfiability problems in order to understand
qualitative features of detection and metastable states. This may have
consequences for developing algorithmic methods to escape metastable states,
thus improving decoding performance."
"We consider the problem of minimizing upper bounds and maximizing lower
bounds on information rates of stationary and ergodic discrete-time channels
with memory. The channels we consider can have a finite number of states, such
as partial response channels, or they can have an infinite state-space, such as
time-varying fading channels. We optimize recently-proposed information rate
bounds for such channels, which make use of auxiliary finite-state machine
channels (FSMCs). Our main contribution in this paper is to provide iterative
expectation-maximization (EM) type algorithms to optimize the parameters of the
auxiliary FSMC to tighten these bounds. We provide an explicit, iterative
algorithm that improves the upper bound at each iteration. We also provide an
effective method for iteratively optimizing the lower bound. To demonstrate the
effectiveness of our algorithms, we provide several examples of partial
response and fading channels, where the proposed optimization techniques
significantly tighten the initial upper and lower bounds. Finally, we compare
our results with an improved variation of the \emph{simplex} local optimization
algorithm, called \emph{Soblex}. This comparison shows that our proposed
algorithms are superior to the Soblex method, both in terms of robustness in
finding the tightest bounds and in computational efficiency. Interestingly,
from a channel coding/decoding perspective, optimizing the lower bound is
related to increasing the achievable mismatched information rate, i.e., the
information rate of a communication system where the decoder at the receiver is
matched to the auxiliary channel, and not to the original channel."
"In cognitive radio (CR) networks, there are scenarios where the secondary
(lower priority) users intend to communicate with each other by
opportunistically utilizing the transmit spectrum originally allocated to the
existing primary (higher priority) users. For such a scenario, a secondary user
usually has to trade off between two conflicting goals at the same time: one is
to maximize its own transmit throughput; and the other is to minimize the
amount of interference it produces at each primary receiver. In this paper, we
study this fundamental tradeoff from an information-theoretic perspective by
characterizing the secondary user's channel capacity under both its own
transmit-power constraint as well as a set of interference-power constraints
each imposed at one of the primary receivers. In particular, this paper
exploits multi-antennas at the secondary transmitter to effectively balance
between spatial multiplexing for the secondary transmission and interference
avoidance at the primary receivers. Convex optimization techniques are used to
design algorithms for the optimal secondary transmit spatial spectrum that
achieves the capacity of the secondary transmission. Suboptimal solutions for
ease of implementation are also presented and their performances are compared
with the optimal solution. Furthermore, algorithms developed for the
single-channel transmission are also extended to the case of multi-channel
transmission whereby the secondary user is able to achieve opportunistic
spectrum sharing via transmit adaptations not only in space, but in time and
frequency domains as well."
"This paper investigates point-to-point information transmission over a
wideband slow-fading channel, modeled as an (asymptotically) large number of
independent identically distributed parallel channels, with the random channel
fading realizations remaining constant over the entire coding block. On the one
hand, in the wideband limit the minimum achievable energy per nat required for
reliable transmission, as a random variable, converges in probability to
certain deterministic quantity. On the other hand, the exponential decay rate
of the outage probability, termed as the wideband outage exponent,
characterizes how the number of parallel channels, {\it i.e.}, the
``bandwidth'', should asymptotically scale in order to achieve a target outage
probability at a target energy per nat. We examine two scenarios: when the
transmitter has no channel state information and adopts uniform transmit power
allocation among parallel channels; and when the transmitter is endowed with an
one-bit channel state feedback for each parallel channel and accordingly
allocates its transmit power. For both scenarios, we evaluate the wideband
minimum energy per nat and the wideband outage exponent, and discuss their
implication for system performance."
"Cognitive radios have been studied recently as a means to utilize spectrum in
a more efficient manner. This paper focuses on the fundamental limits of
operation of a MIMO cognitive radio network with a single licensed user and a
single cognitive user. The channel setting is equivalent to an interference
channel with degraded message sets (with the cognitive user having access to
the licensed user's message). An achievable region and an outer bound is
derived for such a network setting. It is shown that under certain conditions,
the achievable region is optimal for a portion of the capacity region that
includes sum capacity."