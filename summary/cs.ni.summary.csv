summary
"Recent network traffic studies argue that network arrival processes are much
more faithfully modeled using statistically self-similar processes instead of
traditional Poisson processes [LTWW94,PF95]. One difficulty in dealing with
self-similar models is how to efficiently synthesize traces (sample paths)
corresponding to self-similar traffic. We present a fast Fourier transform
method for synthesizing approximate self-similar sample paths for one type of
self-similar process, Fractional Gaussian Noise, and assess its performance and
validity. We find that the method is as fast or faster than existing methods
and appears to generate close approximations to true self-similar sample paths.
We also discuss issues in using such synthesized sample paths for simulating
network traffic, and how an approximation used by our method can dramatically
speed up evaluation of Whittle's estimator for H, the Hurst parameter giving
the strength of long-range dependence present in a self-similar time series."
"Multipoint capabilities are essential for ATM networks to efficiently support
many applications, including IP multicasting and overlay applications. The
current signaling and routing specifications for ATM define point-to-multipoint
capabilities. Multipoint-to-point connection support is also being discussed by
the signaling and PNNI groups, and will be defined in the near future for the
unspecified bit rate (UBR) service. We examine point-to-multipoint and
multipoint-to-point flow control for the available bit rate (ABR) service, as
discussed in the traffic management working group."
"An explicit rate switch scheme monitors the load at each link and gives
feedback to the sources. We define the overload factor as the ratio of the
input rate to the available capacity. In this paper, we present four overload
based ABR switch schemes which provide MCR guarantees. The switch schemes
proposed use the overload factor and other quantities to calculate feedback
rates. A dynamic queue control mechanism is used to achieve efficient usage of
the link, control queues and, achieve constant queuing delay at steady state.
The proposed algorithms are studied and compared using several configurations.
The configurations were chosen to test the performance of the algorithms in
presence of link bottlenecks, source bottlenecks and transient sources. A
comparison of the proposed algorithms based on the simulation results is
presented."
"The main goals of a switch scheme are high utilization, low queuing delay and
fairness. To achieve high utilization the switch scheme can maintain non-zero
(small) queues in steady state which can be used if the sources do not have
data to send. Queue length (delay) can be controlled if part of the link
capacity is used for draining queues in the event of queue build up. In most
schemes a simple threshold function is used for queue control. Better control
of the queue and hence delay can be achieved by using sophisticated queue
control functions. It is very important to design and analyze such queue
control functions. We study step, linear, hyperbolic and inverse hyperbolic
queue control functions. Analytical explanation and simulation results
consistent with analysis are presented. From the study, we conclude that
inverse hyperbolic is the best control function and to reduce complexity the
linear control function can be used since it performs satisfactorily in most
cases."
"In this paper we give a general definition of weighted fairness and show how
this can achieve various fairness definitions, such as those mentioned in the
ATM Forum TM 4.0 Specifications. We discuss how a pricing policy can be mapped
to general weighted (GW) fairness. The GW fairness can be achieved by
calculating the $ExcessFairshare$ (weighted fairshare of the left over
bandwidth) for each VC. We show how a switch algorithm can be modified to
support the GW fairness by using the $ExcessFairshare$. We use ERICA+ as an
example switch algorithm and show how it can be modified to achieve the general
fairness. Simulations results are presented to demonstrate that the modified
switch algorithm achieves GW fairness. An analytical proof for convergence of
the modified ERICA+ algorithm is given in the appendix."
"ATM (asynchronous transfer mode) is the technology chosen for the Broadband
Integrated Services Digital Network (B-ISDN). The ATM ABR (available bit rate)
service can be used to transport ``best-effort'' traffic. In this paper, we
extend our earlier work on the buffer requirements problem for TCP over ABR.
Here, a worst case scenario is generated such that TCP sources send a burst of
data at the time when the sources have large congestion windows and the ACRs
(allowed cell rates) for ABR are high. We find that ABR using the ERICA+ switch
algorithm can control the maximum queue lengths (hence the buffer requirements)
even for the worst case. We present analytical arguments for the expected queue
length and simulation results for different number of sources values and
parameter values."
"Compressed video is well known to be self-similar in nature. We model VBR
carrying Long-Range Dependent (LRD), multiplexed MPEG-2 video sources. The
actual traffic for the model is generated using fast-fourier transform of
generate the fractional gaussian noise (FGN) sequence. Our model of compressed
video sources bears similarity to an MPEG-2 Transport Stream carrying video,
i.e., it is long-range dependent and generates traffic in a piecewise-CBR
fashion. We study the effect of such VBR traffic on ABR carrying TCP traffic.
The effect of such VBR traffic is that the ABR capacity is highly variant. We
find that a switch algorithm like ERICA+ can tolerate this variance in ABR
capacity while maintaining high throughput and low delay. We present simulation
results for terrestrial and satellite configurations."
"In multipoint-to-point connections, the traffic at the root (destination) is
the combination of all traffic originating at the leaves. A crucial concern in
the case of multiple senders is how to define fairness within a multicast group
and among groups and point-to-point connections. Fairness definition can be
complicated since the multipoint connection can have the same identifier
(VPI/VCI) on each link, and senders might not be distinguishable in this case.
Many rate allocation algorithms implicitly assume that there is only one sender
in each VC, which does not hold for multipoint-to-point cases. We give various
possibilities for defining fairness for multipoint connections, and show the
tradeoffs involved. In addition, we show that ATM bandwidth allocation
algorithms need to be adapted to give fair allocations for multipoint-to-point
connections."
"Asynchronous transfer mode (ATM) is the new generation of computer and
communication networks that are being deployed throughout the telecommunication
industry as well as in campus backbones. ATM technology distinguishes itself
from the previous networking protocols in that it has the latest traffic
management technology and thus allows guaranteeing delay, throughput, and other
performance measures. This in turn, allows users to integrate voice, video, and
data on the same network. Available bit rate (ABR) service in ATM has been
designed to fairly distribute all unused capacity to data traffic and is
specified in the ATM Forum's Traffic Management (TM4.0) standard. This paper
will describe the OPNET models that have been developed for ATM and ABR design
and analysis."
"In this paper we describe a hands-on laboratory oriented instructional
package that we have developed for data communications and networking. The
package consists of a software tool, together with instructional material for a
laboratory based networking curriculum. The software is based on a simulation
environment that enables the student to experiment with various networking
protocols, on an easy to use graphical user interface (GUI). Data message
flows, packet losses, control/routing message flows, virtual circuit setups,
link failures, bit errors etc., are some of the features that can be visualized
in this environment. The student can also modify the networking components
provided, as well as add new components using the C programming language. The
instructional material consists of a set of laboratory exercises for flow and
error control (HDLC), IEEE 802.3 CSMA/CD protocol, the token ring protocol,
interconnecting LANs via bridges, TCP congestion avoidance and control, IP
fragmentation and reassembly, ATM PNNI routing and ATM policing. The laboratory
exercises have facilitated the development of a networking curriculum based on
both the traditional computer networking principles, as well as the new
technologies in telecommunication networking. The laboratory environment has
been used in the networking curriculum at The Ohio State University, and is
being piloted at other universities. The entire package is freely available
over the Internet."
"In this paper we present a model to study the end-to-end delay performance of
a satellite-ATM netowrk. We describe a satellite-ATM network architecture. The
architecture presents a trade-off between the on-board switching/processing
features and the complexity of the satellite communication systems. The
end-to-end delay of a connection passing through a satellite constellation
consists of the transmission delay, the uplink and downlink ground
terminal-satellite propagation delay, the inter-satellite link delays, the
on-board switching, processing and buffering delays. In a broadband satellite
network, the propagation and the buffering delays have the most impact on the
overall delay. We present an analysis of the propagation and buffering delay
components for GEO and LEO systems. We model LEO constellations as satellites
evenly spaced in circular orbits around the earth. A simple routing algorithm
for LEO systems calculates locally optimal paths for the end-to-end connection.
This is used to calculate the end-to-end propagation delays for LEO networks.
We present a simulation model to calculate the buffering delay for TCP/IP
traffic over ATM ABR and UBR service categories. We apply this model to
calculate total end-to-end delays for TCP/IP over satellite-ATM networks."
"In this paper we study the design issues in improving TCP performance over
the ATM UBR service. ATM-UBR switches respond to congestion by dropping cells
when their buffers become full. TCP connections running over UBR can experience
low throughput and high unfairness. Intelligent switch drop policies and
end-system policies can improve the performance of TCP over UBR with limited
buffers. We describe the various design options available to the network as
well as to the end systems to improve TCP performance over UBR. We study the
effects of Early Packet Discard, and two per-VC accounting based buffer
management policies. We also study the effects of various TCP end system
congestion control policies including slow start and congestion avoidance, fast
retransmit and recovery and selective acknowledgments. We present simulation
results for various small and large latency configurations with varying buffer
sizes and number of sources."
"Recent enhancements have been proposed to the ATM Unspecified Bit Rate (UBR)
service that guarantee a minimum rate at the frame level to the UBR VCs. These
enhancements have been called Guaranteed Frame Rate (GFR). In this paper, we
discuss the motivation, design and implementation issues for GFR. We present
the design of buffer management and policing mechanisms to implement GFR. We
study the effects of policing, per-VC buffer allocation, and per-VC queuing on
providing GFR to TCP/IP traffic. We conclude that per-VC scheduling is
necessary to provide minimum rate guarantees to TCP traffic. We examine the
role of frame tagging in the presence of scheduling and buffer management for
providing minumum rate guarantees. The use of GFR to support the Internet
Controlled Load Service is also discussed."
"The ATM Guaranteed Frame Rate (GFR) service is intended for best effort
traffic that can benefit from minimum throughput guarantees. Edge devices
connecting LANs to an ATM network can use GFR to transport multiple TCP/IP
connections over a single GFR VC.These devices would typically multiplex VCs
into a single FIFO queue. It has been shown that in general, FIFO queuing is
not sufficient to provide rate guarantees, and per-VC queuing with scheduling
is needed. We show that under conditions of low buffer allocation, it is
possible to control TCP rates with FIFO queuing and buffer management. We
present analysis and simulation results on controlling TCP rates by buffer
management. We present a buffer management policy that provides loose rate
guarantees to SACK TCP sources when the total buffer allocation is low. We
study the performance of this buffer management scheme by simulation."
"In performance analysis and design of communication netword modeling data
traffic is important. With introduction of new applications, the
characteristics of the data traffic changes. We present a brief review the
different models of data traffic and how they have evolved. We present results
of data traffic analysis and simulated traffic, which demonstrates that the
packet train model fits the traffic at source destination level and long-memory
(self-similar) model fits the traffic at the aggregate level."
"The ABR service is designed to fairly allocate the bandwidth unused by higher
priority services. The network indicates to the ABR sources the rates at which
they should transmit to minimize their cell loss. Switches must constantly
measure the demand and available capacity, and divide the capacity fairly among
the contending connections. In order to compute the fair and efficient
allocation for each connection, a switch needs to determine the effective
number of active connections. In this paper, we propose a method for
determining the number of active connections and the fair bandwidth share for
each. We prove the efficiency and fairness of the proposed method analytically,
and simulate it by incorporating it into the ERICA switch algorithm."
"The OSU scheme is a rate-based congestion avoidance scheme for ATM networks
using explicit rate indication. This work was one of the first attempts to
define explicit rate switch mechanisms and the Resource Management (RM) cell
format in Asynchronous Transfer Mode (ATM) networks. The key features of the
scheme include explicit rate feedback, congestion avoidance, fair operation
while maintaining high utilization, use of input rate as a congestion metric,
O(1) complexity. This paper presents an overview of the scheme, presents those
features of the scheme that have now become common features of other switch
algorithms and discusses three extensions of the scheme."
"We propose an explicit rate indication scheme for congestion avoidance in ATM
networks. In this scheme, the network switches monitor their load on each link,
determining a load factor, the available capacity, and the number of currently
active virtual channels. This information is used to advise the sources about
the rates at which they should transmit. The algorithm is designed to achieve
efficiency, fairness, controlled queueing delays, and fast transient response.
The algorithm is also robust to measurement errors caused due to variation in
ABR demand and capacity. We present performance analysis of the scheme using
both analytical arguments and simulation results. The scheme is being
implemented by several ATM switch manufacturers."
"In this paper, we have provided a summary of the design options in
Satellite-ATM technology. A satellite ATM network consists of a space segment
of satellites connected by inter-satellite crosslinks, and a ground segment of
the various ATM networks. A satellite-ATM interface module connects the
satellite network to the ATM networks and performs various call and control
functions. A network control center performs various network management and
resource allocation functions. Several issues such as the ATM service model,
media access protocols, and traffic management issues must be considered when
designing a satellite ATM network to effectively transport Internet traffic. We
have presented the buffer requirements for TCP/IP traffic over ATM-UBR for
satellite latencies. Our results are based on TCP with selective
acknowledgments and a per-VC buffer management policy at the switches. A buffer
size of about 0.5 * RTT to 1 * RTT is sufficient to provide over 98% throughput
to infinite TCP traffic for long latency networks and a large number of
sources. This buffer requirement is independent of the number of sources. The
fairness is high for a large numbers of sources because of the per-VC buffer
management performed at the switches and the nature of TCP traffic."
"We model World Wide Web (WWW) servers and clients running over an ATM network
using the ABR (available bit rate) service. The WWW servers are modeled using a
variant of the SPECweb96 benchmark, while the WWW clients are based on a model
by Mah. The traffic generated by this application is typically bursty, i.e., it
has active and idle periods in transmission. A timeout occurs after given
amount of idle period. During idle period the underlying TCP congestion windows
remain open until a timeout expires. These open windows may be used to send
data in a burst when the application becomes active again. This raises the
possibility of large switch queues if the source rates are not controlled by
ABR. We study this problem and show that ABR scales well with a large number of
bursty TCP sources in the system."
"ABR traffic management for point-to-multipoint connections controls the
source rate to the minimum rate supported by all the branches of the multicast
tree. A number of algorithms have been developed for extending ABR congestion
avoidance algorithms to perform feedback consolidation at the branch points.
This paper discusses various design options and implementation alternatives for
the consolidation algorithms, and proposes a number of new algorithms. The
performance of the proposed algorithms and the previous algorithms is compared
under a variety of conditions. Results indicate that the algorithms we propose
eliminate the consolidation noise (caused if the feedback is returned before
all branches respond), while exhibiting a fast transient response."
"We study the performance of Selective Acknowledgments with TCP over the
ATM-UBR service category. We examine various UBR drop policies, TCP mechanisms
and network configurations to recommend optimal parameters for TCP over UBR. We
discuss various TCP congestion control mechanisms compare their performance for
LAN and WAN networks. We describe the effect of satellite delays on TCP
performance over UBR and present simulation results for LAN, WAN and satellite
networks. SACK TCP improves the performance of TCP over UBR, especially for
large delay networks. Intelligent drop policies at the switches are an
important factor for good performance in local area networks."
"Asynchronous transfer mode (ATM) networks must define multicast capabilities
in order to efficiently support numerous applications, such as video
conferencing and distributed applications, in addition to LAN emulation (LANE)
and Internet protocol (IP) multicasting. Several problems and issues arise in
ATM multicasting, such as signaling, routing, connection admission control, and
traffic management problems. IP integrated services over ATM poses further
challenges to ATM multicasting. Scalability and simplicity are the two main
concerns for ATM multicasting. This paper provides a survey of the current work
on multicasting problems in general, and ATM multicasting in particular. A
number of proposed schemes is examined, such as the schemes MARS, MCS, SEAM,
SMART, RSVP, and various multipoint traffic management and transport-layer
schemes. The paper also indicates a number of key open issues that remain
unresolved."
"The testing group at ATM Forum is working on developing a specification for
performance testing of ATM switches and networks. The emphasis is on the user
perceived frame-level performance. This paper explains what is different about
this new effort and gives its status."
"The Available Bit Rate (ABR) service in ATM networks has been specified to
allow fair and efficient support of data applications over ATM utilizing
capacity left over after servicing higher priority classes. One of the
architectural features in the ABR specification [tm4] is the Virtual
Source/Virtual Destination (VS/VD) option. This option allows a switch to
divide an end-to-end ABR connection into separately controlled ABR segments by
acting like a destination on one segment, and like a source on the other. The
coupling in the VS/VD switch between the two ABR control segments is
implementation specific. In this paper, we model a VS/VD ATM switch and study
the issues in designing coupling between ABR segments. We identify a number of
implementation options for the coupling. A good choice significantly improves
the stability and transient performance of the system and reduces the buffer
requirements at the switches."
"The Available Bit Rate (ABR) service has been developed to support 21st
century data applications over Asynchronous Transfer Mode (ATM). The ABR
service uses a closed-loop rate-based traffic management framework where the
network divides left-over bandwidth among contending sources. The ATM Forum
traffic management group also incorporated open-loop control capabilities to
make the ABR service robust to temporary network failures and source
inactivity. An important problem addressed was whether rate allocations of
sources should be taken away if sources do not use them. The proposed
solutions, popularly known as the Use-It-or-Lose-It (UILI) policies, have had
significant impact on the ABR service capabilities. In this paper we discuss
the design, development, and the final shape of these policies and their impact
on the ABR service. We compare the various alternatives through a performance
evaluation."
"ATM-UBR switches respond to congestion by dropping cells when their buffers
become full. TCP connections running over UBR experience low throughput and
high unfairness. For 100% TCP throughput each switch needs buffers equal to the
sum of the window sizes of all the TCP connections. Intelligent drop policies
can improve the performance of TCP over UBR with limited buffers. The UBR+
service proposes enhancements to UBR for intelligent drop. Early Packet Discard
improves throughput but does not attempt to improve fairness. Selective packet
drop based on per-connection buffer occupancy improves fairness. The Fair
Buffer Allocation scheme further improves both throughput and fairness."
"We extend our earlier studies of buffer requirements of TCP over ABR in two
directions. First, we study the performance of TCP over ABR in an ATM backbone.
On the backbone, the TCP queues are at the edge router and not inside the ATM
network. The router requires buffer equal to the sum of the receiver window
sizes of the participating TCP connections. Second, we introduce various
patterns of VBR background traffic. The VBR background introduces variance in
the ABR capacity and the TCP traffic introduces variance in the ABR demand.
Some simple switch schemes are unable to keep up with the combined effect of
highly varying demands and highly varying ABR capacity. We present our
experiences with refining the ERICA+ switch scheme to handle these conditions."
"The Asynchronous Transfer Mode (ATM) networks are quickly being adopted as
backbones over various parts of the Internet. This paper analyzes the
performance of TCP/IP protocols over ATM network's Available Bit Rate (ABR) and
Unspecified Bit Rate (UBR) services. It is shown that ABR pushes congestion to
the edges of the ATM network while UBR leaves it inside the ATM portion."
"We study the buffering requirements for zero cell loss for TCP/IP over
satellite links using the available bit rate (ABR) and unspecified bit rate
(UBR) services of asynchronous transfer mode (ATM) networks. For the ABR
service, we explore the effect of feedback delay (a factor which depends upon
the position of the bottleneck), the switch scheme used, and background
variable bit rate (VBR) traffic. It is shown that the buffer requirement for
TCP over ABR is independent of the number of TCP sources, but depends on the
aforementioned factors. For the UBR service, we show that the buffer
requirement is the sum of the TCP receiver window sizes. We substantiate our
arguments with simulation results."
"During the design of ABR traffic management at the ATM Forum, we performed
several analyses to ensure that the ABR service will operate efficiently over
satellite links. In the cases where the performance was unacceptable, we
suggested modifications to the traffic management specifications. This paper
describes one such issue related to the count of missing resource management
cells (Crm) parameter of the ABR source behavior. The analysis presented here
led to the changes which are now part of the ATM traffic management (TM 4.0)
specification. In particular, the size of the transient buffer exposure (TBE)
parameter was set to 24 bits, and no size was enforced for the Crm parameter.
This simple change improved the throughput over OC-3 satellite links from 45
Mbps to 140 Mbps."
"Satellite communication systems are the means of realizing a global broadband
integrated services digital network. Due to the statistical nature of the
integrated services traffic, the resulting rate fluctuations and burstiness
render congestion control a complicated, yet indispensable function. The long
propagation delay of the earth-satellite link further imposes severe demands
and constraints on the congestion control schemes, as well as the media access
control techniques and retransmission protocols that can be employed in a
satellite network. The problems in designing satellite network protocols, as
well as some of the solutions proposed to tackle these problems, will be the
primary focus of this survey."
"The Available Bit Rate (ABR) service has been developed to support data
applications over Asynchronous Transfer Mode (ATM) networks. The network
continuously monitors its traffic and provides feedback to the source end
systems. This paper explains the rules that the sources have to follow to
achieve a fair and efficient allocation of network resources."
"We study the buffering requirements for zero cell loss for TCP over ABR. We
show that the maximum buffers required at the switch is proportional to the
maximum round trip time (RTT) of all VCs through the link. The number of
round-trips depends upon the the switch algorithm used. With our ERICA
[erica-final] switch algorithm, we find that the buffering required is
independent of the number of TCP sources. We substantiate our arguments with
simulation results."
"The main goal of this study was to survey current applications of GPS to
distributed systems and networks. Detailed lists of GPS products, current
applications, addresses of manufacturers, and sources for further information
are included in this report."
"An explicit rate indication scheme for congestion avoidance in computer and
telecommunication networks is proposed. The sources monitor their load and
provide the information periodically to the switches. The switches, in turn,
compute the load level and ask the sources to adjust their rates up or down.
The scheme achieves high link utilization, fair allocation of rates among
contending sources and provides quick convergence. A backward congestion
notification option is also provided. The conditions under which this option is
useful are indicated."
"As the speed and the dynamic range of computer networks evolve, the issue of
efficient traffic management becomes increasingly important. This work
describes an approach to traffic management using explicit rate information
provided to the source by the network. We present an asynchronous distributed
algorithm for optimal rate calculation across the network, where optimality is
understood in the maxmin sense. The algorithm quickly converges to the optimal
rates and is shown to be well-behaved in transience."
"Asynchronous Transfer Mode (ATM) has emerged as the most promising technology
in supporting future broadband multimedia communication services. To accelerate
the deployment of ATM technology, the ATM Forum, which is a consortium of
service providers and equipment vendors in the communication industries has
been created to develop implementation and specification agreements. In this
article, we present a brief overview on ATM protocol layers and current
progress on LAN Emulation and Traffic Management in the ATM Forum."
"The paper begins with a discussion of current trends in networking and a
historical reviews of past networking technologies some of which failed. This
leads us to the discussion about what it takes for a new technology to succeed
and what challenges we face in making the current dream of a seamless
world-wide high-speed ATM network a reality.
  Issues in using ATM cells for very high speed applications are presented.
Ensuring that the users benefit from ATM networks involves several other
related disciplines. These are reviewed."
"Congestion control mechanisms for ATM networks as selected by the ATM Forum
traffic management group are described. Reasons behind these selections are
explained. In particular, selection criteria for selection between rate-based
and credit-based approach and the key points of the debate between the two
approaches are presented. The approach that was finally selected and several
other schemes that were considered are described."
"Key issues in upcoming FDDI standards including low-cost fiber, twisted-pair,
SONET mapping, and FDDI follow-on LAN are discussed after a brief introduction
to FDDI and FDDI-II"
"Using a trace of address references, we compared the efficiency of several
different hashing functions, such as cyclic redundancy checking (CRC)
polynomials, Fletcher checksum, folding of address octets using the
exclusive-or operation and bit extraction from the address. Guidelines are
provided for determining the size of the hashmark required to achieve a
specified level of performance."
"Weaknesses in several recently proposed ideas about congestion control and
avoidance in high-speed netwroks are identified. Both sides of the debate
concerning prior reservation of resources versus walk-in service, open-loop
control versus feedback control, rate control versus window control, and
router-based control versus source-based control are presented. The
circumstances under which backpressure is useful or not are discussed, and it
is argued that a single congestion scheme is not sufficient, but that a
combination of several schemes is required for complete congestion management
in a network."
"The performance of Fiber-Distributed Data Interface (FDDI) depends upon
several workload parameters; for example; the arrival pattern, frame size, and
configuration parameters, such as the number of stations on the ring, extent of
the ring, and number of stations that are waiting to transmit. In addition, the
performance is affected by a parameter called the Target Token Rotation Time
(TTRT), which can be controlled by the network manager. We considered the
effect of TTRT on various performance metrics for different ring configurations
and concluded that a TTRT value of 8 ms provides a good performance over a wide
range of configurations and workloads."
"Fiber Distributed Data Interface (FDDI) is a 100 megabits per second fiber
optic local area network (LAN) standard being developed by the American
National Standard Institute (ANSI).
  We analyze the impact of various design decisions on the error detection
capability of the protocol. In particular, we quantify frame error rate, token
loss rate, and undetected error rate. Several characteristics of the 32-bit
frame check sequence (FCS) polynomial, which is also used in IEEE 802 LAN
protocols, are discussed."
"Popular myths that cheaper memory, high-speed links and high-speed processors
will solve the problem of congestion in computer networks are shown to be
false. A simple definition for congestion based on supply and demand of
resources is proposed and is then used to classify various congestion schemes.
The issues that make the congestion problem a difficult one are discussed, and
then the architectural decisions that affect the design of a congestion scheme
are presented. It is argued that long-, medium- and short-term congestion
problems require different solutions. Some of the recent schemes are brifly
surveyed, and areas for further research are discussed."
"The size of computer networks, along with their bandwidths, is growing
exponentially. To support these large, high-speed networks, it is neccessary to
be able to forward packets in a few microseconds. One part of the forwarding
operation consists of searching through a large address databse. This problem
is encountered in the design of bridges, routers, gateways and name servers.
  Caching can reduce the lookup time if there is a locality in the address
reference pattern. Using a destination reference trace measured on an extended
local are a network, we attempt to see if the destination refernces do have a
significant locality.
  We compared the performance of MIN, LRU, FIFO, and random cache replacement
algorithms. We found that the interactive (terminal) traffic in our sample had
quite different locality behavior than that of the noninteractive traffic. The
interactive traffic did not follow the LRU stack model while the
noninteractivetraffic did. Examples are shown of the environments in which
caching can help as well as those in which caching can hurt, unless the cache
size is large."
"In heterogeneous networks, achieving congestion avoidance is difficult
because the congestion feedback from one subnetwork may have no meaning to
source on other other subnetworks. We propose using changes in round-trip delay
as an implicit feedback. Using a black-box model of the network, we derive an
expression for the optimal window as a function of the gradient of the
delay-window curve.
  The problems of selfish optimum and social optimum are also addressed. It is
shown that without a careful design, it is possible to get into a race
condition during heavy congestion, where each user wants more resources than
others, thereby leading to a diverging congestion
  It is shown that congestion avoidance using round trip delay is a promising
approach. The aproach has the advantage that there is absolutely no overhead
for the network itself. It is exemplified by a simple scheme. The performance
of the scheme is analyzed using a simulation model. The scheme is shown to be
efficient, fair, convergent and adaptive to changes in network configuration.
  The scheme as described works only for networks that can ne modelled with
queueing servers with constant service times. Further research is required to
extend it for implementation in practical networks. Several directions for
future research have beensuggested."
"Widespread use of computer networks and the use of varied technology for the
interconnection of computers has made congestion a significant problem.
  In this report, we summarize our research on congestion avoidance. We compare
the concept of congestion avoidance with that of congestion control.
  Briefly, congestion control is a recovery mechanism, while congestion
avoidance is a prevention mechanism. A congestion control scheme helps the
network to recover from the congestion state while a congestion avoidance
scheme allows a network to operate in the region of low delay and high
throughput with minimal queuing, thereby preventing it from entering the
congested state in which packets are lost due to buffer shortage.
  A number of possible alternatives for congestion avoidance were identified.
  From these alternatives we selected one called the binary feedback scheme in
which the network uses a single bit in the network layer header to feed back
the congestion information to its users, which then increase or decrease their
load to make optimal use of the resources. The concept of global optimality in
a distributed system is defined in terms of efficiency and fairness such that
they can be independently quantified and apply to any number of resources and
users.
  The proposed scheme has been simulated and shown to be globally efficient,
fair, responsive, convergent, robust, distributed, and
configuration-independent."
"Congestion is said to occur in the network when the resource demands exceed
the capacity and packets are lost due to too much queuing in the network.
During congestion, the network throughput may drop to zero and the path delay
may become very high. A congestion control scheme helps the network to recover
from the congestion state. A congestion avoidance scheme allows a network to
operate in the region of low delay and high throughput. Such schemes prevent a
network from entering the congested state. Congestion avoidance is a prevention
mechanism while congestion control is a recovery mechanism. We compare the
concept of congestion avoidance with that of flow control and congestion
control. A number of possible alternative for congestion avoidance have been
identified. From these a few were selected for study. The criteria for
selection and goals for these schemes have been described. In particular, we
wanted the scheme to be globally efficient, fair, dynamic, convergent, robust,
distributed, configuration independent, etc. These goals and the test cases
used to verify whether a particular scheme has met the goals have been
described. We model the network and the user policies for congestion avoidance
as a feedback control system. The key components of a generic congestion
avoidance scheme are: congestion detection, congestion feedback, feedback
selector, signal filter, decision function, and increase/decrease algorithms.
These components have been explained."
"During overload, most networks drop packets due to buffer unavailability. The
resulting timeouts at the source provide an implicit mechanism to convey
congestion signals from the network to the source. On a timeout, a source
should not only retransmit the lost packet, but it should also reduce its load
on the network. Based on this realization, we have developed a simple
congestion control scheme using the acknowledgment timeouts as indications of
packet loss and congestion. This scheme does not require any new message
formats, therefore, it can be used in any network with window flow control,
e.g., ARPAnet or ISO."
"The problem of adaptively setting the timeout interval for retransmitting a
packet has been discussed. A layered view of the algorithms has been presented.
It is shown that a timeout algorithm consists of essentially five layers or
procedures which can be independently chosen and modified. A number of timeout
algorithms proposed in the literature have been decomposed into these five
layers.
  One of the key layers not discussed in the literature is that of determining
the sample round trip delay for packets that have been transmitted more than
once. It is shown that this layer has a significant impact on the network
performance.
  Under repeated packet loss, most timeout algorithms either diverge or
converge to a wrong value. A number of alternative schemes have been presented.
It is argued that divergence is preferable to false convergence. It is a
feature that is helpful in reducing network traffic congestion."
"In window flow controlled networks, if a packet is lost the destination has
to decide whether to save (cache) subsequent out-of-order packets. Also, the
source has to decide whether to send just one packet or to send all packets
following it. This leads to four different types of caching schemes.
Simulations show, against our immediate intuition, that regardless of whether
the destination is caching or not, the source should retransmit only one
packet. This paper describes the alternatives to, and provides justification
for, schemes used in Digital Network Architecture and ARPAnet TCP."
"Fairness is an important performance criterion in all resource allocation
schemes, including those in distributed computer systems. However, it is often
specified only qualitatively. The quantitative measures proposed in the
literature are either too specific to a particular application, or suffer from
some undesirable characteristics. In this paper, we have introduced a
quantitative measure called Indiex of FRairness. The index is applicable to any
resource sharing or allocation problem. It is independent of the amount of the
resource. The fairness index always lies between 0 and 1. This boundedness aids
intuitive understanding of the fairness index. For example, a distribution
algorithm with a fairness of 0.10 means that it is unfair to 90% of the users.
Also, the discrimination index can be defined as 1 - fairness index."
Explanation of ABR service in plain language.
"In this paper, a new routing algorithm based on a flooding method is
introduced. Flooding techniques have been used previously, e.g. for
broadcasting the routing table in the ARPAnet [1] and other special purpose
networks [3][4][5]. However, sending data using flooding can often saturate the
network [2] and it is usually regarded as an inefficient broadcast mechanism.
Our approach is to flood a very short packet to explore an optimal route
without relying on a pre-established routing table, and an efficient flood
control algorithm to reduce the signalling traffic overhead. This is an
inherently robust mechanism in the face of a network configuration change,
achieves automatic load sharing across alternative routes and has potential to
solve many contemporary routing problems. An earlier version of this mechanism
was originally developed for virtual circuit establishment in the experimental
Caroline ATM LAN [6][7] at Monash University."
"In this paper, three new dynamic multicast routing algorithms based on the
greedy tree technique are proposed; Source Optimised Tree, Topology Based Tree
and Minimum Diameter Tree. A simulation analysis is presented showing various
performance aspects of the algorithms, in which a comparison is made with the
greedy and core based tree techniques. The effects of the tree source location
on dynamic membership change are also examined. The simulations demonstrate
that the Source Optimised Tree algorithm achieves a significant improvement in
terms of delay and link usage when compared to the Core Based Tree, and greedy
algorithm."
"The objective is to design and build a small, high-bandwidth switch."
"In this paper, we show how Web technologies can be used effectively to (i)
address some of the deficiencies of traditional IP network management
platforms, and (ii) render these expensive platforms redundant. We build on the
concept of embedded management application, proposed by Wellens and Auerbach,
and present two models of network management application designs that rely on
Web technologies. First, the pull model is based on the request/response
paradigm. It is typically used to perform data polling. Several commercial
management platforms already use Web technologies that rely on this model to
provide for ad hoc management; we demonstrate how to extend this to regular
management. Second, the push model is a novel approach which relies on the
publish/subscribe/distribute paradigm. It is better suited to regular
management than the pull model, and allows administrators to conserve network
bandwidth as well as CPU time on the management station. It can be seen as a
generalization of the paradigm commonly used for notification delivery.
Finally, we introduce the concept of the collapsed network management platform,
where these two models coexist."
"A TCP trunk is an IP tunnel under TCP control, capable of carrying packets
from any number of user flows. By exploiting properties of TCP, a TCP trunk
provides elastic and reliable transmission over a network, and automatically
shares the network fairly with other competing trunks. Moreover, by aggregating
user flows into a single trunk flow, TCP trunking can significantly reduce the
number of flows that the network needs to manage, thereby allowing use of
simplified management to achieve improved perfor mance. For example, when
dealing with only a small number of TCP trunk flows, a router with a simple
FIFO buffer can experience low packet loss rates.
  A TCP trunk is a ""soft"" circuit in the sense that it requires no flow states
to be maintained inside the network. Setting up a TCP trunk involves only
configuring the two end nodes. This is in contrast with traditional methods of
configuring circuits via signaling of network nodes.
  A simple packet-dropping mechanism based on packet accounting at the
transmitter of a TCP trunk assures that, when the trunk reduces its bandwidth
in response to network congestion, user TCP flows carried by the trunk will
reduce their bandwidths by the same proportion. Simu lation results have
demonstrated that TCP trunks can provide improved network performance to users,
while achieving high network utilization."
"The Internet has revolutionized the computer and communications world like
nothing before. The invention of the telegraph, telephone, radio, and computer
set the stage for this unprecedented integration of capabilities. The Internet
is at once a world-wide broadcasting capability, a mechanism for information
dissemination, and a medium for collaboration and interaction between
individuals and their computers without regard for geographic location.
  In this paper, several of us involved in the development and evolution of the
Internet share our views of its origins and history. This is intended to be a
brief, necessarily cursory and incomplete history. This history revolves around
four distinct aspects. There is the technological evolution that began with
early research on packet switching and the ARPANET (and related technologies),
and where current research continues to expand the horizons of the
infrastructure along several dimensions, such as scale, performance, and higher
level functionality. There is the operations and management aspect of a global
and complex operational infrastructure. There is the social aspect, which
resulted in a broad community of Internauts working together to create and
evolve the technology. And there is the commercialization aspect, resulting in
an extremely effective transition of research results into a broadly deployed
and available information infrastructure."
"The Cross-Industry Working Team (XIWT), with the support of the Stanford
University Consortium for Research on Information Security and Policy (CRISP),
sponsored a symposium on cross-industry activities aimed at improving the
reliability, dependability, and robustness of the information infrastructure.
Held 3-4 November 1998 in Crystal City, Virginia, the symposium engaged
representatives from industry, academia, and government in discussion of
current and potential cross-industry, cross-sector activities including
information exchange, collaborative operations, and cooperative research and
development. This proceedings summarizes the discussions and results of the
meeting."
"This paper introduces a novel algorithm, the Active Virtual Network
Management Protocol (AVNMP), for predictive network management. It explains how
the AVNMP facilitates the management of an active network by allowing future
predicted state information within an active network to be available to network
management algorithms. This is accomplished by coupling ideas from optimistic
discrete event simulation with active networking. The optimistic discrete event
simulation method used is a form of self-adjusting Time Warp. It is
self-adjusting because the system adjusts for predictions which are inaccurate
beyond a given tolerance. The concept of a streptichron and autoanaplasis are
introduced as mechanisms which take advantage of the enhanced flexibility and
intelligence of active packets. Finally, it is demonstrated that the AVNMP is a
feasible concept."
"There is a trend toward the use of predictive systems in communications
networks. At the systems and network management level predictive capabilities
are focused on anticipating network faults and performance degradation.
Simultaneously, mobile communication networks are being developed with
predictive location and tracking mechanisms. The interactions and synergies
between these systems present a new set of problems. A new predictive network
management framework is developed and examined. The interaction between a
predictive mobile network and the proposed network management system is
discussed. The Rapidly Deployable Radio Network is used as a specific example
to illustrate these interactions."
"This paper describes the design of a control and management network
(orderwire) for a mobile wireless Asynchronous Transfer Mode (ATM) network.
This mobile wireless ATM network is part of the Rapidly Deployable Radio
Network (RDRN). The orderwire system consists of a packet radio network which
overlays the mobile wireless ATM network, each network element in this network
uses Global Positioning System (GPS) information to control a beamforming
antenna subsystem which provides for spatial reuse. This paper also proposes a
novel Virtual Network Configuration (VNC) algorithm for predictive network
configuration. A mobile ATM Private Network-Network Interface (PNNI) based on
VNC is also discussed. Finally, as a prelude to the system implementation,
results of a Maisie simulation of the orderwire system are discussed."
"This paper extends a stochastic theory for buffer fill distribution for
multiple ``on'' and ``off'' sources to a mobile environment. Queue fill
distribution is described by a set of differential equations assuming sources
alternate asynchronously between exponentially distributed periods in ``on''
and ``off'' states. This paper includes the probabilities that mobile sources
have links to a given queue. The sources represent mobile user nodes, and the
queue represents the capacity of a switch. This paper presents a method of
analysis which uses mobile parameters such as speed, call rates per unit area,
cell area, and call duration and determines queue fill distribution at the ATM
cell level. The analytic results are compared with simulation results."
"This research concentrates on the design and analysis of an algorithm
referred to as Virtual Network Configuration (VNC) which uses predicted future
states of a system for faster network configuration and management. VNC is
applied to the configuration of a wireless mobile ATM network. VNC is built on
techniques from parallel discrete event simulation merged with constraints from
real-time systems and applied to mobile ATM configuration and handoff.
  Configuration in a mobile network is a dynamic and continuous process.
Factors such as load, distance, capacity and topology are all constantly
changing in a mobile environment. The VNC algorithm anticipates configuration
changes and speeds the reconfiguration process by pre-computing and caching
results. VNC propagates local prediction results throughout the VNC enhanced
system. The Global Positioning System is an enabling technology for the use of
VNC in mobile networks because it provides location information and accurate
time for each node.
  This research has resulted in well defined structures for the encapsulation
of physical processes within Logical Processes and a generic library for
enhancing a system with VNC. Enhancing an existing system with VNC is straight
forward assuming the existing physical processes do not have side effects. The
benefit of prediction is gained at the cost of additional traffic and
processing. This research includes an analysis of VNC and suggestions for
optimization of the VNC algorithm and its parameters."
"Many applications want to use TCP congestion control to regulate the
transmission rate of a data packet stream. A natural way to achieve this goal
is to transport the data packet stream on a TCP connection. However, because
TCP implements both congestion and error control, transporting a data packet
stream directly using a TCP connection forces the data packet stream to be
subject to TCP's other properties caused by TCP error control, which may be
inappropriate for these applications.
  The TCP decoupling approach proposed in this thesis is a novel way of
applying TCP congestion control to a data packet stream without actually
transporting the data packet stream on a TCP connection. Instead, a TCP
connection using the same network path as the data packet stream is set up
separately and the transmission rate of the data packet stream is then
associated with that of the TCP packets. Since the transmission rate of these
TCP packets is under TCP congestion control, so is that of the data packet
stream. Furthermore, since the data packet stream is not transported on a TCP
connection, the regulated data packet stream is not subject to TCP error
control.
  Because of this flexibility, the TCP decoupling approach opens up many new
opportunities, solves old problems, and improves the performance of some
existing applications. All of these advantages will be demonstrated in the
thesis.
  This thesis presents the design, implementation, and analysis of the TCP
decoupling approach, and its successful applications in TCP trunking, wireless
communication, and multimedia streaming."
"We consider the adaptation of random early detection (RED) as an active queue
management algorithm for TCP traffic in Internet gateways where different
maximum transfer units (MTUs) are used. We studied the two existing RED
variants and point out a weakness in both. The first variant where the drop
probability is independent from the packet size discriminates connections with
smaller MTUs. The second variant results in a very high Packet Loss Ratio
(PLR), and as a consequence low goodput, for connections with higher MTUs. We
show that fairness in terms of loss and goodput can be supplied through an
appropriate setting of the RED algorithm."
"We consider the adaptation of random early detection (RED) as a buffer
management algorithm for TCP traffic in Internet gateways where different
maximum transfer units (MTUs) are used. We studied the two RED variants
described in [4] and point out a weakness in both. The first variant where drop
probability is independent from the packet size discriminates connections with
smaller MTUs. The second variant results in a very high packet loss ratio
(PLR), and as a consequence low goodput, for connections with higher MTUs. We
show that fairness in terms of loss and goodput can be supplied through an
appropriate setting of the RED algorithm."
"We are moving toward a distributed, international, twenty-four hour,
electronic stock exchange. The exchange will use the global Internet, or
internet technology. This system is a natural application of multicast because
there are a large number of receivers that should receive the same information
simultaneously.
  The data requirements for the stock exchange are discussed. The current
multicast protocols lack the reliability, fairness, and scalability needed in
this application. We describe a distributed architecture together with a
reliable multicast protocol, a modification of the RMP protocol, that has
characteristics appropriate for this application.
  The architecture is used in three applications: In the first, we construct a
unified stock ticker of the transactions that are being conducted on the
various physical and electronic exchanges. Our objective is to deliver the the
same combined ticker reliably and simultaneously to all receivers, anywhere in
the world. In the second, we construct a unified sequence of buy and sell
offers that are delivered to a single exchange or a collection of exchanges.
Our objective is to give all traders the same fair access to an exchange
independent of their relative distances to the exchange or the loss
characteristics of the international network. In the third, we construct a
distributed, electronic trading floor that can replace the current exchanges.
This application uses the innovations from the first two applications to
combine their fairness attributes."
"The allocation of scarce spectral resources to support as many user
applications as possible while maintaining reasonable quality of service is a
fundamental problem in wireless communication. We argue that the problem is
best formulated in terms of decision theory. We propose a scheme that takes
decision-theoretic concerns (like preferences) into account and discuss the
difficulties and subtleties involved in applying standard techniques from the
theory of Markov Decision Processes (MDPs) in constructing an algorithm that is
decision-theoretically optimal. As an example of the proposed framework, we
construct such an algorithm under some simplifying assumptions. Additionally,
we present analysis and simulation results that show that our algorithm meets
its design goals. Finally, we investigate how far from optimal one well-known
heuristic is. The main contribution of our results is in providing insight and
guidance for the design of near-optimal admission-control policies."
"Network firewalls and routers use a rule database to decide which packets
will be allowed from one network onto another. By filtering packets the
firewalls and routers can improve security and performance. However, as the
size of the rule list increases, it becomes difficult to maintain and validate
the rules, and lookup latency may increase significantly. Ordered binary
decision diagrams (BDDs) - a compact method of representing and manipulating
boolean expressions - are a potential method of representing the rules. This
paper presents a new algorithm for representing such lists as a BDD and then
shows how the resulting boolean expression can be used to analyse rule sets."
"The three Power-Laws proposed by Faloutsos et al(1999) are important
discoveries among many recent works on finding hidden rules in the seemingly
chaotic Internet topology. In this note, we want to point out that the first
two laws discovered by Faloutsos et al(1999, hereafter, {\it Faloutsos' Power
Laws}) are in fact equivalent. That is, as long as any one of them is true, the
other can be derived from it, and {\it vice versa}. Although these two laws are
equivalent, they provide different ways to measure the exponents of their
corresponding power law relations. We also show that these two measures will
give equivalent results, but with different error bars. We argue that for nodes
of not very large out-degree($\leq 32$ in our simulation), the first Faloutsos'
Power Law is superior to the second one in giving a better estimate of the
exponent, while for nodes of very large out-degree($> 32$) the power law
relation may not be present, at least for the relation between the frequency of
out-degree and node out-degree."
"IP mobility addresses the problem of changing the network point-of-attachment
transparently during movement. Mobile IP is the proposed standard by IETF.
Several studies, however, have shown that Mobile IP has several drawbacks, such
as triangle routing and poor handoff performance. Multicast-based mobility has
been proposed as a promising solution to the above problems, incurring less
end-to-end delays and fast smooth handoff. Nonetheless, such architecture
suffers from multicast state scalability problems with the growth in number of
mobile nodes. This architecture also requires ubiquitous multicast deployment
and more complex security measures. To alleviate these problems, we propose an
intra-domain multicast-based mobility solution. A mobility proxy allocates a
multicast address for each mobile that moves to its domain. The mobile uses
this multicast address within a domain for micro mobility. Also, aggregation is
considered to reduce the multicast state. We conduct multicast state analysis
to study the efficiency of several aggregation techniques. We use extensive
simulation to evaluate our protocol's performance over a variety of real and
generated topologies. We take aggregation gain as metric for our evaluation.
  Our simulation results show that in general leaky aggregation obtains better
gains than perfect aggregation. Also, we notice that aggregation gain increases
with the increase in number of visiting mobile nodes and with the decrease in
number of mobility proxies within a domain."
"Emerging ad hoc networks are infrastructure-less networks consisting of
wireless devices with various power constraints, capabilities and mobility
characteristics. An essential capability in future ad hoc networks is the
ability to provide scalable multicast services. This paper presents a novel
adaptive architecture to support multicast services in large-scale wide-area ad
hoc networks. Existing works on multicast in ad hoc networks address only small
size networks. Our main design goals are scalability, robustness and
efficiency. We propose a self-configuring hierarchy extending zone-based
routing with the notion of contacts based on the small world graphs phenomenon
and new metrics of stability and mobility. We introduce a new geographic-based
multicast address allocation scheme coupled with adaptive anycast based on
group popularity. Our scheme is the first of its kind and promises efficient
and robust operation in the common case. Also, based on the new concept of
rendezvous regions, we provide a bootstrap mechanism for the multicast service;
a challenge generally ignored in previous work."
"The machines and beamlines controlled by VME industrial networks are very
popular in accelerator faculties. Recently new software technology, among of
which are Internet/Intranet application, Java language, and distributed
calculating environment, changes the control manner rapidly. A program based on
DCOM is composed to control of a variable included angle spherical grating
monochromator beamline at National Synchrotron Radiation Laboratory (NSRL) in
China. The control computer with a residential DCOM program is connected to
Intranet by LAN, over which the user-end-operating program located in another
computer sends driving beamline units' commands to the control computer. And
also a web page coded in Java, published by the WWW service running in the
control computer, is simply illustrated how to use web browser to query the
states of or to control the beamline units."
"The beamline network system at SPring-8 consists of three LANs; a BL-LAN for
beamline component control, a BL-USER-LAN for beamline experimental users and
an OA-LAN for the information services. These LANs are interconnected by a
firewall system. Since the network traffic and the number of beamlines have
increased, we upgraded the backbone of BL-USER-LAN from Fast Ethernet to
Gigabit Ethernet. And then, to establish the independency of a beamline and to
raise flexibility of every beamline, we also introduced the IEEE802.1Q Virtual
LAN (VLAN) technology into the BL-USER-LAN. We discuss here a future plan to
build the firewall system with hardware load balancers."
"To study mechanisms that cause the non-Gaussian nature of network traffic, we
analyzed IP flow statistics. For greedy flows in particular, we investigated
the hop counts between source and destination nodes, and classified
applications by the port number. We found that the main flows contributing to
the non-Gaussian nature of network traffic were HTTP flows with relatively
small hop counts compared with the average hop counts of all flows."
"Recently the problem of indexing and locating content in peer-to-peer
networks has received much attention. Previous work suggests caching index
entries at intermediate nodes that lie on the paths taken by search queries,
but until now there has been little focus on how to maintain these intermediate
caches. This paper proposes CUP, a new comprehensive architecture for
Controlled Update Propagation in peer-to-peer networks. CUP asynchronously
builds caches of index entries while answering search queries. It then
propagates updates of index entries to maintain these caches. Under unfavorable
conditions, when compared with standard caching based on expiration times, CUP
reduces the average miss latency by as much as a factor of three. Under
favorable conditions, CUP can reduce the average miss latency by more than a
factor of ten.
  CUP refreshes intermediate caches, reduces query latency, and reduces network
load by coalescing bursts of queries for the same item. CUP controls and
confines propagation to updates whose cost is likely to be recovered by
subsequent queries. CUP gives peer-to-peer nodes the flexibility to use their
own incentive-based policies to determine when to receive and when to propagate
updates. Finally, the small propagation overhead incurred by CUP is more than
compensated for by its savings in cache misses."
"In heterogeneous networks such as today's Internet, the differentiated
services architecture promises to provide QoS guarantees through scalable
service differentiation. Traffic marking is an important component of this
framework. In this paper, we propose two new aggregate markers that are
stateless, scalable and fair. We leverage stateless Active Queue Management
(AQM) algorithms to enable fair and efficient token distribution among
individual flows of an aggregate. The first marker, Probabilistic Aggregate
Marker (PAM), uses the Token Bucket burst size to probabilistically mark
incoming packets to ensure TCP-friendly and proportionally fair marking. The
second marker, Stateless Aggregate Fair Marker (F-SAM) approximates fair
queueing techniques to isolate flows while marking packets of the aggregate. It
distributes tokens evenly among the flows without maintaining per-flow state.
Our simulation results show that our marking strategies show upto 30%
improvement over other commonly used markers while marking flow aggregates.
These improvements are in terms of better average throughput and fairness
indices, in scenarios containing heterogeneous traffic consisting of TCP (both
long lived elephants and short lived mice) and misbehaving UDP flows. As a
bonus, F-SAM helps the mice to win the war against elephants."
"The deterministic network calculus offers an elegant framework for
determining delays and backlog in a network with deterministic service
guarantees to individual traffic flows. This paper addresses the problem of
extending the network calculus to a probabilistic framework with statistical
service guarantees. Here, the key difficulty relates to expressing, in a
statistical setting, an end-to-end (network) service curve as a concatenation
of per-node service curves. The notion of an effective service curve is
developed as a probabilistic bound on the service received by an individual
flow. It is shown that per-node effective service curves can be concatenated to
yield a network effective service curve."
"We propose centralized algorithm of data distribution in the unicast p2p
network. Good example of such networks are meshes of WWW and FTP mirrors.
Simulation of data propogation for different network topologies is performed
and it is shown that proposed method performs up to 200% better then common
apporaches"
"In this study, the concept of small worlds is investigated in the context of
large-scale wireless ad hoc and sensor networks. Wireless networks are spatial
graphs that are usually much more clustered than random networks and have much
higher path length characteristics. We observe that by adding only few random
links, path length of wireless networks can be reduced drastically without
affecting clustering. What is even more interesting is that such links need not
be formed randomly but may be confined to a limited number of hops between the
connected nodes. This has an important practical implication, as now we can
introduce a distributed algorithm in large-scale wireless networks, based on
what we call contacts, to improve the performance of resource discovery in such
networks, without resorting to global flooding. We propose new contact-based
protocols for adding logical short cuts in wireless networks efficiently. The
new protocols take advantage of mobility in order to increase reachability of
the search. We study the performance of our proposed contact-based
architecture, and clarify the context in which large-scale wireless networks
can be turned into small world networks."
"In this paper we propose a novel architecture, CARD, for resource discovery
in large scale Mobile Ad hoc Networks (MANets) which, may scale up to thousands
of nodes and may span wide geographical regions. Unlike previously proposed
schemes, our architecture avoids expensive mechanisms such as global flooding
as well as complex coordination between nodes to form a hierarchy. CARD is also
independent of any external source of information such as GPS. In our
architecture nodes within a limited number of hops from each node form the
neighborhood of that node. Resources within the neighborhood can be readily
accessed with the help of a proactive scheme within the neighborhood. For
accessing resources beyond the neighborhood, each node also maintains a few
distant nodes called contacts. Contacts help in creating a small world in the
network and provide an efficient way to query for resources beyond the
neighborhood. As the number of contacts of a node increases, the network view
(reachability) of the node increases. Paths to contacts are validated
periodically to adapt to mobility. We present mechanisms for contact selection
and maintenance that attempt to increase reachability while minimizing
overhead. Our simulation results show a clear trade-off between increase in
reachability on one hand, and contact selection and maintenance overhead on the
other. Our results suggest that CARD can be configured to provide a desirable
reachability distribution for different network sizes. Comparisons with other
schemes for resource discovery, such as flooding and bordercasting, show our
architecture to be much more efficient and scalable."
"One of the most important metrics in the design of IP mobility protocols is
the handover performance. The current Mobile IP (MIP) standard has been shown
to exhibit poor handover performance. Most other work attempts to modify MIP to
slightly improve its efficiency, while others propose complex techniques to
replace MIP. Rather than taking these approaches, we instead propose a new
architecture for providing efficient and smooth handover, while being able to
co-exist and inter-operate with other technologies. Specifically, we propose an
intra-domain multicast-based mobility architecture, where a visiting mobile is
assigned a multicast address to use while moving within a domain. Efficient
handover is achieved using standard multicast join/prune mechanisms. Two
approaches are proposed and contrasted. The first introduces the concept
proxy-based mobility, while the other uses algorithmic mapping to obtain the
multicast address of visiting mobiles. We show that the algorithmic mapping
approach has several advantages over the proxy approach, and provide mechanisms
to support it. Network simulation (using NS-2) is used to evaluate our scheme
and compare it to other routing-based micro-mobility schemes - CIP and HAWAII.
The proactive handover results show that both M&M and CIP shows low handoff
delay and packet reordering depth as compared to HAWAII. The reason for M&M's
comparable performance with CIP is that both use bi-cast in proactive handover.
The M&M, however, handles multiple border routers in a domain, where CIP fails.
We also provide a handover algorithm leveraging the proactive path setup
capability of M&M, which is expected to outperform CIP in case of reactive
handover."
"We analyzed the non-Gaussian nature of network traffic using some Internet
traffic data. We found that (1) the non-Gaussian nature degrades network
performance, (2) it is caused by `greedy flows' that exist with non-negligible
probability, and (3) a large majority of `greedy flows' are TCP flows having
relatively small hop counts, which correspond to small round-trip times. We
conclude that in a network hat has greedy flows with non-negligible
probability, a traffic controlling scheme or bandwidth design that considers
non-Gaussian nature is essential."
"Conventional optical networks are based on SONET rings, but since rings are
known to use bandwidth inefficiently, there has been much research into shared
mesh protection, which promises significant bandwidth savings. Unfortunately,
most shared mesh protection schemes cannot guarantee that failed traffic will
be restored within the 50 ms timeframe that SONET standards specify. A notable
exception is the p-cycle scheme of Grover and Stamatelakis. We argue, however,
that p-cycles have certain limitations, e.g., there is no easy way to adapt
p-cycles to a path-based protection scheme, and p-cycles seem more suited to
static traffic than to dynamic traffic. In this paper we show that the key to
fast restoration times is not a ring-like topology per se, but rather the
ability to pre-cross-connect protection paths. This leads to the concept of a
pre-cross-connected trail or PXT, which is a structure that is more flexible
than rings and that adapts readily to both path-based and link-based schemes
and to both static and dynamic traffic. The PXT protection scheme achieves fast
restoration speeds, and our simulations, which have been carefully chosen using
ideas from experimental design theory, show that the bandwidth efficiency of
the PXT protection scheme is comparable to that of conventional shared mesh
protection schemes."
"Many ad hoc routing protocols are based on some variant of flooding. Despite
various optimizations, many routing messages are propagated unnecessarily. We
propose a gossiping-based approach, where each node forwards a message with
some probability, to reduce the overhead of the routing protocols. Gossiping
exhibits bimodal behavior in sufficiently large networks: in some executions,
the gossip dies out quickly and hardly any node gets the message; in the
remaining executions, a substantial fraction of the nodes gets the message. The
fraction of executions in which most nodes get the message depends on the
gossiping probability and the topology of the network. In the networks we have
considered, using gossiping probability between 0.6 and 0.8 suffices to ensure
that almost every node gets the message in almost every execution. For large
networks, this simple gossiping protocol uses up to 35% fewer messages than
flooding, with improved performance. Gossiping can also be combined with
various optimizations of flooding to yield further benefits. Simulations show
that adding gossiping to AODV results in significant performance improvement,
even in networks as small as 150 nodes. We expect that the improvement should
be even more significant in larger networks."
"The topology of a wireless multi-hop network can be controlled by varying the
transmission power at each node. In this paper, we give a detailed analysis of
a cone-based distributed topology control algorithm. This algorithm, introduced
in [16], does not assume that nodes have GPS information available; rather it
depends only on directional information. Roughly speaking, the basic idea of
the algorithm is that a node $u$ transmits with the minimum power
$p_{u,\alpha}$ required to ensure that in every cone of degree $\alpha$ around
$u$, there is some node that $u$ can reach with power $p_{u,\alpha}$. We show
that taking $\alpha = 5\pi/6$ is a necessary and sufficient condition to
guarantee that network connectivity is preserved. More precisely, if there is a
path from $s$ to $t$ when every node communicates at maximum power, then, if
$\alpha <= 5\pi/6$, there is still a path in the smallest symmetric graph
$G_\alpha$ containing all edges $(u,v)$ such that $u$ can communicate with $v$
using power $p_{u,\alpha}$. On the other hand, if $\alpha > 5\pi/6$,
connectivity is not necessarily preserved. We also propose a set of
optimizations that further reduce power consumption and prove that they retain
network connectivity. Dynamic reconfiguration in the presence of failures and
mobility is also discussed. Simulation results are presented to demonstrate the
effectiveness of the algorithm and the optimizations."
"We propose a protocol that, given a communication network, computes a
subnetwork such that, for every pair $(u,v)$ of nodes connected in the original
network, there is a minimum-energy path between $u$ and $v$ in the subnetwork
(where a minimum-energy path is one that allows messages to be transmitted with
a minimum use of energy). The network computed by our protocol is in general a
subnetwork of the one computed by the protocol given in [13]. Moreover, our
protocol is computationally simpler. We demonstrate the performance
improvements obtained by using the subnetwork computed by our protocol through
simulation."
"An Open Network Handle System (ONHS) provides an intermediate level of
service between IP numbers and domain names. A handle adheres permanently to an
owner, who may assign and reassign it to different addresses at will. But a
handle is a number, carrying no significance in natural language. Any user
desiring a handle may generate one from a public key. This memo describes a
simple implementation of an Open Network Handle System using the security
extensions to the Domain Name System (DNSSEC)."
"Networked communications inherently depend on the ability of the sender of a
message to indicate through some token how the message should be delivered to a
particular recipient. The tokens that refer messages to recipients are
variously known as routes, addresses,handles, and names} ordered by their
relative nearness to network topology vs. human meaning. All four sorts of
token refer in some way to a recipient, but they are controlled by different
authorities and their meanings depend on different contextual parameters.
  Today's global Internet employs dynamically determined routes, IP addresses,
and domain names. Domain names combine the functions of handles and names. The
high value of domain names as names leads to substantial social and legal
dispute about their assignment, degrading their value as handles. The time has
come to provide a distinct open network handle system (ONHS), using handles
that are not meaningful in natural language and are therefore not subject to
the disputes surrounding the use of names.
  A handle service may be deployed easily as a handle domain within the current
Domain Name System. In order to minimize the administrative load, and maximize
their own autonomy, netizens may use public-key cryptography to assign their
own handles."
"The aim of this paper is an experimental study of cache systems in order to
optimize proxy cache systems and to modernize construction principles. Our
investigations lead to the criteria for the optimal use of storage capacity and
allow the description of the basic effects of the ratio between construction
parts, steady-state performance, optimal size, etc. We want to outline that the
results obtained and the plan of the experiment follow from the theoretical
model. Special consideration is given to the modification of the key formulas
supposed by Wolman at al."
"The aim of this paper is a theoretical study of a cache system in order to
optimize proxy cache systems and to modernize construction principles including
prefetching schemes. Two types of correlations, Zipf-like distribution and
normalizing conditions, play a role of the fundamental laws. A corresponding
system of equations allows to describe the basic effects like ratio between
construction parts, steady-state performance, optimal size, long-term
prefetching, etc. A modification of the fundamental laws leads to the
description of new effects of documents' renewal in the global network. An
internet traffic caching system based on Zipf-like distribution (ZBS) is
invented. The additional module to the cache construction gives an effective
prefetching by lifetime."
"The study of Complex Systems is considered by many to be a new scientific
field, and is distinguished by being a discipline that has applications within
many separate areas of scientific study. The study of Neural Networks, Traffic
Patterns, Artificial Intelligence, Social Systems, and many other scientific
areas can all be considered to fall within the realm of Complex Systems, and
can be studied from this new perspective. The advent of more capable computer
systems has allowed these systems to be simulated and modeled with far greater
ease, and new understanding of computer modeling approaches has allowed the
fledgling science to be studied as never before.
  The preliminary focus of this paper will be to provide a general overview of
the science of Complex Systems, including terminology, definitions, history,
and examples. I will attempt to look at some of the most important trends in
different areas of research, and give a general overview of research methods
that have been used in parallel with computer modeling. Also, I will further
define the areas of the science that concern themselves with computer modeling
and simulation, and I will attempt to make it clear why the science only came
into its own when the proper modeling and simulation tools were finally
available. In addition, although there seems to be general agreement between
different authors and institutes regarding the generalities of the study, there
are some differences in terminology and methodology. I have attempted in this
paper to bring as many elements together as possible, as far as the scope of
the subject is concerned, without losing focus by studying Complex System
techniques that are bound to one particular area of scientific study, unless
that area is that of computer modeling."
"A number of recent studies of the Internet topology at the autonomous systems
level (AS graph) are based on the BGP-based AS connectivity maps (original
maps). The so-called extended maps use additional data sources and contain more
complete pictures of the AS graph. In this paper, we compare an original map,
an extended map and a synthetic map generated by the Barabasi-Albert model. We
examine the recently reported rich-club phenomenon, alternative routing paths
and attack tolerance. We point out that the majority of the missing links of
the original maps are the connecting links between rich nodes (nodes with large
numbers of links) of the extended maps. We show that the missing links are
relevant because links between rich nodes can be crucial for the network
structure."
"The Internet topology at the Autonomous Systems level (AS graph) has a
power--law degree distribution and a tier structure. In this paper, we
introduce the Interactive Growth (IG) model based on the joint growth of new
nodes and new links. This simple and dynamic model compares favorable with
other Internet power--law topology generators because it not only closely
resembles the degree distribution of the AS graph, but also accurately matches
the hierarchical structure, which is measured by the recently reported
rich-club phenomenon."
"Recently we introduced the rich-club phenomenon as a quantitative metric to
characterize the tier structure of the Autonomous Systems level Internet
topology (AS graph) and we proposed the Interactive Growth (IG) model, which
closely matches the degree distribution and hierarchical structure of the AS
graph and compares favourble with other available Internet power-law topology
generators. Our research was based on the widely used BGP AS graph obtained
from the Oregon BGP routing tables. Researchers argue that Traceroute AS graph,
extracted from the traceroute data collected by the CAIDA's active probing
tool, Skitter, is more complete and reliable. To be prudent, in this paper we
analyze and compare topological structures of Traceroute AS graph and BGP AS
graph. Also we compare with two synthetic Internet topologies generated by the
IG model and the well-known Barabasi-Albert (BA) model. Result shows that both
AS graphs show the rich-club phenomenon and have similar tier structures, which
are closely matched by the IG model, however the BA model does not show the
rich-club phenomenon at all."
"The article analyzes a proposed network topology for the ATLAS DAQ DataFlow,
and identifies the Ethernet features required for a proper operation of the
network: MAC address table size, switch performance in terms of throughput and
latency, the use of Flow Control, Virtual LANs and Quality of Service. We
investigate these features on some Ethernet switches, and conclude on their
usefulness for the ATLAS DataFlow network."
"We propose flow-based analysis to estimate quality of an Internet connection.
Using results from the queuing theory we compare two expressions for backbone
traffic that have different scopes of applicability. A curve that shows
dependence of utilization of a link on a number of active flows in it describes
different states of the network. We propose a methodology for plotting such a
curve using data received from a Cisco router by NetFlow protocol, determining
the working area and the overloading point of the network. Our test is an easy
way to find a moment for upgrading the backbone."
"Ad hoc networks rely on the cooperation of the nodes participating in the
network to forward packets for each other. A node may decide not to cooperate
to save its resources while still using the network to relay its traffic. If
too many nodes exhibit this behavior, network performance degrades and
cooperating nodes may find themselves unfairly loaded. Most previous efforts to
counter this behavior have relied on further cooperation between nodes to
exchange reputation information about other nodes. If a node observes another
node not participating correctly, it reports this observation to other nodes
who then take action to avoid being affected and potentially punish the bad
node by refusing to forward its traffic. Unfortunately, such second-hand
reputation information is subject to false accusations and requires maintaining
trust relationships with other nodes. The objective of OCEAN is to avoid this
trust-management machinery and see how far we can get simply by using direct
first-hand observations of other nodes' behavior. We find that, in many
scenarios, OCEAN can do as well as, or even better than, schemes requiring
second-hand reputation exchanges. This encouraging result could possibly help
obviate solutions requiring trust-management for some contexts."
"Several tools exist that collect host-to-host connectivity measurements. To
improve the usability of such measurements, they should be mapped into a
framework consisting of complex subsystems, and the infrastructure that
connects them. We introduce one such framework, and analyze the architectural
implications on the network structure. In our framework, a complex subsystem
consists of several computing facilities and the infrastructure that connects
them: we call it a -monitoring domain-. The task of measuring the connectivity
between -monitoring domains- is considered distinct from the activity of
-storage- and -computing- elements. Therefore we introduce a new element in our
topology: we call it -theodolite- element, since its function is similar to
that of a transponder. Using these basic concepts, we analyze the architectural
implications on the network structure: in a nutshell, if we want that
-theodolites- serve as a reference, than the contribution to the relevant
network metrics due to the -monitoring domain- infrastructure must be
negligible with respect to contributions of the inter-domain infrastructure. In
addition all -theodolites- of a -monitoring domain- must give an image of the
inter-domain infrastructure that is consistent with that experienced by network
applications. We conclude giving a running SQL example of how information about
-monitoring domains- and -theodolites- could be organized, and we outline the
application of such framework in the GLUE schema activity for the network
element"
"It starts out innocently enough - users want to monitor Online data and so
run their own copies of the detector control GUIs in their offices and at home.
But over time, the number of processes making requests for values to display on
GUIs, webpages and stripcharts can grow, and affect the performance of an
Input/Output Controller (IOC) such that it is unable to respond to requests
from requests critical to data-taking. At worst, an IOC can hang, its CPU
having been allocated 100% to responding to network requests.
  For the BaBar Online Detector Control System, we were able to eliminate this
problem and make great gains in security by moving all of the IOCs to a
non-routed, virtual LAN and by enlisting a workstation with two network
interface cards to act as the interface between the virtual LAN and the public
BaBar network. On the interface machine, we run the Experimental Physics
Industrial Control System (EPICS) Channel Access (CA) gateway software
(originating from Advanced Photon Source). This software accepts as inputs, all
the channels which are loaded into the EPICS databases on all the IOCs. It
polls them to update its copy of the values. It answers requests from
applications by sending them the currently cached value.
  We adopted the requirement that data-taking would be independent of the
gateway, so that, in the event of a gateway failure, data-taking would be
uninterrupted. In this way, we avoided introducing any new risk elements to
data-taking. Security rules already in use by the IOC were propagated to the
gateway's own security rules and the security of the IOCs themselves was
improved by removing them from the public BaBar network."
"We show that the Internet topology at the Autonomous System (AS) level has a
rich--club phenomenon. The rich nodes, which are a small number of nodes with
large numbers of links, are very well connected to each other. The rich--club
is a core tier that we measured using the rich--club connectivity and the
node--node link distribution. We obtained this core tier without any heuristic
assumption between the ASes. The rich--club phenomenon is a simple qualitative
way to differentiate between power law topologies and provides a criterion for
new network models. To show this, we compared the measured rich--club of the AS
graph with networks obtained using the Barab\'asi--Albert (BA) scale--free
network model, the Fitness BA model and the Inet--3.0 model."
"Denial of Service (DoS) attacks are one of the most challenging threats to
Internet security. An attacker typically compromises a large number of
vulnerable hosts and uses them to flood the victim's site with malicious
traffic, clogging its tail circuit and interfering with normal traffic. At
present, the network operator of a site under attack has no other resolution
but to respond manually by inserting filters in the appropriate edge routers to
drop attack traffic. However, as DoS attacks become increasingly sophisticated,
manual filter propagation becomes unacceptably slow or even infeasible.
  In this paper, we present Active Internet Traffic Filtering, a new automatic
filter propagation protocol. We argue that this system provides a guaranteed,
significant level of protection against DoS attacks in exchange for a
reasonable, bounded amount of router resources. We also argue that the proposed
system cannot be abused by a malicious node to interfere with normal Internet
operation. Finally, we argue that it retains its efficiency in the face of
continued Internet growth."
"Flooding provides important control and route establishment functionality for
a number of unicast and multicast protocols in Mobile Ad Hoc Networks.
Considering its wide use as a building block for other network layer protocols,
the flooding methodology should deliver a packet from one node to all other
network nodes using as few messages as possible. In this paper, we propose the
Optimized Flooding Protocol (OFP), based on a variation of The Covering Problem
that is encountered in geometry, to minimize the unnecessary transmissions
drastically and still be able to cover the whole region. OFP does not need
hello messages and hence OFP saves a significant amount of wireless bandwidth
and incurs lesser overhead. We present simulation results to show the
efficiency of OFP in both ideal cases and randomly distributed networks.
Moreover, OFP is scalable with respect to density; in fact OFP requires lesser
number of transmissions at higher densities. OFP is also resilient to
transmission errors."
"In this paper, we model the cost incurred by each peer participating in a
peer-to-peer network. Such a cost model allows to gauge potential disincentives
for peers to collaborate, and provides a measure of the ``total cost'' of a
network, which is a possible benchmark to distinguish between proposals. We
characterize the cost imposed on a node as a function of the experienced load
and the node connectivity, and show how our model applies to a few proposed
routing geometries for distributed hash tables (DHTs). We further outline a
number of open questions this research has raised."
"Based on measurements of the Internet topology data, we found out that there
are two mechanisms which are necessary for the correct modeling of the Internet
topology at the Autonomous Systems (AS) level: the Interactive Growth of new
nodes and new internal links, and a nonlinear preferential attachment, where
the preference probability is described by a positive-feedback mechanism. Based
on the above mechanisms, we introduce the Positive-Feedback Preference (PFP)
model which accurately reproduces many topological properties of the AS-level
Internet, including: degree distribution, rich-club connectivity, the maximum
degree, shortest path length, short cycles, disassortative mixing and
betweenness centrality. The PFP model is a phenomenological model which
provides a novel insight into the evolutionary dynamics of real complex
networks."
"A comparison between the topological properties of the measured Internet
topology, at the autonomous system level (AS graph), and the equivalent graphs
generated by two different power law topology generators is presented. Only one
of the synthetic generators reproduces the tier connectivity of the AS graph."
"A distributed denial-of-service (DDoS) attack can flood a victim site with
malicious traffic, causing service disruption or even complete failure.
Public-access sites like amazon or ebay are particularly vulnerable to such
attacks, because they have no way of a priori blocking unauthorized traffic.
  We present Active Internet Traffic Filtering (AITF), a mechanism that
protects public-access sites from highly distributed attacks by causing
undesired traffic to be blocked as close as possible to its sources. We
identify filters as a scarce resource and show that AITF protects a significant
amount of the victim's bandwidth, while requiring from each participating
router a number of filters that can be accommodated by today's routers. AITF is
incrementally deployable, because it offers a substantial benefit even to the
first sites that deploy it."
"In this paper considered question of using pattern recognition methods in
network equipment state identification."
"We consider the problem of providing service guarantees in a high-speed
packet switch. As basic requirements, the switch should be scalable to high
speeds per port, a large number of ports and a large number of traffic flows
with independent guarantees. Existing scalable solutions are based on Virtual
Output Queuing, which is computationally complex when required to provide
service guarantees for a large number of flows.
  We present a novel architecture for packet switching that provides support
for such service guarantees. A cost-effective fabric with small external
speedup is combined with a feedback mechanism that enables the fabric to be
virtually lossless, thus avoiding packet drops indiscriminate of flows. Through
analysis and simulation, we show that this architecture provides accurate
support for service guarantees, has low computational complexity and is
scalable to very high port speeds."
"In this paper, we study diagnosabilities of multiprocessor systems under two
diagnosis models: the PMC model and the comparison model. In each model, we
further consider two different diagnosis strategies: the precise diagnosis
strategy proposed by Preparata et al. and the pessimistic diagnosis strategy
proposed by Friedman. The main result of this paper is to determine
diagnosabilities of regular networks with certain conditions, which include
several widely used multiprocessor systems such as variants of hypercubes and
many others."
"Future smart environments will be characterized by multiple nodes that sense,
collect, and disseminate information about environmental phenomena through a
wireless network. In this paper, we define a set of applications that require a
new form of distributed knowledge about the environment, referred to as
non-uniform information granularity. By non-uniform information granularity we
mean that the required accuracy or precision of information is proportional to
the distance between a source node (information producer) and current sink node
(information consumer). That is, as the distance between the source node and
sink node increases, loss in information precision is acceptable. Applications
that can benefit from this type of knowledge range from battlefield scenarios
to rescue operations. The main objectives of this paper are two-fold: first, we
will precisely define non-uniform information granularity, and second, we will
describe different protocols that achieve non-uniform information dissemination
and analyze these protocols based on complexity, energy consumption, and
accuracy of information."
"The ability of a sensor node to determine its physical location within a
network (Localization) is of fundamental importance in sensor networks.
Interpretating data from sensors will not be possible unless the context of the
data is known; this is most often accomplished by tracking its physical
location. Existing research has focused on localization in static sensor
networks where localization is a one-time (or low frequency) activity. In
contrast, this paper considers localization for mobile sensors: when sensors
are mobile, localization must be invoked periodically to enable the sensors to
track their location. The higher the frequency of localization, the lower the
error introduced because of mobility. However, localization is a costly
operation since it involves both communication and computation. In this paper,
we propose and investigate adaptive and predictive protocols that control the
frequency of localization based on sensor mobility behavior to reduce the
energy requirements for localization while bounding the localization error. We
show that such protocols can significantly reduce the localization energy
without sacrificing accuracy (in fact, improving accuracy for most situations).
Using simulation and analysis we explore the tradeoff between energy efficiency
and localization error due to mobility for several protocols."
"In this paper, we introduce two deterministic models aimed at capturing the
dynamics of congested Internet connections. The first model is a
continuous-time model that combines a system of differential equations with a
sudden change in one of the state variables. The second model is a
discrete-time model with a time step that arises naturally from the system.
Results from these models show good agreement with the well-known ns network
simulator, better than the results of a previous, similar model. This is due in
large part to the use of the sudden change to reflect the impact of lost data
packets. We also discuss the potential use of this model in network traffic
state estimation."
"There is a growing interest in discovery of internet topology at the
interface level. A new generation of highly distributed measurement systems is
currently being deployed. Unfortunately, the research community has not
examined the problem of how to perform such measurements efficiently and in a
network-friendly manner. In this paper we make two contributions toward that
end. First, we show that standard topology discovery methods (e.g., skitter)
are quite inefficient, repeatedly probing the same interfaces. This is a
concern, because when scaled up, such methods will generate so much traffic
that they will begin to resemble DDoS attacks. We measure two kinds of
redundancy in probing (intra- and inter-monitor) and show that both kinds are
important. We show that straightforward approaches to addressing these two
kinds of redundancy must take opposite tacks, and are thus fundamentally in
conflict. Our second contribution is to propose and evaluate Doubletree, an
algorithm that reduces both types of redundancy simultaneously on routers and
end systems. The key ideas are to exploit the tree-like structure of routes to
and from a single point in order to guide when to stop probing, and to probe
each path by starting near its midpoint. Our results show that Doubletree can
reduce both types of measurement load on the network dramatically, while
permitting discovery of nearly the same set of nodes and links. We then show
how to enable efficient communication between monitors through the use of Bloom
filters."
"Wireless LANs have achieved a tremendous amount of growth in recent years.
Among various wireless LAN technologies, the IEEE 802.11b based wireless LAN
technology can be cited as the most prominent technology today. Despite being
widely deployed, 802.11b cannot be termed as a well matured technology.
Although 802.11b is adequate for basic connectivity and packet switching, It is
evident that there is ample scope for its improvement in areas like quality of
service, fairness, performance, security, etc. In this survey report, we
identify and argue that the Medium Access Controller for 802.11b networks is
the prime area for these improvements. To enunciate our claims we highlight
some of the quality of service, fairness, and performance issues related to
802.11b MAC. We also describe and analyze some of the current research aimed at
addressing these issues. We then propose a novel scheme called the Intelligent
Collision Avoidance, seeking to enhance the MAC to address some of the
performance issues in 802.11b and similar networks."
"Wireless sensor networks are finally becoming a reality. In this paper, we
present a scalable architecture for using wireless sensor networks in
combination with wireless Ethernet networks to provide a complete end-to-end
solution to narrow the gap between the low-level information and context
awareness. We developed and implemented a complete proximity detector in order
to give a wearable computer, such as a PDA, location context. Since location is
only one element of contextawareness, we pursued utilizing photo sensors and
temperature sensors in learning as much as possible about the environment. We
used the TinyOS RF Motes as our test bed WSN (Wireless Sensor Network), 802.11
compatible hardware as our wireless Ethernet network, and conventional PCs and
wired 802.3 networks to build the upper levels of the architecture."
"One of the limitations of wireless sensor nodes is their inherent limited
energy resource. Besides maximizing the lifetime of the sensor node, it is
preferable to distribute the energy dissipated throughout the wireless sensor
network in order to minimize maintenance and maximize overall system
performance. We investigate a new routing algorithm that uses diffusion in
order to achieve relatively even power dissipation throughout a wireless sensor
network by making good local decisions. We leverage from concepts of
peer-to-peer networks in which the system acts completely decentralized and all
nodes in the network are equal peers. Our algorithm utilizes the node load,
power levels, and spatial information in order to make the optimal routing
decision. According to our preliminary experimental results, our proposed
algorithm performs well according to its goals."
"One of the limitations of wireless sensor nodes is their inherent limited
energy resource. Besides maximizing the lifetime of the sensor node, it is
preferable to distribute the energy dissipated throughout the wireless sensor
network in order to minimize maintenance and maximize overall system
performance. Any communication protocol that involves synchronization of peer
nodes incurs some overhead for setting up the communication. We introduce a new
algorithm, e3D (energy-efficient Distributed Dynamic Diffusion routing
algorithm), and compare it to two other algorithms, namely directed, and random
clustering communication. We take into account the setup costs and analyze the
energy-efficiency and the useful lifetime of the system. In order to better
understand the characteristics of each algorithm and how well e3D really
performs, we also compare e3D with its optimum counterpart and an optimum
clustering algorithm. The benefit of introducing these ideal algorithms is to
show the upper bound on performance at the cost of an astronomical prohibitive
synchronization costs. We compare the algorithms in terms of system lifetime,
power dissipation distribution, cost of synchronization, and simplicity of the
algorithm. Our simulation results show that e3D performs comparable to its
optimal counterpart while having significantly less overhead."
"Our contribution in this paper is e3D, a diffusion based routing protocol
that prolongs the system lifetime, evenly distributes the power dissipation
throughout the network, and incurs minimal overhead for synchronizing
communication. We compare e3D with other algorithms in terms of system
lifetime, power dissipation distribution, cost of synchronization, and
simplicity of the algorithm."
"This paper introduces relevant statistics for the description of routes in
the internet, seen as a graph at the interface level. Based on the observed
properties, we propose and evaluate methods for generating artificial routes
suitable for simulation purposes. The work in this paper is based upon a study
of over seven million route traces produced by CAIDA's skitter infrastructure."
"The multiplication of architecture description languages, component models
and platforms implies a serious dilemma for component based software
architects. On the one hand, they have to choose a language to describe
concrete configurations which will be automatically deployed on execution
platforms. On the other hand, they wish to capitalize their software
architectures independently of any description languages or platforms. To solve
this problem, we propose a multi personalities environment for the
configuration and the deployment of component based applications. This
environment is composed of a core capturing a canonical model of configuration
and deployment, and a set of personalities tailored to languages and platforms.
This paper details the architecture of such an environment and describes the
personalities for the CORBA and Fractal component models."
"La specification J2EE (Java 2 platform Enterprise Edition) definit une
architecture de serveur d'application Java. Jusqu'a J2EE 1.3, seuls les aspects
de deploiement concernant le developpeur d'applications etaient adresses. Avec
J2EE 1.4, les interfaces et les etapes de deploiement ont ete plus precisement
specifiees dans la specification ""J2EE Deployment"". JOnAS (Java Open
Application Server) est une plate-forme J2EE developpee au sein du consortium
ObjectWeb. Les aspects deploiement sont en cours de developpement. Cet article
decrit les concepts lies au deploiement dans J2EE, ainsi que les problematiques
levees lors de leur mise en oeuvre pour JOnAS. Il n'a pas pour but de presenter
un travail abouti, mais illustre le deploiement par un cas concret et ebauche
une liste de besoins non encore satisfaits dans le domaine.
  -----
  The J2EE (Java 2 platform Enterprise Edition) specification defines an
architecture for Java Application Servers. Until J2EE 1.3, the deployment
aspect was addressed from the developer point of view only. Since J2EE 1.4,
deployment APIs and steps have been more precisely specified within the ""J2EE
Deployment Specification"". JOnAS (Java Open Application Server) is a J2EE
platform implementation by ObjectWeb. The deployment aspects are under
development. This article describes the J2EE Deployment concepts, and the
issues raised when implementing deployment features within JOnAS. It does not
provide a complete solution, but illustrates deployment through a concrete
example and initiates a list of non fulfilled requirements."
"Les developpements logiciels sur les systemes UNIX font de plus en plus appel
aux logiciels libres. Nous proposons une solution de deploiement et de controle
de ces logiciels libres au sein d'une grande organisation. Nous nous attachons
particulierement a resoudre les problemes lies au deploiement multi-sites ainsi
qu'a la gestion de configuration de ces deploiements. L'originalite de notre
approche repose sur sa capacite a etre mise en oeuvre et controlee par les
utilisateurs plutot que par les administrateurs, sans necessiter d'expertise
particuliere, et par les possibilites de deploiement dans des environnements
heterogenes.
  -----
  Free and open source software is more and more used for software developments
on UNIX systems. We are proposing a solution to control the deployment of free
software in the context of a large corporation, focusing on multi-site
deployment and configuration management. The originality of our approach rests
on its ability to be implemented and controlled by users rather than
administrators, without requiring any particular expertise, and on its facility
to be deployed in heterogeneous environments."
"This paper proposes a software architecture for dynamical service adaptation.
The services are constituted by reusable software components. The adaptation's
goal is to optimize the service function of their execution context. For a
first step, the context will take into account just the user needs but other
elements will be added. A particular feature in our proposition is the profiles
that are used not only to describe the context's elements but also the
components itself. An Adapter analyzes the compatibility between all these
profiles and detects the points where the profiles are not compatibles. The
same Adapter search and apply the possible adaptation solutions: component
customization, insertion, extraction or replacement."
"Runtime reconfiguration considered as ""applying required changes to a running
system"" plays an important role for providing high availability not only of
safety- and mission-critical systems, but also for commercial web-applications
offering professional services. Hereby, the main concerns are maintaining the
consistency of the running system during reconfiguration and minimizing its
down-time caused by the reconfiguration. This paper focuses on the platform
independent subsystem that realises deployment and redeployment of J2EE modules
based on the new J2EE Deployment API as a part of the implementation of our
proposed system architecture enabling runtime reconfiguration of
component-based systems. Our ""controlled runtime redeployment"" comprises an
extension of hot deployment and dynamic reloading, complemented by allowing for
structural change"
"Information and communication technologies are moving towards a new stage
where applications will be dynamically deployed, uninstalled, updated and
(re)configured. Several approaches have been followed with the goal of creating
a fully automated and context-aware deployment system. Ideally, this system
should be capable of handling the dynamics of this new situation, without
losing sight of other factors, such as performance, security, availability or
scalability. We will take some of the technologies that follow the principles
of Service Oriented Architectures, SOA, as a paradigm of dynamic environments.
SOA promote the breaking down of applications into sets of loosely coupled
elements, called services. Services can be dynamically bound, deployed,
reconfigured, uninstalled and updated. First of all, we will try to offer a
broad view on the specific deployment issues that arise in these environments.
Later on, we will present our approach to the problem. One of the essential
points that has to be tackled to develop an automated deployment engine will be
to have enough information to carry out tasks without human intervention. In
the article we will focus on the format and contents of deployment descriptors.
Additionally, we will go into the details of the deployment framework for OSGi
enabled gateways that has been developed by our research group. Finally we will
give some concluding remarks and some ideas for future work"
"Deployment of software components for building distributed applications
consists of the coordination of a set of basic tasks like uploading component
binaries to the execution sites, loading them in memory, instantiating
components, interconnecting their ports, setting their business and technical
attributes. The automation of the deployment process then requires the presence
of a software infrastructure distributed itself on the different execution
sites. This paper presents the characteristics of such an infrastructure for
the deployment of CORBA component-based applications. This latter is designed
and implemented in the context of our OpenCCM platform, an open source
implementation of the CORBA Component Model. The main characteristic lays on
the fact that this infrastructure is itself designed as a set of CORBA
component assemblies. This allows its dynamic assembly during its deployment
over the execution sites"
"The deployment of component-based applications relies on a centralized
directory to store the components. This paper describes an approach to
distribute software components to be deployed on a set of peers of a peer to
peer network in order to exploit some associated characteristics (load
balancing, fault-tolerance, self-organisation). The proposed architecture is
situated in the context of OSGI application deployment management. The software
components (bundles) are distributed among a set of nodes participating in the
execution of services. When a node wants to install a component which is not
deployed locally, the component is looked for and installed using a p2p
network.
  -----
  Le deploiement d'applications a composants repose sur une approche d'annuaire
centralise de stockage des composants. Cet article decrit une approche pour
distribuer les composants logiciels a deployer sur un ensemble de noeuds d'un
reseau pair-a-pair afin de pouvoir exploiter certaines caracteristiques
associees (equilibrage de charge, tolerance de panne, auto-organisation).
L'architecture proposee entre dans le cadre de la gestion du deploiement
d'applications sur le modele OSGi. Les composants logiciels (ou bundles) sont
repartis a travers un ensemble de noeuds participant a l'execution de services.
Lorsqu'un noeud veut installer un composant et si celui-ci n'est pas encore
deploye localement, il est recherche et installe en utilisant un reseau p2p"
"Le deploiement est maintenant considere comme une activite a part entiere du
cycle de vie du logiciel. Les grandes entreprises souhaitent pouvoir
automatiser cette etape tout en prenant en compte les caracteristiques de
chaque machine cible. Pour repondre a ces besoins, nous avons defini un
environnement de deploiement : ORYA (Open enviRonment to deploY Applications).
Cet environnement utilise un meta-modele de deploiement, decrit dans ce papier.
Notre approche utilise aussi les technologies des federations et des procedes,
fournissant un environnement flexible et extensible pour l'utilisateur.
  -----
  The deployment is now a full activity of the software lifecycle. Large
enterprises want to automate this step, taking into account characteristics of
each target machine. To satisfy these needs, we have defined an environment for
the deployment phase: ORYA (Open enviRonment to deploY Applications). This
environment uses a deployment metamodel, described in this paper. Our approach
is based also on federation and process federations, providing a flexible and
extensible environment to the user"
"Cet article presente FROGi, une proposition visant a introduire le modele a
composants Fractal a l'interieur de la plateforme de services OSGi. La
motivation derriere ce travail est double. D'un cote, FROGi offre aux
developpeurs de services OSGi un modele a composants extensibles qui facilite
le developpement des bundles ; ces derniers restent toutefois compatibles avec
les bundles ""patrimoniaux"". D'un autre cote, FROGi beneficie de
l'infrastructure de deploiement que represente OSGi et qui facilite la
realisation du conditionnement et du deploiement de composants Fractal. Dans
FROGi, une application Fractal est conditionnee sous la forme d'un ou plusieurs
bundles et elle peut etre deployee de facon partielle et les activites de
deploiement peuvent avoir lieu de facon continue.
  -- This paper presents FROGi, a proposal to introduce the Fractal component
model into the OSGi services platform. There are two motivations for this work.
The first one is to offer a flexible component model to the OSGi developers to
simplify bundle development. Bundles developed with FROGi are nevertheless
compatible with standard bundles. The second motivation is to leverage OSGi's
deployment capabilities to package and deploy Fractal components. In FROGi, a
Fractal application is packaged and delivered as a set of OSGi bundles; such an
application supports partial deployment and additionally, deployment activities
can occur continuously."
"Internet is growing at a fast pace. The link speeds are surging toward 40
Gbps with the emergence of faster link technologies. New applications are
coming up which require intelligent processing at the intermediate routers.
Switches and routers are becoming the bottlenecks in fast communication. On one
hand faster links deliver more packets every second and on the other hand
intelligent processing consumes more CPU cycles at the router. The conflicting
goals of providing faster but computationally expensive processing call for new
approaches in designing routers.
  This survey takes a look at the core functionalities, like packet
classification, buffer memory management, switch scheduling and output link
scheduling performed by a router in its data path processing and discusses the
algorithms that aim to reduce the performance bound for these operations. An
important requirement for the routers is to provide Quality of Service
guarantees. We propose an algorithm to guarantee QoS in Input Queued Routers.
The hardware solution to speed up router operation was Application Specific
Integrated Circuits (ASICs). But the inherent inflexibility of the method is a
demerit as network standards and application requirements are constantly
evolving, which seek a faster turnaround time to keep up with the changes. The
promise of Network Processors (NP) is the flexibility of general-purpose
processors together with the speed of ASICs. We will study the architectural
choices for the design of Network Processors and focus on some of the
commercially available NPs. There is a plethora of NP vendors in the market.
The discussion on the NP benchmarks sets the normalizing platform to evaluate
these NPs."
"The proliferation of IEEE 802.11-based wireless LANs opens up avenues for
creation of several tetherless and mobility oriented services. Most of these
services, like voice over WLAN, media streaming etc., generate delay and
bandwidth sensitive traffic. These traffic flows require undisrupted network
connectivity with some QoS guarantees. Unfortunately, there is no adequate
support built into these wireless LANs towards QoS provisioning. Further, the
network layer handoff latency incurred by mobile nodes in these wireless LANs
is too high for real-time applications to function properly. In this paper, we
describe a QoS mechanism, called Rether, to effectively support bandwidth
guarantee on wireless LANs. Rether is designed to support the current wireless
LAN technologies like 802.11b and 802.11a with a specific capability of being
tailored for QoS oriented technology like 802.11e. We also describe a
low-latency handoff mechanism which expedites network level handoff to provide
real-time applications with an added advantage of seamless mobility."
"Today, component oriented middlewares are used to design, develop and deploy
easily distributed applications, by ensuring the heterogeneity,
interoperability, and reuse of the software modules, and the separation between
the business code encapsulated in the components and the system code managed by
the containers. Several standards answer this definition such as: CCM (CORBA
Component Model), EJB (Enterprise Java Beans) and .Net. However these standards
offer a limited and fixed number of system services, removing any possibility
to add system services or to reconfigure dynamically the middleware. Our works
propose mechanisms to add and to adapt dynamically the system services, based
on a reconfiguration language which is dynamically adaptable to the need of the
reconfiguration, and on a tool of dynamic reconfiguration, a prototype was
achieved for the OpenCCM platform, that is an implementation of the CCM
specification. This work was partially financed by the european project
IST-COACH (2001-34445)."
"Nowadays, numerous component models are used for various purposes: to build
applications, middleware or even operating systems. Those models commonly
support structure reconfiguration, that is modification of application's
architecture at runtime. On the other hand, very few allow implementation
reconfiguration, that is runtime modification of the code of components
building the application. In this article we present the work we performed on
JULIA, a Java-based implementation of the FRACTAL component model, in order for
it to support implementation reconfigurations. We show how we overcame the
limitations of Java class loading mechanism to allow runtime modifications of
components' implementation and interfaces. We also describe the integration of
our solution with the JULIA ADL."
"Software deployment can turn into a baffling problem when the components
being deployed exhibit non-functional requirements. If the platform on which
such components are deployed cannot satisfy their non-functional requirements,
then they may in turn fail to perform satisfactorily. In this paper, we present
a contract-based approach to take a specific category of non-functional
properties specified by components into account, that is those that pertain to
the resources that are necessary for their execution."
"With the development of the networks and the Internet, the problems of
automated deployment on broad scale became increasingly crucial. Software
deployment is a complex process covering several activities going from the
configuration to the retirement of a software product. During the execution of
a deployment process, exceptions can be met which put the site in an incoherent
state. To solve them, we propose an approach based on transactional concepts
which describes the actions to be undertaken when an exceptional situation is
met during the deployment process. The approach guaranties the respect of the
site's consistency by preserving part of the work already carried out by the
process. This article presents our approach and an experimentation made in an
academic deployment system."
"Bossa is a framework to develop new processes schedulers in commodity
operating systems. Although Bossa enables fine-grained management of the
processor through new scheduling policies, deploying an application with its
own scheduler raises some problems. In this paper we study the problems caused
when deploying an application and its scheduler and to adresse these, we
propose to establish Quality of Service contracts and mechanisms to reconfigure
the scheduler hierarchy."
"Software components turn out to be a convenient model to build complex
applications for scientific computing and to run them on a computational grid.
However, deploying complex, component-based applications in a grid environment
is particularly arduous. To prevent the user from directly dealing with a large
number of execution hosts and their heterogeneity within a grid, the
application deployment phase must be as automatic as possible. This paper
describes an architecture for automatic deployment of component-based
applications on computational grids. In the context of the CORBA Component
Model (CCM), this paper details all the steps to achieve an automatic
deployment of components as well as the entities involved: a grid access
middleware and its grid information service (like OGSI), a component deployment
model, as specified by CCM, an enriched application description and a
deployment planner in order to select resources and map components onto
computers."
"Autonomic computing has been proposed recently as a way to address the
difficult management of applications whose complexity is constantly increasing.
Autonomous applications will have to be especially flexible and be able to
monitor themselves permanently. This work presents a framework, Pandora, which
eases the construction of applications that satisfy this double goal. Pandora
relies on an original application programming pattern - based on stackable
layers and message passing - to obtain minimalist model and architecture that
allows to control the overhead imposed by the full reflexivity of the
framework. Besides, a prototype of the framework has been implemented in C++. A
detailed performance study, together with examples of use, complement this
presentation"
"The ever growing software complexity suggests that they will never be bugfree
and therefore secure. Software compagnies regulary publish updates. But maybe
because of lack of time or care or maybe because stopping application is
annoying, such updates are rarely if ever deployed on users' machines. We
propose an integrated tool allowing system administrators to deploy critical
security updates on the fly on applications running remotly without end-user
intervention. Our approach is based on an aspect weaving system, Arachne, that
dynamicaly rewrites binary code. Hence updated applications are still running
while they are updated. Our second tool Minerve integrates Arachne within the
standart updating process: Minerve takes a patch produced by dif and eventually
builds a dynamic patch that can later be woven to update the application on the
fly. In addition, Minerve allows to consult patches translated in a dedicated
language and hence eases auditing tasks."
"The new applications being intended for more and more heterogeneous
environments, it is necessary to propose solutions of development which answer
in best the necessities of adaptation of new services. Component-based
programming partially answers this aim, allowing easy replacement of software
blocks in order to provide the most adapted version of a component.
Nevertheless, most of the industrial component-based model implementations do
not allow to provide to components the most adapted technical services (naming,
trading, security, transaction, etc.). In this paper, we suggest defining
technical services themselves under the shape of components. We shall detail
our proposition, by basing it on the Fractal component model of Objectweb.
Then, we shall bring solutions for the use of these new component-based
technical services and shall propose a set of management components which allow
to administer in a dynamic and stand-alone way the obtained components. Finally
we present the prototype of the proposed solution."
"We study the use of local heuristics to determine spanning subgraphs for use
in the dissemination of information in complex networks. We introduce two
different heuristics and analyze their behavior in giving rise to spanning
subgraphs that perform well in terms of allowing every node of the network to
be reached, of requiring relatively few messages and small node bandwidth for
information dissemination, and also of stretching paths with respect to the
underlying network only modestly. We contribute a detailed mathematical
analysis of one of the heuristics and provide extensive simulation results on
random graphs for both of them. These results indicate that, within certain
limits, spanning subgraphs are indeed expected to emerge that perform well in
respect to all requirements. We also discuss the spanning subgraphs' inherent
resilience to failures and adaptability to topological changes."
"Network-Wide Broadcast (NWB) is a common operation in Mobile Ad hoc Networks
(MANETs) used by routing protocols to discover routes and in group
communication operations. NWB is commonly performed via flooding, which has
been shown to be expensive in dense MANETs because of its high redundancy.
Several efforts have targeted reducing the redundancy of floods. In this work,
we target another problem that can substantially impact the success of NWBs:
since MAC level broadcasts are unreliable, it is possible for critical
rebroadcasts to be lost, leading to a significant drop in the node coverage.
This is especially true under heavy load and in sparse topologies. We show that
the techniques that target reducing the overhead of flooding, reduce its
inherent redundancy and harm its reliability. In addition, we show that static
approaches are more vulnerable to this problem. We then present a selective
rebroadcast approach to improve the robustness of NWBs. We show that our
approach leads to considerable improvement in NWB coverage relative to a
recently proposed solution to this problem, with a small increase in overhead.
The proposed approaches do not require proactive neighbor discovery and are
therefore resilient to mobility. Finally, the solution can be added to
virtually all NWB approaches to improve their reliability."
"A revolution is taking place in telecommunication networks. New services are
appearing on platforms such as third generation cellular phones (3G) and
broadband Internet access. This motivates the transition from mostly switched
to all-IP networks. The replacement of the traditional shallow and well-defined
interface to telephony networks brings accrued flexibility, but also makes the
network accordingly difficult to properly secure. This paper surveys the
implications of this transition on security issues in telecom applications. It
does not give an exhaustive list of security tools or security protocols. Its
goal is rather to initiate the reader to the security issues brought to carrier
class servers by this revolution."
"Spam costs US corporations upwards of $8.9 billion a year, and comprises as
much as 40% of all email received. Solutions exist to reduce the amount of spam
seen by end users, but cannot withstand sophisticated attacks. Worse yet, many
will occasionally misclassify and silently drop legitimate email. Spammers take
advantage of the near-zero cost of sending email to flood the network, knowing
that success even a tiny fraction of the time means a profit. End users,
however, have proven unwilling to pay money to send email to friends and
family.
  We show that it is feasible to extend the existing mail system to reduce the
amount of unwanted email, without misclassifying email, and without charging
well-behaved users. We require that bulk email senders accurately classify each
email message they send as an advertisement with an area of interest or else be
charged a small negative incentive per message delivered. Recipients are able
to filter out email outside their scope of interest, while senders are able to
focus their sendings to the appropriate audience."
"Inspired by the Statistical Physics of complex networks, wireless multihop ad
hoc communication networks are considered in abstracted form. Since such
engineered networks are able to modify their structure via topology control, we
search for optimized network structures, which maximize the end-to-end
throughput performance. A modified version of betweenness centrality is
introduced and shown to be very relevant for the respective modeling. The
calculated optimized network structures lead to a significant increase of the
end-to-end throughput. The discussion of the resulting structural properties
reveals that it will be almost impossible to construct these optimized
topologies in a technologically efficient distributive manner. However, the
modified betweenness centrality also allows to propose a new routing metric for
the end-to-end communication traffic. This approach leads to an even larger
increase of throughput capacity and is easily implementable in a
technologically relevant manner."
"The concept of small worlds is introduced into the physical topology of
wireless networks in this work. A. Helmy provided two con- struction schemes of
small worlds for the wireless networks, link rewiring and link addition, but he
mainly focused on the virtual topology. Based on the broadcasting nature of the
radio transmission, we propose a con- struction scheme of small worlds for the
physical topology of Multiple- Input Multiple-Output (MIMO) wireless networks.
Besides the topology- related topics, we also evaluate the reduction of the
power required by a request."
"This paper is concerned with the characterization of the relationship between
topology and traffic dynamics. We use a model of network generation that allows
the transition from random to scale free networks. Specifically, we consider
three different topological types of network: random, scale-free with \gamma =
3, scale-free with \gamma = 2. By using a novel LRD traffic generator, we
observe best performance, in terms of transmission rates and delivered packets,
in the case of random networks. We show that, even if scale-free networks are
characterized by shorter characteristic-path- length (the lower the exponent,
the lower the path-length), they show worst performances in terms of
communication. We conjecture this could be explained in terms of changes in the
load distribution, defined here as the number of shortest paths going through a
given vertex. In fact, that distribu- tion is characterized by (i) a decreasing
mean (ii) an increas- ing standard deviation, as the networks becomes
scale-free (especially scale-free networks with low exponents). The use of a
degree-independent server also discriminates against a scale-free structure. As
a result, since the model is un- controlled, most packets will go through the
same vertices, favoring the onset of congestion."
"Most MANET (Mobile Ad hoc NETwork) research assumes idealized propagation
models. Experimental results have shown significant divergence from simulation
results due to the effect of signal fading in realistic wireless communication
channels. In this paper, we characterize the impact of fading on protocol
performance. We first study the effect of fading on MAC performance and show
that its effect can be dominating. One of our important conclusions is that
eliminating RTS/CTS packets results in more effective operation under fading.
We also identify an unfairness problem that arises due to backoffs in the
presence of fading. Moreover, fading results in several subtle interactions
between the MAC and routing layers. We identify several of these problems and
make observations about effective approaches for addressing them. For example,
the criteria for determining the best path should not only consider the link
status but also the link order. In addition, because routing protocols rely on
MAC level transmission failure (when the retry limit is exceeded), route
failure errors are often generated unnecessarily. Finally, because MAC level
broadcasts are unreliable, they are especially vulnerable to fading. We analyze
these effects and outline preliminary solutions to them."
"Routing in Delay Tolerant Networks (DTNs) benefits considerably if one can
take advantage of knowledge concerning node mobility. The main contribution of
this paper is the definition of a generic routing scheme for DTNs using a
high-dimensional Euclidean space constructed upon nodes' mobility patterns. For
example, nodes are represented as points having as coordinates their
probability of being found in each possible location. We present simulation
results indicating that such a scheme can be beneficial in a scenario inspired
by studies done on real mobility traces. This work should open the way to
further use of the virtual space formalism in DTN routing."
"In large-scale wireless networks such as mobile ad hoc and sensor networks,
efficient and robust service discovery and data-access mechanisms are both
essential and challenging. Rendezvous-based mechanisms provide a valuable
solution for provisioning a wide range of services. In this paper, we describe
Rendezvous Regions (RRs) - a novel scalable rendezvous-based architecture for
wireless networks. RR is a general architecture proposed for service location
and bootstrapping in ad hoc networks, in addition to data-centric storage,
configuration, and task assignment in sensor networks. In RR the network
topology is divided into geographical regions, where each region is responsible
for a set of keys representing the services or data of interest. Each key is
mapped to a region based on a hash-table-like mapping scheme. A few elected
nodes inside each region are responsible for maintaining the mapped
information. The service or data provider stores the information in the
corresponding region and the seekers retrieve it from there. We run extensive
detailed simulations, and high-level simulations and analysis, to investigate
the design space, and study the architecture in various environments including
node mobility and failures. We evaluate it against other approaches to identify
its merits and limitations. The results show high success rate and low overhead
even with dynamics. RR scales to large number of nodes and is highly robust and
efficient to node failures. It is also robust to node mobility and location
inaccuracy with a significant advantage over point-based rendezvous mechanisms."
"Geocasting is the delivery of packets to nodes within a certain geographic
area. For many applications in wireless ad hoc and sensor networks, geocasting
is an important and frequent communication service. The challenging problem in
geocasting is distributing the packets to all the nodes within the geocast
region with high probability but with low overhead. According to our study we
notice a clear tradeoff between the proportion of nodes in the geocast region
that receive the packet and the overhead incurred by the geocast packet
especially at low densities and irregular distributions. We present two novel
protocols for geocasting that achieve high delivery rate and low overhead by
utilizing the local location information of nodes to combine geographic routing
mechanisms with region flooding. We show that the first protocol
Geographic-Forwarding-Geocast (GFG) has close-to-minimum overhead in dense
networks and that the second protocol Geographic-Forwarding-Perimeter-Geocast
(GFPG) provides guaranteed delivery without global flooding or global network
information even at low densities and with the existence of region gaps or
obstacles. An adaptive version of the second protocol (GFPG*) has the desirable
property of perfect delivery at all densities and close-to-minimum overhead at
high densities. We evaluate our mechanisms and compare them using simulation to
other proposed geocasting mechanisms. The results show the significant
improvement in delivery rate (up to 63% higher delivery percentage in low
density networks) and reduction in overhead (up to 80% reduction) achieved by
our mechanisms. We hope for our protocols to become building block mechanisms
for dependable sensor network architectures that require robust efficient
geocast services."
"The transmission of electric signals on a coupled line with distributed
RLC-parameters is considered as a propagation of a dissipative quasi particle.
A calculation technique is developed, alternative to the one, accepted for
lumped lines. The relativistic wave equation for the transient response is
deduced following the common Ohm-low-type considerations. The exact expressions
for the Green function, for information transfer velocity and for time delay
are obtained on this base. The fundamental restrictions on the measurement
accuracy of the time delay are pointed out. The obtained results are naturally
generalized for the multilevel networks of the arbitrary dimension."
"Today's Internet maps, which are all collected from a small number of vantage
points, are falling short of being accurate. We suggest here a paradigm shift
for this task. DIMES is a distributed measurement infrastructure for the
Internet that is based on the deployment of thousands of light weight
measurement agents around the globe.
  We describe the rationale behind DIMES deployment, discuss its design
trade-offs and algorithmic challenges, and analyze the structure of the
Internet as it seen with DIMES."
"Energy is one of the most important resources in wireless sensor networks.
Recently, the mobility of base station has been exploited to preserve the
energy. But in event driven networks, the mobility issue is quite different
from the continuous monitoring one because only a small portion of sensor node
has data to send at one time. The number of sensor node that forward traffic
should be minimized to prolong the network lifetime. In this paper, we propose
a movement-assisted energy conserving method which tries to reduce the amount
of forwarding sensor node by directing the base station to move close to the
hotspots. This method achieves good performance especially when applied to a
network with a set of cooperative mobile base station. Extensive simulation has
been done to verify the effectiveness of the propose schema."
"We consider networks of anonymous sensors and address the problem of
constructing routes for the delivery of information from a group of sensors in
response to a query by a sink. In order to circumvent the restrictions imposed
by anonymity, we rely on using the power level perceived by the sensors in the
query from the sink. We introduce a simple distributed algorithm to achieve the
building of routes to the sink and evaluate its performance by means of
simulations."
"The development of veracious models of the Internet topology has received a
lot of attention in the last few years. Many proposed models are based on
topologies derived from RouteViews BGP table dumps (BTDs). However, BTDs do not
capture all AS-links of the Internet topology and most importantly the number
of the hidden AS-links is unknown, resulting in AS-graphs of questionable
quality. As a first step to address this problem, we introduce a new
AS-topology discovery methodology that results in more complete and accurate
graphs. Moreover, we use data available from existing measurement facilities,
circumventing the burden of additional measurement infrastructure. We deploy
our methodology and construct an AS-topology that has at least 61.5% more
AS-links than BTD-derived AS-topologies we examined. Finally, we analyze the
temporal and topological properties of the augmented graph and pinpoint the
differences from BTD-derived AS-topologies."
"We conduct the most comprehensive study of WLAN traces to date. Measurements
collected from four major university campuses are analyzed with the aim of
developing fundamental understanding of realistic user behavior in wireless
networks. Both individual user and inter-node (group) behaviors are
investigated and two classes of metrics are devised to capture the underlying
structure of such behaviors.
  For individual user behavior we observe distinct patterns in which most users
are 'on' for a small fraction of the time, the number of access points visited
is very small and the overall on-line user mobility is quite low. We clearly
identify categories of heavy and light users. In general, users exhibit high
degree of similarity over days and weeks.
  For group behavior, we define metrics for encounter patterns and friendship.
Surprisingly, we find that a user, on average, encounters less than 6% of the
network user population within a month, and that encounter and friendship
relations are highly asymmetric. We establish that number of encounters follows
a biPareto distribution, while friendship indexes follow an exponential
distribution. We capture the encounter graph using a small world model, the
characteristics of which reach steady state after only one day.
  We hope for our study to have a great impact on realistic modeling of network
usage and mobility patterns in wireless networks."
"Denial-of-Service (DoS) and Distributed DoS (DDoS) attacks can cause serious
problems in wireless networks due to limited network and host resources.
Attacker traceback is a promising solution to take a proper countermeasure near
the attack origins, to discourage attackers from launching attacks, and for
forensics. However, attacker traceback in Mobile Ad-hoc Networks (MANETs) is a
challenging problem due to the dynamic topology, and limited network resources.
It is especially difficult to trace back attacker(s) when they are moving to
avoid traceback. In this paper, we introduce the ATTENTION protocol framework,
which pays special attention to MAC layer abnormal activity under attack.
ATTENTION consists of three classes, namely, coarse-grained traceback,
fine-grained traceback and spatio-temporal fusion architecture. For
energy-efficient attacker searching in MANETs, we also utilize small-world
model. Our simulation analysis shows 79% of success rate in DoS attacker
traceback with coarse-grained attack signature. In addition, with fine-grained
attack signature, it shows 97% of success rate in DoS attacker traceback and
83% of success rate in DDoS attacker traceback. We also show that ATTENTION has
robustness against node collusion and mobility."
"The current framework of network utility maximization for distributed rate
allocation assumes fixed channel code rates. However, by adapting the physical
layer channel coding, different rate-reliability tradeoffs can be achieved on
each link and for each end user. Consider a network where each user has a
utility function that depends on both signal quality and data rate, and each
link may provide a `fatter' (`thinner') information `pipe' by allowing a higher
(lower) decoding error probability. We propose two distributed, pricing-based
algorithms to attain optimal rate-reliability tradeoff, with an interpretation
that each user provides its willingness to pay for reliability to the network
and the network feeds back congestion prices to users. The proposed algorithms
converge to a tradeoff point between rate and reliability, which is proved to
be globally optimal for codes with sufficiently large codeword lengths and user
utilities with sufficiently negative curvatures."
"Despite prevailing concerns that the current Internet interdomain routing
system will not scale to meet the needs of the 21st century global Internet,
networking research has not yet led to the construction of a new routing
architecture with satisfactory and mathematically provable scalability
characteristics. Worse, continuing empirical trends of the existing routing and
topology structure of the Internet are alarming: the foundational principles of
the current routing and addressing architecture are an inherently bad match for
the naturally evolving structure of Internet interdomain topology. We are
fortunate that a sister discipline, theory of distributed computation, has
developed routing algorithms that offer promising potential for genuinely
scalable routing on realistic Internet-like topologies. Indeed, there are many
recent breakthroughs in the area of compact routing, which has been shown to
drastically outperform, in terms of efficiency and scalability, even the
boldest proposals developed in networking research. Many open questions remain,
but we believe the applicability of compact routing techniques to Internet
interdomain routing is a research area whose potential payoff for the future of
networking is too high to ignore."
"We present a simple model reproducing the long-range autocorrelations and the
power spectrum of the web traffic. The model assumes the traffic as Poisson
flow of files with size distributed according to the power-law. In this model
the long-range autocorrelations are independent of the network properties as
well as of inter-packet time distribution."
"Internet traffic exhibits self-similarity and long-range dependence (LRD) on
various time scales. A well studied issue is the estimation of statistical
parameters characterizing traffic self-similarity and LRD, such as the Hurst
parameter H. In this paper, we propose to adapt the Modified Allan Variance
(MAVAR), a time-domain quantity originally conceived to discriminate fractional
noise in frequency stability measurement, to estimate the Hurst parameter of
LRD traffic traces and, more generally, to identify fractional noise components
in network traffic. This novel method is validated by comparison to one of the
best techniques for analyzing self-similar and LRD traffic: the logscale
diagram based on wavelet analysis. Both methods are applied to pseudo-random
LRD data series, generated with assigned values of H. The superior spectral
sensitivity of MAVAR achieves outstanding accuracy in estimating H, even better
than the logscale method. The behaviour of MAVAR with most common deterministic
signals that yield nonstationarity in data under analysis is also studied.
Finally, both techniques are applied to a real IP traffic trace, providing a
sound example of the usefulness of MAVAR also in traffic characterization, to
complement other established techniques as the logscale method."
"This article first addresses applicability of Euclidean models to the domain
of Internet routing. Those models are found (limitedly) applicable. Then a
simplistic model of routing is constructed for Euclidean plane densely covered
with points-routers. The model guarantees low stretch and logarithmical size of
routing tables at any node. The paper concludes with a discussion on
applicability of the model to real-world Internet routing."
"This paper critically examines some propositions and arguments of
cs.NI/0508021 regarding applicability of hierarchical routing and perspectives
of compact routing. Arguments against the former are found to be inaccurate
while the latter is found to be equivalent to well-known deployed solutions.
Also, multiple (stacked) application of compact-routing solutions is found to
be equivalent to hierarchical routing."
"Decoupling the permanent identifier of a node from the node's
topology-dependent address is a promising approach toward completely scalable
self-organizing networks. A group of proposals that have adopted such an
approach use the same structure to: address nodes, perform routing, and
implement location service. In this way, the consistency of the routing
protocol relies on the coherent sharing of the addressing space among all nodes
in the network. Such proposals use a logical tree-like structure where routes
in this space correspond to routes in the physical level. The advantage of
tree-like spaces is that it allows for simple address assignment and
management. Nevertheless, it has low route selection flexibility, which results
in low routing performance and poor resilience to failures. In this paper, we
propose to increase the number of paths using incomplete hypercubes. The design
of more complex structures, like multi-dimensional Cartesian spaces, improves
the resilience and routing performance due to the flexibility in route
selection. We present a framework for using hypercubes to implement indirect
routing. This framework allows to give a solution adapted to the dynamics of
the network, providing a proactive and reactive routing protocols, our major
contributions. We show that, contrary to traditional approaches, our proposal
supports more dynamic networks and is more robust to node failures."
The paper is taken out.
"Current directions in network routing research have not kept pace with the
latest developments in network architectures, such as peer-to-peer networks,
sensor networks, ad-hoc wireless networks, and overlay networks. A common
characteristic among all of these new technologies is the presence of highly
dynamic network topologies. Currently deployed single-path routing protocols
cannot adequately cope with this dynamism, and existing multi-path algorithms
make trade-offs which lead to less than optimal performance on these networks.
This drives the need for routing protocols designed with the unique
characteristics of these networks in mind.
  In this paper we propose the notion of reachability routing as a solution to
the challenges posed by routing on such dynamic networks. In particular, our
formulation of reachability routing provides cost-sensitive multi-path
forwarding along with loop avoidance within the confines of the Internet
Protocol (IP) architecture. This is achieved through the application of
reinforcement learning within a probabilistic routing framework. Following an
explanation of our design decisions and a description of the algorithm, we
provide an evaluation of the performance of the algorithm on a variety of
network topologies. The results show consistently superior performance compared
to other reinforcement learning based routing algorithms."
"Geographical routing protocols have several desirable features for use in ad
hoc and sensor networks but are susceptible to voids and localization errors.
Virtual coordinate systems are an alternative solution to geographically based
routing protocols that works by overlaying a coordinate system on the sensors
relative to well chosen reference points. VC is resilient to localization
errors; however, we show that it is vulnerable to different forms of the void
problem and have no viable complementary approach to overcome them.
Specifically, we show that there are instances when packets reach nodes with no
viable next hop nodes in the forwarding set. In addition, it is possible for
nodes with the same coordinates to arise at different points in the network in
the presence of voids. This paper identifies and analyzes these problems. It
also compares several existing routing protocols based on Virtual Coordinate
systems. Finally, it presents a new routing algorithm that uses backtracking to
overcome voids to achieve high connectivity in the greedy phase, higher overall
path quality and more resilience to localization errors. We show these
properties using extensive simulation analysis."
"In the wide area master-slave PLC (powerline communication) system, the
source node cannot reach the destination node without packet relay. Due to the
time-variable attenuation in the powerline, the communication distance cannot
be defined. Two kind of dynamic repeater algorithms are developed, dynamic
source routing and flooding based routing. In this paper, we use analytic
approach to compare the performance of those two routing protocols. We give
formulas to calculate the average duration of a polling cycle for each
protocols. Then we present simulation results to bolster the results of our
analysis. We use three metrics, which are bandwidth consumed for routing
signaling, normalized routing load and average duration of a polling cycle to
evaluate those routing protocols."
"We consider the problem of distributing a vaccine for immunizing a scale-free
network against a given virus or worm. We introduce a new method, based on
vaccine dissemination, that seems to reflect more accurately what is expected
to occur in real-world networks. Also, since the dissemination is performed
using only local information, the method can be easily employed in practice.
Using a random-graph framework, we analyze our method both mathematically and
by means of simulations. We demonstrate its efficacy regarding the trade-off
between the expected number of nodes that receive the vaccine and the network's
resulting vulnerability to develop an epidemic as the virus or worm attempts to
infect one of its nodes. For some scenarios, the new method is seen to render
the network practically invulnerable to attacks while requiring only a small
fraction of the nodes to receive the vaccine."
"We present the first complete measurement of the Chinese Internet topology at
the autonomous systems (AS) level based on traceroute data probed from servers
of major ISPs in mainland China. We show that both the Chinese Internet AS
graph and the global Internet AS graph can be accurately reproduced by the
Positive-Feedback Preference (PFP) model with the same parameters. This result
suggests that the Chinese Internet preserves well the topological
characteristics of the global Internet. This is the first demonstration of the
Internet's topological fractality, or self-similarity, performed at the level
of topology evolution modeling."
"Because a delay tolerant network (DTN) can often be partitioned, the problem
of routing is very challenging. However, routing benefits considerably if one
can take advantage of knowledge concerning node mobility. This paper addresses
this problem with a generic algorithm based on the use of a high-dimensional
Euclidean space, that we call MobySpace, constructed upon nodes' mobility
patterns. We provide here an analysis and the large scale evaluation of this
routing scheme in the context of ambient networking by replaying real mobility
traces. The specific MobySpace evaluated is based on the frequency of visit of
nodes for each possible location. We show that the MobySpace can achieve good
performance compared to that of the other algorithms we implemented, especially
when we perform routing on the nodes that have a high connection time. We
determine that the degree of homogeneity of mobility patterns of nodes has a
high impact on routing. And finally, we study the ability of nodes to learn
their own mobility patterns."
"The internet structure is extremely complex. The Positive-Feedback Preference
(PFP) model is a recently introduced internet topology generator. The model
uses two generic algorithms to replicate the evolution dynamics observed on the
internet historic data. The phenomenological model was originally designed to
match only two topology properties of the internet, i.e. the rich-club
connectivity and the exact form of degree distribution. Whereas numerical
evaluation has shown that the PFP model accurately reproduces a large set of
other nontrivial characteristics as well. This paper aims to investigate why
and how this generative model captures so many diverse properties of the
internet. Based on comprehensive simulation results, the paper presents a
detailed analysis on the exact origin of each of the topology properties
produced by the model. This work reveals how network evolution mechanisms
control the obtained topology properties and it also provides insights on
correlations between various structural characteristics of complex networks."
"A fundamental understanding of gain provided by motion prediction in wireless
ad hoc routing is currently lacking. This paper examines benefits in routing
obtainable via prediction. A theoretical best-case non-predictive routing model
is quantified in terms of both message overhead and update time for
non-predictive routing. This best- case model of existing routing performance
is compared with predictive routing. Several specific instances of predictive
improvements in routing are examined. The primary contribution of this paper is
quantification of predictive gain for wireless ad hoc routing."
"The primary contribution of this work is to examine the energy efficiency of
pulse coupled oscillation for time synchronization in a realistic wireless
network environment and to explore the impact of mobility on convergence rate.
Energy coupled oscillation is susceptible to interference; this approach uses
reception and decoding of short packet bursts to eliminate this problem. The
energy efficiency of a commonly used timestamp broadcast algorithm is compared
and contrasted with pulse-coupled oscillation. The emergent pulse coupled
oscillation technique shows greater energy efficiency as well as robustness
with mobility. A proportion of the sensors may be integrated with GPS receivers
in order to obtain a master clock time."
"Motivated by the problem of the coexistence on transmission links of
telecommunication networks of elastic and unresponsive traffic, we study in
this paper the impact on the busy period of an M/M/1 queue of a small
perturbation in the server rate. The perturbation depends upon an independent
stationary process (X(t)) and is quantified by means of a parameter \eps \ll 1.
We specifically compute the two first terms of the power series expansion in
\eps of the mean value of the busy period duration. This allows us to study the
validity of the Reduced Service Rate (RSR) approximation, which consists in
comparing the perturbed M/M/1 queue with the M/M/1 queue where the service rate
is constant and equal to the mean value of the perturbation. For the first term
of the expansion, the two systems are equivalent. For the second term, the
situation is more complex and it is shown that the correlations of the
environment process (X(t)) play a key role."
"We study in this paper the integration of elastic and streaming traffic on a
same link in an IP network. We are specifically interested in the computation
of the mean bit rate obtained by a data transfer. For this purpose, we consider
that the bit rate offered by streaming traffic is low, of the order of
magnitude of a small parameter \eps \ll 1 and related to an auxiliary
stationary Markovian process (X(t)). Under the assumption that data transfers
are exponentially distributed, arrive according to a Poisson process, and share
the available bandwidth according to the ideal processor sharing discipline, we
derive the mean bit rate of a data transfer as a power series expansion in
\eps. Since the system can be described by means of an M/M/1 queue with a
time-varying server rate, which depends upon the parameter \eps and process
(X(t)), the key issue is to compute an expansion of the area swept under the
occupation process of this queue in a busy period. We obtain closed formulas
for the power series expansion in \eps of the mean bit rate, which allow us to
verify the validity of the so-called reduced service rate at the first order.
The second order term yields more insight into the negative impact of the
variability of streaming flows."
"A visualisation tool is presented to facilitate the study on large-scale
communications networks. This tool provides a simple and effective way to
summarise the topology of a complex network at a coarse level."
"It is now well known that Internet traffic exhibits self-similarity, which
cannot be described by traditional Markovian models such as the Poisson
process. The causes of self-similarity of network traffic must be identified
because understanding the nature of network traffic is critical in order to
properly design and implement computer networks and network services like the
World Wide Web. While some researchers have argued self similarity is generated
by the typical applications or caused by Transport layer Protocols, it is also
possible that the CSMA/CD protocol may cause or at least contribute to this
phenomenon. In this paper, we use NS simulator to study the effect of CSMA/CD
Exponential Backoff retransmission algorithm on Traffic Self similarity."
"In the past few years, the network measurement community has been interested
in the problem of internet topology discovery using a large number (hundreds or
thousands) of measurement monitors. The standard way to obtain information
about the internet topology is to use the traceroute tool from a small number
of monitors. Recent papers have made the case that increasing the number of
monitors will give a more accurate view of the topology. However, scaling up
the number of monitors is not a trivial process. Duplication of effort close to
the monitors wastes time by reexploring well-known parts of the network, and
close to destinations might appear to be a distributed denial-of-service (DDoS)
attack as the probes converge from a set of sources towards a given
destination. In prior work, authors of this report proposed Doubletree, an
algorithm for cooperative topology discovery, that reduces the load on the
network, i.e., router IP interfaces and end-hosts, while discovering almost as
many nodes and links as standard approaches based on traceroute. This report
presents our open-source and freely downloadable implementation of Doubletree
in a tool we call traceroute@home. We describe the deployment and validation of
traceroute@home on the PlanetLab testbed and we report on the lessons learned
from this experience. We discuss how traceroute@home can be developed further
and discuss ideas for future improvements."
"Research on performance, robustness, and evolution of the global Internet is
fundamentally handicapped without accurate and thorough knowledge of the nature
and structure of the contractual relationships between Autonomous Systems
(ASs). In this work we introduce novel heuristics for inferring AS
relationships. Our heuristics improve upon previous works in several technical
aspects, which we outline in detail and demonstrate with several examples.
Seeking to increase the value and reliability of our inference results, we then
focus on validation of inferred AS relationships. We perform a survey with ASs'
network administrators to collect information on the actual connectivity and
policies of the surveyed ASs. Based on the survey results, we find that our new
AS relationship inference techniques achieve high levels of accuracy: we
correctly infer 96.5% customer to provider (c2p), 82.8% peer to peer (p2p), and
90.3% sibling to sibling (s2s) relationships. We then cross-compare the
reported AS connectivity with the AS connectivity data contained in BGP tables.
We find that BGP tables miss up to 86.2% of the true adjacencies of the
surveyed ASs. The majority of the missing links are of the p2p type, which
highlights the limitations of present measuring techniques to capture links of
this type. Finally, to make our results easily accessible and practically
useful for the community, we open an AS relationship repository where we
archive, on a weekly basis, and make publicly available the complete Internet
AS-level topology annotated with AS relationship information for every pair of
AS neighbors."
"Failure restoration at the IP layer in IP-over-WDM networks requires to map
the IP topology on the WDM topology in such a way that a failure at the WDM
layer leaves the IP topology connected. Such a mapping is called $survivable$.
As finding a survivable mapping is known to be NP-complete, in practice it
requires a heuristic approach. We have introduced in [1] a novel algorithm
called ``SMART'', that is more effective and scalable than the heuristics known
to date. Moreover, the formal analysis of SMART [2] has led to new
applications: the formal verification of the existence of a survivable mapping,
and a tool tracing and repairing the vulnerable areas of the network. In this
paper we extend the theoretical analysis in [2] by considering $multiple
failures$."
"We consider the problem of network coding across multiple unicasts. We give,
for wired and wireless networks, efficient polynomial time algorithms for
finding optimal network codes within the class of network codes restricted to
XOR coding between pairs of flows."
"Positioning systems in self-organizing networks generally rely on
measurements such as delay and received signal strength, which may be difficult
to obtain and often require dedicated equipment. An alternative to such
approaches is to use simple connectivity information, that is, the presence or
absence of a link between any pair of nodes, and to extend it to hop-counts, in
order to obtain an approximate coordinate system. Such an approximation is
sufficient for a large number of applications, such as routing. In this paper,
we propose Jumps, a positioning system for those self-organizing networks in
which other types of (exact) positioning systems cannot be used or are deemed
to be too costly. Jumps builds a multiple coordinate system based solely on
nodes neighborhood knowledge. Jumps is interesting in the context of wireless
sensor networks, as it neither requires additional embedded equipment nor
relies on any nodes capabilities. While other approaches use only three
hop-count measurements to infer the position of a node, Jumps uses an arbitrary
number. We observe that an increase in the number of measurements leads to an
improvement in the localization process, without requiring a high dense
environment. We show through simulations that Jumps, when compared with
existing approaches, reduces the number of nodes sharing the same coordinates,
which paves the way for functions such as position-based routing."
"This paper has been withdrawn by the author due to the need for further
revision."
"We present a unified analytical framework within which power control, rate
allocation, routing, and congestion control for wireless networks can be
optimized in a coherent and integrated manner. We consider a multi-commodity
flow model with an interference-limited physical-layer scheme in which power
control and routing variables are chosen to minimize the sum of convex link
costs reflecting, for instance, queuing delay. Distributed network algorithms
where joint power control and routing are performed on a node-by-node basis are
presented. We show that with appropriately chosen parameters, these algorithms
iteratively converge to the global optimum from any initial point with finite
cost. Next, we study refinements of the algorithms for more accurate link
capacity models, and extend the results to wireless networks where the
physical-layer achievable rate region is given by an arbitrary convex set, and
the link costs are strictly quasiconvex. Finally, we demonstrate that
congestion control can be seamlessly incorporated into our framework, so that
algorithms developed for power control and routing can naturally be extended to
optimize user input rates."
"To bring coherence between Wireless Access Protocol (WAP) and Hyper Text
Transfer Protocol (HTTP), in this paper, we have proposed an enhanced Internet
framework, which incorporates a new markup language and a browser compatible
with both of the access control protocols. This Markup Language and the browser
enables co-existence of both Hyper Text Markup Language (HTML) and Wireless
Markup Language (WML) contents in a single source file, whereas the browser
incorporates the ability to hold contents compliant with both HTTP and WAP. The
proposed framework also bridges the security gap that is present in the
existing mobile Internet framework.
  Keywords: WAP, WML, HTTP, HTML, browser, parser, wireless devices."
"Conventional IP routing protocols are not suitable for multimedia
applications which have very stringent Quality-of-Service (QoS) demands and
they require a connection oriented service. For multimedia applications it is
expected that the router should be able to forward the packet according to the
demand of the packet and it is necessary to find a path that satisfies the
specific demands of a particular application. In order to address these issues,
in this paper, we have presented a QoS aware IP routing protocol where a router
stores information about the QoS parameters and routes the packet accordingly.
  Keywords: IP Routing Protocol, Quality of Service (QoS) parameter, QoSIP,
Selective Flooding."
"Recent proposals in multicast overlay construction have demonstrated the
importance of exploiting underlying network topology. However, these
topology-aware proposals often rely on incremental and periodic refinements to
improve the system performance. These approaches are therefore neither
scalable, as they induce high communication cost due to refinement overhead,
nor efficient because long convergence time is necessary to obtain a stabilized
structure. In this paper, we propose a highly scalable locating algorithm that
gradually directs newcomers to their a set of their closest nodes without
inducing high overhead. On the basis of this locating process, we build a
robust and scalable topology-aware clustered hierarchical overlay scheme,
called LCC. We conducted both simulations and PlanetLab experiments to evaluate
the performance of LCC. Results show that the locating process entails modest
resources in terms of time and bandwidth. Moreover, LCC demonstrates promising
performance to support large scale multicast applications."
"Geographic routing provides relatively good performance at a much lower
overhead than conventional routing protocols such as AODV. However, the
performance of these protocols is impacted by physical voids, and localization
errors. Accordingly, virtual coordinate systems (VCS) were proposed as an
alternative approach that is resilient to localization errors and that
naturally routes around physical voids. However, we show that VCS is vulnerable
to different forms of the void problem and the performance of greedy routing on
VCS is worse than that of geographic forwarding. We show that these anomalies
are due to the integral nature of VCS, which causes quantization noise in the
estimate of connectivity and node location. We propose an aligned virtual
coordinate system (AVCS) on which the greedy routing success can be
significantly improved. With our approach, and for the first time, we show that
greedy routing on VCS out-performs that on physical coordinate systems even in
the absence of localization errors. We compare AVCS against some of the most
popular geographical routing protocols both on physical coordinate system and
the virtual coordinate systems and show that AVCS significantly improves
performance over the best known solutions."
"Traceroute is a networking tool that allows one to discover the path that
packets take from a source machine, through the network, to a destination
machine. It is widely used as an engineering tool, and also as a scientific
tool, such as for discovery of the network topology at the IP level. In prior
work, authors on this technical report have shown how to improve the efficiency
of route tracing from multiple cooperating monitors. However, it is not unusual
for a route tracing monitor to operate in isolation. Somewhat different
strategies are required for this case, and this report is the first systematic
study of those requirements. Standard traceroute is inefficient when used
repeatedly towards multiple destinations, as it repeatedly probes the same
interfaces close to the source. Others have recognized this inefficiency and
have proposed tracing backwards from the destinations and stopping probing upon
encounter with a previously-seen interface. One of this technical report's
contributions is to quantify for the first time the efficiency of this
approach. Another contribution is to describe the effect of non-responding
destinations on this efficiency. Since a large portion of destination machines
do not reply to probe packets, backwards probing from the destination is often
infeasible. We propose an algorithm to tackle non-responding destinations, and
we find that our algorithm can strongly decrease probing redundancy at the cost
of a small reduction in node and link discovery."
"This paper revisits the issue of route discovery in dynamic source routing
(DSR) for mobile ad hoc networks (MANETs), and puts forward a proposal of a
lightweight non-optimal route suppression technique based on the observation of
a rarely noted but commonly occurring phenomenon in route discovery. The
technique exploits the observed phenomenon to extract query state information
that permits intermediate nodes to identify and suppress the initiation of
route replies with non-optimal routes, even if the route query is received for
the first time. A detailed evaluation of DSR with non-optimal route suppression
is found to yield significant improvements in both protocol efficiency and
performance."
"One vision of future wireless networks is that they will be deeply integrated
and embedded in our lives and will involve the use of personalized mobile
devices. User behavior in such networks is bound to affect the network
performance. It is imperative to study and characterize the fundamental
structure of wireless user behavior in order to model, manage, leverage and
design efficient mobile networks. It is also important to make such study as
realistic as possible, based on extensive measurements collected from existing
deployed wireless networks.
  In this study, using our systematic TRACE approach, we analyze wireless
users' behavioral patterns by extensively mining wireless network logs from two
major university campuses. We represent the data using location preference
vectors, and utilize unsupervised learning (clustering) to classify trends in
user behavior using novel similarity metrics. Matrix decomposition techniques
are used to identify (and differentiate between) major patterns. While our
findings validate intuitive repetitive behavioral trends and user grouping, it
is surprising to find the qualitative commonalities of user behaviors from the
two universities. We discover multi-modal user behavior for more than 60% of
the users, and there are hundreds of distinct groups with unique behavioral
patterns in both campuses. The sizes of the major groups follow a power-law
distribution. Our methods and findings provide an essential step towards
network management and behavior-aware network protocols and applications, to
name a few."
"In the context of ambient networks where each small device must trust its
neighborhood rather than a fixed network, we propose in this paper a
\textit{trust management framework} inspired by known social patterns and based
on the following statements: each mobile constructs itself a local level of
trust what means that it does not accept recommendation by other peers, and the
only relevant parameter, beyond some special cases discussed later, to evaluate
the level of trust is the number of common trusted mobiles. These trusted
mobiles are considered as entries in a local database called history for each
device and we use identity-based cryptography to ensure strong security:
history must be a non-tansferable object."
"We investigate cascade dynamics in threshold-controlled (multiplex)
propagation on random geometric networks. We find that such local dynamics can
serve as an efficient, robust, and reliable prototypical activation protocol in
sensor networks in responding to various alarm scenarios. We also consider the
same dynamics on a modified network by adding a few long-range communication
links, resulting in a small-world network. We find that such construction can
further enhance and optimize the speed of the network's response, while keeping
energy consumption at a manageable level."
"In grid networks, distributed resources are interconnected by wide area
network to support compute and data-intensive applications, which require
reliable and efficient transfer of gigabits (even terabits) of data. Different
from best-effort traffic in Internet, bulk data transfer in grid requires
bandwidth reservation as a fundamental service. Existing reservation schemes
such as RSVP are designed for real-time traffic specified by reservation rate,
transfer start time but with unknown lifetime. In comparison, bulk data
transfer requests are defined in terms of volume and deadline, which provide
more information, and allow more flexibility in reservation schemes, i.e.,
transfer start time can be flexibly chosen, and reservation for a single
request can be divided into multiple intervals with different reservation
rates. We define a flexible reservation framework using time-rate function
algebra, and identify a series of practical reservation scheme families with
increasing generality and potential performance, namely, FixTime-FixRate,
FixTime-FlexRate, FlexTime-FlexRate, and Multi-Interval. Simple heuristics are
used to select representative scheme from each family for performance
comparison. Simulation results show that the increasing flexibility can
potentially improve system performance, minimizing both blocking probability
and mean flow time. We also discuss the distributed implementation of proposed
framework."
"Where distributed agents must share voluminous set membership information,
Bloom filters provide a compact, though lossy, way for them to do so. Numerous
recent networking papers have examined the trade-offs between the bandwidth
consumed by the transmission of Bloom filters, and the error rate, which takes
the form of false positives, and which rises the more the filters are
compressed. In this paper, we introduce the retouched Bloom filter (RBF), an
extension that makes the Bloom filter more flexible by permitting the removal
of selected false positives at the expense of generating random false
negatives. We analytically show that RBFs created through a random process
maintain an overall error rate, expressed as a combination of the false
positive rate and the false negative rate, that is equal to the false positive
rate of the corresponding Bloom filters. We further provide some simple
heuristics and improved algorithms that decrease the false positive rate more
than than the corresponding increase in the false negative rate, when creating
RBFs. Finally, we demonstrate the advantages of an RBF over a Bloom filter in a
distributed network topology measurement application, where information about
large stop sets must be shared among route tracing monitors."
"In the widely used 802.11 standard, the so called performance anomaly is a
well known issue. Several works have tried to solve this problem by introducing
mechanisms such as packet fragmentation, backoff adaptation, or packet
aggregation during a fixed time interval. In this paper, we propose a novel
approach solving the performance anomaly problem by packet aggregation using a
dynamic time interval, which depends on the busy time of the wireless medium.
Our solution differs from other proposition in the literature because of this
dynamic time interval, which allows increasing fairness, reactivity, and in
some cases efficiency. In this article, we emphasize the performance evaluation
of our proposal."
"In future mobility support will require handling roaming in heterogeneous
access networks. In order to enable seamless roaming it is necessary to
minimize the impact of the vertical handoffs. Localized mobility management
schemes such as FMIPv6 and HMIPv6 do not provide sufficient handoff
performance, since they have been designed for horizontal handoffs. In this
paper, we propose the SafetyNet protocol, which allows a Mobile Node to perform
seamless vertical handoffs. Further, we propose a handoff timing algorithm
which allows a Mobile Node to delay or even completely avoid upward vertical
handoffs. We implement the SafetyNet protocol and compare its performance with
the Fast Handovers for Mobile IPv6 protocol in our wireless test bed and
analyze the results. The experimental results indicate that the proposed
SafetyNet protocol can provide an improvement of up to 95% for TCP performance
in vertical handoffs, when compared with FMIPv6 and an improvement of 64% over
FMIPv6 with bicasting. We use numerical analysis of the protocol to show that
its signaling and data transmission overhead is comparable to Fast Mobile IPv6
and significantly smaller than that of FMIPv6 with bicasting."
"We study routing for massively dense wireless networks, i.e., wireless
networks that contain so many nodes that, in addition to their usual
microscopic description, a novel macroscopic description becomes possible. The
macroscopic description is not detailed, but nevertheless contains enough
information to permit a meaningful study and performance optimization of the
network. Within this context, we continue and significantly expand previous
work on the analogy between optimal routing and the propagation of light
according to the laws of Geometrical Optics. Firstly, we pose the analogy in a
more general framework than previously, notably showing how the eikonal
equation, which is the central equation of Geometrical Optics, also appears in
the networking context. Secondly, we develop a methodology for calculating the
cost function, which is the function describing the network at the macroscopic
level. We apply this methodology for two important types of networks: bandwidth
limited and energy limited."
"We study optimal user-network association in an integrated 802.11 WLAN and
3G-UMTS hybrid cell. Assuming saturated resource allocation on the downlink of
WLAN and UMTS networks and a single QoS class of mobiles arriving at an average
location in the hybrid cell, we formulate the problem with two different
approaches: Global and Individual optimality. The Globally optimal association
is formulated as an SMDP (Semi Markov Decision Process) connection routing
decision problem where rewards comprise a financial gain component and an
aggregate network throughput component. The corresponding Dynamic Programming
equations are solved using Value Iteration method and a stationary optimal
policy with neither convex nor concave type switching curve structure is
obtained. Threshold type and symmetric switching curves are observed for the
analogous homogenous network cases. The Individual optimality is studied under
a non-cooperative dynamic game framework with expected service time of a mobile
as the decision cost criteria. It is shown that individual optimality in a
WLAN-UMTS hybrid cell, results in a threshold policy curve of descending
staircase form with increasing Poisson arrival rate of mobiles."
"In 3G UMTS, two main transport channels have been provided for downlink data
transmission: a common FACH channel and a dedicated DCH channel. The
performance of TCP in UMTS depends much on the channel switching policy used.
In this paper, we propose and analyze three new basic threshold-based channel
switching policies for UMTS that we name as QS (Queue Size), FS (Flow Size) and
QSFS (QS & FS combined) policy. These policies significantly improve over a
modified threshold policy in [1] by about 17% in response time metrics. We
further propose and evaluate a new improved switching policy that we call
FS-DCH (at-least flow-size threshold on DCH) policy. This policy is biased
towards short TCP flows of few packets and is thus a cross-layer policy that
improves the performance of TCP by giving priority to the initial few packets
of a flow on the fast DCH channel. Extensive simulation results confirm this
improvement for the case when number of TCP connections is low."
"This article describes our strategy for deploying self-forming ad hoc
networks based on the Internet Protocol version 6 and evaluates the dynamics of
this proposal. Among others, we suggest a technique called adaptive routing
that provides secure intelligent routing capabilities to computer communication
networks. This technique uses the flow label, supports hybrid metrics, network
load sharing, and is not restricted to evaluation of performance on first hop
routers when making routing decisions. Selective anycasting is an extension to
the anycast addressing model that supports exclusion of members of groups that
perform poorly or inappropriately on a per-host basis. Distributed name lookup
is suggested for integrating self-forming and global networks where they
coexist. At last, we pose an address hierarchy to support unmanaged discovery
of services in unknown networks."
"The role of competition and monetary benefits in the design of Content
Delivery Networks (CDNs) is largely an unexplored area. In this paper, we
investigate the effect of competition among the competitive web based CDNs and
show that little difference in their performance may cause significant
financial gain/loss. It turns out that the economy of scale effect is very
significant for the success of a CDN in a competitive market. So CDN peering
might be a good idea. Since performance and conforming to the service level
agreement (SLA) with content providers is very important, we then focus on
designing CDN from this perspective. We provide an asymptotically optimal
static request routing policy for a CDN under a model where a CDN company
guarantees a certain level of user latency to the content providers in the SLA."
"In this paper, we examine the cause of the border effect observed in many
mobility models used to construct simulations of ad hoc networking protocol
performance. We specify conditions under which a node mobility model must
produce spatial mobile node distribution functions that obey the diffusion
equation. In particular demonstrate that these conditions are satisfied by the
random direction (RD) model. We show that it is possible to construct mobility
models that attain uniform steady-state distributions without resorting to
reflection or ``wrapping'' of nodes at the border of a test region. Finally, we
show that the random waypoint (RWP) model may be reproduced by the application
of a ``volume rule'' to an RD model. This volume rule violates the assumptions
that lead to the diffusion equation. We suggest a generalization of the RWP
model that can provide more uniform mobile node distributions."
"The performance of peer-to-peer file replication comes from its piece and
peer selection strategies. Two such strategies have been introduced by the
BitTorrent protocol: the rarest first and choke algorithms. Whereas it is
commonly admitted that BitTorrent performs well, recent studies have proposed
the replacement of the rarest first and choke algorithms in order to improve
efficiency and fairness. In this paper, we use results from real experiments to
advocate that the replacement of the rarest first and choke algorithms cannot
be justified in the context of peer-to-peer file replication in the Internet.
We instrumented a BitTorrent client and ran experiments on real torrents with
different characteristics. Our experimental evaluation is peer oriented,
instead of tracker oriented, which allows us to get detailed information on all
exchanged messages and protocol events. We go beyond the mere observation of
the good efficiency of both algorithms. We show that the rarest first algorithm
guarantees close to ideal diversity of the pieces among peers. In particular,
on our experiments, replacing the rarest first algorithm with source or network
coding solutions cannot be justified. We also show that the choke algorithm in
its latest version fosters reciprocation and is robust to free riders. In
particular, the choke algorithm is fair and its replacement with a bit level
tit-for-tat solution is not appropriate. Finally, we identify new areas of
improvements for efficient peer-to-peer file replication protocols."
"Prior work on routing in delay tolerant networks (DTNs) has commonly made the
assumption that each pair of nodes shares the same inter-contact time
distribution as every other pair. The main argument in this paper is that
researchers should also be looking at heterogeneous inter-contact time
distributions. We demonstrate the presence of such heterogeneity in the
often-used Dartmouth Wi-Fi data set. We also show that DTN routing can benefit
from knowing these distributions. We first introduce a new stochastic model
focusing on the inter-contact time distributions between all pairs of nodes,
which we validate on real connectivity patterns. We then analytically derive
the mean delivery time for a bundle of information traversing the network for
simple single copy routing schemes. The purpose is to examine the theoretic
impact of heterogeneous inter-contact time distributions. Finally, we show that
we can exploit this user diversity to improve routing performance."
"Most wireless terrestrial networks are designed based on the assumption that
the nodes are deployed on a two-dimensional (2D) plane. However, this 2D
assumption is not valid in underwater, atmospheric, or space communications. In
fact, recent interest in underwater acoustic ad hoc and sensor networks hints
at the need to understand how to design networks in 3D. Unfortunately, the
design of 3D networks is surprisingly more difficult than the design of 2D
networks. For example, proofs of Kelvin's conjecture and Kepler's conjecture
required centuries of research to achieve breakthroughs, whereas their 2D
counterparts are trivial to solve. In this paper, we consider the coverage and
connectivity issues of 3D networks, where the goal is to find a node placement
strategy with 100% sensing coverage of a 3D space, while minimizing the number
of nodes required for surveillance. Our results indicate that the use of the
Voronoi tessellation of 3D space to create truncated octahedral cells results
in the best strategy. In this truncated octahedron placement strategy, the
transmission range must be at least 1.7889 times the sensing range in order to
maintain connectivity among nodes. If the transmission range is between 1.4142
and 1.7889 times the sensing range, then a hexagonal prism placement strategy
or a rhombic dodecahedron placement strategy should be used. Although the
required number of nodes in the hexagonal prism and the rhombic dodecahedron
placement strategies is the same, this number is 43.25% higher than the number
of nodes required by the truncated octahedron placement strategy. We verify by
simulation that our placement strategies indeed guarantee ubiquitous coverage.
We believe that our approach and our results presented in this paper could be
used for extending the processes of 2D network design to 3D networks."
"Being motivated by recent developments in the theory of complex networks, we
examine the robustness of communication networks under intentional attack that
takes down network nodes in a decreasing order of their nodal degrees. In this
paper, we study two different effects that have been largely missed in the
existing results: (i) some communication networks, like Internet, are too large
for anyone to have global information of their topologies, which makes the
accurate intentional attack practically impossible; and (ii) most attacks in
communication networks are propagated from one node to its neighborhood
node(s), utilizing local network-topology information only. We show that
incomplete global information has different impacts to the intentional attack
in different circumstances, while local information-based attacks can be
actually highly efficient. Such insights would be helpful for the future
developments of efficient network attack/protection schemes."
"Ad hoc networking specific challenges foster a strong research effort on
efficient protocols design. Routing protocols based on a self-organized
structure have been studied principally for the robustness and the scalability
they provide. On the other hand, self-organization schemes may decrease the
network capacity since they concentrate the traffic on privileged links. This
paper presents four models for evaluating the capacity of a routing schemes on
802.11 like networks. Our approach consists in modeling the radio resource
sharing principles of 802.11 like MAC protocols as a set of linear constraints.
We have implemented two models of fairness. The first one assumes that nodes
have a fair access to the channel, while the second one assumes that on the
radio links. We then develop a pessimistic and an optimistic scenarii of
spatial re-utilization of the medium, yielding a lower bound and an upper bound
on the network capacity for each fairness case. Our models are independent of
the routing protocols and provide therefore a relevant framework for their
comparison. We apply our models to a comparative analysis of the well-known
shortest path base flat routing protocol OLSR against two main self-organized
structure approaches, VSR, and Wu & Li's protocols. This study concludes on the
relevance of self-organized approaches from the network capacity point of view."
"In hierarchical reliable multicast environment, makespan is the time that is
required to fully and successfully transmit a packet from the sender to all
receivers. Low makespan is vital for achieving high throughput with a TCP-like
window based sending scheme. In hierarchical reliable multicast methods, the
number of repair servers and their locations influence the makespan. In this
paper we propose a new method to decide the locations of repair servers that
can reduce the makespan in hierarchical reliable multicast networks. Our method
has a formulation based on mixed integer programming to analyze the makespan
minimization problem. A notable aspect of the formulation is that heterogeneous
links and packet losses are taken into account in the formulation. Three
different heuristics are presented to find the locations of repair servers in
reasonable time in the formulation. Through simulations, three heuristics are
carefully analyzed and compared on networks with different sizes. We also
evaluate our proposals on PGM (Pragmatic General Multicast) reliable multicast
protocol using ns-2 simulation. The results show that the our best heuristic is
close to the lower bound by a factor of 2.3 in terms of makespan and by a
factor of 5.5 in terms of the number of repair servers."
"The Maximum Differential Backlog (MDB) control policy of Tassiulas and
Ephremides has been shown to adaptively maximize the stable throughput of
multi-hop wireless networks with random traffic arrivals and queueing. The
practical implementation of the MDB policy in wireless networks with mutually
interfering links, however, requires the development of distributed
optimization algorithms. Within the context of CDMA-based multi-hop wireless
networks, we develop a set of node-based scaled gradient projection power
control algorithms which solves the MDB optimization problem in a distributed
manner using low communication overhead. As these algorithms require time to
converge to a neighborhood of the optimum, the optimal rates determined by the
MDB policy can only be found iteratively over time. For this, we show that the
iterative MDB policy with convergence time remains throughput optimal."
"In this article, we first provide a taxonomy of dynamic spectrum access. We
then focus on opportunistic spectrum access, the overlay approach under the
hierarchical access model of dynamic spectrum access. we aim to provide an
overview of challenges and recent developments in both technological and
regulatory aspects of opportunistic spectrum access."
"The objective of this paper is to propose models enabling to study the
behaviour of Ethernet switch for Networked Control Systems. Two scheduler
policies are analyzed: the static priority and the WRR (Weighted Round Robin).
The modelling work is based on Coloured Petri Nets. A temporal validation step
based on the simulation of these modelling, shows that the obtained results are
near to the expected behaviour of these scheduler policies."
"Recent interest in networked control systems (NCS) has instigated research in
both communication networks and control. Analysis of NCSs has usually been
performed from either the network or the control point of view, but not many
papers exist where the analysis of both is done in the same context. In this
paper an overall analysis of the networked control system is presented. First,
the procedure of obtaining the upper bound delay value for packet transmission
in the switched Ethernet network is presented. Next, the obtained delay
estimate is utilised in delay compensation for improving the Quality of
Performance (QoP) of the control systems. The presented upper bound delay
algorithm applies ideas from network calculus theory. For the improvement of
QoP, two delay compensation strategies, the Smith predictor based and the
robust control based delay compensation strategies, are presented and compared."
"Recent interest in networked control systems (NCS) has instigated research in
various areas of both communication networks and control. The analysis of NCS
has often been performed either from the network, or the control point of view
and not many papers exist were the analysis of both is done in the same
context. Here a simple overall analysis is presented. In the paper the
procedure of obtaining the upper bound delay value in the switched Ethernet
network is proposed and the obtained delay estimate is used in stability
analysis of the feedback loop and in the control compensation. The upper bound
delay algorithm is based on the network calculus theory, the stability analysis
uses the small gain theorem, and control compensating strategy is based on
Smith predictor, where however the upper bound delay is utilised in obtaining
the delay estimate."
"The work described in this paper is a contribution to the problems of
managing in data-intensive scientific applications. First, we discuss
scientific workflows and motivate there use in scientific applications. Then,
we introduce the concept of cooperative processes and describe their
interactions and uses in a flexible cooperative workflow system called
\textit{Bonita}. Finally, we propose an approach to integrate and synthesize
the data exchanged by the mapping of data-intensive science into Bonita, using
a binary approach, and illustrate the endeavors done to enhance the performance
computations within a dynamic environment."
"The Networked Control Systems (NCS) are complex systems which integrate
information provided by several domians such as automatic control, computer
science, communication network. The work presented in this paper concerns fault
detection, isolation and compensation of communication network. The proposed
method is based on the classical approach of Fault Detection and Isolation and
Fault Tolerant Control (FDI/FTC) currently used in diagnosis. The modelling of
the network to be supervised is based on both couloured petri nets and network
calculus theory often used to represent and analyse the network behaviour. The
goal is to implement inside network devices algorithms enabling to detect,
isolate and compensate communication faults in an autonomous way."
"This paper discusses a method for pipelining the calculation of CRC's, such
as ITU/CCITT CRC32, into a mostly feed-forward architecture. This method allows
several benefits such as independent scaling of circuit frequency and data
throughput. Additionally it allows calculation over packet tails (packet length
not a multiple of CRC input width). Finally it offers the ability to update a
CRC where a subset of data in the packet has changed."
"High-speed photonic switching networks can switch optical signals at the rate
of several terabits per second. However, they suffer from an intrinsic
crosstalk problem when two optical signals cross at the same switch element. To
avoid crosstalk, active connections must be node-disjoint in the switching
network. In this paper, we propose a sequence of decomposition and merge
operations, called conjugate transformation, performed on each switch element
to tackle this problem. The network resulting from this transformation is
called conjugate network. By using the numbering-schemes of networks, we prove
that if the route assignments in the original network are link-disjoint, their
corresponding ones in the conjugate network would be node-disjoint. Thus,
traditional nonblocking switching networks can be transformed into
crosstalk-free optical switches in a routine manner. Furthermore, we show that
crosstalk-free multicast switches can also be obtained from existing
nonblocking multicast switches via the same conjugate transformation."
"Le standard IEEE 802.11 est inefficace pour la transmission multim\'{e}dia en
multipoint. En particulier, les paquets multipoints sont envoy\'{e}s en boucle
ouverte de la m\^{e}me mani\`{e}re que les paquets broadcast. L'absence
d'acquittements rend impossible la mise en oeuvre de m\'{e}canismes de
contr\^{o}le de congestion, de m\'{e}canisme de fiabilisation de la
transmission ainsi que d'algorithmes d'adaptation du d\'{e}bit de transmission
physique. Dans ce rapport, nous proposons de nouveaux m\'{e}canismes de
ransmission multipoint qui se basent sur une approche leader pour renvoyer des
acquittements. Nous nous interessons \`{a} des solutions pratiques qui sont
suceptibles d'\^{e}tre implant\'{e}s dans les cartes r\'{e}seaux sans fil
actuelles et futures et qui restent compatibles avec les stations IEEE 802.11
standards. Nous proposons deux m\'{e}canismes pour adapter le d\'{e}bit de
transmission physique des flots multipoints: un m\'{e}canisme simplifi\'{e}
appel\'{e} LB-ARF et un m\'{e}canisme plus robuste appel\'{e} RRAM. Nos
simulations montrent que pour des environnements statiques, un m\'{e}canisme
aussi simple que LB-ARF suffit pour obtenir de bonnes performances. Le
m\'{e}canisme RRAM est quant \`{a} lui aussi efficace dans des environnements
statiques que lorsque les stations sont mobiles."
"This paper introduces Prawn, a tool for prototyping communication protocols
over IEEE 802.11 networks. Prawn allows researchers to conduct both functional
assessment and performance evaluation as an inherent part of the protocol
design process. Since Prawn runs on real IEEE 802.11 nodes, prototypes can be
evaluated and adjusted under realistic conditions. Once the prototype has been
extensively tested and thoroughly validated, and its functional design tuned
accordingly, it is then ready for implementation. Prawn facilitates prototype
development by providing: (i) a set of building blocks that implement common
functions needed by a wide range of wireless protocols (e.g., neighbor
discovery, link quality assessment, message transmission and reception), and
(ii) an API that allows protocol designers to access Prawn primitives. We show
through a number of case studies how Prawn supports prototyping as part of
protocol design and, as a result of enabling deployment and testing under
real-world scenarios, how Prawn provides useful feedback on protocol operation
and performance."
"We introduce an application of a mobile transient network architecture on top
of the current Internet. This paper is an application extension to a conceptual
mobile network architecture. It attempts to specifically reinforce some of the
powerful notions exposed by the architecture from an application perspective.
Of these notions, we explore the network expansion layer, an overlay of
components and services, that enables a persistent identification network and
other required services. The overlay abstraction introduces several benefits of
which mobility and communication across heterogenous network structures are of
interest to this paper. We present implementations of several components and
protocols including gateways, Agents and the Open Device Access Protocol. Our
present identification network implementation exploits the current
implementation of the Handle System through the use of distributed, global and
persistent identifiers called handles. Handles are used to identify and locate
devices and services abstracting any physical location or network association
from the communicating ends. A communication framework is finally demonstrated
that would allow for mobile devices on the public Internet to have persistent
identifiers and thus be persistently accessible either directly or indirectly.
This application expands IP inter-operability beyond its current boundaries."
"VoIP applications are emerging today as an important component in business
and communication industry. In this paper, we address the intrusion detection
and prevention in VoIP networks and describe how a conceptual solution based on
the Bayes inference approach can be used to reinforce the existent security
mechanisms. Our approach is based on network monitoring and analyzing of the
VoIP-specific traffic. We give a detailed example on attack detection using the
SIP signaling protocol."
"The advances in WDM technology lead to the great interest in traffic grooming
problems. As traffic often changes from time to time, the problem of grooming
dynamic traffic is of great practical value. In this paper, we discuss dynamic
grooming of traffic in star and tree networks. A genetic algorithm (GA) based
approach is proposed to support arbitrary dynamic traffic patterns, which
minimizes the number of ADM's and wavelengths. To evaluate the algorithm,
tighter bounds are derived. Computer simulation results show that our algorithm
is efficient in reducing both the numbers of ADM's and wavelengths in tree and
star networks."
"Traffic grooming is widely employed to reduce the number of ADM's and
wavelengths. We consider the problem of grooming of dynamic traffic in WDM tree
and star networks in this paper. To achieve better results, we used the
bifurcation techniques to the grooming of arbitrary dynamic traffic in a
strictly non-blocking manner in networks. Three splitting methods, including
Traffic-Cutting, Traffic-Dividing and Synthesized-Splitting were proposed. A
genetic algorithm (GA) approach based on these methods was proposed to tackle
such grooming problems in tree and star networks. The performance of these
algorithms was tested under different conditions in star and tree networks.
Computer simulation results showed that our algorithm is efficient in reducing
both the numbers of ADM's and wavelengths."
"Whoever has had his cell phone stolen knows how frustrating it is to be
unable to get his contact list back. To avoid data loss when losing or
destroying a mobile device like a PDA or a cell phone, data is usually
backed-up to a fixed station. However, in the time between the last backup and
the failure, important data can have been produced and then lost. To handle
this issue, we propose a transparent collaborative backup system. Indeed, by
saving data on other mobile devices between two connections to a global
infrastructure, we can resist to such scenarios. In this paper, after a general
description of such a system, we present a way to replicate data on mobile
devices to attain a prerequired resilience for the backup."
"In this document, we compare three existing simulation platforms (OPNET
Modeler, Network Simulator 2, Georgia Tech Sensor Network Simulator). Our
comparative study focuses on ease of use, scalability, ease of implementing
power consumption model and physical layer modeling accuracy, mainly.
Conclusions of this study are presented, and will help us decide which
simulating environment to use for evaluating power-aware self-organizing sensor
networks protocols."
"Peer-to-peer protocols play an increasingly instrumental role in Internet
content distribution. Consequently, it is important to gain a full
understanding of how these protocols behave in practice and how their
parameters impact overall performance. We present the first experimental
investigation of the peer selection strategy of the popular BitTorrent protocol
in an instrumented private torrent. By observing the decisions of more than 40
nodes, we validate three BitTorrent properties that, though widely believed to
hold, have not been demonstrated experimentally. These include the clustering
of similar-bandwidth peers, the effectiveness of BitTorrent's sharing
incentives, and the peers' high average upload utilization. In addition, our
results show that BitTorrent's new choking algorithm in seed state provides
uniform service to all peers, and that an underprovisioned initial seed leads
to the absence of peer clustering and less effective sharing incentives. Based
on our observations, we provide guidelines for seed provisioning by content
providers, and discuss a tracker protocol extension that addresses an
identified limitation of the protocol."
"The distributed control systems are more and more used in many industrial
applications. These systems are often referred as ""Networked control systems"".
The goal of this paper is to show the network influence on feedback control
systems. Two networks are considered: Switched Ethernet network and CAN
fieldbus. The first one represents the non deterministic network and second one
represents the deterministic one. Several scenarii are studied to analyse the
stability of system according to different network parameters (packets losses,
congestion and frame priority). The Truetime simulator is used in this work."
"It is widely believed that the Internet's AS-graph degree distribution obeys
a power-law form. Most of the evidence showing the power-law distribution is
based on BGP data. However, it was recently argued that since BGP collects data
in a tree-like fashion, it only produces a sample of the degree distribution,
and this sample may be biased. This argument was backed by simulation data and
mathematical analysis, which demonstrated that under certain conditions a tree
sampling procedure can produce an artificail power-law in the degree
distribution. Thus, although the observed degree distribution of the AS-graph
follows a power-law, this phenomenon may be an artifact of the sampling
process. In this work we provide some evidence to the contrary. We show, by
analysis and simulation, that when the underlying graph degree distribution
obeys a power-law with an exponent larger than 2, a tree-like sampling process
produces a negligible bias in the sampled degree distribution. Furthermore,
recent data collected from the DIMES project, which is not based on BGP
sampling, indicates that the underlying AS-graph indeed obeys a power-law
degree distribution with an exponent larger than 2. By combining this empirical
data with our analysis, we conclude that the bias in the degree distribution
calculated from BGP data is negligible."
"This article studies disruption tolerant networks (DTNs) where each node
knows the probabilistic distribution of contacts with other nodes. It proposes
a framework that allows one to formalize the behaviour of such a network. It
generalizes extreme cases that have been studied before where (a) either nodes
only know their contact frequency with each other or (b) they have a perfect
knowledge of who meets who and when. This paper then gives an example of how
this framework can be used; it shows how one can find a packet forwarding
algorithm optimized to meet the 'delay/bandwidth consumption' trade-off:
packets are duplicated so as to (statistically) guarantee a given delay or
delivery probability, but not too much so as to reduce the bandwidth, energy,
and memory consumption."
"Internet topology analysis has recently experienced a surge of interest in
computer science, physics, and the mathematical sciences. However, researchers
from these different disciplines tend to approach the same problem from
different angles. As a result, the field of Internet topology analysis and
modeling must untangle sets of inconsistent findings, conflicting claims, and
contradicting statements.
  On May 10-12, 2006, CAIDA hosted the Workshop on Internet topology (WIT). By
bringing together a group of researchers spanning the areas of computer
science, physics, and the mathematical sciences, the workshop aimed to improve
communication across these scientific disciplines, enable interdisciplinary
crossfertilization, identify commonalities in the different approaches, promote
synergy where it exists, and utilize the richness that results from exploring
similar problems from multiple perspectives.
  This report describes the findings of the workshop, outlines a set of
relevant open research problems identified by participants, and concludes with
recommendations that can benefit all scientific communities interested in
Internet topology research."
"Assessing mobility in a thorough fashion is a crucial step toward more
efficient mobile network design. Recent research on mobility has focused on two
main points: analyzing models and studying their impact on data transport.
These works investigate the consequences of mobility. In this paper, instead,
we focus on the causes of mobility. Starting from established research in
sociology, we propose SIMPS, a mobility model of human crowd motion. This model
defines two complimentary behaviors, namely socialize and isolate, that
regulate an individual with regard to her/his own sociability level. SIMPS
leads to results that agree with scaling laws observed both in small-scale and
large-scale human motion. Although our model defines only two simple individual
behaviors, we observe many emerging collective behaviors (group
formation/splitting, path formation, and evolution). To our knowledge, SIMPS is
the first model in the networking community that tackles the roots governing
mobility."
"Distributed Denial-of-Service (DDoS) attacks are a major problem in the
Internet today. In one form of a DDoS attack, a large number of compromised
hosts send unwanted traffic to the victim, thus exhausting the resources of the
victim and preventing it from serving its legitimate clients. One of the main
mechanisms that have been proposed to deal with DDoS is filtering, which allows
routers to selectively block unwanted traffic. Given the magnitude of DDoS
attacks and the high cost of filters in the routers today, the successful
mitigation of a DDoS attack using filtering crucially depends on the efficient
allocation of filtering resources. In this paper, we consider a single router,
typically the gateway of the victim, with a limited number of available
filters. We study how to optimally allocate filters to attack sources, or
entire domains of attack sources, so as to maximize the amount of good traffic
preserved, under a constraint on the number of filters. We formulate the
problem as an optimization problem and solve it optimally using dynamic
programming, study the properties of the optimal allocation, experiment with a
simple heuristic and evaluate our solutions for a range of realistic
attack-scenarios. First, we look at a single-tier where the collateral damage
is high due to the filtering at the granularity of domains. Second, we look at
the two-tier problem where we have an additional constraint on the number of
filters and the filtering is performed on the granularity of attackers and
domains."
"We introduce a model for decentralized networks with collaborating peers. The
model is based on the stable matching theory which is applied to systems with a
global ranking utility function. We consider the dynamics of peers searching
for efficient collaborators and we prove that a unique stable solution exists.
We prove that the system converges towards the stable solution and analyze its
speed of convergence. We also study the stratification properties of the model,
both when all collaborations are possible and for random possible
collaborations. We present the corresponding fluid limit on the choice of
collaborators in the random case. As a practical example, we study the
BitTorrent Tit-for-Tat policy. For this system, our model provides an
interesting insight on peer download rates and a possible way to optimize peer
strategy."
"Networked Controlled Systems (NCSs) are more and more used in industrial
applications. They are strongly connected to real-time constraints because
important delays induced by the network can lead to an unstable process
control. Usually, the network used in NCSs is shared with many others
applications requiring different Quality of Service. The objective of this
paper is to optimize the tuning of the network scheduling mechanisms in taking
into account the level of Quality of Control. The goal is to maximize the
bandwidth allocation for unconstrained frames in guarantying that the control
constraints are respected. In this paper, we focus on switched Ethernet network
implementing the Classification of Service (IEEE 802.1p) based on a Weighted
Round Robin policy."
"In a wireless network, mobile nodes (MNs) repeatedly perform tasks such as
layer 2 (L2) handoff, layer 3 (L3) handoff and authentication. These tasks are
critical, particularly for real-time applications such as VoIP. We propose a
novel approach, namely Cooperative Roaming (CR), in which MNs can collaborate
with each other and share useful information about the network in which they
move. We show how we can achieve seamless L2 and L3 handoffs regardless of the
authentication mechanism used and without any changes to either the
infrastructure or the protocol. In particular, we provide a working
implementation of CR and show how, with CR, MNs can achieve a total L2+L3
handoff time of less than 16 ms in an open network and of about 21 ms in an
IEEE 802.11i network. We consider behaviors typical of IEEE 802.11 networks,
although many of the concepts and problems addressed here apply to any kind of
mobile network."
"It is now commonly accepted that the unit disk graph used to model the
physical layer in wireless networks does not reflect real radio transmissions,
and that the lognormal shadowing model better suits to experimental
simulations. Previous work on realistic scenarios focused on unicast, while
broadcast requirements are fundamentally different and cannot be derived from
unicast case. Therefore, broadcast protocols must be adapted in order to still
be efficient under realistic assumptions. In this paper, we study the
well-known multipoint relay protocol (MPR). In the latter, each node has to
choose a set of neighbors to act as relays in order to cover the whole 2-hop
neighborhood. We give experimental results showing that the original method
provided to select the set of relays does not give good results with the
realistic model. We also provide three new heuristics in replacement and their
performances which demonstrate that they better suit to the considered model.
The first one maximizes the probability of correct reception between the node
and the considered relays multiplied by their coverage in the 2-hop
neighborhood. The second one replaces the coverage by the average of the
probabilities of correct reception between the considered neighbor and the
2-hop neighbors it covers. Finally, the third heuristic keeps the same concept
as the second one, but tries to maximize the coverage level of the 2-hop
neighborhood: 2-hop neighbors are still being considered as uncovered while
their coverage level is not higher than a given coverage threshold, many
neighbors may thus be selected to cover the same 2-hop neighbors."
"It is well-known that wide-area networks face today several performance and
reliability problems. In this work, we propose to solve these problems by
connecting two or more local-area networks together via a Redundant Array of
Internet Links (or RAIL) and by proactively replicating each packet over these
links. In that sense, RAIL is for networks what RAID (Redundant Array of
Inexpensive Disks) was for disks. In this paper, we describe the RAIL approach,
present our prototype (called the RAILedge), and evaluate its performance.
First, we demonstrate that using multiple Internet links significantly improves
the end-to-end performance in terms of network-level as well as
application-level metrics for Voice-over-IP and TCP. Second, we show that a
delay padding mechanism is needed to complement RAIL when there is significant
delay disparity between the paths. Third, we show that two paths provide most
of the benefit, if carefully managed. Finally, we discuss a RAIL-network
architecture, where RAILedges make use of path redundancy, route control and
application-specific mechanisms, to improve WAN performance."
"We consider the RIPE WHOIS Internet data as characterized by the Cooperative
Association for Internet Data Analysis (CAIDA), and show that the Tempered
Preferential Attachment model [1] provides an excellent fit to this data.
  [1] D'Souza, Borgs, Chayes, Berger and Kleinberg, to appear PNAS USA, 2007."
"In this paper, the topology dynamic of Gnutella is studied through phase
space. The dynamic changes in peer degree are studied as a time series in two
dimensional phase space which is defined as the number of connected leaves and
the number of connected ultras. The reported degrees are concentrated in three
special Software related regions that we named as Ultra Stable Region, Leaf
Stable Region and Transition Belt. A method is proposed to classify degree
traces in phase space into different classes. Connection churn then is studied
along with the churn in degree. It shows that the topological structure of
Gnutella is rather stable in its connection degree but not the topology itself.
The connection drop rate is estimated and the live time of connections is
inferred afterwards. M/M/m/m loss queue system is introduced to model the
degree keeping process in Gnutella. This model revealed that the degree stable
is ensured by large new connection efforts. In other words the stable in
topological structure of Gnutella is a results of essential unstable in its
topology. That opens a challenge to the basic design philosophy of this
network."
"This dissertation has extensively looked into all aspects of VoIP
commu-nications technology, and information presented in preceding chapters,
which build up a solid framework to discuss the conceptual design model, and
investigate features that could be incorporated for actual Pro-jects, with
parameters that are tested on field values. The dissertation follows a
five-course model, for answering different questions, both tech-nical and
businesslike, around central issues, that have been crucial to explanation of
the topic; starting with a general overview of VoIP tech-nology, analyzing
current VoIP encryption methods, identifying security threats, designing a
robust VoIP system based on particulars discussed in preceding chapters, and
finally, a VoIP simulation."
"The aim of this technical report is to give a short overview of known
techniques for network tomography (introduced in the paper of Vardi (1996)),
extended by a Bayesian approach originating Tebaldi and West (1998). Since the
studies of A.K. Erlang (1878-1929) on telephone networks in the last
millennium, lots of needs are seen in todays applications of networks and
network tomography, so for instance networks are a critical component of the
information structure supporting finance, commerce and even civil and national
defence. An attack on a network can be performed as an intrusion in the network
or as sending a lot of fault information and disturbing the network flow. Such
attacks can be detected by modelling the traffic flows in a network, by
counting the source destination packets and even by measuring counts over time
and by drawing a comparison with this 'time series' for instance."
"We address the design of opportunistic spectrum access (OSA) strategies that
allow secondary users to independently search for and exploit instantaneous
spectrum availability. Integrated in the joint design are three basic
components: a spectrum sensor that identifies spectrum opportunities, a sensing
strategy that determines which channels in the spectrum to sense, and an access
strategy that decides whether to access based on imperfect sensing outcomes.
  We formulate the joint PHY-MAC design of OSA as a constrained partially
observable Markov decision process (POMDP). Constrained POMDPs generally
require randomized policies to achieve optimality, which are often intractable.
By exploiting the rich structure of the underlying problem, we establish a
separation principle for the joint design of OSA. This separation principle
reveals the optimality of myopic policies for the design of the spectrum sensor
and the access strategy, leading to closed-form optimal solutions. Furthermore,
decoupling the design of the sensing strategy from that of the spectrum sensor
and the access strategy, the separation principle reduces the constrained POMDP
to an unconstrained one, which admits deterministic optimal policies. Numerical
examples are provided to study the design tradeoffs, the interaction between
the spectrum sensor and the sensing and access strategies, and the robustness
of the ensuing design to model mismatch."
"We develop bounds on the capacity of wireless networks when the traffic is
non-uniform, i.e., not all nodes are required to receive and send similar
volumes of traffic. Our results are asymptotic, i.e., they hold with
probability going to unity as the number of nodes goes to infinity. We study
\emph{(i)} asymmetric networks, where the numbers of sources and destinations
of traffic are unequal, \emph{(ii)} multicast networks, in which each created
packet has multiple destinations, \emph{(iii)} cluster networks, that consist
of clients and a limited number of cluster heads, and each client wants to
communicate with any of the cluster heads, and \emph{(iv)} hybrid networks, in
which the nodes are supported by a limited infrastructure. Our findings
quantify the fundamental capabilities of these wireless networks to handle
traffic bottlenecks, and point to correct design principles that achieve the
capacity without resorting to overly complicated protocols."
"We study the interaction between the AIMD (Additive Increase Multiplicative
Decrease) congestion control and a bottleneck router with Drop Tail buffer. We
consider the problem in the framework of deterministic hybrid models. First, we
show that the hybrid model of the interaction between the AIMD congestion
control and bottleneck router always converges to a cyclic behavior. We
characterize the cycles. Necessary and sufficient conditions for the absence of
multiple jumps of congestion window in the same cycle are obtained. Then, we
propose an analytical framework for the optimal choice of the router buffer
size. We formulate the problem of the optimal router buffer size as a
multi-criteria optimization problem, in which the Lagrange function corresponds
to a linear combination of the average goodput and the average delay in the
queue. The solution to the optimization problem provides further evidence that
the buffer size should be reduced in the presence of traffic aggregation. Our
analytical results are confirmed by simulations performed with Simulink and the
NS simulator."
"Peer-to-peer protocols play an increasingly instrumental role in Internet
content distribution. It is therefore important to gain a complete
understanding of how these protocols behave in practice and how their operating
parameters affect overall system performance. This paper presents the first
detailed experimental investigation of the peer selection strategy in the
popular BitTorrent protocol. By observing more than 40 nodes in instrumented
private torrents, we validate three protocol properties that, though believed
to hold, have not been previously demonstrated experimentally: the clustering
of similar-bandwidth peers, the effectiveness of BitTorrent's sharing
incentives, and the peers' high uplink utilization. In addition, we observe
that BitTorrent's modified choking algorithm in seed state provides uniform
service to all peers, and that an underprovisioned initial seed leads to
absence of peer clustering and less effective sharing incentives. Based on our
results, we provide guidelines for seed provisioning by content providers, and
discuss a tracker protocol extension that addresses an identified limitation of
the protocol."
"Wireless LAN is currently enjoying rapid deployment in University
departments, business offices, hospitals and homes. It becomes an inexpensive
technology and allows multiple numbers of the households to simultaneously
access the internet while roaming about the house. In the present work, the
design and development of a wireless LAN is highlighted which utilizes direct
sequence spread spectrum (DSSS) technology at 900MHz RF carrier frequency in
its physical layer. This provides enormous security in the physical layer and
hence it is very difficult to hack or jam the network. The installation cost is
also less due to the use of 900 MHz RF carrier frequency.."
"Since the proposition of Quality of Service architectures by the IETF, the
interaction between TCP and the QoS services has been intensively studied. This
paper proposes to look forward to the results obtained in terms of TCP
throughput guarantee in the DiffServ Assured Forwarding (DiffServ/AF) service
and to present an overview of the different proposals to solve the problem. It
has been demonstrated that the standardized IETF DiffServ conditioners such as
the token bucket color marker and the time sliding window color maker were not
good TCP traffic descriptors. Starting with this point, several propositions
have been made and most of them presents new marking schemes in order to
replace or improve the traditional token bucket color marker. The main problem
is that TCP congestion control is not designed to work with the AF service.
Indeed, both mechanisms are antagonists. TCP has the property to share in a
fair manner the bottleneck bandwidth between flows while DiffServ network
provides a level of service controllable and predictable. In this paper, we
build a classification of all the propositions made during these last years and
compare them. As a result, we will see that these conditioning schemes can be
separated in three sets of action level and that the conditioning at the
network edge level is the most accepted one. We conclude that the problem is
still unsolved and that TCP, conditioned or not conditioned, remains
inappropriate to the DiffServ/AF service."
"The IEEE 802.11e standard revises the Medium Access Control (MAC) layer of
the former IEEE 802.11 standard for Quality-of-Service (QoS) provision in the
Wireless Local Area Networks (WLANs). The Enhanced Distributed Channel Access
(EDCA) function of 802.11e defines multiple Access Categories (AC) with
AC-specific Contention Window (CW) sizes, Arbitration Interframe Space (AIFS)
values, and Transmit Opportunity (TXOP) limits to support MAC-level QoS and
prioritization. We propose an analytical model for the EDCA function which
incorporates an accurate CW, AIFS, and TXOP differentiation at any traffic
load. The proposed model is also shown to capture the effect of MAC layer
buffer size on the performance. Analytical and simulation results are compared
to demonstrate the accuracy of the proposed approach for varying traffic loads,
EDCA parameters, and MAC layer buffer space."
"We develop distributed algorithms to allocate resources in multi-hop wireless
networks with the aim of minimizing total cost. In order to observe the
fundamental duplexing constraint that co-located transmitters and receivers
cannot operate simultaneously on the same frequency band, we first devise a
spectrum allocation scheme that divides the whole spectrum into multiple
sub-bands and activates conflict-free links on each sub-band. We show that the
minimum number of required sub-bands grows asymptotically at a logarithmic rate
with the chromatic number of network connectivity graph. A simple distributed
and asynchronous algorithm is developed to feasibly activate links on the
available sub-bands. Given a feasible spectrum allocation, we then design
node-based distributed algorithms for optimally controlling the transmission
powers on active links for each sub-band, jointly with traffic routes and user
input rates in response to channel states and traffic demands. We show that
under specified conditions, the algorithms asymptotically converge to the
optimal operating point."
"We study Batch Processor-Sharing (BPS) queuing model with hyper-exponential
service time distribution and Poisson batch arrival process. One of the main
goals to study BPS is the possibility of its application in size-based
scheduling, which is used in differentiation between Short and Long flows in
the Internet. In the case of hyper-exponential service time distribution we
find an analytical expression of the expected conditional response time for the
BPS queue. We show, that the expected conditional response time is a concave
function of the service time. We apply the received results to the Two Level
Processor-Sharing (TLPS) model with hyper-exponential service time distribution
and find the expression of the expected response time for the TLPS model. TLPS
scheduling discipline can be applied to size-based differentiation in TCP/IP
networks and Web server request handling."
"Netsukuku is a P2P network system designed to handle a large number of nodes
with minimal CPU and memory resources. It can be easily used to build a
worldwide distributed, anonymous and not controlled network, separated from the
Internet, without the support of any servers, ISPs or authority controls. In
this document, we give a generic and non technical description of the Netsukuku
network, emphasizing its main ideas and features."
"This document describes the QSPN, the routing discovery algorithm used by
Netsukuku. Through a deductive analysis the main proprieties of the QSPN are
shown. Moreover, a second version of the algorithm, is presented."
"In this document, we describe the fractal structure of the Netsukuku
topology. Moreover, we show how it is possible to use the QSPN v2 on the high
levels of the fractal."
"We present the Abnormal Netsukuku Domain Name Anarchy system. ANDNA is the
distributed, non hierarchical and decentralised system of hostname management
used in the Netsukuku network."
"Wireless networking is rapidly growing and becomes an inexpensive technology
which allows multiple users to simultaneously access the network and the
internet while roaming about the campus. In the present work, the software
development of a wireless LAN(WLAN) is highlighted. This WLAN utilizes direct
sequence spread spectrum (DSSS) technology at 902MHz RF carrier frequency in
its physical layer. Cost effective installation and antijaming property of
spread spectrum technology are the major advantages of this work."
"Scientists and Technologists involved in the development of radar and remote
sensing systems all over the world are now trying to involve themselves in
saving of manpower in the form of developing a new application of their ideas
in Intelligent Transport system(ITS). The world statistics shows that by
incorporating such wireless radar system in the car would decrease the world
road accident by 8-10% yearly. The wireless technology has to be chosen
properly which is capable of tackling the severe interferences present in the
open road. A combined digital technology like Spread spectrum along with
diversity reception will help a lot in this regard. Accordingly, the choice is
for FHSS based space diversity system which will utilize carrier frequency
around 5.8 GHz ISM band with available bandwidth of 80 MHz and no license. For
efficient design, the radio channel is characterized on which the design is
based. Out of two available modes e.g. Communication and Radar modes, the radar
mode is providing the conditional measurement of the range of the nearest car
after authentication of the received code, thus ensuring the reliability and
accuracy of measurement. To make the system operational in simultaneous mode,
we have started the Software Defined Radio approach for best speed and
flexibility."
"With the technological growth of broadband wireless technology like CDMA and
UWB, a lots of development efforts towards wireless communication system and
Imaging radar system are well justified. Efforts are also being imparted
towards a Convergence Technology.. the convergence between a communication and
radar technology which will result in ITS (Intelligent Transport System) and
other applications. This encourages present authors for this development. They
are trying to utilize or converge the communication technologies towards radar
and to achieve the Interference free and clutter free quality remote images of
targets using DS-UWB wireless technology."
"Resources in a distributed system can be identified using identifiers based
on random numbers. When using a distributed hash table to resolve such
identifiers to network locations, the straightforward approach is to store the
network location directly in the hash table entry associated with an
identifier. When a mobile host contains a large number of resources, this
requires that all of the associated hash table entries must be updated when its
network address changes.
  We propose an alternative approach where we store a host identifier in the
entry associated with a resource identifier and the actual network address of
the host in a separate host entry. This can drastically reduce the time
required for updating the distributed hash table when a mobile host changes its
network address. We also investigate under which circumstances our approach
should or should not be used. We evaluate and confirm the usefulness of our
approach with experiments run on top of OpenDHT."
"Mobile multi-hop ad hoc networks allow establishing local groups of
communicating devices in a self-organizing way. However, in a global setting
such networks fail to work properly due to network partitioning. Providing that
devices are capable of communicating both locally-e.g. using Wi-Fi or
Bluetooth-and additionally also with arbitrary remote devices-e.g. using
GSM/UMTS links-the objective is to find efficient ways of inter-linking
multiple network partitions. Tackling this problem of topology control, we
focus on the class of small-world networks that obey two distinguishing
characteristics: they have a strong local clustering while still retaining a
small average distance between two nodes. This paper reports on results gained
investigating the question if small-world properties are indicative for an
efficient link management in multiple multi-hop ad hoc network partitions."
"In this paper, we study video streaming over wireless networks with network
coding capabilities. We build upon recent work, which demonstrated that network
coding can increase throughput over a broadcast medium, by mixing packets from
different flows into a single packet, thus increasing the information content
per transmission. Our key insight is that, when the transmitted flows are video
streams, network codes should be selected so as to maximize not only the
network throughput but also the video quality. We propose video-aware
opportunistic network coding schemes that take into account both (i) the
decodability of network codes by several receivers and (ii) the importance and
deadlines of video packets. Simulation results show that our schemes
significantly improve both video quality and throughput."
"We analyze the Two Level Processor Sharing (TLPS) scheduling discipline with
the hyper-exponential job size distribution and with the Poisson arrival
process. TLPS is a convenient model to study the benefit of the file size based
differentiation in TCP/IP networks. In the case of the hyper-exponential job
size distribution with two phases, we find a closed form analytic expression
for the expected sojourn time and an approximation for the optimal value of the
threshold that minimizes the expected sojourn time. In the case of the
hyper-exponential job size distribution with more than two phases, we derive a
tight upper bound for the expected sojourn time conditioned on the job size. We
show that when the variance of the job size distribution increases, the gain in
system performance increases and the sensitivity to the choice of the threshold
near its optimal value decreases."
"The traffic behavior of University of Louisville network with the
interconnected backbone routers and the number of Virtual Local Area Network
(VLAN) subnets is investigated using the Random Matrix Theory (RMT) approach.
We employ the system of equal interval time series of traffic counts at all
router to router and router to subnet connections as a representation of the
inter-VLAN traffic. The cross-correlation matrix C of the traffic rate changes
between different traffic time series is calculated and tested against
null-hypothesis of random interactions.
  The majority of the eigenvalues \lambda_{i} of matrix C fall within the
bounds predicted by the RMT for the eigenvalues of random correlation matrices.
The distribution of eigenvalues and eigenvectors outside of the RMT bounds
displays prominent and systematic deviations from the RMT predictions.
Moreover, these deviations are stable in time.
  The method we use provides a unique possibility to accomplish three
concurrent tasks of traffic analysis. The method verifies the uncongested state
of the network, by establishing the profile of random interactions. It
recognizes the system-specific large-scale interactions, by establishing the
profile of stable in time non-random interactions. Finally, by looking into the
eigenstatistics we are able to detect and allocate anomalies of network traffic
interactions."
"In recent years, there has been a strong interest in measuring the available
bandwidth of network paths. Several methods and techniques have been proposed
and various measurement tools have been developed and evaluated. However, there
have been few comparative studies with regards to the actual performance of
these tools. This paper presents a study of available bandwidth measurement
techniques and undertakes a comparative analysis in terms of accuracy,
intrusiveness and response time of active probing tools. Finally, measurement
errors and the uncertainty of the tools are analysed and overall conclusions
made."
"Multicast is a central challenge for emerging multi-hop wireless
architectures such as wireless mesh networks, because of its substantial cost
in terms of bandwidth. In this report, we study one specific case of multicast:
broadcasting, sending data from one source to all nodes, in a multi-hop
wireless network. The broadcast we focus on is based on network coding, a
promising avenue for reducing cost; previous work of ours showed that the
performance of network coding with simple heuristics is asymptotically optimal:
each transmission is beneficial to nearly every receiver. This is for
homogenous and large networks of the plan. But for small, sparse or for
inhomogeneous networks, some additional heuristics are required. This report
proposes such additional new heuristics (for selecting rates) for broadcasting
with network coding. Our heuristics are intended to use only simple local
topology information. We detail the logic of the heuristics, and with
experimental results, we illustrate the behavior of the heuristics, and
demonstrate their excellent performance."
"In this paper, we conduct extensive simulations to understand the properties
of the overlay generated by BitTorrent. We start by analyzing how the overlay
properties impact the efficiency of BitTorrent. We focus on the average peer
set size (i.e., average number of neighbors), the time for a peer to reach its
maximum peer set size, and the diameter of the overlay. In particular, we show
that the later a peer arrives in a torrent, the longer it takes to reach its
maximum peer set size. Then, we evaluate the impact of the maximum peer set
size, the maximum number of outgoing connections per peer, and the number of
NATed peers on the overlay properties. We show that BitTorrent generates a
robust overlay, but that this overlay is not a random graph. In particular, the
connectivity of a peer to its neighbors depends on its arriving order in the
torrent. We also show that a large number of NATed peers significantly
compromise the robustness of the overlay to attacks. Finally, we evaluate the
impact of peer exchange on the overlay properties, and we show that it
generates a chain-like overlay with a large diameter, which will adversely
impact the efficiency of large torrents."
"Wireless sensor networks (WSN) have recently received an increasing interest.
They are now expected to be deployed for long periods of time, thus requiring
software updates. Updating the software code automatically on a huge number of
sensors is a tremendous task, as ''by hand'' updates can obviously not be
considered, especially when all participating sensors are embedded on mobile
entities. In this paper, we investigate an approach to automatically update
software in mobile sensor-based application when no localization mechanism is
available. We leverage the peer-to-peer cooperation paradigm to achieve a good
trade-off between reliability and scalability of code propagation. More
specifically, we present the design and evaluation of GCP ({\emph Gossip-based
Code Propagation}), a distributed software update algorithm for mobile wireless
sensor networks. GCP relies on two different mechanisms (piggy-backing and
forwarding control) to improve significantly the load balance without
sacrificing on the propagation speed. We compare GCP against traditional
dissemination approaches. Simulation results based on both synthetic and
realistic workloads show that GCP achieves a good convergence speed while
balancing the load evenly between sensors."
"Random walk can be used as a centrality measure of a directed graph. However,
if the graph is reducible the random walk will be absorbed in some subset of
nodes and will never visit the rest of the graph. In Google PageRank the
problem was solved by introduction of uniform random jumps with some
probability. Up to the present, there is no clear criterion for the choice this
parameter. We propose to use parameter-free centrality measure which is based
on the notion of quasi-stationary distribution. Specifically we suggest four
quasi-stationary based centrality measures, analyze them and conclude that they
produce approximately the same ranking. The new centrality measures can be
applied in spam detection to detect ``link farms'' and in image search to find
photo albums."
"We study efficient broadcasting for wireless sensor networks, with network
coding. We address this issue for homogeneous sensor networks in the plane. Our
results are based on a simple principle (IREN/IRON), which sets the same rate
on most of the nodes (wireless links) of the network. With this rate selection,
we give a value of the maximum achievable broadcast rate of the source: our
central result is a proof of the value of the min-cut for such networks, viewed
as hypergraphs. Our metric for efficiency is the number of transmissions
necessary to transmit one packet from the source to every destination: we show
that IREN/IRON achieves near optimality for large networks; that is,
asymptotically, nearly every transmission brings new information from the
source to the receiver. As a consequence, network coding asymptotically
outperforms any scheme that does not use network coding."
"We consider the decoding of bit interleaved coded modulation (BICM) applied
to multiband OFDM for practical scenarios where only a noisy (possibly very
bad) estimate of the channel is available at the receiver. First, a decoding
metric based on the channel it a posteriori probability density, conditioned on
the channel estimate is derived and used for decoding BICM multiband OFDM.
Then, we characterize the limits of reliable information rates in terms of the
maximal achievable outage rates associated to the proposed metric. We also
compare our results with the outage rates of a system using a theoretical
decoder. Our results are useful for designing a communication system where a
prescribed quality of service (QoS), in terms of achievable target rates with
small error probability, must be satisfied even in the presence of imperfect
channel estimation. Numerical results over both realistic UWB and theoretical
Rayleigh fading channels show that the proposed method provides significant
gain in terms of BER and outage rates compared to the classical mismatched
detector, without introducing any additional complexity."
"We consider the decoding of bit interleaved coded modulation (BICM) applied
to both multiband and MIMO OFDM systems for typical scenarios where only a
noisy (possibly very bad) estimate of the channel is provided by sending a
limited number of pilot symbols. First, by using a Bayesian framework involving
the channel a posteriori density, we adopt a practical decoding metric that is
robust to the presence of channel estimation errors. Then this metric is used
in the demapping part of BICM multiband and MIMO OFDM receivers. We also
compare our results with the performance of a mismatched decoder that replaces
the channel by its estimate in the decoding metric. Numerical results over both
realistic UWB and theoretical Rayleigh fading channels show that the proposed
method provides significant gain in terms of bit error rate compared to the
classical mismatched detector, without introducing any additional complexity."
"This paper introduces an expectation-maximization (EM) algorithm within a
wavelet domain Bayesian framework for semi-blind channel estimation of
multiband OFDM based UWB communications. A prior distribution is chosen for the
wavelet coefficients of the unknown channel impulse response in order to model
a sparseness property of the wavelet representation. This prior yields, in
maximum a posteriori estimation, a thresholding rule within the EM algorithm.
We particularly focus on reducing the number of estimated parameters by
iteratively discarding ``unsignificant'' wavelet coefficients from the
estimation process. Simulation results using UWB channels issued from both
models and measurements show that under sparsity conditions, the proposed
algorithm outperforms pilot based channel estimation in terms of mean square
error and bit error rate and enhances the estimation accuracy with less
computational complexity than traditional semi-blind methods."
"Optimal decoding of bit interleaved coded modulation (BICM) MIMO-OFDM where
an imperfect channel estimate is available at the receiver is investigated.
First, by using a Bayesian approach involving the channel a posteriori density,
we derive a practical decoding metric for general memoryless channels that is
robust to the presence of channel estimation errors. Then, we evaluate the
outage rates achieved by a decoder that uses our proposed metric. The
performance of the proposed decoder is compared to the classical mismatched
decoder and a theoretical decoder defined as the best decoder in the presence
of imperfect channel estimation. Numerical results over Rayleigh block fading
MIMO-OFDM channels show that the proposed decoder outperforms mismatched
decoding in terms of bit error rate and outage capacity without introducing any
additional complexity."
"The many-to-many social communication activity on the popular technology-news
website Slashdot has been studied. We have concentrated on the dynamics of
message production without considering semantic relations and have found
regular temporal patterns in the reaction time of the community to a news-post
as well as in single user behavior. The statistics of these activities follow
log-normal distributions. Daily and weekly oscillatory cycles, which cause
slight variations of this simple behavior, are identified. A superposition of
two log-normal distributions can account for these variations. The findings are
remarkable since the distribution of the number of comments per users, which is
also analyzed, indicates a great amount of heterogeneity in the community. The
reader may find surprising that only a few parameters allow a detailed
description, or even prediction, of social many-to-many information exchange in
this kind of popular public spaces."
"While there exist compact routing schemes designed for grids, trees, and
Internet-like topologies that offer routing tables of sizes that scale
logarithmically with the network size, we demonstrate in this paper that in
view of recent results in compact routing research, such logarithmic scaling on
Internet-like topologies is fundamentally impossible in the presence of
topology dynamics or topology-independent (flat) addressing. We use analytic
arguments to show that the number of routing control messages per topology
change cannot scale better than linearly on Internet-like topologies. We also
employ simulations to confirm that logarithmic routing table size scaling gets
broken by topology-independent addressing, a cornerstone of popular
locator-identifier split proposals aiming at improving routing scaling in the
presence of network topology dynamics or host mobility. These pessimistic
findings lead us to the conclusion that a fundamental re-examination of
assumptions behind routing models and abstractions is needed in order to find a
routing architecture that would be able to scale ``indefinitely.''"
"As the web grows and the amount of traffics on the web server increase,
problems related to performance begin to appear. Some of the problems, such as
the number of users that can access the server simultaneously, the number of
requests that can be handled by the server per second (requests per second) to
bandwidth consumption and hardware utilization like memories and CPU. To give
better quality of service (\textbf{\textit{QoS}}), web hosting providers and
also the system administrators and network administrators who manage the server
need a benchmark application to measure the capabilities of their servers.
Later, the application intends to work under Linux/Unix -- like platforms and
built using Erlang/OTP R11 as a concurrent oriented language under Fedora Core
Linux 5.0. \textbf{\textit{WiiBench}} is divided into two main parts, the
controller section and the launcher section. Controller is the core of the
application. It has several duties, such as read the benchmark scenario file,
configure the program based on the scenario, initialize the launcher section,
gather the benchmark results from local and remote Erlang node where the
launcher runs and write them in a log file (later the log file will be used to
generate a report page for the sysadmin). Controller also has function as a
timer which act as timing for user inters arrival to the server. Launcher
generates a number of users based on the scenario, initialize them and start
the benchmark by sending requests to the web server. The clients also gather
the benchmark result and send them to the controller."
"Considering a wireless sensor network whose nodes are distributed randomly
over a given area, a probability model for the network lifetime is provided.
Using this model and assuming that packet generation follows a Poisson
distribution, an analytical expression for the complementary cumulative density
function (ccdf) of the lifetime is obtained. Using this ccdf, one can
accurately find the probability that the network achieves a given lifetime. It
is also shown that when the number of sensors, $N$, is large, with an error
exponentially decaying with $N$, one can predict whether or not a certain
lifetime can be achieved. The results of this work are obtained for both
multi-hop and single-hop wireless sensor networks and are verified with
computer simulation. The approaches of this paper are shown to be applicable to
other packet generation models and the effect of the area shape is also
investigated."
"In this paper, the problem of pinning control for synchronization of complex
dynamical networks is discussed. A cost function of the controlled network is
defined by the feedback gain and the coupling strength of the network. An
interesting result is that lower cost is achieved by the control scheme of
pinning nodes with smaller degrees. Some rigorous mathematical analysis is
presented for achieving lower cost in the synchronization of different
star-shaped networks. Numerical simulations on some non-regular complex
networks generated by the Barabasi-Albert model and various star-shaped
networks are shown for verification and illustration."
"In this paper, the synchronizability problem of dynamical networks is
addressed, where better synchronizability means that the network synchronizes
faster with lower-overshoot. The L2 norm of the error vector e is taken as a
performance index to measure this kind of synchronizability. For the
equilibrium synchronization case, it is shown that there is a close
relationship between the L2 norm of the error vector e and the H2 norm of the
transfer function G of the linearized network about the equilibrium point.
Consequently, the effect of the network coupling topology on the H2 norm of the
transfer function G is analyzed. Finally, an optimal controller is designed,
according to the so-called LQR problem in modern control theory, which can
drive the whole network to its equilibrium point and meanwhile minimize the L2
norm of the output of the linearized network."
"For most wireless services with variable rate transmission, both average rate
and rate oscillation are important performance metrics. The traditional
performance criterion, utility of average transmission rate, boosts the average
rate but also results in high rate oscillations. We introduce a utility
function of instantaneous transmission rates. It is capable of facilitating the
resource allocation with flexible combinations of average rate and rate
oscillation. Based on the new utility, we consider the time and power
allocation in a time-shared wireless network. Two adaptation policies are
developed, namely, time sharing (TS) and joint time sharing and power control
(JTPC). An extension to quantized time sharing with limited channel feedback
(QTSL) for practical systems is also discussed. Simulation results show that by
controlling the concavity of the utility function, a tradeoff between the
average rate and rate oscillation can be easily made."
"Optical WDM mesh networks are able to transport huge amount of information.
The use of such technology however poses the problem of protection against
failures such as fibre cuts. One of the principal methods for link protection
used in optical WDM networks is pre-configured protection cycle (p-cycle). The
major problem of this method of protection resides in finding the optimal set
of p-cycles which protect the network for a given distribution of working
capacity. Existing heuristics generate a large set of p-cycle candidates which
are entirely independent of the network state, and from then the good sub-set
of p-cycles which will protect the network is selected. In this paper, we
propose a new algorithm of generation of p-cycles based on the incremental
aggregation of the shortest cycles. Our generation of p-cycles depends on the
state of the network. This enables us to choose an efficient set of p-cycles
which will protect the network. The set of p-cycles that we generate is the
final set which will protect the network, in other words our heuristic does not
go through the additional step of p-cycle selection"
"Redundant sensing capabilities are often required in sensor network
applications due to various reasons, e.g. robustness, fault tolerance, or
increased accuracy. At the same time high sensor redundancy offers the
possibility of increasing network lifetime by scheduling sleep intervals for
some sensors and still providing continuous service with help of the remaining
active sensors. In this paper centralized and distributed algorithms are
proposed to solve the k-coverage sensing problem and maximize network lifetime.
When physically possible, the proposed robust Controlled Greedy Sleep Algorithm
provides guaranteed service independently of node and communication errors in
the network. The performance of the algorithm is illustrated and compared to
results of a random solution by simulation examples."
"Wireless microsensor networks, which have been the topic of intensive
research in recent years, are now emerging in industrial applications. An
important milestone in this transition has been the release of the IEEE
802.15.4 standard that specifies interoperable wireless physical and medium
access control layers targeted to sensor node radios. In this paper, we
evaluate the potential of an 802.15.4 radio for use in an ultra low power
sensor node operating in a dense network. Starting from measurements carried
out on the off-the-shelf radio, effective radio activation and link adaptation
policies are derived. It is shown that, in a typical sensor network scenario,
the average power per node can be reduced down to 211m mm mW. Next, the energy
consumption breakdown between the different phases of a packet transmission is
presented, indicating which part of the transceiver architecture can most
effectively be optimized in order to further reduce the radio power, enabling
self-powered wireless microsensor networks."
"Ultra-wideband (UWB) communication is an emerging wireless technology that
promises high data rates over short distances and precise locationing. The
large available bandwidth and the constraint of a maximum power spectral
density drives a unique set of system challenges. This paper addresses these
challenges using two UWB transceivers and a discrete prototype platform."
"Fast wireless access has rapidly become commonplace. Wireless access points
and Hotspot servers are sprouting everywhere. Battery lifetime continues to be
a critical issue in mobile computing. This paper first gives an overview of
WLAN energy saving strategies, followed by an illustration of a system-level
methodology for saving power in heterogeneous wireless environments."
"This paper retraces the historical development of wireless LAN technology in
the context of the pursuit of ever higher data rate, describes the significant
technical breakthroughs that are now occurring, and speculates on future
directions that the technology may take over the remainder of the decade. The
challenges that these developments have created for low power operation are
considered, as well as some of the opportunities that are presented to mitigate
them. The importance of MIMO as an emerging technology for 802.11 is
specifically highlighted, both in terms of the significant increase in data
rate and range that it enables as well as the considerable challenge that it
presents for the development of low power wireless LAN products."
"In this paper, we provide a throughput analysis of the IEEE 802.11 protocol
at the data link layer in non-saturated traffic conditions taking into account
the impact of both transmission channel and capture effects in Rayleigh fading
environment. The impact of both non-ideal channel and capture become important
in terms of the actual observed throughput in typical network conditions
whereby traffic is mainly unsaturated, especially in an environment of high
interference.
  We extend the multi-dimensional Markovian state transition model
characterizing the behavior at the MAC layer by including transmission states
that account for packet transmission failures due to errors caused by
propagation through the channel, along with a state characterizing the system
when there are no packets to be transmitted in the buffer of a station.
Finally, we derive a linear model of the throughput along with its interval of
validity.
  Simulation results closely match the theoretical derivations confirming the
effectiveness of the proposed model."
"In this paper, we provide a saturation throughput analysis of the IEEE 802.11
protocol at the data link layer by including the impact of both transmission
channel and capture effects in Rayleigh fading environment. Impacts of both
non-ideal channel and capture effects, specially in an environment of high
interference, become important in terms of the actual observed throughput. As
far as the 4-way handshaking mechanism is concerned, we extend the
multi-dimensional Markovian state transition model characterizing the behavior
at the MAC layer by including transmission states that account for packet
transmission failures due to errors caused by propagation through the channel.
This way, any channel model characterizing the physical transmission medium can
be accommodated, including AWGN and fading channels. We also extend the Markov
model in order to consider the behavior of the contention window when employing
the basic 2-way handshaking mechanism.
  Under the usual assumptions regarding the traffic generated per node and
independence of packet collisions, we solve for the stationary probabilities of
the Markov chain and develop expressions for the saturation throughput as a
function of the number of terminals, packet sizes, raw channel error rates,
capture probability, and other key system parameters. The theoretical
derivations are then compared to simulation results confirming the
effectiveness of the proposed models."
"We propose a linear model of the throughput of the IEEE 802.11 Distributed
Coordination Function (DCF) protocol at the data link layer in non-saturated
traffic conditions. We show that the throughput is a linear function of the
packet arrival rate (PAR) $\lambda$ with a slope depending on both the number
of contending stations and the average payload length. We also derive the
interval of validity of the proposed model by showing the presence of a
critical $\lambda$, above which the station begins operating in saturated
traffic conditions.
  The analysis is based on the multi-dimensional Markovian state transition
model proposed by Liaw \textit{et al.} with the aim of describing the behaviour
of the MAC layer in unsaturated traffic conditions. Simulation results closely
match the theoretical derivations, confirming the effectiveness of the proposed
linear model."
"In this paper, we provide a throughput analysis of the IEEE 802.11 protocol
at the data link layer in non-saturated traffic conditions taking into account
the impact of both transmission channel and capture effects in Rayleigh fading
environment. Impacts of both non-ideal channel and capture become important in
terms of the actual observed throughput in typical network conditions whereby
traffic is mainly unsaturated, specially in an environment of high
interference.
  We extend the multi-dimensional Markovian state transition model
characterizing the behavior at the MAC layer by including transmission states
that account for packet transmission failures due to errors caused by
propagation through the channel, along with a state characterizing the system
when there are no packets to be transmitted in the buffer of a station."
"In this note we analyse various stability properties of the max-min fair Rate
Control Protocol (RCP) operating with small buffers. We first tackle the issue
of stability for networks with arbitrary topologies. We prove that the max-min
fair RCP fluid model is globally stable in the absence of propagation delays,
and also derive a set of conditions for local stability when arbitrary
heterogeneous propagation delays are present. The network delay stability
result assumes that, at equilibrium, there is only one bottleneck link along
each route. Lastly, in the simpler setting of a single link, single delay
model, we investigate the impact of the loss of local stability via a Hopf
bifurcation."
"Advanced channel reservation is emerging as an important feature of ultra
high-speed networks requiring the transfer of large files. Applications include
scientific data transfers and database backup. In this paper, we present two
new, on-line algorithms for advanced reservation, called BatchAll and BatchLim,
that are guaranteed to achieve optimal throughput performance, based on
multi-commodity flow arguments. Both algorithms are shown to have
polynomial-time complexity and provable bounds on the maximum delay for
1+epsilon bandwidth augmented networks. The BatchLim algorithm returns the
completion time of a connection immediately as a request is placed, but at the
expense of a slightly looser competitive ratio than that of BatchAll. We also
present a simple approach that limits the number of parallel paths used by the
algorithms while provably bounding the maximum reduction factor in the
transmission throughput. We show that, although the number of different paths
can be exponentially large, the actual number of paths needed to approximate
the flow is quite small and proportional to the number of edges in the network.
Simulations for a number of topologies show that, in practice, 3 to 5 parallel
paths are sufficient to achieve close to optimal performance. The performance
of the competitive algorithms are also compared to a greedy benchmark, both
through analysis and simulation."
"In this paper we consider security-related and energy-efficiency issues in
multi-hop wireless networks. We start our work from the observation, known in
the literature, that shortest path routing creates congested areas in multi-hop
wireless networks. These areas are critical--they generate both security and
energy efficiency issues. We attack these problems and set out routing in outer
space, a new routing mechanism that transforms any shortest path routing
protocol (or approximated versions of it) into a new protocol that, in case of
uniform traffic, guarantees that every node of the network is responsible for
relaying the same number of messages, on expectation. We can show that a
network that uses routing in outer space does not have congested areas, does
not have the associated security-related issues, does not encourage selfish
positioning, and, in spite of using more energy globally, lives longer of the
same network using the original routing protocol."
"In this work we consider the problem of downlink resource allocation for
proportional fairness of long term received rates of data users and quality of
service for real time sessions in an OFDMA-based wireless system. The base
station allocates available power and bandwidth to individual users based on
long term average received rates, QoS based rate constraints and channel
conditions. We solve the underlying constrained optimization problem and
propose an algorithm that achieves the optimal allocation. Numerical evaluation
results show that the proposed algorithm provides better QoS to voice and video
sessions while providing more and fair rates to data users in comparison with
existing schemes."
"In this work we propose an efficient resource allocation algorithm for OFDMA
based wireless systems supporting heterogeneous traffic. The proposed algorithm
provides proportionally fairness to data users and short term rate guarantees
to real-time users. Based on the QoS requirements, buffer occupancy and channel
conditions, we propose a scheme for rate requirement determination for delay
constrained sessions. Then we formulate and solve the proportional fair rate
allocation problem subject to those rate requirements and power/bandwidth
constraints. Simulations results show that the proposed algorithm provides
significant improvement with respect to the benchmark algorithm."
"The Semantic Web drives towards the use of the Web for interacting with
logically interconnected data. Through knowledge models such as Resource
Description Framework (RDF), the Semantic Web provides a unifying
representation of richly structured data. Adding logic to the Web implies the
use of rules to make inferences, choose courses of action, and answer
questions. This logic must be powerful enough to describe complex properties of
objects but not so powerful that agents can be tricked by being asked to
consider a paradox. The Web has several characteristics that can lead to
problems when existing logics are used, in particular, the inconsistencies that
inevitably arise due to the openness of the Web, where anyone can assert
anything. N3Logic is a logic that allows rules to be expressed in a Web
environment. It extends RDF with syntax for nested graphs and quantified
variables and with predicates for implication and accessing resources on the
Web, and functions including cryptographic, string, math. The main goal of
N3Logic is to be a minimal extension to the RDF data model such that the same
language can be used for logic and data. In this paper, we describe N3Logic and
illustrate through examples why it is an appropriate logic for the Web."
"In this paper, the relationship between the network synchronizability and the
edge distribution of its associated graph is investigated. First, it is shown
that adding one edge to a cycle definitely decreases the network
sychronizability. Then, since sometimes the synchronizability can be enhanced
by changing the network structure, the question of whether the networks with
more edges are easier to synchronize is addressed. It is shown by examples that
the answer is negative. This reveals that generally there are redundant edges
in a network, which not only make no contributions to synchronization but
actually may reduce the synchronizability. Moreover, an example shows that the
node betweenness centrality is not always a good indicator for the network
synchronizability. Finally, some more examples are presented to illustrate how
the network synchronizability varies following the addition of edges, where all
the examples show that the network synchronizability globally increases but
locally fluctuates as the number of added edges increases."
"Epidemics-inspired techniques have received huge attention in recent years
from the distributed systems and networking communities. These algorithms and
protocols rely on probabilistic message replication and redundancy to ensure
reliable communication. Moreover, they have been successfully exploited to
support group communication in distributed systems, broadcasting, multicasting
and information dissemination in fixed and mobile networks. However, in most of
the existing work, the probability of infection is determined heuristically,
without relying on any analytical model. This often leads to unnecessarily high
transmission overheads.
  In this paper we show that models of epidemic spreading in complex networks
can be applied to the problem of tuning and controlling the dissemination of
information in wireless ad hoc networks composed of devices carried by
individuals, i.e., human-based networks. The novelty of our idea resides in the
evaluation and exploitation of the structure of the underlying human network
for the automatic tuning of the dissemination process in order to improve the
protocol performance. We evaluate the results using synthetic mobility models
and real human contacts traces."
"In ad hoc networks scalability is a critical requirement if these
technologies have to reach their full potential. Most of the proposed routing
protocols do not operate efficiently with networks of more than a few hundred
nodes. In this paper, we propose an augmented tree-based address space
structure and a hierarchical multi-path routing protocol, referred to as
Augmented Tree-based Routing (ATR), which utilizes such a structure in order to
solve the scalability problem and to gain good resilience against node
failure/mobility and link congestion/instability. Simulation results and
performance comparisons with existing protocols substantiate the effectiveness
of the ATR."
"We consider a network model where the nodes are grouped into a number of
clusters and propose a distributed dynamic frequency allocation algorithm that
achieves performance close to that of a centralized optimal algorithm. Each
cluster chooses its transmission frequency band based on its knowledge of the
interference that it experiences. The convergence of the proposed distributed
algorithm to a sub-optimal frequency allocation pattern is proved. For some
specific cases of spatial distributions of the clusters in the network,
asymptotic bounds on the performance of the algorithm are derived and
comparisons to the performance of optimal centralized solutions are made. These
analytic results and additional simulation studies verify performance close to
that of an optimum centralized frequency allocation algorithm. It is
demonstrated that the algorithm achieves about 90% of the Shannon capacities
corresponding to the optimum/near-optimum centralized frequency band
assignments. Furthermore, we consider the scenario where each cluster can be in
active or inactive mode according to a two-state Markov model. We derive
conditions to guarantee finite steady state variance for the output of the
algorithm using stochastic analysis. Further simulation studies confirm the
results of stochastic modeling and the performance of the algorithm in the
time-varying setup."
"The discovery of Autonomous Systems (ASes) interconnections and the inference
of their commercial Type-of-Relationships (ToR) has been extensively studied
during the last few years. The main motivation is to accurately calculate
AS-level paths and to provide better topological view of the Internet. An
inherent problem in current algorithms is their extensive use of heuristics.
Such heuristics incur unbounded errors which are spread over all inferred
relationships. We propose a near-deterministic algorithm for solving the ToR
inference problem. Our algorithm uses as input the Internet core, which is a
dense sub-graph of top-level ASes. We test several methods for creating such a
core and demonstrate the robustness of the algorithm to the core's size and
density, the inference period, and errors in the core.
  We evaluate our algorithm using AS-level paths collected from RouteViews BGP
paths and DIMES traceroute measurements. Our proposed algorithm
deterministically infers over 95% of the approximately 58,000 AS topology
links. The inference becomes stable when using a week worth of data and as
little as 20 ASes in the core. The algorithm infers 2-3 times more peer-to-peer
relationships in edges discovered only by DIMES than in RouteViews edges,
validating the DIMES promise to discover periphery AS edges."
"This paper considers the requirements to ensure bounded mean queuing delay
and non-starvation in a slotted Aloha network operating the exponential backoff
protocol. It is well-known that the maximum possible throughput of a slotted
Aloha system with a large number of nodes is 1/e = 0.3679. Indeed, a saturation
throughput of 1/e can be achieved with an exponential backoff factor of r =
e/(e-1)=1.5820. The binary backoff factor of r = 2 is assumed in the majority
of prior work, and in many practical multiple-access networks such as the
Ethernet and WiFi. For slotted Aloha, the saturation throughput 0.3466 for r =
2 is reasonably close to the maximum of 1/e, and one could hardly raise
objection to adopting r = 2 in the system. However, this paper shows that if
mean queuing delay is to be bounded, then the sustainable throughput when r = 2
is only 0.2158, a drastic 41% drop from 1/e . Fortunately, the optimal setting
of r = 1.3757 under the bounded mean-delay requirement allows us to achieve
sustainable throughput of 0.3545, a penalty of only less than 4% relative to
1/e. A general conclusion is that the value of r may significantly affect the
queuing delay performance. Besides analyzing mean queuing delay, this paper
also delves into the phenomenon of starvation, wherein some nodes are deprived
of service for an extended period of time while other nodes hog the system.
Specifically, we propose a quantitative definition for starvation and show that
the conditions to guarantee bounded mean delay and non-starved operation are
one of the same, thus uniting these two notions. Finally, we show that when
mean delay is large and starvation occurs, the performance results obtained
from simulation experiments may not converge. A quantitative discussion of this
issue is provided in this paper."
"The main goal of routing protocol is to efficiency delivers data from source
to destination. All routing protocols are the same in this goal, but the way
they adopt to achieve it is different, so routing strategy has an egregious
role on the performance of an ad hoc network. Most of routing protocols
proposed for ad hoc networks have a flat structure. These protocols expand the
control overhead packets to discover or maintain a route. On the other hand a
number of hierarchical-based routing protocols have been developed, mostly are
based on layered design. These protocols improve network performances
especially when the network size grows up since details about remote portion of
network can be handled in an aggregate manner. Although, there is another
approach to design a protocol called cross-layer design. Using this approach
information can exchange between different layer of protocol stack, result in
optimizing network performances.
  In this paper, we intend to exert cross-layer design to optimize Cluster
Based Routing Protocol (Cross-CBRP). Using NS-2 network simulator we evaluate
rate of cluster head changes, throughput and packet delivery ratio. Comparisons
denote that Cross-CBRP has better performances with respect to the original
CBRP."
"The aim of this paper is twofold. On one hand, it presents a
multi-dimensional Markovian state transition model characterizing the behavior
at the Medium Access Control (MAC) layer by including transmission states that
account for packet transmission failures due to errors caused by propagation
through the channel, along with a state characterizing the system when there
are no packets to be transmitted in the queue of a station (to model
non-saturated traffic conditions). On the other hand, it provides a throughput
analysis of the IEEE 802.11 protocol at the data link layer in both saturated
and non-saturated traffic conditions taking into account the impact of both
transmission channel and multirate transmission in Rayleigh fading environment.
Simulation results closely match the theoretical derivations confirming the
effectiveness of the proposed model."
"In this article, we present a multi-tool method for the development and the
analysis of a new medium access method. IEEE 802.15.4 / ZigBee technology has
been used as a basis for this new determinist MAC layer which enables a high
level of QoS. This WPAN can be typically used for wireless sensor networks
which require strong temporal constraints. To validate the proposed protocol,
three complementary and adequate tools are used: Petri Nets for the formal
validation of the algorithm, a dedicated simulator for the temporal aspects,
and some measures on a real prototype based on a couple of ZigBee FREESCALE
components for the hardware characterization of layers #1 and #2."
"Peer-to-peer content distribution systems have been enjoying great
popularity, and are now gaining momentum as a means of disseminating video
streams over the Internet. In many of these protocols, including the popular
BitTorrent, content is split into mostly fixed-size pieces, allowing a client
to download data from many peers simultaneously. This makes piece size
potentially critical for performance. However, previous research efforts have
largely overlooked this parameter, opting to focus on others instead. This
paper presents the results of real experiments with varying piece sizes on a
controlled BitTorrent testbed. We demonstrate that this parameter is indeed
critical, as it determines the degree of parallelism in the system, and we
investigate optimal piece sizes for distributing small and large content. We
also pinpoint a related design trade-off, and explain how BitTorrent's choice
of dividing pieces into subpieces attempts to address it."
"A restless multi-armed bandit problem that arises in multichannel
opportunistic communications is considered, where channels are modeled as
independent and identical Gilbert-Elliot channels and channel state
observations are subject to errors. A simple structure of the myopic policy is
established under a certain condition on the false alarm probability of the
channel state detector. It is shown that the myopic policy has a semi-universal
structure that reduces channel selection to a simple round-robin procedure and
obviates the need to know the underlying Markov transition probabilities. The
optimality of the myopic policy is proved for the case of two channels and
conjectured for the general case based on numerical examples."
"Modeling Internet growth is important both for understanding the current
network and to predict and improve its future. To date, Internet models have
typically attempted to explain a subset of the following characteristics:
network structure, traffic flow, geography, and economy. In this paper we
present a discrete, agent-based model, that integrates all of them. We show
that the model generates networks with topologies, dynamics, and (more
speculatively) spatial distributions that are similar to the Internet."
"To date, most analysis of WLANs has been focused on their operation under
saturation condition. This work is an attempt to understand the fundamental
performance of WLANs under unsaturated condition. In particular, we are
interested in the delay performance when collisions of packets are resolved by
an exponential backoff mechanism. Using a multiple-vacation queueing model, we
derive an explicit expression for packet delay distribution, from which
necessary conditions for finite mean delay and delay jitter are established. It
is found that under some circumstances, mean delay and delay jitter may
approach infinity even when the traffic load is way below the saturation
throughput. Saturation throughput is therefore not a sound measure of WLAN
capacity when the underlying applications are delay sensitive. To bridge the
gap, we define safe-bounded-mean-delay (SBMD) throughput and
safe-bounded-delay-jitter (SBDJ) throughput that reflect the actual network
capacity users can enjoy when they require bounded mean delay and delay jitter,
respectively. The analytical model in this paper is general enough to cover
both single-packet reception (SPR) and multi-packet reception (MPR) WLANs, as
well as carrier-sensing and non-carrier-sensing networks. We show that the SBMD
and SBDJ throughputs scale super-linearly with the MPR capability of a network.
Together with our earlier work that proves super-linear throughput scaling
under saturation condition, our results here complete the demonstration of MPR
as a powerful capacity-enhancement technique for both delay-sensitive and
delay-tolerant applications."
"We introduce four algorithms for packet transport in complex networks. These
algorithms use deterministic rules which depend, in different ways, on the
degree of the node, the number of packets posted down each edge, the mean
delivery time of packets sent down each edge to each destination and the time
since an edge last transmitted a packet. On scale-free networks all our
algorithms are considerably more efficient and can handle a larger load than
the random walk algorithm. We consider in detail various attributes of our
algorithms, for instance we show that an algorithm that bases its decisions on
the mean delivery time jams unless it incorporates information about the degree
of the destination node."
"Due to emerging real-time and multimedia applications, efficient routing of
information packets in dynamically changing communication network requires that
as the load levels, traffic patterns and topology of the network change, the
routing policy also adapts. We focused in this paper on QoS based routing by
developing a neuro-dynamic programming to construct dynamic state dependent
routing policies. We propose an approach based on adaptive algorithm for packet
routing using reinforcement learning which optimizes two criteria: cumulative
cost path and end-to-end delay. Numerical results obtained with OPNET simulator
for different packet interarrival times statistical distributions with
different levels of traffic's load show that the proposed approach gives better
results compared to standard optimal path routing algorithms."
"In this proposition, we present a link management technique for pro-active
routing protocols for ad-hoc networks. This new mechanism is based on signal
strength hence cross layer approach is used. The hysteresis mechanism provided
by OLSR is improved upon by using signal strength in combination with the hello
loss based hysteresis. The signal power is used to determine if the
link-quality is improving or deteriorating while packet losses are handled
through the hysteresis mechanism specified in OLSR RFC. This not only makes the
link management more robust but also helps in anticipating link breakages
thereby greatly improving the performance."
"The surest way to increase the system capacity of a wireless link is by
getting the transmitter and receiver closer to each other, which creates the
dual benefits of higher quality links and more spatial reuse. In a network with
nomadic users, this inevitably involves deploying more infrastructure,
typically in the form of microcells, hotspots, distributed antennas, or relays.
A less expensive alternative is the recent concept of femtocells, also called
home base-stations, which are data access points installed by home users get
better indoor voice and data coverage. In this article, we overview the
technical and business arguments for femtocells, and describe the
state-of-the-art on each front. We also describe the technical challenges
facing femtocell networks, and give some preliminary ideas for how to overcome
them."
"We investigate in this paper the performance of a simple file sharing
principle. For this purpose, we consider a system composed of N peers becoming
active at exponential random times; the system is initiated with only one
server offering the desired file and the other peers after becoming active try
to download it. Once the file has been downloaded by a peer, this one
immediately becomes a server. To investigate the transient behavior of this
file sharing system, we study the instant when the system shifts from a
congested state where all servers available are saturated by incoming demands
to a state where a growing number of servers are idle. In spite of its apparent
simplicity, this queueing model (with a random number of servers) turns out to
be quite difficult to analyze. A formulation in terms of an urn and ball model
is proposed and corresponding scaling results are derived. These asymptotic
results are then compared against simulations."
"Experimentation is important when designing communication protocols for
Wireless Sensor Networks. Lower-layers have a major impact on upper-layer
performance, and the complexity of the phenomena can not be entirely captured
by analysis or simulation. In this report, we go through the complete process,
from designing an energy-efficient self-organizing communication architecture
(MAC, routing and application layers) to real-life experimentation roll-outs.
The presented communication architecture includes a MAC protocol which avoids
building and maintaining neighborhood tables, and a geographically-inspired
routing protocol over virtual coordinates. The application consists of a mobile
sink interrogating a wireless sensor network based on the requests issued by a
disconnected base station. After the design process of this architecture, we
verify it functions correctly by simulation, and we perform a temporal
verification. This study is needed to calculate the maximum speed the mobile
sink can take. We detail the implementation, and the results of the off-site
experimentation (energy consumption at PHY layer, collision probability at MAC
layer, and routing). Finally, we report on the real-world deployment where we
have mounted the mobile sink node on a radio-controlled airplane."
"This paper presents a modified proportional fairness (PF) criterion suitable
for mitigating the \textit{rate anomaly} problem of multirate IEEE 802.11
Wireless LANs employing the mandatory Distributed Coordination Function (DCF)
option. Compared to the widely adopted assumption of saturated network, the
proposed criterion can be applied to general networks whereby the contending
stations are characterized by specific packet arrival rates, $\lambda_s$, and
transmission rates $R_d^{s}$.
  The throughput allocation resulting from the proposed algorithm is able to
greatly increase the aggregate throughput of the DCF while ensuring fairness
levels among the stations of the same order of the ones available with the
classical PF criterion. Put simply, each station is allocated a throughput that
depends on a suitable normalization of its packet rate, which, to some extent,
measures the frequency by which the station tries to gain access to the
channel. Simulation results are presented for some sample scenarios, confirming
the effectiveness of the proposed criterion."
"Recent literature has proved that stable dynamic routing algorithms have
solid theoretical foundation that makes them suitable to be implemented in a
real protocol, and used in practice in many different operational network
contexts. Such algorithms inherit much of the properties of congestion
controllers implementing one of the possible combination of AQM/ECN schemes at
nodes and flow control at sources. In this paper we propose a linear program
formulation of the multi-commodity flow problem with congestion control, under
max-min fairness, comprising demands with or without exogenous peak rates. Our
evaluations of the gain, using path diversity, in scenarios as intra-domain
traffic engineering and wireless mesh networks encourages real implementations,
especially in presence of hot spots demands and non uniform traffic matrices.
We propose a flow aware perspective of the subject by using a natural
multi-path extension to current congestion controllers and show its performance
with respect to current proposals. Since flow aware architectures exploiting
path diversity are feasible, scalable, robust and nearly optimal in presence of
flows with exogenous peak rates, we claim that our solution rethinked in the
context of realistic traffic assumptions performs as better as an optimal
approach with all the additional benefits of the flow aware paradigm."
"Discriminatory Processor Sharing policy introduced by Kleinrock is of a great
interest in many application areas, including telecommunications, web
applications and TCP flow modelling. Under the DPS policy the job priority is
controlled by the vector of weights. Verifying the vector of weights it is
possible to modify the service rates of the jobs and optimize system
characteristics. In the present paper we present the results concerning the
comparison of two DPS policies with different weight vectors. We show the
monotonicity of the expected sojourn time of the system depending on the weight
vector under certain condition on the system. Namely, the system has to consist
of classes with means which are quite different from each other. The classes
with similar means can be organized together and considered as one class, so
the given restriction can be overcame."
"A well-known approach to intradomain traffic engineering consists in finding
the set of link weights that minimizes a network-wide objective function for a
given intradomain traffic matrix. This approach is inadequate because it
ignores a potential impact on interdomain routing. Indeed, the resulting set of
link weights may trigger BGP to change the BGP next hop for some destination
prefixes, to enforce hot-potato routing policies. In turn, this results in
changes in the intradomain traffic matrix that have not been anticipated by the
link weights optimizer, possibly leading to degraded network performance.
  We propose a BGP-aware link weights optimization method that takes these
effects into account, and even turns them into an advantage. This method uses
the interdomain traffic matrix and other available BGP data, to extend the
intradomain topology with external virtual nodes and links, on which all the
well-tuned heuristics of a classical link weights optimizer can be applied. A
key innovative asset of our method is its ability to also optimize the traffic
on the interdomain peering links. We show, using an operational network as a
case study, that our approach does so efficiently at almost no extra
computational cost."
"The locator/identifier split approach assumes separating functions of a
locator (i.e. topology--dependent attachment point address) and identifier
(topology-independent unique identifier) currently both served by an IP
address. This work is an attempt to redefine semantics of MAC address to make
it a pure layer-2 locator instead of a pure globally-unique identifier. Such an
exercise might be interesting from the standpoint of Ethernet scaling and Metro
Ethernet technologies. From the global routing perspective, introduction of
multihoming, traffic engineering and failover at the 2nd layer may reduce
pressure on the 3rd layer."
"Network coding is a recently proposed method for transmitting data, which has
been shown to have potential to improve wireless network performance. We study
network coding for one specific case of multicast, broadcasting, from one
source to all nodes of the network. We use network coding as a loss tolerant,
energy-efficient, method for broadcast. Our emphasis is on mobile networks. Our
contribution is the proposal of DRAGONCAST, a protocol to perform network
coding in such a dynamically evolving environment. It is based on three
building blocks: a method to permit real-time decoding of network coding, a
method to adjust the network coding transmission rates, and a method for
ensuring the termination of the broadcast. The performance and behavior of the
method are explored experimentally by simulations; they illustrate the
excellent performance of the protocol."
"Several users of our AS relationship inference data
(http://www.caida.org/data/active/as-relationships/), released with cs/0604017,
asked us why it contained AS relationship cycles, e.g., cases where AS A is a
provider of AS B, B is a provider of C, and C is a provider of A, or other
cycle types. Having been answering these questions in private communications,
we have eventually decided to write down our answers here for future reference."
"We propose behavior-oriented services as a new paradigm of communication in
mobile human networks. Our study is motivated by the tight user-network
coupling in future mobile societies. In such a paradigm, messages are sent to
inferred behavioral profiles, instead of explicit IDs. Our paper provides a
systematic framework in providing such services. First, user behavioral
profiles are constructed based on traces collected from two large wireless
networks, and their spatio-temporal stability is analyzed. The implicit
relationship discovered between mobile users could be utilized to provide a
service for message delivery and discovery in various network environments. As
an example application, we provide a detailed design of such a service in
challenged opportunistic network architecture, named CSI. We provide a fully
distributed solution using behavioral profile space gradients and small world
structures.
  Our analysis shows that user behavioral profiles are surprisingly stable,
i.e., the similarity of the behavioral profile of a user to its future
behavioral profile is above 0.8 for two days and 0.75 for one week, and remains
above 0.6 for five weeks. The correlation coefficient of the similarity metrics
between a user pair at different time instants is above 0.7 for four days, 0.62
for a week, and remains above 0.5 for two weeks. Leveraging such a stability in
user behaviors, the CSI service achieves delivery rate very close to the
delay-optimal strategy (above 94%), with minimal overhead (less than 84% of the
optimal). We believe that this new paradigm will act as an enabler of multiple
new services in mobile societies, and is potentially applicable in
server-based, heterogeneous or infrastructure-less wireless environments."
"Voice Service Providers (VSPs) participating in VoIP peering frequently want
to withhold their identity and related privacy-sensitive information from other
parties during the VoIP communication. A number of existing documents on VoIP
privacy exist, but most of them focus on end user privacy. By summarizing and
extending existing work, we present a unified privacy mechanism for both VoIP
users and service providers. We also show a case study on how VSPs can use this
mechanism for identity and topology hiding in VoIP peering."
"In contrast with most internet topology measurement research, our concern
here is not to obtain a map as complete and precise as possible of the whole
internet. Instead, we claim that each machine's view of this topology, which we
call ego-centered view, is an object worth of study in itself. We design and
implement an ego-centered measurement tool, and perform radar-like measurements
consisting of repeated measurements of such views of the internet topology. We
conduct long-term (several weeks) and high-speed (one round every few minutes)
measurements of this kind from more than one hundred monitors, and we provide
the obtained data. We also show that these data may be used to detect events in
the dynamics of internet topology."
"Many models have been proposed to generate Internet Autonomous System (AS)
topologies, most of which make structural assumptions about the AS graph. In
this paper we compare AS topology generation models with several observed AS
topologies. In contrast to most previous works, we avoid making assumptions
about which topological properties are important to characterize the AS
topology. Our analysis shows that, although matching degree-based properties,
the existing AS topology generation models fail to capture the complexity of
the local interconnection structure between ASs. Furthermore, we use BGP data
from multiple vantage points to show that additional measurement locations
significantly affect local structure properties, such as clustering and node
centrality. Degree-based properties, however, are not notably affected by
additional measurements locations. These observations are particularly valid in
the core. The shortcomings of AS topology generation models stems from an
underestimation of the complexity of the connectivity in the core caused by
inappropriate use of BGP data."
"Vehicle-to-vehicle communications can be used effectively for intelligent
transport systems (ITS) and location-aware services. The ability to disseminate
information in an ad-hoc fashion allows pertinent information to propagate
faster through the network. In the realm of ITS, the ability to spread warning
information faster and further is of great advantage to the receivers of this
information. In this paper we propose and present a message-dissemination
procedure that uses vehicular wireless protocols for influencing traffic flow,
reducing congestion in road networks. The computational experiments presented
in this paper show how an intelligent driver model (IDM) and car-following
model can be adapted to 'react' to the reception of information. This model
also presents the advantages of coupling together traffic modelling tools and
network simulation tools."
"This paper has been withdrawn due to errors in the analysis of data with
Carrier Access Rate control and statistical methodologies."
"Energy efficiency and transmission delay are very important parameters for
wireless multi-hop networks. Previous works that study energy efficiency and
delay are based on the assumption of reliable links. However, the unreliability
of the channel is inevitable in wireless multi-hop networks. This paper
investigates the trade-off between the energy consumption and the end-to-end
delay of multi-hop communications in a wireless network using an unreliable
link model. It provides a closed form expression of the lower bound on the
energy-delay trade-off for different channel models (AWGN, Raleigh flat fading
and Nakagami block-fading) in a linear network. These analytical results are
also verified in 2-dimensional Poisson networks using simulations. The main
contribution of this work is the use of a probabilistic link model to define
the energy efficiency of the system and capture the energy-delay trade-offs.
Hence, it provides a more realistic lower bound on both the energy efficiency
and the energy-delay trade-off since it does not restrict the study to the set
of perfect links as proposed in earlier works."
"We consider power control in cognitive radio networks where secondary users
identify and exploit instantaneous and local spectrum opportunities without
causing unacceptable interference to primary users. We qualitatively
characterize the impact of the transmission power of secondary users on the
occurrence of spectrum opportunities and the reliability of opportunity
detection. Based on a Poisson model of the primary network, we quantify these
impacts by showing that (i) the probability of spectrum opportunity decreases
exponentially with respect to the transmission power of secondary users, where
the exponential decay constant is given by the traffic load of primary users;
(ii) reliable opportunity detection is achieved in the two extreme regimes in
terms of the ratio between the transmission power of secondary users and that
of primary users. Such analytical characterizations allow us to study power
control for optimal transport throughput under constraints on the interference
to primary users. Furthermore, we reveal the difference between detecting
primary signals and detecting spectrum opportunities, and demonstrate the
complex relationship between physical layer spectrum sensing and MAC layer
throughput. The dependency of this PHY-MAC interaction on the application type
and the use of handshake signaling such as RTS/CTS is illustrated."
"Acyclic preferences recently appeared as an elegant way to model many
distributed systems. An acyclic instance admits a unique stable configuration,
which can reveal the performance of the system. In this paper, we give the
statistical properties of the stable configuration for three classes of acyclic
preferences: node-based preferences, distance-based preferences, and random
acyclic systems. Using random overlay graphs, we prove using mean-field and
fluid-limit techniques that these systems have an asymptotically continuous
independent rank distribution for a proper scaling, and the analytical solution
is compared to simulations. These results provide a theoretical ground for
validating the performance of bandwidth-based or proximity-based unstructured
systems."
"This paper focuses on multirate IEEE 802.11 Wireless LAN employing the
mandatory Distributed Coordination Function (DCF) option. Its aim is threefold.
Upon starting from the multi-dimensional Markovian state transition model
proposed by Malone \textit{et.al.} for characterizing the behavior of the IEEE
802.11 protocol at the Medium Access Control layer, it presents an extension
accounting for packet transmission failures due to channel errors. Second, it
establishes the conditions under which a network constituted by $N$ stations,
each station transmitting with its own bit rate, $R^{(s)}_d$, and packet rate,
$\lambda_s$, can be assumed loaded. Finally, it proposes a modified
Proportional Fairness (PF) criterion, suitable for mitigating the \textit{rate
anomaly} problem of multirate loaded IEEE 802.11 Wireless LANs, employing the
mandatory DCF option. Compared to the widely adopted assumption of saturated
network, the proposed fairness criterion can be applied to general loaded
networks.
  The throughput allocation resulting from the proposed algorithm is able to
greatly increase the aggregate throughput of the DCF, while ensuring fairness
levels among the stations of the same order as the ones guaranteed by the
classical PF criterion.
  Simulation results are presented for some sample scenarios, confirming the
effectiveness of the proposed criterion for optimized throughput allocation."
"An ad-hoc wireless network is a collection of nodes that come together to
dynamically create a network, with no fixed infrastructure or centralized
administration. An ad-hoc network is characterized by energy constrained nodes,
bandwidth constrained links and dynamic topology. With the growing use of
wireless networks (including ad-hoc networks) for real-time applications, such
as voice, video, and real-time data, the need for Quality of Service (QoS)
guarantees in terms of delay, bandwidth, and packet loss is becoming
increasingly important. Providing QoS in ad-hoc networks is a challenging task
because of dynamic nature of network topology and imprecise state information.
Hence, it is important to have a dynamic routing protocol with fast re-routing
capability, which also provides stable route during the life-time of the flows.
  In this thesis, we have proposed a novel, energy aware, stable routing
protocol named, Stability-based QoS-capable Ad-hoc On-demand Distance Vector
(SQ-AODV), which is an enhancement of the well-known Ad-hoc On-demand Distance
Vector (AODV) routing protocol for ad-hoc wireless networks. SQ-AODV utilizes a
cross-layer design approach in which information about the residual energy of a
node is used for route selection and maintenance. An important feature of
SQ-AODV protocol is that it uses only local information and requires no
additional communication or co-operation between the network nodes. SQ-AODV
possesses a make-before-break re-routing capability that enables near-zero
packet drops and is compatible with the basic AODV data formats and operation,
making it easy to adopt in ad-hoc networks."
"In this paper, we investigate the network utility maximization problem in
FDMA systems. We summarize with a suite of criteria on designing utility
functions so as to achieve the global optimization convex. After proposing the
general form of the utility functions, we present examples of commonly used
utility function forms that are consistent with the criteria proposed in this
paper, which include the well-known proportional fairness function and the
sigmoidal-like functions. In the second part of this paper, we use numerical
results to demonstrate a case study based on the criteria mentioned above,
which deals with the subcarrier scheduling problem with dynamic rate allocation
in FDMA system."
"A distributed spiral algorithm for distributed optimization in WSN is
proposed. By forming a spiral-shape message passing scheme among clusters,
without loss of estimation accuracy and convergence speed, the algorithm is
proved to converge with a lower total transport cost than the distributed
in-cluster algorithm."
"Conventional heterogeneous-traffic scheduling schemes utilize zero-delay
constraint for real-time services, which aims to minimize the average packet
delay among real-time users. However, in light or moderate load networks this
strategy is unnecessary and leads to low data throughput for non-real-time
users. In this paper, we propose a heuristic scheduling scheme to solve this
problem. The scheme measures and assigns scheduling priorities to both
real-time and non-real-time users, and schedules the radio resources for the
two user classes simultaneously. Simulation results show that the proposed
scheme efficiently handles the heterogeneous-traffic scheduling with diverse
QoS requirements and alleviates the unfairness between real-time and
non-real-time services under various traffic loads."
"In this paper, we compare the performances of cooperative and distributed
spectrum sensing in wireless sensor networks. After introducing the basic
problem, we describe two strategies: 1) a cooperative sensing strategy, which
takes advantage of cooperation diversity gain to increase probability of
detection and 2) a distributed sensing strategy, which by passing the results
in an inter-node manner increases energy efficiency and fairness among nodes.
Then, we compare the performances of the strategies in terms of three criteria:
agility, energy efficiency, and robustness against SNR changes, and summarize
the comparison. It shows that: 1) the non-cooperative strategy has the best
fairness of energy consumption, 2) the cooperative strategy leads to the best
agility, and 3) the distributed strategy leads to the lowest energy consumption
and the best robustness against SNR changes."
"This paper first presents a parallel solution for the Flowshop Scheduling
Problem in parallel environment, and then proposes a novel load balancing
strategy. The proposed Proportional Fairness Strategy (PFS) takes computational
performance of computing process sets into account, and assigns additional load
to computing nodes proportionally to their evaluated performance. In order to
efficiently utilize the power of parallel resource, we also discuss the data
structure used in communications among computational nodes and design an
optimized data transfer strategy. This data transfer strategy combined with the
proposed load balancing strategy have been implemented and tested on a super
computer consisted of 86 CPUs using MPI as the middleware. The results show
that the proposed PFS achieves better performance in terms of computing time
than the existing Adaptive Contracting Within Neighborhood Strategy. We also
show that the combination of both the Proportional Fairness Strategy and the
proposed data transferring strategy achieves additional 13~15% improvement in
efficiency of parallelism."
"This paper presents a capture of the queries managed by an eDonkey server
during almost 10 weeks, leading to the observation of almost 9 billion messages
involving almost 90 million users and more than 275 million distinct files.
Acquisition and management of such data raises several challenges, which we
discuss as well as the solutions we developed. We obtain a very rich dataset,
orders of magnitude larger than previously avalaible ones, which we provide for
public use. We finally present basic analysis of the obtained data, which
already gives evidence of non-trivial features."
"We study a sensor node with an energy harvesting source. The generated energy
can be stored in a buffer. The sensor node periodically senses a random field
and generates a packet. These packets are stored in a queue and transmitted
using the energy available at that time. We obtain energy management policies
that are throughput optimal, i.e., the data queue stays stable for the largest
possible data rate. Next we obtain energy management policies which minimize
the mean delay in the queue.We also compare performance of several easily
implementable sub-optimal energy management policies. A greedy policy is
identified which, in low SNR regime, is throughput optimal and also minimizes
mean delay."
"This paper presents a new rate based call gapping method. The main advantage
is that it provides maximal throughput, priority handling and fairness for
traffic classes without queues, unlike Token Bucket which provides only the
first two or Weighted Fair Queuing that uses queues. The Token Bucket is used
for call gapping because it has good throughput characteristics. For this
reason we present a mixture of the two methods keeping the good properties of
both. A mathematical model has been developed to support our proposal. It
defines the three requirements and proves theorems about if they are satisfied
with the different call gapping mechanisms. Simulation, numerical results and
statistical discussion are also presented to underpin the findings."
"This work calls into question a substantial body of past work on CSMA
wireless networks. In the majority of studies on CSMA wireless networks, a
contention graph is used to model the carrier sensing relationships (CS) among
links. This is a 0-1 model in which two links can either sense each other
completely or not. In real experiments, we observed that this is generally not
the case: the CS relationship between the links are often probabilistic and can
vary dynamically over time. This is the case even if the distance between the
links is fixed and there is no drastic change in the environment. Furthermore,
this partial carrier sensing relationship is prevalent and occurs over a wide
range of distances between the links. This observation is not consistent with
the 0-1 contention graph and implies that many results and conclusions drawn
from previous theoretical studies need to be re-examined. This paper
establishes a more accurate CS model with the objective of laying down a
foundation for future theoretical studies that reflect reality. Towards that
end, we set up detailed experiments to investigate the partial carrier sensing
phenomenon. We discuss the implications and the use of our partial carrier
sensing model in network analysis."
"This paper proposes a new reliability algorithm specifically useful when
retransmission is either problematic or not possible. In case of multimedia or
multicast communications and in the context of the Delay Tolerant Networking
(DTN), the classical retransmission schemes can be counterproductive in terms
of data transfer performance or not possible when the acknowledgment path is
not always available. Indeed, over long delay links, packets retransmission has
a meaning of cost and must be minimized.In this paper, we detail a novel
reliability mechanism with an implicit acknowledgment strategy that could be
used either within these new DTN proposals, for multimedia traffic or in the
context of multicast transport protocols. This proposal is based on a new
on-the-fly erasure coding concept specifically designed to operate efficient
reliable transfer over bi-directional links. This proposal, named Tetrys,
allows to unify a full reliability with an error correction scheme. In this
paper, we model the performance of this proposal and demonstrate with a
prototype, that we can achieve a full reliability without acknowledgment path
confirmation. Indeed, the main findings are that Tetrys is not sensitive to the
loss of acknowledgments while ensuring a faster data availability to the
application compared to other traditional acknowledgment schemes. Finally, we
pave the first step of the integration of such algorithm inside a congestion
controlled protocol."
"There is a trend towards using wireless technologies in networked control
systems. However, the adverse properties of the radio channels make it
difficult to design and implement control systems in wireless environments. To
attack the uncertainty in available communication resources in wireless control
systems closed over WLAN, a cross-layer adaptive feedback scheduling (CLAFS)
scheme is developed, which takes advantage of the co-design of control and
wireless communications. By exploiting cross-layer design, CLAFS adjusts the
sampling periods of control systems at the application layer based on
information about deadline miss ratio and transmission rate from the physical
layer. Within the framework of feedback scheduling, the control performance is
maximized through controlling the deadline miss ratio. Key design parameters of
the feedback scheduler are adapted to dynamic changes in the channel condition.
An event-driven invocation mechanism for the feedback scheduler is also
developed. Simulation results show that the proposed approach is efficient in
dealing with channel capacity variations and noise interference, thus providing
an enabling technology for control over WLAN."
"This paper investigates the impact of carrier frequency offset (CFO) and
sampling frequency offset (SFO) on the performance of different MIMO-OFDM
schemes with high spectral efficiency for next generation of terrestrial
digital TV. We analyze particularly orthogonal Alamouti scheme, and
non-orthogonal (NO) schemes like VBLAST, linear dispersion (LD) code and Golden
code. This analysis gives a global view on the best suitable MIMO-OFDM scheme
with respect to CFO and SFO. We show that for high spectral efficiency,
Alamouti is more sensitive to CFO and SFO. Moreover, we show that all studied
MIMO-OFDM schemes are sensitive to CFO when it is greater than 1% of
inter-carrier spacing. Their sensitivity due to SFO is less than that due to
CFO."
"This article investigates the effect of equal and unequal received powers on
the performances of different MIMO-OFDM schemes for terrestrial digital TV.
More precisely, we focus on three types of non-orthogonal schemes: the BLAST
scheme, the Linear Dispersion (LD) code and the Golden code, and we compare
their performances to that of Alamouti scheme. Using two receiving antennas, we
show that for moderate attenuation on the second antenna and high spectral
efficiency, Golden code outperforms other schemes. However, Alamouti scheme
presents the best performance for low spectral efficiency and equal received
powers or when one antenna is dramatically damaged. When three antennas are
used, we show that Golden code offers the highest robustness to power unbalance
at the receiving side"
"This article introduces a 3D space-time-space block code for future
terrestrial digital TV in single frequency networks. The proposed 3D code is
based on a double layer structure designed for inter-cell and intra-cell space
time coded transmissions. We show that this new structure is particularly
efficient for SFN environments regardless of the location of the receiver. It
is then suitable for fixed, portable and mobile receptions."
"Bit error rate (BER) prediction over channel realisations has emerged as an
active research area. In this paper, we give analytical signal to interference
and noise ratio (SINR) evaluation of MIMO-OFDM systems using an iterative
receiver. Using this analytical SINR expression, we propose an accurate BER
prediction method based on effective exponential SINR mapping (EESM) method. We
show by simulations that our method is independent of the channel realisation
and of the MIMO scheme. It is only dependent on the modulation and coding
scheme."
"This paper investigates the impact of carrier frequency offset (CFO) on the
performance of different MIMO-OFDM schemes with high spectral efficiency for
next generation of terrestrial digital TV. We show that all studied MIMO-OFDM
schemes are sensitive to CFO when it is greater than 1% of inter-carrier
spacing. We show also that the Alamouti scheme is the most sensitive MIMO
scheme to CFO"
"This letter introduces a 3D space-time-space block code for future digital TV
systems. The code is based on a double layer structure for inter-cell and
intra-cell transmission mode in single frequency networks. Without increasing
the complexity of the receiver, the proposed code is very efficient for
different transmission scenarios."
"Orthogonal frequency-division multiplexing (OFDM) is the most popular
transmission technology in digital terrestrial broadcasting (DTTB), adopted by
many DTTB standards. In this paper, the bit error rate (BER) performance of two
DTTB systems, namely cyclic prefix OFDM (CP-OFDM) based DVB-T and time domain
synchronous OFDM (TDS-OFDM) based DTMB, is evaluated in different channel
conditions. Spectrum utilization and power efficiency are also discussed to
demonstrate the transmission overhead of both systems. Simulation results show
that the performances of the two systems are much close. Given the same ratio
of guard interval (GI), the DVB-T outperforms DTMB in terms of signal to noise
ratio (SNR) in Gaussian and Ricean channels, while DTMB behaves better
performance in Rayleigh channel in higher code rates and higher orders of
constellation thanks to its efficient channel coding and interleaving scheme."
"In this paper, we propose a novel channel estimation technique based on
spread pilots for digital video broadcasting. This technique consists in adding
a linear preceding function before the OFDM modulation and dedicating one of
the preceding sequence to transmit the pilot symbols for the channel
estimation. The merits of this technique are its simplicity, its flexibility,
and the gains in terms of spectral efficiency and useful bit rate obtained
compared to the classical pilot based estimation schemes used in DVB standards.
The performance evaluated over realistic channel models, shows the efficiency
of this technique which turns out to be a promising channel estimation
technique for the future terrestrial video broadcasting systems."
"In this paper, we propose a novel channel estimation technique based on 2D
spread pilots. The merits of this technique are its simplicity, its flexibility
regarding the transmission scenarios, and the spectral efficiency gain obtained
compared to the classical pilot based estimation schemes used in DVB standards.
We derive the analytical expression of the mean square error of the estimator
and show it is a function of the autocorrelation of the channel in both time
and frequency domains. The performance evaluated over a realistic channel model
shows the efficiency of this technique which turns out to be a promising
channel estimation for the future mobile video broadcasting systems."
"In wireless sensor networks, bandwidth is one of precious resources to
multimedia applications. To get more bandwidth, multipath routing is one
appropriate solution provided that inter-path interferences are minimized. In
this paper, we address the problem of interfering paths in the context of
wireless multimedia sensor networks and consider both intra-session as well as
inter-session interferences. Our main objective is to provide necessary
bandwidth to multimedia applications through non-interfering paths while
increasing the network lifetime. To do so, we adopt an incremental approach
where for a given session, only one path is built at once. Additional paths are
built when required, typically in case of congestion or bandwidth shortage.
Interference awareness and energy saving are achieved by switching a subset of
sensor nodes in a {\em passive state} in which they do not take part in the
routing process. Despite the routing overhead introduced by the incremental
approach we adopt, our simulations show that this can be compensated by the
overall achieved throughput and the amount of consumed energy per correctly
received packet especially for relatively long sessions such as multimedia
ones. This is mainly due to the fact that a small number of non-interfering
paths allows for better performances than a large number of interfering ones."
"Wireless sensor networks hold a great potential in the deployment of several
applications of a paramount importance in our daily life. Video sensors are
able to improve a number of these applications where new approaches adapted to
both wireless sensor networks and video transport specific characteristics are
required. The aim of this work is to provide the necessary bandwidth and to
alleviate the congestion problem to video streaming. In this paper, we
investigate various load repartition strategies for congestion control
mechanism on top of a multipath routing feature. Simulations are performed in
order to get insight into the performances of our proposals."
"Using time domain measurements, we assess the feasibility of time-reversal
technique in ultra-wideband (UWB) communication. A typical indoor propagation
channel is selected for the exploration. The channel response between receive
and transmit antenna pairs is measured using time domain equipments which
include an arbitrary wave generator (AWG) and a digital storage oscilloscope
(DSO). The time-reversed version of the channel response is constructed with
AWG and re-transmitted in the channel. The equivalent time reversed channel
response is recorded. The properties of the time reversal technique in the line
of sight (LOS) co-polar and cross-polar scenarios are measured."
"UWB communication is a recent research area for indoor propagation channels.
Time Reversal (TR) communication in UWB has shown promising results for
improving the system performance. In multiuser environment, the system
performance is significantly degraded due to the interference among different
users. TR reduces the interference caused by multiusers due to its spatial
focusing property. The performance of a multiuser TR communication system is
further improved if the TR filter is modified. In this paper, multiuser TR in
UWB communication is investigated using simple TR filter and a modified TR
filter with circular shift operation. The concept of circular shift in TR is
analytically studied. Thereafter, the channel impulse responses (CIR) of a
typical indoor laboratory environment are measured. The measured CIRs are used
to analyze the received signal peak power and signal to interference ratio
(SIR) with and without performing the circular shift operation in a multiuser
environment."
"Existing transport protocols, be it TCP, SCTP or DCCP, do not provide an
efficient congestion control mechanism for heterogeneous wired-cum-wireless
networks. Solutions involving implicit loss discrimination schemes have been
proposed but were never implemented. Appropriate mechanisms can dramatically
improve bandwidth usage over the Internet, especially for multimedia transport
based on partial reliability. In this paper we have implemented and evaluated a
congestion control mechanism that implicitly discriminates congestion and
wireless losses in the datagram congestion control protocol (DCCP) congestion
control identification (CCID) framework. The new CCID was implemented as a NS-2
module. Comparisons were made with the TCP-like CCID and showed that the
bandwidth utilization was improved by more than 30% and up to 50% in
significant setups."
"The sharing of network traces is an important prerequisite for the
development and evaluation of efficient anomaly detection mechanisms.
Unfortunately, privacy concerns and data protection laws prevent network
operators from sharing these data. Anonymization is a promising solution in
this context; however, it is unclear if the sanitization of data preserves the
traffic characteristics or introduces artifacts that may falsify traffic
analysis results. In this paper, we examine the utility of anonymized flow
traces for anomaly detection. We quantitatively evaluate the impact of IP
address anonymization, namely variations of permutation and truncation, on the
detectability of large-scale anomalies. Specifically, we analyze three weeks of
un-sampled and non-anonymized network traces from a medium-sized backbone
network. We find that all anonymization techniques, except prefix-preserving
permutation, degrade the utility of data for anomaly detection. We show that
the degree of degradation depends to a large extent on the nature and mix of
anomalies present in a trace. Moreover, we present a case study that
illustrates how traffic characteristics of individual hosts are distorted by
anonymization."
"A protocol named Threshold Bipolar (TB) is proposed as a fetching strategy at
the startup stage of p2p live streaming systems. In this protocol, chunks are
fetched consecutively from buffer head at the beginning. After the buffer is
filled into a threshold, chunks at the buffer tail will be fetched first while
keeping the contiguously filled part in the buffer above the threshold even
when the buffer is drained at a playback rate. High download rate, small
startup latency and natural strategy handover can be reached at the same time
by this protocol. Important parameters in this protocol are identified. The
buffer progress under this protocol is then expressed as piecewise lines
specified by those parameters. Startup traces of peers measured from PPLive are
studied to show the real performance of TB protocol in a real system. A simple
design model of TB protocol is proposed to reveal important considerations in a
practical design."
"In a radio network with single source-destination pair and some relays, a
link between any two nodes is considered to have same or zero path loss.
However in practice some links may have considerably high path loss than others
but still being useful. In this report, we take into account signals received
from these links also. \indent Our system model consists of a
source-destination pair with two layers of relays in which weaker links between
source and second layer and between the first layer and destination are also
considered. We propose some protocols in this system model, run simulations
under optimum power allocation, and compare these protocols. We show that under
reasonable channel strength of these weaker links, the proposed protocols
perform ($ \approx 2$ dB) better than the existing basic protocol. As expected,
the degree of improvement increases with the strength of the weaker links. We
also show that with the receive channel knowledge in relays, the reliability
and data rate are improved."
"Burst contention is a well known challenging problem in Optical Burst
Switching (OBS) networks. Deflection routing is used to resolve contention.
Burst retransmission is used to reduce the Burst Loss Ratio (BLR) by
retransmitting dropped bursts. Previous works show that combining deflection
and retransmission outperforms both pure deflection and pure retransmission
approaches. This paper proposes a new Adaptive Hybrid Deflection and
Retransmission (AHDR) approach that dynamically combines deflection and
retransmission approaches based on network conditions such as BLR and link
utilization. Network Simulator 2 (ns-2) is used to simulate the proposed
approach on different network topologies. Simulation results show that the
proposed approach outperforms static approaches in terms of BLR and goodput."
"In a two tier cellular network -- comprised of a central macrocell underlaid
with shorter range femtocell hotspots -- cross-tier interference limits overall
capacity with universal frequency reuse. To quantify near-far effects with
universal frequency reuse, this paper derives a fundamental relation providing
the largest feasible cellular Signal-to-Interference-Plus-Noise Ratio (SINR),
given any set of feasible femtocell SINRs. We provide a link budget analysis
which enables simple and accurate performance insights in a two-tier network. A
distributed utility-based SINR adaptation at femtocells is proposed in order to
alleviate cross-tier interference at the macrocell from cochannel femtocells.
The Foschini-Miljanic (FM) algorithm is a special case of the adaptation. Each
femtocell maximizes their individual utility consisting of a SINR based reward
less an incurred cost (interference to the macrocell). Numerical results show
greater than 30% improvement in mean femtocell SINRs relative to FM. In the
event that cross-tier interference prevents a cellular user from obtaining its
SINR target, an algorithm is proposed that reduces transmission powers of the
strongest femtocell interferers. The algorithm ensures that a cellular user
achieves its SINR target even with 100 femtocells/cell-site, and requires a
worst case SINR reduction of only 16% at femtocells. These results motivate
design of power control schemes requiring minimal network overhead in two-tier
networks with shared spectrum."
"Realistic mobility models are fundamental to evaluate the performance of
protocols in mobile ad hoc networks. Unfortunately, there are no mobility
models that capture the non-homogeneous behaviors in both space and time
commonly found in reality, while at the same time being easy to use and
analyze. Motivated by this, we propose a time-variant community mobility model,
referred to as the TVC model, which realistically captures spatial and temporal
correlations. We devise the communities that lead to skewed location visiting
preferences, and time periods that allow us to model time dependent behaviors
and periodic re-appearances of nodes at specific locations.
  To demonstrate the power and flexibility of the TVC model, we use it to
generate synthetic traces that match the characteristics of a number of
qualitatively different mobility traces, including wireless LAN traces,
vehicular mobility traces, and human encounter traces. More importantly, we
show that, despite the high level of realism achieved, our TVC model is still
theoretically tractable. To establish this, we derive a number of important
quantities related to protocol performance, such as the average node degree,
the hitting time, and the meeting time, and provide examples of how to utilize
this theory to guide design decisions in routing protocols."
"The lack of extensive research in the application of inexpensive wireless
sensor nodes for the early detection of wildfires motivated us to investigate
the cost of such a network. As a first step, in this paper we present several
results which relate the time to detection and the burned area to the number of
sensor nodes in the region which is protected. We prove that the probability
distribution of the burned area at the moment of detection is approximately
exponential, given that some hypotheses hold: the positions of the sensor nodes
are independent random variables uniformly distributed and the number of sensor
nodes is large. This conclusion depends neither on the number of ignition
points nor on the propagation model of the fire."
"We study sensor networks with energy harvesting nodes. The generated energy
at a node can be stored in a buffer. A sensor node periodically senses a random
field and generates a packet. These packets are stored in a queue and
transmitted using the energy available at that time at the node. For such
networks we develop efficient energy management policies. First, for a single
node, we obtain policies that are throughput optimal, i.e., the data queue
stays stable for the largest possible data rate. Next we obtain energy
management policies which minimize the mean delay in the queue. We also compare
performance of several easily implementable suboptimal policies. A greedy
policy is identified which, in low SNR regime, is throughput optimal and also
minimizes mean delay. Next using the results for a single node, we develop
efficient MAC policies."
"Both IEEE 802.15.4 and 802.15.4a standards allow for dynamic channel
allocation and use of multiple channels available at their physical layers but
its MAC protocols are designed only for single channel. Also, sensor's
transceivers such as CC2420 provide multiple channels and as shown in [1], [2]
and [3] channel switch latency of CC2420 transceiver is just about 200$\mu$s.
In order to enhance both energy efficiency and to shorten end to end delay, we
propose, in this report, a spectrum-efficient frequency allocation schemes that
are able to statically assign control channels and dynamically reuse data
channels for Personal Area Networks (PANs) inside a Large-Scale WSN based on
UWB technology."
"How can we protect the network infrastructure from malicious traffic, such as
scanning, malicious code propagation, and distributed denial-of-service (DDoS)
attacks? One mechanism for blocking malicious traffic is filtering: access
control lists (ACLs) can selectively block traffic based on fields of the IP
header. Filters (ACLs) are already available in the routers today but are a
scarce resource because they are stored in the expensive ternary content
addressable memory (TCAM).
  In this paper, we develop, for the first time, a framework for studying
filter selection as a resource allocation problem. Within this framework, we
study five practical cases of source address/prefix filtering, which correspond
to different attack scenarios and operator's policies. We show that filter
selection optimization leads to novel variations of the multidimensional
knapsack problem and we design optimal, yet computationally efficient,
algorithms to solve them. We also evaluate our approach using data from
Dshield.org and demonstrate that it brings significant benefits in practice.
Our set of algorithms is a building block that can be immediately used by
operators and manufacturers to block malicious traffic in a cost-efficient way."
"This paper proposes an analytical framework for peer-to-peer (P2P) networks
and introduces schemes for building P2P networks to approach the minimum
weighted average download time (WADT). In the considered P2P framework, the
server, which has the information of all the download bandwidths and upload
bandwidths of the peers, minimizes the weighted average download time by
determining the optimal transmission rate from the server to the peers and from
the peers to the other peers. This paper first defines the static P2P network,
the hierarchical P2P network and the strictly hierarchical P2P network. Any
static P2P network can be decomposed into an equivalent network of sub-peers
that is strictly hierarchical. This paper shows that convex optimization can
minimize the WADT for P2P networks by equivalently minimizing the WADT for
strictly hierarchical networks of sub-peers. This paper then gives an upper
bound for minimizing WADT by constructing a hierarchical P2P network, and lower
bound by weakening the constraints of the convex problem. Both the upper bound
and the lower bound are very tight. This paper also provides several suboptimal
solutions for minimizing the WADT for strictly hierarchical networks, in which
peer selection algorithms and chunk selection algorithm can be locally
designed."
"It has been known that heterogeneous networks are vulnerable to the
intentional removal of a small fraction of highly connected or loaded nodes,
which implies that, to protect a network effectively, a few important nodes
should be allocated with more defense resources than the others. However, if
too many resources are allocated to the few important nodes, the numerous
less-important nodes will be less protected, which, when attacked all together,
still capable of causing a devastating damage. A natural question therefore is
how to efficiently distribute the limited defense resources among the network
nodes such that the network damage is minimized whatever attack strategy the
attacker may take. In this paper, taking into account the factor of attack
cost, we will revisit the problem of network security and search for efficient
network defense against the cost-based attacks. The study shows that, for a
general complex network, there will exist an optimal distribution of the
defense resources, with which the network is well protected from cost-based
attacks. Furthermore, it is found that the configuration of the optimal defense
is dependent on the network parameters. Specifically, network that has a larger
size, sparser connection and more heterogeneous structure will be more
benefited from the defense optimization."
"The back-pressure algorithm is a well-known throughput-optimal algorithm.
However, its delay performance may be quite poor even when the traffic load is
not close to network capacity due to the following two reasons. First, each
node has to maintain a separate queue for each commodity in the network, and
only one queue is served at a time. Second, the back-pressure routing algorithm
may route some packets along very long routes. In this paper, we present
solutions to address both of the above issues, and hence, improve the delay
performance of the back-pressure algorithm. One of the suggested solutions also
decreases the complexity of the queueing data structures to be maintained at
each node."
"We consider a transmission of a delay-sensitive data stream from a single
source to a single destination. The reliability of this transmission may suffer
from bursty packet losses - the predominant type of failures in today's
Internet. An effective and well studied solution to this problem is to protect
the data by a Forward Error Correction (FEC) code and send the FEC packets over
multiple paths.
  In this paper we show that the performance of such a multipath FEC scheme can
often be further improved. Our key observation is that the propagation times on
the available paths often significantly differ, typically by 10-100ms.
  We propose to exploit these differences by appropriate packet scheduling that
we call `Spread'. We evaluate our solution with a precise, analytical
formulation and trace-driven simulations. Our studies show that Spread
substantially outperforms the state-of-the-art solutions. It typically achieves
two- to five-fold improvement (reduction) in the effective loss rate. Or
conversely, keeping the same level of effective loss rate, Spread significantly
decreases the observed delays and helps fighting the delay jitter."
"Burst contention is a well-known challenging problem in Optical Burst
Switching (OBS) networks. Deflection routing is used to resolve contention.
Burst retransmission is used to reduce the Burst Loss Ratio (BLR) by
retransmitting dropped bursts. Previous works show that combining deflection
and retransmission outperforms both pure deflection and pure retransmission
approaches. This paper proposes a new Adaptive Hybrid Deflection and
Retransmission (AHDR) approach that dynamically combines deflection and
retransmission approaches based on network conditions such as BLR and link
utilization. Network Simulator 2 (ns-2) is used to simulate the proposed
approach on different network topologies. Simulation results show that the
proposed approach outperforms static approaches in terms of BLR by using an
adaptive decision threshold."
"We propose a transceiver architecture for automatic beamforming and
instantaneous setup of a multigigabit-per-second wireless link between two
millimeter wave radios. The retro-directive architecture eliminates necessity
of slow and complex digital algorithms required for searching and tracking the
directions of opposite end radios. Simulations predict <5 micro-seconds setup
time for a 2-Gbps bidirectional 60-GHz communication link between two 10-meters
apart radios. The radios have 4-element arrayed antennas, and use QPSK
modulation with 1.5 GHz analog bandwidth."
"In this paper, we propose and evaluate a distributed protocol to manage trust
diffusion in ad hoc networks. In this protocol, each node i maintains a \trust
value"" about an other node j which is computed both as a result of the
exchanges with node j itself and as a function of the opinion that other nodes
have about j. These two aspects are respectively weighted by a trust index that
measures the trust quality the node has in its own experiences and by a trust
index representing the trust the node has in the opinions of the other nodes.
Simulations have been realized to validate the robustness of this protocol
against three kinds of attacks: simple coalitions, Trojan attacks and detonator
attacks."
"Mobile wireless network research focuses on scenarios at the extremes of the
network connectivity continuum where the probability of all nodes being
connected is either close to unity, assuming connected paths between all nodes
(mobile ad hoc networks), or it is close to zero, assuming no multi-hop paths
exist at all (delay-tolerant networks). In this paper, we argue that a sizable
fraction of networks lies between these extremes and is characterized by the
existence of partial paths, i.e. multi-hop path segments that allow forwarding
data closer to the destination even when no end-to-end path is available. A
fundamental issue in such networks is dealing with disruptions of end-to-end
paths. Under a stochastic model, we compare the performance of the established
end-to-end retransmission (ignoring partial paths), against a forwarding
mechanism that leverages partial paths to forward data closer to the
destination even during disruption periods. Perhaps surprisingly, the
alternative mechanism is not necessarily superior. However, under a stochastic
monotonicity condition between current v.s. future path length, which we
demonstrate to hold in typical network models, we manage to prove superiority
of the alternative mechanism in stochastic dominance terms. We believe that
this study could serve as a foundation to design more efficient data transfer
protocols for partially-connected networks, which could potentially help
reducing the gap between applications that can be supported over disconnected
networks and those requiring full connectivity."
"In this paper, we study the setting of carrier-sensing range in 802.11
networks under the (cumulative) physical interference model. Specifically, we
identify a carrier-sensing range that will prevent collisions in 802.11
networks due to carrier-sensing failure under the physical interference model.
We find that the carrier-sensing range required under the physical interference
model must be larger than that required under the protocol (pairwise)
interference model by a multiplicative factor. For example, if the SINR
requirement is 10dB and the path-loss exponent is 4, the factor is 1.4.
Furthermore, given a fixed pathloss exponent of 4, the factor increases as the
SINR requirement increases. However, the limit of the factor is 1.84 as the
SINR requirement goes to infinity."
"Minimizing handoff latency and achieving near-zero packet loss is critical
for delivering multimedia infotainment applications to fast-moving vehicles
that are likely to encounter frequent handoffs. In this paper, we propose a
dual-radio cross-layer handoff scheme for infrastructure-mode 802.11 Wireless
Networks that achieve this goal. We present performance results of an
implementation of our algorithm in a Linux-based On-Board-Unit prototype."
"We propose in this paper an on-line algorithm based on Bloom filters for
identifying large flows in IP traffic (a.k.a. elephants). Because of the large
number of small flows, hash tables of these algorithms have to be regularly
refreshed. Recognizing that the periodic erasure scheme usually used in the
technical literature turns out to be quite inefficient when using real traffic
traces over a long period of time, we introduce a simple adaptive scheme that
closely follows the variations of traffic. When tested against real traffic
traces, the proposed on-line algorithm performs well in the sense that the
detection ratio of long flows by the algorithm over a long time period is quite
high. Beyond the identification of elephants, this same class of algorithms is
applied to the closely related problem of detection of anomalies in IP traffic,
e.g., SYN flood due for instance to attacks. An algorithm for detecting SYN and
volume flood anomalies in Internet traffic is designed. Experiments show that
an anomaly is detected in less than one minute and the targeted destinations
are identified at the same time."
"This paper addresses the problem of reliable transmission of data through a
sensor network. We focus on networks rapidly deployed in harsh environments.
For these networks, important design requirements are fast data transmission
and rapid network setup, as well as minimized energy consumption for increased
network lifetime. We propose a novel broadcasting solution that accounts for
the interference impact and the congestion level of the channel, in order to
improve robustness, energy consumption and delay performance, compared to a
benchmark routing protocol, the GRAB algorithm. Three solutions are proposed:
P-GRAB, a probabilistic routing algorithm for interference mitigation, U-GRAB,
a utility-based algorithm that adjusts to real-time congestion and UP-GRAB, a
combination of P-GRAB and U-GRAB. It is shown that P-GRAB provides the best
performance for geometry-aware networks while the U-GRAB approach is the best
option for unreliable and unstable networks."
"This paper investigates the overload problem of a single congested router in
TCP (Transmission Control Protocol) networks. To cope with the congestion
phenomenon, we design a feedback control based on a multiple time-delays model
of the set TCP/AQM (Active Queue Management). Indeed, using robust control
tools, especially in the quadratic separation framework, the TCP/AQM model is
rewritten as an intercon- nected system and a structured state feedback is
constructed to stabilize the network variables. Finally, we illustrate the
proposed methodology with a numerical example and simulations using NS-2
simulator."
"Recent research has shown the link between congestion control in
communication networks and feedback control system. In this paper, the design
of an active queue management (AQM) which can be viewed as a controller, is
considered. Based on a state space representation of a linearized fluid flow
model of TCP, the AQM design is converted to a state feedback synthesis problem
for time delay systems. Finally, an example extracted from the literature and
simulations via a network simulator NS (under cross traffic conditions) support
our study."
"For the last few years, we assist to a growing interest of designing AQM
(Active Queue Management) using control theory. In this paper, we focus on the
synthesis of an AQM based on the Lyapunov theory for time delay systems. With
the help of a recently developed Lyapunov-Krasovskii functional and using a
state space representation of a linearized fluid model of TCP, two robust AQMs
stabilizing the TCP model are constructed. Notice that our results are
constructive and the synthesis problem is reduced to a convex optimization
scheme expressed in terms of linear matrix inequalities (LMIs). Finally, an
example extracted from the literature and simulations via {\it NS simulator}
support our study."
"Several studies have considered control theory tools for traffic control in
communication networks, as for example the congestion control issue in IP
(Internet Protocol) routers. In this paper, we propose to design a linear
observer for time-delay systems to address the traffic monitoring issue in
TCP/AQM (Transmission Control Protocol/Active Queue Management) networks. Due
to several propagation delays and the queueing delay, the set TCP/AQM is
modeled as a multiple delayed system of a particular form. Hence, appropriate
robust control tools as quadratic separation are adopted to construct a delay
dependent observer for TCP flows estimation. Note that, the developed mechanism
enables also the anomaly detection issue for a class of DoS (Denial of Service)
attacks. At last, simulations via the network simulator NS-2 and an emulation
experiment validate the proposed methodology."
"A new method of estimating some statistical characteristics of TCP flows in
the Internet is developed in this paper. For this purpose, a new set of random
variables (referred to as observables) is defined. When dealing with sampled
traffic, these observables can easily be computed from sampled data. By
adopting a convenient mouse/elephant dichotomy also dependent on traffic, it is
shown how these variables give a reliable statistical representation of the
number of packets transmitted by large flows during successive time intervals
with an appropriate duration. A mathematical framework is developed to estimate
the accuracy of the method. As an application, it is shown how one can estimate
the number of large TCP flows when only sampled traffic is available. The
algorithm proposed is tested against experimental data collected from different
types of IP networks."
"We present a novel geographical routing scheme for spontaneous wireless mesh
networks. Greedy geographical routing has many advantages, but suffers from
packet losses occurring at the border of voids. In this paper, we propose a
flexible greedy routing scheme that can be adapted to any variant of
geographical routing and works for any connectivity graph, not necessarily Unit
Disk Graphs. The idea is to reactively detect voids, backtrack packets, and
propagate information on blocked sectors to reduce packet loss. We also propose
an extrapolating algorithm to reduce the latency of void discovery and to limit
route stretch. Performance evaluation via simulation shows that our modified
greedy routing avoids most of packet losses."
"The stack in various forms has been widely used as an architectural template
for networking systems. Recently the stack has been subject to criticism for a
lack of flexibility. However, when it comes right down to it nobody has offered
a truly compelling alternative. Various cross-layer optimizations have been
proposed, but these optimizations are frequently hacks to achieve a particular
goal and offer no direct insight into why the existing network stack is
inadequate. We propose that a fundamental problem with the existing network
stack is that it attempts to layer functionality that is not well-suited to
layering. In this work we use a ""bottom up"" model of information computation,
storage, and transfer and the ""top down"" goals of networking systems to
formulate a modular decomposition of networking systems. Based on this modular
decomposition we propose a semantic layered structure for networking systems
that eliminates many awkward cross-layer interactions that arise in the
canonical layered stack."
"Energy consumption is the most challenging issue in routing protocol design
for mobile ad-hoc networks (MANETs), since mobile nodes are battery powered.
Furthermore, replacing or recharging batteries is often impossible in critical
environments such as in military or rescue missions. In a MANET, the energy
depletion of a node does not affect the node itself only, but the overall
network lifetime. In this paper, we present multipath and energy-aware on
demand source routing (MEA-DSR) protocol, which exploits route diversity and
information about batteries-energy levels for balancing energy consumption
between mobile nodes. Simulation results, have shown that MEA-DSR protocol is
more energy efficient than DSR in almost mobility scenarios."
"Mobile ad hoc networks (MANETs) consist of a collection of wireless mobile
nodes which dynamically exchange data without reliance on a fixed base station
or a wired backbone network, which makes routing a crucial issue for the design
of a ad hoc networks. In this paper we discussed a hybrid multipath routing
protocol named MP-OLSR. It is based on the link state algorithm and employs
periodic exchange of messages to maintain topology information of the networks.
In the mean time, it updates the routing table in an on-demand scheme and
forwards the packets in multiple paths which have been determined at the
source. If a link failure is detected, the algorithm recovers the route
automatically. Concerning the instability of the wireless networks, the
redundancy coding is used to improve the delivery ratio. The simulation in NS2
shows that the new protocol can effectively improve the performance of the
networks."
"In this paper we discussed the application and the implementation of
multipath routing and multiple description coding (MDC) extension of OLSR,
called MP-OLSR. It is based on the link state algorithm and employs periodic
exchange of messages to maintain topology information of the networks. In the
mean time, it updates the routing table in an on-demand scheme and forwards the
packets in multiple paths which have been determined at the source. If a link
failure is detected, the algorithm recovers the route automatically. Concerning
the instability of the wireless networks, the multiple description coding is
used to improve reliability of the network transmission, and several methods
are proposed to allocate the redundancy in different paths. The simulation in
NS2 shows that the new protocol can effectively improve the performance of the
networks. The implementation of MP-OLSR is also proposed in the end."
"A Mobile Ad-Hoc Network (MANET) is a collection of wireless mobile nodes
forming a temporary network without using any centralized access point,
infrastructure, or centralized administration. In this paper we introduce an
Energy Efficient Location Aided Routing (EELAR) Protocol for MANETs that is
based on the Location Aided Routing (LAR). EELAR makes significant reduction in
the energy consumption of the mobile nodes batteries by limiting the area of
discovering a new route to a smaller zone. Thus, control packets overhead is
significantly reduced. In EELAR a reference wireless base station is used and
the network's circular area centered at the base station is divided into six
equal sub-areas. At route discovery instead of flooding control packets to the
whole network area, they are flooded to only the sub-area of the destination
mobile node. The base station stores locations of the mobile nodes in a
position table. To show the efficiency of the proposed protocol we present
simulations using NS-2. Simulation results show that EELAR protocol makes an
improvement in control packet overhead and delivery ratio compared to AODV,
LAR, and DSR protocols."
"Our objective is to provide guaranteed packet delivery service in time
constrained sensor networks. The wireless network is a highly variable
environment, where available link bandwidth may vary with network load. Since
multimedia applications require higher bandwidth so we use FSO links for their
transmission. The main advantage of FSO links is that they offer higher
bandwidth and security, while RF links offer more reliability. The routing in
this multitier network is based on directional geographic routing protocol, in
which sensors route their data via multihop paths, to a powerful base station,
through a cluster head. Some modifications have also been incorporated in the
MAC layer to improve the QoS of such systems."
"The growth of real-time content streaming over the Internet has resulted in
the use of peer-to-peer (P2P) approaches for scalable content delivery. In such
P2P streaming systems, each peer maintains a playout buffer of content chunks
which it attempts to fill by contacting other peers in the network. The
objective is to ensure that the chunk to be played out is available with high
probability while keeping the buffer size small. Given that a particular peer
has been selected, a \emph{policy} is a rule that suggests which chunks should
be requested by the peer from other peers.. We consider consider a number of
recently suggested policies consistent with buffer minimization for a given
target of skip free playout. We first study a \emph{rarest-first} policy that
attempts to obtain chunks farthest from playout, and a \emph{greedy} policy
that attempts to obtain chunks nearest to playout. We show that they both have
similar buffer scalings (as a function of the number of peers of target
probability of skip-free probability). We then study a hybrid policy which
achieves order sense improvements over both policies and can achieve order
optimal performance. We validate our results using simulations."
"GSM networks are very expensive. The network design process requires too many
decisions in a combinatorial explosion. For this reason, the larger is the
network, the harder is to achieve a totally human based optimized solution. The
BSC (Base Station Control) nodes have to be geographically well allocated to
reduce the transmission costs. There are decisions of association between BTS
and BSC those impacts in the correct dimensioning of these BSC. The choice of
BSC quantity and model capable of carrying the cumulated traffic of its
affiliated BTS nodes in turn reflects on the total cost. In addition, the last
component of the total cost is due to transmission for linking BSC nodes to
MSC. These trunks have a major significance since the number of required E1
lines is larger than BTS to BSC link. This work presents an integer programming
model and a computational tool for designing GSM (Global System for Mobile
Communications) networks, regarding BSS (Base Station Subsystem) with optimized
cost."
"Timer-based mechanisms are often used to help a given (sink) node select the
best helper node among many available nodes. Specifically, a node transmits a
packet when its timer expires, and the timer value is a monotone non-increasing
function of its local suitability metric. The best node is selected
successfully if no other node's timer expires within a 'vulnerability' window
after its timer expiry, and so long as the sink can hear the available nodes.
In this paper, we show that the optimal metric-to-timer mapping that (i)
maximizes the probability of success or (ii) minimizes the average selection
time subject to a minimum constraint on the probability of success, maps the
metric into a set of discrete timer values. We specify, in closed-form, the
optimal scheme as a function of the maximum selection duration, the
vulnerability window, and the number of nodes. An asymptotic characterization
of the optimal scheme turns out to be elegant and insightful. For any
probability distribution function of the metric, the optimal scheme is
scalable, distributed, and performs much better than the popular inverse metric
timer mapping. It even compares favorably with splitting-based selection, when
the latter's feedback overhead is accounted for."
"Optimal dissemination schemes have previously been studied for peer-to-peer
live streaming applications. Live streaming being a delay-sensitive
application, fine tuning of dissemination parameters is crucial. In this
report, we investigate optimal sizing of chunks, the units of data exchange,
and probe sets, the number peers a given node probes before transmitting
chunks. Chunk size can have significant impact on diffusion rate (chunk miss
ratio), diffusion delay, and overhead. The size of the probe set can also
affect these metrics, primarily through the choices available for chunk
dissemination. We perform extensive simulations on the so-called random-peer,
latest-useful dissemination scheme. Our results show that size does matter,
with the optimal size being not too small in both cases."
"Epidemic-style diffusion schemes have been previously proposed for achieving
peer-to-peer live streaming. Their performance trade-offs have been deeply
analyzed for homogeneous systems, where all peers have the same upload
capacity. However, epidemic schemes designed for heterogeneous systems have not
been completely understood yet. In this report we focus on the peer selection
process and propose a generic model that encompasses a large class of
algorithms. The process is modeled as a combination of two functions, an aware
one and an agnostic one. By means of simulations, we analyze the
awareness-agnostism trade-offs on the peer selection process and the impact of
the source distribution policy in non-homogeneous networks. We highlight that
the early diffusion of a given chunk is crucial for its overall diffusion
performance, and a fairness trade-off arises between the performance of
heterogeneous peers, as a function of the level of awareness."
"Distributed live streaming has brought a lot of interest in the past few
years. In the homogeneous case (all nodes having the same capacity), many
algorithms have been proposed, which have been proven almost optimal or
optimal. On the other hand, the performance of heterogeneous systems is not
completely understood yet. In this paper, we investigate the impact of
heterogeneity on the achievable delay of chunk-based live streaming systems. We
propose several models for taking the atomicity of a chunk into account. For
all these models, when considering the transmission of a single chunk,
heterogeneity is indeed a ``blessing'', in the sense that the achievable delay
is always faster than an equivalent homogeneous system. But for a stream of
chunks, we show that it can be a ``curse'': there is systems where the
achievable delay can be arbitrary greater compared to equivalent homogeneous
systems. However, if the system is slightly bandwidth-overprovisioned, optimal
single chunk diffusion schemes can be adapted to a stream of chunks, leading to
near-optimal, faster than homogeneous systems, heterogeneous live streaming
systems."
"Connectivity patterns in intermittently-connected mobile networks (ICMN) can
be modeled as edge-Markovian dynamic graphs. We propose a new model for
epidemic propagation on such graphs and calculate a closed-form expression that
links the best achievable delivery ratio to common ICMN parameters such as
message size, maximum tolerated delay, and link lifetime. These theoretical
results are compared to those obtained by replaying a real-life contact trace."
"Structure pathology detection is an important security task in building
construction, which is performed by an operator by looking manually for damages
on the materials. This activity could be dangerous if the structure is hidden
or difficult to reach. On the other hand, embedded devices and wireless sensor
networks (WSN) are becoming popular and cheap, enabling the design of an
alternative pathology detection system to monitor structures based on these
technologies. This article introduces a ZigBee WSN system, intending to be
autonomous, easy to use and with low power consumption. Its functional parts
are fully discussed with diagrams, as well as the protocol used to collect
samples from sensor nodes. Finally, several tests focused on range and power
consumption of our prototype are shown, analysing whether the results obtained
were as expected or not."
"The democratization of wireless networks combined to the emergence of mobile
devices increasingly autonomous and efficient lead to new services. Positioning
services become overcrowded. Accuracy is the main quality criteria in
positioning. But to better appreciate this one a coefficient is needed. In this
paper we present Geometric and Signal Strength Dilution of Precision (DOP) for
positioning systems based on Wi-Fi and Signal Strength measurements."
"Emergence of online content voting networks allows users to share and rate
content including social news, photos and videos. The basic idea behind online
content voting networks is that aggregate user activities (e.g., submitting and
rating content) makes high-quality content thrive through the unprecedented
scale, high dynamics and divergent quality of user generated content (UGC). To
better understand the nature and impact of online content voting networks, we
have analyzed Digg, a popular online social news aggregator and rating website.
Based on a large amount of data collected, we provide an in-depth study of
Digg. In particular, we study structural properties of Digg social network,
impact of social network properties on user digging activities and vice versa,
distribution of user diggs, content promotion, and information filtering. We
also provide insight into design of content promotion algorithms and
recommendation-assisted content discovery. Overall, we believe that the results
presented in this paper are crucial in understanding online content rating
networks."
"A cross-layer scheme, namely ALOHA With Collision Resolution (ALOHA-CR), is
proposed for high throughput wireless communications in a cellular scenario.
Transmissions occur in a time-slotted ALOHA-type fashion but with an important
difference: simultaneous transmissions of two users can be successful. If more
than two users transmit in the same slot the collision cannot be resolved and
retransmission is required. If only one user transmits, the transmitted packet
is recovered with some probability, depending on the state of the channel. If
two users transmit the collision is resolved and the packets are recovered by
first over-sampling the collision signal and then exploiting independent
information about the two users that is contained in the signal polyphase
components. The ALOHA-CR throughput is derived under the infinite backlog
assumption and also under the assumption of finite backlog. The contention
probability is determined under these two assumptions in order to maximize the
network throughput and maintain stability. Queuing delay analysis for network
users is also conducted. The performance of ALOHA-CR is demonstrated on the
Wireless Open Access Research Platform (WARP) test-bed containing five software
defined radio nodes. Analysis and test-bed results indicate that ALOHA-CR leads
to significant increase in throughput and reduction of service delays."
"Mobile IPv6 will be an integral part of the next generation Internet
protocol. The importance of mobility in the Internet gets keep on increasing.
Current specification of Mobile IPv6 does not provide proper support for
reliability in the mobile network and there are other problems associated with
it. In this paper, we propose Virtual Private Network (VPN) based Home Agent
Reliability Protocol (VHAHA) as a complete system architecture and extension to
Mobile IPv6 that supports reliability and offers solutions to the security
problems that are found in Mobile IP registration part. The key features of
this protocol over other protocols are: better survivability, transparent
failure detection and recovery, reduced complexity of the system and workload,
secure data transfer and improved overall performance."
"Accurate and fast packet delivery rate (PDR) estimation, used in evaluating
wireless link quality, is a prerequisite to increase the performance of mobile,
multi-hop and multi-rate wireless ad hoc networks. Unfortunately, contemporary
PDR estimation methods, i.e. beacon-based packet counting in Estimated
Transmission Time and Expected Transmission Count metrics, have unsatisfactory
performance. Therefore, in this paper we propose a novel PDR estimation method
based on SNR profiles. We classify all possible link quality estimation methods
and compare them analytically against our design. Results show that it leads to
a more efficient link quality estimation. Further investigations with the
prototype implementation of our method in IEEE 802.11b/g testbeds reveal that
the accuracy of PDR estimation in mobile scenarios can be improved up to 50% in
comparison to generic packet-based PDR. Experiments with the same prototype on
link and routing layers for different measurement scenarios show that it leads
to a better rate adaptation and route selection in the form of end-to-end
throughput increase compared to traditional packet counting methods."
"TBecause of limited energy of nodes, an important issue for sensor network is
efficient use of the energy. The clustering technique reduces energy
consumption as cluster head sends sensed information to a sink node. Because of
such character of clustering technique, electing cluster head is an important
element for networks. This paper proposes RCFT (ReClustering Formation
Technique) that reconstruct clusters in hierarchical sensor networks. RCFT is a
protocol that reconstructed clusters considering position of a cluster head and
nodes in randomly constructed clusters. And this paper demonstrated that
clusters are composed evenly through simulation, accordingly this simulation
shows the result reducing energy consumption."
"What is the impact of obstacles on the graphs of connections between stations
in Mobile Ad hoc Networks? In order to answer, at least partially, this
question, the first step is to define both an environment with obstacles and a
mobility model for the stations in such an environment. The present paper
focuses on a new way of considering the mobility within environments with
obstacles, while keeping the core ideas of the well-known Random WayPoint
mobility model (a.k.a RWP). Based on a mesh-partitioning of the space, we
propose a new model called RSP-O-G for which we compute the spatial
distribution of stations and analyse how the presence of obstacles impacts this
distribution compared to the distribution when no obstacles are present.
Coupled with a simple model of radio propagation, and according to the density
of stations in the environment, we study the mean degree of the connection
graphs corresponding to such mobile ad hoc networks."
"The evolution of the Internet during the last years, has lead to a dramatic
increase of the size of its graph at the Autonomous System (AS) level. Soon -
if not already - its size will make the latter impractical for use from the
research community, e.g. for protocol testing. Reproducing a smaller size,
snapshot of the AS graph is thus important. However, the first step towards
this direction is to obtain the ability to faithfully reproduce the full AS
topology. The objective of our work, is to create a generator able to
accurately emulate and reproduce the distinctive properties of the Internet
graph. Our approach is based on (a) the identification of the jellyfish-like
structure [1] of the Internet and (b) the consideration of the peer-to-peer and
customer-provider relations between ASs. We are the first to exploit the
distinctive structure of the Internet graph together with utilizing the
information provided by the AS relationships in order to create a tool with the
aforementioned capabilities. Comparing our generator with the existing ones in
the literature, the main difference is found on the fact that our tool does not
try to satisfy specific metrics, but tries to remain faithful to the conceptual
model of the Internet structure. In addition, our approach can lead to (i) the
identification of important attributes and patterns in the Internet AS
topology, as well as, (ii) the extraction of valuable information on the
various relationships between ASs and their effect on the formulation of the
Internet structure. We implement our graph generator and we evaluate it using
the largest and most recent available dataset for the AS topology. Our
evaluations, clearly show the ability of our tool to capture the structural
properties of the Internet topology at the AS level with high accuracy."
"This paper discusses the advent of new technologies which have emerged under
the area of Location Based Services (LBS). An innovative implementation and
approach has been presented for design of applications which are inventive and
attractive towards the user. Spatial Trigger is one of the most promising
additions to the LBS technologies. This paper describes ways in which mobile
advertisement services can be introduced effectively in the cellular market by
bringing innovation in them through effective usage of Spatial Triggers. Hence,
opening new horizons to make the consumer cellular networks, commercially, more
effective and informative."
"This paper presents the design and the realization of a hybrid wireless
Gigabit Ethernet indoor communications system operating at 60 GHz. As the 60
GHz radio link operates only in a single-room configuration, an additional
Radio over Fiber (RoF) link is used to ensure the communications within all the
rooms of a residential environment. The system uses low complexity baseband
processing modules. A byte synchronization technique is designed to provide a
high value of the preamble detection probability and a very small value of the
false detection probability. Conventional RS (255, 239) encoder and decoder are
used for channel forward error correction (FEC). The FEC parameters are
determined by the tradeoff between higher coding gain and hardware complexity.
The results of bit error rate measurements at 875 Mbps are presented for
various antennas configurations."
"This paper presents the study and the realization of a hybrid 60 GHz wireless
communications system. As the 60 GHz radio link operates only in a single-room
configuration, an additional Radio over Fibre (RoF) link is used to ensure the
communications in all the rooms of a residential environment. A single carrier
architecture is adopted. The system uses low complexity baseband processing
modules. A byte/frame synchronization technique is designed to provide a high
value of the preamble detection probability and a very small value of the false
alarm probability. Conventional RS (255, 239) encoder and decoder are used to
correct errors in the transmission channel. Results of Bit Error Rate (BER)
measurements are presented for various antennas configurations."
"This paper presents the study and the realization at IETR of a high data rate
60 GHz wireless communications system. The system uses a simple single carrier
architecture. The receiver architecture is based on a differential demodulation
which minimizes the intersymbol interference (ISI) effect and a signal
processing unit composed of a joint frame and byte synchronization block and a
conventional RS (255, 239) decoder. The byte synchronization technique provides
a high preamble detection probability and a very small value of the false
detection probability. First measurement results show a good communication link
quality in line of sight environments with directional antennas."
"This report considers the class of applications of sensor networks in which
each sensor node makes measurements, such as temperature or humidity, at the
precise location of the node. Such spot-sensing applications approximate the
physical condition of the entire region of interest by the measurements made at
only the points where the sensor nodes are located. Given a certain density of
nodes in a region, a more spatially uniform distribution of the nodes leads to
a better approximation of the physical condition of the region. This report
considers the error in this approximation and seeks to improve the quality of
representation of the physical condition of the points in the region in the
data collected by the sensor network. We develop two essential metrics which
together allow a rigorous quantitative assessment of the quality of
representation achieved: the average representation error and the unevenness of
representation error, the latter based on a well-accepted measure of inequality
used in economics. We present the rationale behind the use of these metrics and
derive relevant theoretical bounds on them in the common scenario of a planar
region of arbitrary shape covered by a sensor network deployment. A simple new
heuristic algorithm is presented for each node to determine if and when it
should sense or sleep to conserve energy while also preserving the quality of
representation. Simulation results show that it achieves a significant
improvement in the quality of representation compared to other related
distributed algorithms. Interestingly, our results also show that improved
spatial uniformity has the welcome side-effect of a significant increase in the
network lifetime."
"In this paper, we analyze a round-based pricing scheme that encourages
favorable behavior from users of real-time P2P applications like P2PTV. In the
design of pricing schemes, we consider price to be a function of usage and
capacity of download/upload streams, and quality of content served. Users are
consumers and servers at the same time in such networks, and often exhibit
behavior that is unfavorable towards maximization of social benefits.
Traditionally, network designers have overcome this difficulty by building-in
traffic latencies. However, using simulations, we show that appropriate pricing
schemes and usage terms can enable designers to limit required traffic
latencies, and be able to earn nearly 30% extra revenue from providing P2PTV
services. The service provider adjusts the prices of individual programs
incrementally within rounds, while making relatively large-scale adjustments at
the end of each round. Through simulations, we show that it is most beneficial
for the service provider to carry out 5 such rounds of price adjustments for
maximizing his average profit and minimizing the associated standard deviation
at the same time."
"The last few decades have seen considerable research progress in
microelectronics and integrated circuits, system-on-chip design, wireless
communication, and sensor technology. This progress has enabled the seamless
integration of autonomous wireless sensor nodes around a human body to create a
Body Sensor Network (BSN). The development of a proactive and ambulatory BSN
induces a number of enormous issues and challenges. This paper presents the
technical hurdles during the design and implementation of a low-power Medium
Access Control (MAC) protocol for in-body and on-body sensor networks. We
analyze the performance of IEEE 802.15.4 protocol for the on-body sensor
network. We also provide a comprehensive insight into the heterogeneous
characteristics of the in-body sensor network. A low-power technique called
Pattern-Based Wake-up Table is proposed to handle the normal traffic in a BSN.
The proposed technique provides a reliable solution towards low-power
communication in the in-body sensor network."
"Wireless relaying is one of the promising solutions to overcome the channel
impairments and provide high data rate coverage that appears for beyond 3G
mobile communications. In this paper we present an end to end BER performance
analysis of dual hop wireless communication systems equipped with multiple
decode and forward relays over the Rayleigh fading channel with relay
selection. We select the best relay based on end to end channel conditions. We
apply orthogonal space time block coding (OSTBC) at source, and also present
how the multiple antennas at the source terminal affects the end to end BER
performance. This intermediate relay technique will cover long distance where
destination is out of reach from source."
"Current advances in wireless communication, microelectronics, semiconductor
technologies, and intelligent sensors have contributed to the development of
unobtrusive WBANs. These networks provide long term health monitoring of
patients without any constraint in their normal activities. Traditional MAC
protocols do not accommodate the assorted WBAN traffic requirements in a power
efficient manner. In this paper, we present a brief discussion on the
development process of a low power MAC protocol for WBANs. We observe the
behavior of a beacon-enabled IEEE 802.15.4 for on-body sensor networks. We
further propose a low power technique called traffic based wakeup mechanism for
a WBAN that exploits the traffic patterns of the BAN Nodes to ensure power
efficient and reliable communication."
"The advent of fourth generation technologies in wireless networks and the
rapid growth of 3G have heralded an era that will require researchers to find
reliable and easily implement-able solutions to the problem of poor TCP
performance in the wireless environment. Since a large part of the Internet is
TCP-based, solving this problem will be instrumental in determining if the move
from wired to wireless will be seamless or not. This paper proposes a scheme
that uses the base station's ability to predict the time at which the link may
be going down and to estimate the period for which the mobile would be
unreachable due to conditions like fading. By using cross-layer and ACK pacing
algorithms, the base station prevents the fixed host from timing out while
waiting for ACKs from the mobile. This in turn prevents TCP on the fixed host
from bringing down the throughput drastically due to temporary network
conditions, caused by mobility or the unreliability of wireless links.
Experimental results indicate a reasonable increase in throughput when the ACK
holding scheme is used."
"The dual-cross scenario of the hybrid wireless sensor networks (WSNs) is
studied and a novel MIMO Cluster Cooperative Assignment Cross Layer Scheduling
Scheme (MCCA-CLSS) is proposed in this paper. The comparison and the
predominance of the proposed scheme are demonstrated, the clusters are
optimized. With the help of the simulations, the relative energy consumption
and the end-to-end blocking probability are all improved. The addressing ratio
of success in the condition of the unchanged parameters and external
information can be increased and the network can tolerate more hops to support
reliable transportation by the proposed scheme."
"A special scenario of the topology in the hybrid Cognitive Ad-hoc networks is
studied and a novel cross layer scheme is proposed in this paper. The proposed
scheme integrated the attributes both of the new performance evaluation machine
check time metric and the topology space in special scenario. The topology and
power consumption of each node can all be optimized due to the minimum link
occupation with the help of this scheme. Simulation results show that the novel
scheme can give schedule guarantee to the multi-channel networks in the
variable node loads and transmission powers, and make the node stable to
support multi-hops at the same time."
"This paper presents an empirical discussion on the design and implementation
of a power-efficient Medium Access Control (MAC) protocol for in-body and
on-body sensor networks. We analyze the performance of a beacon-enabled IEEE
802.15.4, PB-TDMA, and S-MAC protocols for on-body sensor networks. We further
present a Traffic Based Wakeup Mechanism that utilizes the traffic patterns of
the BAN Nodes (BNs) to accommodate the entire BSN traffic. To enable a logical
connection between different BNs working on different frequency bands, a method
called Bridging function is proposed. The Bridging function integrates all BNs
working on different bands into a complete BSN."
"Recent advances in intelligent sensors, microelectronics and integrated
circuit, system-on-chip design and low power wireless communication introduced
the development of miniaturised and autonomous sensor nodes. These tiny sensor
nodes can be deployed to develop a proactive Body Sensor Network (BSN). The
rapid advancement in ultra low-power RF (radio frequency) technology enables
invasive and non-invasive devices to communicate with a remote station. This
communication revolutionizes healthcare system by enabling long term health
monitoring of a patient and providing real time feedback to the medical
experts. In this paper, we present In-body and On-body communication networks
with a special focus on the methodologies of wireless communication between
implanted medical devices with external monitoring equipment and recent
technological growth in both areas. We also discuss open issues and challenges
in a BSN."
"Recent advances in wideband impulse technology, low power communication along
with unlicensed band have enabled ultra wide band (UWB) as a leading technology
for future wireless applications. This paper outlines the applications of
emerging UWB technology in a private and commercial sector. We further talk
about UWB technology for a wireless body area network (WBAN)."
"An ""RF sensor"" network can monitor RSS values on links in the network and
perform device-free localization, i.e., locating a person or object moving in
the area in which the network is deployed. This paper provides a statistical
model for the RSS variance as a function of the person's position w.r.t. the
transmitter (TX) and receiver (RX). We show that the ensemble mean of the RSS
variance has an approximately linear relationship with the expected total
affected power (ETAP). We then use analysis to derive approximate expressions
for the ETAP as a function of the person's position, for both scattering and
reflection. Counterintuitively, we show that reflection, not scattering, causes
the RSS variance contours to be shaped like Cassini ovals. Experimental tests
reported here and in past literature are shown to validate the analysis."
"As the Internet becomes severely overburdened with exponentially growing
traffic demand, it becomes a general belief that a new generation data network
is in urgent need today. However, standing at this crossroad, we find that we
are in a situation that lacks a theory of network designing. This issue becomes
even more serious as the recent progress of network measurement and modeling
challenges the foundation of network research in the past decades.
  This paper tries to set up a scientific foundation for network designing by
formalizing it as a multi-objective optimization process and quantifying the
way different designing choices independently and collectively influence these
objectives. A cartesian coordinate system is introduced to map the effect of
each designing scheme to a coordinate. We investigated the achievable area of
the network designing space and proved some boundary conditions. It is shown
that different kind of networks display different shapes of achievable areas in
the cartesian coordinate and exhibit different abilities to achieve
cost-effective and scalable designing. In particular, we found that the
philosophy underlying current empirical network designing and engineering fails
to meet the cost-effective and evolvable requirements of network designing. We
demonstrated that the efficient routing combined with effective betweenness
based link bandwidth allocation scheme is a cost-effective and scalable design
for BA-like scale-free networks, whereas if other designing choices cannot be
determined beforehand, ER network is a markedly good candidate for
cost-effective and scalable design."
"Many Web-based data sources and services are available as feeds, a model that
provides consumers with a loosely coupled way of interacting with providers.
The current feed model is limited in its capabilities, however. Though it is
simple to implement and scales well, it cannot be transferred to a wider range
of application scenarios. This paper conceptualizes feeds as a way to serialize
query results, describes the current hardcoded query semantics of such a
perspective, and surveys the ways in which extensions of this hardcoded model
have been proposed or implemented. Our generalized view of feeds as query
result serializations has implications for the applicability of feeds as a
generic Web service for any collection that is providing access to individual
information items. As one interesting and compelling class of applications, we
describe a simple way in which a query-based approach to feeds can be used to
support location-based services."
"This paper proposes a mathematical justification of the phenomenon of extreme
congestion at a very limited number of nodes in very large networks. It is
argued that this phenomenon occurs as a combination of the negative curvature
property of the network together with minimum length routing. More
specifically, it is shown that, in a large n-dimensional hyperbolic ball B of
radius R viewed as a roughly similar model of a Gromov hyperbolic network, the
proportion of traffic paths transiting through a small ball near the center is
independent of the radius R whereas, in a Euclidean ball, the same proportion
scales as 1/R^{n-1}. This discrepancy persists for the traffic load, which at
the center of the hyperbolic ball scales as the square of the volume, whereas
the same traffic load scales as the volume to the power (n+1)/n in the
Euclidean ball. This provides a theoretical justification of the experimental
exponent discrepancy observed by Narayan and Saniee between traffic loads in
Gromov-hyperbolic networks from the Rocketfuel data base and synthetic
Euclidean lattice networks. It is further conjectured that for networks that do
not enjoy the obvious symmetry of hyperbolic and Euclidean balls, the point of
maximum traffic is near the center of mass of the network."
"This paper aims to identify the operational region of a link in terms of its
utilization and alert operators at the point where the link becomes overloaded
and requires a capacity upgrade. The number of active flows is considered the
real network state and is proposed to use a proxy for utilization. The Gaussian
approximation gives the expression for the confidence interval on an
operational region. The easy rule has been formulated to display the network
defects by means of measurements of router loading and number of active flows.
Mean flow performance is considered as the basic universal index characterized
quality of network services provided to single user."
"We propose an architecture, Flare, that is a structured and easy way to
develop applications rapidly, in a multitude of languages, which make use of
online storage of data and management of users. The architecture eliminates the
need for server-side programming in most cases, creation and management of
online database storage servers, re-creation of user management schemes and
writing a lot of unnecessary code for accessing different web-based services
using their APIs. A Web API provides a common API for various web-based
services like Blogger [2], Wordpress, MSN Live, Facebook [3] etc. Access
Libraries provided for major programming languages and platforms make it easy
to develop applications using the Flare Web Service. We demonstrate a simple
micro-blogging service developed using these APIs in two modes: a graphical
browser-based mode, and a command-line mode in C++, which provide two different
interfaces to the same account and data."
"Packet-dispersion based measurement tools insert pairs of probe packets with
a known separation into the network for transmission over a unicast path or a
multicast tree. Samples of the separation between the probe pairs at the
destination(s) are observed. Heuristic techniques are then used by these tools
to estimate the path characteristics from the observations. In this paper we
present a queueing theoretic setting for packet-dispersion based probing.
Analogous to network tomography, we develop techniques to estimate the
parameters of the arrival process to the individual links from the samples of
the output separations, i.e., from the end-to-end measurements. The links are
modeled as independent discrete time queues with i.i.d. arrivals. We first
obtain an algorithm to obtain the (joint) distribution of the separation
between the probes at the destination(s) for a given distribution of the
spacing at the input. The parameter estimates of the arrival process are
obtained as the minimizer of a cost function between the empirical and
calculated distributions. We also carry out extensive simulations and numerical
experiments to study the performance of the estimation algorithm under the
fairly `harsh' conditions of non stationarity of the arrival process. We find
that the estimations work fairly well for two queues in series and for
multicast."
"The goal of congestion control is to avoid congestion in network elements. A
network element is congested if it is being offered more traffic than it can
process. To detect such situations and to neutralize them we should monitor
traffic in the network. In this paper, we propose using Cisco's NetFlow
technology, which allows collecting statistics about traffic in the network by
generating special NetFlow packets. Cisco's routers can send NetFlow packets to
a special node, so we can collect these packets, analyze its content and detect
network congestion. We use Cisco's feature as example, some other vendors
(Juniper, 3COM, Alcatel, etc.) provide similar features for their routers. We
also consider a simple system, which collects statistical information about
network elements, determines overloaded elements and identifies flows, which
congest them."
"Due to the advancement of computing and communication technology, networked
control systems may soon become prevalent in many control applications. While
the capability of employing the communication network in the control loop
certainly provides many benefits, it also raises several challenges which need
to be overcome to utilize the benefits.
  In this chapter, we focus on one major challenge: a middleware framework that
enables a networked control system to be implemented. Indeed our thesis is that
a middleware for networked control sys important for the future of networked
control systems.
  We discuss the fundamental issues which need to be considered in the design
and development of an appropriate middleware for networked control systems. We
describe \emph{Etherware}, a middleware for networked control system which has
been developed at the University of Illinois, as an example of such a
middleware framework, to illustrate how these issues can be addressed in the
design of a middleware. Using a networked inverted pendulum control system as
an example, we demonstrate the powerful capabilities provided by Etherware for
a networked control system."
"Sensor networks are particularly applicable to the tracking of objects in
motion. For such applications, it may not necessary that the whole region be
covered by sensors as long as the uncovered region is not too large. This
notion has been formalized by Balasubramanian et.al. as the problem of
$\kappa$-weak coverage. This model of coverage provides guarantees about the
regions in which the objects may move undetected. In this paper, we analyse the
theoretical aspects of the problem and provide guarantees about the lifetime
achievable. We introduce a number of practical algorithms and analyse their
significance. The main contribution is a novel linear programming based
algorithm which provides near-optimal lifetime. Through extensive
experimentation, we analyse the performance of these algorithms based on
several parameters defined."
"Relay selection for cooperative communications promises significant
performance improvements, and is, therefore, attracting considerable attention.
While several criteria have been proposed for selecting one or more relays,
distributed mechanisms that perform the selection have received relatively less
attention. In this paper, we develop a novel, yet simple, asymptotic analysis
of a splitting-based multiple access selection algorithm to find the single
best relay. The analysis leads to simpler and alternate expressions for the
average number of slots required to find the best user. By introducing a new
`contention load' parameter, the analysis shows that the parameter settings
used in the existing literature can be improved upon. New and simple bounds are
also derived. Furthermore, we propose a new algorithm that addresses the
general problem of selecting the best $Q \ge 1$ relays, and analyze and
optimize it. Even for a large number of relays, the algorithm selects the best
two relays within 4.406 slots and the best three within 6.491 slots, on
average. We also propose a new and simple scheme for the practically relevant
case of discrete metrics. Altogether, our results develop a unifying
perspective about the general problem of distributed selection in cooperative
systems and several other multi-node systems."
"Energy efficiency is a corner stone of sustainability in data center and
high-performance networking. However, at present there is a notable structural
mismatch between network silicon development targets and network equipment
utilization patterns in the field. In particular, some aspects of network
energy utilization (eg load-proportional energy consumption) routinely stay out
of focus during system design and implementation. Drawing from hands-on
research and development in high-speed and grid networking, we identify a novel
approach to energy efficiency in network engineering. In this paper, we
demonstrate how the problem of efficient network system design can be dissected
into smaller sections based on timescales of traffic processing. The newly
proposed approach allows R&D efforts to be tightly paired to resources and
sustainability targets to improve energy efficiency in many classes of network
and telecom devices."
"Towards employing low complexity transceivers for signal reception in
Ultra-Wideband (UWB) systems, Transmitted Reference (TR) and Differential TR
(DTR) schemes have attracted researchers attention. In this letter, we
introduce an alternative, less complex scheme, called Self Reference (SR) UWB
transceiver, which uses a modified replica of the received signal itself as
reference pulse, resulting in double data rates compared to TR schemes.
Moreover, SR eliminates the need for delay lines at the receiver side, which
constitute a major drawback of the conventional TR and DTR schemes, while it
also requires no channel estimations, resulting in lower complexity
implementations and power savings. The performance of the SR scheme is
investigated in high-frequency (HF) channels, showing that it offers a better
or comparable performance to that of DTR, depending on the channel conditions."
"In optical WDM networks, since each lightpath can carry a huge mount of
traffic, failures may seriously damage the end user applications. Hence fault
tolerance becomes an important issue on these networks. The light path which
carries traffic during normal operation is called as primary path. The traffic
is rerouted on a backup path in case of a failure. In this paper we propose to
design a reliable and fault tolerant routing algorithm for establishing primary
and backup paths. In order to establish the primary path, this algorithm uses
load balancing in which link cost metrics are estimated based on the current
load of the links. In backup path setup, the source calculates the blocking
probability through the received feedback from the destination by sending a
small fraction of probe packets along the existing paths. It then selects the
optimal light path with the lowest blocking probability. Based on the
simulation results, we show that the reliable and fault tolerant routing
algorithm reduces the blocking probability and latency while increasing the
throughput and channel utilization."
"BitTorrent suffers from one fundamental problem: the long-term availability
of content. This occurs on a massive-scale with 38% of torrents becoming
unavailable within the first month. In this paper we explore this problem by
performing two large-scale measurement studies including 46K torrents and 29M
users. The studies go significantly beyond any previous work by combining
per-node, per-torrent and system-wide observations to ascertain the causes,
characteristics and repercussions of file unavailability. The study confirms
the conclusion from previous works that seeders have a significant impact on
both performance and availability. However, we also present some crucial new
findings: (i) the presence of seeders is not the sole factor involved in file
availability, (ii) 23.5% of nodes that operate in seedless torrents can finish
their downloads, and (iii) BitTorrent availability is discontinuous, operating
in cycles of temporary unavailability. Due to our new findings, we consider it
is important to revisit the solution space; to this end, we perform large-scale
trace-based simulations to explore the potential of two abstract approaches."
"The challenges of optimizing end-to-end performance over diverse Internet
paths has driven widespread adoption of in-path optimizers, which can
destructively interfere with TCP's end-to-end semantics and with each other,
and are incompatible with end-to-end IPsec. We identify the architectural cause
of these conflicts and resolve them in Tng, an experimental next-generation
transport services architecture, by factoring congestion control from
end-to-end semantic functions. Through a technique we call ""queue sharing"", Tng
enables in-path devices to interpose on, split, and optimize congestion
controlled flows without affecting or seeing the end-to-end content riding
these flows. Simulations show that Tng's decoupling cleanly addresses several
common performance problems, such as communication over lossy wireless links
and reduction of buffering-induced latency on residential links. A working
prototype and several incremental deployment paths suggest Tng's practicality."
"Packet and flow scheduling algorithms for WiMAX has been a topic of interest
for a long time since the very inception of WiMAX networks. WiMAX offers
advantages particularly in terms of Quality of service it offers over a longer
range at the MAC level. In our work, we propose two credit based scheduling
schemes one in which completed flows distributes the left over credits equally
to all higher priority uncompleted flows(ODRREDC) and another in which
completed flows give away all the excess credits to the highest priority
uncompleted flow(ODRRSDC). Both the schemes are compatible with 802.16 MAC
protocol and can efficiently serve real time bursty traffic with reduced
latency and hence improved QOS for real time flows. We compare the two proposed
schemes for their latency, bandwidth utilization and throughput for real time
burst flows with the opportunity based Deficit Round Robin scheduling scheme.
While the ODRR scheduler focuses on reducing the credits for the flows with
errors, our approach also distributes these remaining credits together with the
credits from completed flows equally among the higher priority uncompleted
flows or totally to the highest priority uncompleted flow."
"The WIMAX technology based on air interface standard 802.16 wireless MAN is
configured in the same way as a traditional cellular network with base stations
using point to multipoint architecture to drive a service over a radius up to
several kilometers. The range and the Non Line of Sight (NLOS) ability of WIMAX
make the system very attractive for users, but there will be slightly higher
BER at low SNR. The aim of this paper is the comparative study of different
guard time intervals effect for improving BER at different SNR under digital
modulation (QPSK, 16QAM and 64QAM) techniques and different communication
channels AWGN and fading channels Stanford University Interim (SUI 1) of an
WIMAX system. The comparison between these effects with Reed-Solomon (RS)
encoder with Convolutional encoder (half) rated codes in FEC channel coding
will be investigated. The simulation results of estimated Bit Error Rate (BER)
displays that the implementation of interleaved RS code (255,239,8) with (half)
rated Convolutional code of 0.25 guard time intervals under QPSK modulation
technique over AWGN channel is highly effective to combat in the Wimax
communication system. To complete this performance analysis in Wimax based
systems, a segment of audio signal is used for analysis. The transmitted audio
message is found to have retrieved effectively under noisy situation."
"Multicasting is effective when its group members are sparse and the speed is
low. On the other hand, broadcasting is effective when the group members dense
and the speed are high. Since mobile ad hoc networks are highly dynamic in
nature, either of the above two strategies can be adopted at different
scenarios. In this paper, we propose an ant agent based adaptive, multicast
protocol that exploits group members desire to simplify multicast routing and
invoke broadcast operations in appropriate localized regimes. By reducing the
number of group members that participate in the construction of the multicast
structure and by providing robustness to mobility by performing broadcasts in
densely clustered local regions, the proposed protocol achieves packet delivery
statistics that are comparable to that with a pure multicast protocol but with
significantly lower overheads. By our simulation results, we show that our
proposed protocol achieves increased Packet Delivery Fraction (PDF) with
reduced overhead and routing load."
"Solving free riding and selecting a reliable service provider in P2P networks
has been separately investigated in last few years. Using trust has shown to be
one of the best ways of solving these problems. But using this approach to
simultaneously deal with both problems makes it impossible for newcomers to
join the network and the expansion of network is prevented. In this paper we
used the game theory to model the behavior of peers and developed a mechanism
in which free riding and providing bad service are dominated strategies for
peers. At the same time newcomers can participate and are encouraged to be
active in the network. The proposed model has been simulated and the results
showed that the trust value of free riders and bad service providers converge
to a finite value and trust of peers who provide good service is monotonically
increased despite the time they join the network."
"Congestion in network occurs due to exceed in aggregate demand as compared to
the accessible capacity of the resources. Network congestion will increase as
network speed increases and new effective congestion control methods are
needed, especially to handle bursty traffic of todays very high speed networks.
Since late 90s numerous schemes i.e. [1]...[10] etc. have been proposed. This
paper concentrates on comparative study of the different congestion control
schemes based on some key performance metrics. An effort has been made to judge
the performance of Maximum Entropy (ME) based solution for a steady state
GE/GE/1/N censored queues with partial buffer sharing scheme against these key
performance metrics."
"In hierarchical Mobile IPv6 networks, Mobility Anchor Point (MAP) may become
a single point of bottleneck as it handles more and more mobile nodes (MNs). A
number of schemes have been proposed to achieve load balancing among different
MAPs. However, signaling reduction is still imperfect because these schemes
also avoid the effect of the number of CNs. Also only the balancing of MN is
performed, but not the balancing of the actual traffic load, since CN of each
MN may be different. This paper proposes an efficient admission control
algorithm along with a replacement mechanism for HMIPv6 networks. The admission
control algorithm is based on the number of serving CNs and achieves actual
load balancing among MAPs. Moreover, a replacement mechanism is introduced to
decrease the new MN blocking probability and the handoff MN dropping
probability. By simulation results, we show that, the handoff delay and packet
loss are reduced in our scheme, when compared with the standard HMIPv6 based
handoff."
"This paper deals with the performance of Worldwide Interoperability for
Microwave Access (WiMAX), when we enhance its physical layer attributes with
help of different encoding techniques. For this evaluation Space Time Block
Codes (STBC) and Turbo codes are separately introduced into the architecture of
WiMAX that works on adaptive modulation technique."
"Integrating the sensing capabilities in Internet Protocol network will open
the opportunities to build a wide range of novel multimedia applications. The
problem when using sensors (e.g. temperature sensor, camera, audio, humidity,
etc.) connected to the network is to know dynamically at any time if they are
always connected or not, what type of data they can transmit and where they are
geographically located. This paper describes an application enabler: IMS Sensor
Search Engine Enabler (iSSEE), which allows IMS applications using sensors and
IMS based devices, to get information about the sensor availability, its
location and the type of the sensor. Using data collected by sensors and from
the web, mash-ups convergent applications use cases are proposed by combining
the contents from heterogeneous data."
"This paper proposes and investigates the concept of a safe carrier-sensing
range that can guarantee interference safe (also termed hidden-node-free)
transmissions in CSMA networks under the cumulative interference model.
Compared with the safe carrier-sensing range under the commonly assumed but
less realistic pairwise interference model, we show that the safe
carrier-sensing range required under the cumulative interference model is
larger by a constant multiplicative factor. The concept of a safe
carrier-sensing range, although amenable to elegant analytical results, is
inherently not compatible with the conventional power threshold carrier-sensing
mechanism (e.g., that used in IEEE 802.11). Specifically, the absolute power
sensed by a node in the conventional mechanism does not contain enough
information for it to derive its distances from other concurrent transmitter
nodes. We show that, fortunately, a carrier-sensing mechanism called
Incremental-Power Carrier-Sensing (IPCS) can realize the carrier-sensing range
concept in a simple way. Instead of monitoring the absolute detected power, the
IPCS mechanism monitors every increment in the detected power. This means that
IPCS can separate the detected power of every concurrent transmitter, and map
the power profile to the required distance information."
"A mobile ad-hoc network (MANET) is collection of intercommunicating mobile
hosts forming a spontaneous network without using established network
infrastructure. Unlike the cellular or infrastructure networks who have a wired
backbone connecting the base-station, the MANETs have neither fixed routers nor
fixed locations. Their performance largely depend upon the routing mechanism &
nature of mobility. Earlier research hints that the Destination Sequenced
Distance Vector (DSDV) routing protocol is one of the most efficient and
popular protocols, as far as general parameters have been concerned.[1,6] We
have experimentally evaluated, the performance metrics for network load, packet
delivery fraction and end-to-end delay with DSDV Protocol using NS2
Simulator.This paper presents, the performance of DSDV protocol for four
different mobility models namely: Random Waypoint, Reference Point Group
Mobility, Gauss Markov & Manhattan Mobility Model having varying network load &
speed. The experimental results suggest that DSDV protocol with RPGM mobility
model has optimized results for varying network load and speed."
"The large scale content distribution systems were improved broadly using the
replication techniques. The demanded contents can be brought closer to the
clients by multiplying the source of information geographically, which in turn
reduce both the access latency and the network traffic. The system scalability
can be improved by distributing the load across multiple servers which is
proposed by replication. If a copy of the requested object (e.g., a web page or
an image) is located in its closer proximity then the clients would feel low
access latency. Depending on the position of the replicas, the effectiveness of
replication tends to a large extent. A QoS based overlay network architecture
involving an intelligent replica placement algorithm is proposed in this paper.
Its main goal is to improve the network utilization and fault tolerance of the
P2P system. In addition to the replica placement, it also has a caching
technique, to reduce the search latency. We are able to show that our proposed
architecture attains less latency and better throughput with reduced bandwidth
usage, through the simulation results."
"Advantages of hypercube network and torus topology are used to derive an
embedded architecture for product network known as torus embedded hypercube
scalable interconnection network. This paper analyzes torus embedded hypercube
network pertinent to parallel architecture. The network metrics are used to
show how good embedded network can be designed for parallel computation.
Network parameter analysis and comparison of embedded network with basic
networks is presented."
"This paper studies the optimal investment and pricing decisions of a
cognitive mobile virtual network operator (C-MVNO) under spectrum supply
uncertainty. Compared with a traditional MVNO who often leases spectrum via
long-term contracts, a C-MVNO can acquire spectrum dynamically in short-term by
both sensing the empty ""spectrum holes"" of licensed bands and dynamically
leasing from the spectrum owner. As a result, a C-MVNO can make flexible
investment and pricing decisions to match the current demands of the secondary
unlicensed users. Compared to dynamic spectrum leasing, spectrum sensing is
typically cheaper, but the obtained useful spectrum amount is random due to
primary licensed users' stochastic traffic. The C-MVNO needs to determine the
optimal amounts of spectrum sensing and leasing by evaluating the trade off
between cost and uncertainty. The C-MVNO also needs to determine the optimal
price to sell the spectrum to the secondary unlicensed users, taking into
account wireless heterogeneity of users such as different maximum transmission
power levels and channel gains. We model and analyze the interactions between
the C-MVNO and secondary unlicensed users as a Stackelberg game. We show
several interesting properties of the network equilibrium, including threshold
structures of the optimal investment and pricing decisions, the independence of
the optimal price on users' wireless characteristics, and guaranteed fair and
predictable QoS among users. We prove that these properties hold for general
SNR regime and general continuous distributions of sensing uncertainty. We show
that spectrum sensing can significantly improve the C-MVNO's expected profit
and users' payoffs."
"The internet is now-a-days experiencing a stress due to some inherent
problems with the main interdomain routing protocol, boarder gateway protocol
(BGP), the amount of time it takes to converge, number of update message
exchanged followed by a failure to stabilize, the amount of time required to
get a valid alternate path following the failure, the way size of routing table
increasing, and security issues like integrity and privacy of routing tables
and routing updates exchanged among the routers, are of our primary concern. In
our proposed research work we plan to address aforementioned issues related to
internet routing specially in boarder gateway protocol to enable BGP to offer
expeditious unswerving routing to corroborate nascent internet. We plan to make
some changes in the design of boarder gateway protocol and may introduce
addition of extra features in BGP to help support above mentioned objective."
"The purposes of this paper have to discuss issues related to Network Traffic
Management. A relatively new category of network management is fast becoming a
necessity in converged business Networks. Mid-sized and large organizations are
finding they must control network traffic behavior to assure that their
strategic applications always get the resources they need to perform optimally.
Controlling network traffic requires limiting bandwidth to certain
applications, guaranteeing minimum bandwidth to others, and marking traffic
with high or low priorities. This exercise is called Network Traffic
Management."
"As the number and size of the Network increases, the deficiencies persist,
including network security problems. But there is no shortage of technologies
offered as universal remedy - EIGRP,BGP, OSPF, VoIP, IPv6, IPTV, MPLS, WiFi, to
name a few. There are multiple factors for the current situation. Now a day
during emergent and blossoming stages of network development is no longer
sufficient when the networks are mature and have become everyday tool for
social and business interactions. A new model of network is necessary to find
solutions for today's pressing problems, especially those related to network
security. In this paper out factors leading to current stagnation discusses
critical assumptions behind current networks, how many of them are no longer
valid and have become barriers for implementing real solutions. The paper
concludes by offering new directions for future needs and solving current
challenges."
"We analyze the multihop delay of ad hoc cognitive radio networks, where the
transmission delay of each hop consists of the propagation delay and the
waiting time for the availability of the communication channel (i.e., the
occurrence of a spectrum opportunity at this hop). Using theories and
techniques from continuum percolation and ergodicity, we establish the scaling
law of the minimum multihop delay with respect to the source-destination
distance in cognitive radio networks. When the propagation delay is negligible,
we show the starkly different scaling behavior of the minimum multihop delay in
instantaneously connected networks as compared to networks that are only
intermittently connected due to scarcity of spectrum opportunities.
Specifically, if the network is instantaneously connected, the minimum multihop
delay is asymptotically independent of the distance; if the network is only
intermittently connected, the minimum multihop delay scales linearly with the
distance. When the propagation delay is nonnegligible but small, we show that
although the scaling order is always linear, the scaling rate for an
instantaneously connected network can be orders of magnitude smaller than the
one for an intermittently connected network."
"Multi-channel wireless networks are increasingly being employed as
infrastructure networks, e.g. in metro areas. Nodes in these networks
frequently employ directional antennas to improve spatial throughput. In such
networks, given a source and destination, it is of interest to compute an
optimal path and channel assignment on every link in the path such that the
path bandwidth is the same as that of the link bandwidth and such a path
satisfies the constraint that no two consecutive links on the path are assigned
the same channel, referred to as ""Channel Discontinuity Constraint"" (CDC).
CDC-paths are also quite useful for TDMA system, where preferably every
consecutive links along a path are assigned different time slots.
  This paper contains several contributions. We first present an $O(N^{2})$
distributed algorithm for discovering the shortest CDC-path between given
source and destination. This improves the running time of the $O(N^{3})$
centralized algorithm of Ahuja et al. for finding the minimum-weight CDC-path.
Our second result is a generalized $t$-spanner for CDC-path; For any $\theta>0$
we show how to construct a sub-network containing only $O(\frac{N}{\theta})$
edges, such that that length of shortest CDC-paths between arbitrary sources
and destinations increases by only a factor of at most
$(1-2\sin{\tfrac{\theta}{2}})^{-2}$. We propose a novel algorithm to compute
the spanner in a distributed manner using only $O(n\log{n})$ messages. An
important conclusion of this scheme is in the case of directional antennas are
used. In this case, it is enough to consider only the two closest nodes in each
cone."
"The telephony over IP (ToIP) is becoming a new trend in technology widely
used nowadays in almost all business sectors. Its concepts rely on transiting
the telephone communications through the IP network. Today, this technology is
deployed increasingly what the cause of emergence of companies is offering this
service as Switzernet. For several highly demanded destinations, recently fake
vendors appeared in the market offering voice termination but providing only
false answer supervision. The answered signal is returned immediately and calls
are being charged without being connected. Different techniques are used to
keep the calling party on the line. One of these techniques is to play a record
of a ring back tone (while the call is already being charged). Another, more
sophisticated technique is to play a human voice randomly picked up from a set
of records containing contents similar to: hello, hello, I cannot hear you
Apart the fact that the fallaciously established calls are charged at rates of
real calls, such malicious routes seriously handicap the switching process. The
system does not detect a failure on signaling level and is unable to attempt
the call via backup routes, the call technically being already connected. Once
the call flow falls into such trap, the calls will continue being routed via
the fraudulent route until a manual intervention."
"Nodes of minimum connected dominating set (MCDS) form a virtual backbone in a
wireless adhoc network. In this paper, a modified approach is presented to
determine MCDS of an underlying graph of a Wireless Adhoc network. Simulation
results for a variety of graphs indicate that the approach is efficient in
determining the MCDS as compared to other existing techniques."
"A lot of work has been done on routing protocols for mobile ad hoc networks,
but still standardization of them requires some more issues less addressed by
the existing routing protocols. In this paper a new paradigm of maintaining
multiple connections in adhoc routing protocols has been highlighted which may
be crucial for efficient routing in mobile ad hoc networks. The problem of
multiple connections has been hardly worked on in adhoc networks. In this paper
the solution of route maintenance if nodes are maintaining multiple connections
has been proposed. This idea not only helps to solve the multiple connections
problem, but also take care of proper bandwidth distribution to different
connections as per different traffic types. Study has been incorporated on
existing AODV with changes. Simulation studies have been performed over packet
delivery ratio, throughput and message overheads. Results show that the
proposed solution for multiple connections is efficient and worth implementing
in existing as well as new protocols."
"Multi-hop random access networks have received much attention due to their
distributed nature which facilitates deploying many new applications over the
sensor and computer networks. Recently, utility maximization framework is
applied in order to optimize performance of such networks however delay is not
limited and proposed algorithms result in very large transmission delays. In
this paper, we will analyze delay in random access multi-hop networks and solve
the delay-constrained utility maximization problem. We define the network
utility as a combination of rate utility and energy cost functions and solve
the following two problems: 'optimal medium access control with link delay
constraint' and, 'optimal congestion and contention control with end-to-end
delay constraint'. The optimal tradeoff between delay, rate, and energy is
achieved for different values of delay constraint and the scaling factors
between rate and energy. Different distributed solutions will be proposed for
each problem and their performance will be compared in terms of convergence and
complexity."
"A new framework to perform routing at the Autonomous System level is proposed
in this paper. This mechanism, called Chain Routing, uses complete orders as
its main topological unit. Since complete orders are acyclic digraphs that
possess a known topology, it is possible to define an acyclic structure to
route packets between a group of Autonomous Systems. The adoption of complete
orders also allows easy identification and avoidance of persistent route
oscillations, eliminates the possibility of developing transient loops in
paths, and provides a structure that facilitates the implementation of traffic
engineering. Moreover, by combining Chain Routing with other mechanisms that
implement complete orders in time, we suggest that it is possible to design a
new routing protocol which could be more reliable and stable than BGP's current
implementation. Although Chain Routing will require an increase of the message
overhead and greater coordination between network administrators, the rewards
in stability and resilience should more than compensate for this effort."
"The main challenges of cognitive radio include spectrum sensing at the
physical (PHY) layer to detect the activity of primary users and spectrum
sharing at the medium access control (MAC) layer to coordinate access among
coexisting secondary users. In this paper, we consider a cognitive radio
network in which a primary user shares a channel with secondary users that
cannot distinguish the signals of the primary user from those of a secondary
user. We propose a class of distributed cognitive MAC protocols to achieve
efficient spectrum sharing among the secondary users while protecting the
primary user from potential interference by the secondary users. By using a MAC
protocol with one-slot memory, we can obtain high channel utilization by the
secondary users while limiting interference to the primary user at a low level.
The results of this paper suggest the possibility of utilizing MAC design in
cognitive radio networks to overcome limitations in spectrum sensing at the PHY
layer as well as to achieve spectrum sharing at the MAC layer."
"The connectivity of the Internet at the Autonomous System level is influenced
by the network operator policies implemented. These in turn impose a direction
to the announcement of address advertisements and, consequently, to the paths
that can be used to reach back such destinations. We propose to use directed
graphs to properly represent how destinations propagate through the Internet
and the number of arc-disjoint paths to quantify this network's path diversity.
Moreover, in order to understand the effects that policies have on the
connectivity of the Internet, numerical analyses of the resulting directed
graphs were conducted. Results demonstrate that, even after policies have been
applied, there is still path diversity which the Border Gateway Protocol cannot
currently exploit."
"The processing, computation and memory requirements posed by emerging mobile
broadband services require adaptive memory management and prefetching
techniques at the mobile terminals for satisfactory application performance and
sustained device battery lifetime. In this work we investigate a scenario where
tasks with varied computational requirements are fetched by a mobile device
from a central server over an error prone wireless link. We examine the buffer
dynamics at the mobile terminal and the central server under varying wireless
channel connectivity and device memory congestion states as variable sizes
tasks are executed on the terminal. Our goal is to minimize the latency
experienced by these tasks while judiciously utilizing the device buffering
capability. We use a dynamic programming framework to model the optimal
prefetching policy. We further propose a) a prefetching algorithm Fetch-or- Not
(FON), which uses quasi-static assumption on system state to make prefetching
decisions, and b) a prefetching policy RFON, which uses randomized
approximation to the optimal solution thus obviating the need for dynamic
online optimization and substantially reducing the computational complexity.
Through performance evaluation under slow and fast fading scenarios we show
that proposed algorithms come close to performance of the optimal scheme."
"Network intrusion detection systems play a critical role in protecting the
information infrastructure of an organization. Due to the sophistication and
complexity of techniques used for the analysis they are commonly based on
general-purpose workstations. Although cost-efficient, these general-purpose
systems are found to be inadequate as they are unable to perform efficiently at
high packet rates. The resulting packet loss degrades the system's overall
effectiveness, as the analyzing capability of the system is reduced. It has
been found that the performance of these sensors can be improved significantly
by filtering out unwanted packets. This paper presents the design of a
Programmable Ethernet Interface Card that is used to offload signature matching
from software and thereby improve the detection ratio and performance of the
system."
"We propose a network implementation with enhanced security at the physical
layer by means of time-hopping CDMA, supporting cryptographically secure
point-to-point and point-to-multipoint communication. In particular, we analyze
an active star topology optical network implementation capable of supporting
128 simultaneous users up to 20 km apart. The feasibility of the proposed
scheme is demonstrated through numerical simulation."
"Wireless sensor networks (WSNs) can be a valuable decision-support tool for
farmers. This motivated our deployment of a WSN system to support rain-fed
agriculture in India. We defined promising use cases and resolved technical
challenges throughout a two-year deployment of our COMMON-Sense Net system,
which provided farmers with environment data. However, the direct use of this
technology in the field did not foster the expected participation of the
population. This made it difficult to develop the intended decision-support
system. Based on this experience, we take the following position in this paper:
currently, the deployment of WSN technology in developing regions is more
likely to be effective if it targets scientists and technical personnel as
users, rather than the farmers themselves. We base this claim on the lessons
learned from the COMMON-Sense system deployment and the results of an extensive
user experiment with agriculture scientists, which we describe in this paper."
"Vehicular Ad Hoc Networks (VANETs) are a peculiar subclass of mobile ad hoc
networks that raise a number of technical challenges, notably from the point of
view of their mobility models. In this paper, we provide a thorough analysis of
the connectivity of such networks by leveraging on well-known results of
percolation theory. By means of simulations, we study the influence of a number
of parameters, including vehicle density, proportion of equipped vehicles, and
radio communication range. We also study the influence of traffic lights and
roadside units. Our results provide insights on the behavior of connectivity.
We believe this paper to be a valuable framework to assess the feasibility and
performance of future applications relying on vehicular connectivity in urban
scenarios."
"This paper addresses an overview of the wireless sensor networks. It is shown
that MEMS/NEMS technologies and SIP concept are well suited for advanced
architectures. It is also shown analog architectures have to be compatible with
digital signal techniques to develop smart network of microsystem."
"This paper addresses the radio interface problematic for MANET (Mobile Ad-hoc
NETwork) applications. Here we propose to study the radio reconfigurability in
order to provide a unique physical layer which is able to deal with all MANET
applications. For implementing this reconfigurable physical layer, we propose
to use Impulse Radio Ultra WideBand (IRUWB). This paper presents also our two
level design approach for obtaining our reconfigurable IR-UWB receiver on FPGA
(Field Programmable Gate Array)."
"We introduce the radio reconfigurability thanks to IRUWB mostly digital
architecture for MANET context. This particular context implies some
constraints on the radio interface such as low cost, low power, small
dimensions and simplicity. Here, we propose an implementation of dynamic
reconfigurable receiver on ASIC, and FPGA, after having explained the
advantages of mostly digital radio for reconfigurability. In this paper, by
studying our prototypes, we could prove that reconfigurability is on the
contrary with MANET constraints needs. The proposed solution allows data rate,
radio range, energy and spectrum occupation reconfigurability."
"This paper presents our approach of the radio interface problematic for
Wireless Sensor Network. We introduce the WSN context and constraints
associated. We propose an IR-UWB solution and illustrate why it could be a
viable solution for WSN. A high level modelling and simulation platform for
IR-UWB radio interface is proposed on Matlab. It allows us to determine
according to BER versus Eb/N0 criteria and the WSN constraints what kind of
design is more adequate. Moreover, a co-design co-simulation platform Matlab
VHDL is proposed here. Using this platform we designed IR-UWB transceiver
having reconfigurable capabilities, such as data rate reconfiguration, time
hopping code, spectrum occupation and radio range reconfiguration."
"Impulse Radio Ultra Wide Band (IR-UWB) is a promising technology to address
Wireless Sensor Network (WSN) constraints. However, existing network simulation
tools do not provide a complete WSN simulation architecture, with the IR-UWB
specificities at the PHYsical (PHY) and the Medium Access Control (MAC) layers.
In this paper, we propose a WSN simulation architecture based on the IR-UWB
technique. At the PHY layer, we take into account the pulse collision by
dealing with the pulse propagation delay. We also modelled MAC protocols
specific to IRUWB, for WSN applications. To completely fit the WSN simulation
requirements, we propose a generic and reusable sensor and sensing channel
model. Most of the WSN application performances can be evaluated thanks to the
proposed simulation architecture. The proposed models are implemented on a
scalable and well known network simulator: Global Mobile Information System
Simulator (GloMoSim). However, they can be reused for all other packet based
simulation platforms."
"This paper describes a detailed performance evaluation of distributed Medium
Access Control (MAC) protocols for Wireless Sensor Networks based on Impulse
Radio Ultra Wideband (IR-UWB) Physical layer (PHY). Two main classes of Medium
Access Control protocol have been considered: Slotted and UnSlotted with
reliability. The reliability is based on Automatic Repeat ReQuest (ARQ). The
performance evaluation is performed using a complete Wireless Sensor Networks
(WSN) simulator built on the Global Mobile Information System Simulator
(GloMoSim). The optimal operating parameters are first discussed for IR-UWB in
terms of slot size, retransmission delay and the number of retransmission, then
a comparison between IR-UWB and other transmission techniques in terms of
reliability latency and power efficiency."
"Starting from the Shannon channel capacity, we propose an IR-UWB channel
capacity based on the delay spread for multipath time variant channels. This
IR-UWB channel capacity is obtained from the no ISI (Inter Symbol Interference)
assumption and for binary modulations. The impact of the kind of implementation
is considered on the IR-UWB channel capacity. This study is lead for mixed and
mostly digital implementation. The key parameters and theirs impacts on the
channel capacity are exposed in each case: the data converters for mostly
digital implementations and the pulse generator capabilities for mixed
implementations. Finally, these two implementations are compared from a data
rate point of view. Their behaviors regarding an increase of the operating
frequency are also studied."
"This paper presents a performance evaluation of Wireless Sensor Networks
(WSN) based on Impulse Radio Ultra Wideband (IR-UWB) over a new simulation
platform developed for this purpose. The simulation platform is built on an
existing network simulator: Global Mobile Information System Simulator
(GloMoSim). It mainly focuses on the accurately modeling of IR-UWB PHYsical
(PHY) and Medium Access Control (MAC) layer. Pulse collision is modeled
according to the used time hopping sequence (THS) and the pulse propagation
delay in order to increase the simulation fidelity. It also includes a
detection and identification application based on a new sensing channel and new
sensor device models. The proposed architecture is generic so it can be reused
for any simulation platform. The performance evaluation is based on one of the
typical WSN applications: local area protection, where sensor nodes are densely
scattered in an access regulated area in order to detect, identify and report
non authorized accesses to a base station for analysis. Two networks topologies
using different protocol stacks are investigated. Their performance evaluation
is presented in terms of reliability and latency."
"A new model is proposed giving the channel capability of a MB-IR-UWB system
versus the number of subband and the duty cycle. The architecture simulated
shows data rate ranging from 1.434 Gbits/s to 0.9 Gbits/s for 16 to 10 subbands
and duty cycle ranging from 20% to 12%."
"The IEEE 802.11 standard offers a cheap and promising solution for small
scale wireless networks. Due to the self configuring nature, WLANs do not
require large scale infrastructure deployment, and are scalable and easily
maintainable which incited its popularity in both literature and industry. In
real environment, these networks operate mostly under unsaturated condition. We
investigate performance of such a network with m-retry limit BEB based DCF. We
consider imperfect channel with provision for power capture. Our method employs
a Markov model and represents the most common performance measures in terms of
network parameters making the model and mathematical analysis useful in network
design and planning. We also explore the effects of packet error, network size,
initial contention window, and retry limit on overall performance of WLANs."
"The impact of the type of implementation is considered on the IR-UWB channel
capacity. This study is lead for analog and mostly digital implementation. Key
parameters and theirs impacts on the channel capacity are exposed in each case:
data converters for mostly digital implementations and pulse generators
capabilities for analog implementations. These two implementations are compared
from a data rate point of view. Their behaviors regarding an increase of the
operating frequency are also studied"
"The increasing role of home automation in routine life and the rising demand
for sensor networks enhanced wireless personal area networks (WPANs)
development, pervasiveness of wireless & wired network, and research. Soon
arose the need of implementing the Internet Protocol in these devices in order
to WPAN standards, raising the way for questions on how to provide seamless
communication between wired and wireless technologies. After a quick overview
of the Low-rate WPAN standard (IEEE 802.15.4) and the Zigbee stack, this paper
focuses on understanding the implications when interconnecting low powered IEEE
802.15.4 devices and a wired IPv6 domain. Subsequently the focus will be on
existing approaches to connect LoWPAN devices to the internet and on how these
approaches try to solve these challenges, concluding with a critical analysis
of interoperability problems."
"This paper presents the realistic approach towards the quantitative analysis
and simulation of Energy Efficient Hierarchical Cluster (EEHC)-based routing
for wireless sensor networks. Here the efforts have been done to combine
analytical hardware model with the modified EEHC-based routing model. The
dependence of various performance metrics like: optimum number of clusters,
Energy Consumption, and Energy consumed per round etc. based on analytical
hardware sensor model and EEHC model has been presented."
"A Virtual Private Network (VPN) provides private network connections over a
publicly accessible shared network. The effective allocation of bandwidth for
VPNs assumes significance in the present scenario due to varied traffic. Each
VPN endpoint specifies bounds on the total amount of traffic that it is likely
to send or receive at any time. The network provider tailors the VPN so that
there is sufficient bandwidth for any traffic matrix that is consistent with
these bounds. The approach incorporates the use of Ad-hoc On demand Distance
Vector (AODV) protocol, with a view to accomplish an enhancement in the
performance of the mobile networks. The NS2 based simulation results are
evaluated in terms of its metrics for different bandwidth allocations, besides
analyzing its performance in the event of exigencies such as link failures. The
results highlight the suitability of the proposed strategy in the context of
real time applications."
"Mobile Ad hoc Networks are highly dynamic networks. Quality of Service (QoS)
routing in such networks is usually limited by the network breakage due to
either node mobility or energy depletion of the mobile nodes. Also, to fulfill
certain quality parameters, presence of multiple node-disjoint paths becomes
essential. Such paths aid in the optimal traffic distribution and reliability
in case of path breakages. Thus, to cater various challenges in QoS routing in
Mobile Add hoc Networks, a Node Disjoint Multipath Routing Considering Link and
Node Stability (NDMLNR) protocol has been proposed by the authors. The metric
used to select the paths takes into account the stability of the nodes and the
corresponding links. This paper studies various challenges in the QoS routing
and presents the characteristic evaluation of NDMLNR w.r.t various existing
protocols in this area."
"Paper has been withdrawn due to non-compliance with IJCSI terms and
conditions."
"Today, SIP is a protocol par Excellence in the field of communication over
Internet. But, the fact that it belongs to the application layer constitutes a
weakness vis-a-vis the NAT traversal. This weakness is due to the way in which
the server replies to the requests of clients on the one hand. On the other, it
is caused by the dynamic allocation of UDP ports for emission and reception of
packets RTP/RTCP. The TURN Protocol may face this weakness. However, its use
requires a certain number of exchanges between the clients and a TURN server
before establishing the multimedia sessions and this increase the latent time.
In this article, we propose to adapt TURN protocol for applications based on
SIP protocol such as telephony over Internet, conference video, etc. This
adaptation optimises the establishment of multimedia sessions by integrating a
manager of TCP connections and multimedia flow controller into SIP Proxy
server."
"RFID is not a new technology and has passed through many decades of use in
military, airline, library, security, healthcare, sports, animal farms and
other areas. Industries use RFID for various applications such as
personal/vehicle access control, departmental store security, equipment
tracking, baggage, fast food establishments, logistics, etc. The enhancement in
RFID technology has brought advantages that are related to resource
optimization, increased efficiency within business processes, and enhanced
customer care, overall improvements in business operations and healthcare. Our
research is part of a big project; its aim is to produce a model for mobile
technology implementation of hospital patients' movement process. However, the
focus of this paper is to explore the main RFID components, i.e. the tag,
antenna and reader. The results of the investigations conducted on the three
RFID components will be used to develop our research model."
"VANETs (Vehicular Ad hoc Networks) are highly mobile wireless ad hoc networks
and will play an important role in public safety communications and commercial
applications. Routing of data in VANETs is a challenging task due to rapidly
changing topology and high speed mobility of vehicles. Conventional routing
protocols in MANETs (Mobile Ad hoc Networks) are unable to fully address the
unique characteristics in vehicular networks. In this paper, we propose EBGR
(Edge Node Based Greedy Routing), a reliable greedy position based routing
approach to forward packets to the node present in the edge of the transmission
range of source/forwarding node as most suitable next hop, with consideration
of nodes moving in the direction of the destination. We propose Revival
Mobility model (RMM) to evaluate the performance of our routing technique. This
paper presents a detailed description of our approach and simulation results
show that packet delivery ratio is improved considerably compared to other
routing techniques of VANET."
"In this paper we build upon the recent observation that the 802.11 rate
region is log-convex and, for the first time, characterise max-min fair rate
allocations for a large class of 802.11 wireless mesh networks. By exploiting
features of the 802.11e/n MAC, in particular TXOP packet bursting, we are able
to use this characterisation to establish a straightforward, practically
implementable approach for achieving max-min throughput fairness. We
demonstrate that this approach can be readily extended to encompass time-based
fairness in multi-rate 802.11 mesh networks."
"The traditional TCP congestion control mechanism encounters a number of new
problems and suffers a poor performance when the IEEE 802.11 MAC protocol is
used in multihop ad hoc networks. Many of the problems result from medium
contention at the MAC layer. In this paper, I first illustrate that severe
medium contention and congestion are intimately coupled, and TCP s congestion
control algorithm becomes too coarse in its granularity, causing throughput
instability and excessively long delay. Further, we illustrate TCP s severe
unfairness problem due to the medium contention and the tradeoff between
aggregate throughput and fairness. Then, based on the novel use of channel
busyness ratio, a more accurate metric to characterize the network utilization
and congestion status, I propose a new wireless congestion control protocol
(WCCP) to efficiently and fairly support the transport service in multihop ad
hoc networks. In this protocol, each forwarding node along a traffic flow
exercises the internode and intranode fair resource allocation and determines
the MAC layer feedback accordingly. The endtoend feedback, which is ultimately
determined by the bottleneck node along the flow, is carried back to the source
to control its sending rate. Extensive simulations show that WCCP significantly
outperforms traditional TCP in terms of channel utilization, delay, and
fairness, and eliminates the starvation problem."
"Distributed contention based Medium Access Control (MAC) protocols are the
fundamental components for IEEE 802.11 based Wireless Local Area Networks
(WLANs). Contention windows (CW) change dynamically to adapt to the current
contention level, Upon each packet collision, a station doubles its CW to
reduce further collision of packets. IEEE 802.11 Distributed Coordination
Function (DCF) suffers from a common problem in erroneous channel. They cannot
distinguish noise lost packets from collision lost packets. In both situations
a station does not receive its ACK and doubles the CW to reduce further packet
collisions. This increases backoff overhead unnecessarily in addition to the
noise lost packets, reduces the throughput significantly. Furthermore, the
aggregate throughput of a practical WLAN strongly depends on the channel
conditions. In real radio environment, the received signal power at the access
point from a station is subjected to deterministic path loss, shadowing and
fast multipath fading. In this paper, we propose a new saturation throughput
analysis for IEEE 802.11 DCF considering erroneous channel and capture effects.
To alleviate the low performance of IEEE 802.11 DCF, we introduce a mechanism
that greatly outperforms under noisy environment with low network traffic and
compare their performances to the existing standards. We extend the
multidimensional Markov chain model initially proposed by Bianchi(3) to
characterize the behavior of DCF in order to account both real channel
conditions and capture effects, especially in a high interference radio
environment."
"WLAN localization has become an active research field recently. Due to the
wide WLAN deployment, WLAN localization provides ubiquitous coverage and adds
to the value of the wireless network by providing the location of its users
without using any additional hardware. However, WLAN localization systems
usually require constructing a radio map, which is a major barrier of WLAN
localization systems' deployment. The radio map stores information about the
signal strength from different signal strength streams at selected locations in
the site of interest. Typical construction of a radio map involves measurements
and calibrations making it a tedious and time-consuming operation. In this
paper, we present the AROMA system that automatically constructs accurate
active and passive radio maps for both device-based and device-free WLAN
localization systems. AROMA has three main goals: high accuracy, low
computational requirements, and minimum user overhead. To achieve high
accuracy, AROMA uses 3D ray tracing enhanced with the uniform theory of
diffraction (UTD) to model the electric field behavior and the human shadowing
effect. AROMA also automates a number of routine tasks, such as importing
building models and automatic sampling of the area of interest, to reduce the
user's overhead. Finally, AROMA uses a number of optimization techniques to
reduce the computational requirements. We present our system architecture and
describe the details of its different components that allow AROMA to achieve
its goals. We evaluate AROMA in two different testbeds. Our experiments show
that the predicted signal strength differs from the measurements by a maximum
average absolute error of 3.18 dBm achieving a maximum localization error of
2.44m for both the device-based and device-free cases."
"Wireless mobile grids are one of the emerging grid types, which help to pool
the resources of several willing and cooperative mobile devices to resolve a
computationally intensive task. The mobile grids exhibit stronger challenges
like mobility management of devices, providing transparent access to grid
resources, task management and handling of limited resources so that resources
are shared efficiently. Task execution on these devices should not be affected
by their mobility. The proposed work presents performance evaluation of
wireless mobile grid using normal walk mobility model. The normal walk model
represents daily motion of users and the direction of motion is mostly
symmetric in a real life environment, thus it is effective in location updating
of a mobile station and in turn helps task distribution among these available
mobile stations. Some of the performance parameters such as Task Execution
Time, task failure rate, communication overhead on Brokering Server and
Monitoring Cost are discussed."
"We propose an adaptive transmission technique for free space optical (FSO)
systems, operating in atmospheric turbulence and employing subcarrier phase
shift keying (S-PSK) intensity modulation. Exploiting the constant envelope
characteristics of S-PSK, the proposed technique offers efficient utilization
of the FSO channel capacity by adapting the modulation order of S-PSK,
according to the instantaneous state of turbulence induced fading and a
pre-defined bit error rate (BER) requirement. Novel expressions for the
spectral efficiency and average BER of the proposed adaptive FSO system are
presented and performance investigations under various turbulence conditions
and target BER requirements are carried out. Numerical results indicate that
significant spectral efficiency gains are offered without increasing the
transmitted average optical power or sacrificing BER requirements, in
moderate-to-strong turbulence conditions. Furthermore, the proposed variable
rate transmission technique is applied to multiple input multiple output (MIMO)
FSO systems, providing additional improvement in the achieved spectral
efficiency as the number of the transmit and/or receive apertures increases."
"WiMAX technology is based on the IEEE 802.16 specification of which IEEE
802.16-2004 and 802.16e amendment are Physical (PHY) layer specifications. IEEE
802.16-2004 currently supports several multiple-antenna options including
Space-Time Codes (STC), Multiple-Input Multiple-Output (MIMO) antenna systems
and Adaptive Antenna Systems (AAS). The most recent WiMAX standard (802.16e)
supports broadband applications to mobile terminals and laptops. Using Adaptive
Modulation and Coding (AMC) we analyze the performance of OFDM physical layer
in WiMAX based on the simulation results of Bit Error Rate (BER), and data
throughput. The performance analysis of OFDM PHY is done. In this paper, an
extension to the basic SISO mode, a number of 2 by 2 MIMO extensions are
analysed under different combinations of digital modulation (QPSK, 16QAM and
64QAM) and Convolutional Code (CC) with half, two-third and three quarter rated
codes. The intent of this paper is to provide an idea of the benefits of
multiple antenna systems over single antenna systems in WiMAX type deployments."
"Over the recent years a considerable amount of effort has been devoted
towards the performance evaluation and prediction of Mobile Networks.
Performance modeling and evaluation of mobile networks are very important in
view of their ever expending usage and the multiplicity of their component
parts together with the complexity of their functioning. The present paper
addresses current issues in traffic management and congestion control by
(signal to interference plus noise ratio) SINR prediction congestion control,
routing and optimization of cellular mobile networks."
"The capability to provide network service even under a significant network
system element disruption is the backbone for the survival of route optimize of
mobile network Technology in today s world. Keeping this view in mind, the
present paper highlights a new method based on memetic algorithm."
"Channel properties influence the development of wireless communication
systems. Unlike wired channels that are stationary and predictable, radio
channels are extremely random and dont offer easy analysis. A Radio Propagation
Model (RPM), also known as the Radio Wave Propagation Model (RWPM), is an
empirical mathematical formulation for the characterization of radio wave
propagation as a function of frequency. In mobile radio systems, path loss
models are necessary for proper planning, interference estimations, frequency
assignments and cell parameters which are the basic for network planning
process as well as Location Based Services (LBS) techniques. Propagation models
that predict the mean signal strength for an arbitrary transmitter receiver (T
R) separation distance which is useful in estimating the radio coverage area of
a transmitter are called large scale propagation models, since they
characterize signal strength over large TR separation distances. In this paper,
the large scale propagation performance of Okumura, Hata, and Lee models has
been compared varying Mobile Station (MS) antenna height, Transmitter Receiver
(TR) distance and Base Station (BS) antenna height, considering the system to
operate at 900 MHz. Through the MATLAB simulation it is turned out that the
Okumura model shows the better performance than that of the other large scale
propagation models."
"With the proliferation of mobile computing devices, the demand for continuous
network connectivity regardless of physical location has spurred interest in
the use of mobile ad hoc networks. Since Transmission Control Protocol (TCP) is
the standard network protocol for communication in the internet, any wireless
network with Internet service need to be compatible with TCP. TCP is tuned to
perform well in traditional wired networks, where packet losses occur mostly
because of congestion. However, TCP connections in Ad-hoc mobile networks are
plagued by problems such as high bit error rates, frequent route changes,
multipath routing and temporary network partitions. The throughput of TCP over
such connection is not satisfactory, because TCP misinterprets the packet loss
or delay as congestion and invokes congestion control and avoidance algorithm.
In this research, the performance of TCP in Adhoc mobile network with high Bit
Error rate (BER) and mobility is studied and investigated. Simulation model is
implemented and experiments are performed using the Network Simulatior 2 (NS2)."
"In this paper, we have to concentrate on implementation of Weighted
Clustering Algorithm with the help of Genetic Algorithm (GA).Here we have
developed new algorithm for the implementation of GA-based approach with the
help of Weighted Clustering Algorithm (WCA) (4). ClusterHead chosen is a
important thing for clustering in adhoc networks. So, we have shown the
optimization technique for the minimization of ClusterHeads(CH) based on some
parameter such as degree difference, Battery power (Pv), degree of mobility,
and sum of the distances of a node in adhoc networks. ClusterHeads selection of
adhoc networks is an important thing for clustering. Here, we have discussed
the performance comparison between deterministic approach and GA based
approach. In this performance comparison, we have seen that GA does not always
give the good result compare to deterministic WCA algorithm. Here we have seen
connectivity (connectivity can be measured by the probability that a node is
reachable to any other node.) is better than the deterministic WCA algorithm
(4)."
"Passive optical networks are increasingly used for access to the Internet and
it is important to understand the performance of future long-reach,
multi-channel variants. In this paper we discuss requirements on the dynamic
bandwidth allocation (DBA) algorithm used to manage the upstream resource in a
WDM EPON and propose a simple novel DBA algorithm that is considerably more
efficient than classical approaches. We demonstrate that the algorithm emulates
a multi-server polling system and derive capacity formulas that are valid for
general traffic processes. We evaluate delay performance by simulation
demonstrating the superiority of the proposed scheduler. The proposed scheduler
offers considerable flexibility and is particularly efficient in long-reach
access networks where propagation times are high."
"As passive optical networks (PON) are increasingly deployed to provide high
speed Internet access, it is important to understand their fundamental traffic
capacity limits. The paper discusses performance models applicable to
wavelength division multiplexing (WDM) EPONs and GPONs under the assumption
that users access the fibre via optical network units equipped with tunable
transmitters. The considered stochastic models are based on multiserver polling
systems for which explicit analytical results are not known. A large system
asymptotic, mean-field approximation, is used to derive closed form solutions
of these complex systems. Convergence of the mean field dynamics is proved in
the case of a simple network configuration. Simulation results show that, for a
realistic sized PON, the mean field approximation is accurate."
"Initially TCP was designed with the notion in mind that wired networks are
generally reliable and any segment loss in a transmission is due to congestion
in the network rather than an unreliable medium (The assumptions is that the
packet loss caused by damage is much less than 1 percent) . This notion doesnt
hold in wireless parts of the network. Wireless links are highly unreliable and
they lose segments all the time due to a number of factors. Very few papers are
available which uses TCP for MANET. In this paper, an attempt have been made to
justify the use of TCP variants (Tahoe and Reno) for loss of packet due to
random noise introduces in the MANET. For the present analysis the simulation
has been carried out for TCP variants (Tahoe and Reno) by introduces 0, 10, 20
and 30 percent noise. The comparison of TCP variants is made by running
simulation for 0, 10, 20 and 30 percent of data packet loss due to noise in the
transmission link and the effect of throughput and congestion window has been
examined. During the simulation we have observed that throughput has been
decreased when a drop of multiple segments happens, further we have observed in
the case of TCP variant (Reno) throughput is better at 1 percent (Figure 5)
which implies a network with short burst of error and low BER, causing only one
segment to be lost. When multiple segments are lost due to error prone nature
of link, Tahoe perform better than Reno (Figure 13), that gives a significant
saving of time (64.28 percent) in comparison with Reno (Table 4). Several
simulations have been run with ns 2 simulator in order to acquire a better
understanding of these TCP variants and the way they perform their function. We
conclude with a discussion of whether these TCP versions can be used in Mobile
Ad hoc Network."
"Networks on Chip is a recent solution paradigm adopted to increase the
performance of Multicore designs. The key idea is to interconnect various
computation modules (IP cores) in a network fashion and transport packets
simultaneously across them, thereby gaining performance. In addition to
improving performance by having multiple packets in flight, NoCs also present a
host of other advantages including scalability, power efficiency, and component
reuse through modular design. This work focuses on design and development of
high performance communication architectures for FPGAs using NoCs Once
completely developed, the above methodology could be used to augment the
current FPGA design flow for implementing multicore SoC applications. We design
and implement an NoC framework for FPGAs, MultiClock OnChip Network for
Reconfigurable Systems (MoCReS). We propose a novel microarchitecture for a
hybrid two layer router that supports both packetswitched communications,
across its local and directional ports, as well as, time multiplexed
circuitswitched communications among the multiple IP cores directly connected
to it. Results from place and route VHDL models of the advanced router
architecture show an average improvement of 20.4 percent in NoC bandwidth
(maximum of 24 percent compared to a traditional NoC). We parameterize the
hybrid router model over the number of ports, channel width and bRAM depth and
develop a library of network components (MoClib Library). For your paper to be
published in the conference proceedings, you must use this document as both an
instruction set and as a template into which you can type your own text. If
your paper does not conform to the required format, you will be asked to fix
it."
"The design and development of a complex system requires an adequate
methodology and efficient instrumental support in order to early detect and
correct anomalies in the functional and non-functional properties of the tested
protocols. Among the various tools used to provide experimental support for
such developments, network emulation relies on real-time production of
impairments on real traffic according to a communication model, either
realistically or not.
  This paper aims at simply presenting to newcomers in network emulation
(students, engineers, ...) basic principles and practices illustrated with a
few commonly used tools. The motivation behind is to fill a gap in terms of
introductory and pragmatic papers in this domain.
  The study particularly considers centralized approaches, allowing cheap and
easy implementation in the context of research labs or industrial developments.
In addition, an architectural model for emulation systems is proposed, defining
three complementary levels, namely hardware, impairment and model levels. With
the help of this architectural framework, various existing tools are situated
and described. Various approaches for modeling the emulation actions are
studied, such as impairment-based scenarios and virtual architectures,
real-time discrete simulation and trace-based systems. Those modeling
approaches are described and compared in terms of services and we study their
ability to respond to various designer needs to assess when emulation is
needed."
"Dynamic Spectrum Access systems exploit temporarily available spectrum
(`white spaces') and can spread transmissions over a number of non-contiguous
sub-channels. Such methods are highly beneficial in terms of spectrum
utilization. However, excessive fragmentation degrades performance and hence
off-sets the benefits. Thus, there is a need to study these processes so as to
determine how to ensure acceptable levels of fragmentation. Hence, we present
experimental and analytical results derived from a mathematical model. We model
a system operating at capacity serving requests for bandwidth by assigning a
collection of gaps (sub-channels) with no limitations on the fragment size. Our
main theoretical result shows that even if fragments can be arbitrarily small,
the system does not degrade with time. Namely, the average total number of
fragments remains bounded. Within the very difficult class of dynamic
fragmentation models (including models of storage fragmentation), this result
appears to be the first of its kind. Extensive experimental results describe
behavior, at times unexpected, of fragmentation under different algorithms. Our
model also applies to dynamic linked-list storage allocation, and provides a
novel analysis in that domain. We prove that, interestingly, the 50% rule of
the classical (non-fragmented) allocation model carries over to our model.
Overall, the paper provides insights into the potential behavior of practical
fragmentation algorithms."
"Information theoretic Broadcast Channels (BC) and Multiple Access Channels
(MAC) enable a single node to transmit data simultaneously to multiple nodes,
and multiple nodes to transmit data simultaneously to a single node
respectively. In this paper, we address the problem of link scheduling in
multihop wireless networks containing nodes with BC and MAC capabilities. We
first propose an interference model that extends protocol interference models,
originally designed for point to point channels, to include the possibility of
BC and MAC. Due to the high complexity of optimal link schedulers, we introduce
the Multiuser Greedy Maximum Weight algorithm for link scheduling in multihop
wireless networks containing BCs and MACs. Given a network graph, we develop
new local pooling conditions and show that the performance of our algorithm can
be fully characterized using the associated parameter, the multiuser local
pooling factor. We provide examples of some network graphs, on which we apply
local pooling conditions and derive the multiuser local pooling factor. We
prove optimality of our algorithm in tree networks and show that the
exploitation of BCs and MACs improve the throughput performance considerably in
multihop wireless networks."
"Multiple Input Multiple Output (MIMO) systems have recently emerged as a key
technology in wireless communication systems for increasing both data rates and
system performance. There are many schemes that can be applied to MIMO systems
such as space time block codes, space time trellis codes, and the Vertical Bell
Labs Space-Time Architecture (V-BLAST). This paper proposes a novel signal
detector scheme called MIMO detectors to enhance the performance in MIMO
channels. We study the general MIMO system, the general V-BLAST architecture
with Maximum Likelihood (ML), Zero- Forcing (ZF), Minimum Mean- Square Error
(MMSE), and Ordered Successive Interference Cancellation (SIC) detectors and
simulate this structure in Rayleigh fading channel. Also compares the
performances of MIMO system with different modulation techniques in Fading and
AWGN channels. Base on frame error rates and bit error rates, we compare the
performance and the computational complexity of these schemes with other
existence model.Simulations shown that V-BLAST implements a detection
technique, i.e. SIC receiver, based on ZF or MMSE combined with symbol
cancellation and optimal ordering to improve the performance with lower
complexity, although ML receiver appears to have the best SER performance-BLAST
achieves symbol error rates close to the ML scheme while retaining the
lowcomplexity nature of the V-BLAST."
"Denial of service attacks (DoS) can cause significant financial damages.
Flooding and Malicious packets are two kinds of DoS attacks. This paper
presents a new security approach which stops malicious packets and prevents
flooding in the critical systems. New concepts of packet stamp a
dynamic-multi-communication-point mechanism has been identified for this
proposed approach to make the prevention of flooding attacks easier and the
performing of malicious packet attacks harder. In addition, dynamic key
encryption technique has been adapted as a part of the proposed approach to
enhance its functionality."
"One of the biggest drawbacks of the wireless environment is the limited
bandwidth. However, the users sharing this limited bandwidth have been
increasing considerably.Space Division Multiple Access (SDMA) is a new
technology by which the capacity of existing mobile communication systems can
economically be increased. This paper has been presented how the capacity can
be enhanced by using SDMA with smart antennas in mobile communications system.
Based on Adaptive Antenna Array (AAA) technology the spatial dimension of the
existing system is exploited by means of forming independent radio beams in
each of the original channels. This paper analyses the comparison of average
Bit Error Rate (BER) of SDMA and CDMA technique and the different ways in which
SDMA can be introduced to increase the capacity of a cellular system. The
probability of error is found for a standard omni directional base station
antenna, and another set of curves is found for flat top beam having a
directivity of 5.1dB. It is assumed that k separate flat top beams can be
formed by base station and pointed each of the k users within the cell of
interest. Noticing that for an average probability of error greater than 0.1 in
a propagation path loss environment of n=4, the flat top beam will support 200
users, whereas the omni-directional antenna will support only 50 users. This
increase the number of user is roughly equal to the directivity offered by the
flat top beam system, and illustrates the promise SDMA offers for improving
capacity in wireless system. Here multipath fading is not considered."
"Commercial cellular networks, like the systems based on DS-CDMA, face many
types of interferences such as multi-user interference inside each sector in a
cell to interoperate interference. Independent Component Analysis (ICA) has
been used as an advanced preprocessing tool for blind suppression of
interfering signals in DS-CDMA communication systems. The role of ICA is to
provide an interference-mitigated signal to the conventional detection. This
paper evaluates the performance of some major ICA algorithms like Cardoso's
joint approximate diagonalization of eigen matrices (JADE), Hyvarinen's fixed
point algorithm and Comon's algorithm to solve the symbol estimation problem of
the multi users in a DSCDMA communication system. The main focus is on blind
separation of convolved CDMA mixture and the improvement of the downlink symbol
estimation. The results of numerical experiment are compared with those
obtained by the Single User Detection (SUD) receiver, ICA detector and combined
SUD-ICA detector."
"In this paper, Estimation of Distribution Algorithm (EDA) is used for Zone
Routing Protocol (ZRP) in Mobile Ad-hoc Network instead of Genetic Algorithm
(GA). It is an evolutionary approach, it is used when the network size grows
and the search space increases. When the destination is outside the zone, EDA
is applied to find the route with minimum cost and time. Finally, the
implementation of proposed method is compared with Genetic ZRP, i.e., GZRP and
the result demonstrates better performance for the proposed method. Since the
method provides a set of paths to the destination, it results in load balance
to the network. As both EDA and GA use random search method to reach the
optimal point, the searching cost reduced significantly, especially when the
number of data is large."
"MIMO (Multi Input Multi Output) wireless communication system is an
innovative solution to improve the bandwidth efficiency by exploiting
multipath-richness of the propagation environment. The degree of
multipath-richness of the channel will determine the capacity gain attainable
by MIMO deployment. Therefore, it is very important to have accurate knowledge
of the propagation environment/radio channel before MIMO implement. The radio
channel behavior can be estimated by channel measurement or channel sounding.
CDM (Code Division multiplexing) is one of the channel sounding techniques that
allow accurate measurement at the cost of hardware complexity. CDM based
channel sounder, requires code with excellent autocorrelation and
cross-correlation properties which generally difficult to achieve
simultaneously. Theoretical analysis and computer simulation result
demonstrated that, having excellent correlation propertied Loosely Synchronous
(LS) code sequence perform efficiently. Finally, the an efficient LS code
generator as a data source for transmitter implemented in Xilinx FPGA that can
be integrated into CDM based 2x2 MIMO complete channel sounder."
"We propose a scheme to reduce the overhead associated with channel state
information (CSI) feedback required for opportunistic scheduling in
multicarrier access networks. We study the case where CSI is partially
overheard by mobiles and one can suppress transmitting CSI reports for time
varying channel of inferior quality. As a means to assess channel quality and
exploit multiuser diversity we adopt maximum quantile (MQ) scheduling. We show
that the problem of minimizing the average feedback overhead can be formulated
as a Bayesian network problem. A greedy heuristic using probabilistic inference
is proposed to deal with the NP-hardness of the problem. Leveraging properties
of MQ scheduling we first show that networks having tree-like overhearing
graphs admit simple inference. We then present a class of more general network
structures for which exact inference is computationally tractable. Simulation
results are provided to demonstrate the improvements offered by the proposed
heuristic."
"In this paper, we analyze the performance of random load resampling and
migration strategies in parallel server systems. Clients initially attach to an
arbitrary server, but may switch server independently at random instants of
time in an attempt to improve their service rate. This approach to load
balancing contrasts with traditional approaches where clients make smart server
selections upon arrival (e.g., Join-the-Shortest-Queue policy and variants
thereof). Load resampling is particularly relevant in scenarios where clients
cannot predict the load of a server before being actually attached to it. An
important example is in wireless spectrum sharing where clients try to share a
set of frequency bands in a distributed manner."
"Wireless Sensor Networks (WSNs) have gained worldwide attention in recent
years, particularly with the proliferation in Micro-Electro-Mechanical Systems
(MEMS) technology which has facilitated the development of smart sensors. The
paper discusses about classification of WSN and challenges of the Next
Generation WSN. One of the major challenges of Next Generation WSN is reduction
of power consumption. The two approaches are discussed: Ultra-Low-Power
Networks and Energy Harvesting. The paper also discusses about some major
applications as designing low cost secured Intelligent Buildings, In-Home
Health care and Agriculture."
"CDMA is a multiple access method in which the user's uses spread spectrum
techniques and occupy the entire spectrum whenever they transmit. In wireless
communication signal-to-noise ratio (SNR) is the very important parameter that
influences the system performance. Any mode of mobile transmission is not free
from channel impairment such as noise, interference and fading. This channel
impairment caused signal distortion and degradation in SNR.Also there are
differences between uplink (forward channel) and downlink (reverse
channel).Along with these differences, both the links use different codes for
chanellizing the individual users. This paper simulates the expressions for the
pdfs of the SNR for both uplink and downlink transmission assuming that the
system is operating at an average signal-to-noise ratio is 6dB per information
bit."
"The number of users using wireless Local Area Network is increasing
exponentially and their behavior is changing day after day. Nowadays, users of
wireless LAN are using huge amount of bandwidth because of the explosive growth
of some services and applications such as video sharing. This situation imposes
massive pressure on the wireless LAN performance especially in term of fairness
among wireless stations. The limited resources are not distributed fairly in
saturated conditions. The most important resource is the access point buffer
space. This importance is a result of access point being the bottleneck between
two different types of networks. These two types are wired network with
relatively huge bandwidth and wireless network with much smaller bandwidth.
Also the unfairness problem is keep getting worse because of the greedy nature
Transmission Control Protocol (TCP). In this paper, we conduct a comprehensive
study on wireless LAN dynamics and proposed a new mathematical model that
describes the performance and effects of its behavior. We validate the proposed
model by using the simulation technique. The proposed model was able to produce
very good approximation in most of the cases. It also gave us a great insight
into the effective variables in the wireless LAN behavior and what are the
dimensions of the unfairness problem."
"There is now an increased understanding of the need for realistic link layer
models in the wireless sensor networks. In this paper, we have used
mathematical techniques from communication theory to model and analyze low
power wireless links. Our work provides theoretical models for the link layer
showing how Packet Reception Rate vary with Signal to Noise Ratio and distance
for different modulation schemes and a comparison between MICA2 and TinyNode in
terms of PRR."
"We are interested in unicast traffic over wireless networks that employ
constructive inter-session network coding, including single-hop and multi-hop
schemes. In this setting, TCP flows do not fully exploit the network coding
opportunities due to their bursty behavior and due to the fact that TCP is
agnostic to the underlying network coding. In order to improve the performance
of TCP flows over coded wireless networks, we take the following steps. First,
we formulate the problem as network utility maximization and we present a
distributed solution. Second, mimicking the structure of the optimal solution,
we propose a ""network-coding aware"" queue management scheme (NCAQM) at
intermediate nodes; we make no changes to TCP or to the MAC protocol (802.11).
We demonstrate, via simulation, that NCAQM significantly improves TCP
performance compared to TCP over baseline schemes."
"One of the main issues in the design of sensor networks is energy efficient
communication of time-critical data. Energy wastage can be caused by failed
packet transmission attempts at each node due to channel dynamics and
interference. Therefore transmission control techniques that are unaware of the
channel dynamics can lead to suboptimal channel use patterns. In this paper we
propose a transmission controller that utilizes different ""grades"" of channel
side information to schedule packet transmissions in an optimal way, while
meeting a deadline constraint for all packets waiting in the transmission
queue. The wireless channel is modeled as a finite-state Markov channel. We are
specifically interested in the case where the transmitter has low-grade channel
side information that can be obtained based solely on the ACK/NAK sequence for
the previous transmissions. Our scheduler is readily implementable and it is
based on the dynamic programming solution to the finite-horizon transmission
control problem. We also calculate the information theoretic capacity of the
finite state Markov channel with feedback containing different grades of
channel side information including that, obtained through the ACK/NAK sequence.
We illustrate that our scheduler achieves a given throughput at a power level
that is fairly close to the fundamental limit achievable over the channel."
"In this paper correspondence between experimental data for packet delay and
two theoretical types of distribution is investigated. Statistical tests have
shown that only exponential distribution can be used for the description of
packet delays in global network. Precision experimental data to within
microseconds are gathered by means of the RIPE Test Box. Statistical
verification of hypothesis has shown that distribution parameters remain
constants during 500 second intervals at least. In paper cumulative
distribution function and generating function for packet delay in network are
in an explicit form written down, the algorithm of search of parameters of
distribution is resulted."
"The packet is the fundamental unit of transportation in modern communication
networks such as the Internet. Physical layer scheduling decisions are made at
the level of packets, and packet-level models with exogenous arrival processes
have long been employed to study network performance, as well as design
scheduling policies that more efficiently utilize network resources. On the
other hand, a user of the network is more concerned with end-to-end bandwidth,
which is allocated through congestion control policies such as TCP.
Utility-based flow-level models have played an important role in understanding
congestion control protocols. In summary, these two classes of models have
provided separate insights for flow-level and packet-level dynamics of a
network."
"In a network, arrival process is converted into departure process through
network elements. The departure process suffer propagation delay in the link,
processing delay at the network elements like router and data loss due to
buffer overflow or congestion. For providing guaranteed service resources need
to be reserved before conversation takes place. To reserve such resources
estimation of them are indispensable. The idea of service curve gives
beforehand deterministic value of these parameters. In this paper, we aim to
minimum and maximum buffer space required in the router, minimum link capacity
required to guarantee a pre-specified end-to-end delay for an ongoing session
in a wired-cum-wireless scenario by analyzing minimum and maximum service
curve. We assume that the network we are analyzing is an IP based mobile
network. The findings of the work are presented in the form of tables which can
be used for resource reservation to offer quality service to end-users."
"Mobile WiMAX (Worldwide Interoperability for Microwave Access) is being
touted as the most promising and potential broadband wireless technology. And
the popularity rate has been surging to newer heights as the knowledge-backed
service era unfolds steadily. Especially Mobile WiMAX is being projected as a
real and strategic boon for developing counties such as India due to its
wireless coverage acreage is phenomenally high. Mobile WiMAX has spurred
tremendous interest from operators seeking to deploy high-performance yet
cost-effective broadband wireless networks. The IEEE 802.16e standard based
Mobile WiMAX system will be investigated for the purpose of Quality of Service
provisioning. As a technical challenge, radio resource management will be
primarily considered and main is the costly spectrum and the increasingly more
demanding applications with ever growing number of subscribers. It is necessary
to provide Quality of Service (QoS) guaranteed with different characteristics.
As a possible solution the scheduling algorithms will be taken into main
consideration and the present well known algorithms will be described. In this
paper, we have highlighted the following critical issues for Mobile WiMAX
technologies. This paper specifically discussed about the below mentioned in
detail. - QoS Requirements For IEEE 802.16 Service Classes, Achieving efficient
radio resource management. - Deficit Round Robin (DRR) Scheduling algorithm. -
Modified Deficit Round Robin (MDRR) scheduling algorithm's attributes,
properties and architecture. System Model And Scenarios Using OPNET Modeler
Software. - Simulation Limitations And Constraints."
"The Principal Component Analysis (PCA) is a data dimensionality reduction
technique well-suited for processing data from sensor networks. It can be
applied to tasks like compression, event detection, and event recognition. This
technique is based on a linear transform where the sensor measurements are
projected on a set of principal components. When sensor measurements are
correlated, a small set of principal components can explain most of the
measurements variability. This allows to significantly decrease the amount of
radio communication and of energy consumption. In this paper, we show that the
power iteration method can be distributed in a sensor network in order to
compute an approximation of the principal components. The proposed
implementation relies on an aggregation service, which has recently been shown
to provide a suitable framework for distributing the computation of a linear
transform within a sensor network. We also extend this previous work by
providing a detailed analysis of the computational, memory, and communication
costs involved. A compression experiment involving real data validates the
algorithm and illustrates the tradeoffs between accuracy and communication
costs."
"This paper investigates the issue of connectivity of a wireless adhoc network
in the presence of channel impairments. We derive analytical expressions for
the node isolation probability in an adhoc network in the presence of
Nakagami-m fading with superimposed lognormal shadowing. The node isolation
probability is the probability that a randomly chosen node is not able to
communicate with none of the other nodes in the network. An extensive
investigation into the impact of path loss exponent, lognormal shadowing,
Nakagami fading severity index, node density, and diversity order on the node
isolation probability is conducted. The presented results are beneficial for
the practical design of ad hoc networks."
"In this paper we demonstrate that the Canine Pose Estimation (CPE) system can
provide a reliable estimate for some poses and when coupled with effective
wireless transmission over a mesh network. Pose estimates are time sensitive,
thus it is important that pose data arrives at its destination quickly.
Propagation delay and packet delivery ratio measuring algorithms were developed
and used to appraise Wireless Mesh Network (WMN) performance as a means of
carriage for this time-critical data. The experiments were conducted in the
rooms of a building where the radio characteristics closely resembled those of
a partially collapsed building-a typical US&R environment. This paper presents
the results of the experiments, which demonstrate that it is possible to
receive the canine pose estimation data in realtime although accuracy of the
results depend on the network size and the deployment environment."
"Position-based routing protocols take advantage of location information to
perform a stateless and efficient routing. To enable position-based routing, a
node must be able to discover the location of the messages' destination node.
This task is typically accomplished by a location service. Recently, several
location service protocols have been developed for ad hoc networks. In this
paper we propose a novel location service called PHLS: Predictive Hierarchical
Location Service. In PHLS, the entire network is partitioned into a hierarchy
of smaller and smaller regions. For each node, one node in each-level region of
the hierarchy is chosen as its local location server. When the network
initializes or when a node attaches the network, nodes contact their local
location server with their current location information (ie. position and
velocity). Then, they only need to update their location server when they move
away from their current region. Finally, nodes query their location servers and
get the exact or predicted location of destination nodes."
"In this paper, we propose a hybrid medium access control protocol (H-MAC) for
wireless sensor networks. It is based on the IEEE 802.11's power saving
mechanism (PSM) and slotted aloha, and utilizes multiple slots dynamically to
improve performance. Existing MAC protocols for sensor networks reduce energy
consumptions by introducing variation in an active/sleep mechanism. But they
may not provide energy efficiency in varying traffic conditions as well as they
did not address Quality of Service (QoS) issues. H-MAC, the propose MAC
protocol maintains energy efficiency as well as QoS issues like latency,
throughput, and channel utilization. Our numerical results show that H-MAC has
significant improvements in QoS parameters than the existing MAC protocols for
sensor networks while consuming comparable amount of energy."
"We present an evolutionary programming algorithm for solving the dynamic
routing and wavelength assignment (DRWA) problem in optical wavelength-division
multiplexing (WDM) networks under wavelength continuity constraint. We assume
an ideal physical channel and therefore neglect the blocking of connection
requests due to the physical impairments. The problem formulation includes
suitable constraints that enable the algorithm to balance the load among the
individuals and thus results in a lower blocking probability and lower mean
execution time than the existing bio-inspired algorithms available in the
literature for the DRWA problems. Three types of wavelength assignment
techniques, such as First fit, Random, and Round Robin wavelength assignment
techniques have been investigated here. The ability to guarantee both low
blocking probability without any wavelength converters and small delay makes
the improved algorithm very attractive for current optical switching networks."
"In the wireless environment, dissemination techniques may improve data access
for the users. In this paper, we show a description of dissemination
architecture that fits the overall telecommunication network. This architecture
is designed to provide efficient data access and power saving for the mobile
units. A concurrency control approach, MCD, is suggested for data consistency
and conflict checking. A performance study shows that the power consumption,
space overhead, and response time associated with MCD is far less than other
previous techniques."
"This paper is a quantitative analysis on packet switched network with a view
to generalize load balancing and determination of appropriate routing algorithm
in multipath environment. Several routing algorithms have been introduced for
routing of packets from source to destination. Some of them route packets
accurately with increased workload and some of them drastically cut down the
workload. A few of them can find out a minimum workload deviation for both UDP
and TCP packets. We simulated these approaches in a well defined simulator,
analyzed and evaluated their performance. After expanding our analysis with
varying weights and number of paths we found that the recently proposed routing
algorithm Mixed Weighted Fair Routing (MWFR) outperforms the existing routing
algorithms by reducing the routing and network overhead and saving the scarce
bandwidth as well as CPU consumption for packet switching networks."
"Multicast is the ability of a communication network to accept a single
message from an application and to deliver copies of the message to multiple
recipients at different location. With the development of Internet, Multicast
is widely applied in all kinds of multimedia real-time application: distributed
multimedia systems, collaborative computing, video-conferencing, distance
education, etc. In order to construct a delay-constrained multicast routing
tree, average distance heuristic (ADH) algorithm is analyzed firstly. Then a
delay-constrained algorithm called DCADH (delay-constrained average distance
heuristic) is presented. By using ADH a least cost multicast routing tree can
be constructed; if the path delay can't meet the delay upper bound, a shortest
delay path which is computed by Dijkstra algorithm will be merged into the
existing multicast routing tree to meet the delay upper bound. Simulation
experiments show that DCADH has a good performance in achieving a low-cost
multicast routing tree."
"This paper presents a new approach in the management of mobile ad hoc
networks. Our alternative, based on mobile agent technology, allows the design
of mobile centralized server in ad hoc network, where it is not obvious to
think about a centralized management, due to the absence of any administration
or fixed infrastructure in these networks. The aim of this centralized approach
is to provide permanent availability of services in ad hoc networks which are
characterized by a distributed management. In order to evaluate the performance
of the proposed approach, we apply it to solve the problem of mobile code
localization in ad hoc networks. A comparative study, based upon a simulation,
of centralized and distributed localization protocols in terms of messages
number exchanged and response time shows that the centralized approach in a
distributed form is more interesting than a totally centralized approach."
"Peer-to-Peer (P2P) networks provide a significant solution for file sharing
among peers connected to Internet. It is fast and completely decentralised
system with robustness. But due to absence of a server documents on a P2P
network are not rated which makes it difficult for a peer to obtain precise
information in result of a query. In past, some researchers tried to attach
ratings to the peers itself but it was complex and less effective. In this
paper, a novel P2P architecture is proposed which attaches ratings to the
uploaded document directly. These ratings then become as <Rating> element in
its XML advertisement which has several child elements for information
classification. The attached <Rating> element is extracted from the
advertisement in real time and the document is then sorted accordingly.
Therefore, the information can be easily sorted based on a request by a peer
according to the relevance of matter. The information regarding relevance is
obtained by the peer issuing the query. This research leads to a smart P2P
model, the Rated-Resource P2P network (R2P2P)."
"We consider stability and network capacity in discrete time queueing systems.
Relationships between four common notions of stability are described.
Specifically, we consider rate stability, mean rate stability, steady state
stability, and strong stability. We then consider networks of queues with
random events and control actions that can be implemented over time to affect
arrivals and service at the queues. The control actions also generate a vector
of additional network attributes. We characterize the network capacity region,
being the closure of the set of all rate vectors that can be supported subject
to network stability and to additional time average attribute constraints. We
show that (under mild technical assumptions) the capacity region is the same
under all four stability definitions. Our capacity achievability proof uses the
drift-plus-penalty method of Lyapunov optimization, and provides full details
for the case when network states obey a decaying memory property, which holds
for finite state ergodic systems and more general systems."
"LA planning in cellular network is useful for minimizing location management
cost in GSM network. In fact, size of LA can be optimized to create a balance
between the LA update rate and expected paging rate within LA. To get optimal
result for LA planning in cellular network simulated annealing algorithm is
used. Simulated annealing give optimal results in acceptable run-time."
"In recent times the cost of mobile communication has dropped significantly
leading to a dramatic increase in mobile phone usage. The widespread usage has
led mobiles to emerge as a strong alternative for other applications one of
which is tracking. This has enabled law-enforcing agencies to detect
overspeeding vehicles and organizations to keep track its employees. The 3
major ways of tracking being employed presently are (a) via GPS [1] (b) signal
attenuation property of a packet [3] and (c) using GSM Network [2]. The initial
cost of GPS is very high resulting in low usage whereas (b) needs a very high
precision measuring device. The paper presents a GSM-based tracking technique
which eliminates the above mentioned overheads, implements it in NS2 and shows
the limitations of the real life simulation. An accuracy of 97% was achieved
during NS2 simulation which is comparable to the above mentioned alternate
methods of tracking."
"For high data rate ultra wideband communication system, performance
comparison of Rake, MMSE and Rake-MMSE receivers is attempted in this paper.
Further a detail study on Rake-MMSE time domain equalizers is carried out
taking into account all the important parameters such as the effect of the
number of Rake fingers and equalizer taps on the error rate performance. This
receiver combats inter-symbol interference by taking advantages of both the
Rake and equalizer structure. The bit error rate performances are investigated
using MATLAB simulation on IEEE 802.15.3a defined UWB channel models.
Simulation results show that the bit error rate probability of Rake-MMSE
receiver is much better than Rake receiver and MMSE equalizer. Study on
non-line of sight indoor channel models illustrates that bit error rate
performance of Rake-MMSE (both LE and DFE) improves for CM3 model with smaller
spread compared to CM4 channel model. It is indicated that for a MMSE equalizer
operating at low to medium SNR values, the number of Rake fingers is the
dominant factor to improve system performance, while at high SNR values the
number of equalizer taps plays a more significant role in reducing the error
rate."
"There are many challenges when designing and deploying wireless sensor
networks (WSNs). One of the key challenges is how to make full use of the
limited energy to prolong the lifetime of the network, because energy is a
valuable resource in WSNs. The status of energy consumption should be
continuously monitored after network deployment. In this paper, we propose
coverage and connectivity aware neural network based energy efficient routing
in WSN with the objective of maximizing the network lifetime. In the proposed
scheme, the problem is formulated as linear programming (LP) with coverage and
connectivity aware constraints. Cluster head selection is proposed using
adaptive learning in neural networks followed by coverage and connectivity
aware routing with data transmission. The proposed scheme is compared with
existing schemes with respect to the parameters such as number of alive nodes,
packet delivery fraction, and node residual energy. The simulation results show
that the proposed scheme can be used in wide area of applications in WSNs."
"Secured communication in ad hoc wireless networks is primarily important,
because the communication signals are openly available as they propagate
through air and are more susceptible to attacks ranging from passive
eavesdropping to active interfering. The lack of any central coordination and
shared wireless medium makes them more vulnerable to attacks than wired
networks. Nodes act both as hosts and routers and are interconnected by Multi-
hop communication path for forwarding and receiving packets to/from other
nodes. The objective of this paper is to propose a key exchange and encryption
mechanism that aims to use the MAC address as an additional parameter as the
message specific key[to encrypt]and forward data among the nodes. The nodes are
organized in spanning tree fashion, as they avoid forming cycles and exchange
of key occurs only with authenticated neighbors in ad hoc networks, where nodes
join or leave the network dynamically."
"Mobile Ad Hoc Network (MANET) is a collection of two or more devices or nodes
or terminals with wireless communications and networking capability that
communicate with each other without the aid of any centralized administrator
also the wireless nodes that can dynamically form a network to exchange
information without using any existing fixed network infrastructure. And it's
an autonomous system in which mobile hosts connected by wireless links are free
to be dynamically and some time act as routers at the same time, and we discuss
in this paper the distinct characteristics of traditional wired networks,
including network configuration may change at any time, there is no direction
or limit the movement and so on, and thus needed a new optional path Agreement
(Routing Protocol) to identify nodes for these actions communicate with each
other path, An ideal choice way the agreement should not only be able to find
the right path, and the Ad Hoc Network must be able to adapt to changing
network of this type at any time. and we talk in details in this paper all the
information of Mobile Ad Hoc Network which include the History of ad hoc,
wireless ad hoc, wireless mobile approaches and types of mobile ad Hoc
networks, and then we present more than 13 types of the routing Ad Hoc Networks
protocols have been proposed. In this paper, the more representative of routing
protocols, analysis of individual characteristics and advantages and
disadvantages to collate and compare, and present the all applications or the
Possible Service of Ad Hoc Networks."
"In 1991, Gnanajothi [4] proved that the path graph P_n with n vertex and n-1
edge is odd graceful, and the cycle graph C_m with m vertex and m edges is odd
graceful if and only if m even, she proved the cycle graph is not graceful if m
odd. In this paper, firstly, we studied the graph C_m $\cup$ P_m when m = 4,
6,8,10 and then we proved that the graph C_ $\cup$ P_n is odd graceful if m is
even. Finally, we described an algorithm to label the vertices and the edges of
the vertex set V(C_m $\cup$ P_n) and the edge set E(C_m $\cup$ P_n)."
"Cryptography literally means ""The art & science of secret writing & sending a
message between two parties in such a way that its contents cannot be
understood by someone other than the intended recipient"". and Quantum word is
related with ""Light"". Thus, Quantum Cryptography is a way of descripting any
information in the form of quantum particles. There are no classical
cryptographic systems which are perfectly secure. In contrast to Classical
cryptography which depends upon Mathematics, Quantum Cryptography utilizes the
concepts of Quantum Physics which provides us the security against the
cleverest marauders of the present age. In the view of increasing need of
Network and Information Security, we do require methods to overcome the
Molecular Computing technologies (A future technology) and other techniques of
the various codebrakers. Both the parts i.e. Quantum Key distribution and
Information transference from Sender to Receiver are much efficient and secure.
It is based upon BB84 protocol. It can be of great use for Govt. agencies such
as Banks, Insurance, Brokerages firms, financial institutions, e-commerce and
most important is the Defense & security of any country. It is a Cryptographic
communication system in which the original users can detect unauthorized
eavesdropper and in addition it gives a guarantee of no eavesdropping. It
proves to be the ultra secure mode of communication b/w two intended parties."
"In wireless mesh networks such as WLAN (IEEE 802.11s) or WMAN (IEEE 802.11),
each node should help to relay packets of neighboring nodes toward gateway
using multi-hop routing mechanisms. Wireless mesh networks usually intensively
deploy mesh nodes to deal with the problem of dead spot communication. However,
the higher density of nodes deployed, the higher radio interference occurred.
This causes significant degradation of system performance. In this paper, we
first convert network problems into geometry problems in graph theory, and then
solve the interference problem by geometric algorithms. We first define line
intersection in a graph to reflect radio interference problem in a wireless
mesh network. We then use plan sweep algorithm to find intersection lines, if
any; employ Voronoi diagram algorithm to delimit the regions among nodes; use
Delaunay Triangulation algorithm to reconstruct the graph in order to minimize
the interference among nodes. Finally, we use standard deviation to prune off
those longer links (higher interference links) to have a further enhancement.
The proposed hybrid solution is proved to be able to significantly reduce
interference in a wireless mesh network in O(n log n) time complexity."
"Congestion is an important issue which researchers focus on in the
Transmission Control Protocol (TCP) network environment. To keep the stability
of the whole network, congestion control algorithms have been extensively
studied. Queue management method employed by the routers is one of the
important issues in the congestion control study. Active queue management (AQM)
has been proposed as a router-based mechanism for early detection of congestion
inside the network. In this paper we analyzed several active queue management
algorithms with respect to their abilities of maintaining high resource
utilization, identifying and restricting disproportionate bandwidth usage, and
their deployment complexity. We compare the performance of FRED, BLUE, SFB, and
CHOKe based on simulation results, using RED and Drop Tail as the evaluation
baseline. The characteristics of different algorithms are also discussed and
compared. Simulation is done by using Network Simulator(NS2) and the graphs are
drawn using X- graph."
"Disruption Tolerant Networks (DTN) have been a popular subject of recent
research and development. These networks are characterized by frequent, lengthy
outages and a lack of contemporaneous end-to-end paths. In this work we discuss
techniques for extending IP to operate more effectively in DTN scenarios. Our
scheme, Disruption Tolerant IP (DIP) uses existing IP packet headers, uses the
existing socket API for applications, is compatible with IPsec, and uses
familiar Policy-Based Routing techniques for network management."
"Radio Frequency IDentification (RFID) is a dedicated short range
communication technology. The term RFID is used to describe various
technologies that use radio waves to automatically identify people or objects.
RFID is a method of remotely storing and retrieving data using RFID tag. Radio
Frequency Identification (RFID) technology has been attracting considerable
attention with the expectation of improved supply chain visibility for consumer
goods, apparel, and pharmaceutical manufacturers, as well as retailers and
government procurement agencies. RFID technology is used today in many
applications, including security and access control, transportation and supply
chain tracking. Supply Chain Management (SCM) is now at the centre stage of
Manufacturing and service organizations. According to the strategies in
markets, supply chains and logistics are naturally being modelled as
distributed systems. The economic importance has motivated both private
companies and academic researchers to pursue the use of operations research and
management service tools to improve the efficiency of Transportation. Referring
to such scenario, in this work RFID Technique adopted with hybrid algorithm to
optimize supply chain distribution network."
"Hidden nodes in a wireless network refer to nodes that are out of range of
other nodes or a collection of nodes. We will discuss a few problems introduced
by the RTS/CTS mechanism of collision avoidance and focus on the virtual
jamming problem, which allows a malicious node to effectively jam a large
fragment of a wireless network at minimum expense of power. We have also
discussed WiCCP (Wireless Central Coordinated Protocol) which is a protocol
booster that also provides good solution to hidden nodes."
"This paper deals with providing Quality of Service (QoS) over IP based
networks. We are going to give a brief survey about this topic, and present our
work at this area. There are many solutions of the problem, but the
standardization of the methods is not finished yet. At the moment there are two
kinds of approaches of the reservation problem. The distributed method handles
the network nodes independently, and get the nodes making their own admittance
decisions along the reservation path (i.e. Border Gateway Reservation Protocol
BGRP. The centralized way -we discuss in details-, which collects the network
nodes into domains, and handles them using a network manager. Generally there
are two significant parts of the network management: intra domain, and
inter-domain. This article focuses on making reservations over several domains,
which is the part of the inter-domain functions."
"Performance of routing protocols in mobile ad-hoc networks is greatly
affected by the dynamic nature of nodes, route failures, wireless channels with
variable bandwidth and scalability issues. A mobility model imitates the real
world movement of mobile nodes and is central component to simulation based
studies. In this paper we consider mobility nodes which mimic the vehicular
motion of nodes like Manhattan mobility model and City Section mobility model.
We also propose a new Group Vehicular mobility model that takes the best
features of group mobility models like Reference Point Group mobility model and
applies it to vehicular models. We analyze the performance of our model known
as Group Vehicular mobility model (GVMM) and other vehicular mobility models
with various metrics. This analysis provides us with an insight about the
impact of mobility models on the performance of routing protocols for ad-hoc
networks. The routing protocols are simulated and measured for performance and
finally we arrive at the correlation about the impact of mobility models on
routing protocols, which are central to the design of mobile adhoc networks."
"This paper presents the design and development of a context-aware
notification system for university students using RFID technology. This system
is leveraging on the student's matrix card as the RFID tag (sensor), RFID
reader and server as the processors and screen monitor at the various locations
in the campus as the actuator of the output. This system aims to deliver urgent
notifications to the intended students immediately at their respective
locations. In addition, the system is also able to display personalized
information based on the students' preferences and current location when
accessing the system. The background of the study, the design approaches for
this system and the preliminary evaluation of the prototype are presented in
this paper. The evaluation results have indicated that the the proposed system
is useful and easy to use."
"This report deals with security in wireless sensor networks (WSNs),
especially in network layer. Multiple secure routing protocols have been
proposed in the literature. However, they often use the cryptography to secure
routing functionalities. The cryptography alone is not enough to defend against
multiple attacks due to the node compromise. Therefore, we need more
algorithmic solutions. In this report, we focus on the behavior of routing
protocols to determine which properties make them more resilient to attacks.
Our aim is to find some answers to the following questions. Are there any
existing protocols, not designed initially for security, but which already
contain some inherently resilient properties against attacks under which some
portion of the network nodes is compromised? If yes, which specific behaviors
are making these protocols more resilient? We propose in this report an
overview of security strategies for WSNs in general, including existing attacks
and defensive measures. In this report we focus at the network layer in
particular, and an analysis of the behavior of four particular routing
protocols is provided to determine their inherent resiliency to insider
attacks. The protocols considered are: Dynamic Source Routing (DSR),
Gradient-Based Routing (GBR), Greedy Forwarding (GF) and Random Walk Routing
(RWR)."
"Diversity is a powerful means to increase the transmission performance of
wireless communications. For the case of fountain codes relaying, it has been
shown previously that introducing diversity is also beneficial since it
counteracts transmission losses on the channel. Instead of simply hop-by-hop
forwarding information, each sensor node diversifies the information flow using
XOR combinations of stored packets. This approach has been shown to be
efficient for random linear fountain codes. However, random linear codes
exhibit high decoding complexity. In this paper, we propose diversity increased
relaying strategies for the more realistic Luby Transform code in order to
maintain high transmission performance with low decoding computational
complexity in a linear network. Results are provided herein for a linear
network assuming uniform imperfect channel states."
"Traditional end-to-end congestion control mechanisms assume data transferring
happens between each pair user. In contrast, in a P2P network, many peers may
locally keep a copy of a specific data object. If the path between a pair of
peers is congested, the requesting peer who wants to download data will switch
to another peer in its neighbor peer list to fetch the data instead of
decreasing the download rate from the current peer. Thus, it is critical to
study the performance in multi-point-to-multi-point (M2M) transport protocol in
a P2P network. In this paper, we build a mathematical model for identifying the
key parameters for the M2M transport protocol and also the relationships among
these parameters. Finally, we conduct simulation experiments to validate our
model."
"Storage networking technology has enjoyed strong growth in recent years, but
security concerns and threats facing networked data have grown equally fast.
Today, there are many potential threats that are targeted at storage networks,
including data modification, destruction and theft, DoS attacks, malware,
hardware theft and unauthorized access, among others. In order for a Storage
Area Network (SAN) to be secure, each of these threats must be individually
addressed. In this paper, we present a comparative study by implementing
different security methods in IP Storage network."
"Constantly growing demands of high productivity and security of computer
systems and computer networks call the interest of specialists in the
environment of construction of optimum topologies of computer mediums. In
earliest phases of design, the study of the topological influence of the
processes that happen in computer systems and computer networks allows to
obtain useful information which possesses a significant value in the subsequent
design. It has always been tried to represent the different computer network
topologies using appropriate graph models. Graphs have huge contributions
towards the performance improvement factor of a network. Some major
contributors are de-Bruijn, Hypercube, Mesh and Pascal. They had been studied a
lot and different new features were always a part of research outcome. As per
the definition of interconnection network it is equivalent that a suitable
graph can represent the physical and logical layout very efficiently. In this
present study Pascal graph is researched again and a new characteristics has
been discovered. From the perspective of network topologies Pascal graph and
its properties were first studied more than two decades back. Since then, a
numerous graph models have emerged with potentials to be used as network
topologies. This new property is guaranteed to make an everlasting mark towards
the reliability of this graph to be used as a substantial contributor as a
computer network topology. This shows its credentials over so many other
topologies. This study reviews the characteristics of the Pascal graph and the
new property is established using appropriate algorithm and the results."
"VANETs (Vehicular Ad hoc Networks) are highly mobile wireless ad hoc networks
and will play an important role in public safety communications and commercial
applications. Routing of data in VANETs is a challenging task due to rapidly
changing topology and high speed mobility of vehicles. Position based routing
protocols are becoming popular due to advancement and availability of GPS
devices. One of the critical issues of VANETs are frequent path disruptions
caused by high speed mobility of vehicle that leads to broken links which
results in low throughput and high overhead . This paper argues the use of
information on vehicles' movement information (e.g., position, direction, speed
of vehicles) to predict a possible link-breakage event prior to its occurrence.
So in this paper we propose a Reliable Directional Greedy routing (RDGR), a
reliable position based routing approach which obtains position, speed and
direction of its neighboring nodes from GPS. This approach incorporates
potential score based strategy, which calculates link stability between
neighbor nodes in distributed fashion for reliable forwarding of data packet."
"Mobile data services are penetrating mobile markets rapidly. The mobile
industry relies heavily on data service to replace the traditional voice
services with the evolution of the wireless technology and market. A reliable
packet service network is critical to the mobile operators to maintain their
core competence in data service market. Furthermore, mobile operators need to
develop effective operational models to manage the varying mix of voice, data
and video traffic on a single network. Application of statistical models could
prove to be an effective approach. This paper first introduces the architecture
of Universal Mobile Telecommunications System (UMTS) packet switched (PS)
network and then applies multivariate statistical analysis to Key Performance
Indicators (KPI) monitored from network entities in UMTS PS network to guide
the long term capacity planning for the network. The approach proposed in this
paper could be helpful to mobile operators in operating and maintaining their
3G packet switched networks for the long run."
"The wide-band code division multiple access (WCDMA) based 3G and beyond
cellular mobile wireless networks are expected to provide a diverse range of
multimedia services to mobile users with guaranteed quality of service (QoS).
To serve diverse quality of service requirements of these networks it
necessitates new radio resource management strategies for effective utilization
of network resources with coding schemes. Call admission control (CAC) is a
significant component in wireless networks to guarantee quality of service
requirements and also to enhance the network resilience. In this paper capacity
enhancement for WCDMA network with convolutional coding scheme is discussed and
compared with block code and without coding scheme to achieve a better balance
between resource utilization and quality of service provisioning. The model of
this network is valid for the real-time (RT) and non-real-time (NRT) services
having different data rate. Simulation results demonstrate the effectiveness of
the network using convolutional code in terms of capacity enhancement and QoS
of the voice and video services."
"This paper presents a comprehensive analytical study of two competitive
cognitive operators' spectrum leasing and pricing strategies, taking into
account operators' heterogeneity in leasing costs and users' heterogeneity in
transmission power and channel conditions. We model the interactions between
operators and users as a three-stage dynamic game, where operators make
simultaneous spectrum leasing and pricing decisions in Stages I and II, and
users make purchase decisions in Stage III. Using backward induction, we are
able to completely characterize the game's equilibria. We show that both
operators make the equilibrium leasing and pricing decisions based on simple
threshold policies. Moreover, two operators always choose the same equilibrium
price despite their difference in leasing costs. Each user receives the same
signal-to-noise-ratio (SNR) at the equilibrium, and the obtained payoff is
linear in its transmission power and channel gain. We also compare the duopoly
equilibrium with the coordinated case where two operators cooperate to maximize
their total profit. We show that the maximum loss of total profit due to
operators' competition is no larger than 25%. The users, however, always
benefit from operators' competition in terms of their payoffs. We show that
most of these insights are robust in the general SNR regime."
"The development of mobile devices (CPU, memory, and storage) and the
introduction of mobile networks (Ad-Hoc, Wi-Fi, WiMAX, and 3.5G) have opened
new opportunities for next generation of mobile services. It becomes more
convenience and desirable for mobile internet users to be connected everywhere.
However, ubiquitous mobile access connectivity faces interoperation issues
between wireless network providers and wireless network technologies. Although
mobile users would like to get as many services as possible while they travel,
there is a lack of technology to identify visited users in current foreign
network authentication systems. This challenge lies in the fact that a foreign
network provider does not initially have the authentication credentials of a
mobile user. Existing approaches use roaming agreement to exchange
authentication information between home network and foreign network. This paper
proposes a roaming agreement-less approach designed based on our ubiquitous
mobile access model. Our approach consist of two tokens, Passport
(identification token) and Visa (authorisation token) to provide the mobile
user with a flexible authentication method to access foreign network services.
The security analysis indicates that our proposal is more suitable for
ubiquitous mobile communication especially in roaming agreement-less
environment."
"In this paper, we study the impact of network latency on the time required to
download a file distributed using BitTorrent. This study is essential to
understand if testbeds can be used for experimental evaluation of BitTorrent.
We observe that the network latency has a marginal impact on the time required
to download a file; hence, BitTorrent experiments can performed on testbeds."
"Mobile operators currently prefer optimizing their radio networks via
re-homing or cutting over the cell sites in 2G or 3G networks. The core
network, as the parental part of radio network, is inevitably impacted by the
re-homing in radio domain. This paper introduces the cell site re-homing in
radio network and analyzes its impact on the performance of GSM/UMTS core
network. The possible re-homing models are created and analyzed for core
networks. The paper concludes that appropriate re-homing in radio domain, using
correct algorithms, not only optimizes the radio network but also helps improve
the QoS of the core network and saves the carriers' OPEX and CAPEX on their
core networks."
"During the last three decades the Internet has experienced fascinating
evolution, both exponential growth in traffic and rapid expansion in topology.
The size of the Internet becomes enormous, yet the network is very `small' in
the sense that it is extremely efficient to route data packets across the
global Internet. This paper provides a brief review on three fundamental
properties of the Internet topology at the autonomous systems (AS) level.
Firstly the Internet has a power-law degree distribution, which means the
majority of nodes on the Internet AS graph have small numbers of links, whereas
a few nodes have very large numbers of links. Secondly the Internet exhibits a
property called disassortative mixing, which means poorly-connected nodes tend
to link with well-connected nodes, and vice versa. Thirdly the best-connected
nodes, or the rich nodes, are tightly interconnected with each other forming a
rich-club. We explain that it is these structural properties that make the
global Internet so 'small'."
"We consider wireless communication networks where network users are subject
to critical events such as emergencies and crises. If a critical event occurs
to a user, the user needs to send critical traffic as early as possible.
However, most existing medium access control (MAC) protocols are not adequate
to meet the urgent need for data transmission by users with critical traffic.
In this paper, we devise a class of distributed MAC protocols that achieve
coordination using the finite-length memory of users containing their own
observations and traffic types. We formulate a protocol design problem and find
optimal protocols that solve the problem. We show that the proposed protocols
enable a user with critical traffic to transmit its critical traffic without
interruption from other users after a short delay while allowing users to share
the channel efficiently when there is no critical traffic. Moreover, the
proposed protocols require short memory and can be implemented without explicit
message passing."
"Enhancing the current services or deploying new services operating in RF
spectrum requires more licensed spectrum which may not be provided by the
regulatory bodies because of spectrum scarcity. On the contrary, recent studies
suggest that many portions of the licensed spectrum remains unused or underused
for significant period of time raising the issue of spectrum access without
license in an opportunistic manner. Among all the spectrum accessing
techniques, sensing based methods are considered optimal for their simplicity
and cost effectiveness. In this paper, we introduce a new cooperative spectrum
sensing technique which considers the spatial variation of secondary
(unlicensed) users and each user's contribution is weighted by a factor that
depends on received power and path loss. Compared to existing techniques, the
proposed one increases the sensing ability and spectrum utilization, and offers
greater robustness to noise uncertainty. Moreover, this cooperative technique
uses very simple energy detector as its building block thereby reduces the cost
and operational complexity."
"To solve the parameter sensitive issue of the traditional RED (random early
detection) algorithm, an adaptive buffer management algorithm called PAFD
(packet adaptive fair dropping) is proposed. This algorithm supports DiffServ
(differentiated services) model of QoS (quality of service). In this algorithm,
both of fairness and throughput are considered. The smooth buffer occupancy
rate function is adopted to adjust the parameters. By implementing buffer
management and packet scheduling on Intel IXP2400, the viability of QoS
mechanisms on NPs (network processors) is verified. The simulation shows that
the PAFD smoothes the flow curve, and achieves better balance between fairness
and network throughput. It also demonstrates that this algorithm meets the
requirements of fast data packet processing, and the hardware resource
utilization of NPs is higher."
"Mobility management and bandwidth management are two major research issues in
a cellular mobile network. Mobility management consists of two basic
components: location management and handoff management. To Provide QoS to the
users Handoff is a key element in wireless cellular networks. It is often
initiated either by crossing a cell boundary or by deterioration in the quality
of signal in the current channel. In this paper, a new admission control policy
for cellular mobile network is being proposed. Two important QoS parameter in
cellular networks are Call Dropping Probability (CDP) and Handoff Dropping
Probability (HDP). CDP represents the probability that a call is dropped due to
a handoff failure. HDP represents the probability of a handoff failure due to
insufficient available resources in the target cell. Most of the algorithms try
to limit the HDP to some target maximum but not CDP. In this paper, we show
that when HDP is controlled, the CDP is also controlled to a minimum extent
while maintaining lower blocking rates for new calls in the system."
"Voice over Internet protocol (VoIP) is one of the most important applications
for the IEEE 802.11 wireless local area networks (WLANs). For network planners
who are deploying VoIP over WLANs, one of the important issues is the VoIP
capacity. VoIP bandwidth consumption over a WAN is one of the most important
factors to consider when building a VoIP infrastructure. Failure to account for
VoIP bandwidth requirements will severely limit the reliability of a VoIP
system and place a huge burden on the WAN infrastructure. Less bandwidth
utilization is the key reasons for reduced number of channel accesses in VOIP.
But in the QoS point of view the free bandwidth of atleast 1-5% will improve
the voice quality. This proposal utilizes the maximum bandwidth by leaving 1-5%
free bandwidth. A Bandwidth Data rate Moderation (BDM) algorithm has been
proposed which correlates the data rate specified in IEEE802.11b with the free
bandwidth. At each time BDM will calculate the bandwidth utilization before
sending the packet to improve performance and voice quality of VoIP. The
bandwidth calculation in BDM can be done by using Erlang and VOIP bandwidth
calculator. Finally, ns2 experimental study shows the relationship between
bandwidth utilization, free bandwidth and data rate. The paper concludes that
marginal VoIP call rate has been increased by BDM algorithm."
"IEEE 802.16m amends the IEEE 802.16 Wireless MAN-OFDMA specification to
provide an advanced air interface for operation in licenced bands. It will meet
the cellular layer requirements of IMT-Advanced next generation mobile
networks. It will be designed to provide significantly improved performance
compared to other high rate broadband cellular network systems. For the next
generation mobile networks, it is important to consider increasing peak,
sustained data reates, corresponding spectral efficiencies, system capacity and
cell coverage as well as decreasing latency and providing QoS while carefully
considering overall system complexity. In this paper we provide an overview of
the state-of-the-art mobile WiMAX technology and its development. We focus our
discussion on Physical Layer, MAC Layer, Schedular,QoS provisioning and mobile
WiMAX specification."
"In this paper, we propose fairness-oriented packet scheduling (PS) schemes
with power-efficient control mechanism for future packet radio systems. In
general, the radio resource management functionality plays an important role in
new OFDMA based networks. The control of the network resource division among
the users is performed by packet scheduling functionality based on maximizing
cell coverage and capacity satisfying, and certain quality of service
requirements. Moreover, multiantenna transmit-receive schemes provide
additional flexibility to packet scheduler functionality. In order to mitigate
inter-cell and co-channel interference problems in OFDMA cellular networks soft
frequency reuse with different power masks patterns is used. Stemming from the
earlier enhanced proportional fair scheduler studies for single-input
multiple-output (SIMO) and multiple-input multipleoutput (MIMO) systems, we
extend the development of efficient packet scheduling algorithms by adding
transmit power considerations in the overall priority metrics calculations and
scheduling decisions. Furthermore, we evaluate the proposed scheduling schemes
by simulating practical orthogonal frequency division multiple access (OFDMA)
based packet radio system in terms of throughput, coverage and fairness
distribution among users. As a concrete example, under reduced overall transmit
power constraint and unequal power distribution for different sub-bands, we
demonstrate that by using the proposed power-aware multi-user scheduling
schemes, significant coverage and fairness improvements in the order of 70% and
20%, respectively, can be obtained, at the expense of average throughput loss
of only 15%."
"Sensor networks are currently an active research area mainly due to the
potential of their applications. In this paper we investigate the use of
Wireless Sensor Networks (WSN) for air pollution monitoring in Mauritius. With
the fast growing industrial activities on the island, the problem of air
pollution is becoming a major concern for the health of the population. We
proposed an innovative system named Wireless Sensor Network Air Pollution
Monitoring System (WAPMS) to monitor air pollution in Mauritius through the use
of wireless sensors deployed in huge numbers around the island. The proposed
system makes use of an Air Quality Index (AQI) which is presently not available
in Mauritius. In order to improve the efficiency of WAPMS, we have designed and
implemented a new data aggregation algorithm named Recursive Converging
Quartiles (RCQ). The algorithm is used to merge data to eliminate duplicates,
filter out invalid readings and summarise them into a simpler form which
significantly reduce the amount of data to be transmitted to the sink and thus
saving energy. For better power management we used a hierarchical routing
protocol in WAPMS and caused the motes to sleep during idle time."
"Energy is one of the most important and scarce resources in Wireless Sensor
Networks (WSN). WSN nodes work with the embedded operating system called
TinyOS, which addresses the constrains of the WSN nodes such as limited
processing power, memory, energy, etc and it uses the collection Tree Protocol
(CTP) to collect the data from the sensor nodes. It uses either the four-bit
link estimation or Link Estimation Exchange Protocol (LEEP) to predict the bi
directional quality of the wireless link between the nodes and the next hop
candidate is based on the estimated link quality. The residual energy of the
node is an important key factor, which plays a vital role in the lifetime of
the network and hence this has to taken as one of the metric in the parent
selection. In this work, we consider the remaining energy of the node as one of
the metric to decide the parent in addition to the link quality metrics. The
proposed protocol was compared with CTP protocol in terms of number of packets
forwarded by each node and packet reception ratio (PRR) of the network. This
work was simulated in TOSSIM simulator and the same was tested in Crossbow IRIS
radio test bed. The results show that our algorithm performs better than CTP in
terms of load distribution and hence the increased lifetime"
"The nature of Mobile Ad hoc NETworks (MANETs) makes them suitable to be
utilized in the context of an extreme emergency for all involved rescue teams.
We use the term emergency MANETs (eMANETs) in order to describe next generation
IP-based networks, which are deployed in emergency cases such as forest fires
and terrorist attacks. The main goal within the realm of eMANETs is to provide
emergency workers with intelligent devices such as smart phones and PDAs. This
technology allows communication ""islets"" to be established between the members
of the same or different emergency teams (policemen, firemen, paramedics). In
this article, we discuss an adaptive and secure routing protocol developed for
the purposes of eMANETs. We evaluate the performance of the protocol by
comparing it with other widely used routing protocols for MANETs. We finally
show that the overhead introduced due to security considerations is affordable
to support secure ad-hoc communications among lightweight devices."
"The advent of ubiquitous computing and the proliferation of portable
computing devices have raised the importance of mobile ad-hoc network. A major
challenge lies in adapting multicast communication into such environments where
mobility and link failures are inevitable. The purpose of this paper is to
study impact of mobility models in performance of multicast routing protocols
in MANET. In this work, three widely used mobility models such as Random Way
Point, Reference Point Group and Manhattan mobility models and three popular
multicast routing protocols such as On-Demand Multicast Routing Protocol,
Multicast Ad hoc On-demand Distance Vector Routing protocol and Adaptive Demand
driven Multicast Routing protocol have been chosen and implemented in NS2.
Several experiments have been carried out to study the relative strengths,
weakness and applicability of multicast protocols to these mobility models."
"In this paper we develop an integrated model for request mechanism and data
transmission in multi-channel wireless local area networks. We calculated the
performance parameters for single and multi-channel wireless networks when the
channel is noisy. The proposed model is general it can be applied to different
wireless networks such as IEEE802.11x, IEEE802.16, CDMA operated networks and
Hiperlan\2."
"This paper addresses handover decision instability which impacts negatively
on both user perception and network performances. To this aim, a new technique
called The HandOver Decision STAbility Technique (HODSTAT) is proposed for
horizontal handover in Wireless Local Area Networks (WLAN) based on IEEE
802.11standard. HODSTAT is based on a hysteresis margin analysis that, combined
with a utilitybased function, evaluates the need for the handover and
determines if the handover is needed or avoided. Indeed, if a Mobile Terminal
(MT) only transiently hands over to a better network, the gain from using this
new network may be diminished by the handover overhead and short usage
duration. The approach that we adopt throughout this article aims at reducing
the minimum handover occurrence that leads to the interruption of network
connectivity (this is due to the nature of handover in WLAN which is a break
before make which causes additional delay and packet loss). To this end, MT
rather performs a handover only if the connectivity of the current network is
threatened or if the performance of a neighboring network is really better
comparing the current one with a hysteresis margin. This hysteresis should make
a tradeoff between handover occurrence and the necessity to change the current
network of attachment. Our extensive simulation results show that our proposed
algorithm outperforms other decision stability approaches for handover decision
algorithm."
"Recent studies have shown that the majority of today's internet traffic is
related to Peer to Peer (P2P) traffic. The study of bandwidth in P2P networks
is very important. Because it helps us in more efficient capacity planning and
QoS provisioning when we would like to design a large scale computer networks.
In this paper motivated by the behavior of peers (sources or seeds) that is
modeled by Ornstein Uhlenbeck (OU) process, we propose a model for bandwidth in
P2P networks. This model is represented with a stochastic integral. We also
model the bandwidth when we have multiple downloads or uploads. The
autocovariance structure of bandwidth in either case is studied and the
statistical parameters such as mean, variance and autocovariance are obtained.
We then study the queue length behavior of the bandwidth model. The methods for
generating synthetic bandwidth process and estimation of the bandwidth
parameters using maximum likehood estimation are presented."
"In the next generation wireless networks, the growing demand for new wireless
applications is accompanied with high expectations for better quality of
service (QoS) fulfillment especially for multimedia applications. Furthermore,
the coexistence of future unlicensed users with existing licensed users is
becoming a challenging task in the next generation communication systems to
overcome the underutilization of the spectrum. A QoS and interference aware
resource allocation is thus of special interest in order to respond to the
heterogeneous constraints of the next generation networks. In this work, we
address the issue of resource allocation under heterogeneous constraints for
unlicensed multiband ultra-wideband (UWB) systems in the context of Future Home
Networks, i.e. the wireless personal area network (WPAN). The problem is first
studied analytically using a heterogeneous constrained optimization problem
formulation. After studying the characteristics of the optimal solution, we
propose a low-complexity suboptimal algorithm based on a cross-layer approach
that combines information provided by the PHY and MAC layers. While the PHY
layer is responsible for providing the channel quality of the unlicensed UWB
users as well as their interference power that they cause on licensed users,
the MAC layer is responsible for classifying the unlicensed users using a
two-class based approach that guarantees for multimedia services a
high-priority level compared to other services. Combined in an efficient and
simple way, the PHY and MAC information present the key elements of the aimed
resource allocation. Simulation results demonstrate that the proposed scheme
provides a good tradeoff between the QoS satisfaction of the unlicensed
applications with hard QoS requirements and the limitation of the interference
affecting the licensed users."
"In the next generation network (NGN) environment specific consideration is on
bandwidth minimization, because this reduces the cost of network. In response
to the growing market demand for multimedia traffic transmission, NGN concept
has been produced. The next generation network provides multimedia services
over high speed networks, which supports DVD quality video on demand. Although
it has numerous advantages, more exploration of the large-scale deployment
video on demand is still needed. The focus of the research presented in this
paper is a class based admission control by the complete partitioning of the
video on demand server. In this paper we present analytically and by simulation
how the blockage probability of the server significantly affects the on demand
video request and the service. We also present how the blockage probability
affects the performance of the video on demand server."
"Orchestrating a live field trial of wireless mobile networking involves
significant cost and logistical issues relating to mobile platforms, support
personnel, network and experiment automation and support equipment. The
significant cost and logistics required to execute such a field trial can also
be limiting in terms of achieving meaningful test results that exercise a
practical number of mobile nodes over a significant set of test conditions
within a given time. There is no argument that field trials are an important
component of dynamic network testing. A field test of prototype will show
whether simulations were on right track or not, but that's a big leap to take;
going from the simulator directly to the real thing. In conceiving our work, we
envisioned a mobile network emulation system that is low cost, flexible and
controllable. This paper describes our wireless MANET test bed under
development which emulates an actual MANET. Here, we focuses that, this test
bed allows the users to automatically generate arbitrary logically network
topologies in order to perform real time operations on adhoc network at a
relatively low cost in a laboratory environment without having to physically
move the nodes in the adhoc network. Thus, we try to ""compress"" wireless
network so that it fits on a single table."
"Routers are one of the important entities in computer networks specially the
Internet. Forwarding IP packets is a valuable and vital function in Internet
routers. Routers extract destination IP address from packets and lookup those
addresses in their own routing table. This task is called IP lookup. Internet
address lookup is a challenging problem due to the increasing routing table
sizes. Ternary Content-Addressable Memories (TCAMs) are becoming very popular
for designing high-throughput address lookup-engines on routers: they are fast,
cost-effective and simple to manage. Despite the TCAMs speed, their high power
consumption is their major drawback. In this paper, Multilevel Enabling
Technique (MLET), a power efficient TCAM based hardware architecture has been
proposed. This scheme is employed after an Espresso-II minimization algorithm
to achieve lower power consumption. The performance evaluation of the proposed
approach shows that it can save considerable amount of routing table's power
consumption."
"Communication protocols and techniques are often evaluated using simulation
techniques. However, the use of formal modeling and analysis techniques for
verification and evaluation in particular for Wireless Sensor Networks (WSN)
becomes a necessity. In this paper we present a formal analysis of the backoff
procedure integrated in the medium access control protocol named ECo-MAC
designed for WSN. We describe this backoff procedure in terms of discrete time
Markov chains (DTMCs) and evaluated using the well known probabilistic model
checker PRISM. After checking the different invariants of the proposed model,
we study the effect of contention window length (in number of time contention
unit) on the acceptable number of simultaneous senders in a neighborhood of a
given receiver. The obtained quantitative results confirm those provided by the
simulation using OPNET tool and justify the validity of the adopted value for
the time contention unit TCU."
"With the advent of large-scale cloud computing infrastructure, network
extension and migration has emerged as a major challenge in the management of
modern enterprise networks. Many enterprises are considering extending or
relocating their network components, in whole or in part, to remote, private
and public data centers, in order to attain scalability, failure resilience,
and cost savings for their network applications. In this paper, we conduct a
first rigorous study on the extension and migration of an enterprise network
while preserving its performance and security requirements, such as layer
2/layer 3 reachability, and middle-box traversal through load balancer,
intrusion detection and ACLs. We formulate this increasingly important problem,
present preliminary designs, and conduct experiments to validate the
feasibility of our designs."
"Wireless Sensor Networks (WSNs) generate massive amount of live data and
events sensed through dispersedly deployed tiny sensors. This generated data
needed to be disseminate to the sink with slight consumption of network
resources. One of the ways to efficiently transmit this bulk data is gossiping.
An important consideration in gossip-based dissemination protocols is to keep
routing table up to date. Considering the inherent resource constrained nature
of adhoc wireless sensor networks, we propose a gossip based protocol that
consumes little resources. Our proposed scheme aims to keep the routing table
size R as low as possible yet it ensures that the diameter is small too. We
learned the performance of our proposed protocol through simulations. Results
show that our proposed protocol attains major improvement in network
reachability and connectivity."
"For stationary wireless ad hoc networks, one of the key challenging issues in
routing and multicasting is to conserve as much energy as possible without
compromising path efficiency measured as end-to-end delay. In this paper, we
address the problem of path efficient and energy aware multicasting in static
wireless ad hoc networks. We propose a novel distributed scalable algorithm for
finding a virtual multicast backbone (VMB). Based on this VMB, we have further
developed a multicasting scheme that jointly improves path efficiency and
energy conservation. By exploiting inherent broadcast advantage of wireless
communication and employing a more realistic energy consumption model for
wireless communication which not only depends on radio propagation losses but
also on energy losses in transceiver circuitry, our simulation results show
that the proposed VMB-based multicasting scheme outperforms existing prominent
tree based energy conserving, path efficient multicasting schemes."
"Over the years, communication speed of networks has increased from a few Kbps
to several Mbps, as also the bandwidth demand, Communication Protocols, however
have not improved to that extent. With the advent of Wavelength Division
Multiplexing (WDM), it is now possible to ""tune"" protocols to current and
future demands. The purpose of this paper is to evolve a High Speed Network
architecture, which will cater to the needs of bandwidth-consuming
applications, such as voice, video and high definition image transmission."
"In some underwater sensor networks, sensor nodes may be deployed at various
depths of an ocean making those networks three-dimensional (3D). While most
terrestrial sensor networks can usually be modeled as two dimensional (2D)
networks, these underwater sensor networks must be modeled as 3D networks. This
leads to new research challenges in the area of network architecture and
topology. In this paper, we present two different network architectures for 3D
underwater sensor networks. The first one is a hierarchical architecture that
uses a relatively small number of robust backbone nodes to create the network
where a large number of inexpensive sensors communicate with their nearest
backbone nodes, and packets from a backbone node to the sink is routed through
other backbone nodes. This hierarchical approach allows creating a network of
smaller number of expensive backbone nodes while keeping the mobile sensors
simple and inexpensive. Along with network topology, we also study energy
efficiency and frequency reuse issues for such 3D networks. The second approach
is a nonhierarchical architecture which assumes that all nodes are identical
and randomly deployed. It partitions the whole 3D network space into identical
cells and keeps one node active in each cell such that sensing coverage and
connectivity are maintained while limiting the energy consumed. We also study
closeness to optimality of our proposed scheme."
"In the current Internet, there is no clean way for affected parties to react
to poor forwarding performance: when a domain violates its Service Level
Agreement (SLA) with a contractual partner, the partner must resort to ad-hoc
probing-based monitoring to determine the existence and extent of the
violation. Instead, we propose a new, systematic approach to the problem of
forwarding-performance verification. Our mechanism relies on voluntary
reporting, allowing each domain to disclose its loss and delay performance to
its neighbors; it does not disclose any information regarding the participating
domains' topology or routing policies beyond what is already publicly
available. Most importantly, it enables verifiable performance measurements,
i.e., domains cannot abuse it to significantly exaggerate their performance.
Finally, our mechanism is tunable, allowing each participating domain to
determine how many resources to devote to it independently (i.e., without any
inter-domain coordination), exposing a controllable trade-off between
performance-verification quality and resource consumption. Our mechanism comes
at the cost of deploying modest functionality at the participating domains'
border routers; we show that it requires reasonable processing and memory
resources within modern network capabilities."
"Routing the packets efficiently in mobile ad hoc network does not have end to
end paths. Multiple copies are forwarded from the source to the destination. To
deal with such networks, researches introduced flooding based routing schemes
which leads to high probability of delivery. But the flooding based routing
schemes suffered with contention and large delays. Here the proposed protocol
""Spray Select Focus"", sprays a few message copies into the network, neighbors
receives a copy and by that relay nodes we are choosing the shortest route and
then route that copy towards the destination. Previous works assumption is that
there is no contention and dead ends. But we argue that contention and dead
ends must be considered for finding efficiency in routing. So we are including
a network which has contention and dead ends and we applied the proposed
protocol. We can say that this protocol works well for the contention based
network."
"Multicast plays an important role in implementing the group communications in
bandwidth scarce multihop mobile ad hoc networks. However, due to the dynamic
topology of MANETs it is very difficult to build optimal multicast trees and
maintaining group membership, making even more challenging to implement
scalable and robust multicast in Mobile Ad hoc Networks (MANET). A scalable and
energy efficient location aware multicast algorithm, called SEELAMP, for mobile
ad hoc networks is presented in the paper that is based on creation of shared
tree using the physical location of the nodes for the multicast sessions. It
constructs a shared bi-directional multicast tree for its routing operations
rather than a mesh, which helps in achieving more efficient multicast delivery.
The algorithm uses the concept of small overlapped zones around each node for
proactive topology maintenance with in the zone. Protocol depends on the
location information obtained using a distributed location service, which
effectively reduces the overheads for route searching and shared multicast tree
maintenance. In this paper a new technique of local connectivity management is
being proposed that attempts to improve the performance and reliability. It
employs a preventive route reconfiguration to avoid the latency in case of link
breakages and to prevent the network from splitting."
"Wireless Sensor Networks are basically used for gathering information needed
by smart environments but they are particularly useful in unattended situations
where terrain, climate and other environmental constraints may hinder in the
deployment of wired/conventional networks. Unlike traditional networks, these
sensor networks do not have a continuous power supply at their disposal. Rather
the individual sensors are battery operated and the lifetime of the individual
sensors and thus the overall network depend heavily on duty cycle of these
sensors. Analysis on WSNs shows that communication module is the main part
which consumes most of the sensor energy and that is why energy conservation is
the major optimization goal. Since routing protocols and MAC protocols directly
access the communication module therefore the design of protocols in these two
domains should take into account the energy conservation goal. In this paper,
we discuss different state-of-the-art protocols both in MAC and routing domains
that have been proposed for WSNs to achieve the overall goal of prolonging the
network lifetime. The routing protocols in WSNs are generally categorized into
three groups - data centric, hierarchical and location-based but we focus on
only the first two categories because location-based routing protocols
generally require a prior knowledge about sensors location which most of the
times is not available due to random deployment of the sensors. We then discuss
how schedule-based and contention-based MAC protocols can contribute to achieve
optimal utilization of the limited energy resource by avoiding or reducing the
chances of collisions and thus the need for retransmission."
"In recent years, WLAN technology has been gaining popularity around the world
with its sub standard 802.11b receiving major deployments in many indoor and
outdoor environments. In this article we investigate the performance of IEEE
802.11b infrastructure networks in the lossless and lossy environments by means
of a simulation study. Also, this study shows how the FIFO discipline of the
802.11b MAC affects on the global performance when at least one channel is
under the influence of the bursty errors. Furthermore, this paper proposes a
channel aware backoff algorithm for the Access Point (AP) to prioritize its
transmissions and to accelerate the transmissions in the poor radio channels to
enhance the performance of the real time applications. The final results of
this simulation study showed that the proposed algorithm is able to enhance the
throughput and the delay in lossy environment by an average of 49% and 83%
respectively."
"As the demand of, requesting the Internet without any disturbance by the
mobile users of any network is increasing the IETF started working on Network
Mobility (NEMO). Maintaining the session of all the nodes in mobile network
with its home network and external nodes can be provided by the basic Network
Mobility support protocol. It provides mobility at IP level to complete
networks, allowing a Mobile Network to change its point of attachment to the
Internet, while maintaining the ongoing sessions of the nodes of the network.
The Mobile Router (MR) manages the mobility even though the nodes don't know
the status of mobility. This article discusses few basic concepts and
limitations of NEMO protocol and proposes two ways to optimize the NEMO routing
technique for registered and unregistered Correspondent Nodes (CN) of the
Mobile Network Node (MNN)."
This paper has been withdrawn.
"Clustering in wireless sensor networks (WSNs) is an important technique to
ease topology management and routing. Clustering provides an effective method
for prolonging lifetime of a WSN. This paper proposes energy efficient
multi-level clustering schemes for wireless sensor networks. Wireless sensor
nodes are extremely energy constrained with a limited transmission range. Due
to large area of deployment, the network needs to have a multi-level clustering
protocol that will enable far-off nodes to communicate with the base station.
Simulation is used to analyze the proposed protocols and compare their
performance with existing protocol EEMC. Simulation results demonstrate that
our proposed protocols are effective in prolonging the network lifetime."
"Internet faces the problem of congestion due to its increased use. AQM
algorithm is a solution to the problem of congestion control in the Internet.
There are various existing algorithms that have evolved over the past few years
to solve the problem of congestion in IP networks. Congested link causes many
problems such as large delay, underutilization of the link and packet drops in
burst. There are various existing algorithms that have evolved over the past
few years to solve the problem of congestion in IP networks. In this paper,
study of these existing algorithms is done. This paper discusses algorithms
based on various congestion-metrics and classifies them based on certain
factors. This helps in identifying the algorithms that regulate the congestion
more effectively."
"Admission control schemes and scheduling algorithms are designed to offer QoS
services in 802.16/802.16e networks and a number of studies have investigated
these issues. But the channel condition and priority of traffic classes are
very rarely considered in the existing scheduling algorithms. Although a number
of energy saving mechanisms have been proposed for the IEEE 802.16e, to
minimize the power consumption of IEEE 802.16e mobile stations with multiple
real-time connections has not yet been investigated. Moreover, they mainly
consider non real- time connections in IEEE 802.16e networks. In this paper, we
propose to design an adaptive power efficient packet scheduling algorithm that
provides a minimum fair allocation of the channel bandwidth for each packet
flow and additionally minimizes the power consumption. In the adaptive
scheduling algorithm, packets are transmitted as per allotted slots from
different priority of traffic classes adaptively, depending on the channel
condition. Suppose if the buffer size of the high priority traffic queues with
bad channel condition exceeds a threshold, then the priority of those flows
will be increased by adjusting the sleep duty cycle of existing low priority
traffic, to prevent the starvation. By simulation results, we show that our
proposed scheduler achieves better channel utilization while minimizing the
delay and power consumption."
"Mobile Ad hoc NETworks (MANETs) are leaving the confines of research
laboratories, to find place in real-world deployments. Outside specialized
domains (military, vehicular, etc.), city-wide communitynetworks are emerging,
connecting regular Internet users with each other, and with the Internet, via
MANETs. Growing to encompass more than a handful of ""trusted participants"", the
question of preserving the MANET network connectivity, even when faced with
careless or malicious participants, arises, and must be addressed. A first step
towards protecting a MANET is to analyze the vulnerabilities of the routing
protocol, managing the connectivity. By understanding how the algorithms of the
routing protocol operate, and how these can be exploited by those with ill
intent, countermeasures can be developed, readying MANETs for wider deployment
and use. This paper takes an abstract look at the algorithms that constitute
the Optimized Link State Routing Protocol version 2 (OLSRv2), and identifies
for each protocol element the possible vulnerabilities and attacks -- in a
certain way, provides a ""cookbook"" for how to best attack an operational OLSRv2
network, or for how to proceed with developing protective countermeasures
against these attacks."
"Backpressure-based adaptive routing algorithms where each packet is routed
along a possibly different path have been extensively studied in the
literature. However, such algorithms typically result in poor delay performance
and involve high implementation complexity. In this paper, we develop a new
adaptive routing algorithm built upon the widely-studied back-pressure
algorithm. We decouple the routing and scheduling components of the algorithm
by designing a probabilistic routing table which is used to route packets to
per-destination queues. The scheduling decisions in the case of wireless
networks are made using counters called shadow queues. The results are also
extended to the case of networks which employ simple forms of network coding.
In that case, our algorithm provides a low-complexity solution to optimally
exploit the routing-coding tradeoff."
"Wireless sensor networks are conceived to monitor a certain application or
physical phenomena and are supposed to function for several years without any
human intervention for maintenance. Thus, the main issue in sensor networks is
often to extend the lifetime of the network by reducing energy consumption. On
the other hand, some applications have high priority traffic that needs to be
transferred within a bounded end-to-end delay while maintaining an energy
efficient behavior. We propose MaCARI, a time segmentation protocol that saves
energy, improves the overall performance of the network and enables quality of
service in terms of guaranteed access to the medium and end-to-end delays. This
time segmentation is achieved by synchronizing the activity of nodes using a
tree-based beacon propagation and allocating activity periods for each cluster
of nodes. The tree-based topology is inspired from the cluster-tree proposed by
the ZigBee standard. The efficiency of our protocol is proven analytically, by
simulation and through real testbed measurements."
"The IEEE 802.16 technology (WiMAX) is a promising technology for providing
last-mile connectivity by radio link due to its high speed data rates, low cost
of deployment, and large coverage area. However, the maximum number of channels
defined in the current system may cause a potential bottleneck and limit the
overall system capacity. The aim of this paper is to compare the impact on
system performance of different solutions used to mitigate the impairments due
to the radio channel. In particular, taking into account the WiMAX system
capacity as well as application delays, the paper presents the simulation
results obtained when a static QPSK 1/2 Modulation and Coding Scheme (MCS) is
adopted. Then, the study is aimed at evaluating the improvements introduced by
the adoption of an adaptive modulation and coding (AMC) and an AMC jointly with
Hybrid Automatic Repeat reQuest (HARQ). Results indicate that the best strategy
is to use an aggressive AMC table with the HARQ."
"Design and simulation of future mobile networks will center around human
interests and behavior. We propose a design paradigm for mobile networks driven
by realistic models of users' on-line behavior, based on mining of billions of
wireless-LAN records. We introduce a systematic method for large-scale
multi-dimensional coclustering of web activity for thousands of mobile users at
79 locations. We find surprisingly that users can be consistently modeled using
ten clusters with disjoint profiles. Access patterns from multiple locations
show differential user behavior. This is the first study to obtain such
detailed results for mobile Internet usage."
"In a virtualized infrastructure where physical resources are shared, a single
physical server failure will terminate several virtual servers and crippling
the virtual infrastructures which contained those virtual servers. In the worst
case, more failures may cascade from overloading the remaining servers. To
guarantee some level of reliability, each virtual infrastructure, at
instantiation, should be augmented with backup virtual nodes and links that
have sufficient capacities. This ensures that, when physical failures occur,
sufficient computing resources are available and the virtual network topology
is preserved. However, in doing so, the utilization of the physical
infrastructure may be greatly reduced. This can be circumvented if backup
resources are pooled and shared across multiple virtual infrastructures, and
intelligently embedded in the physical infrastructure. These techniques can
reduce the physical footprint of virtual backups while guaranteeing
reliability."
"In this work we introduce the principles of an algorithm that constructs and
maintains a spanning forest in a mobile telecommunication network-a MANET. The
algorithm is based on the random walk of a token and is entirely decentralized.
A probability analysis is performed when the network is static. Then we show
that performances can be slightly enhanced when adding a memory process in the
walk on the token."
"Random scale-free overlay topologies provide a number of properties like for
example high resilience against failures of random nodes, small (average)
diameter as well as good expansion and congestion characteristics that make
them interesting for the use in large-scale distributed systems. A number of
these properties have been shown to be influenced by the exponent \gamma of
their degree distribution P(k) ~ k^{-\gamma}. In this article, we present a
distributed rewiring scheme that is suitable to effectuate scale-free overlay
topologies with an adjustable exponent. The scheme uses a biased random walk
strategy to sample new endpoints of edges being rewired and relies on a simple
equilibrium model for scale-free networks. The bias of the random walk strategy
can be tuned to produce random scale-free networks with arbitrary degree
distribution exponents greater than two. We argue that the rewiring strategy
can be implemented in a distributed fashion based on a node's information about
its immediate neighbors. We present both analytical arguments as well as
results that have been obtained using an implementation of the proposed
protocol."
"The geographical location of Internet IP addresses has an importance both for
academic research and commercial applications. Thus, both commercial and
academic databases and tools are available for mapping IP addresses to
geographic locations. Evaluating the accuracy of these mapping services is
complex since obtaining diverse large scale ground truth is very hard. In this
work we evaluate mapping services using an algorithm that groups IP addresses
to PoPs, based on structure and delay. This way we are able to group close to
100,000 IP addresses world wide into groups that are known to share a
geo-location with high confidence. We provide insight into the strength and
weaknesses of IP geolocation databases, and discuss their accuracy and
encountered anomalies."
"Given a bidirected ring with capacities and a demand graph, we present an
approximation algorithm to the problem of finding the minimum $\alpha$ such
that there exists a feasible unsplittable routing of the demands after
multiplying each capacity by $\alpha$. We also give an approximation scheme to
the problem."
"Space Division Multiple Access (SDMA) based Medium Access Control (MAC)
protocols have been proposed to enable concurrent communications and improve
link throughput in Multi-Input Multi-Output (MIMO) Ad Hoc networks. For the
most part, the works appearing in the literature make idealized and simplifying
assumptions about the underlying physical layer as well as some aspects of the
link adaptation protocol. The result is that the performance predicted by such
works may not necessarily be a good predictor of actual performance in a fully
deployed system. In this paper we look to introduce elements into the SDMA MAC
concept that would allow us to better predict their performance under realistic
operating conditions. Using a generic SDMA-MAC we look at how the network sum
throughput changes with the introduction of the following: $(a)$ use of the
more practical MMSE algorithm instead of the zero-forcing or SVD based nulling
algorithms used for receive beamnulling; $(b)$ impact of channel estimation
errors; $(c)$ introduction of link adaptation mechanism specifically designed
for concurrent SDMA MACs; $(d)$ incorporation of TX beamforming along with RX
beamnulling. Following on the transmission window during which concurrent
transmissions are allowed by the MAC, we qualify the impact of each of these
four elements in isolation. At the conclusion, the performance of a system that
incorporates elements $a-d$ is presented and compared against the baseline
system, showing an improvement of up to 5x in the overall network sum
throughput."
"Multiple Input Multiple Output (MIMO) wireless communication link has been
theoretically proven to be reliable and capable of achieving high capacity.
However, these two advantageous characteristics tend to be addressed separately
in many major researches. Researches on various approaches to attain both
characteristics in a single MIMO system are still on-going and an established
approach is yet to be concluded. To address this problem, in this paper a
Vertical Bell Laboratories Layered Space-Time (V-BLAST) MIMO enhanced with
Rate-Compatible Convolutional (RCPC) codes with Zero Forcing (ZF) and Minimum
Mean Squared Error (MMSE)-based detection is proposed. The analytical BER of
the system is presented and numerically analyzed. The system performance is
analyzed in Nakagami-m fading channel, which provides accuracy and flexibility
in matching the signals statistics compared to other fading models. The
complexity which arises in the calculations of the RCPC codes parameters is
significantly reduced by using equivalent convolutional codes. Results show
that the use of high-rate code allows for bandwidth efficiency and at the same
time does not severely degrades the system performance. It is also shown that
the MMSE-based system outperforms the conventional ZF-based system especially
in the low Eb/N0 region and in severe fading conditions."
"This paper proposes new scheme for efficient rate allocation in conjunction
with reducing peak-to-average power ratio (PAPR) in orthogonal
frequency-division multiplexing (OFDM) systems. Modification of the set
partitioning in hierarchical trees (SPIHT) image coder is proposed to generate
four different groups of bit-stream relative to its significances. The
significant bits, the sign bits, the set bits and the refinement bits are
transmitted in four different groups. The proposed method for reducing the PAPR
utilizes twice the unequal error protection (UEP), using the Read-Solomon codes
(RS), in conjunction with bit-rate allocation and selective interleaving to
provide minimum PAPR. The output bit-stream from the source code (SPIHT) will
be started by the most significant types of bits (first group of bits). The
optimal unequal error protection (UEP) of the four groups is proposed based on
the channel destortion. The proposed structure provides significant improvement
in bit error rate (BER) performance. Performed computer simulations have shown
that the proposed scheme outperform the performance of most of the recent PAPR
reduction techniques in most cases. Moreover, the simulation results indicate
that the proposed scheme provides significantly better PSNR performance in
comparison to well-known robust coding schemes."
"This paper proposes a two-stage optical packet switch with second stage of
recirculate switch of FDL to reduce the number of the FDL used in the switch
for contention resolution. The contention resolution scheme with priority in
packet releasing from FDL is tested in the two-stage switch for performance
evaluation. Simulation result shows that zero packet loss rate achievable with
{\i}< 0.8 for 32x 32 two-stage switch."
"Motivated by MIMO broad-band fading channel model, in this section we deals
with the capacity behaviour of wireless MIMO and OFDM based spatial
multiplexing systems in broad-band fading environments for the case where the
channel is unknown at the transmitter and perfectly known at the receiver. This
influence the propagation and system parameters on ergodic capacity, we
furthermore demonstrate that, unlike the single-input single-output (SISO)
case, delay spread channels may provide advantage over flat-fading channels not
only in terms of outage capacity but also in terms of ergodic capacity.
Therefore, MIMO delay spread channels will in general provide both higher
diversity gain and higher multiplexing gain than MIMO flat-fading channels."
"The topic of this paper is the evaluation of QoS parameters in live Pre-Wimax
environments. The main contribution is the validation of an analytical
delay-jitter behavior model. These models can be used in optimization
algorithms in order to provide opportunistic and reliable all-IP networks. It
allows understanding the impact of the jitter constraints on the throughput and
packet loss in wireless systems. However, we show that the real-time QoS
requirements of real-time and interactive services can be avoided to a large
degree by controlling only the packet delay-jitter in a fixed and mobile
environment. The QoS metrics have been computed from live measurements in a
Pre-Wimax realistic environment (Toulouse/Blagnac Airport)."
"The analysis and simulation of transmit and receive pulse shaping filter is
an important aspect of digital wireless communication since it has a direct
effect on error probabilities. Pulse shaping for wireless communication over
time as well as frequency selective channels is the need of hour for 3G and 4G
systems. The pulse shaping filter is a useful means to shape the signal
spectrum and avoid interferences. Basically digital filters are used to modify
the characteristics of signal in time and frequency domain and have been
recognized as primary digital signal processing operations."
"Voice communication over internet not be possible without a reliable data
network, this was first available when distributed network topologies were used
in conjunction with data packets. Early network used single centre node network
in which a single workstation (Server) is responsible for the communication.
This posed problems as if there was a fault with the centre node, (workstation)
nothing would work. This problem was solved by the distributed system in which
reliability increases by spreading the load between many nodes. The idea of
packet switching & distributed network were combined, this combination were
increased reliability, speed & responsible for voice communication over
internet, Voice-over-IP (VoIP)These data packets travel through a
packet-switched network such as the Internet and arrive at their destination
where they are decompressed using a compatible Codec (audio coder/decoder) and
converted back to analogue audio. This paper deals with the Simulink
architecture for VoIP network."
"Handoff has become an inevitable part of wireless cellular communication,
Soon users will carry small portable handheld devices which will incorporate
the computer, phone, camera, GPS, personal control module etc. This paper
proposes a new scheme to deal with seam less roaming and reduce failed
handoffs. The simulation is done using software called Qualnet meant for
wireless communication. The results clearly indicate the advantages of this new
scheme."
"This paper presents a Q-learning based scheme for managing the partial
coverage problem and the ill-effects of free riding in unstructured P2P
networks. Based on various parameter values collected during query routing,
reward for the actions are computed and these rewards are used for updating the
corresponding Q-values of peers. Thus, the routing is done through only nodes
which have shown high performance in the past. Simulation experiments are
conducted in several times and the results are plotted. Results show that the
proposed scheme effectively manages free riders, generates high hit ratio,
reduces network traffic and manages partial coverage problem."
"This paper presents a solution for reducing the ill effects of free-riders in
decentralised unstructured P2P networks. An autonomous replication scheme is
proposed to improve the availability and enhance system performance. Q-learning
is widely employed in different situations to improve the accuracy in decision
making by each peer. Based on the performance of neighbours of a peer, every
neighbour is awarded different levels of ranks. At the same time a
low-performing node is allowed to improve its rank in different ways.
Simulation results show that Q-learning based free riding control mechanism
effectively limits the services received by free-riders and also encourages the
low-performing neighbours to improve their position. The popular files are
autonomously replicated to nodes possessing required parameters. Due to this
improvement of quantity of popular files, free riders are given opportunity to
lift their position for active participation in the network for sharing files.
Q-feed effectively manages queries from free riders and reduces network traffic
significantly"
"In this paper, we consider the problem of blocking malicious traffic on the
Internet, via source-based filtering. In particular, we consider filtering via
access control lists (ACLs): these are already available at the routers today
but are a scarce resource because they are stored in the expensive ternary
content addressable memory (TCAM). Aggregation (by filtering source prefixes
instead of individual IP addresses) helps reduce the number of filters, but
comes also at the cost of blocking legitimate traffic originating from the
filtered prefixes. We show how to optimally choose which source prefixes to
filter, for a variety of realistic attack scenarios and operators' policies. In
each scenario, we design optimal, yet computationally efficient, algorithms.
Using logs from Dshield.org, we evaluate the algorithms and demonstrate that
they bring significant benefit in practice."
"The area of mobile ad hoc networking has received considerable attention of
the research community in recent years. These networks have gained immense
popularity primarily due to their infrastructure-less mode of operation which
makes them a suitable candidate for deployment in emergency scenarios like
relief operation, battlefield etc., where either the pre-existing
infrastructure is totally damaged or it is not possible to establish a new
infrastructure quickly. However, MANETs are constrained due to the limited
transmission range of the mobile nodes which reduces the total coverage area.
Sometimes the infrastructure-less ad hoc network may be combined with a fixed
network to form a hybrid network which can cover a wider area with the
advantage of having less fixed infrastructure. In such a combined network, for
transferring data, we need base stations which act as gateways between the
wired and wireless domains. Due to the hybrid nature of these networks, routing
is considered a challenging task. Several routing protocols have been proposed
and tested under various traffic conditions. However, the simulations of such
routing protocols usually do not consider the hybrid network scenario. In this
work we have carried out a systematic performance study of the two prominent
routing protocols: Destination Sequenced Distance Vector Routing (DSDV) and
Dynamic Source Routing (DSR) protocols in the hybrid networking environment. We
have analyzed the performance differentials on the basis of three metrics -
packet delivery fraction, average end-to-end delay and normalized routing load
under varying pause time with different number of sources using NS2 based
simulation."
"In this paper, we study stability and latency of routing in wireless networks
where it is assumed that no collision will occur. Our approach is inspired by
the adversarial queuing theory, which is amended in order to model wireless
communication. More precisely, there is an adversary that specifies
transmission rates of wireless links and injects data in such a way that an
average number of data injected in a single round and routed through a single
wireless link is at most $r$, for a given $r\in (0,1)$. We also assume that the
additional ""burst"" of data injected during any time interval and scheduled via
a single link is bounded by a given parameter $b$.
  Under this scenario, we show that the nodes following so called {\em
work-conserving} scheduling policies, not necessarily the same, are guaranteed
stability (i.e., bounded queues) and reasonably small data latency (i.e.,
bounded time on data delivery), for injection rates $r<1/d$, where $d$ is the
maximum length of a routing path. Furthermore, we also show that such a bound
is asymptotically optimal on $d$."
"This paper experimentally evaluates the effects of applying autonomic
management to the scheduling of maintenance operations in a deployed Chord
network, for various membership churn and workload patterns. Two versions of an
autonomic management policy were compared with a static configuration. The
autonomic policies varied with respect to the aggressiveness with which they
responded to peer access error rates and to wasted maintenance operations. In
most experiments, significant improvements due to autonomic management were
observed in the performance of routing operations and the quantity of data
transmitted between network members. Of the autonomic policies, the more
aggressive version gave slightly better results."
"Throughput improvement of the Wireless LANs has been a constant area of
research. Most of the work in this area, focuses on designing throughput
optimal schemes for fully connected networks (no hidden nodes). But, we
demonstrate that the proposed schemes, though perform optimally in fully
connected network, achieve significantly lesser throughput even than that of
standard IEEE 802.11 in a network with hidden nodes. This motivates the need
for designing schemes that provide near optimal performance even when hidden
nodes are present. The primary reason for the failure of existing protocols in
the presence of hidden nodes is that these protocols are based on the model
developed by Bianchi. However this model does not hold when hidden nodes exist.
Moreover, analyzing networks with hidden nodes is still an open problem. Thus,
designing throughput optimal schemes in networks with hidden nodes is
particularly challenging. The novelty of our approach is that it is not based
on any underlying mathematical model, rather it directly tunes the control
variables so as to maximize the throughput. We demonstrate that this model
independent approach achieves maximum throughput in networks with hidden
terminals as well. Apart from this major contribution, we present stochastic
approximation based algorithms for achieving weighted fairness in a connected
networks. We also present a throughput optimal exponential backoff based random
access algorithm. We demonstrate that the exponential backoff based scheme may
outperform an optimal p-persistent scheme in networks with hidden terminals.
This demonstrates the merit of exponential backoff based random access schemes
which was deemed unnecessary by results shown by Bianchi."
"A new generation of ""behavior-aware"" delay tolerant networks is emerging in
what may define future mobile social networks. With the introduction of novel
behavior-aware protocols, services and architectures, there is a pressing need
to understand and realistically model mobile users behavioral characteristics,
their similarity and clustering. Such models are essential for the analysis,
performance evaluation, and simulation of future DTNs. This paper addresses
issues related to mobile user similarity, its definition, analysis and
modeling. To define similarity, we adopt a behavioral-profile based on users
location preferences using their on-line association matrix and its SVD, then
calculate the behavioral distance to capture user similarity. This measures the
difference of the major spatio-temporal behavioral trends and can be used to
cluster users into similarity groups or communities. We then analyze and
contrast similarity distributions of mobile user populations in two settings:
(i) based on real measurements from four major campuses with over ten thousand
users for a month, and (ii) based on existing mobility models, including random
direction and time-varying community models. Our results show a rich set of
similar communities in real mobile societies with distinct behavioral clusters
of users. This is true for all the traces studied, with the trend being
consistent over time. Surprisingly, however, we find that the existing mobility
models do not explicitly capture similarity and result in homogeneous users
that are all similar to each other. Thus the richness and diversity of user
behavioral patterns is not captured to any degree in the existing models. These
findings strongly suggest that similarity should be explicitly captured in
future mobility models, which motivates the need to re-visit mobility modeling
in the future."
"In the literature of Round-Robin scheduling scheme, each job is processed,
one after the another after giving a fix quantum. In case of First-come
first-served, each process is executed, if the previously arrived processed is
completed. Both these scheduling schemes are used in this paper as its special
cases. A Markov chain model is used to compare several scheduling schemes of
the class. An index measure is defined to compare the model based efficiency of
different scheduling schemes. One scheduling scheme which is the mixture of
FIFO and round robin is found efficient in terms of model based study. The
system simulation procedure is used to derive the conclusion of the content"
"In this paper,a new design of wireless sensor network (WSN)node is discussed
which is based on components with ultra low power.We ha e de eloped a Low cost
and low power WSN Node using MSP430 and nRF24L01.The architectural circuit
details are presented.This architecture fulfils the requirements like low
cost,low power,compact size and self organization.Various tests are carried out
to test the performance of the nRF24L01 module.The packet loss,free Space loss
(FSL)and battery lifetime calculations are described.These test results will
help the researchers to build new applications using abo e node and to work
efficiently with nRF24L01."
"The associatie memory feature of the Hopfield type recurrent neural network
is used for the pattern storage and pattern authentication.This paper outlines
an optimization relaxation approach for signature verification based on the
Hopfield neural network (HNN)which is a recurrent network.The standard sample
signature of the customer is cross matched with the one supplied on the
Cheque.The difference percentage is obtained by calculating the different
pixels in both the images.The network topology is built so that each pixel in
the difference image is a neuron in the network.Each neuron is categorized by
its states,which in turn signifies that if the particular pixel is changed.The
network converges to unwavering condition based on the energy function which is
derived in experiments.The Hopfield's model allows each node to take on two
binary state values (changed/unchanged)for each pixel.The performance of the
proposed technique is evaluated by applying it in various binary and gray scale
images.This paper contributes in finding an automated scheme for verification
of authentic signature on bank Cheques.The derived energy function allows a
trade off between the influence of its neighborhood and its own criterion.This
device is able to recall as well as complete partially specified inputs.The
network is trained via a storage prescription that forces stable states to
correspond to (local)minima of a network ""energy"" function."
"In the faceless world of the Internet,online fraud is one of the greatest
reasons of loss for web merchants.Advanced solutions are needed to protect e
businesses from the constant problems of fraud.Many popular fraud detection
algorithms require supervised training,which needs human intervention to
prepare training cases.Since it is quite often for an online transaction
database to ha e Terabyte level storage,human investigation to identify
fraudulent transactions is very costly.This paper describes the automatic
design of user profiling method for the purpose of fraud detection.We use a FP
(Frequent Pattern) Tree rule learning algorithm to adaptively profile
legitimate customer behavior in a transaction database.Then the incoming
transactions are compared against the user profile to uncover the anomalies The
anomaly outputs are used as input to an accumulation system for combining
evidence to generate high confidence fraud alert value. Favorable experimental
results are presented."
"Reliable transport protocols such as TCP are tuned to perform well in
traditional networks where packet losses occur mostly because of congestion.
Many applications of wireless sensor networks are useful only when connected to
an external network. Previous research on transport layer protocols for sensor
networks has focused on designing protocols specifically targeted for sensor
networks. The deployment of TCP/IP in sensor networks would, however, enable
direct connection between the sensor network and external TCP/IP networks. In
this paper we focus on the performance of TCP in the context of wireless sensor
networks. TCP is known to exhibit poor performance in wireless environments,
both in terms of throughput and energy efficiency. To overcome these problems
we introduce a mechanism called TCP Segment Caching .We show by simulation that
TCP Segment Caching significantly improves TCP Performance so that TCP can be
useful e en in wireless sensor"
"Computer system models provide detailed answer to system performance.In this
paper a two stage tandem network system with Blocking and Feedback is
considered and it performance has been analyzed by spectral expansion
method.The study state system with balance equations has been discussed."
"Campus Grid computing involves heterogeneous resources of an organization
working in collaboration to sol e the problems that cannot be addressed by a
single resource. However, basic problem for Campus Grid users is how to disco
er the best resources required for the particular type of a job. There are
various approaches using which Grid Discovery can be performed. This paper pro
ides the grid resource discovery solutions for Campus Grid using Globus Toolkit
which will enable us to customize the resource information according to the
requirements based on the jobs to be run on the Campus Grid and present it in
our own format. Here we propose building up our own service on top of globus
MDS in order to process the information provided by MDS and use it in our
Campus Grid Portal."
"computers into the real world, to serve humans where the ubiquitous network
is the underneath infrastructure. In order to provide ubiquitous services
(u-Service) which deliver useful information to service users without human
intervention, this paper implements a proactive information delivery system
using Bluetooth technology. Bluetooth is a lowpowered networking service that
supports several protocol profiles, most importantly file transfer.Combined
together, ubiquitous computing and Bluetooth ha e the potential to furnish
ubiquitous solutions (u-Solutions) that are efficient, employ simplified design
characteristics, and collaboratively perform functions they are otherwise not
capable. Thus, this paper first addresses the current Bluetooth technology.
Then, it suggests and develops the proactive information delivery system
utilizing Bluetooth and ubiquitous computing network concepts. The proactive
information delivery system can be used in many ubiquitous applications such as
ubiquitous commerce (u-Commerce) and ubiquitous education (u- Education)"
"The Model / View / Controller design pattern divides an application
environment into three components to handle the user-interactions, computations
and output respectively. This separation greatly favors architectural
reusability. The pattern works well in the case of single-address space and not
proven to be efficient for web applications involving multiple address spaces.
Web applications force the designers to decide which of the components of the
pattern are to be partitioned between the server and client(s) before the
design phase commences. For any rapidly growing web application, it is very
difficult to incorporate future changes in policies related to partitioning.
One solution to this problem is to duplicate the Model and controller
components at both server and client(s). However, this may add further problems
like delayed data fetch, security and scalability issues. In order to overcome
this, a new architecture SPIM has been proposed that deals with the
partitioning problem in an alternative way. SPIM shows tremendous improvements
in performance when compared with a similar architecture."
"IEEE 802.16 standard supports two different topologies: point to multipoint
(PMP) and Mesh. In this paper, a QoS mechanism for point to multipoint of IEEE
802.16 and BS scheduler for PMP Mode is proposed. This paper also describes
quality of service over WiMAX networks. Average WiMAX delay, Average WiMAX load
and Average WiMAX throughput at base station is analyzed and compared by
applying different scheduler at Base station and at fixed nodes."
"As the technology rapidly grows, the trend is clear that the use of mobile
devices is gain an attention nowadays, thus designing a system by integrating
it with notification feature is becoming an important aspect especially in
tracking and monitoring system. Conventional security surveillance systems
require the constant attention from the user, to monitor the location
concurrently. In order to reduce the cost of computing power and advance
technology of mobile phone in widespread acceptance of the Internet as a viable
communication medium, this paper is aimed to design a low cost web-based system
as a platform to view the image captured. When the network camera detects any
movement from the intruders, it automatically captures the image and sends it
to the database of the web-based directly by the network through File Transfer
Protocol (FTP). The camera is attached through an Ethernet connection and power
source. Therefore, the camera can be viewed from either standard Web browser or
cell phone. Nowadays, when a security camera is installed, user is notified as
long as the camera is switched on since any slight movement requires the
attention of the supervisor. The utility of the system has proven
theoretically. This system will also notify the user by sending a notification
through Short Messages Services (SMS)."
"Denial-of-service (DOS) attacks increasingly gained reputation over the past
few years. As the Internet becomes more ubiquitous, the threat of the
denial-of-service attacks becomes more realistic and important for individuals,
businesses, governmental organizations, and even countries. There is intensive
need to detect an attack in progress as soon as possible. The efficiency of
diagnosing the DOS attack using concepts of queuing theory and performance
parameter of the system has been investigated in the present work, as the
servers definitely have some mechanisms to store and process the requests.
Utilizing this concept of queuing theory, the collection of data patterns were
generated. With the performance parameter of the system, the analysis of the
data pattern had been made to diagnose the network anomaly. Performance
analysis and results show the accuracy of the proposed scheme in detecting
anomalies."
"The widespread emergence of the Internet as a platform for electronic data
distribution and the advent of structured information have revolutionized our
ability to deliver information to any corner of the world. Although Service
Oriented Architecture (SOA) is a paradigm for organizing and utilizing
distributed capabilities that may be under the control of different ownership
domains and implemented using various technology stacks and every organization
may not be geared up for this. To harness the various software / service
resources placed on various systems, we have proposed and implemented a model
that is able to establish discovery and sharing in load balanced P-grid
environment. The experimental results show that the proposed approach has
dramatically lowered the network traffic (nearly negligible), while achieving
load balancing in P2P grid systems. Our model is able to support discovery and
sharing of resources also."
"Last year, the official BitTorrent client switched to LEDBAT, a new
congestion control algorithm targeting a lower-than Best Effort transport
service. In this paper, we study this new protocol through packet-level
simulations, with a special focus on a performance comparison with other
lower-than Best Effort protocols such as TCP-LP and TCP-NICE: our aim is indeed
to quantify and relatively weight the level of Low-priority provided by such
protocols.
  Our results show that LEDBAT transport generally achieves the lowest possible
level of priority, with the default configurations of TCP-NICE and TCP-LP
representing increasing levels of aggressiveness. In addition, we perform a
careful sensitivity analysis of LEDBAT performance, by tuning its main
parameters in both inter-protocol (against TCP) and intra-protocol (against
LEDBAT itself) scenarios. In the inter-protocol case, although in case of
misconfiguration LEDBAT competes as aggressively as TCP, however we show that
it is not possible to achieve an arbitrary level of low-priority by merely
tuning its parameters. In the intra-protocol case, we show that coexistence of
legacy flows with slightly dissimilar settings, or experiencing different
network conditions, can result in significant unfairness."
"BitTorrent developers have recently introduced a new application layer
congestion control algorithm based on UDP framing at transport layer and
currently under definition at the IETF LEDBAT Working Group. LEDBAT is a
delay-based protocol which aims at offering a ""lower than Best Effort"" data
transfer service, with a lower priority with respect to elastic TCP and
interactive traffic (e.g., VoIP, game). However, in its current specification,
LEDBAT is affected by a late-comer advantage: indeed the last flow arriving at
the bottleneck is more aggressive due to a wrong estimation of the base delay
and finally takes over all resources. In this work, we study several solutions
to the late-comer problem by means of packet level simulations and simple
analysis: in the investigation process, we individuate the root cause for
LEDBAT unfairness and propose effective countermeasures."
"This paper presents a thorough survey of recent work addressing energy
efficient multicast routing protocols and secure multicast routing protocols in
Mobile Ad hoc Networks (MANETs). There are so many issues and solutions which
witness the need of energy management and security in ad hoc wireless networks.
The objective of a multicast routing protocol for MANETs is to support the
propagation of data from a sender to all the receivers of a multicast group
while trying to use the available bandwidth efficiently in the presence of
frequent topology changes. Multicasting can improve the efficiency of the
wireless link when sending multiple copies of messages by exploiting the
inherent broadcast property of wireless transmission. Secure multicast routing
plays a significant role in MANETs. However, offering energy efficient and
secure multicast routing is a difficult and challenging task. In recent years,
various multicast routing protocols have been proposed for MANETs. These
protocols have distinguishing features and use different mechanisms"
"In sensor networks communication by broadcast methods involves many hazards,
especially collision. Several MAC layer protocols have been proposed to resolve
the problem of collision namely ARBP, where the best achieved success rate is
90%. We hereby propose a MAC protocol which achieves a greater success rate
(Success rate is defined as the percentage of delivered packets at the source
reaching the destination successfully) by reducing the number of collisions,
but by trading off the average propagation delay of transmission. Our proposed
protocols are also shown to be more energy efficient in terms of energy
dissipation per message delivery, compared to the currently existing protocol."
"Managing network complexity, accommodating greater numbers of subscribers,
improving coverage to support data services (e.g. email, video, and music
downloads), keeping up to speed with fast-changing technology, and driving
maximum value from existing networks - all while reducing CapEX and OpEX and
ensuring Quality of Service (QoS) for the network and Quality of Experience
(QoE) for the user. These are just some of the pressing business issues faced
by mobileservice providers, summarized by the demand to ""achieve more, for
less."" The ultimate goal of optimization techniques at the network and
application layer is to ensure End-user perceived QoS. The next generation
networks (NGN), a composite environment of proven telecommunications and
Internet-oriented mechanisms have become generally recognized as the
telecommunications environment of the future. However, the nature of the NGN
environment presents several complex issues regarding quality assurance that
have not existed in the legacy environments (e.g., multi-network, multi-vendor,
and multi-operator IP-based telecommunications environment, distributed
intelligence, third-party provisioning, fixed-wireless and mobile access,
etc.). In this Research Paper, a service aware policy-based approach to NGN
quality assurance is presented, taking into account both perceptual quality of
experience and technologydependant quality of service issues. The respective
procedures, entities, mechanisms, and profiles are discussed. The purpose of
the presented approach is in research, development, and discussion of pursuing
the end-to-end controllability of the quality of the multimedia NGN-based
communications in an environment that is best effort in its nature and promotes
end user's access agnosticism, service agility, and global mobility."
"Asterisk and Open IMS use SIP signal protocol to enable both of them can be
connected. To facilitate both relationships, Enum server- that is able to
translate the numbering address such as PSTN (E.164) to URI address (Uniform
Resource Identifier)- can be used. In this research, we interconnect Open IMS
and Asterisk server Enum server. We then analyze the server performance and PDD
(Post Dial Delay) values resulted by the system. As the result of the
experiment, we found that, for a call from Open IMS user to analog Asterisk
telephone (FXS) with a arrival call each servers is 30 call/sec, the maximum
PDD value is 493.656 ms. Open IMS is able to serve maximum 30 call/s with
computer processor 1.55 GHz, while the Asterisk with computer processor 3.0
GHz, may serve up to 55 call/sec. Enum on server with 1.15 GHz computer
processor have the capability of serving maximum of 8156 queries/sec."
"In this paper we are proposing a new concept in MAC layer protocol design for
Cognitive radio by combining information held by physical layer and MAC layer
with analytical engine based on knowledge based reasoning approach. In the
proposed system a cross layer information regarding signal to interference and
noise ratio (SINR) and received power are analyzed with help of knowledge based
reasoning system to determine minimum power to transmit and size of contention
window, to minimize backoff, collision, save power and drop packets. The
performance analysis of the proposed protocol indicates improvement in power
saving, lowering backoff and significant decrease in number of drop packets.
The simulation environment was implement using OMNET++ discrete simulation tool
with Mobilty framework and MiXiM simulation library."
"Nowadays, It has been shown that spectrum scarcity increased due to
tremendous growth of new players in wireless base system by the evolution of
the radio communication. Resent survey found that there are many areas of the
radio spectrum that are occupied by authorized user/primary user (PU), which
are not fully utilized. Cognitive radios (CR) prove to next generation wireless
communication system that proposed as a way to reuse this under-utilised
spectrum in an opportunistic and non-interfering basis. A CR is a self-directed
entity in a wireless communications environment that senses its environment,
tracks changes, and reacts upon its findings and frequently exchanges
information with the networks for secondary user (SU). However, CR facing
collision problem with tracks changes i.e. reallocating of other empty channels
for SU while PU arrives. In this paper, channels reallocation technique based
on DNA sequence alignment algorithm for CR networks has been proposed."
"During the past few years, advances in mobile communication theory have
enabled the development and deployment of different wireless technologies,
complementary to each other. Hence, their integration can realize a unified
wireless system that has the best features of the individual networks.
Next-Generation Wireless Systems (NGWS) integrate different wireless systems,
each of which is optimized for some specific services and coverage area to
provide ubiquitous communications to the mobile users. In this paper, we
propose to enhance the handoff performance of mobile IP in wireless IP networks
by reducing the false handoff probability in the NGWS handoff management
protocol. Based on the information of false handoff probability, we analyze its
effect on mobile speed and handoff signaling delay."
"Delay/Disruption-Tolerant Network (DTN) protocols typically address sparse
intermittently connected networks whereas Mobile Ad-hoc Network (MANET)
protocols address the fairly stable and fully connected ones. But many
intermediate situations may occur on mobility dynamics or radio link
instability. In such cases, where the network frequently splits into evolving
connected groups, none of the conventional routing paradigms (DTN or MANET) are
fully satisfactory. In this paper we propose HYMAD, a Hybrid DTN-MANET routing
protocol which uses DTN between disjoint groups of nodes while using MANET
routing within these groups. HYMAD is fully decentralized and only makes use of
topological information exchanges between the nodes. The strength of HYMAD lies
in its ability to adapt to the changing connectivity patterns of the network.
We evaluate the scheme in simulation by replaying synthetic and real life
mobility traces which exhibit a broad range of connectivity dynamics. The
results show that HYMAD introduces limited overhead and outperforms the
multi-copy Spray-and-Wait DTN routing protocol it extends, both in terms of
delivery ratio and delay. This hybrid DTN-MANET approach offers a promising
venue for the delivery of elastic data in mobile ad-hoc networks as it retains
the resilience of a \textit{pure} DTN protocol while significantly improving
performance."
"Energy consumption and delay incurred in packet delivery are the two
important metrics for measuring the performance of geographic routing protocols
for Wireless Adhoc and Sensor Networks (WASN). A protocol capable of ensuring
both lesser energy consumption and experiencing lesser delay in packet delivery
is thus suitable for networks which are delay sensitive and energy hungry at
the same time. Thus a smart packet forwarding technique addressing both the
issues is thus the one looked for by any geographic routing protocol. In the
present paper we have proposed a Fermat point based forwarding technique which
reduces the delay experienced during packet delivery as well as the energy
consumed for transmission and reception of data packets."
"In this paper we analytically propose an alternative approach to achieve
better fairness in scheduling mechanisms which could provide better quality of
service particularly for real time application. Our proposal oppose the
allocation of the bandwidth which adopted by all previous scheduling mechanism.
It rather adopt the opposition approach be proposing the notion of
Maxmin-charge which fairly distribute the congestion. Furthermore, analytical
proposition of novel mechanism named as Just Queueing is been demonstrated."
"Hahn and Wallsten wrote that network neutrality ""usually means that broadband
service providers charge consumers only once for Internet access, do not favor
one content provider over another, and do not charge content providers for
sending information over broadband lines to end users."" In this paper we study
the implications of non-neutral behaviors under a simple model of linear
demand-response to usage-based prices. We take into account advertising
revenues and consider both cooperative and non-cooperative scenarios. In
particular, we model the impact of side-payments between service and content
providers. We also consider the effect of service discrimination by access
providers, as well as an extension of our model to non-monopolistic content
providers."
"In this paper, we study the optimal secondary-link beamforming pattern that
balances between the SU's throughput and the interference it causes to PUs in
MIMO cognitive radio networks. In particular, we aim to maximize the throughput
of the SU, while keeping the interference temperature at the primary receivers
below a certain threshold.
  Unlike traditional MIMO systems, SUs may not have the luxury of knowing the
channel state information (CSI) on the links to PUs. This presents a key
challenge for a secondary transmitter to steer interference away from primary
receivers. In this paper, we consider three scenarios, namely when the
secondary transmitter has complete, partial, or no knowledge about the channels
to the primary receivers. In particular, when complete CSI is not available,
the interference-temperature constraints are to be satisfied with high
probability, thus resulting in chance constraints that are typically hard to
deal with. Our contribution is fourfold. First, by analyzing the distributional
characteristics of MIMO channels, we propose a unified homogeneous QCQP
formulation that can be applied to all three scenarios. The homogeneous QCQP
formulation, though non-convex, is amenable to semidefinite programming (SDP)
relaxation methods. Secondly, we show that the SDP relaxation admits no gap
when the number of primary links is no larger than two. Thirdly, we propose a
randomized polynomial-time algorithm for constructing a near-optimal solution
to the QCQP problem when there are more than two primary links. Finally, we
show that when the secondary transmitter has no CSI on the links to primary
receivers, the optimal solution to the QCQP problem can be found by a simple
matrix eigenvalue-eigenvector computation, which can be done much more
efficiently than solving the QCQP directly."
"Multi-packet reception (MPR) has been recognized as a powerful
capacity-enhancement technique for random-access wireless local area networks
(WLANs). As is common with all random access protocols, the wireless channel is
often under-utilized in MPR WLANs. In this paper, we propose a novel
multi-round contention random-access protocol to address this problem. This
work complements the existing random-access methods that are based on
single-round contention. In the proposed scheme, stations are given multiple
chances to contend for the channel until there are a sufficient number of
``winning"" stations that can share the MPR channel for data packet
transmission. The key issue here is the identification of the optimal time to
stop the contention process and start data transmission. The solution
corresponds to finding a desired tradeoff between channel utilization and
contention overhead. In this paper, we conduct a rigorous analysis to
characterize the optimal strategy using the theory of optimal stopping. An
interesting result is that the optimal stopping strategy is a simple
threshold-based rule, which stops the contention process as soon as the total
number of winning stations exceeds a certain threshold. Compared with the
conventional single-round contention protocol, the multi-round contention
scheme significantly enhances channel utilization when the MPR capability of
the channel is small to medium. Meanwhile, the scheme automatically falls back
to single-round contention when the MPR capability is very large, in which case
the throughput penalty due to random access is already small even with
single-round contention."
"Interference at the radio receiver is a key source of degradation in quality
of service of wireless communication systems. This paper presents a unified
framework for OFDM/FBMC interference characterization and analysis in
asynchronous environment. Multi-user interference is caused by the timing
synchronization errors which lead to the destruction of the orthogonality
between subcarriers. In this paper, we develop a theoretical analysis of the
asynchronous interference considering the multi-path effects on the
interference signal. We further propose an accurate model for interference that
provides a useful computational tool in order to evaluate the performance of an
OFDM/FBMC system in a frequency selective fading environment. Finally,
simulation results confirmed the accuracy of the proposed model."
"In this work, we study the target detection and tracking problem in mobile
sensor networks, where the performance metrics of interest are probability of
detection and tracking coverage, when the target can be stationary or mobile
and its duration is finite. We propose a physical coverage-based mobility
model, where the mobile sensor nodes move such that the overlap between the
covered areas by different mobile nodes is small. It is shown that for
stationary target scenario the proposed mobility model can achieve a desired
detection probability with a significantly lower number of mobile nodes
especially when the detection requirements are highly stringent. Similarly,
when the target is mobile the coverage-based mobility model produces a
consistently higher detection probability compared to other models under
investigation."
"In this paper we show that in a multiclass Markovian network with unit rate
servers, the condition that the average load $\rho$ at every server is less
than unity is indeed sufficient for the stability or positive recurrence for
\emph{any} work conserving scheduling policy and \emph{class-independent}
routing. We use a variation of the positive recurrence criterion for
multidimensional discrete-time Markov chains over countable state spaces due to
Rosberg (JAP, Vol.~17, No.~3, 1980) and a monotonicity argument to establish
this assertion."
"Adaptive OFDMA has recently been recognized as a promising technique for
providing high spectral efficiency in future broadband wireless systems. The
research over the last decade on adaptive OFDMA systems has focused on adapting
the allocation of radio resources, such as subcarriers and power, to the
instantaneous channel conditions of all users. However, such ""fast"" adaptation
requires high computational complexity and excessive signaling overhead. This
hinders the deployment of adaptive OFDMA systems worldwide. This paper proposes
a slow adaptive OFDMA scheme, in which the subcarrier allocation is updated on
a much slower timescale than that of the fluctuation of instantaneous channel
conditions. Meanwhile, the data rate requirements of individual users are
accommodated on the fast timescale with high probability, thereby meeting the
requirements except occasional outage. Such an objective has a natural chance
constrained programming formulation, which is known to be intractable. To
circumvent this difficulty, we formulate safe tractable constraints for the
problem based on recent advances in chance constrained programming. We then
develop a polynomial-time algorithm for computing an optimal solution to the
reformulated problem. Our results show that the proposed slow adaptation scheme
drastically reduces both computational cost and control signaling overhead when
compared with the conventional fast adaptive OFDMA. Our work can be viewed as
an initial attempt to apply the chance constrained programming methodology to
wireless system designs. Given that most wireless systems can tolerate an
occasional dip in the quality of service, we hope that the proposed methodology
will find further applications in wireless communications."
"Thanks to its simplicity and cost efficiency, wireless local area network
(WLAN) enjoys unique advantages in providing high-speed and low-cost wireless
services in hot spots and indoor environments. Traditional WLAN
medium-access-control (MAC) protocols assume that only one station can transmit
at a time: simultaneous transmissions of more than one station cause the
destruction of all packets involved. By exploiting recent advances in PHY-layer
multiuser detection (MUD) techniques, it is possible for a receiver to receive
multiple packets simultaneously. This paper argues that such multipacket
reception (MPR) capability can greatly enhance the capacity of future WLANs. In
addition, the paper provides the MAC-layer and PHY-layer designs needed to
achieve the improved capacity. First, to demonstrate MPR as a powerful
capacity-enhancement technique, we prove a ""super-linearity"" result, which
states that the system throughput per unit cost increases as the MPR capability
increases. Second, we show that the commonly deployed binary exponential
backoff (BEB) algorithm in today's WLAN MAC may not be optimal in an MPR
system, and that the optimal backoff factor increases with the MPR capability,
the number of packets that can be received simultaneously. Third, based on the
above insights, we design a joint MAC-PHY layer protocol for an IEEE
802.11-like WLAN that incorporates advanced PHY-layer signal processing
techniques to implement MPR."
"Network latency and packet loss are considered to be an important requirement
for realistic evaluation of Peer-to-Peer protocols. Dedicated clusters, such as
Grid'5000, do not provide the variety of network latency and packet loss rates
that can be found in the Internet. However, compared to the experiments
performed on testbeds such as PlanetLab, the experiments performed on dedicated
clusters are reproducible, as the computational resources are not shared. In
this paper, we perform experiments to study the impact of network latency and
packet loss on the time required to download a file using BitTorrent. In our
experiments, we observe a less than 15% increase on the time required to
download a file when we increase the round-trip time between any two peers,
from 0 ms to 400 ms, and the packet loss rate, from 0% to 5%. Our main
conclusion is that the underlying network latency and packet loss have a
marginal impact on the time required to download a file using BitTorrent.
Hence, dedicated clusters such as Grid'5000 can be safely used to perform
realistic and reproducible BitTorrent experiments."
"The increasing demand for reliable Web applications gives a central role to
Web testing. Most of the existing works are focused on the definition of novel
testing techniques, specifically tailored to the Web. However, no attempt was
carried out so far to understand the specific nature of Web faults. This paper
presents a user session based testing technique that clusters user sessions
based on the service profile and selects a set of representative user sessions
from each cluster and tailored by augmentation with additional requests to
cover the dependence relationships between web pages. The created suite not
only can significantly reduce the size of the collected user sessions, also
viable to exercise fault sensitive paths. The results demonstrate that our
approach consistently detected the majority of known faults using a relatively
small number of test cases and will be a powerful system when more and more
user sessions are being clustered."
"A mobile agent is a program that is not bound to the system on which it began
execution, but rather travels amongst the hosts in the network with its code
and current execution state (i.e. Distributed Environment).The implementation
of distributed applications can be based on a multiplicity of technologies,
e.g. plain sockets, Remote Procedure Call (RPC), Remote Method Invocation
(RMI), Java Message Service (JMS), .NET Remoting, or Web Services. These
technologies differ widely in complexity, interoperability, standardization,
and ease of use. The Mobile Agent technology is emerging as an alternative to
build a smart generation of highly distributed systems. In this work, we
investigate the performance aspect of agent-based technologies for information
retrieval. We present a comparative performance evaluation model of Mobile
Agents versus .Net remoting by means of an analytical approach. A quantitative
measurements are performed to compare .Net remoting and mobile agents using
communication time, code size (agent code), Data size, number of node as
performance parameters in this research work. The results depict that Mobile
Agent paradigm offers a superior performance compared to .Net remoting
paradigm, offers fast computational speed; procure lower invocation cost by
making local invocations instead of remote invocations over the network,
thereby reducing network bandwidth."
"Peer-to-peer (P2P) networks establish loosely coupled application-level
overlays on top of the Internet to facilitate efficient sharing of resources.
It can be roughly classified as either structured or unstructured networks.
Without stringent constraints over the network topology, unstructured P2P
networks can be constructed very efficiently and are therefore considered
suitable to the Internet environment. However, the random search strategies
adopted by these networks usually perform poorly with a large network size. To
enhance the search performance in unstructured P2P networks through exploiting
users' common interest patterns captured within a probability-theoretic
framework termed the user interest model (UIM). A search protocol and a routing
table updating protocol are further proposed in order to expedite the search
process through self organizing the P2P network into a small world. Both
theoretical and experimental analyses are conducted and demonstrated the
effectiveness and efficiency of the approach."
"These Network Coding improves the network operation beyond the traditional
routing or store-and-forward, by mixing of data stream within a network.
Network coding techniques explicitly minimizes the total no of transmission in
wireless network. The Coding-aware routing maximizes the coding opportunity by
finding the coding possible path for every packet in the network. Here we
propose CORMEN: a new coding-aware routing mechanism based on opportunistic
routing. In CORMEN, every node independently can take the decision whether to
code packets or not and forwarding of packets is based on the coding
opportunity available."
"Wireless ad hoc networks are power constrained since nodes operate with
limited battery energy. Thus, energy consumption is crucial in the design of
new ad hoc routing protocols. In order to maximize the lifetime of ad hoc
networks, traffic should be sent via a route that can be avoid nodes with low
energy. In addition, considering that the nodes of ad hoc networks are mobile,
it is possible that a created path is broken because of nodes mobility and
establishment of a new path would be done again. This is because of sending
additional control packets, accordingly, energy consumption increases. Also, it
should avoid nodes which have more buffered packets. Maybe, because of long
queue, some of these packets are dropped and transmitted again. This is the
reason for wasting of energy. In this paper we propose a new energy efficient
algorithm, that uses a new cost function and avoid nodes with characteristics
which mentioned above. We show that this algorithm improves the network energy
consumption by using this new cost function."
"We consider the problem of scheduling in multihop wireless networks subject
to interference constraints. We consider a graph based representation of
wireless networks, where scheduled links adhere to the K-hop link interference
model. We develop a distributed greedy heuristic for this scheduling problem.
Further, we show that this distributed greedy heuristic computes the exact same
schedule as the centralized greedy heuristic."
"Spectrum is a scarce commodity, and considering the spectrum scarcity faced
by the wireless-based service providers led to high congestion levels.
Technical inefficiencies from pooled spectrum (this is nothing but the ""common
carrier principle"" adopted in oil/gas/electricity pipelines/networks.), since
all ad hoc networks share a common pool of channels, exhausting the available
channels will force ad hoc networks to block the services. Researchers found
that cognitive radio (CR) technology may resolve the spectrum scarcity. CR
network proved to next generation wireless communication system that proposed
as a way to reuse under-utilised spectrum of licensee user (primary network) in
an opportunistic and non-interfering basis. A CR is a self-configuring entity
in a wireless networking that senses its environment, tracks changes, and
frequently exchanges information with their networks. Adding this layer of such
intelligence to the ad hoc network by looking at the overall geography of the
network known as cognitive radio ad hoc networks (CRAHNs). However, CRAHN
facing challenges and condition become worst while tracks changes i.e.
reallocation of another under-utilised channels while primary network user
arrives. In this paper, channels or resource reallocation technique based on
bio-inspired computing algorithm for CRAHN has been proposed."
"Vehicular Ad hoc Networks is one of the most challenging research area in the
field of Mobile Ad Hoc Networks, in this research we propose a flexible,
simple, and scalable design for revocation list distribution in VANET, which
will reduce channel overhead and eliminate the use of CRL. Also it will
increase the security of the network and helps in identifying the adversary
vehicles."
"Inferring plausible node mobility based only on information from wireless
contact traces is a difficult problem. Working with mobility information allows
richer protocol simulations, particularly in dense networks, but requires
complex set-ups to measure. On the other hand, contact information is easier to
measure but only allows for simplistic simulation models. In a contact trace a
lot of node movement information is irretrievably lost so the original
positions and velocities are in general out of reach. In this paper, we propose
a fast heuristic algorithm, inspired by dynamic force-based graph drawing,
capable of inferring a plausible movement from any contact trace, and evaluate
it on both synthetic and real-life contact traces. Our results reveal that (i)
the quality of the inferred mobility is directly linked to the precision of the
measured contact trace, and (ii) the simple addition of appropriate
anticipation forces between nodes leads to an accurate inferred mobility."
"Use of ARIMA model in Sensor network The basic idea of our energy efficient
information collection scheme is to suppress data transmission if the data
sampled by sensor nodes are predictable by the sink node. This is done in two
phases 1) Preliminary Data Collection- During this phase sink node collects
enough data so that it can build up ARIMA model for each node. Then sink node
selects a model for the particular node and sends back the corresponding model
parameters to the node and also keeps them with it. Selecting the model for a
node there is a tradeoff between energy consumption and accuracy of prediction.
So we choose the model according to C = {\alpha} xMAE + (1 - {\alpha}) x rtran
0=< {\alpha} =<1 where the model should minimize C. Here MAE is Mean Absolute
Error which is normalized by some predefined error tolerance and rtran is the
ratio of number of samples transmitted over total number of samples. 2)
Adaptive Data Collection- After the sensor node has received the model
parameters it checks each actual data value with the data value calculated from
the parameters received. If there is deviation beyond some predefined error
tolerance then only it sends the original data value to the sink node."
"Denial of Service (DoS) attacks frequently happen on the Internet, paralyzing
Internet services and causing millions of dollars of financial loss. This work
presents NetFence, a scalable DoS-resistant network architecture. NetFence uses
a novel mechanism, secure congestion policing feedback, to enable robust
congestion policing inside the network. Bottleneck routers update the feedback
in packet headers to signal congestion, and access routers use it to police
senders' traffic. Targeted DoS victims can use the secure congestion policing
feedback as capability tokens to suppress unwanted traffic. When compromised
senders and receivers organize into pairs to congest a network link, NetFence
provably guarantees a legitimate sender its fair share of network resources
without keeping per-host state at the congested link. We use a Linux
implementation, ns-2 simulations, and theoretical analysis to show that
NetFence is an effective and scalable DoS solution: it reduces the amount of
state maintained by a congested router from per-host to at most per-(Autonomous
System)."
"In this paper, we consider the problem of modelling the average delay in an
IEEE 802.11 DCF wireless mesh network with a single root node under light
traffic. We derive expression for mean delay for a co-located wireless mesh
network, when packet generation is homogeneous Poisson process with rate
\lambda. We also show how our analysis can be extended for non-homogeneous
Poisson packet generation. We model mean delay by decoupling queues into
independent M/M/1 queues. Extensive simulations are conducted to verify the
analytical results."
"As many sensor network applications require deployment in remote and
hard-to-reach areas, it is critical to ensure that such networks are capable of
operating unattended for long durations. Consequently, the concept of using
nodes with energy replenishment capabilities has been gaining popularity.
However, new techniques and protocols must be developed to maximize the
performance of sensor networks with energy replenishment. Here, we analyze
limits of the performance of sensor nodes with limited energy, being
replenished at a variable rate. We provide a simple localized energy management
scheme that achieves a performance close to that with an unlimited energy
source, and at the same time keeps the probability of complete battery
discharge low. Based on the insights developed, we address the problem of
energy management for energy-replenishing nodes with finite battery and finite
data buffer capacities. To this end, we give an energy management scheme that
achieves the optimal utility asymptotically while keeping both the battery
discharge and data loss probabilities low."
"Wireless sensor networks are collections of large number of sensor nodes. The
sensor nodes are featured with limited energy, computation and transmission
power. Each node in the network coordinates with every other node in forwarding
their packets to reach the destination. Since these nodes operate in a
physically insecure environment; they are vulnerable to different types of
attacks such as selective forwarding and sinkhole. These attacks can inject
malicious packets by compromising the node. Geographical routing protocols of
wireless sensor networks have been developed without considering the security
aspects against these attacks. In this paper, a secure routing protocol named
secured greedy perimeter stateless routing protocol (S-GPSR) is proposed for
mobile sensor networks by incorporating trust based mechanism in the existing
greedy perimeter stateless routing protocol (GPSR). Simulation results prove
that S-GPSR outperforms the GPSR by reducing the overhead and improving the
delivery ratio of the networks."
"Environmental monitoring is often performed through a wireless sensor
network, whose nodes are randomly deployed over the geographical region of
interest. Sensors sample a physical phenomenon (the so-called field) and send
their measurements to a {\em sink} node, which is in charge of reconstructing
the field from such irregular samples. In this work, we focus on scenarios of
practical interest where the sensor deployment is unfeasible in certain areas
of the geographical region, e.g., due to terrain asperities, and the delivery
of sensor measurements to the sink may fail due to fading or to transmission
collisions among sensors simultaneously accessing the wireless medium. Under
these conditions, we carry out an asymptotic analysis and evaluate the quality
of the estimation of a d-dimensional field when the sink uses linear filtering
as a reconstruction technique. Specifically, given the matrix representing the
sampling system, V, we derive both the moments and an expression of the
limiting spectral distribution of VV*, as the size of V goes to infinity and
its aspect ratio has a finite limit bounded away from zero. By using such
asymptotic results, we approximate the mean square error on the estimated field
through the eta-transform of VV*, and derive the sensor network performance
under the conditions described above."
"The sensor nodes in a Wireless Sensor Network are generally constrained with
limited power supply. Efficient power management is a must for any sensor
network to keep the sensor nodes in the network to be operational for a longer
period of time this increasing the lifetime of the sensor network. Hierarchy
based routing enables the sensor networks to be deployed in larger areas. In
this paper we present a hierarchical cluster based routing protocol which
improves the scalability as the data travels from one cluster level to another
covering a greater amount of distance and increases the lifetime of the
wireless sensor network by distributing the power dissipation load evenly among
all the sensor nodes within the network. Also the time delay in case of
critical data to be received by the Base Station has also been lowered."
"The performance analysis of long file TCP controlled transfers in a WLAN in
infrastructure mode is available in the present literature with one of the main
assumptions being equal window size for all TCP connections. In this paper, we
extend the analysis to TCP-controlled long file uploads and downloads with
different TCP windows. Our approach is based on simple Markov chain given in
the paper [1], [2] with arbitrary window sizes. We presented simulation results
to show the accuracy of the analytical model."
"The Next Generation Networks (NGN) aims to integrate for IP-based telecom
infrastructures and provide most advance & high speed emerging value added
services. NGN capable to provide higher innovative services, these services
will able to integrate communication and Web service into a single platform. IP
Multimedia Subsystem, a NGN leading technology, enables a variety of
NGN-compliant communications services to interoperate while being accessed
through different kinds of access networks, preferably broadband. IMS-NGN
services essential by both consumer and corporate users are by now used to
access services, even communications services through the web and web-based
communities and social networks, It is key for success of IMS-based services to
be provided with efficient web access, so users can benefit from those new
services by using web-based applications and user interfaces, not only NGN-IMS
User Equipments and SIP protocol. Many Service are under planning which
provided only under convergence of IMS & Web 2.0. Convergence between Web 2.0
and NGN-IMS creates and serves new invented innovative, entertainment and
information appealing as well as user centric services and applications. These
services merge features from WWW and Communication worlds. On the one hand,
interactivity, ubiquity, social orientation, user participation and content
generation, etc. are relevant characteristics coming from Web 2.0 services.
Parallel IMS enables services including multimedia telephony, media sharing
(video-audio), instant messaging with presence and context, online directory,
etc. all of them applicable to mobile, fixed or convergent telecom networks.
With this paper, this paper brings out the benefits of adopting web 2.0
technologies for telecom services. As the services are today mainly driven by
the user's needs, and proposed the concept of unique customizable service
interface."
"The Next Generation Networks (NGN) aims to integrate for IP-based telecom
infrastructures and provide most advance & high speed emerging value added
services. NGN capable to provide higher innovative services, these services
will able to integrate communication and Web service into a single platform. IP
Multimedia Subsystem, a NGN leading technology, enables a variety of
NGN-compliant communications services to interoperate while being accessed
through different kinds of access networks, preferably broadband. IMS-NGN
services essential by both consumer and corporate users are by now used to
access services, even communications services through the web and web-based
communities and social networks, It is key for success of IMS-based services to
be provided with efficient web access, so users can benefit from those new
services by using web-based applications and user interfaces, not only NGN-IMS
User Equipments and SIP protocol. Many Service are under planning which
provided only under convergence of IMS & Web 2.0. Convergence between Web 2.0
and NGN-IMS creates and serves new invented innovative, entertainment and
information appealing as well as user centric services and applications. These
services merge features from WWW and Communication worlds. On the one hand,
interactivity, ubiquity, social orientation, user participation and content
generation, etc. are relevant characteristics coming from Web 2.0 services.
Parallel IMS enables services including multimedia telephony, media sharing
(video-audio), instant messaging with presence and context, online directory,
etc. all of them applicable to mobile, fixed or convergent telecom networks.
With this paper, this paper brings out the benefits of adopting web 2.0
technologies for telecom services. As the services are today mainly driven by
the user's needs, and proposed the concept of unique customizable service
interface."
"The main focus of this article is to achieve prolonged network lifetime with
overall energy efficiency in wireless sensor networks through controlled
utilization of limited energy. Major percentage of energy in wireless sensor
network is consumed during routing from source to destination, retransmission
of data on packet loss. For improvement, cross layered algorithm is proposed
for routing and retransmission scheme. Simulation and results shows that this
approach can save the overall energy consumption"
"This paper aims to propose a significant way of remote access and real time
monitoring of a particular geographic area by integrating wireless sensor
clouds with existing Telecom infrastructure and applications built around them
through a gateway. This utility is very potent for environment monitoring in
harsh and inaccessible places like mines, nuclear reactors, etc. We demonstrate
a scaled down version of multi-hop network of wireless sensor nodes and its
integration with existing telecom network infrastructure via a gateway. The
kind of results achieved like temperature monitoring etc. gives a glimpse of an
enormous step ahead in mine safety."
"With recent advances in wireless communication, networking, and low power
sensor technology, wireless sensor network (WSN) systems have begun to take
significant roles in various applications ranging from environmental sensing to
mobile healthcare sensing. While some WSN applications only require a lim- ited
amount of bandwidth, new emerging applications operate with a notice- ably
large amount of data transfers. One way to deal with such applications is to
maximize the available capacity by utilizing the use of multiple wireless
channels. This work proposes DynaChannAl, a distributed dynamic wireless
channel algorithm with the goal of effectively distributing nodes on multiple
wireless channels in WSN systems. Specifically, DynaChannAl targets applica-
tions where mobile nodes connect to a pre-existing wireless backbone and takes
the expected end-to-end queuing delay as its core metric. We use the link qual-
ity indicator (LQI) values provided by IEEE 802.15.4 radios white-list
potential links with good link quality and evaluate such links with the
aggregated packet transmission latency at each hop. Our approach is useful for
applications that require minimal end-to-end delay (i.e., healthcare
applications). DynaChannAl is a light weight and highly adoptable scheme that
can be easily incorporated with various pre-developed components and
pre-deployed applications. We eval- uate DynaChannAl in on a 45 node WSN
testbed. As the first study to consider end-to-end latency as the core metric
for channel allocation in WSN systems, the experimental results indicate that
DynaChannAl successfully distributes multi- ple (mobile) source nodes on
different wireless channels and enables the nodes to select wireless channel
and links that can minimize the end-to-end latency."
"Efficient resource discovery and availability improvement are very important
issues in unstructured P2P networks. In this paper, a bio-inspired resource
discovery scheme inspired by the principle of elephants migration is proposed.
A replication scheme based on Q-learning and erasure codes is also introduced.
Simulation results show that the proposed schemes significantly increases query
success rate and availability, and reduces the network traffic as the resources
are effectively distributed to well-performing nodes."
"Traffic and channel-data rate combined with the stream oriented methodology
can provide a scheme for offering optimized and guaranteed QoS. In this work a
stream oriented modeled scheme is proposed based on each node's self-scheduling
energy management. This scheme is taking into account the overall packet loss
in order to form the optimal effective -for the end-to-end connection-
throughput response. The scheme also -quantitatively- takes into account the
asymmetrical nature of wireless links and the caching activity that is used for
data revocation in the ad-hoc based connectivity scenario. Through the designed
middleware and the architectural layering and through experimental simulation,
the proposed energy-aware management scheme is thoroughly evaluated in order to
meet the parameters' values where the optimal throughput response for each
device/user is achieved."
"The paper has two objectives. The first is to study rigorously the transient
behavior of some P2P networks whenever information is replicated and
disseminated according to epidemic-like dynamics. The second is to use the
insight gained from the previous analysis in order to predict how efficient are
measures taken against peer-to-peer (P2P) networks. We first introduce a
stochastic model which extends a classical epidemic model and characterize the
P2P swarm behavior in presence of free riding peers. We then study a second
model in which a peer initiates a contact with another peer chosen randomly. In
both cases the network is shown to exhibit a phase transition: a small change
in the parameters causes a large change in the behavior of the network. We
show, in particular, how the phase transition affects measures that content
provider networks may take against P2P networks that distribute non-authorized
music or books, and what is the efficiency of counter-measures."
"Given the respective advantages of the two complimentary techniques for
peer-to-peer media streaming (namely tree-based push and mesh-based pull),
there is a strong trend of combining them into a hybrid streaming system.
Backed by recently proposed mechanisms to identify stable peers, such a hybrid
system usually consists of backbone trees formed by the stable peers and other
overlay structures in the second tier to accommodate the remaining peers. In
this paper, we embrace the hybrid push-pull structure for peer-to-peer media
streaming. Our protocol is dominated by a multi-tree push mechanism to minimize
the delay in the backbone and is complemented by other overlay structures to
cope with peer dynamics. What mainly distinguishes our multi-tree pushing from
the conventional ones is an unbalanced tree design guided by the so called
snow-ball streaming, which has a provable minimum delay and can be smoothly
""melded"" with virtually any other existing overlay structures lying in the
second tier. We design algorithms to construct and maintain our SNowbAll
multi-tree Pushing (SNAP) overlay, and we also illustrate how to smoothly weld
the SNAP backbone with the second tier. Finally, we perform simulations in
ns-2; the results indicate that our approach outperforms a recently proposed
hybrid streaming system."
"One of the big challenges in ad hoc network design is packet routing. Studies
have shown that on-demand routing protocols perform better than table-driven
routing protocols. In order to avoid route discovery for each packet, on-demand
routing protocols cache routes previously learnt. A node in ad hoc network
learns routing information by overhearing or forwarding packets to other nodes
and keep learned routes in its route cache. However, node movement results
broken links and therefore increases risk of cache pollution. Ensuring cache
freshness in on-demand routing protocols, therefore, presents a serious
challenge. A lot of research has been done in route cache organization,
however, little effort has been done for route cache timeout policy to prevent
stale routes from being used. In this paper we propose a new cross-layer
framework to improve route cache performance in on-demand routing protocols.
The proposed framework presents novel use of Received Signal Strength Indicator
(RSSI) information to choose cache timeout of individual links in route cache."
"Loss tomography has received considerable attention in recent years and a
number of estimators have been proposed. Unfortunately, almost all of them are
devoted to the tree topology despite the general topology is more common in
practice. In addition, most of the works presented in the literature rely on
iterative approximation to search for the maximum of a likelihood function
formed from observations, which have been known neither scalable nor efficient.
In contrast to the tree topology, there is few paper dedicated to the general
topology because of the lack of understanding the impacts created by the probes
sent by different sources. We in this paper present the analytical results
obtained recently for the general topology that show the correlation created by
the probes sent by multiple sources to a node located in an intersection of
multiple trees. The correlation is expressed by a set of polynomials of the
pass rates of the paths connecting the sources to the node. In addition to the
expression, a closed form solution is proposed to obtain the MLE of the pass
rates of the paths connecting the sources to the node. Then, two strategies are
proposed to estimate the loss rate of a link for the general topology: one is
path-based and the other is link-based, depending on whether we need to obtain
the pass rate of a path first. The two strategies are compared in the context
of the general topology that shows each has its advantages and the link-based
one is more general. Apart from proving the estimates obtained are the MLEs, we
prove the estimator presented here has the optimal asymptotic property."
"Mobile ad-hoc network (MANET) is infrastructureless, self-organizable, multi
hop packet switched network. A number of routing protocols for MANETs have been
proposed in recent years. Dynamic Source Routing (DSR) protocol is one of the
most popular routing protocol for ad hoc networks. This paper presents a novel
method to enhance route maintenance part of DSR protocol. Our proposed route
maintenance significantly increases the efficiency of the protocol at the time
of route failures."
"Many mobile ad hoc network protocols use simple flooding, in order to adapt
to changes in time varying network topology. Most of the times, a network-wide
flood results in redundant packets and increases network congestion,
probability of packet collision, low utilization of available bandwidth, and
most important, higher power consumption. In this paper, we propose a new
cross-layer broadcast scheme to minimize broadcast traffic in mobile ad hoc
networks. Our scheme is based on use of received signal strength indicator,
RSSI, value to reduce the number of broadcast packets. The effectiveness of the
proposed technique is verified using simulations."
"Conserving power in mobile ad-hoc and sensor networks is a big challenge.
Most of the nodes in these networks, in general, are battery powered,
therefore, an efficient power saving protocol is required to extend the
lifetime of such networks. A lot of work has been done and several protocols
have been proposed to address this problem. Gossip based protocols, which are
based on the results of percolation theory, significantly reduce power
consumption with very little implementation overhead. However, not much work
has been done to make gossiping battery aware. In this paper we introduce a
simple gossip based battery aware sleep protocol. The protocol allows low
battery nodes to sleep more, therefore, improves overall network lifetime."
"In this report we propose a MultiObjective (MO) performance evaluation
framework for wireless ad hoc networks where criteria such as capacity,
robustness, energy and delay are optimized concurrently. Within such a
framework, we can determine both the Pareto-optimal performance bounds and the
networking parameters that provide these bounds. The originality of this
approach is that it accounts for the inherent broadcast properties of the
transmission and finely models the interference distribution. In the proposed
model, the network performance can be optimized when several flows (source-
destination transmissions) exist. One benefit of our approach is that the
complexity does not grow with the number of flows. The other major contribution
of this paper is the new analytical formulation of the performance metrics. It
relies on a matrix representation of the constraints imposed by the
interference- limited and broadcast wireless channel. Because of the similarity
of this matrix with a Markovian transition matrix, we can exploit classical
results from Markov chains theory to derive steady state performance metrics
relative to capacity, robustness, energy and delay. Another very interesting
feature of these new metrics is that the Pareto-optimal solutions related to
them provide a tight bound on capacity, robustness, energy and delay."
"In this paper an improved version of the graded precision localization
algorithm GRADELOC, called IGRADELOC is proposed. The performance of GRADELOC
is dependent on the regions formed by the overlapping radio ranges of the nodes
of the underlying sensor network. A different region pattern could
significantly alter the nature and precision of localization. In IGRADELOC, two
improvements are suggested. Firstly, modifications are proposed in the radio
range of the fixed-grid nodes, keeping in mind the actual radio range of
commonly available nodes, to allow for routing through them. Routing is not
addressed by GRADELOC, but is of prime importance to the deployment of any
adhoc network, especially sensor networks. A theoretical model expressing the
radio range in terms of the cell dimensions of the grid infrastructure is
proposed, to help in carrying out a deployment plan which achieves the
desirable precision of coarse-grained localization. Secondly, in GRADELOC it is
observed that fine-grained localization does not achieve significant
performance benefits over coarse-grained localization. In IGRADELOC, this
factor is addressed with the introduction of a parameter that could be used to
improve and fine-tune the precision of fine-grained localization."
"We study the problem of tracking an object moving through a network of
wireless sensors. In order to conserve energy, the sensors may be put into a
sleep mode with a timer that determines their sleep duration. It is assumed
that an asleep sensor cannot be communicated with or woken up, and hence the
sleep duration needs to be determined at the time the sensor goes to sleep
based on all the information available to the sensor. Having sleeping sensors
in the network could result in degraded tracking performance, therefore, there
is a tradeoff between energy usage and tracking performance. We design sleeping
policies that attempt to optimize this tradeoff and characterize their
performance. As an extension to our previous work in this area [1], we consider
generalized models for object movement, object sensing, and tracking cost. For
discrete state spaces and continuous Gaussian observations, we derive a lower
bound on the optimal energy-tracking tradeoff. It is shown that in the low
tracking error regime, the generated policies approach the derived lower bound."
"It is well known that links in CSMA wireless networks are prone to
starvation. Prior works focused almost exclusively on equilibrium starvation.
In this paper, we show that links in CSMA wireless networks are also
susceptible to temporal starvation. Specifically, although some links have good
equilibrium throughputs and do not suffer from equilibrium starvation, they can
still have no throughput for extended periods from time to time. Given its
impact on quality of service, it is important to understand and characterize
temporal starvation. To this end, we develop a ""trap theory"" to analyze
temporal throughput fluctuation. The trap theory serves two functions. First,
it allows us to derive new mathematical results that shed light on the
transient behavior of CSMA networks. For example, we show that the duration of
a trap, during which some links receive no throughput, is insensitive to the
distributions of the backoff countdown and transmission time (packet duration)
in the CSMA protocol. Second, we can develop analytical tools for computing the
""degrees of starvation"" for CSMA networks to aid network design. For example,
given a CSMA network, we can determine whether it suffers from starvation, and
if so, which links will starve. Furthermore, the likelihood and durations of
temporal starvation can also be computed. We believe that the ability to
identify and characterize temporal starvation as established in this paper will
serve as an important first step toward the design of effective remedies for
it."
"We have explored our own innovative work about the design & development of
internal location-identification system for mobile devices based on integration
of RFID and wireless technology. The function of our system is based on
strategically located passive RFID tags placed on objects around building which
are identified using an RFID reader attached to a mobile device. The mobile
device reads the RFID tag and through the wireless network, sends the request
to the server. The server resolves the request and sends the desired
location-based information back to the mobile device. We had addressed that we
can go through the RFID technology for internal location identification
(indoor), which provides us better location accuracy because of no contact
between the tag and the reader, and the system requires no line of sight. In
this paper we had also focused on the issues of RFID technologies i.e.
Non-line-of-sight & High inventory speeds."
"In this paper, we consider the problem of modelling the average delay
experienced by a packet in a single cell IEEE 802.11 DCF wireless local area
network. The packet arrival process at each node i is assumed to be Poisson
with rate parameter \lambda_i. Since the nodes are sharing a single channel,
they have to contend with one another for a successful transmission. The mean
delay for a packet has been approximated by modelling the system as a 1-limited
Random Polling system with zero switchover time. We show that even for
non-homogeneous packet arrival processes, the mean delay of packets across the
queues are same and depends on the system utilization factor and the aggregate
throughput of the MAC. Extensive simulations are conducted to verify the
analytical results."
"A fundamental choice in femtocell deployments is the set of users which are
allowed to access each femtocell. Closed access restricts the set to
specifically registered users, while open access allows any mobile subscriber
to use any femtocell. Which one is preferable depends strongly on the distance
between the macrocell base station (MBS) and femtocell. The main results of the
paper are lemmas which provide expressions for the SINR distribution for
various zones within a cell as a function of this MBS-femto distance. The
average sum throughput (or any other SINR-based metric) of home users and
cellular users under open and closed access can be readily determined from
these expressions. We show that unlike in the uplink, the interests of home and
cellular users are in conflict, with home users preferring closed access and
cellular users preferring open access. The conflict is most pronounced for
femtocells near the cell edge, when there are many cellular users and fewer
femtocells. To mitigate this conflict, we propose a middle way which we term
shared access in which femtocells allocate an adjustable number of time-slots
between home and cellular users such that a specified minimum rate for each can
be achieved. The optimal such sharing fraction is derived. Analysis shows that
shared access achieves at least the overall throughput of open access while
also satisfying rate requirements, while closed access fails for cellular users
and open access fails for the home user."
"Nowadays, mobile phones are indispensable devices; it has become a trend now
that college and university students are owners of such devices and this
particular factor plays a very important role behind the coming up with the
proposed system. ""PC 2 Phone event Announcer"", is the name of the new proposed
system suggested to solve the existing communication problem between the
College staff and students. As the name suggests, it can be deduced that the
system will involve computers and phones, more specifically mobile phones."
"The following paper presents various methods and implementation techniques
used to harvest metadata efficiently from a Kademlia Distributed Hashtable
(DHT) as used in the BitTorrent P2P network to build an index of publicly
available files in the BitTorrent ecosystem. The indexer design makes various
tradeoffs between throughput and fairness towards other DHT nodes while also
being scaleable in a distributed environment."
"It has been recently brought into spotlight that through the exploitation of
network coding concepts at physical-layer, the interference property of the
wireless media can be proven to be a blessing in disguise. Nonetheless, most of
the previous studies on this subject have either held unrealistic assumptions
about the network properties, thus making them basically theoretical, or have
otherwise been limited to fairly simple network topologies. We, on the other
hand, believe to have devised a novel scheme, called Real Amplitude Scaling
(RAS), that relaxes the aforementioned restrictions, and works with a wider
range of network topologies and in circumstances that are closer to practice,
for instance in lack of symbol-level synchronization and in the presence of
noise, channel distortion and severe interference from other sources. The
simulation results confirmed the superior performance of the proposed method in
low SNRs, as well as the high SNR limits, where the effect of quantization
error in the digital techniques becomes comparable to the channel."
"Multi-cell processing, also called Coordinated Multiple Point (CoMP), is a
very promising distributed multi-antennas technique that uses neighbour cell's
antennas. This is expected to be part of next generation cellular networks
standards such as LTE-A. Small cell networks in dense urban environment are
mainly limited by interferences and CoMP can strongly take advantage of this
fact to improve cell-edge users' throughput. This paper provides an analytical
derivation of the capacity outage probability for CoMP experiencing fast
Rayleigh fading. Only the average received power (slow varying fading) has to
be known, and perfect Channel State Information (CSI) is not required. An
optimisation of the successfully received data-rate is then derived with
respect to the number of cooperating stations and the outage probability,
illustrated by numerical examples."
"We address the problem of opportunistic multiuser scheduling in downlink
networks with Markov-modeled outage channels. We consider the scenario in which
the scheduler does not have full knowledge of the channel state information,
but instead estimates the channel state information by exploiting the memory
inherent in the Markov channels along with ARQ-styled feedback from the
scheduled users. Opportunistic scheduling is optimized in two stages: (1)
Channel estimation and rate adaptation to maximize the expected immediate rate
of the scheduled user; (2) User scheduling, based on the optimized immediate
rate, to maximize the overall long term sum-throughput of the downlink. The
scheduling problem is a partially observable Markov decision process with the
classic 'exploitation vs exploration' trade-off that is difficult to quantify.
We therefore study the problem in the framework of Restless Multi-armed Bandit
Processes (RMBP) and perform a Whittle's indexability analysis. Whittle's
indexability is traditionally known to be hard to establish and the index
policy derived based on Whittle's indexability is known to have optimality
properties in various settings. We show that the problem of downlink scheduling
under imperfect channel state information is Whittle indexable and derive the
Whittle's index policy in closed form. Via extensive numerical experiments, we
show that the index policy has near-optimal performance.
  Our work reveals that, under incomplete channel state information, exploiting
channel memory for opportunistic scheduling can result in significant
performance gains and that almost all of these gains can be realized using an
easy-to-implement index policy."
"The MIMO wireless channel offers a rich ground for quality of service
analysis. In this work, we present a stochastic network calculus analysis of a
MIMO system, operating in spatial multiplexing mode, using moment generating
functions (MGF). We quantify the spatial multiplexing gain, achieved through
multiple antennas, for flow level quality of service (QoS) performance.
Specifically we use Gilbert-Elliot model to describe individual spatial paths
between the antenna pairs and model the whole channel by an N-State Markov
Chain, where N depends upon the degrees of freedom available in the MIMO
system. We derive probabilistic delay bounds for the system and show the impact
of increasing the number of antennas on the delay bounds under various
conditions, such as channel burstiness, signal strength and fading speed.
Further we present results for multi-hop scenarios under statistical
independence."
"By combining the features of CSMA and TDMA, fully decentralised WLAN MAC
schemes have recently been proposed that converge to collision-free schedules.
In this paper we describe a MAC with optimal long-run throughput that is almost
decentralised. We then design two \changed{schemes} that are practically
realisable, decentralised approximations of this optimal scheme and operate
with different amounts of sensing information. We achieve this by (1)
introducing learning algorithms that can substantially speed up convergence to
collision free operation; (2) developing a decentralised schedule length
adaptation scheme that provides long-run fair (uniform) access to the medium
while maintaining collision-free access for arbitrary numbers of stations."
"The U.S. Air Force currently is in the process of developing an Airborne
Network (AN) to provide support to its combat aircrafts on a mission. The
reliability needed for continuous operation of an AN is difficult to achieve
through completely infrastructure-less mobile ad hoc networks. In this paper we
first propose an architecture for an AN where airborne networking platforms
(ANPs - aircrafts, UAVs and satellites) form the backbone of the AN. In this
architecture, the ANPs can be viewed as mobile base stations and the combat
aircrafts on a mission as mobile clients. The combat aircrafts on a mission
move through a space called air corridor. The goal of the AN design is to form
a backbone network with the ANPs with two properties: (i) the backbone network
remains connected at all times, even though the topology of the network changes
with the movement of the ANPs, and (ii) the entire 3D space of the air corridor
is under radio coverage at all times by the continuously moving ANPs.
  In addition to proposing an architecture for an AN, the contributions of the
paper include, development of an algorithm that finds the velocity and
transmission range of the ANPs so that the dynamically changing backbone
network remains connected at all times, development of a routing algorithm that
ensures a connection between the source-destination node pair with the fewest
number of path switching, given the dimensions of the air corridor and the
radius of the coverage sphere associated with an ANP, development of an
algorithm that finds the fewest number of ANPs required to provide complete
coverage of the air corridor at all times, development of an algorithm that
provides connected-coverage to the air corridor at all times, and development
of a visualization tool that depicts the movement patterns of the ANPs and the
resulting dynamic graph and the coverage volume of the backbone network."
"This paper presents a TDMA based energy efficient cognitive radio
multichannel medium access control (MAC) protocol called ECR-MAC for wireless
Ad Hoc Networks. ECR-MAC requires only a single half-duplex radio transceiver
on each node that integrates the spectrum sensing at physical (PHY) layer and
the packet scheduling at MAC layer. In addition to explicit frequency
negotiation which is adopted by conventional multichannel MAC protocols,
ECR-MAC introduces lightweight explicit time negotiation. This two-dimensional
negotiation enables ECR-MAC to exploit the advantage of both multiple channels
and TDMA, and achieve aggressive power savings by allowing nodes that are not
involved in communication to go into doze mode. The IEEE 802.11 standard allows
for the use of multiple channels available at the PHY layer, but its MAC
protocol is designed only for a single channel. A single channel MAC protocol
does not work well in a multichannel environment, because of the multichannel
hidden terminal problem. The proposed energy efficient ECR-MAC protocol allows
SUs to identify and use the unused frequency spectrum in a way that constrains
the level of interference to the primary users (PUs). Extensive simulation
results show that our proposed ECR-MAC protocol successfully exploits multiple
channels and significantly improves network performance by using the licensed
spectrum band opportunistically and protects QoS provisioning over cognitive
radio ad hoc networks."
"This paper proposes a cross-layer based cognitive radio multichannel medium
access control (MAC) protocol with TDMA, which integrate the spectrum sensing
at physical (PHY) layer and the packet scheduling at MAC layer, for the ad hoc
wireless networks. The IEEE 802.11 standard allows for the use of multiple
channels available at the PHY layer, but its MAC protocol is designed only for
a single channel. A single channel MAC protocol does not work well in a
multichannel environment, because of the multichannel hidden terminal problem.
Our proposed protocol enables secondary users (SUs) to utilize multiple
channels by switching channels dynamically, thus increasing network throughput.
In our proposed protocol, each SU is equipped with only one spectrum agile
transceiver, but solves the multichannel hidden terminal problem using temporal
synchronization. The proposed cognitive radio MAC (CR-MAC) protocol allows SUs
to identify and use the unused frequency spectrum in a way that constrains the
level of interference to the primary users (PUs). Our scheme improves network
throughput significantly, especially when the network is highly congested. The
simulation results show that our proposed CR-MAC protocol successfully exploits
multiple channels and significantly improves network performance by using the
licensed spectrum band opportunistically and protects PUs from interference,
even in hidden terminal situations."
"Rapid progress in microelectromechanical system (MEMS) and radio frequency
(RF) design has enabled the development of low-power, inexpensive, and
network-enabled microsensors. These sensor nodes are capable of capturing
various physical information, such as temperature, pressure, motion of an
object, etc as well as mapping such physical characteristics of the environment
to quantitative measurements. A typical wireless sensor network (WSN) consists
of hundreds to thousands of such sensor nodes linked by a wireless medium. In
this paper, we present a comparative investigation of energy consumption for
few commercially available chipsets such as TR1001, CC1000 and CC1010 based on
different scheduling methods for two types of deployment strategies. We
conducted our experiment within the OMNeT++ simulator."
"In P2P systems, large volumes of data are declustered naturally across a
large number of peers. But it is very difficult to control the initial data
distribution because every user has the freedom to share any data with other
users. The system scalability can be improved by distributing the load across
multiple servers which is proposed by replication. The large scale content
distribution systems were improved broadly using the replication techniques.
The demanded contents can be brought closer to the clients by multiplying the
source of information geographically, which in turn reduce both the access
latency and the network traffic. In addition to this, due to the intrinsic
dynamism of the P2P environment, static data distribution cannot be expected to
guarantee good load balancing. If the hot peers become bottleneck, it leads to
increased user response time and significant performance degradation of the
system. Hence an effective load balancing mechanism is necessary in such cases
and it can be attained efficiently by intelligent data replication. In this
paper, we propose a cluster based replication architecture for load-balancing
in peer-to-peer content distribution systems. In addition to an intelligent
replica placement technique, it also consists of an effective load balancing
technique. In the intelligent replica placement technique, peers are grouped
into strong and weak clusters based on their weight vector which comprises
available capacity, CPU speed, access latency and memory size. In order to
achieve complete load balancing across the system, an intracluster and
inter-cluster load balancing algorithms are proposed. We are able to show that
our proposed architecture attains less latency and better throughput with
reduced bandwidth usage, through the simulation results."
"Achieving optimal transmission throughput in data networks in a multi-hop
wireless networks is fundamental but hard problem. The situation is aggravated
when nodes are mobile. Further, multi-rate system make the analysis of
throughput more complicated. In mobile scenario, link may break or be created
as nodes are moving within communication range. `Route Discovery' which is to
find the optimal route and transmission schedule is an important issue. Route
discovery entails some cost; so one would not like to initiate discovery too
often. On the other hand, not discovering reasonably often entails the risk of
being stuck with a suboptimal route and/or schedule, which hurts end-to-end
throughput. The implementation of the routing decision problem in one
dimensional mobile ad hoc network as Markov decision process problem is already
is discussed in the paper [1]. A heuristic based on threshold policy is
discussed in the same paper without giving a way to find the threshold. In this
paper, we suggested a rule for setting the threshold, given the parameters of
the system. We also point out that our results remain valid in a slightly
different mobility model; this model is a first step towards an `open' network
in which existing relay nodes can leave and/or new relay nodes can join the
network."
"In a multirate WLAN with a single access point (AP) and several stations
(STAs), we obtain analytical expressions for TCP-controlled long file transfer
throughputs allowing nonzero propagation delays between the file server and
STAs. We extend our earlier work in [3] to obtain AP and STA throughputs in a
multirate WLAN, and use these in a closed BCMP queueing network model to obtain
TCP throughputs. Simulation show that our approach is able to predict observed
throughputs with a high degree of accuracy."
"Routing algorithms for wireless sensor networks can be broadly divided into
two classes - proactive and reactive. Proactive routing is suitable for a
network with a fixed topology. On the other hand, reactive routing is more
suitable for a set of mobile nodes where routes are created on demand and there
is not much time to evaluate the worthiness of a route, the prime concern being
reachability due to constantly changing node positions. Sensor networks route
events of interest from source(s) to destination(s) where appropriate actions
could be taken. However, with mobile sensor nodes, it is not only important to
know the events but the location of the nodes generating the events. Most
sensor nodes are not equipped with expensive GPS or accurate RSSI computation
hardware to aid localization. Keeping these in view, we propose a modified
reactive routing algorithm, with added support for localization, to localize
mobile sensor nodes on the basis of information received from fixed sensor
nodes during mutual exchange of routing control packets. The accuracy of
localization depends on the ratio of the number of fixed nodes to the number of
mobile nodes and the topology of the fixed nodes. A typical application
scenario would be a mix of mobile nodes and fixed nodes, where fixed nodes know
their absolute location and the location of mobile nodes is derived from the
fixed nodes, in step with the reactive routing protocol in action. The modified
algorithm would be suitable for deployments where the approximate position of a
mobile node (i.e. the event location) is required but there is no external
support infrastructure available for localization."
"We analyze TCP-controlled bulk file transfers in a single station (STA) WLAN
with nonzero propagation delay between the file server and the WLAN. Our
approach is to model the flow of packets as a closed queueing network (BCMP
network) with 3 service centres, one each for the Access Point (AP) and the
STA, and the third for the propagation delay. The service rates of the first
two are obtained by analyzing the WLAN MAC. Simulations show a very close match
with the theory."
"In this paper, we consider the problem of modelling the average delay
experienced by an application packets of variable length in a single cell IEEE
802.11 DCF wireless local area network. The packet arrival process at each node
i is assumed to be a stationary and independent increment random process with
mean ai and second moment a(2) i . The packet lengths at node i are assumed to
be i.i.d random variables Pi with finite mean and second moment. A closed form
expression has been derived for the same. We assume the input arrival process
across queues to be uncorrelated Poison processes. As the nodes share a single
channel, they have to contend with one another for a successful transmission.
The mean delay for a packet has been approximated by modelling the system as a
1-limited Random Polling system with zero switchover times. Extensive
simulations are conducted to verify the analytical results."
"Studying animal movement and distribution is of critical importance to
addressing environmental challenges including invasive species, infectious
diseases, climate and land-use change. Motion sensitive camera traps offer a
visual sensor to record the presence of a broad range of species providing
location -specific information on movement and behavior. Modern digital camera
traps that record video present new analytical opportunities, but also new data
management challenges. This paper describes our experience with a terrestrial
animal monitoring system at Barro Colorado Island, Panama. Our camera network
captured the spatio-temporal dynamics of terrestrial bird and mammal activity
at the site - data relevant to immediate science questions, and long-term
conservation issues. We believe that the experience gained and lessons learned
during our year long deployment and testing of the camera traps as well as the
developed solutions are applicable to broader sensor network applications and
are valuable for the advancement of the sensor network research. We suggest
that the continued development of these hardware, software, and analytical
tools, in concert, offer an exciting sensor-network solution to monitoring of
animal populations which could realistically scale over larger areas and time
spans."
"The OSI model, developed by ISO in 1984, attempts to summarize complicated
network cases on layers. Moreover, network troubles are expressed by taking the
model into account. However, there has been no standardization for network
troubles up to now. Network troubles have only been expressed by the name of
the related layer. In this paper, it is pointed out that possible troubles on
the related layer vary and possible troubles on each layer are categorized for
functional network administration and they are standardized in an eligible way.
The proposed model for network trouble shooting was developed considering the
OSI model."
"A crucial issue for a mobile ad hoc network is the handling of a large number
of nodes. As more nodes join the mobile ad hoc network, contention and
congestion are more likely. The on demand routing protocols which broadcasts
control packets to discover routes to the destination nodes, generate a high
number of broadcast packets in a larger networks causing contention and
collision. We propose an efficient route discovery protocol, which reduces the
number of broadcast packet, using controlled flooding technique. The simulation
results show that the proposed probabilistic flooding decreases the number of
control packets floating in the network during route discovery phase, without
lowering the success ratio of path discoveries. Furthermore, the proposed
method adapts to the normal network conditions. The results show that up to 70%
of control packet traffic is saved in route discovery phase when the network is
denser."
"This paper proposes a fair and efficient QoS scheduling scheme for IEEE
802.16 BWA systems that satisfies both throughput and delay guarantee to
various real and non-real time applications. The proposed QoS scheduling scheme
is compared with an existing QoS scheduling scheme proposed in literature in
recent past. Simulation results show that the proposed scheduling scheme can
provide a tight QoS guarantee in terms of delay, delay violation rate and
throughput for all types of traffic as defined in the WiMAX standard, thereby
maintaining the fairness and helps to eliminate starvation of lower priority
class services. Bandwidth utilization of the system and fairness index of the
resources are also encountered to validate the QoS provided by our proposed
scheduling scheme."
"In this paper we address the problem of prolonging the lifetime of wireless
sensor networks (WSNs) deployed to monitor an area of interest. In this
scenario, a helpful approach is to reduce coverage redundancy and therefore the
energy expenditure due to coverage. We introduce the first algorithm which
reduces coverage redundancy by means of Sensor Activation and sensing Radius
Adaptation (SARA)in a general applicative scenario with two classes of devices:
sensors that can adapt their sensing range (adjustable sensors) and sensors
that cannot (fixed sensors). In particular, SARA activates only a subset of all
the available sensors and reduces the sensing range of the adjustable sensors
that have been activated. In doing so, SARA also takes possible heterogeneous
coverage capabilities of sensors belonging to the same class into account. It
specifically addresses device heterogeneity by modeling the coverage problem in
the Laguerre geometry through Voronoi-Laguerre diagrams.
  SARA executes quickly and is guaranteed to terminate. It provides a
configuration of the active set of sensors that lifetime and coverage
requirements of demanding WSN applications, not met by current solutions.
  By means of extensive simulations we show that SARA achieves a network
lifetime that is significantly superior to that obtained by previous algorithms
in all the considered scenarios."
"In this paper, we propose an architecture for Network of Information mobile
node (NetInf MN). It bears characteristics and features of basic NetInf node
architecture with features introduced in the LISP MN architecture. We also
introduce a virtual node layer for mobility management in the Network of
Information. Therefore, by adopting this architecture no major changes in the
contemporary network topologies is required. Thus, making our approach more
practical."
"This paper introduces the security and trust concepts in wireless sensor
networks and explains the difference between them, stating that even though
both terms are used interchangeably when defining a secure system, they are not
the same. The difference between reputation and trust is also explained,
highlighting that reputation partially affects trust. A survey of trust and
reputation systems in various domains is conducted, with more details given to
models in ad-hoc and sensor networks as they are closely related to each other
and to our research interests. The methodologies used to model trust and their
references are presented. The factors affecting trust updating are summarised
and some examples of the systems in which these factors have been implemented
are given. The survey states that, even though researchers have started to
explore the issue of trust in wireless sensor networks, they are still
examining the trust associated with routing messages between nodes (binary
events). However, wireless sensor networks are mainly deployed to monitor
events and report data, both continuous and discrete. This leads to the
development of new trust models addressing the continuous data issue and also
to combine the data trust and the communication trust to infer the total trust."
"This article has been withdrawn by arXiv administrators because it
plagiarises http://www2.ece.ohio-state.edu/~ekici/papers/crnroutingsurvey.pdf"
"The Session Initiation Protocol (SIP) server overload management has
attracted interest since SIP is being widely deployed in the Next Generation
Networks (NGN) as a core signaling protocol. Yet all existing SIP overload
control work is focused on SIP-over-UDP, despite the fact that TCP is
increasingly seen as the more viable choice of SIP transport. This paper
answers the following questions: is the existing TCP flow control capable of
handling the SIP overload problem? If not, why and how can we make it work? We
provide a comprehensive explanation of the default SIP-over-TCP overload
behavior through server instrumentation. We also propose and implement novel
but simple overload control algorithms without any kernel or protocol level
modification. Experimental evaluation shows that with our mechanism the
overload performance improves from its original zero throughput to nearly full
capacity. Our work leads to the important general insight that the traditional
notion of TCP flow control alone is incapable of managing overload for
time-critical session-based applications, which would be applicable not only to
SIP, but also to a wide range of other common applications such as database
servers."
"Vehicular Ad hoc Networks (VANET) are expected to have great potential to
improve both traffic safety and comfort in the future. When many vehicles want
to access data through roadside unit, data scheduling become an important
issue. In this paper, we identify some challenges in roadside based data
access. To address these challenges we first review some existing scheduling
schemes. We then propose a priority scheduling and finally show that using this
idea can increase QOS compare to previous algorithms."
"Correlation in user connectivity patterns is generally considered a problem
for system designers, since it results in peaks of demand and also in the
scarcity of resources for peer-to-peer applications. The other side of the coin
is that these connectivity patterns are often predictable and that, to some
extent, they can be dealt with proactively.
  In this work, we build predictors aiming to determine the probability that
any given user will be online at any given time in the future. We evaluate the
quality of these predictors on various large traces from instant messaging and
file sharing applications.
  We also illustrate how availability prediction can be applied to enhance the
behavior of peer-to-peer applications: we show through simulation how data
availability is substantially increased in a distributed hash table simply by
adjusting data placement policies according to peer availability prediction and
without requiring any additional storage from any peer."
"Applications such as traffic engineering and network provisioning can greatly
benefit from knowing, in real time, what is the largest input rate at which it
is possible to transmit on a given path without causing congestion. We consider
a probabilistic formulation for available bandwidth where the user specifies
the probability of achieving an output rate almost as large as the input rate.
We are interested in estimating and tracking the network-wide probabilistic
available bandwidth (PAB) on multiple paths simultaneously with minimal
overhead on the network. We propose a novel framework based on chirps, Bayesian
inference, belief propagation and active sampling to estimate the PAB. We also
consider the time evolution of the PAB by forming a dynamic model and designing
a tracking algorithm based on particle filters. We implement our method in a
lightweight and practical tool that has been deployed on the PlanetLab network
to do online experiments. We show through these experiments and simulations
that our approach outperforms block-based algorithms in terms of input rate
cost and probability of successful transmission."
"In duty-cycled wireless sensor networks, the nodes switch between active and
dormant states, and each node may determine its active/dormant schedule
independently. This complicates the Minimum-Energy Multicasting (MEM) problem,
which has been primarily studied in always-active wireless ad-hoc networks. In
this paper, we study the duty-cycle-aware MEM problem in wireless sensor
networks, and we present a formulation of the Minimum-Energy Multicast Tree
Construction and Scheduling (MEMTCS) problem. We prove that the MEMTCS problem
is NP-hard, and it is unlikely to have an approximation algorithm with a
performance ratio of $(1-o(1))\ln\Delta$, where $\Delta$ is the maximum node
degree in a network. We propose a polynomial-time approximation algorithm for
the MEMTCS problem with a performance ratio of $\mathcal{O}(H(\Delta+1))$,
where $H(\cdot)$ is the harmonic number. We also provide a distributed
implementation of our algorithm. Finally, we perform extensive simulations and
the results demonstrate that our algorithm significantly outperform other known
algorithms in terms of both the total energy cost and the transmission
redundancy."
"Research in location determination for GSM phones has gained interest
recently as it enables a wide set of location based services. RSSI-based
techniques have been the preferred method for GSM localization on the handset
as RSSI information is available in all cell phones. Although the GSM standard
allows for a cell phone to receive signal strength information from up to seven
cell towers, many of today's cell phones are low-end phones, with limited API
support, that gives only information about the associated cell tower. In
addition, in many places in the world, the density of cell towers is very small
and therefore, the available cell tower information for localization is very
limited. This raises the challenge of accurately determining the cell phone
location with very limited information, mainly the RSSI of the associated cell
tower. In this paper we propose a Hidden Markov Model based solution that
leverages the signal strength history from only the associated cell tower to
achieve accurate GSM localization. We discuss the challenges of implementing
our system and present the details of our system and how it addresses the
challenges. To evaluate our proposed system, we implemented it on Androidbased
phones. Results for two different testbeds, representing urban and rural
environments, show that our system provides at least 156% enhancement in median
error in rural areas and at least 68% enhancement in median error in urban
areas compared to current RSSI-based GSM localization systems"
"Reduction of unnecessary energy consumption is becoming a major concern in
wired networking, because of the potential economical benefits and of its
expected environmental impact. These issues, usually referred to as ""green
networking"", relate to embedding energy-awareness in the design, in the devices
and in the protocols of networks. In this work, we first formulate a more
precise definition of the ""green"" attribute. We furthermore identify a few
paradigms that are the key enablers of energy-aware networking research. We
then overview the current state of the art and provide a taxonomy of the
relevant work, with a special focus on wired networking. At a high level, we
identify four branches of green networking research that stem from different
observations on the root causes of energy waste, namely (i) Adaptive Link Rate,
(ii) Interface proxying, (iii) Energy-aware infrastructures and (iv)
Energy-aware applications. In this work, we do not only explore specific
proposals pertaining to each of the above branches, but also offer a
perspective for research."
"Rate adaptation plays a key role in determining the performance of wireless
LANs. In this paper, we introduce a semi-Markovian framework to analyze the
performance of two of the most popular rate adaptation algorithms used in
wireless LANs, namely Automatic Rate Fallback (ARF) and Adaptive Automatic Rate
Fallback (AARF). Given our modeling assumptions, the analysis is exact and
provides closed form expressions for the achievable throughput of ARF and AARF.
We illustrate the benefit of our analysis by numerically comparing the
throughput performance of ARF and AARF in two different channel regimes. The
results show that neither of these algorithms consistently outperforms the
other. We thus propose and analyze a new variant to AARF, called Persistent
AARF (or PAARF), and show that it achieves a good compromise between the two
algorithms, often performing close to the best algorithm in each of the studied
regimes. The numerical results also shed light into the impact of MAC overhead
on the performance of the three algorithms. In particular, they show that the
more conservative strategies AARF and PAARF scale better as the bit rate
increases."
"The natural or man-made disaster demands an efficient communication and
coordination among first responders to save life and other community resources.
Normally, the traditional communication infrastructures such as land line or
cellular networks are damaged and don't provide adequate communication services
to first responders for exchanging emergency related information. Wireless ad
hoc networks such as mobile ad hoc networks, wireless sensor networks and
wireless mesh networks are the promising alternatives in such type of
situations. The security requirements for emergency response communications
include privacy, data integrity, authentication, key management, access control
and availability. Various ad hoc communication frameworks have been proposed
for emergency response situations. The majority of the proposed frameworks
don't provide adequate security services for reliable and secure information
exchange. This paper presents a survey of the proposed emergency response
communication frameworks and the potential security services required by them
to provide reliable and secure information exchange during emergency
situations."
"Contemporary personal computing devices are increasingly required to be
portable and mobile enabling user's wireless access, to wired network
infrastructures and services. This approach to mobile computing and
communication is only appropriate in situations where a coherent infrastructure
is available. There are many situations where these requirements are not
fulfilled such as; developing nations, rural areas, natural disasters, and
military conflicts to name but a few. A practical solution is to use mobile
devices interconnected via a wireless medium to form a network, known as a
Mobile Ad-hoc Network (MANET), and provide the services normally found in wired
networks. Security in MANETs is an issue of paramount importance due to the
wireless nature of the communication links. Additionally due to the lack of
central administration security issues are different from conventional
networks. For the purposes of this article we have used the ""WMN test-bed"" to
enable secure routing in MANETs. The use of cryptography is an efficient proven
way of securing data in communications, but some cryptographic algorithms are
not as efficient as others and require more processing power, which is
detrimental to MANETs. In this article we have assessed different cryptographic
approaches to securing the OLSR (Optimised Link State Routing) protocol to
provide a basis for research. We conclude the paper with a series of
performance evaluation results regarding different cryptographic and hashing
schemes. Our findings clearly show that the most efficient combination of
algorithms used for authentication and encryption are SHA-1 and AES
respectively. Using this combination over their counterparts will lead to a
considerable reduction in processing time and delay on the network, creating an
efficient transaction moving towards satisfying resource constraints and
security requirements."
"Low-power and Lossy Networks (LLNs), like wireless networks based upon the
IEEE 802.15.4 standard, have strong energy constraints, and are moreover
subject to frequent transmission errors, not only due to congestion but also to
collisions and to radio channel conditions. This paper introduces an analytical
model to compute the total energy consumption in an LLN due to the TCP
protocol. The model allows us to highlight some tradeoffs as regards the choice
of the TCP maximum segment size, of the Forward Error Correction (FEC)
redundancy ratio, and of the number of link-layer retransmissions, in order to
minimize the total energy consumption."
"In this paper the problem of scheduling with power control in wireless
networks is studied: given a set of communication requests, one needs to assign
the powers of the network nodes, and schedule the transmissions so that they
can be done in a minimum time, taking into account the signal interference of
concurrently transmitting nodes. The signal interference is modeled by SINR
constraints. Approximation algorithms are given for this problem, which use the
mean power assignment. The problem of schduling with fixed mean power
assignment is also considered, and approximation guarantees are proven."
"BitTorrent has recently introduced LEDBAT, a novel application-layer
congestion control protocol for data exchange. The protocol design starts from
the assumption that network bottlenecks are at the access of the network, and
that thus user traffic competes creating self-inducing congestion. To relieve
from this phenomenon, LEDBAT is designed to quickly infer that self-induced
congestion is approaching (by detecting relative changes of the one-way delay
in the transmission path), and to react by reducing the sending rate prior that
congestion occurs. Prior work has however shown LEDBAT to be affected by a
latecomer advantage, where newly arriving connections can starve already
existing flows. In this work, we propose modifications to the congestion window
update mechanism of the LEDBAT protocol that aim at solving this issue,
guaranteeing thus intra-protocol fairness and efficiency. Closed-form
expressions for the stationary throughput and queue occupancy are provided via
a fluid model, whose accuracy is confirmed by means of ns2 packet level
simulations. Our results show that the proposed change can effective solve the
latecomer issue, without affecting the other original LEDBAT goals at the same
time."
"This paper examines the use of Wireless Sensor Networks interfaced with light
fittings to allow for daylight substitution techniques to reduce energy usage
in existing buildings. This creates a wire free system for existing buildings
with minimal disruption and cost."
"Transmit power control in wireless networks has long been recognized as an
effective mechanism to mitigate co-channel interference. Due to the highly
non-convex nature, optimal power control is known to be difficult to achieve if
a system utility is to be maximized. To date, there does not yet exist a
distributed power control algorithm that maximizes any form of system utility,
despite the importance of distributed implementation for the wireless
infrastructureless networks such as ad hoc and sensor networks. This paper
fills this gap by developing a Gibbs Sampling based Asynchronous distributed
power control algorithm (referred to as GLAD). The proposed algorithm quickly
converges to the global optimal solution regardless of the concavity,
continuity, differentiability and monotonicity of the utility function. Same as
other existing distributed power control algorithms, GLAD requires extensive
message passing among all users in the network, which leads to high signaling
overhead and high processing complexity. To address this issue, this paper
further proposes a variant of the GLAD algorithm, referred to as I-GLAD, where
the prefix ""I"" stands for infrequent message passing. The convergence of I-GLAD
can be proved regardless of the reduction in the message passing rate. To
further reduce the processing complexity at each transmitter, we develop an
enhanced version of I-GLAD, referred to as NI-GLAD, where only the control
messages from the neighboring links are processed. Our simulation results show
that I-GLAD approximately converges to the global optimal solution regardless
of the type of the system utility function. Meanwhile, the optimality of the
solution obtained by NI-GLAD depends on the selection of the neighborhood size."
"The Dynamic Source Routing protocol (DSR) is a simple and efficient routing
protocol designed specifically for use in multi-hop wireless ad hoc networks of
mobile nodes. Preemptive DSR(PDSR) is the modified version of DSR. The main
objective of this paper is to analyze and compare the performance of Preemptive
DSR and Temporarily Ordered Routing Algorithm(TORA).It discusses the effect of
variation in number of nodes and average speed on protocol performance.
Simulation results (provided by the instructor) are analyzed to get an insight
into the operation of TORA and PDSR in small/large sized networks with
slow/fast moving nodes. Results show that PDSR outperforms TORA in terms of the
number of MANET control packets used to maintain/erase routes. Also, it is
concluded that TORA is a better choice than PDSR for fast moving highly
connected set of nodes. It is also observed that DSR provides better data
throughput than TORA and that routes can be created faster in PDSR than in
TORA. This paper tries to explain the reasons behind the nature of the results."
"Wireless Sensor Networks (WSNs) are highly distributed networks consisting of
a large number of tiny, low-cost, light-weight wireless nodes deployed to
monitor an environment or a system. Each node in a WSN consists of three
subsystems: the sensor subsystem which senses the environment, the processing
subsystem which performs local computations on the sensed data, and the
communication subsystem which is responsible for message exchange with
neighboring sensor nodes. While an individual sensor node has limited sensing
region, processing power, and energy, networking a large number of sensor nodes
give rise to a robust, reliable, and accurate sensor network covering a wide
region. Thus, routing in WSNs is a very important issue. This paper presents a
query-based routing protocol for a WSN that provides different levels of
Quality of Service (QoS): energy-efficiency, reliability, low latency and
fault-tolerance-under different application scenarios. The algorithm has low
computational complexity but can dynamically guarantee different QoS support
depending on the requirement of the applications. The novelty of the proposed
algorithm is its ability to provide multiple QoS support without
reconfiguration and redeployment of the sensor nodes. The algorithm is
implemented in network simulator ns-2 and its performance has been evaluated.
The results show that the algorithm is more efficient than some of the
currently existing routing algorithms for WSNs."
"The proliferation of peer-to-peer (P2P) file sharing protocols is due to
their efficient and scalable methods for data dissemination to numerous users.
But many of these networks have no provisions to provide users with long term
access to files after the initial interest has diminished, nor are they able to
guarantee protection for users from malicious clients that wish to implicate
them in incriminating activities. As such, users may turn to supplementary
measures for storing and transferring data in P2P systems. We present a new
file sharing paradigm, called a Graffiti Network, which allows peers to harness
the potentially unlimited storage of the Internet as a third-party
intermediary. Our key contributions in this paper are (1) an overview of a
distributed system based on this new threat model and (2) a measurement of its
viability through a one-year deployment study using a popular web-publishing
platform. The results of this experiment motivate a discussion about the
challenges of mitigating this type of file sharing in a hostile network
environment and how web site operators can protect their resources."
"In this paper we study the dynamic aspects of the coverage of a mobile sensor
network resulting from continuous movement of sensors. As sensors move around,
initially uncovered locations are likely to be covered at a later time. A
larger area is covered as time continues, and intruders that might never be
detected in a stationary sensor network can now be detected by moving sensors.
However, this improvement in coverage is achieved at the cost that a location
is covered only part of the time, alternating between covered and not covered.
We characterize area coverage at specific time instants and during time
intervals, as well as the time durations that a location is covered and
uncovered. We further characterize the time it takes to detect a randomly
located intruder. For mobile intruders, we take a game theoretic approach and
derive optimal mobility strategies for both sensors and intruders. Our results
show that sensor mobility brings about unique dynamic coverage properties not
present in a stationary sensor network, and that mobility can be exploited to
compensate for the lack of sensors to improve coverage."
"We consider the sizing of network buffers in 802.11 based networks. Wireless
networks face a number of fundamental issues that do not arise in wired
networks. We demonstrate that the use of fixed size buffers in 802.11 networks
inevitably leads to either undesirable channel under-utilization or unnecessary
high delays. We present two novel dynamic buffer sizing algorithms that achieve
high throughput while maintaining low delay across a wide range of network
conditions. Experimental measurements demonstrate the utility of the proposed
algorithms in a production WLAN and a lab testbed."
"When data productions and consumptions are heavily unbalanced and when the
origins of data queries are spatially and temporally distributed, the so called
in-network data storage paradigm supersedes the conventional data collection
paradigm in wireless sensor networks (WSNs). In this paper, we first introduce
geometric quorum systems (along with their metrics) to incarnate the idea of
in-network data storage. These quorum systems are ""geometric"" because curves
(rather than discrete node sets) are used to form quorums. We then propose
GeoQuorum as a new quorum system, for which the quorum forming curves are
parameterized. Though our proposal stems from the existing work on using curves
to guide data replication and retrieval in dense WSNs, we significantly expand
this design methodology, by endowing GeoQuorum with a great flexibility to
fine-tune itself towards different application requirements. In particular, the
tunability allows GeoQuorum to substantially improve the load balancing
performance and to remain competitive in energy efficiency. Both our analysis
and simulations confirm the performance enhancement brought by GeoQuorum."
"Objective-The main purpose of this paper is to construct a data accuracy
model for the maximal set of sensor nodes that sense a point event and forms a
cluster with fully connected network between them. We determine the minimal set
of sensor nodes that are sufficient to give approximately the same data
accuracy achieve by the maximal set of sensor nodes. Design
approach/Procedure-L set of sensor nodes are randomly deployed over a region Z.
Since a point event S has occurred in the region Z, M maximal set of sensor
nodes wake up and start sensing the point event. The set of M sensor nodes
forms a cluster with fully connected network and remaining set of sensor nodes
continue to be in sleep mode. One sensor node is elected randomly as a cluster
head (CH) node which can estimate the data accuracy for the cluster before data
aggregation and finally send the data to the sink node. Findings - Since we
simulate the data accuracy for the cluster (M set of sensor nodes) at CH node,
there exist P minimal set of sensor nodes which give approximately the same
data accuracy level achieve by M set of sensor nodes .Moreover we find that as
the distance from the point event to the number of sensor nodes increases, the
data accuracy also get decreases. Design Limitation -This model is only
applicable to estimate data accuracy for the point event where the sensed data
are assumed to be spatially correlated with approximately same variations.
Practical implementation-Detect point event e.g. fire in forest.
Inventive/Novel idea - This is the first time that a data accuracy model is
performed for the cluster before data aggregation at the CH node which can
reduce data redundancy and communication overhead."
"Objective-The main purpose of this paper is to construct a distributed
clustering algorithm such that each distributed cluster can perform the data
accuracy at their respective cluster head node before data aggregation and
transmit the data to the sink node. Design approach/Procedure - We investigate
that the data are spatially correlated among the sensor nodes which form the
clusters in the spatial domain. Due to high correlation of data, these clusters
of sensor nodes are overlapped in the spatial domain. To overcome this problem,
we construct a distributed clustering algorithm with non-overlapping irregular
clusters in the spatial domain. Then each distributed cluster can perform data
accuracy at the cluster head node and finally send the data to the sink node.
Findings- Simulation result shows the associate sensor nodes of each
distributed cluster and clarifies their data accuracy profile in the spatial
domain. We demonstrate the simulation results for a single cluster to verify
that their exist an optimal cluster which give approximately the same data
accuracy level achieve by the single cluster. Moreover we find that as the
distance from the tracing point to the number of sensor node increases the data
accuracy decreases. Design Limitations - This model is only applicable to
estimate data accuracy for distributed clusters where the sensed data are
assumed to be spatially correlated with approximately same variations.
Practical implementation - Measure the moisture content in the distributed
agricultural field. Inventive/Novel idea- This is the first time that a data
accuracy model is performed for the distributed clusters before data
aggregation at the cluster head node which can reduce data redundancy and
communication overhead."
"The increasing popularity of web-based applications has led to several
critical services being provided over the Internet. This has made it imperative
to monitor the network traffic so as to prevent malicious attackers from
depleting the resources of the network and denying services to legitimate
users. This paper has presented a mechanism for protecting a web-server against
a distributed denial of service (DDoS) attack. Incoming traffic to the server
is continuously monitored and any abnormal rise in the inbound traffic is
immediately detected. The detection algorithm is based on a statistical
analysis of the inbound traffic on the server and a robust hypothesis testing
framework. While the detection process is on, the sessions from the legitimate
sources are not disrupted and the load on the server is restored to the normal
level by blocking the traffic from the attacking sources. To cater to different
scenarios, the detection algorithm has various modules with varying level of
computational and memory overheads for their execution. While the approximate
modules are fast in detection and involve less overhead, they have lower
detection accuracy. The accurate modules involve complex detection logic and
hence involve more overhead for their execution, but they have very high
detection accuracy. Simulations carried out on the proposed mechanism have
produced results that demonstrate effectiveness of the scheme."
"Access networks, in particular, Digital Subscriber Line (DSL) equipment, are
a significant source of energy consumption for wireline operators. Replacing
large monolithic DSLAMs with smaller remote DSLAM units closer to customers can
reduce the energy consumption as well as increase the reach of the access
network. This paper attempts to formalize the design and optimization of the
""last mile"" wireline access network with energy as one of the costs to be
minimized. In particular, the placement of remote DSLAM units needs to be
optimized. We propose solutions for two scenarios. For the scenario where an
existing all-copper network from the central office to the customers is to be
transformed into a fiber-copper network with remote DSLAM units, we present
optimal polynomial-time solutions. In the green-field scenario, both the access
network layout and the placement of remote DSLAM units must be determined. We
show that this problem is NP-complete. We present an optimal ILP formulation
and also design an efficient heuristic-based approach to build a
power-and-cost-optimized access network. Our heuristic-based approach yields
results that are very close to optimal. We show how the power consumption of
the access network can be reduced by carefully laying the access network and
introducing remote DSLAM units."
"Wireless Sensor Networks (WSNs) are rapidly emerging as an important new area
in wireless and mobile computing research. Applications of WSNs are numerous
and growing, and range from indoor deployment scenarios in the home and office
to outdoor deployment scenarios in adversary's territory in a tactical
battleground (Akyildiz et al., 2002). For military environment, dispersal of
WSNs into an adversary's territory enables the detection and tracking of enemy
soldiers and vehicles. For home/office environments, indoor sensor networks
offer the ability to monitor the health of the elderly and to detect intruders
via a wireless home security system. In each of these scenarios, lives and
livelihoods may depend on the timeliness and correctness of the sensor data
obtained from dispersed sensor nodes. As a result, such WSNs must be secured to
prevent an intruder from obstructing the delivery of correct sensor data and
from forging sensor data. To address the latter problem, end-to-end data
integrity checksums and post-processing of senor data can be used to identify
forged sensor data (Estrin et al., 1999; Hu et al., 2003a; Ye et al., 2004).
The focus of this chapter is on routing security in WSNs. Most of the currently
existing routing protocols for WSNs make an optimization on the limited
capabilities of the nodes and the application-specific nature of the network,
but do not any the security aspects of the protocols. Although these protocols
have not been designed with security as a goal, it is extremely important to
analyze their security properties. When the defender has the liabilities of
insecure wireless communication, limited node capabilities, and possible
insider threats, and the adversaries can use powerful laptops with high energy
and long range communication to attack the network, designing a secure routing
protocol for WSNs is obviously a non-trivial task."
"Location distinction is defined as determining whether or not the position of
a device has changed. We introduce methods and metrics for performing location
distinction in multiple-input multiple-output (MIMO) wireless networks. Using
MIMO channel measurements from two different testbeds, we evaluate the
performance of temporal signature-based location distinction with varying
system parameters, and show that it can be applied to MIMO channels with
favorable results. In particular, a 2x2 MIMO channel with a bandwidth of 80 MHz
allows a 64-fold reduction in miss rate over the SISO channel for a fixed false
alarm rate, achieving as small as 4 x 10^-4 probability of false alarm for a
2.4 x 10^-4 probability of missed detection. The very high reliability of MIMO
location distinction enables location distinction systems to detect the change
in position of a transmitter even when using a single receiver."
"Our work is motivated by geographical forwarding of sporadic alarm packets to
a base station in a wireless sensor network (WSN), where the nodes are
sleep-wake cycling periodically and asynchronously. When a node (referred to as
the source) gets a packet to forward, either by detecting an event or from an
upstream node, it has to wait for its neighbors in a forwarding set (referred
to as relays) to wake-up. Each of the relays is associated with a random reward
(e.g., the progress made towards the sink) that is iid. To begin with, the
source is uncertain about the number of relays, their wake-up times and the
reward values, but knows their distributions. At each relay wake-up instant,
when a relay reveals its reward value, the source's problem is to forward the
packet or to wait for further relays to wake-up. In this setting, we seek to
minimize the expected waiting time at the source subject to a lower bound on
the average reward. In terms of the operations research literature, our work
can be considered as a variant of the asset selling problem. We formulate the
relay selection problem as a partially observable Markov decision process
(POMDP), where the unknown state is the number of relays. We begin by
considering the case where the source knows the number of relays. For the
general case, where the source only knows a pmf on the number of relays, it has
to maintain a posterior pmf on the number of relays and forward the packet iff
the pmf is in an optimum stopping set. We show that the optimum stopping set is
convex and obtain inner and outer bounds to this set. The computational
complexity of the above policies motivates us to formulate an alternative
simplified model, the optimal policy for which is a simple threshold rule. We
provide simulation results to compare the performance of the various one-hop
and end-to-end forwarding policies."
"In operational networks, nodes are connected via multiple links for load
sharing and redundancy. This is done to make sure that a failure of a link does
not disconnect or isolate some parts of the network. However, link failures
have an effect on routing, as the routers find alternate paths for the traffic
originally flowing through the link which has failed. This effect is severe in
case of failure of a critical link in the network, such as backbone links or
the links carrying higher traffic loads. When routing is done using the Open
Shortest Path First (OSPF) routing protocol, the original weight selection for
the normal state topology may not be as efficient for the failure state. In
this paper, we investigate the single link failure issue with an objective to
find a weight setting which results in efficient routing in normal and failure
states. We engineer Tabu Search Iterative heuristic using two different
implementation strategies to solve the OSPF weight setting problem for link
failure scenarios. We evaluate these heuristics and show through experimental
results that both heuristics efficiently handle weight setting for the failure
state. A comparison of both strategies is also presented."
"At the same time as the emergence of multimedia in mobile Ad hoc networks,
research for the introduction of the quality of service (QoS) has received much
attention. However, when designing a QoS solution, the estimation of the
available resources still represents one of the main issues. This paper
suggests an approach to estimate available resources on a node. This approach
is based on the estimation of the busy ratio of the shared canal. We consider
in our estimation the several constraints related to the Ad hoc transmission
mode such as Interference phenomena. This approach is implemented on the AODV
routing protocol. We call AODVwithQOS our new routing protocol. We also
performed a performance evaluation by simulations using NS2 simulator. The
results confirm that AODVwithQoS provides QoS support in ad hoc wireless
networks with good performance and low overhead."
"Peer-to-Peer protocols currently form the most heavily used protocol class in
the Internet, with BitTorrent, the most popular protocol for content
distribution, as its flagship.
  A high number of studies and investigations have been undertaken to measure,
analyse and improve the inner workings of the BitTorrent protocol. Approaches
such as tracker message analysis, network probing and packet sniffing have been
deployed to understand and enhance BitTorrent's internal behaviour.
  In this paper we present a novel approach that aims to collect, process and
analyse large amounts of local peer information in BitTorrent swarms. We
classify the information as periodic status information able to be monitored in
real time and as verbose logging information to be used for subsequent
analysis. We have designed and implemented a retrieval, storage and
presentation infrastructure that enables easy analysis of BitTorrent protocol
internals. Our approach can be employed both as a comparison tool, as well as a
measurement system of how network characteristics and protocol implementation
influence the overall BitTorrent swarm performance.
  We base our approach on a framework that allows easy swarm creation and
control for different BitTorrent clients. With the help of a virtualized
infrastructure and a client-server software layer we are able to create,
command and manage large sized BitTorrent swarms. The framework allows a user
to run, schedule, start, stop clients within a swarm and collect information
regarding their behavior."
"In this paper, we propose an efficient mobility control algorithm for the
downlink multi-cell orthogonal frequency division multiplexing access (OFDMA)
system for co-channel interference reduction. It divides each cell into several
areas. The mobile nodes in each area find their own optimal position according
to their present location. Both the signal to interference plus noise ratio
(SINR) and the capacity for each node are increased by the proposed mobility
control algorithm. Simulation results say that, even the frequency reuse factor
(FRF) is equal to 1, the average capacity is improved after applying the
mobility control algorithm, compared to existing partial frequency reuse (PFR)
scheme."
"We propose an algorithm to locate the most critical nodes to network
robustness. Such critical nodes may be thought of as those most related to the
notion of network centrality. Our proposal relies only on a localized spectral
analysis of a limited subnetwork centered at each node in the network. We also
present a procedure allowing the navigation from any node towards a critical
node following only local information computed by the proposed algorithm.
Experimental results confirm the effectiveness of our proposal considering
networks of different scales and topological characteristics."
"Network virtualization is an important concept to overcome the ossification
of today's Internet as it facilitates innovation also in the network core and
as it promises a more efficient use of the given resources and infrastructure.
Virtual networks (VNets) provide an abstraction of the physical network:
multiple VNets may cohabit the same physical network, but can be based on
completely different protocol stacks (also beyond IP). One of the main
challenges in network virtualization is the efficient admission control and
embedding of VNets. The demand for virtual networks (e.g., for a video
conference) can be hard to predict, and once the request is accepted, the
specification / QoS guarantees must be ensured throughout the VNet's lifetime.
This requires an admission control algorithm which only selects high-benefit
VNets in times of scarce resources, and an embedding algorithm which realizes
the VNet in such a way that the likelihood that future requests can be embedded
as well is maximized.
  This article describes a generic algorithm for the online VNet embedding
problem which does not rely on any knowledge of the future VNet requests but
whose performance is competitive to an optimal offline algorithm that has
complete knowledge of the request sequence in advance: the so-called
competitive ratio is, loosely speaking, logarithmic in the sum of the
resources. Our algorithm is generic in the sense that it supports multiple
traffic models, multiple routing models, and even allows for nonuniform
benefits and durations of VNet requests."
"Privacy-preserving techniques for distributed computation have been proposed
recently as a promising framework in collaborative inter-domain network
monitoring. Several different approaches exist to solve such class of problems,
e.g., Homomorphic Encryption (HE) and Secure Multiparty Computation (SMC) based
on Shamir's Secret Sharing algorithm (SSS). Such techniques are complete from a
computation-theoretic perspective: given a set of private inputs, it is
possible to perform arbitrary computation tasks without revealing any of the
intermediate results. In fact, HE and SSS can operate also on secret inputs
and/or provide secret outputs. However, they are computationally expensive and
do not scale well in the number of players and/or in the rate of computation
tasks. In this paper we advocate the use of ""elementary"" (as opposite to
""complete"") Secure Multiparty Computation (E-SMC) procedures for traffic
monitoring. E-SMC supports only simple computations with private input and
public output, i.e., it can not handle secret input nor secret (intermediate)
output. Such a simplification brings a dramatic reduction in complexity and
enables massive-scale implementation with acceptable delay and overhead.
Notwithstanding its simplicity, we claim that an E-SMC scheme is sufficient to
perform a great variety of computation tasks of practical relevance to
collaborative network monitoring, including, e.g., anonymous publishing and set
operations. This is achieved by combining a E-SMC scheme with data structures
like Bloom Filters and bitmap strings."
"We design a cross-layer approach to optimize the joint use of multi-packet
reception and network coding, in order to relieve congestion. We construct a
model for the behavior of the 802.11 MAC and apply it to several key canonical
topology components and their extensions to any number of nodes. The results
obtained from this model match the available experimental results, which are
for routing and opportunistic network coding, with fidelity. Using this model,
we show that fairness allocation by the MAC can seriously impact performance;
hence, we devise a new MAC that not only substantially improves throughput
relative to the current 802.11 MAC, but also provides fairness to flows of
information rather than to nodes. We show that the proper combination of
network coding, multi-packet reception, and our new MAC protocol achieves
super-additive throughput gains of up to 6.3 times that of routing alone with
the use of the standard 802.11 MAC. Finally, we extend the model to analyze the
asymptotic behavior of our new MAC as the number of nodes increases."
"In this paper, we tackle the problem of opportunistic spectrum access in
large-scale cognitive radio networks, where the unlicensed Secondary Users (SU)
access the frequency channels partially occupied by the licensed Primary Users
(PU). Each channel is characterized by an availability probability unknown to
the SUs. We apply evolutionary game theory to model the spectrum access problem
and develop distributed spectrum access policies based on imitation, a behavior
rule widely applied in human societies consisting of imitating successful
behavior. We first develop two imitation-based spectrum access policies based
on the basic Proportional Imitation (PI) rule and the more advanced Double
Imitation (DI) rule given that a SU can imitate any other SUs. We then adapt
the proposed policies to a more practical scenario where a SU can only imitate
the other SUs operating on the same channel. A systematic theoretical analysis
is presented for both scenarios on the induced imitation dynamics and the
convergence properties of the proposed policies to an imitation-stable
equilibrium, which is also the $\epsilon$-optimum of the system. Simple,
natural and incentive-compatible, the proposed imitation-based spectrum access
policies can be implemented distributedly based on solely local interactions
and thus is especially suited in decentralized adaptive learning environments
as cognitive radio networks."
"Distributed denial of service attacks are often considered a security
problem. While this may be the way to view the problem with today's Internet,
new network architectures attempting to address the issue should view it as a
scalability problem. In addition, they need to address the problem based on a
rigorous foundation."
"Nowadays scarcity of spectrum availability is increasing highly. Adding
cognition to the existing Wireless Sensor Network (WSN) infrastructure will
help in this situation. As sensor nodes in WSN are limited with some constrains
like power, efforts are required to increase the lifetime and other performance
measures of the network. In this paper we propose the idea of Doubly Cognitive
WSN. The basic idea is to progressively allocate the sensing resources only to
the most promising areas of the spectrum. This work is based on Artificial
Neural Network as well as on Support Vector Machine (SVM) concept. As the load
of sensing resource is reduced significantly, this approach will save the
energy of the nodes, and also reduce the sensing time dramatically. The
proposed work can be enhanced by doing the pattern analysis thing after a
sufficiently long time again and again to review the strategy of sensing. Thus
Doubly Cognitive WSN will enable current WSN to overcome the spectrum scarcity
as well as save the energy of the sensor nodes."
"Minimization of the number of cluster heads in a wireless sensor network is a
very important problem to reduce channel contention and to improve the
efficiency of the algorithm when executed at the level of cluster-heads. In
this paper, we propose an efficient method based on genetic algorithms (GAs) to
solve a sensor network optimization problem. Long communication distances
between sensors and a sink in a sensor network can greatly drain the energy of
sensors and reduce the lifetime of a network. By clustering a sensor network
into a number of independent clusters using a GA, we can greatly minimize the
total communication distance, thus prolonging the network lifetime. Simulation
results show that our algorithm can quickly find a good solution."
"Intersession network coding (NC) can provide significant performance benefits
via mixing packets at wireless routers; these benefits are especially
pronounced when NC is applied in conjunction with intelligent link scheduling.
NC however imposes certain processing operations, such as encoding, decoding,
copying and storage. When not utilized carefully, all these operations can
induce tremendous processing overheads in practical, wireless, multi-rate
settings. Our measurements with prior NC implementations suggest that such
processing operations severely degrade the router throughput, especially at
high bit rates. Motivated by this, we design {\bf NCRAWL}, a Network Coding
framework for Rate Adaptive Wireless Links. The design of NCRAWL facilitates
low overhead NC functionalities, thereby effectively approaching the
theoretically expected capacity benefits of joint NC and scheduling. We
implement and evaluate NCRAWL on a wireless testbed. Our experiments
demonstrate that NCRAWL meets the theoretical predicted throughput gain while
requiring much less CPU processing, compared to related frameworks."
"To understand the factors that encourage the deployment of a new networking
technology, we must be able to model how such technology gets deployed. We
investigate how network structure influences deployment with a simple
deployment model and different network models through computer simulations. The
results indicate that a realistic model of networking technology deployment
should take network structure into account."
"We deal with the problem of streaming multiple video streams between pairs of
nodes in a multi-hop wireless ad hoc network. The nodes are static, know their
locations, and are synchronized (via GPS). We introduce a new interference
model that uses variable interference radiuses. We present an algorithm for
computing a frequency assignment and a schedule whose goal is to maximize
throughput over all the video streams. In addition, we developed a localized
flow-control mechanism to stabilize the queue lengths.
  We simulated traffic scheduled by the algorithm using OMNET++/MixiM (i.e.,
physical SINR interference model with 802.11g) to test whether the computed
throughput is achieved. The results of the simulation show that the computed
solution is \SINR-feasible and achieves predictable stable throughputs."
"The mmWave communication system is operating at a regime with high number of
antennas and very limited number of RF analog chains. Large number of antennas
are used to extend the communication range for recovering the high path loss
while fewer RF analog chains are designed to reduce transmit and processing
power and hardware complexity. In this regime, typical MIMO algorithms are not
applicable.
  Before any communication starts, devices are needed to align their beam
pointing angles towards each other. An efficient searching protocol to obtain
the best beam angle pair is therefore needed. It is called BeamForming (BF)
training protocol.
  This paper presents a new BF training technique called beam coding. Each beam
angle is assigned unique signature code. By coding multiple beam angles and
steering at their angles simultaneously in a training packet, the best beam
angle pair can be obtained in a few packets. The proposed BF training technique
not only shows the robustness in non-line-of-sight environment, but also
provides very flat power variations within a packet in contrast to the IEEE
802.11ad standard whose scheme may lead to large dynamic range of signals due
to beam angles varying across a training packet."
"Enabling real time applications in wireless sensor networks requires certain
delay and bandwidth which pose more challenges in the design of routing
protocols. The algorithm that is used for packet routing in such applications
should be able to establish a tradeoff between end to end delay parameter and
energy consumption. In this paper, we propose a new multi path routing
algorithm for real time applications in wireless sensor networks namely QEMPAR
which is QoS aware and can increase the network lifetime. Simulation results
show that the proposed algorithm is more efficient than previous algorithms in
providing quality of service requirements of real-time applications."
"Nowadays, multimedia and real-time applications consume much network
resources and so, need high flow rates and very small transfer delay. The
current ad hoc networks (MANETs), in their original state, are not able to
satisfy the requirements of quality of service (QoS). Researches for improving
QoS in these networks are main topics and a subject of intensive researches. In
Adhoc networks, the routing phase plays an important role for improving QoS.
Numerous routing protocols (proactive, reactive and hybrid) were proposed. AODV
(Adhoc On demand Distance Vector) is probably the more treated in literature In
this article, we propose a new variant based on the AODV which gives better
results than the original AODV protocol with respect of a set of QoS parameters
and under different constraints, taking into account the limited resources of
mobile environments (bandwidth, energy, etc...). The proposed variant (M-AODV)
suggests that the discovering operation for paths reconstruction should be done
from the source. It also defines a new mechanism for determining multiple
disjoint (separated) routes. To validate our solution, simulations were made
under Network Simulator (NS2). We measure traffic control and packet loss rate
under diverse constraints (mobility, energy and scale)."
"Coloring is used in wireless networks to improve communication efficiency,
mainly in terms of bandwidth, energy and possibly end-to-end delays. In this
research report, we define the h-hop node coloring problem, with h any positive
integer. We prove that the associated decision problem is NP-complete. We then
present a 3-hop distributed coloring algorithm that is optimized for dense
networks: a node does not need to exchange the priorities and colors of its
2-hop neighbors. Through simulation results, we highlight the impact of
priority assignment on the number of colors obtained for any network. We then
focus on grids and identify a color pattern that can be reproduced to color the
whole grid. We show how the coloring algorithm can use regularity properties to
obtain a periodic color pattern with the optimal number of colors. We then
consider grids with holes and study how to extend our results."
"With the fast development of video and voice network applications, CDN
(Content Distribution Networks) and P2P (Peer-to-Peer) content distribution
technologies have gradually matured. How to effectively use Internet resources
thus has attracted more and more attentions. For the study of resource pricing,
a whole pricing strategy containing pricing models, mechanisms and methods
covers all the related topics. We first introduce three basic Internet resource
pricing models through an Internet cost analysis. Then, with the evolution of
service types, we introduce several corresponding mechanisms which can ensure
pricing implementation and resource allocation. On network resource pricing
methods, we discuss the utility optimization in economics, and emphasize two
classes of pricing methods (including system optimization and entities'
strategic optimizations). Finally, we conclude the paper and forecast the
research direction on pricing strategy which is applicable to novel service
situation in the near future."
"This paper argues for the adoption of a information centric system model
instead of the current service-oriented one. We present an architecture for a
global information storage and dissemination network which provides for
efficient interaction and coordination among autonomous actors through a shared
information space. We believe that the resulting, loosely coupled systems,
while probabilistic in nature, will lead to robust outcomes at large scales."
"Mobile Adhoc Network is a kind of wireless ad hoc network where nodes are
connected wirelessly and the network is self configuring. MANET may work in a
standalone manner or may be a part of another network. In this paper we have
compared Random Walk Mobility Model and Random Waypoint Mobility Model over two
reactive routing protocols Dynamic Source Routing (DSR) and Adhoc On-Demand
Distance Vector Routing (AODV) protocol and one Proactive routing protocol
Distance Sequenced Distance Vector Routing (DSDV) Our analysis showed that DSR,
AODV & DSDV under Random Walk and Random Way Point Mobility models have similar
results for similar inputs however as the pause time increases so does the
difference in performance rises. They show that their motion, direction, angle
of direction, speed is same under both mobility models. We have made their
analysis on packet delivery ratio, throughput and routing overhead. We have
tested them with different criteria like different number of nodes, speed and
different maximum number of connections."
"Consider a wireless network of n nodes represented by a graph G=(V, E) where
an edge (i,j) models the fact that transmissions of i and j interfere with each
other, i.e. simultaneous transmissions of i and j become unsuccessful. Hence it
is required that at each time instance a set of non-interfering nodes
(corresponding to an independent set in G) access the wireless medium. To
utilize wireless resources efficiently, it is required to arbitrate the access
of medium among interfering nodes properly. Moreover, to be of practical use,
such a mechanism is required to be totally distributed as well as simple. As
the main result of this paper, we provide such a medium access algorithm. It is
randomized, totally distributed and simple: each node attempts to access medium
at each time with probability that is a function of its local information. We
establish efficiency of the algorithm by showing that the corresponding network
Markov chain is positive recurrent as long as the demand imposed on the network
can be supported by the wireless network (using any algorithm). In that sense,
the proposed algorithm is optimal in terms of utilizing wireless resources. The
algorithm is oblivious to the network graph structure, in contrast with the
so-called `polynomial back-off' algorithm by Hastad-Leighton-Rogoff (STOC '87,
SICOMP '96) that is established to be optimal for the complete graph and
bipartite graphs (by Goldberg-MacKenzie (SODA '96, JCSS '99))."
"We present a novel feedback protocol for wireless broadcast networks that
utilize linear network coding. We consider transmission of packets from one
source to many receivers over a single-hop broadcast erasure channel. Our
method utilizes a predictive model to request feedback only when the
probability that all receivers have completed decoding is significant. In
addition, our proposed NACK-based feedback mechanism enables all receivers to
request, within a single time slot, the number of retransmissions needed for
successful decoding. We present simulation results as well as analytical
results that show the favorable scalability of our technique as the number of
receivers, file size, and packet erasure probability increase. We also show the
robustness of this scheme to uncertainty in the predictive model, including
uncertainty in the number of receiving nodes and the packet erasure
probability, as well as to losses of the feedback itself. Our scheme, SMART, is
shown to perform nearly as well as an omniscient transmitter that requires no
feedback. Furthermore, SMART, is shown to outperform current state of the art
methods at any given erasure probability, file size, and numbers of receivers."
"The goal of traffic management is efficiently utilizing network resources via
adapting of source sending rates and routes selection. Traditionally, this
problem is formulated into a utilization maximization problem. The single-path
routing scheme fails to react to instantaneous network congestion. Multi-path
routing schemes thus have been proposed aiming at improving network efficiency.
Unfortunately, the natural optimization problem to consider is concave but not
strictly concave. It thus brings a huge challenge to design stable multi-path
congestion control algorithms.
  In this paper, we propose a generalized multi-path utility maximization model
to consider the problem of routes selection and flow control, and derive a
family of multi-path dual congestion control algorithms. We show that the
proposed algorithms are stable in the absence of delays. We also derive
decentralized and scalable sufficient conditions for a particular scheme when
propagation delays exist in networks. Simulations are implemented using both
Matlab and NS2, on which evaluation of the proposed multi-path dual algorithms
is exerted. The comparison results, between the proposed algorithms and the
other two existing algorithms, show that the proposed multi-path dual
algorithms with appropriate parameter settings can achieve a stable aggregated
throughput while maintaining fairness among the involved users."
"Mesurer avec pr\'ecision la dynamique des graphes de terrain est une t\^ache
difficile, car les propri\'et\'es observ\'ees peuvent \^etre biais\'ees pour
diff\'erentes raisons, en particulier le fait que la p\'eriode de mesure soit
finie. Dans ce papier, nous introduisons une m\'ethodologie g\'en\'erale qui
nous permet de savoir si la fen\^etre d'observation est suffisamment longue
pour caract\'eriser une propri\'et\'e donn\'ee dans n'importe quel syst\`eme
dynamique. Nous appliquons cette m\'ethodologie \`a l'\'etude des dur\'ees de
sessions et des dur\'ees de vie des fichiers sur deux jeux de donn\'ees P2P.
Nous montrons que le comportement des propri\'et\'es est diff\'erent : pour les
dur\'ees de sessions, notre m\'ethodologie nous permet de caract\'eriser avec
pr\'ecision la forme de leur distribution. Par contre, pour les dur\'ees de vie
des fichiers, nous montrons que cette propri\'et\'e ne peut pas \^etre
caract\'eris\'ee, soit parce qu'elle n'est pas stationnaire, soit parce que la
dur\'ee de notre mesure est trop courte."
"In this paper, we first propose a general interpolation algorithm in a free
module of a linearized polynomial ring, and then apply this algorithm to decode
several important families of codes, Gabidulin codes, KK codes and MV codes.
Our decoding algorithm for Gabidulin codes is different from the polynomial
reconstruction algorithm by Loidreau. When applied to decode KK codes, our
interpolation algorithm is equivalent to the Sudan-style list-1 decoding
algorithm proposed by K/""otter and Kschischang for KK codes. The general
interpolation approach is also capable of solving the interpolation problem for
the list decoding of MV codes proposed by Mahdavifar and Vardy, and has a lower
complexity than solving linear equations."
"The exponential increase of multimedia services by the mobile users requires
seamless connectivity with cost effective Quality of Service QoS provisioning.
For providing such on-demand QoS, the network needs to utilize the radio
channels among the Mobile Hosts (MHs) effectively. We use vector genetic
algorithm VGA for temporal imploration of sharable channel(s) from the
neighbouring cells to fulfill the needs of a cell. We propose a new micro-level
temporal channel imploration mechanism MiCi, which promptly allocates available
borrowing channel s of the neighbouring cell(s) to the needy cell. The novelty
of MiCi is scalability, high availability, and on demand allocation of the
channels to the desired cells. The performance of our model has been tested by
simulation against a standard FCA scheme as well as a Greedy Borrowing
Heuristic. In all the test cases MiCi shows promising results in comparison to
both the schemes."
"Seeders (peers that do not request anything but contribute to the system) are
a powerful concept in peer-to-peer (P2P). They allow to leverage the capacities
of a P2P system. While seeding is a natural idea for filesharing or
video-on-demand applications, it seems somehow counter-intuitive in the context
of live streaming. This paper aims at describing the feasibility and
performance of P2P live seeding. After a formal definition of ""live seeding""
and efficiency, we consider the theoretical performance of systems where the
overhead is neglected. We then propose a linear overhead model and extend the
results for this model, for a single seeder and for a set of seeders as well
(it is not always possible to perfectly aggregate individual efficiencies in a
given system)."
"In this work, we develop a distributed source routing algorithm for topology
discovery suitable for ISP transport networks, that is however inspired by
opportunistic algorithms used in ad hoc wireless networks. We propose a
plug-and-play control plane, able to find multiple paths toward the same
destination, and introduce a novel algorithm, called adaptive probabilistic
flooding, to achieve this goal. By keeping a small amount of state in routers
taking part in the discovery process, our technique significantly limits the
amount of control messages exchanged with flooding -- and, at the same time, it
only minimally affects the quality of the discovered multiple path with respect
to the optimal solution. Simple analytical bounds, confirmed by results
gathered with extensive simulation on four realistic topologies, show our
approach to be of high practical interest."
"Intel Ethernet Flow Director is an advanced network interface card (NIC)
technology. It provides the benefits of parallel receive processing in
multiprocessing environments and can automatically steer incoming network data
to the same core on which its application process resides. However, our
analysis and experiments show that Flow Director cannot guarantee in-order
packet delivery in multiprocessing environments. Packet reordering causes
various negative impacts. E.g., TCP performs poorly with severe packet
reordering. In this paper, we use a simplified model to analyze why Flow
Director can cause packet reordering. Our experiments verify our analysis."
"Receive side scaling (RSS) is a network interface card (NIC) technology. It
provides the benefits of parallel receive processing in multiprocessing
environments. However, existing RSS-enabled NICs lack a critical data steering
mechanism that would automatically steer incoming network data to the same core
on which its application process resides. This absence causes inefficient cache
usage if an application is not running on the core on which RSS has scheduled
the received traffic to be processed. In Linux systems, it cannot even ensure
that packets in a TCP flow are processed by a single core, even if the
interrupts for the flow are pinned to a specific core. This results in degraded
performance. In this paper, we develop such a data steering mechanism in the
NIC for multicore or multiprocessor systems. This data steering mechanism is
mainly targeted at TCP, but it can be extended to other transport layer
protocols. We term a NIC with such a data steering mechanism ""A Transport
Friendly NIC"" (A-TFN). Experimental results have proven the effectiveness of
A-TFN in accelerating TCP/IP performance."
"The paper aims to design cross-layer optimal scheduling algorithms for
cooperative multi-hop Cognitive Radio Networks (CRNs), where secondary users
(SUs) assist primary user (PU)'s multi-hop transmissions and in return gain
authorization to access a share of the spectrum. We build two models for two
different types of PUs, corresponding to elastic and inelastic service classes.
For CRNs with elastic service, the PU maximizes its throughput while assigning
a time-share of the channel to SUs proportional to SUs' assistance. For the
inelastic case, the PU is guaranteed a minimum utility. The proposed algorithm
for elastic PU model can achieve arbitrarily close to the optimal PU
throughput, while the proposed algorithm for inelastic PU model can achieve
arbitrarily close to the optimal SU utility. Both algorithms provide
deterministic upper-bounds for PU queue backlogs. In addition, we show a
tradeoff between throughput/utility and PU's average end-to-end delay
upper-bounds for both algorithms. Furthermore, the algorithms work in both
backlogged as well as arbitrary arrival rate systems."
"Recently, wireless communication industries have begun to extend their
services to machine-type communication devices as well as to user equipments.
Such machine-type communication devices as meters and sensors need intermittent
uplink resources to report measured or sensed data to their serving data
collector. It is however hard to dedicate limited uplink resources to each of
them. Thus, efficient service of a tremendous number of devices with low
activities may consider simple random access as a solution. The data collectors
receiving the measured data from many sensors simultaneously can successfully
decode only signals with signal-to-interference-plus-noise-ratio (SINR) above a
certain value. The main design issues for this environment become how many data
collectors are needed, how much power sensor nodes transmit with, and how
wireless channels affect the performance. This paper provides answers to those
questions through a stochastic analysis based on a spatial point process and on
simulations."
"In this paper, we explore what \emph{network economics} is all about,
focusing on the interesting topics brought about by the Internet. Our intent is
make this a brief survey, useful as an outline for a course on this topic, with
an extended list of references. We try to make it as intuitive and readable as
possible. We also deliberately try to be critical at times, and hope our
interpretation of the topic will lead to interests for further discussions by
those doing research in the same field."
"We consider wireless mesh networks and the problem of scheduling the links of
a given set of routes under the assumption of a heavy-traffic pattern. We
assume some TDMA protocol provides a background of synchronized time slots and
seek to schedule the routes' links to maximize the number of packets that get
delivered to their destinations per time slot. Our approach is to construct an
undirected graph G and to heuristically obtain node multicolorings for G that
can be turned into efficient link schedules. In G each node represents a link
to be scheduled and the edges are set up to represent every possible
interference for any given set of interference assumptions. We present two
multicoloring-based heuristics and study their performance through extensive
simulations. One of the two heuristics is based on relaxing the notion of a
node multicoloring by dynamically exploiting the availability of communication
opportunities that would otherwise be wasted. We have found that, as a
consequence, its performance is significantly superior to the other's."
"The goal of this paper is to establish a general approach for analyzing
queueing models with repeated inhomogeneous vacations. The server goes on for a
vacation if the inactivity prolongs more than the vacation trigger duration.
Once the system enters in vacation mode, it may continue for several
consecutive vacations. At the end of a vacation, the server goes on another
vacation, possibly with a different probability distribution; if during the
previous vacation there have been no arrivals. However the system enters in
vacation mode only if the inactivity is persisted beyond defined trigger
duration. In order to get an insight on the influence of parameters on the
performance, we choose to study a simple M/G/1 queue (Poisson arrivals and
general independent service times) which has the advantage of being tractable
analytically. The theoretical model is applied to the problem of power saving
for mobile devices in which the sleep durations of a device correspond to the
vacations of the server. Various system performance metrics such as the frame
response time and the economy of energy are derived. A constrained optimization
problem is formulated to maximize the economy of energy achieved in power save
mode, with constraints as QoS conditions to be met. An illustration of the
proposed methods is shown with a WiMAX system scenario to obtain design
parameters for better performance. Our analysis allows us not only to optimize
the system parameters for a given traffic intensity but also to propose
parameters that provide the best performance under worst case conditions."
"There is a trend of applying machine learning algorithms to cognitive radio.
One fundamental open problem is to determine how and where these algorithms are
useful in a cognitive radio network. In radar and sensing signal processing,
the control of degrees of freedom (DOF)---or dimensionality---is the first
step, called pre-processing. In this paper, the combination of dimensionality
reduction with SVM is proposed apart from only applying SVM for classification
in cognitive radio. Measured Wi-Fi signals with high signal to noise ratio
(SNR) are employed to the experiments. The DOF of Wi-Fi signals is extracted by
dimensionality reduction techniques. Experimental results show that with
dimensionality reduction, the performance of classification is much better with
fewer features than that of without dimensionality reduction. The error rates
of classification with only one feature of the proposed algorithm can match the
error rates of 13 features of the original data. The proposed method will be
further tested in our cognitive radio network testbed."
"""Handover"" is one of the techniques used to achieve the service continuity in
Fourth generation wireless networks (FGWNs). Seamless continuity is the main
goal in fourth generation Wireless networks (FGWNs), when a mobile terminal
(MT) is in overlapping area for service continuity Handover mechanism are
mainly used While moving in the heterogeneous wireless networks continual
connection is the main challenge. Vertical handover is used as a technique to
minimize the processing delay in heterogeneous wireless networks this paper,
Vertical handover decision schemes are compared and Technique of order
preference by similarity to ideal solution (TOPSIS) in a distributed manner.
TOPSIS is used to choose the best network from the available Visitor networks
(VTs) for the continuous connection by the mobile terminal. In our work we
mainly concentrated to the handover decision Phase and to reduce the processing
delay in the period of handover"
"The type of business relationships between the Internet autonomous systems
(AS) determines the BGP inter-domain routing. Previous works on inferring AS
relationships relied on the connectivity information between ASes. In this
paper we infer AS relationships by analysing the routing polices of ASes
encoded in the BGP attributes Communities and the Locpref. We accumulate BGP
data from RouteViews, RIPE RIS and the public Route Servers in August 2010 and
February 2011. Based on the routing policies extracted from data of the two BGP
attributes, we obtain AS relationships for 39% links in our data, which include
all links among the Tier-1 ASes and most links between Tier-1 and Tier-2 ASes.
We also reveal a number of special AS relationships, namely the hybrid
relationship, the partial-transit relationship, the indirect peering
relationship and the backup links. These special relationships are relevant to
a better understanding of the Internet routing. Our work provides a profound
methodological progress for inferring the AS relationships."
"Intelligent products carrying their own information are more and more present
nowadays. In recent years, some authors argued the usage of such products for
the Supply Chain Management Industry. Indeed, a multitude of informational
vectors take place in such environments like fixed databases or manufactured
products on which we are able to embed significant proportion of data. By
considering distributed database systems, we can allocate specific data
fragments to the product useful to manage its own evolution. The paper aims to
analyze the Supply Chain performance according to different strategies of
information distribution. Thus, different distribution patterns between
informational vectors are studied. The purpose is to determine the key factors
which lead to improve information distribution performance in term of time
properties."
"Ecosystems monitoring is essential to properly understand their development
and the effects of events, both climatological and anthropological in nature.
The amount of data used in these assessments is increasing at very high rates.
This is due to increasing availability of sensing systems and the development
of new techniques to analyze sensor data. The Enviro-Net Project encompasses
several of such sensor system deployments across five countries in the
Americas. These deployments use a few different ground-based sensor systems,
installed at different heights monitoring the conditions in tropical dry
forests over long periods of time. This paper presents our experience in
deploying and maintaining these systems, retrieving and pre-processing the
data, and describes the Web portal developed to help with data management,
visualization and analysis."
"We consider Multi-User MIMO (MU-MIMO) scheduling in the 3GPP LTE-Advanced
(3GPP LTE-A) cellular uplink. The 3GPP LTE-A uplink allows for precoded
multi-stream (precoded MIMO) transmission from each scheduled user and also
allows flexible multi-user (MU) scheduling wherein multiple users can be
assigned the same time-frequency resource. However, exploiting these features
is made challenging by certain practical constraints that have been imposed in
order to maintain a low signaling overhead. We show that while the scheduling
problem in the 3GPP LTE-A cellular uplink is NP-hard, it can be formulated as
the maximization of a submodular set function subject to one matroid and
multiple knapsack constraints. We then propose constant-factor polynomial-time
approximation algorithms and demonstrate their superior performance via
simulations."
"We note a fact which is simple, but may be useful for the networking research
community: essentially any change to BGP's decision process can cause
divergence --- or convergence when BGP would otherwise diverge."
"This paper describes the background of smart information infrastructure and
the needs for smart grid information security. It introduces the conceptual
analysis to the methodology with the application of hermeneutic circle and
information security functional requirement identification. Information
security for the grid market cover matters includes automation and
communications industry that affects the operation of electric power systems
and the functioning of the utilities that manage them and its awareness of this
information infrastructure has become critical to the reliability of the power
system. Community benefits from of cost savings, flexibility and deployment
along with the establishment of wireless communications. However, concern
revolves around the security protections for easily accessible devices such as
the smart meter and the related communications hardware. On the other hand, the
changing points between traditional versus smart grid networking trend and the
information security importance on the communication field reflects the
criticality of grid information security functional requirement identification.
The goal of this paper is to identify the functional requirement and relate its
significance addresses to the consumer requirement of an information security
of a smart grid. Vulnerabilities may bring forth possibility for an attacker to
penetrate a network, make headway admission to control software, alter it to
load conditions that destabilize the grid in unpredictable ways. Focusing on
the grid information security functional requirement is stepping ahead in
developing consumer trust and satisfaction toward smart grid completeness."
"Policy-based management (PBM) is being used as technological solution on the
managing and controlling complex networks and systems. One of the most
important issues involved in the life-cycle of PBM is the policies creation
because the future decisions made by the management system depend on this, and
therefore, the network behavior. In this paper we present a novel model for
creating management policies in telecommunications networks. We propose a model
which includes a Policy Creation Process, Actors, Policy Abstraction Levels and
a Procedure for Creating Policies. An implementation of the proposed model over
the Technology Division at University of Cauca is included."
"Wireless sensor networks (WSNs) suffers from the hot spot problem where the
sensor nodes closest to the base station are need to relay more packet than the
nodes farther away from the base station. Thus, lifetime of sensory network
depends on these closest nodes. Clustering methods are used to extend the
lifetime of a wireless sensor network. However, current clustering algorithms
usually utilize two techniques; selecting cluster heads with more residual
energy, and rotating cluster heads periodically to distribute the energy
consumption among nodes in each cluster and lengthen the network lifetime. Most
of the algorithms use random selection for selecting the cluster heads. Here,
we propose a novel trajectory clustering technique for selecting the cluster
heads in WSNs. Our algorithm selects the cluster heads based on traffic and
rotates periodically. It provides the first trajectory based clustering
technique for selecting the cluster heads and to extenuate the hot spot problem
by prolonging the network lifetime."
"Wireless sensor networks (WSNs) suffers from the hot spot problem where the
sensor nodes closest to the base station are need to relay more packet than the
nodes farther away from the base station. Thus, lifetime of sensory network
depends on these closest nodes. Clustering methods are used to extend the
lifetime of a wireless sensor network. However, current clustering algorithms
usually utilize two techniques; selecting cluster heads with more residual
energy, and rotating cluster heads periodically to distribute the energy
consumption among nodes in each cluster and lengthen the network lifetime. Most
of the algorithms use random selection for selecting the cluster heads. Here,
we propose a novel trajectory clustering technique for selecting the cluster
heads in WSNs. Our algorithm selects the cluster heads based on traffic and
rotates periodically. It provides the first trajectory based clustering
technique for selecting the cluster heads and to extenuate the hot spot problem
by prolonging the network lifetime."
"Wireless sensor networks (WSNs) suffers from the hot spot problem where the
sensor nodes closest to the base station are need to relay more packet than the
nodes farther away from the base station. Thus, lifetime of sensory network
depends on these closest nodes. Clustering methods are used to extend the
lifetime of a wireless sensor network. However, current clustering algorithms
usually utilize two techniques; selecting cluster heads with more residual
energy, and rotating cluster heads periodically to distribute the energy
consumption among nodes in each cluster and lengthen the network lifetime. Most
of the algorithms use random selection for selecting the cluster heads. Here,
we propose a Fault Tolerant Trajectory Clustering (FTTC) technique for
selecting the cluster heads in WSNs. Our algorithm selects the cluster heads
based on traffic and rotates periodically. It provides the first Fault Tolerant
Trajectory based clustering technique for selecting the cluster heads and to
extenuate the hot spot problem by prolonging the network lifetime."
"In this paper, we study the optimal placement and optimal number of base
stations added to an existing wireless data network through the interference
gradient method. This proposed method considers a sub-region of the existing
wireless data network, hereafter called region of interest. In this region, the
provider wants to increase the network coverage and the users throughput. In
this aim, the provider needs to determine the optimal number of base stations
to be added and their optimal placement. The proposed approach is based on the
Delaunay triangulation of the region of interest and the gradient descent
method in each triangle to compute the minimum interference locations. We
quantify the increase of coverage and throughput."
"The analysis of large-scale complex networks is a major challenge in the Big
Data domain. Given the large-scale of the complex networks researchers commonly
deal with nowadays, the use of localized information (i.e. restricted to a
limited neighborhood around each node of the network) for centrality-based
analysis is gaining momentum in the recent literature. In this context, we
propose a framework for the Distributed Assessment of Network Centralities
(DANCE) in complex networks. DANCE offers a single environment that allows the
use of different localized centrality proposals, which can be tailored to
specific applications. This environment can be thus useful given the vast
potential applicability of centrality-based analysis on large-scale complex
networks found in different areas, such as Biology, Physics, Sociology, or
Computer Science. Since the localized centrality proposals DANCE implements
employ only localized information, DANCE can easily benefit from parallel
processing environments and run on different computing architectures. To
illustrate this, we present a parallel implementation of DANCE and show how it
can be applied to the analysis of large-scale complex networks using different
kinds of network centralities. This implementation is made available to complex
network researchers and practitioners interested in using it through a
scientific web portal."
"For two-tier networks consisting of macrocells and femtocells, the channel
access mechanism can be configured to be open access, closed access, or hybrid
access. Hybrid access arises as a compromise between open and closed access
mechanisms, in which a fraction of available spectrum resource is shared to
nonsubscribers while the remaining reserved for subscribers. This paper focuses
on a hybrid access mechanism for multi-channel femtocells which employ
orthogonal spectrum access schemes. Considering a randomized channel assignment
strategy, we analyze the performance in the downlink. Using stochastic geometry
as technical tools, we model the distribution of femtocells as Poisson point
process or Neyman-Scott cluster process and derive the distributions of
signal-to-interference-plus-noise ratios, and mean achievable rates, of both
nonsubscribers and subscribers. The established expressions are amenable to
numerical evaluation, and shed key insights into the performance tradeoff
between subscribers and nonsubscribers. The analytical results are corroborated
by numerical simulations."
"Within the wireless mesh network, a bottleneck problem arises as the number
of concurrent traffic flows (NCTF) increases over a single common control
channel, as it is for most conventional networks. To alleviate this problem,
this paper proposes a two-stage coordination multi-radio multi-channel MAC
(TSC-M2MAC) protocol that designates all available channels as both control
channels and data channels in a time division manner through a two-stage
coordination. At the first stage, a load balancing breadth-first-search-based
vertex coloring algorithm for multi-radio conflict graph is proposed to
intelligently allocate multiple control channels. At the second stage, a
REQ/ACK/RES mechanism is proposed to realize dynamical channel allocation for
data transmission. At this stage, the Channel-and-Radio Utilization Structure
(CRUS) maintained by each node is able to alleviate the hidden nodes problem;
also, the proposed adaptive adjustment algorithm for the Channel Negotiation
and Allocation (CNA) sub-interval is able to cope with the variation of NCTF.
In addition, we design a power saving mechanism for the TSC-M2MAC to decrease
its energy consumption. Simulation results show that the proposed protocol is
able to achieve higher throughput and lower end-to-end packet delay than
conventional schemes. They also show that the TSC-M2MAC can achieve load
balancing, save energy, and remain stable when the network becomes saturated."
"Peer-to-Peer (P2P) content sharing systems are susceptible to the content
pollution attack, in which attackers aggressively inject polluted contents into
the systems to reduce the availability of authentic contents, thus decreasing
the confidence of participating users.
  In this paper, we design a pollution-free P2P content sharing system, Green,
by exploiting the inherent content-based information and the social-based
reputation. In Green, a content provider (i.e., creator or sharer) publishes
the information of his shared contents to a group of content maintainers
self-organized in a security overlay for providing the mechanisms of redundancy
and reliability, so that a content requestor can obtain and filter the
information of his requested content from the associated maintainers. We employ
a reputation model to help the requestor better identify the polluted contents,
and then utilize the social (friend-related) information to enhance the
effectiveness and efficiency of our reputation model. Now, the requestor could
easily select an authentic content version for downloading. While downloading,
each requestor performs a realtime integrity verification and takes prompt
protection to handle the content pollution. To further improve the system
performance, we devise a scalable probabilistic verification scheme.
  Green is broadly applicable for both structured and unstructured overlay
applications, and moreover, it is able to defeat various kinds of content
pollution attacks without incurring significant overhead on the participating
users. The evaluation in massive-scale networks validates the success of Green
against the content pollution."
"We study network loss tomography based on observing average loss rates over a
set of paths forming a tree -- a severely underdetermined linear problem for
the unknown link loss probabilities. We examine in detail the role of sparsity
as a regularising principle, pointing out that the problem is technically
distinct from others in the compressed sensing literature. While sparsity has
been applied in the context of tomography, key questions regarding uniqueness
and recovery remain unanswered. Our work exploits the tree structure of path
measurements to derive sufficient conditions for sparse solutions to be unique
and the condition that $\ell_1$ minimization recovers the true underlying
solution. We present a fast single-pass linear algorithm for $\ell_1$
minimization and prove that a minimum $\ell_1$ solution is both unique and
sparsest for tree topologies. By considering the placement of lossy links
within trees, we show that sparse solutions remain unique more often than is
commonly supposed. We prove similar results for a noisy version of the problem."
"The performances of the routing protocols are important since they compute
the primary path between source and destination. In addition, routing protocols
need to detect failure within a short period of time when nodes move to start
updating the routing table in order to find a new primary path to the
destination. Meantime, loss of packets and end-to- end delays will increase
thereby reducing throughput and degrading the performance of the network. This
paper proposes a new algorithm, DBRT (Driven Backup Routing Table), to improve
the existing proactive protocols such as DSDV (Destination Sequenced Distance
Vector) protocol by creating a backup routing table to provide multiple
alternative routes. The DBRT algorithm identifies adjacent nodes for each node
in the same range and then selects one of these as a backup next hop according
to the available path to the destination. The results show that loss of data
packets, throughput and end-to-end delay times between source and destination
are improved. The results show that the new protocol does not degrade the
network's performance despite sending extra messages to construct and update
the new backup routing table. Simulations (using an NS2 simulator) are
undertaken to demonstrate the difference between using a DSDV protocol with or
without the proposed schema."
"With the increasing demands for real-time applications traffic in net- works
such as video and voice a high convergence time for the existing routing
protocols when failure occurred is required. These applications can be very
sensitive to packet loss when link/node goes down. In this paper, we propose
two algorithms schemas for the link state protocol to reroute the traffic in
two states; first, pre-calculated an alternative and disjoint path with the
primary one from the source to the destination by re-routing traffic through
it, regardless of the locations of failure and the number of failed links.
Second, rerouting the traffic via an alternative path from a node whose local
link is down without the need to wait until the source node knows about the
failure. This is achieved by creating a new backup routing table based on the
original routing table which is computed by the dijkstra algorithm. The goal of
these algorithms is to reduce loss of packets, end-to-end delay time, improve
throughput and avoiding local loop when nodes re-converge the topology in case
of failure."
"Network traffic is difficult to monitor and analyze, especially in
high-bandwidth networks. Performance analysis, in particular, presents extreme
complexity and scalability challenges. GPU (Graphics Processing Unit)
technology has been utilized recently to accelerate general purpose scientific
and engineering computing. GPUs offer extreme thread-level parallelism with
hundreds of simple cores. Their data-parallel execution model can rapidly solve
large problems with inherent data parallelism. At Fermilab, we have prototyped
a GPU-accelerated network performance monitoring system, called G-NetMon, to
support large-scale scientific collaborations. In this work, we explore new
opportunities in network traffic monitoring and analysis with GPUs. Our system
exploits the data parallelism that exists within network flow data to provide
fast analysis of bulk data movement between Fermilab and collaboration sites.
Experiments demonstrate that our G-NetMon can rapidly detect sub-optimal bulk
data movements."
"Beaconless position-based forwarding protocols have recently evolved as a
promising solution for packet forwarding in wireless sensor networks. However,
as the node density grows, the overhead incurred in the process of relay
selection grows significantly. As such, end-to-end performance in terms of
energy and latency is adversely impacted. With the motivation of developing a
packet forwarding mechanism that is tolerant to node density, an alternative
position-based protocol is proposed in this paper. In contrast to existing
beaconless protocols, the proposed protocol is designed such that it eliminates
the need for potential relays to undergo a relay selection process. Rather, any
eligible relay may decide to forward the packet ahead, thus significantly
reducing the underlying overhead. The operation of the proposed protocol is
empowered by exploiting favorable features of orthogonal frequency division
multiplexing (OFDM) at the physical layer. The end-to-end performance of the
proposed protocol is evaluated against existing beaconless position-based
protocols analytically and as well by means of simulations. The proposed
protocol is demonstrated in this paper to be more efficient. In particular, it
is shown that for the same amount of energy the proposed protocol transports
one bit from source to destination much quicker."
"Common WLAN pathologies include low signal-to-noise ratio, congestion, hidden
terminals or interference from non-802.11 devices and phenomena. Prior work has
focused on the detection and diagnosis of such problems using layer-2
information from 802.11 devices and special-purpose access points and monitors,
which may not be generally available. Here, we investigate a userlevel
approach: is it possible to detect and diagnose 802.11 pathologies with
strictly user-level active probing, without any cooperation from, and without
any visibility in, layer-2 devices? In this paper, we present preliminary but
promising results indicating that such diagnostics are feasible."
"Vehicular Ad Hoc Networks (VANET) is a subclass of Mobile ad hoc networks
which provides a distinguished approach for Intelligent Transport System (ITS).
The survey of routing protocols in VANET is important and necessary for smart
ITS. This paper discusses the advantages / disadvantages and the applications
of various routing protocols for vehicular ad hoc networks. It explores the
motivation behind the designed, and traces the evolution of these routing
protocols. F inally the paper concludes by a tabular comparison of the various
routing protocols for VANET."
"In this paper, we develop a partner selection protocol for enhancing the
network lifetime in cooperative wireless networks. The case-study is the
cooperative relayed transmission from fixed indoor nodes to a common outdoor
access point. A stochastic bivariate model for the spatial distribution of the
fading parameters that govern the link performance, namely the Rician K-factor
and the path-loss, is proposed and validated by means of real channel
measurements. The partner selection protocol is based on the real-time
estimation of a function of these fading parameters, i.e., the coding gain. To
reduce the complexity of the link quality assessment, a Bayesian approach is
proposed that uses the site-specific bivariate model as a-priori information
for the coding gain estimation. This link quality estimator allows network
lifetime gains almost as if all K-factor values were known. Furthermore, it
suits IEEE 802.15.4 compliant networks as it efficiently exploits the
information acquired from the receiver signal strength indicator. Extensive
numerical results highlight the trade-off between complexity, robustness to
model mismatches and network lifetime performance. We show for instance that
infrequent updates of the site-specific model through K-factor estimation over
a subset of links are sufficient to at least double the network lifetime with
respect to existing algorithms based on path loss information only."
"We construct a simple network model to provide insight into network design
strategies. We show that the model can be used to address various approaches to
network coding, MAC, and multi-packet reception so that their effects on
network throughput can be evaluated. We consider several topology components
which exhibit the same non-monotonic saturation behavior found within the Katti
et. al. COPE experiments. We further show that fairness allocation by the MAC
can seriously impact performance and cause this non-monotonic saturation. Using
our model, we develop a MAC that provides monotonic saturation, higher
saturation throughput gains and fairness among flows rather than nodes. The
proposed model provides an estimate of the achievable gains for the cross-layer
design of network coding, multi-packet reception, and MAC showing that
super-additive throughput gains on the order of six times that of routing are
possible."
"Through several studies, it has been highlighted that mobility patterns in
mobile networks are driven by human behaviors. This effect has been
particularly observed in intermittently connected networks like DTN (Delay
Tolerant Networks). Given that common social intentions generate similar human
behavior, it is relevant to exploit this knowledge in the network protocols
design, e.g. to identify the closeness degree between two nodes. In this paper,
we propose a temporal link prediction technique for DTN which quantifies the
behavior similarity between each pair of nodes and makes use of it to predict
future links. Our prediction method keeps track of the spatio-temporal aspects
of nodes behaviors organized as a third-order tensor that aims to records the
evolution of the network topology. After collapsing the tensor information, we
compute the degree of similarity for each pair of nodes using the Katz measure.
This metric gives us an indication on the link occurrence between two nodes
relying on their closeness. We show the efficiency of this method by applying
it on three mobility traces: two real traces and one synthetic trace. Through
several simulations, we demonstrate the effectiveness of the technique
regarding another approach based on a similarity metric used in DTN. The
validity of this method is proven when the computation of score is made in a
distributed way (i.e. with local information). We attest that the tensor-based
technique is effective for temporal link prediction applied to the
intermittently connected networks. Furthermore, we think that this technique
can go beyond the realm of DTN and we believe this can be further applied on
every case of figure in which there is a need to derive the underlying social
structure of a network of mobile users."
"Objective: The main objective of this paper is to construct a distributed
clustering algorithm based upon spatial data correlation among sensor nodes and
perform data accuracy for each distributed cluster at their respective cluster
head node. Design Procedure/Approach: We investigate that due to deployment of
high density of sensor nodes in the sensor field, spatial data are highly
correlated among sensor nodes in spatial domain. Based on high data correlation
among sensor nodes, we propose a non -overlapping irregular distributed
clustering algorithm with different sizes to collect most accurate or precise
data at the cluster head node for each respective distributed cluster. To
collect the most accurate data at the cluster head node for each distributed
cluster in sensor field, we propose a Data accuracy model and compare the
results with Information accuracy model. Finding: Simulation results shows that
our propose Data accuracy model collects more accurate data and gives better
performance than Information accuracy model at the cluster head node for each
respective distributed cluster in our propose distributed clustering
algorithm.Morover there exist a optimal cluster of sensor nodes which is
adequate to perform approximately the same data accuracy achieve by a cluster.
Practical Implementation: Measuring humidity and moisture content in an
agricultural field, measuring temperature in physical environment. Inventive
/Novel Idea: A distributed clustering algorithm is proposed based on spatial
data correlation among sensor nodes with Data accuracy model."
"The rapid increase of vehicular traffic and congestion on the highways began
hampering the safe and efficient movement of traffic. Consequently, year by
year, we see the ascending rate of car accidents and casualties in most of the
countries. Therefore, exploiting the new technologies, e.g. wireless sensor
networks, is required as a solution of reduction of these saddening and
reprehensible statistics. This has motivated us to propose a novel and
comprehensive system to utilize Wireless Sensor Networks for vehicular
networks. We coin the vehicular network employing wireless Sensor networks as
Vehicular Ad Hoc and Sensor Network, or VASNET in short. The proposed VASNET is
particularly for highway traffic .VASNET is a self-organizing Ad Hoc and sensor
network comprised of a large number of sensor nodes. In VASNET there are two
kinds of sensor nodes, some are embedded on the vehicles-vehicular nodes- and
others are deployed in predetermined distances besides the highway road, known
as Road Side Sensor nodes (RSS). The vehicular nodes are used to sense the
velocity of the vehicle for instance. We can have some Base Stations (BS) such
as Police Traffic Station, Firefighting Group and Rescue Team. The base
stations may be stationary or mobile. VASNET provides capability of wireless
communication between vehicular nodes and stationary nodes, to increase safety
and comfort for vehicles on the highway roads. In this paper we explain main
fundamentals and challenges of VASNET."
"Many energy-aware routing protocols have been proposed for wireless sensor
networks. Most of them are only energy savers and do not take care about energy
balancing. The energy saver protocols try to decrease the energy consumption of
the network as a whole; however the energy manager protocols balance the energy
consumption in the network to avoid network partitioning. This means that
energy saver protocols are not necessarily energy balancing and vice versa.
However, the lifetime of wireless sensor network is strictly depending on
energy consumption; therefore, energy management is an essential task to be
considered. This paper proposes an energy aware routing protocol, named FEAR,
which considers energy balancing and energy saving. It finds a fair trade-off
between energy balancing and energy saving by fuzzy set concept. FEAR routing
protocol is simulated and evaluated by Glomosim simulator."
"Network of Information (NetInf) is a term coined for networks which unlike
contemporary network are not node centric. As the name indicates, information
supersedes nodes in the network. In this report, we propose an architecture of
mobile node for NetInf. We call it NetInf Mobile Node. It is an extension of
the basic node architecture proposed for NetInf. It is compatible to NetInf and
TCP/IP based networks. The Virtual Node Layer modules in the architecture
provide support for managing mobility, power consumption of the node as well
data relaying/storing services. In- ner/Outer Locator Construction Routers (I/O
LCTR) are two functions introduced in NetInf mobile nodes to operate between
NetInf and non- NetInf sites. The basic purpose of NetInf mobile node is to
maintain the QoS during mobility events. The handoff/handover are critical
situations during mobility where chances of QoS degradation of an ongoing
session are high. This report presents one such scenario in which QoS of an
appli- cation is maintained during a handoff situations in heterogeneous
wireless network environment through our proposed algorithm."
"In this paper, we have modeled the routing over- head generated by three
reactive routing protocols; Ad-hoc On-demand Distance Vector (AODV), Dynamic
Source Routing (DSR) and DYnamic MANET On-deman (DYMO). Routing performed by
reactive protocols consists of two phases; route discovery and route
maintenance. Total cost paid by a protocol for efficient routing is sum of the
cost paid in the form of energy consumed and time spent. These protocols
majorly focus on the optimization performed by expanding ring search algorithm
to control the flooding generated by the mechanism of blind flooding. So, we
have modeled the energy consumed and time spent per packet both for route
discovery and route maintenance. The proposed framework is evaluated in NS-2 to
compare performance of the chosen routing protocols."
"In this paper, we identify and analyze the requirements to design a new
routing link metric for wireless multihop networks. Considering these
requirements, when a link metric is proposed, then both the design and
implementation of the link metric with a routing protocol become easy.
Secondly, the underlying network issues can easily be tackled. Thirdly, an
appreciable performance of the network is guaranteed. Along with the existing
implementation of three link metrics Expected Transmission Count (ETX), Minimum
Delay (MD), and Minimum Loss (ML), we implement inverse ETX; invETX with
Optimized Link State Routing (OLSR) using NS-2.34. The simulation results show
that how the computational burden of a metric degrades the performance of the
respective protocol and how a metric has to trade-off between different
performance parameters."
"In this paper, we evaluate, analyze, and compare the impact of mobility on
the behavior of three reactive protocols (AODV, DSR, DYMO) and three proactive
protocols (DSDV, FSR, OLSR) in multi-hop wireless networks. We take into
account throughput, end-to-end delay, and normalized routing load as
performance parameters. Based upon the extensive simulation results in NS-2, we
rank all of six protocols according to the performance parameters. Besides
providing the interesting facts regarding the response of each protocol on
varying mobilities and speeds, we also study the trade-offs, the routing
protocols have to make. Such as, to achieve throughput, a protocol has to pay
some cost in the form of increased end-to-end delay or routing overhead."
"We consider the scheduling problem in downlink wireless networks with
heterogeneous, Markov-modulated, ON/OFF channels. It is well-known that the
performance of scheduling over fading channels relies heavily on the accuracy
of the available Channel State Information (CSI), which is costly to acquire.
Thus, we consider the CSI acquisition via a practical ARQ-based feedback
mechanism whereby channel states are revealed at the end of only scheduled
users' transmissions. In the assumed presence of temporally-correlated channel
evolutions, the desired scheduler must optimally balance the
exploitation-exploration trade-off, whereby it schedules transmissions both to
exploit those channels with up-to-date CSI and to explore the current state of
those with outdated CSI.
  In earlier works, Whittle's Index Policy had been suggested as a
low-complexity and high-performance solution to this problem. However,
analyzing its performance in the typical scenario of statistically
heterogeneous channel state processes has remained elusive and challenging,
mainly because of the highly-coupled and complex dynamics it possesses. In this
work, we overcome these difficulties to rigorously establish the asymptotic
optimality properties of Whittle's Index Policy in the limiting regime of many
users. More specifically: (1) we prove the local optimality of Whittle's Index
Policy, provided that the initial state of the system is within a certain
neighborhood of a carefully selected state; (2) we then establish the global
optimality of Whittle's Index Policy under a recurrence assumption that is
verified numerically for our problem. These results establish that Whittle's
Index Policy possesses analytically provable optimality characteristics for
scheduling over heterogeneous and temporally-correlated channels."
"The number of contending neighbors of a node in a multihop ad hoc network has
to be adjusted while analyzing the performance of the network such as computing
the end-to-end delays along a path from a given source to a destination. In
this paper, we describe a method to adjust the number of contending neighbors
of a node in a multihop wireless ad hoc network. Our method is based on the
minimum number of neighbors that has to be common between two consecutive nodes
along a path. We derive an analytical expression for the adjustment factor."
"We propose a new model for peer-to-peer networking which takes the network
bottlenecks into account beyond the access. This model allows one to cope with
key features of P2P networking like degree or locality constraints or the fact
that distant peers often have a smaller rate than nearby peers. We show that
the spatial point process describing peers in their steady state then exhibits
an interesting repulsion phenomenon. We analyze two asymptotic regimes of the
peer-to-peer network: the fluid regime and the hard--core regime. We get closed
form expressions for the mean (and in some cases the law) of the peer latency
and the download rate obtained by a peer as well as for the spatial density of
peers in the steady state of each regime, as well as an accurate approximation
that holds for all regimes. The analytical results are based on a mix of
mathematical analysis and dimensional analysis and have important design
implications. The first of them is the existence of a setting where the
equilibrium mean latency is a decreasing function of the load, a phenomenon
that we call super-scalability."
"Mobile Ad-hoc Network (MANET) is the self organizing collection of mobile
nodes. The communication in MANET is done via a wireless media. Ad hoc wireless
networks have massive commercial and military potential because of their
mobility support. Due to demanding real time multimedia applications, Quality
of Services (QoS) support in such infrastructure less networks have become
essential. QoS routing in mobile Ad-Hoc networks is challenging due to rapid
change in network topology. Consequently, the available state information for
routing is inherently imprecise. QoS routing may suffer badly due to several
factors including radio interference on available bandwidth, and inefficient
flooding of information to the adjacent nodes. As a result the performance of
the network degrades substantially. This paper aims at the solution for energy
efficient QoS routing by best utilization of network resources such as energy
and bandwidth. A comparative study shows that despite the overhead due to QoS
management, this solution performs better than classical OLSR protocol in terms
of QoS and efficient utilization of energy."
"Currently, the development of research around VoIP experience a tremendous
growth. In the community of open source Asterisk represents a reliable
alternative for a lower cost solution. In this same community as the SIP
protocol is a supplement to the more asterisk PBX. to share the benefits
claimed by proponents of free software co-existence with other Asterisk server
is not yet proven. In this context this paper we show a comparison of the use
of simplified resource material for the apache server using the HTTP protocol
and server that uses the asterisk SIP."
"Stochastic network calculus is an evolving theory which accounts for
statistical multiplexing and uses an envelope approach for probabilistic delay
and backlog analysis of networks. One of the key ideas of stochastic network
calculus is the possibility to describe service offered at network node as a
stochastic service envelope, which in turn can be used to describe the
stochastic service available in a network of nodes and determine end-to-end
probabilistic delay and backlog bounds. This paper introduces a new definition
of stochastic service envelopes which yield a simple network service envelope
and tighter end-to-end performance bounds. It is shown for ($\sigma(\theta),
\rho(\theta)$) - constrained traffic model that the end-to-end performance
measures computed using the new stochastic network service envelope are tight
in comparison to the ones obtained using the existing start-of-the-art
definition of statistical network service envelope and are bounded by ${\cal
O}(H \log{H})$, where $H$ is the number of nodes traversed by the arrival
traffic."
"In wireless networks, the transmission rate of a link is determined by
received signal strength, interference from simultaneous transmissions, and
available coding-modulation schemes. Rate allocation is a key problem in
wireless network design, but a very challenging problem because: (i) wireless
interference is global, i.e., a transmission interferes all other simultaneous
transmissions, and (ii) the rate-power relation is non-convex and
non-continuous, where the discontinuity is due to limited number of
coding-modulation choices in practical systems. In this paper, we propose a
distributed power control and coding-modulation adaptation algorithm using
annealed Gibbs sampling, which achieves throughput optimality in an arbitrary
network topology. We consider a realistic
Signal-to-Interference-and-Noise-Ratio (SINR) based interference model, and
assume continuous power space and finite rate options (coding-modulation
choices). Our algorithm first decomposes network-wide interference to local
interference by properly choosing a ""neighborhood"" for each transmitter and
bounding the interference from non-neighbor nodes. The power update policy is
then carefully designed to emulate a Gibbs sampler over a Markov chain with a
continuous state space. We further exploit the technique of simulated annealing
to speed up the convergence of the algorithm to the optimal power and
coding-modulation configuration. Finally, simulation results demonstrate the
superior performance of the proposed algorithm."
"Energy efficiency in cellular networks is a growing concern for cellular
operators to not only maintain profitability, but also to reduce the overall
environment effects. This emerging trend of achieving energy efficiency in
cellular networks is motivating the standardization authorities and network
operators to continuously explore future technologies in order to bring
improvements in the entire network infrastructure. In this article, we present
a brief survey of methods to improve the power efficiency of cellular networks,
explore some research issues and challenges and suggest some techniques to
enable an energy efficient or ""green"" cellular network. Since base stations
consume a maximum portion of the total energy used in a cellular system, we
will first provide a comprehensive survey on techniques to obtain energy
savings in base stations. Next, we discuss how heterogeneous network deployment
based on micro, pico and femto-cells can be used to achieve this goal. Since
cognitive radio and cooperative relaying are undisputed future technologies in
this regard, we propose a research vision to make these technologies more
energy efficient. Lastly, we explore some broader perspectives in realizing a
""green"" cellular network technology"
"To reduce datacenter energy consumption and cost, current practice has
considered demand-proportional resource provisioning schemes, where servers are
turned on/off according to the load of requests.
  Most existing work considers instantaneous (Internet) requests only, which
are explicitly or implicitly assumed to be delay-sensitive. On the other hand,
in datacenters, there exist a vast amount of delay-tolerant jobs, such as
background/maintainance jobs. In this paper, we explicitly differentiate
delay-sensitive jobs and delay tolerant jobs. We focus on the problem of using
delay-tolerant jobs to fill the extra capacity of datacenters, referred to as
trough/valley filling. Giving a higher priority to delay-sensitive jobs, our
schemes complement to most existing demand-proportional resource provisioning
schemes. Our goal is to design intelligent trough filling mechanisms that are
energy efficient and also achieve good delay performance. Specifically, we
propose two joint dynamic speed scaling and traffic shifting schemes, one
subgradient-based and the other queue-based. Our schemes assume little
statistical information of the system, which is usually difficult to obtain in
practice. In both schemes, energy cost saving comes from dynamic speed scaling,
statistical multiplexing, electricity price diversity, and service efficiency
diversity. In addition, good delay performance is achieved in the queue-based
scheme via load shifting and capacity allocation based on queue conditions.
Practical issues that may arise in datacenter networks are considered,
including capacity and bandwidth constraint, service agility constraint, and
load shifting cost. We use both artificial and real datacenter traces to
evaluate the proposed schemes."
"Frame Synchronization (FS) is required in several communication standards in
order to recover the individual frames that have been aggregated in a burst.
This paper proposes a low-delay and reducedcomplexity Sliding Trellis
(ST)-based FS technique, compared to our previously proposed trellis-based FS
method. Each burst is divided into overlapping windows in which FS is
performed. Useful information is propagated from one window to the next. The
proposed method makes use of soft information provided by the channel, but also
of all sources of redundancy present in the protocol stack. An illustration of
our STbased approach for the WiMAX Media Access Control (MAC) layer is
provided. When FS is performed on bursts transmitted over Rayleigh fading
channel, the ST-based approach reduces the FS latency and complexity at the
cost of a very small performance degradation compared to our full complexity
trellis-based FS and outperforms state-of-the-art FS techniques."
"This paper introduces a reconstruction approach for the input signal of an
oversampled filter bank (OFB) when the sub-bands generated at its output are
quantized and transmitted over a noisy channel. This approach exploits the
redundancy introduced by the OFB and the fact that the quantization noise is
bounded. A maximum-likelihood estimate of the input signal is evaluated, which
only considers the vectors of quantization indexes corresponding to subband
signals that could have been generated by the OFB and that are compliant with
the quantization errors. When considering an OFB with an oversampling ratio of
3/2 and a transmission of quantized subbands on an AWGN channel, compared to a
classical decoder, the performance gains are up to 9 dB in terms of SNR for the
reconstructed signal, and 3 dB in terms of channel SNR."
"Systems of networked mobile robots, such as unmanned aerial or ground
vehicles, will play important roles in future military and commercial
applications. The communications for such systems will typically be over
wireless links and may require that the robots form an ad hoc network and
communicate on a peer-to-peer basis. In this paper, we consider the problem of
optimizing the network topology to minimize the total traffic in a network
required to support a given set of data flows under constraints on the amount
of movement possible at each mobile robot. In this paper, we consider a
subclass of this problem in which the initial and final topologies are trees,
and the movement restrictions are given in terms of the number of edges in the
graph that must be traversed. We develop algorithms to optimize the network
topology while maintaining network connectivity during the topology
reconfiguration process. Our topology reconfiguration algorithm uses the
concept of prefix labelling and routing to move nodes through the network while
maintaining network connectivity. We develop two algorithms to determine the
final network topology: an optimal, but computationally complex algorithm, and
a greedy suboptimal algorithm that has much lower complexity. We present
simulation results to compare the performance of these algorithm."
"Recovery from transient failures is one of the prime issues in the context of
distributed systems. These systems demand to have transparent yet efficient
techniques to achieve the same. Checkpoint is defined as a designated place in
a program where normal processing of a system is interrupted to preserve the
status information. Checkpointing is a process of saving status information.
Mobile computing systems often suffer from high failure rates that are
transient and independent in nature. To add reliability and high availability
to such distributed systems, checkpoint based rollback recovery is one of the
widely used techniques for applications such as scientific computing, database,
telecommunication applications and mission critical applications. This paper
surveys the algorithms which have been reported in the literature for
checkpointing in Mobile Computing Systems."
"Recent trends show that there are swift developments and fast convergence of
wireless and mobile communication networks with internet services to provide
the quality of ubiquitous access to network users. Most of the wireless
networks and mobile cellular networks are moving to be all IP based. These
networks are connected through the private IP core networks using the TCP/IP
protocol or through the Internet. As such, there is room to improve the
mobility support through the Internet and support ubiquitous network access by
providing seamless handover. This is especially true with the invention of
portable mobile and laptop devices that can be connected almost everywhere at
any time. However, the recent explosion on the usage of mobile and laptop
devices has also generated several issues in terms of performance and quality
of service. Nowadays, mobile users demand high quality performance, best
quality of services and seamless connections that support real-time application
such as audio and video streaming. The goal of this paper is to study the
impact and evaluate the mobility management protocols under micro mobility
domain on link layer and network layer handover performance. Therefore, this
paper proposes an integration solution of network-based mobility management
framework, based on Proxy Mobile IPv6, to alleviate handover latency, packet
loss and increase throughput and the performance of video transmission when
mobile host moves to new network during handover on high speed mobility.
Simulations are conducted to analyze the relationship between the network
performances with the moving speed of mobile host over mobility protocols.
Based on simulation results, we presented and analyzed the results of mobility
protocols under intra-domain traffics in micro mobility domain."
"Fast Handovers for the MIPv6 (FMIPv6) has been proposed to reduce the
Handover latency, in the IETF. It could not find the acceptable reduction, so
led to more efforts to improve it and however the creation of multiple Handover
methods in the literature. A stable connection is very important in mobile
services so the mobility of device would not cause any interruption in network
services and thus mobility management plays a very important role. Mobile IPv6
has become a general solution for supporting mobility between different
networks on the internet which a flawless connection needs to be managed
properly. In order to select the appropriate method, in this paper, all the
proposed methods have been classified according to the identified performance
metrics. Call blocking probability, Handover blocking probability, Probability
of an unnecessary handover, Duration of interruption and delay, as the most
important Handover algorithm performance metrics are introduced. The AHP method
will be deployed to weight the metrics in a sample topology according to the
selected sound application. Then the TOPSIS method will be employed to find the
appropriate Handover algorithm."
"Today's wireless networks are characterized by fixed spectrum assignment
policy. The limited available spectrum and the inefficiency in the spectrum
usage necessitate a new communication paradigm to exploit the existing wireless
spectrum opportunistically. Cognitive radio is a paradigm for wireless
communication in which either a network or a wireless node changes its
transmission or reception parameters to communicate efficiently avoiding
interference with licensed or unlicensed users. In this work, a fuzzy logic
based system for spectrum management is proposed where the radio can share
unused spectrum depending on some parameters like distance, signal strength,
node velocity and availability of unused spectrum. The system is simulated and
is found to give satisfactory results."
"Recently, the world has witnessed the increasing occurrence of disasters,
some of natural origin and others caused by man. The intensity of the
phenomenon that cause such disasters, the frequency in which they occur, the
number of people affected and the material damage caused by them have been
growing substantially. Disasters are defined as natural, technological, and
human-initiated events that disrupt the normal functioning of the economy and
society on a large scale. Areas where disasters have occurred bring many
dangers to rescue teams and the communication network infrastructure is usually
destroyed. To manage these hazards, different wireless technologies can be
launched in the area of disaster. This paper discusses the innovative wireless
technologies for Disaster Management. Specifically, issues related to the
design of Hierarchical Hybrid Communication Network (arising in the
communication network for disaster relief) are discussed."
"Data mining is used to extract hidden information from large databases. In
Peer-to-Peer context, a challenging problem is how to find the appropriate Peer
to deal with a given query without overly consuming bandwidth. Different
methods proposed routing strategies of queries taking into account the P2P
network at hand. An unstructured P2P system based on an organization of Peers
around Super-Peers that are connected to Super-Super-Peer according to their
semantic domains is considered. This paper integrates Decision Trees in P2P
architectures for predicting Query-Suitable Super-Peers representing a
community of Peers, where one among them is able to answer the given query. In
fact, by analyzing the queries' log file, a predictive model that avoids
flooding queries in the P2P networks constructed by predicting the appropriate
Super-Peer, and hence the Peer to answer the query. The proposed architecture
is based on a Decision Tree (Base-Knowledge - BK). The efficiency of these
architectures is discussed considering architecture without knowledge
(Baseline) using only the flooding queries method to answer queries. The
advantage of this knowledge based model is the robustness in Queries routing
mechanism and scalability in P2P Network."
"In Peer-to-Peer context, a challenging problem is how to find the appropriate
peer to deal with a given query without overly consuming bandwidth? Different
methods proposed routing strategies of queries taking into account the P2P
network at hand. This paper considers an unstructured P2P system based on an
organization of peers around Super-Peers that are connected to Super-Super-
Peer according to their semantic domains; By analyzing the queries log file, a
predictive model that avoids flooding queries in the P2P network is constructed
after predicting the appropriate Super-Peer, and hence the peer to answer the
query. A challenging problem in a schema-based Peer-to-Peer (P2P) system is how
to locate peers that are relevant to a given query. In this paper,
architecture, based on (Super-)Peers is proposed, focusing on query routing.
The approach to be implemented, groups together (Super-)Peers that have similar
interests for an efficient query routing method. In such groups, called
Super-Super-Peers (SSP), Super-Peers submit queries that are often processed by
members of this group. A SSP is a specific Super-Peer which contains knowledge
about: 1. its Super-Peers and 2. The other SSP. Knowledge is extracted by using
data mining techniques (e.g. Decision Tree algorithms) starting from queries of
peers that transit on the network. The advantage of this distributed knowledge
is that, it avoids making semantic mapping between heterogeneous data sources
owned by (Super-)Peers, each time the system decides to route query to other
(Super-) Peers. The set of SSP improves the robustness in queries routing
mechanism, and the scalability in P2P Network. Compared with a baseline
approach,the proposal architecture shows the effect of the data mining with
better performance in respect to response time and precision."
"In this paper, we analyze the numerical stability of the popular Longley-Rice
Irregular Terrain Model (ITM). This model is widely used to plan wireless
networks and in simulation-validated research and hence its stability is of
fundamental importance to the correctness of a large amount of work. We take a
systematic approach by first porting the reference ITM implementation to a
multiprecision framework and then generating loss predictions along many random
paths using real terrain data. We find that the ITM is not unstable for common
numerical precisions and practical prediction scenarios."
"In this paper, we present a detailed framework consisting of modeling of
routing overhead generated by three widely used proactive routing protocols;
Destination-Sequenced Distance Vector (DSDV), Fish-eye State Routing (FSR) and
Optimized Link State Routing (OLSR). The questions like, how these protocols
differ from each other on the basis of implementing different routing
strategies, how neighbor estimation errors affect broadcast of route requests,
how reduction of broadcast overhead achieves bandwidth, how to cope with the
problem of mobility and density, etc, are attempted to respond. In all of the
above mentioned situations, routing overhead and delay generated by the chosen
protocols can exactly be calculated from our modeled equations. Finally, we
analyze the performance of selected routing protocols using our proposed
framework in NS-2 by considering different performance parameters; Route
REQuest (RREQ) packet generation, End-to-End Delay (E2ED) and Normalized
Routing Load (NRL) with respect to varying rates of mobility and density of
nodes in the underlying wireless network."
"This paper deals with HOW to analyze the requirements for setting up the WAN
Optimizer. The criteria's that needs to be taken into account, the steps
involved in the analysis of WAN optimization requirement. These entire analyses
will give a complete framework for setting up a WAN optimizer within an
organization and the organization will have a clear record on the analysis made
before setting up this WAN Optimizer."
"To cope with the increasing demand of wireless communication services
multi-carrier systems are being used. Radio resources are very limited and
efficient usages of these resources are inevitable to get optimum performance
of the system. Paging channel is a low-bandwidth channel and one of the most
important channels on which system performance depends significantly. Therefore
it is vulnerable to even moderate overloads. In this paper, an efficient paging
algorithm, Concurrent Search, is proposed for efficient use of paging channel
in Multi- carrier CDMA system instead of existing sequential searching
algorithm. It is shown by the simulation that the paging performance in
proposed algorithm is far better than the existing system."
"Sensor network has been recognized as the most significant technology for
next century. Despites of its potential application, wireless sensor network
encounters resource restriction such as low power, reduced bandwidth and
specially limited power sources. This work proposes an efficient technique for
the conservation of energy in a wireless sensor network (WSN) by forming an
effective cluster of the network nodes distributed over a wide range of
geographical area. The clustering scheme is developed around a specified class
of cellular automata (CA) referred to as the modified cyclic cellular automata
(mCCA). It sets a number of nodes in stand-by mode at an instance of time
without compromising the area of network coverage and thereby conserves the
battery power. The proposed scheme also determines an effective cluster size
where the inter-cluster and intra-cluster communication cost is minimum. The
simulation results establish that the cyclic cellular automata based clustering
for energy conservation in sensor networks (CCABC) is more reliable than the
existing schemes where clustering and CA based energy saving technique is used."
"The throughput benefits of random linear network codes have been studied
extensively for wirelined and wireless erasure networks. It is often assumed
that all nodes within a network perform coding operations. In
energy-constrained systems, however, coding subgraphs should be chosen to
control the number of coding nodes while maintaining throughput. In this paper,
we explore the strategic use of network coding in the wireless packet erasure
relay channel according to both throughput and energy metrics. In the relay
channel, a single source communicates to a single sink through the aid of a
half-duplex relay. The fluid flow model is used to describe the case where both
the source and the relay are coding, and Markov chain models are proposed to
describe packet evolution if only the source or only the relay is coding. In
addition to transmission energy, we take into account coding and reception
energies. We show that coding at the relay alone while operating in a rateless
fashion is neither throughput nor energy efficient. Given a set of system
parameters, our analysis determines the optimal amount of time the relay should
participate in the transmission, and where coding should be performed."
"There have been a bulk of analytic results about the performance of cellular
networks where base stations are regularly located on a hexagonal or square
lattice. This regular model cannot reflect the reality, and tends to
overestimate the network performance. Moreover, tractable analysis can be
performed only for a fixed location user (e.g., cell center or edge user). In
this paper, we use the stochastic geometry approach, where base stations can be
modeled as a homogeneous Poisson point process. We also consider the user
density, and derive the user outage probability that an arbitrary user is under
outage owing to low signal-to-interference-plus-noise ratio or high congestion
by multiple users. Using the result, we calculate the density of success
transmissions in the downlink cellular network. An interesting observation is
that the success transmission density increases with the base station density,
but the increasing rate diminishes. This means that the number of base stations
installed should be more than $n$-times to increase the network capacity by a
factor of $n$. Our results will provide a framework for performance analysis of
the wireless infrastructure with a high density of access points, which will
significantly reduce the burden of network-level simulations."
"Rapid progress made in the field of sensor technology, wireless
communication, and computer networks in recent past, led to the development of
wireless Ad-hoc sensor networks, consisting of small, low-cost sensors, which
can monitor wide and remote areas with precision and liveliness unseen to the
date without the intervention of a human operator. This work comes up with a
stochastic model for periodic sensor-deployment (in face of their limited
amount of battery-life) to maintain a minimal node-connectivity in wireless
sensor networks. The node deployment cannot be modeled by using results from
conventional continuous birth-death process, since new nodes are added to the
network in bursts, i.e. the birth process is not continuous in practical
situations. We analyze the periodic node deployment process using discrete
birth-continuous death process and obtain two important statistical measures of
the existing number of nodes in the network, namely the mean and variance. We
show that the above mentioned sequences of mean and variances always converge
to finite steady state values, thus ensuring the stability of the system. We
also develop a cost function for the process of periodic deployment of sensor
nodes and minimize it to find the optimal time ({\tau}) and optimum number of
re-deployment (q) for maintaining minimum connectivity in the network."
"Recent increase in energy prices has led researchers to find better ways for
capacity provisioning in data centers to reduce energy wastage due to the
variation in workload. This paper explores the opportunity for cost saving
utilizing the flexibility from the Service Level Agreements (SLAs) and proposes
a novel approach for capacity provisioning under bounded latency requirements
of the workload. We investigate how many servers to be kept active and how much
workload to be delayed for energy saving while meeting every deadline. We
present an offline LP formulation for capacity provisioning by dynamic deferral
and give two online algorithms to determine the capacity of the data center and
the assignment of workload to servers dynamically. We prove the feasibility of
the online algorithms and show that their worst case performance are bounded by
a constant factor with respect to the offline formulation. We validate our
algorithms on a MapReduce workload by provisioning capacity on a Hadoop cluster
and show that the algorithms actually perform much better in practice compared
to the naive `follow the workload' provisioning, resulting in 20-40%
cost-savings."
"Adhoc networks are characterized by connectivity through a collection of
wireless nodes and fast changing network topology. Wireless nodes are free to
move independent of each other which makes routing much difficult. This calls
for the need of an efficient dynamic routing protocol. Mesh-based multicast
routing technique establishes communications between mobile nodes of wireless
adhoc networks in a faster and efficient way. In this article the performance
of prominent on-demand routing protocols for mobile adhoc networks such as
ODMRP (On Demand Multicast Routing Protocol), AODV (Adhoc on Demand Distance
Vector) and FSR (Fisheye State Routing protocol) was studied. The parameters
viz., average throughput, packet delivery ration and end-to-end delay were
evaluated. From the simulation results and analysis, a suitable routing
protocol can be chosen for a specified network. The results show that the ODMRP
protocol performance is remarkably superior as compared with AODV and FSR
routing protocols. Keywords: MANET, Multicast Routing, ODMRP, AODV, FSR."
"Mobile Agents (MAs) represent a distributed computing technology that
promises to address the scalability problems of centralized network management.
A critical issue that will affect the wider adoption of MA paradigm in
management applications is the development of MA Platforms (MAPs) expressly
oriented to distributed management. However, most of available platforms impose
considerable burden on network and system resources and also lack of essential
functionality. In this paper, we discuss the design considerations and
implementation details of a complete MAP research prototype that sufficiently
addresses all the aforementioned issues. Our MAP has been implemented in Java
and tailored for network and systems management applications."
"In this paper, we show how to exploit real-time communication applications to
determine the IP address of a targeted user. We focus our study on Skype,
although other real-time communication applications may have similar privacy
issues. We first design a scheme that calls an identified targeted user
inconspicuously to find his IP address, which can be done even if he is behind
a NAT. By calling the user periodically, we can then observe the mobility of
the user. We show how to scale the scheme to observe the mobility patterns of
tens of thousands of users. We also consider the linkability threat, in which
the identified user is linked to his Internet usage. We illustrate this threat
by combining Skype and BitTorrent to show that it is possible to determine the
file-sharing usage of identified users. We devise a scheme based on the
identification field of the IP datagrams to verify with high accuracy whether
the identified user is participating in specific torrents. We conclude that any
Internet user can leverage Skype, and potentially other real-time communication
systems, to observe the mobility and file-sharing usage of tens of millions of
identified users."
"Mobile ad hoc networks (MANETs) are self-configuring wireless networks that
lack permanent infrastructure and are formed among mobile nodes on demand.
Rapid node mobility results in dramatic channel variation, or fading, that
degrades MANET performance. Employing channel state information (CSI) at the
transmitter can improve the throughput of routing and medium access control
(MAC) protocols for mobile ad hoc networks. Several routing algorithms in the
literature explicitly incorporate the fading signal strength into the routing
metric, thus selecting the routes with strong channel conditions. While these
studies show that adaptation to the time-variant channel gain is beneficial in
MANETs, they do not address the effect of the outdated fading CSI at the
transmitter. For realistic mobile node speeds, the channel gain is rapidly
varying, and becomes quickly outdated due the feedback delay. We analyze the
link throughput of joint rate adaptation and adaptive relay selection in the
presence of imperfect CSI. Moreover, for an 802.11 network that employs
geographic opportunistic routing with adaptive rate and relay selection, we
propose a novel method to reduce the effect of the feedback delay at the MAC
layer in the presence of Rayleigh fading. This method exploits channel
reciprocity and fading prediction and does not require significant modification
to the existing 802.11 frame structure. Extensive network simulations
demonstrate that the proposed approach significantly improves the throughput,
delay, and packet delivery ratio for high mobile velocities relative to
previously proposed approaches that employ outdated CSI at the transmitter."
"Seamless continuity is the main goal and challenge in fourth generation
Wireless networks (FGWNs), to achieve seamless connectivity ""HANDOVER""
technique is used,Handover mechanism are mainly used when a mobile terminal(MT)
is in overlapping area for service continuity. In Heterogeneous wireless
networks main challenge is continual connection among the different networks
like WiFi, WiMax, WLAN, WPAN etc. In this paper, Vertical handover decision
schemes are compared, Simple Additive Weighting method (SAW) and Weighted
product model (WPM) are used to choose the best network from the available
Visitor networks(VTs) for the continuous connection by the mobile terminal. In
our work we mainly concentrated to the handover decision phase and to reduce
the processing delay in the period of handover. In this paper both SAW and WPM
methods are compared with the Qos parameters of the mobile terminal (MT) to
connect with the best network. Keywords: Handover, Vertical handover decision
schemes, Simple additive weighting, Weight product method."
"Designing energy-efficient all-to-all multicasting protocols is of of great
importance for multi-hop wireless networks such as wireless sensor networks and
wireless ad hoc networks. In an all-to-all multicast session, there exists a
set of wireless destination nodes, and each destination node needs to send some
data packets to all other destination nodes. We consider the problem of
building a shared multicast tree spanning the destination nodes such that the
total energy consumption of realizing an all-to-all multicast session using the
shared multicast tree is minimized. Since building such a multicast tree has
been proved to be NP-complete, we provide both centralized and distributed
approximation algorithms with provable approximation ratios for it. When the
transmission power of each wireless node is fixed, our centralized and
distributed algorithms have the approximation ratios of $4ln(\Delta+1)+7$ and
13, respectively, where $\Delta$ is the maximum node degree in the network.
When the transmission power of each wireless node is adjustable, both of our
centralized and distributed algorithms have the constant approximation ratio of
145."
"We study how long range directional beams can be used for self-organization
of a wireless network to exhibit small world properties. Using simulation
results for randomized beamforming as a guideline, we identify crucial design
issues for algorithm design. Subsequently, we propose an algorithm for
deterministic creation of small worlds. We define a new centrality measure that
estimates the structural importance of nodes based on traffic flow in the
network, which is used to identify the optimum nodes for beamforming. This
results in significant reduction in path length while maintaining connectivity."
"We investigate the problem of distributed sensors' failure detection in
networks with a small number of defective sensors, whose measurements differ
significantly from neighboring sensor measurements. Defective sensors are
represented by non-zero values in binary sparse signals. We build on the sparse
nature of the binary sensor failure signals and propose a new distributed
detection algorithm based on Group Testing (GT). The distributed GT algorithm
estimates the set of defective sensors from a small number of linearly
independent binary messages exchanged by the sensors. The distributed GT
algorithm uses a low complexity distance decoder that is robust to noisy
messages. We first consider networks with only one defective sensor and
determine the minimal number of linearly independent messages needed for
detection of the defective sensor with high probability. We then extend our
study to the detection of multiple defective sensors by modifying appropriately
the message exchange protocol and the decoding procedure. We show through
experimentation that, for small and medium sized networks, the number of
messages required for successful detection is actually smaller than the minimal
number computed in the analysis. Simulations demonstrate that the proposed
method outperforms methods based on random walk measurements collection in
terms of detection performance and convergence rate. Finally, the proposed
method is resilient to network dynamics due to the effective gossip-based
message dissemination protocol."
"In an autonomous wireless sensor network, self-organization of the nodes is
essential to achieve network wide characteristics. We believe that connectivity
in wireless autonomous networks can be increased and overall average path
length can be reduced by using beamforming and bio-inspired algorithms. Recent
works on the use of beamforming in wireless networks mostly assume the
knowledge of the network in aggregation to either heterogeneous or hybrid
deployment. We propose that without the global knowledge or the introduction of
any special feature, the average path length can be reduced with the help of
inspirations from the nature and simple interactions between neighboring nodes.
Our algorithm also reduces the number of disconnected components within the
network. Our results show that reduction in the average path length and the
number of disconnected components can be achieved using very simple local rules
and without the full network knowledge."
"One of the most important problems in wireless sensor network is to develop a
routing protocol that has energy efficiency. Since the power of the sensor
Nodes are limited, conserving energy and network life is a critical issue in
wireless sensor network. Clustering is one of the known methods widely used to
face these challenges. In this paper, a cluster based communication protocol
with considering the low energy consumption in wireless sensor networks, is
introduced which balances the energy load among sensor nodes. The nodes close
to each other have more overlap; they sense the same data from environment and
cause a waste of energy by generating repetitive data. In this paper, a cluster
based routing protocol is introduced, in the proposed protocol, in each round a
certain number of nodes are specified; the nodes which have at least one
neighboring node at a distance less than the threshold. Then, among them the
nodes with less energy and greater overlap with their neighbors have been
chosen to go to sleep mode, Also, the energy imbalance among sensor nodes is
reduced by integrating the distance of the nodes from the base station into
clustering policies."
"Cognitive Radio (CR) aims to increase the spectrum utilization by allowing
secondary users (SU) to access unused licensed spectrum bands. To maximize the
throughput given limited sensing capability, SUs need to strike a balance
between sensing the channels that are not heavily used by primary users (PU)
and avoiding collisions with other SUs. To randomize sensing decisions without
resorting to multiuser sensing policies, it is proposed to exploit the
spatially-variant fading channel conditions on different links by adapting the
reward to the channel state information (CSI). Moreover, the proposed
channel-adaptive policy favors links with high achievable transmission rate and
thus further improves the network throughput."
"Given a network infrastructure (e.g., data-center or on-chip-network) and a
distribution on the source-destination requests, the expected path (route)
length is an important measure for the performance, efficiency and power
consumption of the network. In this work we initiate a study on self-adjusting
networks: networks that use local-distributed mechanisms to adjust the position
of the nodes (e.g., virtual machines) in the network to best fit the route
requests distribution. Finding the optimal placement of nodes is defined as the
minimum expected path length (MEPL) problem. This is a generalization of the
minimum linear arrangement (MLA) problem where the network infrastructure is a
line and the computation is done centrally. In contrast to previous work, we
study the distributed version and give efficient and simple approximation
algorithms for interesting and practically relevant special cases of the
problem. In particular, we consider grid networks in which the distribution of
requests is a symmetric product distribution. In this setting, we show that a
simple greedy policy of position switching between neighboring nodes to locally
minimize an objective function, achieves good approximation ratios. We are able
to prove this result using the useful notions of expected rank of the
distribution and the expected distance to the center of the graph."
"the storage infrastructure is the foundation on which information relies and
therefore must support a company's business objectives and business model. In
this environment, simply deploying more and faster storage devices is not
enough; a new kind of infrastructure is needed, one that provides more enhanced
network availability, data accessibility, and system manageability than is
provided by today's infrastructure. The SAN meets this challenge. The SAN
liberates the storage device, so it is not on a particular server bus, and
attaches it directly to the network. In other words, storage is externalized
and functionally distributed across the organization. The SAN also enables the
centralizing of storage devices and the clustering of servers, which makes for
easier and less expensive administration. So the idea is to create an
intelligent SAN infrastructure that stretches to meet increased demands, allows
highly available and heterogeneous access to expanding information."
"We propose an all-optical networking solution for a wide area network (WAN)
based on the notion of multipoint-to-multipoint lightpaths that, for short, we
call ""multipaths"". A multipath concentrates the traffic of a group of source
nodes on a wavelength channel using an adapted MAC protocol and multicasts this
traffic to a group of destination nodes that extract their own data from the
confluent stream. The proposed network can be built using existing components
and appears less complex and more efficient in terms of energy consumption than
alternatives like OPS and OBS. The paper presents the multipath architecture
and compares its energy consumption to that of a classical router-based ISP
network. A flow-aware dynamic bandwidth allocation algorithm is proposed and
shown to have excellent performance in terms of throughput and delay."
"IP Multicast is one of the most absolute method for large bandwidth Internet
applications such as video conference, IPTV, E-Learning and Telemedicine etc.,
But due to security and management reason IP Multicast is not enabled in
Internet backbone routers. To achieve these challenges, lot of Application
Layer Multicast (ALM) has been proposed. All the existing protocols such as
NICE, ZIGZAG and OMNI are trying to reduce average delay by forming a Multicast
tree. But still that problem has not been addressed fully. We are proposing a
new protocol called NetRawALM, which will address the average delay,
Reliability between nodes, Scalability of conference, Heterogeneity and
resilient data distribution for real time multimedia applications by
constructing the Network based Resource aware Multicast tree algorithm. This is
very dynamic and decentralised. The proposed architecture is a LAN aware; it is
used to reduce Internet Traffic."
"In this paper, we propose QoS aware MAC protocol for Wire- less Sensor
Networks and its cross layer extension to network layer for providing QoS in
delay sensitive WSN scenarios. In WSNs, there can be two types of traffic one
is event driven traffic which requires immedi- ate attention and another is
periodic reporting. Event driven traffic is classified as Class I(delay
sensitive) traffic and periodic reporting is clas- sified as Class II(Best
Effort) Traffic. MAC layer adaptation can take place in terms of (i) Dynamic
contention window adjustment per class, (ii) Reducing the delay suffered by
difference in Sleep schedules(DSS) of communicating nodes by dynamically
adjusting Duty Cycle based on Utilization and DSS delay of class I traffic,
(iii) Different DIFS (DCF Inter Frame Spacing) per class, (iv) Adjusting all
the three schemes pro- posed above simultaneously. Cross layer extension is
also proposed, in which MAC layer uses network layer's next hop information for
better adaptation of duty cycle based on DSS delay. Routing protocols can uti-
lize MAC layer parameter DSS delay to select the routes which offer least DSS
delay latency, there by minimizing the overall end-to-end delay."
"Graph coloring is used in wireless networks to optimize network resources:
bandwidth and energy. Nodes access the medium according to their color. It is
the responsibility of the coloring algorithm to ensure that interfering nodes
do not have the same color. In this research report, we focus on wireless
sensor networks with grid topologies. How does a coloring algorithm take
advantage of the regularity of grid topology to provide an optimal periodic
coloring, that is a coloring with the minimum number of colors? We propose the
Vector-Based Coloring Method, denoted VCM, a new method that is able to provide
an optimal periodic coloring for any radio transmission range and for any h-hop
coloring, h>=1. This method consists in determining at which grid nodes a color
can be reproduced without creating interferences between these nodes while
minimizing the number of colors used. We compare the number of colors provided
by VCM with the number of colors obtained by a distributed coloring algorithm
with line and column priority assignments. We also provide bounds on the number
of colors of optimal general colorings of the infinite grid, and show that
periodic colorings (and thus VCM) are asymptotically optimal. Finally, we
discuss the applicability of this method to a real wireless network."
"This paper proposes and evaluates a new position-based Parallel Routing
Protocol (PRP) for simultaneously routing multiple data packets over disjoint
paths in a mobile ad-hoc network (MANET) for higher reliability and reduced
communication delays. PRP views the geographical region where the MANET is
located as a virtual 2-dimensional grid of cells. Cell-disjoint (parallel)
paths between grid cells are constructed and used for building pre-computed
routing tables. A single gateway node in each grid cell handles routing through
that grid cell reducing routing overheads. Each node maintains updated
information about its own location in the virtual grid using GPS. Nodes also
keep track of the location of other nodes using a new proposed cell-based
broadcasting algorithm. Nodes exchange energy level information with neighbors
allowing energy-aware selection of the gateway nodes. Performance evaluation
results have been derived showing the attractiveness of the proposed parallel
routing protocol from different respects including low communication delays,
high packet delivery ratios, high routing path stability, and low routing
overheads."
"Human motion in the vicinity of a wireless link causes variations in the link
received signal strength (RSS). Device-free localization (DFL) systems, such as
variance-based radio tomographic imaging (VRTI), use these RSS variations in a
static wireless network to detect, locate and track people in the area of the
network, even through walls. However, intrinsic motion, such as branches moving
in the wind and rotating or vibrating machinery, also causes RSS variations
which degrade the performance of a DFL system. In this paper, we propose and
evaluate two estimators to reduce the impact of the variations caused by
intrinsic motion. One estimator uses subspace decomposition, and the other
estimator uses a least squares formulation. Experimental results show that both
estimators reduce localization root mean squared error by about 40% compared to
VRTI. In addition, the Kalman filter tracking results from both estimators have
97% of errors less than 1.3 m, more than 60% improvement compared to tracking
results from VRTI."
"The main cause of wasted energy consumption in wireless sensor networks is
packet collision. The packet scheduling algorithm is therefore introduced to
solve this problem. Some packet scheduling algorithms can also influence and
delay the data transmitting in the real-time wireless sensor networks. This
paper presents the packet scheduling algorithm (PSA) in order to reduce the
packet congestion in MAC layer leading to reduce the overall of packet
collision in the system The PSA is compared with the simple CSMA/CA and other
approaches using network topology benchmarks in mathematical method. The
performances of our PSA are better than the standard (CSMA/CA). The PSA
produces better throughput than other algorithms. On other hand, the average
delay of PSA is higher than previous works. However, the PSA utilizes the
channel better than all algorithms."
"Industry experience indicates that the ability to incrementally expand data
centers is essential. However, existing high-bandwidth network designs have
rigid structure that interferes with incremental expansion. We present
Jellyfish, a high-capacity network interconnect, which, by adopting a random
graph topology, yields itself naturally to incremental expansion. Somewhat
surprisingly, Jellyfish is more cost-efficient than a fat-tree: A Jellyfish
interconnect built using the same equipment as a fat-tree, supports as many as
25% more servers at full capacity at the scale of a few thousand nodes, and
this advantage improves with scale. Jellyfish also allows great flexibility in
building networks with different degrees of oversubscription. However,
Jellyfish's unstructured design brings new challenges in routing, physical
layout, and wiring. We describe and evaluate approaches that resolve these
challenges effectively, indicating that Jellyfish could be deployed in today's
data centers."
"In today's world Wireless Ad-hoc sensor network, consists of small sensor
nodes having limited resources, has a great potential to solve problems in
various domain including disaster management. In this paper ""QCS-protocol"" is
modified which was introduced in our previous paper [1] and named as ""Modified
QCS-protocol"". This is the backbone of our Intelligent Energy Efficient Ad-hoc
Sensor Network. Two other protocols ""Irregular Information Transfer"" & ""Final
Broadcast-Petrol Flow"" protocol are also modified to enhance performance of the
new version of QCS protocol to run the system properly and to make the network
more energy efficient and perfect. The challenges in WASN are- limited node
power, Ad-hoc organization of network and reliability. Most of the existing
approaches addressed the problems separately, but not in a totality. This paper
shows how the network can have unlimited life and all time readiness with
overall stability to send information to the base station with minimum power
dissipation with the help of multimode ""same type"" sensor nodes and type
categorization of generated information. Moreover an effort is made to give
some light to the implementation issues and analyzed overall performance of the
network by MATLAB simulation."
"The Wireless Sensors Network (WSN) is an emergent technology resulting from
progress of various fields. Many applications of networks WSN are born. One of
the applications which have an operational effectiveness relates to the field
of health and allows a medical remote support. Miniature wireless sensors,
strategically placed on the human body, create a Wireless Body Sensor Network
(WBSN) which allows supervising various essential biological signals (rate of
heartbeat, pressure, etc). The sensitivity of medical information requires
mechanisms of safety. This performance constitutes a challenge for WBSN because
of their limitation in resources energy and data-processing. In this paper we
propose a new approach to symmetric cryptographic key establishment, based on
biometrics physiology. This approach takes into account WBSN constraints and
its topology."
"This paper is concerned with the issue of side payments between content
providers (CPs) and Internet service (access bandwidth) providers (ISPs) in an
Internet that is potentially not neutral. We herein generalize past results
modeling the ISP and CP interaction as a noncooperative game in two directions.
We consider different demand response models (price sensitivities) for
different provider types in order to explore when side payments are profitable
to the ISP. Also, we consider convex (non-linear) demand response to model
demand triggered by traffic which is sensitive to access bandwidth congestion,
particularly delay-sensitive interactive real-time applications. Finally, we
consider a model with two competing ""eyeball"" ISPs with transit pricing of net
traffic at their peering point to study the effects of caching remote content."
"Statistical network calculus is the probabilistic extension of network
calculus, which uses a simple envelope approach to describe arrival traffic and
service available for the arrival traffic in a node. One of the key features of
network calculus is the possibility to describe the service available in a
network using a network service envelope constructed from the service envelopes
of the individual nodes constituting the network. It have been shown that the
end-to-end worst case performance measures computed using the network service
envelope is bounded by $ {\cal O} (H) $, where $H$ is the number of nodes
traversed by a flow. There have been many attempts to achieve a similar linear
scaling for end-to-end probabilistic performance measures but with limited
success. In this paper, we present a simple general proof of computing
end-to-end probabilistic performance measures using network calculus that grow
linearly in the number of nodes ($H$)."
"The signal to noise ratio (SNR) is one of the important measures for reducing
the noise.A technique that uses a linear prediction error filter (LPEF) and an
adaptive digital filter (ADF) to achieve noise reduction in a speech and image
degraded by additive background noise is proposed. Since a speech signal can be
represented as the stationary signal over a short interval of time, most of
speech signal can be predicted by the LPEF. This estimation is performed by the
ADF which is used as system identification. Noise reduction is achieved by
subtracting the reconstructed noise from the speech degraded by additive
background noise. Most of the MR image accelerating methods suffers from
degradation of acquired images, which is often correlated with the degree of
acceleration. However, Wideband MRI is a novel technique that transcends such
flaws.In this paper we proposed LPEF and ADF for reducing the noise in speech
and also we demonstrate that Wideband MRI is capable of obtaining images with
identical quality as conventional MR images in terms of SNR in wireless LAN."
"Noise is the major problem while working with wireless LAN. In this paper we
analyze the noise by using active receiving antenna and also propose the
detection mechanism based on RF energy duration. The standard back off
mechanism of 802.11 wireless LAN (WLAN) increases the contention window when a
transmission failure occurs in order to alleviate contentions in a WLAN. In
addition, many proposed schemes for 802.11 WLAN behave adaptively to
transmission failures. Transmission failures in WLANs occur mostly by two
causes: collision and channel noise. However, in 802.11 WLAN, a station cannot
know the cause of a transmission failure, thus the adaptive schemes assume the
ideal situation in which all transmission failures occur by only one of two
causes. For this reason, they may behave erroneously in a real world where
transmission failures occur by both causes. In this paper, we propose a novel
scheme to detect collision, which utilizes transmission time information and RF
energy duration on the channel. By detecting collisions, a station can
differentiate the causes of transmission failures and the adaptive schemes can
operate correctly by using the detection information."
"Standard congestion control cannot detect link failure losses which occur due
to mobility and power scarcity in multi-hop Ad-Hoc network (MANET). Moreover,
successive executions of Back-off algorithm deficiently grow Retransmission
Timeout (RTO) exponentially for new route. The importance of detecting and
responding link failure losses is to prevent sender from remaining idle
unnecessarily and manage number of packet retransmission overhead. In contrast
to Cross-layer approaches which require feedback information from lower layers,
this paper operates purely in Transport layer. This paper explores an
end-to-end threshold-based algorithm which enhances congestion control to
address link failure loss in MANET. It consists of two phases. First,
threshold-based loss classification algorithm distinguishes losses due to link
failure by estimating queue usage based on Relative One-way Trip Time (ROTT).
Second phase adjusts RTO for new route by comparing capabilities of new route
to the broken route using available information in Transport layer such as ROTT
and number of hops."
"It has been a challenging issue to provide digital quality multimedia data
stream to the remote user through the distributed system. The main aspects to
design the real distributed system, which reduce the cost of the network by
means of reduce packet loss and enhanced over all system performance. Since the
number of user increased rapidly in the network it posed heavy load to the
video servers. The requested clients, servers are all distributed in nature and
the data stream delivered to the user without error. In this work I have
presented the performance of the video on demand server by efficient traffic
control at real time with respect to incoming multirate traffic pattern . In
this work, I present how the overall system performance gradually decreases
when the client population sized in the clusters increase. This work indicated
the load balancing required for the on demand video distributed system to
provide efficient cost effective service to the local or remote clients."
"Security in any of the networks became an important issue in this paper we
have implemented a security mechanism on Medium Access Control layer by Assured
Neighbor based Security Protocol to provide authentication and confidentiality
of packets along with High speed transmission for Ad hoc networks. Here we have
divided the protocol into two different parts. The first part deals with
Routing layer information; in this part we have tried to implement a possible
strategy for detecting and isolating the malicious nodes. A trust counter for
each node is determined which can be actively increased and decreased depending
upon the trust value for the purpose of forwarding the packets from source node
to destination node with the help of intermediate nodes. A threshold level is
also predetermined to detect the malicious nodes. If the value of the node in
trust counter is less than the threshold value then the node is denoted
'malicious'. The second part of our protocol deals with the security in the
link layer. For this security reason we have used CTR (Counter) approach for
authentication and encryption. We have simulated all our strategies and schemes
in NS-2, the result of which gives a conclusion that our proposed protocol i.e.
Assured Neighbor based Security Protocol can perform high packet delivery
against various intruders and also packet delivery ratio against mobility with
low delays and low overheads."
"Context-aware applications have been gaining huge interest in the last few
years. With cell phones becoming ubiquitous computing devices, cell phone
localization has become an important research problem. In this paper, we
present CellSense, a prob- abilistic RSSI-based fingerprinting location
determi- nation system for GSM phones. We discuss the chal- lenges of
implementing a probabilistic fingerprinting localization technique in GSM
networks and present the details of the CellSense systemand how it addresses
these challenges. We then extend the proposed system using a hybrid technique
that combines probabilistic and deterministic estimation to achieve both high
ac- curacy and low computational overhead.Moreover, the accuracy of the hybrid
technique is robust to changes in its parameter values. To evaluate our
proposed system, we implemented CellSense on Android-based phones. Results from
two different testbeds, represent- ing urban and rural environments, for three
differ- ent cellular providers show that CellSense provides at least 108.57%
enhancement in accuracy in rural areas and at least 89.03% in urban areas
compared to the current state of the art RSSI-based GSM localization systems.
In additional, the proposed hybrid technique provides more than 6 times and 5.4
times reduction in computational requirements compared to the state of the art
RSSI-based GSM localization systems for the rural and urban testbeds
respectively.We also evaluate the effect of changing the different system
parameters on the accuracy-complexity tradeoff and how the cell towers density
and fingerprint density affect the system performance."
"A NoC is composed by IP cores (Intellectual Propriety) and switches connected
among themselves by communication channels. End-to-End Delay (EED)
communication is accomplished by the exchange of data among IP cores. Often,
the structure of particular messages is not adequate for the communication
purposes. This leads to the concept of packet switching. In the context of
NoCs, packets are composed by header, payload, and trailer. Packets are divided
into small pieces called Flits. It appears of importance, to meet the required
performance in NoC hardware resources. It should be specified in an earlier
step of the system design. The main attention should be given to the choice of
some network parameters such as the physical buffer size in the node. The EED
and packet loss are some of the critical QoS metrics. Some real-time and
multimedia applications bound up these parameters and require specific hardware
resources and particular management approaches in the NoC switch. A traffic
contract (SLA, Service Level Agreement) specifies the ability of a network or
protocol to give guaranteed performance, throughput or latency bounds based on
mutually agreed measures, usually by prioritizing traffic. A defined Quality of
Service (QoS) may be required for some types of network real time traffic or
multimedia applications. The main goal of this paper is, using the Network on
Chip modeling architecture, to define a QoS metric. We focus on the network
delay bound and packet losses. This approach is based on the Network Calculus
theory, a mathematical model to represent the data flows behavior between IPs
interconnected over NoC. We propose an approach of QoS-metric based on
QoS-parameter prioritization factors for multi applications-service using
calculus model."
"Till today we dreamt of imperceptible delay in a network. The computer
science research grows today faster than ever offering more and more services
(computational representational, graphical, intelligent implication etc) to its
user. But the problem lies in ""greater the volume of services greater the
problem of delay"". So tracing delay, or performance analysis focusing on time
required for computation, in a existing or newly configured network is
necessary to conclude the improvement. In this paper, we have done the job of
delay analysis in a multi-server system,. For this proposed work we have used
continuous -parameter Markov chains (Non -Birth -Death Process),for developing
the required models, and for developing the simulator we have used queuing
networking, different scheduling algorithms at the servers queue and process
scheduling . The work can be further extended to test the performance of
wireless domain."
"We develop a generalized optimization framework for graph-based
semi-supervised learning. The framework gives as particular cases the Standard
Laplacian, Normalized Laplacian and PageRank based methods. We have also
provided new probabilistic interpretation based on random walks and
characterized the limiting behaviour of the methods. The random walk based
interpretation allows us to explain di erences between the performances of
methods with di erent smoothing kernels. It appears that the PageRank based
method is robust with respect to the choice of the regularization parameter and
the labelled data. We illustrate our theoretical results with two realistic
datasets, characterizing di erent challenges: Les Miserables characters social
network and Wikipedia hyper-link graph. The graph-based semi-supervised
learning classi- es the Wikipedia articles with very good precision and perfect
recall employing only the information about the hyper-text links."
"We study the problem of wireless sensor network design by deploying a minimum
number of additional relay nodes (to minimize network cost) at a subset of
given potential relay locations, in order to convey the data from already
existing sensor nodes (hereafter called source nodes) to a Base Station, while
meeting a certain specified hop count bound (the hop count bound is chosen to
ensure a pre-determined probability of the data being delivered to the BS
within a given maximum delay). We study two variations of the problem.
  First we sudy the problem of guaranteed QoS connected network design, where
the objective is to have at least one path from each source to the BS with the
specified hop count bound. We show that the problem is NP-Hard. For a problem
in which the number of existing sensor nodes and potential relay locations is
n, we propose an O(n) approximation algorithm of polynomial time complexity.
Results show that the algorithm performs efficiently (in over 90% of the tested
scenarios, it gave solutions that were either optimal or were worse than
optimal by just one relay) in various randomly generated network scenarios.
  Next, we study the problem of survivable network design with guaranteed QoS,
i.e, the requirement is to have at least k > 1 node disjoint hop constrained
paths from each source to the BS. We show that the problem is NP-Hard. We also
show that the problem of finding a feasible solution to this optimization
problem is NP-Complete. We propose two polynomial time heuristics for this
problem, and compare their performance on various randomly generated network
scenarios."
"BitTorrent, one of the most widespread used P2P application for file-sharing,
recently got rid of TCP by introducing an application-level congestion control
protocol named uTP. The aim of this new protocol is to efficiently use the
available link capacity, while minimizing its interference with the rest of
user traffic (e.g., Web, VoIP and gaming) sharing the same access bottleneck.
In this paper we perform an experimental study of the impact of uTP on the
torrent completion time, the metric that better captures the user experience.
We run BitTorrent applications in a flash crowd scenario over a dedicated
cluster platform, under both homogeneous and heterogeneous swarm population.
Experiments show that an all-uTP swarms have shorter torrent download time with
respect to all-TCP swarms. Interestingly, at the same time, we observe that
even shorter completion times can be achieved under careful mixtures of TCP and
uTP traffic."
"Network calculus is an elegant theory which uses envelopes to determine the
worst-case performance bounds in a network. Statistical network calculus is the
probabilistic version of network calculus, which strives to retain the
simplicity of envelope approach from network calculus and use the arguments of
statistical multiplexing to determine probabilistic performance bounds in a
network. The tightness of the determined probabilistic bounds depends on the
efficiency of modelling stochastic properties of the arrival traffic and the
service available to the traffic at a network node. The notion of effective
bandwidth from large deviations theory is a well known statistical descriptor
of arrival traffic. Similarly, the notion of effective capacity summarizes the
time varying resource availability to the arrival traffic at a network node.
The main contribution of this paper is to establish an end-to-end stochastic
network calculus with the notions of effective bandwidth and effective capacity
which provides efficient end-to-end delay and backlog bounds that grows
linearly in the number of nodes ($H$) traversed by the arrival traffic, under
the assumption of independence."
"More and more critical Wireless Sensor Networks (WSNs) applications are
emerging. Those applications need reliability and respect of time constraints.
The underlying mechanisms such as MAC and routing must handle such
requirements. Our approach to the time constraint problem is to bound the
hop-count between a node and the sink and the time it takes to do a hop so the
end-to-end delay can be bounded and the communications are thus real-time. For
reliability purpose we propose to select forwarder nodes depending on how they
are connected in the direction of the sink. In order to be able to do so we
need a coordinate (or a metric) that gives information on hop-count, that
allows to strongly differentiate nodes and gives information on the
connectivity of each node keeping in mind the intrinsic constraints of WSWs
such as energy consumption, autonomy, etc. Due to the efficiency and
scalability of greedy routing in WSNs and the financial cost of GPS chips,
Virtual Coordinate Systems (VCSs) for WSNs have been proposed. A category of
VCSs is based on the hop-count from the sink, this scheme leads to many nodes
having the same coordinate. The main advantage of this system is that the hops
number of a packet from a source to the sink is known. Nevertheless, it does
not allow to differentiate the nodes with the same hop-count. In this report we
propose a novel hop-count-based VCS which aims at classifying the nodes having
the same hop-count depending on their connectivity and at differentiating nodes
in a 2-hop neighborhood. Those properties make the coordinates, which also can
be viewed as a local identifier, a very powerful metric which can be used in
WSNs mechanisms."
"Now days, interests in the application of Wireless Body Area Network (WBAN)
have grown considerably. A number of tiny wireless sensors, strategically
placed on the human body, create a wireless body area network that can monitor
various vital signs, providing real-time feedback to the user and medical
personnel. This communication needs to be energy efficient and highly reliable
while keeping delays low. In this paper we present hardware and software
architecture for BAN and also we offer reliable communication and data
aggregation."
"XML and XML Schema are widely used in different domains for the definition of
standards that enhance the interoperability between parts exchanging
information through the Internet. The size and complexity of some standards,
and their associated schemas, have been growing with time as new use case
scenarios and data models are added to them. The common approach to deal with
the complexity of producing XML processing code based on these schemas is the
use of XML data binding generators. Unfortunately, these tools do not always
produce code that ?ts the limitations of resource-constrained devices, such as
mobile phones, in the presence of large schemas. In this paper we present
Instance-based XML data binding, an approach to produce compact
application-specific XML processing code for mobile devices. The approach
utilises information extracted from a set of XML documents about how the
application make use of the schemas."
"Formal analysis techniques are widely used today in order to verify and
analyze communication protocols. In this work, we launch a quantitative
verification analysis for the low- cost Radio Frequency Identification (RFID)
protocol proposed by Song and Mitchell. The analysis exploits a Discrete-Time
Markov Chain (DTMC) using the well-known PRISM model checker. We have managed
to represent up to 100 RFID tags communicating with a reader and quantify each
RFID session according to the protocol's computation and transmission cost
requirements. As a consequence, not only does the proposed analysis provide
quantitative verification results, but also it constitutes a methodology for
RFID designers who want to validate their products under specific cost
requirements."
"Smart grid, regarded as the next generation power grid, uses two-way flows of
electricity and information to create a widely distributed automated energy
delivery network. In this work we present our vision on smart grid from the
perspective of wireless communications and networking technologies. We present
wireless communication and networking paradigms for four typical scenarios in
the future smart grid and also point out the research challenges of the
wireless communication and networking technologies used in smart grid"
"Subscriber satisfaction and maximum radio resource utilization are the
pivotal criteria in communication system design. In multi-Carrier CDMA system,
different paging algorithms are used for locating user within the shortest
possible time and best possible utilization of radio resources. Different
paging algorithms underscored different techniques based on the different
purposes. However, low servicing time of sequential search and better
utilization of radio resources of concurrent search can be utilized
simultaneously by swapping of the algorithms. In this paper, intelligent
mechanism has been developed for dynamic algorithm assignment basing on
time-varying traffic demand, which is predicted by radial basis neural network;
and its performance has been analyzed are based on prediction efficiency of
different types of data. High prediction efficiency is observed with a good
correlation coefficient (0.99) and subsequently better performance is achieved
by dynamic paging algorithm assignment. This claim is substantiated by the
result of proposed intelligent paging strategy."
"Modern radio communication is faced with a problem about how to distribute
restricted frequency to users in a certain space. Since our task is to minimize
the number of repeaters, a natural idea is enlarging coverage area. However,
coverage has restrictions. First, service area has to be divided economically
as repeater's coverage is limited. In this paper, our fundamental method is to
adopt seamless cellular network division. Second, underlying physics content in
frequency distribution problem is interference between two close frequencies.
Consequently, we choose a proper frequency width of 0.1MHz and a relevantly
reliable setting to apply one frequency several times.
  We make a few general assumptions to simplify real situation. For instance,
immobile users yield to homogenous distribution; repeaters can receive and
transmit information in any given frequency in duplex operation; coverage is
mainly decided by antenna height.
  Two models are built up to solve 1000 users and 10000 users situations
respectively. In order to utilize restricted frequency and PL code, three
stratified terms - ""cell"", ""cluster"", ""group"" - are introduced to describe the
models in detail. Under our analysis, 91 repeaters for 1000 users and 469
repeaters for 10000 users are viable results.
  Next, to test stability and sensitivity of models, we give total
consideration to the variation of sum of users, antenna height, and frequency
width and service radius. Evaluation about models is offered qualitatively.
Finally, two practical cases are put forward to gain a partial knowledge of
mountainous area. The brief method in dealing with mountains is classified
discussion in two ideal conditions. It may provide some constructive
suggestions to avoid shortcomings or take proper measures in similar locations."
"We present an implementation of Multipath TCP (MPTCP) under the NS-3 open
source network simulator. MPTCP is a promising extension of TCP currently
considered by the recent eponymous IETF working group, with the objective of
improving the performance of TCP, especially its robustness to variable network
conditions. We describe this new protocol, its main functions and our
implementation in NS-3. Besides this implementation compliant to the current
versions of the IETF drafts, we have also added and compared various packet
reordering mechanisms. We indeed notice that such mechanisms highly improve the
performance of MPTCP. We believe that our implementation could be useful for
future works in MPTCP performance evaluation, especially to compare packet
reordering algorithms or coupling congestion control mechanisms between
subfows."
"The concepts of MIMO MC-CDMA are not new but the new technologies to improve
their functioning are an emerging area of research. In general, most mobile
communication systems transmit bits of information in the radio space to the
receiver. The radio channels in mobile radio systems are usually multipath
fading channels, which cause inter-symbol interference (ISI) in the received
signal. To remove ISI from the signal, there is a need of strong equalizer. In
this thesis we have focused on simulating the MIMO MC-CDMA systems in MATLAB
and designed the channel estimation for them."
"In this paper, the performance of high speed optical fiber based network is
analysed by using dispersion compensating module (DCM). The optimal operating
condition of the DCM is obtained by considering dispersion management
configurations for the symmetrical system i.e Pre-compensation &
Post-compensation. The dispersion compensating fiber (DCF) is tested for a
single span, single channel system operating at a speed of 10 Gb/s with a
transmitting wavelength of 1550 nm, over 120 km single mode fibre by using the
compensating fiber for 24 km,30km and 35Km. So far, most of the investigations
for single mode fiber (SMF) transmission at high amplifier spacings in the
order of 90 km to 120 km is focused on conventional Non Return to Zero(NRZ)
format. The simulation results are validated by analysing the Q-factor and Bit
error rate (BER) in the numerical simulator OptSim."
"High Peak to Average Power Ratio (PAPR) of the transmitted signal is a
serious problem in multicarrier modulation systems. In this paper a new
technique for reduction in PAPR of the Multicarrier Code Division Multiple
Access (MC CDMA) signals based on combining the Discrete Transform either
Discrete Cosine Transform (DCT) or multi-resolution Discrete Wavelet Transform
(DWT) with companding is proposed. It is analyzed and implemented using MATLAB.
Simulation results of reduction in PAPR and power Spectral Density (PSD) of the
MC CDMA with companding and without companding are compared with the MC CDMA
with DCT and companding, DWT and companding systems. The new technique proposed
is to make use of multi-resolution DWT in combination with companding in order
to achieve a very substantial reduction in PAPR of the MC CDMA signal"
"Cooperative diversity is a technique in which various radio terminals relay
signals for each other. Cooperative diversity results when cooperative
communications is used primarily to leverage the spatial diversity available
among distributed radios. In this paper different cooperative diversity schemes
and their applications in various wireless networks are discussed. In this
paper the impact of cooperative diversity on the energy consumption and
lifetime of sensor network and the impact of cooperation in cognitive radio are
discussed. Here, user scheduling and radio resource allocation techniques are
also discussed which are developed in order to efficiently integrate various
cooperative diversity schemes for the emerging IEEE 802.16j based systems."
"Segment retransmissions are an essential tool in assuring reliable end-to-end
communication in the Internet. Their crucial role in TCP design and operation
has been studied extensively, in particular with respect to identifying
non-conformant, buggy, or underperforming behaviour. However, TCP segment
retransmissions are often overlooked when examining and analyzing large traffic
traces. In fact, some have come to believe that retransmissions are a rare
oddity, characteristically associated with faulty network paths, which,
typically, tend to disappear as networking technology advances and link
capacities grow. We find that this may be far from the reality experienced by
TCP flows. We quantify aggregate TCP segment retransmission rates using
publicly available network traces from six passive monitoring points attached
to the egress gateways at large sites. In virtually half of the traces examined
we observed aggregate TCP retransmission rates exceeding 1%, and of these,
about half again had retransmission rates exceeding 2%. Even for sites with low
utilization and high capacity gateway links, retransmission rates of 1%, and
sometimes higher, were not uncommon. Our results complement, extend and bring
up to date partial and incomplete results in previous work, and show that TCP
retransmissions continue to constitute a non-negligible percentage of the
overall traffic, despite significant advances across the board in
telecommunications technologies and network protocols. The results presented
are pertinent to end-to-end protocol designers and evaluators as they provide a
range of ""realistic"" scenarios under which, and a ""marker"" against which,
simulation studies can be configured and calibrated, and future protocols
evaluated."
"In this paper, we investigate the performance of bidirectional relay
selection using amplify-and-forward protocol with imperfect channel state
information, i.e., delay effect and channel estimation error. The asymptotic
expression of end-to-end SER in high SNR regime is derived in a closed form,
which indicates that the delay effect causes the loss of both coding gain and
diversity order, while the channel estimation error merely affects the coding
gain. Finally, analytical results are verified by Monte-Carlo simulations."
"Typical applications of the mobile ad-hoc network, MANET, are in disaster
recovery operations which have to respect time constraint needs. Since MANET is
affected by limited resources such as power constraints, it is a challenge to
respect the deadline of a real-time data. This paper proposes the Energy and
Delay aware based on Dynamic Source Routing protocol, ED-DSR. ED-DSR
efficiently utilizes the network resources such as the intermediate mobile
nodes energy and load. It ensures both timeliness and energy efficiency by
avoiding low-power and overloaded intermediate mobile nodes. Through
simulations, we compare our proposed routing protocol with the basic routing
protocol Dynamic Source Routing, DSR. Weighting factors are introduced to
improve the route selection. Simulation results, using the NS-2 simulator, show
that the proposed protocol prolongs the network lifetime (up to 66%), increases
the volume of packets delivered while meeting the data flows real-time
constraints and shortens the endto- end delay."
"In this paper, we dynamically select the transmission rate and design
wireless network coding to improve the quality of services such as delay for
time critical applications. With low transmission rate, and hence longer
transmission range, more packets may be encoded together, which increases the
coding opportunity. However, low transmission rate may incur extra transmission
delay, which is intolerable for time critical applications. We design a novel
joint rate selection and wireless network coding (RSNC) scheme with delay
constraint, so as to minimize the total number of packets that miss their
deadlines at the destination nodes. We prove that the proposed problem is
NPhard, and propose a novel graph model and transmission metric which consider
both the heterogenous transmission rates and the packet deadline constraints
during the graph construction. Using the graph model, we mathematically
formulate the problem and design an efficient algorithm to determine the
transmission rate and coding strategy for each transmission. Finally,
simulation results demonstrate the superiority of the RSNC scheme."
"In this paper the variation principles from theoretical physics is considered
that would describe the process of routing in computer networks. The total
traffic which is currently served on all hops of the route has been chosen as
the quantity to minimize. Universal metric function has been found for dynamic
routing taking into account the packet loss effect. An attempt to derive the
metric of the most popular dynamic routing protocols such as RIP, OSPF, EIGRP
from universal metric was made."
"Mobile browser is known to be slow because of the bottleneck in resource
loading. Client-only solutions to improve resource loading are attractive
because they are immediately deployable, scalable, and secure. We present the
first publicly known treatment of client-only solutions to understand how much
they can improve mobile browser speed without infrastructure support.
Leveraging an unprecedented set of web usage data collected from 24 iPhone
users continuously over one year, we examine the three fundamental, orthogonal
approaches a client-only solution can take: caching, prefetching, and
speculative loading, which is first proposed and studied in this work.
Speculative loading predicts and speculatively loads the subresources needed to
open a web page once its URL is given. We show that while caching and
prefetching are highly limited for mobile browsing, speculative loading can be
significantly more effective. Empirically, we show that client-only solutions
can improve the browser speed by about 1.4 second on average for web sites
visited by the 24 iPhone users. We also report the design, realization, and
evaluation of speculative loading in a WebKit-based browser called Tempo. On
average, Tempo can reduce browser delay by 1 second (~20%)."
"ISPs are increasingly selling ""tiered"" contracts, which offer Internet
connectivity to wholesale customers in bundles, at rates based on the cost of
the links that the traffic in the bundle is traversing. Although providers have
already begun to implement and deploy tiered pricing contracts, little is known
about how such pricing affects ISPs and their customers. While contracts that
sell connectivity on finer granularities improve market efficiency, they are
also more costly for ISPs to implement and more difficult for customers to
understand. In this work we present two contributions: (1) we develop a novel
way of mapping traffic and topology data to a demand and cost model; and (2) we
fit this model on three large real-world networks: an European transit ISP, a
content distribution network, and an academic research network, and run
counterfactuals to evaluate the effects of different pricing strategies on both
the ISP profit and the consumer surplus. We highlight three core findings.
First, ISPs gain most of the profits with only three or four pricing tiers and
likely have little incentive to increase granularity of pricing even further.
Second, we show that consumer surplus follows closely, if not precisely, the
increases in ISP profit with more pricing tiers. Finally, the common ISP
practice of structuring tiered contracts according to the cost of carrying the
traffic flows (e.g., offering a discount for traffic that is local) can be
suboptimal and that dividing contracts based on both traffic demand and the
cost of carrying it into only three or four tiers yields near-optimal profit
for the ISP."
"Given the rapid increase in traffic, greater demands have been put on
research in high-speed switching systems. Such systems have to simultaneously
meet several constraints, e.g., high throughput, low delay and low complexity.
This makes it challenging to design an efficient scheduling algorithm, and has
consequently drawn considerable research interest. However, previous results
either cannot provide a 100% throughput guarantee without a speedup, or require
a complex centralized scheduler. In this paper, we design a distributed 100%
throughput algorithm for crosspoint buffered switches, called DISQUO, with very
limited message passing. We prove that DISQUO can achieve 100% throughput for
any admissible Bernoulli traffic, with a low time complexity of O(1) per port
and a few bits message exchanging in every time slot. To the best of our
knowledge, it is the first distributed algorithm that can provide a 100%
throughput for a crosspoint buffered switch."
A Multipath Transport Protocol for Future Internet
"Vehicular sensor network (VSN) is an emerging technology, which combines
wireless communication offered by vehicular ad hoc networks (VANET) with
sensing devices installed in vehicles. VSN creates a huge opportunity to extend
the road-side sensor infrastructure of existing traffic control systems. The
efficient use of the wireless communication medium is one of the basic issues
in VSN applications development. This paper introduces a novel method of
selective data collection for traffic control applications, which provides a
significant reduction in data amounts transmitted through VSN. The underlying
idea is to detect the necessity of data transfers on the basis of uncertainty
determination of the traffic control decisions. According to the proposed
approach, sensor data are transmitted from vehicles to the control node only at
selected time moments. Data collected in VSN are processed using on-line
traffic simulation technique, which enables traffic flow prediction,
performance evaluation of control strategies and uncertainty estimation. If
precision of the resulting information is insufficient, the optimal control
strategy cannot be derived without ambiguity. As a result the control decision
becomes uncertain and it is a signal informing that new traffic data from VSN
are necessary to provide more precise prediction and to reduce the uncertainty
of decision. The proposed method can be applied in traffic control systems of
different types e.g. traffic signals, variable speed limits, and dynamic route
guidance. The effectiveness of this method is illustrated in an experimental
study on traffic control at signalised intersection."
"Many works have studied the Internet topology, but few have investigated the
question of how it evolves over time. This paper focuses on the Internet
routing IP-level topology and proposes a first step towards realistic modeling
of its dynamics. We study periodic measurements of routing trees from a single
monitor to a fixed destination set and identify invariant properties of its
dynamics. We then propose a simple model for the underlying mechanisms of the
topology dynamics. Simulations show that it effectively captures the observed
behaviors, thus providing key insights of relevant mechanisms governing the
Internet routing dynamics."
"Our current world is revolutionned by the networks which are interconnecting
any machine to any other one. Nowadays equipments are plugged to the network by
the way of many different network adapters (Ethernet, wifi, GSM), and network
equipments are more and more interconnected to each other creating a highly
mesh network. Thus, redundant paths are created between any two endpoints
enabling multipath or the use of multiple paths to handle a communication. The
benefits of multipath are the enhancement of robustness against failure, and
minimizing communication cost. In the present work, we will talk about some
solutions of multipath applied to the transport level. We will expose the most
important problem facing multipath which is packet reordering, and we will also
expose results of our simulations."
"With the escalation of the IEEE 802.11 based wireless networks, voice over IP
and analogous applications are also used over wireless networks. Recently, the
wireless LAN systems are spaciously deployed for public Internet services. In
public wireless LAN systems, reliable user authentication and mobility support
are indispensable issues. When a mobile device budges out the range of one
access point (AP) and endeavor to connect to new AP, it performs handoff.
Contemporarily, PNC and SNC were proposed to propagate the MN context to the
entire neighboring AP's on the wireless network with the help of neighbor
graph. In this paper, we proposed a non-overlapping AP's caching scheme (NACS),
which propagates the mobile node context to those AP's which do not overlap
with the current AP. To capture the topology of non-overlapping AP's in the
wireless network, non-overlapping graph (NOG) is generated at each AP.
Simulation results shows that NACS reduces the signaling cost of propagating
the MN context to the neighbor AP's in the wireless network."
"We investigate the design of a broadcast system in order to maximise the
throughput. This task is usually challenging due to the channel variability.
Forty years ago, Cover introduced and compared two schemes: time sharing and
superposition coding. Even if the second scheme was proved to be optimal for
some channels, modern satellite communications systems such as DVB-SH and
DVB-S2 mainly rely on time sharing strategy to optimize the throughput. They
consider hierarchical modulation, a practical implementation of superposition
coding, but only for unequal error protection or backward compatibility
purposes. We propose in this article to combine time sharing and hierarchical
modulation together and show how this scheme can improve the performance in
terms of available rate. We introduce the hierarchical 16-APSK to boost the
performance of the DVB-S2 standard. We also evaluate various strategies to
group the receivers in pairs when using hierarchical modulation. Finally, we
show in a realistic use case based on DVB-S2 that the combined scheme can
provide throughput gains greater than 10% compared to the best time sharing
strategy."
"Link failures in wide area networks are common and cause significant data
losses. Mesh-based protection schemes offer high capacity efficiency but they
are slow and require complex signaling. Additionally, real-time reconfiguration
of a cross-connect threatens their transmission integrity. On the other hand,
coding-based protection schemes are proactive. Therefore, they have higher
restoration speed, lower signaling complexity, and higher transmission
integrity. This paper introduces a coding-based protection scheme, named Coded
Path Protection (CPP). In CPP, a backup copy of the primary data is encoded
with other data streams, resulting in capacity savings. This paper presents an
optimal and simple capacity placement and coding group formation algorithm. The
algorithm converts the sharing structure of any solution of a Shared Path
Protection (SPP) technique into a coding structure with minimum extra capacity.
We conducted quantitative and qualitative comparisons of our technique with the
SPP and, another technique, known as p-cycle protection. Simulation results
confirm that the CPP is significantly faster than the SPP and p-cycle
techniques. CPP incurs marginal extra capacity on top of SPP. Its capacity
efficiency is lower than the p-cycle technique for dense networks but can be
higher for sparse networks. In addition, unlike p-cycle protection, CPP is
inherently suitable for the wavelength continuity constraint in optical
networks."
"We address a major flaw in the abovementioned paper, which proposes to
calculate effective capacity of random channels by the use of central limit
theorem. We analytically show that the authors are incorrect in finding the
effective capacity by first taking the limit of cumulative random process
rather than taking the limit of moment generating function of the same process.
We later quantify our results over a correlated ON-OFF process."
"The paper presents a methodology of transmitting voice in SMS (Short Message
Service) over GSM network. Usually SMS contents are text based and limited to
140 bytes. It supports national and international roaming, but also supported
by other telecommunication such as TDMA (Time Division Multiple Access), CDMA
(Code Division Multiple Access) as well. It can sent/ receive simultaneously
with other services. Such features make it favorable for this methodology. For
this an application is developed using J2ME platform which is supported by all
mobile phones in the world. This algorithm's test is conducted on N95 having
Symbian Operating System (OS)."
"A mobile Ad-hoc network (MANET) is a dynamic multi hop wireless network
established by a group of nodes in which there is no central administration.
Due to mobility of nodes and dynamic network topology, the routing is one of
the most important challenges in ad-hoc networks. Several routing algorithms
for MANETs have been proposed by the researchers which have been classified
into various categories, however, the most prominent categories are proactive,
reactive and hybrid. The performance comparison of routing protocols for MANETs
has been presented by other researcher also, however, none of these works
considers proactive, reactive and hybrid protocols together. In this paper, the
performance of proactive (DSDV), reactive (DSR and AODV) and hybrid (ZRP)
routing protocols has been compared. The performance differentials are analyzed
on the basis of throughput, average delay, routing overhead and number of
packets dropped with a variation of number of nodes, pause time and mobility."
"As wireless access technologies grow rapidly, the recent studies have focused
on granting mobile users the ability of roaming across different wireless
networks in a seamless manner thus offering seamless mobility. The different
characteristics of each wireless technology with regards to QoS brought many
challenges for provisioning the continuous services (audio/video streaming) in
a seamless way. In this paper, we intend to review the existing context-aware
methods which offered solutions for service continuity. We looked at the types
of context information used in each solution. Through this study, it is clear
that context awareness plays a significant role in handover process in order to
satisfy users demanding seamless services. Therefore, the goal of this paper is
to compare the existing methods grouped as general, IMS based, and WLAN/WiMAX
solutions in terms of several criteria, such as interworking architecture,
service continuity, and QoS provisioning."
"In this paper, we study a new approach to model the overall Energy
Consumption (EC) in Wireless Sensor Networks (WSN). First, we extract
parameters involving in the EC of WSNs. The dependency between configuration
parameters and the average residual energy of a specific application is then
investigated. Our approach has three key steps: profiling, parameter reduction,
and modeling. In profiling, a sensor network simulator is re-run 800 times with
different values of the configuration parameters in order to profile the
average residual energy in nodes. In the parameter reduction, three statistical
analyses (p-value, linear and non-linear correlation) are applied to the
outcome of profiled experiments in order to separate the effective parameters
on WSN residual energy. Finally, linear regression is used to model the
relation between the chosen effective parameters and the residual energy. The
evaluation based on running the simulator for another 200 times with different
values of the effective parameters shows that the model can predict the
residual energy of nodes in WSN with average error of less than 13%."
"Minimizing the energy consumption of a wireless sensor network application is
crucial for effective realization of the intended application in terms of cost,
lifetime, and functionality. However, the minimizing task is hardly possible as
no overall energy cost function is available for optimization. Optimizing a
specific component of the total energy cost does not help in reducing the total
energy cost as this reduction may be negated by an increase in the energy
consumption of other components of the application. Recently we proposed
Hierarchy Energy Driven Architecture as a robust architecture that takes into
account all principal energy constituents of wireless sensor network
applications. Based on the proposed architecture, this paper presents a single
overall model and proposes a feasible formulation to express the overall energy
consumption of a generic wireless sensor network application in terms of its
energy constituents. The formulation offers a concrete expression for
evaluating the performance of a wireless sensor network application, optimizing
its constituent's operations, and designing more energy-efficient applications.
The paper also presents simulation results to demonstrate the feasibility of
our model and energy formulation"
Wireless mesh cubes are used to improve the channel signaling.
"In randomly deployed networks, such as sensor networks, an important problem
for each node is to discover its \textit{neighbor} nodes so that the
connectivity amongst nodes can be established. In this paper, we consider this
problem by incorporating the physical layer parameters in contrast to the most
of the previous work which assumed a collision channel. Specifically, the pilot
signals that nodes transmit are successfully decoded if the strength of the
received signal relative to the interference is sufficiently high. Thus, each
node must extract signal parameter information from the superposition of an
unknown number of received signals. This problem falls naturally in the purview
of random set theory (RST) which generalizes standard probability theory by
assigning \textit{sets}, rather than values, to random outcomes. The
contributions in the paper are twofold: first, we introduce the realistic
effect of physical layer considerations in the evaluation of the performance of
\textit{logical} discovery algorithms; such an introduction is necessary for
the accurate assessment of how an algorithm performs. Secondly, given the
\textit{double} uncertainty of the environment (that is, the lack of knowledge
of the number of neighbors along with the lack of knowledge of the individual
signal parameters), we adopt the viewpoint of RST and demonstrate its advantage
relative to classical matched filter detection method."
"In this paper, a family of ant colony algorithms called DAACA for data
aggregation has been presented which contains three phases: the initialization,
packet transmission and operations on pheromones. After initialization, each
node estimates the remaining energy and the amount of pheromones to compute the
probabilities used for dynamically selecting the next hop. After certain rounds
of transmissions, the pheromones adjustment is performed periodically, which
combines the advantages of both global and local pheromones adjustment for
evaporating or depositing pheromones. Four different pheromones adjustment
strategies are designed to achieve the global optimal network lifetime, namely
Basic-DAACA, ES-DAACA, MM-DAACA and ACS-DAACA. Compared with some other data
aggregation algorithms, DAACA shows higher superiority on average degree of
nodes, energy efficiency, prolonging the network lifetime, computation
complexity and success ratio of one hop transmission. At last we analyze the
characteristic of DAACA in the aspects of robustness, fault tolerance and
scalability."
"Many localization algorithms and systems have been developed by means of
wireless sensor networks for both indoor and outdoor environments. To achieve
higher localization accuracy, extra hardware equipments are utilized by most of
the existing localization solutions, which increase the cost and considerably
limit the location-based applications. The Internet of Things (IOT) integrates
many technologies, such as Internet, Zigbee, Bluetooth, infrared, WiFi, GPRS,
3G, etc, which can enable different ways to obtain the location information of
various objects. Location-based service is a primary service of the IOT, while
localization accuracy is a key issue. In this paper, a higher accuracy
localization scheme is proposed which can effectively satisfy diverse
requirements for many indoor and outdoor location services. The proposed scheme
composes of two phases: 1) partition phase, in which the target region is split
into small grids; 2) localization refinement phase, in which a higher accuracy
of localization can be obtained by applying an algorithm designed in the paper.
A trial system is set up to verify correctness of the proposed scheme and
furthermore to illustrate its feasibility and availability. The experimental
results show that the proposed scheme can improve the localization accuracy."
"One of the key issues in mobile communication is to find the current location
of mobile terminal (MT) to deliver the services, which is called as location
management (LM). Increasing users and diverse services demand for a
high-quality skeleton for LM. As an MT moves within a cellular network, it
registers its new location to the nearest base station (BS). When a call
arrives for an MT, the network searches the target MT in the area where it was
last registered. This paper presents comprehensive classification of existing
major LM schemes, their comparative study and factors influencing their
performance. Finally, guidelines for developing and rating a LM scheme are
suggested with the help of LPCIC rule, which is the main contribution of this
paper."
"In energy constrained wireless sensor networks, it is significant to make
full use of the limited energy and maximize the network lifetime even when
facing some unexpected situation. In this paper, all sensor nodes are grouped
into clusters, and for each cluster, it has a mobile cluster head to manage the
whole cluster. We consider an emergent situation that one of the mobile cluster
heads is broken down, and hence the whole cluster is consequently out of work.
An efficient approach is proposed for recovering the failure cluster by
selecting multiple static sensor nodes as the cluster heads to collect packets
and transmit them to the sink node. Improved simulated annealing algorithm is
utilized to achieve the uniform deployment of the cluster heads. The new
cluster heads are dynamically changed in order to keep balanced energy
consumption. Among the new cluster heads, packets are transmitted through
multi-hop forwarding path which is cost-lowest path found by Dijkstra's
algorithm. A balanced energy consumption model is provided to help find the
cost-lowest path and prolong the lifetime of the network. The forwarding path
is updated dynamically according to the cost of the path and residual energy of
the node in that path. The experimental results show that the failure cluster
is recovered and the lifetime of the cluster is prolonged."
"Congestions in wireless sensor networks (WSNs) could potentially cause packet
loss, throughput impairment and energy waste. To address this issue, a
hop-by-hop cross-layer congestion control scheme (HCCC) built on
contention-based MAC protocol is proposed in this paper. According to MAC-layer
channel information including buffer occupancy ratio and congestion degree of
local node, HCCC dynamically adjusts channel access priority in MAC layer and
data transmission rate of the node to tackle the problem of congestion.
Simulations have been conducted to compare HCCC against closely-related
existing schemes. The results show that HCCC exhibits considerable superiority
in terms of packets loss ratio, throughput and energy efficiency."
"With the increasing popularity of wireless networks, wireless local area
networks (WLANs) have attracted significant research interest, which play a
critical role in providing anywhere and anytime connectivity. For WLANs the
IEEE 802.11 standard is the most mature technology and has been widely adopted
for wireless networks. This paper analyzes real-time performance of the IEEE
802.11 standard that adopts the MAC protocol of Distributed Coordination
Function (DCF) operating in infrastructure mode. Extensive simulations have
been done to examine how the network performance in terms of realtime metrics
including effective data rate, latency and packet loss rate will be impacted by
some critical parameters (e.g. CWmin and packet payload). The results are
presented and analyzed. The analysis of simulation results can provide support
for parameter configuration and optimization of WLANs for realtime
applications."
"Cyber-physical systems (CPS) can be viewed as a new generation of systems
with integrated control, communication and computational capabilities. Like the
internet transformed how humans interact with one another, cyber-physical
systems will transform how people interact with the physical world. Currently,
the study of CPS is still in its infancy and there exist many research issues
and challenges ranging from electricity power, health care, transportation and
smart building etc. In this paper, an introduction of CPeSC3 (cyber physical
enhanced secured wireless sensor networks (WSNs) integrated cloud computing for
u-life care) architecture and its application to the health care monitoring and
decision support systems is given. The proposed CPeSC3 architecture is composed
of three main components, namely 1) communication core, 2) computation core,
and 3) resource scheduling and management core. Detailed analysis and
explanation are given for relevant models such as cloud computing, real time
scheduling and security models. Finally, a medical health care application
scenario is presented based on our practical test-bed which has been built for
3 years."
"IEEE 802.15.4 supports a Guaranteed Time Slot (GTS) allocation mechanism for
time-critical and delay-sensitive data transmissions in Wireless Personal Area
Networks (WPANs). However, the inflexible first-come-first-served GTS
allocation policy and the passive deallocation mechanism significantly reduce
network efficiency. In this paper, we propose an Adaptive and Real-Time GTS
Allocation Scheme (ART-GAS) to provide differentiated services for devices with
different priorities, which guarantees data transmissions for time-sensitive
and high-traffic devices. The bandwidth utilization in IEEE 802.15.4-based PAN
is improved. Simulation results show that our ART-GAS algorithm significantly
outperforms the existing GTS mechanism specified in IEEE 802.15.4."
"With the rapid development of new and innovative applications for mobile
devices like smartphones, advances in battery technology have not kept pace
with rapidly growing energy demands. Thus energy consumption has become a more
and more important issue of mobile devices. To meet the requirements of saving
energy, it is critical to monitor and analyze the energy consumption of
applications on smartphones. For this purpose, we develop a smart energy
monitoring system called SEMO for smartphones using Android operating system.
It can profile mobile applications with battery usage information, which is
vital for both developers and users."
"Mobile devices have been shipped with multiple wireless network interfaces in
order to meet their diverse communication and networking demands. In this
paper, we propose an A-GPS assisted scheme that discovers the nearest Wi-Fi
network access points (APs) by using user's location information. This allows
the user to switch to the Wi-Fi interface in an intelligent manner when she/he
arrives at the nearest Wi-Fi network AP. Therefore, it avoids the long periods
in idle state and greatly reduces the number of unnecessary Wi-Fi scans on the
mobile device. The experimental results demonstrate that our scheme effectively
saves energy for mobile devices integrated with Wi-Fi and cellular interfaces."
"We design and analyze a mechanism for forming coalitions of peers in a data
swarming system where peers have heterogeneous upload capacities. A coalition
is a set of peers that explicitly cooperate with other peers inside the
coalition via choking, data replication, and capacity allocation strategies.
Further, each peer interacts with other peers outside its coalition via
potentially distinct choking, data replication, and capacity allocation
strategies. Following on our preliminary work in IEEE ICNP 2011 that
demonstrated significant performance benefits of coalitions, we present here a
comprehensive analysis of the choking and data replication strategies for
coalitions.
  We first develop an analytical model to understand a simple random choking
strategy as a within-coalition strategy and show that it accurately predicts a
coalition's performance. Our analysis formally shows that the random choking
strategy can help a coalition achieve near-optimal performance by optimally
choosing the re-choking interval lengths and the number unchoke slots. Further,
our analytical model can be easily adapted to model a BitTorrent-like swarm. We
also introduce a simple data replication strategy which significantly improves
data availability within a coalition as compared to the rarest-first piece
replication strategy employed in BitTorrent systems. We further propose a
cooperation-aware better response strategy that achieves convergence of the
dynamic coalition formation process when peers freely join or leave any
coalition. Finally, using extensive simulations, we demonstrate improvements in
the performance of a swarming system due to coalition formation."
"Sensors have limited resources so it is important to manage the resources
efficiently to maximize their use. A sensor's battery is a crucial resource as
it singly determines the lifetime of sensor network applications. Since these
devices are useful only when they are able to communicate with the world, radio
transceiver of a sensor as an I/O and a costly unit plays a key role in its
lifetime. This resource often consumes a big portion of the sensor's energy as
it must be active most of the time to announce the existence of the sensor in
the network. As such the radio component has to deal with its embedded sensor
network whose parameters and operations have significant effects on the
sensor's lifetime. In existing energy models, hardware is considered, but the
environment and the network's parameters did not receive adequate attention.
Energy consumption components of traditional network architecture are often
considered individually and separately, and their influences on each other have
not been considered in these approaches. In this paper we consider all possible
tasks of a sensor in its embedded network and propose an energy management
model. We categorize these tasks in five energy consuming constituents. The
sensor's Energy Consumption (EC) is modeled on its energy consuming
constituents and their input parameters and tasks. The sensor's EC can thus be
reduced by managing and executing efficiently the tasks of its constituents.
The proposed approach can be effective for power management, and it also can be
used to guide the design of energy efficient wireless sensor networks through
network parameterization and optimization."
"In contrast to the classical cyclic prefix (CP)-OFDM, the time domain
synchronous (TDS)-OFDM employs a known pseudo noise (PN) sequence as guard
interval (GI). Conventional channel estimation methods for TDS-OFDM are based
on the exploitation of the PN sequence and consequently suffer from intersymbol
interference (ISI). This paper proposes a novel dataaided channel estimation
method which combines the channel estimates obtained from the PN sequence and,
most importantly, additional channel estimates extracted from OFDM data
symbols. Data-aided channel estimation is carried out using the rebuilt OFDM
data symbols as virtual training sequences. In contrast to the classical turbo
channel estimation, interleaving and decoding functions are not included in the
feedback loop when rebuilding OFDM data symbols thereby reducing the
complexity. Several improved techniques are proposed to refine the data-aided
channel estimates, namely one-dimensional (1-D)/two-dimensional (2-D) moving
average and Wiener filtering. Finally, the MMSE criteria is used to obtain the
best combination results and an iterative process is proposed to progressively
refine the estimation. Both MSE and BER simulations using specifications of the
DTMB system are carried out to prove the effectiveness of the proposed
algorithm even in very harsh channel conditions such as in the single frequency
network (SFN) case."
"In this paper, we propose LMEEC, a cluster-based routing protocol with low
energy consumption for wireless sensor networks. Our protocol is based on a
strategy which aims to provide a more reasonable exploitation of the selected
nodes (cluster-heads) energy. Simulation results show the effectiveness of
LMEEC in decreasing the energy consumption, and in prolonging the network
lifetime, compared to LEACH."
"The Internet is constantly changing, and its hierarchy was recently shown to
become flatter. Recent studies of inter-domain traffic showed that large
content providers drive this change by bypassing tier-1 networks and reaching
closer to their users, enabling them to save transit costs and reduce reliance
of transit networks as new services are being deployed, and traffic shaping is
becoming increasingly popular.
  In this paper we take a first look at the evolving connectivity of large
content provider networks, from a topological point of view of the autonomous
systems (AS) graph. We perform a 5-year longitudinal study of the topological
trends of large content providers, by analyzing several large content providers
and comparing these trends to those observed for large tier-1 networks. We
study trends in the connectivity of the networks, neighbor diversity and
geographical spread, their hierarchy, the adoption of IXPs as a convenient
method for peering, and their centrality. Our observations indicate that
content providers gradually increase and diversify their connectivity, enabling
them to improve their centrality in the graph, and as a result, tier-1 networks
lose dominance over time."
"A wireless network is realized by mobile devices which communicate over radio
channels. Since, experiments of real life problem with real devices are very
difficult, simulation is used very often. Among many other important properties
that have to be defined for simulative experiments, the mobility model and the
radio propagation model have to be selected carefully. Both have strong impact
on the performance of mobile wireless networks, e.g., the performance of
routing protocols varies with these models. There are many mobility and radio
propagation models proposed in literature. Each of them was developed with
different objectives and is not suited for every physical scenario. The radio
propagation models used in common wireless network simulators, in general
researcher consider simple radio propagation models and neglect obstacles in
the propagation environment. In this paper, we study the performance of
wireless networks simulation by consider different Radio propagation models
with considering obstacles in the propagation environment. In this paper we
analyzed the performance of wireless networks by OPNET Modeler .In this paper
we quantify the parameters such as throughput, packet received attenuation."
"In this paper we propose a new routing protocol with low energy consumption
for wireless sensor networks based on the clustering approach. Our protocol is
based on a strategy which aims at providing a more equitable exploitation of
the selected nodes (cluster-heads) energy by distributing their load of the
managed sensors during the clustering process. In order to save the energy
dissipated while transmitting sensed data to the base station, the multi-hops
routing strategy is used to arrange the communication of the data between
cluster-heads nodes. Simulation results demonstrate that our proposed protocol
decreases the energy consumption and prolongs the network lifetime."
"The vision of next generation wireless network (NGWN) is to integrate
different wireless access technologies, each with its own characteristics, into
a common IP-based core network to provide mobile user with service continuity
and seamless roaming. One of the major issues for the converged heterogeneous
networks is providing a seamless vertical handover (VHO) with QoS support. In
this paper we have reviewed the various interworking architectures and handover
scenarios between UMTS and WiMAX. Also, we have compared the proposed solutions
based on different criteria and revealed the pros and cons of each scheme. The
comparison aids to adopt a better interworking and handover mechanism in NGWN."
"The knowledge of end-to-end network distances is essential to many Internet
applications. As active probing of all pairwise distances is infeasible in
large-scale networks, a natural idea is to measure a few pairs and to predict
the other ones without actually measuring them. This paper formulates the
distance prediction problem as matrix completion where unknown entries of an
incomplete matrix of pairwise distances are to be predicted. The problem is
solvable because strong correlations among network distances exist and cause
the constructed distance matrix to be low rank. The new formulation circumvents
the well-known drawbacks of existing approaches based on Euclidean embedding.
  A new algorithm, so-called Decentralized Matrix Factorization by Stochastic
Gradient Descent (DMFSGD), is proposed to solve the network distance prediction
problem. By letting network nodes exchange messages with each other, the
algorithm is fully decentralized and only requires each node to collect and to
process local measurements, with neither explicit matrix constructions nor
special nodes such as landmarks and central servers. In addition, we compared
comprehensively matrix factorization and Euclidean embedding to demonstrate the
suitability of the former on network distance prediction. We further studied
the incorporation of a robust loss function and of non-negativity constraints.
Extensive experiments on various publicly-available datasets of network delays
show not only the scalability and the accuracy of our approach but also its
usability in real Internet applications."
"Multistage Interconnection Networks (MINs) are very popular in switching and
communication applications. A MIN connects N inputs to N outputs and is
referred as an N \times N MIN; having size N. Optical Multistage
Interconnection Network (OMIN) represents an important class of Interconnection
networks. Crosstalk is the basic problem of OMIN. Switch Conflict and Link
Conflict are the two main reason of crosstalk. In this paper, we are
considering both problems. A number of techniques like Optical window, Improved
Window, Heuristic, Genetic, and Zero have been proposed earlier in this
research domain. In this paper, we have proposed two algorithms called Address
Selection Algorithm and Route Selection Algorithm (RSA). RSA is based on
Improved Window Method. We have applied the proposed algorithms on existing
Omega network, having shuffle-exchange connection pattern. The main
functionality of ASA and RSA is to minimize the number of switch and link
conflicts in the network and to provide conflict free routes."
"We provide an analytical study of the impact of packet skipping and
opportunistic network coding on the timely communication of messages through a
single network element. In a first step, we consider a single-server queueing
system with Poisson arrivals, exponential service times, and a single buffer
position. Packets arriving at a network node have a fixed deadline before which
they should reach the destination. To preserve server capacity, we introduce a
thresholding policy, based on remaining time until deadline expiration, to
decide whether to serve a packet or skip its service. The obtained goodput
improvement of the system is derived, as well as the operating conditions under
which thresholding can enhance performance. Subsequently, we focus our analysis
on a system that supports network coding instead of thresholding. We
characterize the impact of network coding at a router node on the delivery of
packets associated with deadlines. We model the router node as a queueing
system where packets arrive from two independent Poisson flows and undergo
opportunistic coding operations. We obtain an exact expression for the goodput
of the system and study the achievable gain. Finally, we provide an analytical
model that considers both network coding and packet skipping, capturing their
joint performance. A comparative analysis between the aforementioned approaches
is provided."
"Source-controlled routing has been proposed as a way to improve flexibility
of future network architectures, as well as simplifying the data plane.
However, if a packet specifies its path, this precludes fast local re-routing
within the network. We propose SlickPackets, a novel solution that allows
packets to slip around failures by specifying alternate paths in their headers,
in the form of compactly-encoded directed acyclic graphs. We show that this can
be accomplished with reasonably small packet headers for real network
topologies, and results in responsiveness to failures that is competitive with
past approaches that require much more state within the network. Our approach
thus enables fast failure response while preserving the benefits of
source-controlled routing."
"In this paper, we propose to improve the performance of the channel
estimation for LTE Downlink systems under the effect of the channel length. As
LTE Downlink system is a MIMO-OFDMA based system, a cyclic prefix (CP) is
inserted at the beginning of each transmitted OFDM symbol in order to mitigate
both inter-carrier interference (ICI) and inter-symbol interference (ISI). The
inserted CP is usually equal to or longer than the channel length. However, the
cyclic prefix can be shorter because of some unforeseen channel behaviour.
Previous works have shown that in the case where the cyclic prefix is equal to
or longer than the channel length, LMMSE performs better than LSE but at the
cost of computational complexity .In the other case, LMMSE performs also better
than LS only for low SNR values. However, LS shows better performance for LTE
Downlink systems for high SNR values. Therefore, we propose a hybrid LS-LMMSE
channel estimation technique robust to the channel length effect. MATLAB
Monte-Carlo simulations are used to evaluate the performance of the proposed
estimator in terms of Mean Square Error (MSE) and Bit Error Rate (BER) for 2x2
LTE Downlink systems."
"Location information is the major component in location based applications.
This information is used in different safety and service oriented applications
to provide users with services according to their Geolocation. There are many
approaches to locate mobile nodes in indoor and outdoor environments. In this
paper, we are interested in outdoor localization particularly in cellular
networks of mobile nodes and presented a localization method based on cell and
user location information. Our localization method is based on hello message
delay (sending and receiving time) and coordinate information of Base
Transceiver Station (BTSs). To validate our method across cellular network, we
implemented and simulated our method in two scenarios i.e. maintaining database
of base stations in centralize and distributed system. Simulation results show
the effectiveness of our approach and its implementation applicability in
telecommunication systems."
"In this paper we present a model for the lifetime of wireless sensor
networks. The model takes into consideration several parameters such as the
total number of sensors, network size, percentage of sink nodes, location of
sensors, the mobility of sensors, and power consumption. A definition of the
life time of the network based on three different criteria is introduced;
percentage of available power to total power, percentage of alive sensors to
total sensors, and percentage of alive sink sensors to total sink sensors. A
Matlab based simulator is developed for the introduced model. A number of
wireless sensor networks scenarios are presented and discussed."
"The focus of user behavior in the Internet has changed over the recent years
towards being driven by exchanging and accessing information. Many advances in
networking technologies have utilized this change by focusing on the content of
an exchange rather than the endpoints exchanging the content. Network coding
and information centric networking are two examples of these technology trends,
each being developed largely independent so far. This paper brings these areas
together in an evolutionary as well as explorative setting for a new
internetworking architecture. We outline opportunities for applying network
coding in a novel and performance-enhancing way that could eventually push
forward the case for information centric network itself."
"This paper is a thorough study of a digital broadcasting system adapted to
the small mountainous island of Mauritius. A digital LAN was designed with
MPEG-2 signals. The compressed signals were transmitted using DVB-T and QAM
modulators. QAM-16 and QAM-64 modulators were designed and tested with a
simulator under critical conditions of AWGN and phase noises. Results obtained
from simulation have shown that Digital video broadcast with a single frequency
network (SFN) is possible in Mauritius with QAM-64 and QAM-16 modulators
applying COFDM mode of transmission. However, this study has also shown that
QAM-16 modulator had a better performance at low AWGN values (less than 12 dB)
and can be adopted for Mauritius Island, provided that the number of
transmitted channels is not high enough."
"The Internet Threat Monitoring (ITM),is a globally scoped Internet monitoring
system whose goal is to measure, detect, characterize, and track threats such
as distribute denial of service(DDoS) attacks and worms. To block the
monitoring system in the internet the attackers are targeted the ITM system. In
this paper we address flooding attack against ITM system in which the attacker
attempt to exhaust the network and ITM's resources, such as network bandwidth,
computing power, or operating system data structures by sending the malicious
traffic. We propose an information-theoretic frame work that models the
flooding attacks using Botnet on ITM. Based on this model we generalize the
flooding attacks and propose an effective attack detection using Honeypots."
"The explosive increase in data demand coupled with the rapid deployment of
various wireless access technologies have led to the increase of number of
multi-homed or multi-interface enabled devices. Fully exploiting these
interfaces has motivated researchers to propose numerous solutions that
aggregate their available bandwidths to increase overall throughput and satisfy
the end-user's growing data demand. These solutions, however, have faced a
steep deployment barrier that we attempt to overcome in this paper. We propose
a Deployable Bandwidth Aggregation System (DBAS) for multi-interface enabled
devices. Our system does not introduce any intermediate hardware, modify
current operating systems, modify socket implementations, nor require changes
to current applications or legacy servers. The DBAS architecture is designed to
automatically estimate the characteristics of applications and dynamically
schedule various connections or packets to different interfaces. Since our main
focus is deployability, we fully implement DBAS on the Windows operating system
and evaluate various modes of operation. Our implementation and simulation
results show that DBAS achieves throughput gains up to 193% compared to current
operating systems, while operating as an out-of-the-box standard Windows
executable, highlighting its deployability and ease of use."
"Combining cognitive radio technology with user cooperation could be
advantageous to both primary and secondary transmissions. In this paper, we
propose a first relaying scheme for cognitive radio networks (called ""Adaptive
relaying scheme 1""), where one relay node can assist the primary or the
secondary transmission with the objective of improving the outage probability
of the secondary transmission with respect to a primary outage probability
threshold. Upper bound expressions of the secondary outage probability using
the proposed scheme are derived over Rayleigh fading channels. Numerical and
simulation results show that the secondary outage probability using the
proposed scheme is lower than that of other relaying schemes. Then, we extend
the proposed scheme to the case where the relay node has the ability to decode
both the primary and secondary signals and also can assist simultaneously both
transmissions. Simulations show the performance improvement that can be
obtained due to this extension in terms of secondary outage probability."
"We develop an approximate analytical technique for evaluating the performance
of multi-hop networks based on beacon-less CSMA/CA as standardised in IEEE
802.15.4, a popular standard for wireless sensor networks. The network
comprises sensor nodes, which generate measurement packets, and relay nodes
which only forward packets. We consider a detailed stochastic process at each
node, and analyse this process taking into account the interaction with
neighbouring nodes via certain unknown variables (e.g., channel sensing rates,
collision probabilities, etc.). By coupling these analyses of the various
nodes, we obtain fixed point equations that can be solved numerically to obtain
the unknown variables, thereby yielding approximations of time average
performance measures, such as packet discard probabilities and average queueing
delays. Different analyses arise for networks with no hidden nodes and networks
with hidden nodes. We apply this approach to the performance analysis of tree
networks rooted at a data sink. Finally, we provide a validation of our
analysis technique against simulations."
"In this paper we consider the Max-Weight protocol for routing and scheduling
in wireless networks under an adversarial model. This protocol has received a
significant amount of attention dating back to the papers of Tassiulas and
Ephremides. In particular, this protocol is known to be throughput-optimal
whenever the traffic patterns and propagation conditions are governed by a
stationary stochastic process.
  However, the standard proof of throughput optimality (which is based on the
negative drift of a quadratic potential function) does not hold when the
traffic patterns and the edge capacity changes over time are governed by an
arbitrary adversarial process. Such an environment appears frequently in many
practical wireless scenarios when the assumption that channel conditions are
governed by a stationary stochastic process does not readily apply.
  In this paper we prove that even in the above adversarial setting, the
Max-Weight protocol keeps the queues in the network stable (i.e. keeps the
queue sizes bounded) whenever this is feasible by some routing and scheduling
algorithm. However, the proof is somewhat more complex than the negative
potential drift argument that applied in the stationary case. Our proof holds
for any arbitrary interference relationships among edges. We also prove the
stability of $\ep$-approximate Max-Weight under the adversarial model. We
conclude the paper with a discussion of queue sizes in the adversarial model as
well as a set of simulation results."
"We present DISco, a storage and communication middleware designed to enable
distributed and task-centric autonomic control of networks.
  DISco is designed to enable multi-agent identification of anomalous
situations -- so-called ""challenges"" -- and assist coordinated remediation that
maintains degraded -- but acceptable -- service level, while keeping a track of
the challenge evolution in order to enable human-assisted diagnosis of flaws in
the network. We propose to use state-of-art peer-to-peer publish/subscribe and
distributed storage as core building blocks for the DISco service."
"We investigate on the scalability of multihop wireless communications, a
major concern in networking, for the case that users access content replicated
across the nodes. In contrast to the standard paradigm of randomly selected
communicating pairs, content replication is efficient for certain regimes of
file popularity, cache and network size. Our study begins with the detailed
joint content replication and delivery problem on a 2D square grid, a hard
combinatorial optimization. This is reduced to a simpler problem based on
replication density, whose performance is of the same order as the original.
Assuming a Zipf popularity law, and letting the size of content and network
both go to infinity, we identify the scaling laws and regimes of the required
link capacity, ranging from O(\sqrt{N}) down to O(1)."
"Node misbehavior due to selfish or malicious behavior could significantly
degrade the performance of MANET because most existing routing protocols in
MANET aim to find the most efficient path. Overhearing and reputation based
cooperation schemes have been used to detect and isolate the misbehaving nodes
as well as to force them to cooperate. Performance analysis has been done for
the network traffic using OCEAN over DSR on ns2 while considering the low
energy levels for mobile nodes. Throughput, energy level, routing packets and
normalized routing overhead are analyzed for OCEAN and normal DSR to show the
impact of OCEAN on the overall network performance."
"In this paper, we consider resource allocation in the 3GPP Long Term
Evolution (LTE) cellular uplink, which will be the most widely deployed next
generation cellular uplink. The key features of the 3GPP LTE uplink (UL) are
that it is based on a modified form of the orthogonal frequency division
multiplexing based multiple access (OFDMA) which enables channel dependent
frequency selective scheduling, and that it allows for multi-user (MU)
scheduling wherein multiple users can be assigned the same time-frequency
resource. In addition to the considerable spectral efficiency improvements that
are possible by exploiting these two features, the LTE UL allows for transmit
antenna selection together with the possibility to employ advanced receivers at
the base-station, which promise further gains. However, several practical
constraints that seek to maintain a low signaling overhead, are also imposed.
In this paper, we show that the resulting resource allocation problem is
APX-hard and then propose a local ratio test (LRT) based constant-factor
polynomial-time approximation algorithm. We then propose two enhancements to
this algorithm as well as a sequential LRT based MU scheduling algorithm that
offers a constant-factor approximation and is another useful choice in the
complexity versus performance tradeoff. Further, user pre-selection, wherein a
smaller pool of good users is pre-selected and a sophisticated scheduling
algorithm is then employed on the selected pool, is also examined. We suggest
several such user pre-selection algorithms, some of which are shown to offer
constant-factor approximations to the pre-selection problem. Detailed
evaluations reveal that the proposed algorithms and their enhancements offer
significant gains."
"Thanks to the potential they hold and the variety of their application
domains, Multimedia Wireless Sensor Networks (MWSN) are forecast to become
highly integrated into our daily activities. Due to the carried content nature,
mainly composed of images and/or video streams with high throughput and delay
constraints, Quality of Service in the context of MWSN is a crucial issue. In
this paper, we propose a QoS and energy aware geographic routing protocol for
MWSN: QGRP. The proposed protocol addresses bandwidth, delay and energy
constraints associated with MWSN. QGRP adopts an analytical model of IEEE
802.11 Distributed Coordination Function (DCF) to estimate available bandwidth
and generates loop-free routing paths."
"This dissertation is a study on the design and analysis of novel, optimal
routing and rate control algorithms in wireless, mobile communication networks.
Congestion control and routing algorithms upto now have been designed and
optimized for wired or wireless mesh networks. In those networks, optimal
algorithms (optimal in the sense that either the throughput is maximized or
delay is minimized, or the network operation cost is minimized) can be
engineered based on the classic time scale decomposition assumption that the
dynamics of the network are either fast enough so that these algorithms
essentially see the average or slow enough that any changes can be tracked to
allow the algorithms to adapt over time. However, as technological advancements
enable integration of ever more mobile nodes into communication networks, any
rate control or routing algorithms based, for example, on averaging out the
capacity of the wireless mobile link or tracking the instantaneous capacity
will perform poorly. The common element in our solution to engineering
efficient routing and rate control algorithms for mobile wireless networks is
to make the wireless mobile links seem as if they are wired or wireless links
to all but few nodes that directly see the mobile links (either the mobiles or
nodes that can transmit to or receive from the mobiles) through an appropriate
use of queuing structures at these selected nodes. This approach allows us to
design end-to-end rate control or routing algorithms for wireless mobile
networks so that neither averaging nor instantaneous tracking is necessary."
"A typical web search engine consists of three principal parts: crawling
engine, indexing engine, and searching engine. The present work aims to
optimize the performance of the crawling engine. The crawling engine finds new
web pages and updates web pages existing in the database of the web search
engine. The crawling engine has several robots collecting information from the
Internet. We first calculate various performance measures of the system (e.g.,
probability of arbitrary page loss due to the buffer overflow, probability of
starvation of the system, the average time waiting in the buffer). Intuitively,
we would like to avoid system starvation and at the same time to minimize the
information loss. We formulate the problem as a multi-criteria optimization
problem and attributing a weight to each criterion. We solve it in the class of
threshold policies. We consider a very general web page arrival process modeled
by Batch Marked Markov Arrival Process and a very general service time modeled
by Phase-type distribution. The model has been applied to the performance
evaluation and optimization of the crawler designed by INRIA Maestro team in
the framework of the RIAM INRIA-Canon research project."
"Major wireless operators are nowadays facing network capacity issues in
striving to meet the growing demands of mobile users. At the same time,
3G-enabled devices increasingly benefit from ad hoc radio connectivity (e.g.,
Wi-Fi). In this context of hybrid connectivity, we propose Push-and-track, a
content dissemina- tion framework that harnesses ad hoc communication
opportunities to minimize the load on the wireless infrastructure while
guaranteeing tight delivery delays. It achieves this through a control loop
that collects user-sent acknowledgements to determine if new copies need to be
reinjected into the network through the 3G interface. Push-and-Track is
flexible and can be applied to a variety of scenarios, including periodic
message flooding and floating data. For the former, this paper examines
multiple strategies to determine how many copies of the content should be
injected, when, and to whom; for the latter, it examines the achievable offload
ratio depending on the freshness constraints. The short delay-tolerance of
common content, such as news or road traffic updates, make them suitable for
such a system. Use cases with a long delay-tolerance, such as software updates,
are an even better fit. Based on a realistic large-scale vehicular dataset from
the city of Bologna composed of more than 10,000 vehicles, we demonstrate that
Push-and-Track consistently meets its delivery objectives while reducing the
use of the 3G network by about 90%."
"The proposed system highlights a novel approach of exclusive verification
process using gain protocol for ensuring security among both the parties
(client-service provider) in m-commerce application with cloud enabled service.
The proposed system is based on the potential to verify the clients with
trusted hand held device depending on the set of frequent events and actions to
be carried out. The framework of the proposed work is design after collecting a
real time data sets from an android enabled hand set, which when subjected to
gain protocol, will result in detection of malicious behavior of illegal
clients in the network. The real time experiment is performed with applicable
datasets gather, which show the best result for identifying threats from last 2
months data collected."
"We give a livelock free routing algorithm for any allowed network. Unlike
some other solutions to this problem:
  1) packets entering the network have an absolute upper bound on the time to
reach their destination; 2) under light loads, packets are delivered to their
destinations in nearly optimal time; 3) packets with desired paths far away
from congested areas will have routing times far shorter than packets wanting
to access congested areas; 4) if the network becomes congested and later
clears, the network operates just as it would have when it was initially under
a light load.
  The main ideas of this note appear in a different form in my 1994 patent
5,369,745. This note adds to those results and makes them more mathematical."
"In this paper we introduce a novel Automatic Repeat reQuest (ARQ) scheme for
cooperative wireless networks. Our scheme adopts network coding techniques in
order to enhance the total bandwidth of the network by minimizing the total
number of transmissions. The performance of the proposed approach is evaluated
by means of computer simulations and compared to other cooperative schemes,
while an analytical solution is provided to validate the results."
"A wireless sensor network has a wide application domain which is expanding
everyday and they have been deployed pertaining to their application area. An
application independent approach is yet to come to terms with the ongoing
exploitation of the WSNs. In this paper we propose a decentralized lifetime
maximizing tree for application independent data aggregation scheme using the
clustering for data delivery in WSNs. The proposed tree will minimize the
energy consumption which has been a resisting factor in the smooth working of
WSNs as well as minimize the distance between the communicating nodes under the
control of a sub-sink which further communicate and transfer data to the sink
node."
"Real-time applications are performance critical applications that require
bounded service latency. In multi-hop wireless ad-hoc and sensor networks,
communication delays are dominant over processing delays. Therefore, to enable
real-time applications in such networks, the communication latency must be
bounded. In this paper, we derive expressions of real-time capacity that
characterize the ability of a network to deliver data on time as well as
develop network protocols that achieve this capacity. Real-time capacity
expressions are obtained and analyzed for the earliest deadline first, deadline
monotonic. This paper presents a treatment of the real-time capacity limits.
The limits are derived for two extreme traffic topologies namely, the load
balanced topology and the convergecast (i.e., many-to-one) topology. It
considers DM and EDF scheduling algorithms, and discusses the implications of
the capacity limit expressions."
"We show that, for independent interfering sources and a signal link with
exponentially distributed received power, the total probability of outage can
be decomposed as a simple expression of the outages from the individual
interfering sources. We give a mathematical proof of this result, and discuss
some immediate implications, showing how it results in important
simplifications to statistical outage analysis. We also discuss its application
to two active topics of study: spectrum sharing, and sum of interference powers
(e.g., lognormal) analysis."
"This Article presents a thorough overview of QoS routing metrics, resources
and factors affecting performance of QoS routing protocols. The relative
strength, weakness, and applicability of existing QoS routing protocols are
also studied and compared. QoS routing protocols are classified according to
the QoS metrics used type of QoS guarantee assured."
"In this paper we argue that contextual multi-armed bandit algorithms could
open avenues for designing self-learning security modules for computer networks
and related tasks. The paper has two contributions: a conceptual one and an
algorithmical one. The conceptual contribution is to formulate -- as an example
-- the real-world problem of preventing SPIT (Spam in VoIP networks), which is
currently not satisfyingly addressed by standard techniques, as a sequential
learning problem, namely as a contextual multi-armed bandit. Our second
contribution is to present CMABFAS, a new algorithm for general contextual
multi-armed bandit learning that specifically targets domains with finite
actions. We illustrate how CMABFAS could be used to design a fully
self-learning SPIT filter that does not rely on feedback from the end-user
(i.e., does not require labeled data) and report first simulation results."
"This paper presents a formal framework for identifying and filtering SPIT
calls (SPam in Internet Telephony) in an outbound scenario with provable
optimal performance. In so doing, our work is largely different from related
previous work: our goal is to rigorously formalize the problem in terms of
mathematical decision theory, find the optimal solution to the problem, and
derive concrete bounds for its expected loss (number of mistakes the SPIT
filter will make in the worst case).
  This goal is achieved by considering an abstracted scenario amenable to
theoretical analysis, namely SPIT detection in an outbound scenario with pure
sources. Our methodology is to first define the cost of making an error (false
positive and false negative), apply Wald's sequential probability ratio test to
the individual sources, and then determine analytically error probabilities
such that the resulting expected loss is minimized.
  The benefits of our approach are: (1) the method is optimal (in a sense
defined in the paper); (2) the method does not rely on manual tuning and
tweaking of parameters but is completely self-contained and mathematically
justified; (3) the method is computationally simple and scalable. These are
desirable features that would make our method a component of choice in larger,
autonomic frameworks."
"Orthogonal Frequency Division Multiple Access (OFDMA) is a multi-user version
of the Orthogonal Frequency Division Multiplexing (OFDM) transmission
technique, which divides a wideband channel into a number of orthogonal
narrowband subchannels, called subcarriers. An OFDMA system takes advantage of
both frequency diversity (FD) gain and frequency-selective scheduling (FSS)
gain. A FD gain is achieved by allocating a user the subcarriers distributed
over the entire frequency band whereas a FSS gain is achieved by allocating a
user adjacent subcarriers located within a subband of a small bandwidth having
the most favorable channel conditions among other subbands in the entire
frequency band. Multi-User Multiple Input Multiple Output (MU-MIMO) is a
promising technology to increase spectral efficiency. A well-known MU-MIMO mode
is Space-Division Multiple Access (SDMA) which can be used in the downlink
direction to allow a group of spatially separable users to share the same
time/frequency resources. In this paper, we study the gain from FSS in
SDMA-OFDMA systems using the example of WiMAX. Therefore, a complete SDMA-OFDMA
MAC scheduling solution supporting both FD and FSS is proposed. The proposed
solution is analyzed in a typical urban macro-cell scenario by means of
system-level packet-based simulations, with detailed MAC and physical layer
abstractions. By explicitly simulating the MAC layer overhead (MAP) which is
required to signal every packed data burst in the OFDMA frame we can present
the overall performance to be expected at the MAC layer. Our results show that
in general the gain from FSS when applying SDMA is low. However, under specific
conditions, small number of BS antennas or large channel bandwidth, a
significant gain can be achieved from FSS."
"Deployment of sensor network in hostile environment makes it mainly
vulnerable to battery drainage attacks because it is impossible to recharge or
replace the battery power of sensor nodes. Among different types of security
threats, low power sensor nodes are immensely affected by the attacks which
cause random drainage of the energy level of sensors, leading to death of the
nodes. The most dangerous type of attack in this category is sleep deprivation,
where target of the intruder is to maximize the power consumption of sensor
nodes, so that their lifetime is minimized. Most of the existing works on sleep
deprivation attack detection involve a lot of overhead, leading to poor
throughput. The need of the day is to design a model for detecting intrusions
accurately in an energy efficient manner. This paper proposes a hierarchical
framework based on distributed collaborative mechanism for detecting sleep
deprivation torture in wireless sensor network efficiently. Proposed model uses
anomaly detection technique in two steps to reduce the probability of false
intrusion."
"Security of Wireless sensor network (WSN) becomes a very important issue with
the rapid development of WSN that is vulnerable to a wide range of attacks due
to deployment in the hostile environment and having limited resources.
Intrusion detection system is one of the major and efficient defensive methods
against attacks in WSN. A particularly devastating attack is the sleep
deprivation attack, where a malicious node forces legitimate nodes to waste
their energy by resisting the sensor nodes from going into low power sleep
mode. The goal of this attack is to maximize the power consumption of the
target node, thereby decreasing its battery life. Existing works on sleep
deprivation attack have mainly focused on mitigation using MAC based protocols,
such as S-MAC, T-MAC, B-MAC, etc. In this article, a brief review of some of
the recent intrusion detection systems in wireless sensor network environment
is presented. Finally, we propose a framework of cluster based layered
countermeasure that can efficiently mitigate sleep deprivation attack in WSN.
Simulation results on MATLAB exhibit the effectiveness of the proposed model in
detecting sleep-deprivation attacks."
"It is suggested to use multi-layer graphs as a mathematical model in the
design of MPLS networks. The application of this model makes it possible to
design multi-service telecommunication systems simultaneously at several levels
and to reduce the problem to the search of the minimum weight graph."
"This study is devoted to the problem of parametric synthesis of multi-service
telecommunication sys-tems. The main characteristics of telecommunication
systems, which are brought to account in an article, are multilayer structure
formed by the overlayed networks and presence flows with self-similarity
effect. For accounting these features of modern telecommunications systems is
proposed to use a multi-layered graph for describing the system structure, and
self-similar processes model for modeling flows in a network. Solution of
parametric synthesis problem is reduced to a nonlinear programming problem
which is solved by using gradient descent method."
"Considered in this paper is the method of describingc of telecommunications
systems providing VoD service using multi-layer graph. The paper describes the
relations between the structural elements at each hierarchical level of the
multi-layer graph. Transfer of video is a resource consuming task, and it
requires an optimal configuration of the studied system. The usage of the
multi-layer graph makes it possible to consider the telecommunication system as
a whole and avoid falling in the local optimums when solving optimization
problems."
"In this paper, we analyze the performance of cooperative content caching in
vehicular ad hoc networks (VANETs). In particular, we characterize, using
analysis and simulations, the behavior of the probability of outage (i.e. not
finding a requested data chunk at a neighbor) under freeway vehicular mobility.
First, we introduce a formal definition for the probability of outage in the
context of cooperative content caching. Second, we characterize, analytically,
the outage probability under vehicular and random mobility scenarios. Next, we
verify the analytical results using simulations and compare the performance
under a number of plausible mobility scenarios. This provides key insights into
the problem and the involved trade-offs and enable us to assess the potential
opportunity offered by the, somewhat structured, vehicular mobility that can be
exploited by cooperative content caching schemes. The presented numerical
results exhibit complete agreement between the analytical and simulation
studies. Finally, we observe that vehicular mobility creates opportunities for
enhanced outage performance under practically relevant scenarios."
"We consider the problem of routing packets across a multi-hop network
consisting of multiple sources of traffic and wireless links while ensuring
bounded expected delay. Each packet transmission can be overheard by a random
subset of receiver nodes among which the next relay is selected
opportunistically.
  The main challenge in the design of minimum-delay routing policies is
balancing the trade-off between routing the packets along the shortest paths to
the destination and distributing traffic according to the maximum backpressure.
Combining important aspects of shortest path and backpressure routing, this
paper provides a systematic development of a distributed opportunistic routing
policy with congestion diversity ({D-ORCD}).
  {D-ORCD} uses a measure of draining time to opportunistically identify and
route packets along the paths with an expected low overall congestion. {D-ORCD}
is proved to ensure a bounded expected delay for all networks and under any
admissible traffic. Furthermore, this paper proposes a practical implementation
which empirically optimizes critical algorithm parameters and their effects on
delay as well as protocol overhead. Realistic Qualnet simulations for
802.11-based networks demonstrate a significant improvement in the average
delay over comparative solutions in the literature. %Finally, various practical
modifications to {D-ORCD} are proposed and their performance are evaluated."
"Motivated by the benefits of small world networks, we propose a
self-organization framework for wireless ad hoc networks. We investigate the
use of directional beamforming for creating long-range short cuts between
nodes. Using simulation results for randomized beamforming as a guideline, we
identify crucial design issues for algorithm design. Our results show that,
while significant path length reduction is achievable, this is accompanied by
the problem of asymmetric paths between nodes. Subsequently, we propose a
distributed algorithm for small world creation that achieves path length
reduction while maintaining connectivity. We define a new centrality measure
that estimates the structural importance of nodes based on traffic flow in the
network, which is used to identify the optimum nodes for beamforming. We show,
using simulations, that this leads to significant reduction in path length
while maintaining connectivity."
"Due to deployment in hostile environment, wireless sensor network is
vulnerable to various attacks. Exhausted sensor nodes in sensor network become
a challenging issue because it disrupts the normal connectivity of the network.
Affected nodes give rise to denial of service that resists to get the objective
of sensor network in real life. A mathematical model based on Absorbing Markov
Chain (AMC)is proposed for Denial of Sleep attack detection in sensor network.
In this mechanism, whether sensor network is affected by denial of sleep attack
or not can be decided by considering expected death time of sensor network
under normal scenario."
"An increasing number of retail energy markets show price fluctuations,
providing users with the opportunity to buy energy at lower than average
prices. We propose to temporarily store this inexpensive energy in a battery,
and use it to satisfy demand when energy prices are high, thus allowing users
to exploit the price variations without having to shift their demand to the
low-price periods. We study the battery control policy that yields the best
performance, i.e., minimizes the total discounted costs. The optimal policy is
shown to have a threshold structure, and we derive these thresholds in a few
special cases. The cost savings obtained from energy storage are demonstrated
through extensive numerical experiments, and we offer various directions for
future research."
"We consider wireless mesh networks and the problem of routing end-to-end
traffic over multiple paths for the same origin-destination pair with minimal
interference. We introduce a heuristic for path determination with two
distinguishing characteristics. First, it works by refining an extant set of
paths, determined previously by a single- or multi-path routing algorithm.
Second, it is totally local, in the sense that it can be run by each of the
origins on information that is available no farther than the node's immediate
neighborhood. We have conducted extensive computational experiments with the
new heuristic, using AODV and OLSR, as well as their multi-path variants, as
underlying routing methods. For two different CSMA settings (as implemented by
802.11) and one TDMA setting running a path-oriented link scheduling algorithm,
we have demonstrated that the new heuristic is capable of improving the average
throughput network-wide. When working from the paths generated by the
multi-path routing algorithms, the heuristic is also capable to provide a more
evenly distributed traffic pattern."
"Greedy Maximal Scheduling (GMS) is an attractive low-complexity scheme for
scheduling in wireless networks. Recent work has characterized its throughput
for the case when there is no fading/channel variations. This paper aims to
understand the effect of channel variations on the relative throughput
performance of GMS vis-a-vis that of an optimal scheduler facing the same
fading. The effect is not a-priori obvious because, on the one hand, fading
could help by decoupling/precluding global states that lead to poor GMS
performance, while on the other hand fading adds another degree of freedom in
which an event unfavourable to GMS could occur.
  We show that both these situations can occur when fading is adversarial. In
particular, we first define the notion of a {\em Fading Local Pooling factor
(F-LPF)}, and show that it exactly characterizes the throughput of GMS in this
setting. We also derive general upper and lower bounds on F-LPF. Using these
bounds, we provide two example networks - one where the relative performance of
GMS is worse than if there were no fading, and one where it is better."
"Opportunistic Routing (OR) is a novel routing technique for wireless mesh
networks that exploits the broadcast nature of the wireless medium. OR combines
frames from multiple receivers and therefore creates a form of Spatial
Diversity, called MAC Diversity. The gain from OR is especially high in
networks where the majority of links has a high packet loss probability. The
updated IEEE 802.11n standard improves the physical layer with the ability to
use multiple transmit and receive antennas, i.e. Multiple-Input and
Multiple-Output (MIMO), and therefore already offers spatial diversity on the
physical layer, i.e. called Physical Diversity, which improves the reliability
of a wireless link by reducing its error rate. In this paper we quantify the
gain from MAC diversity as utilized by OR in the presence of PHY diversity as
provided by a MIMO system like 802.11n. We experimented with an IEEE 802.11n
indoor testbed and analyzed the nature of packet losses. Our experiment results
show negligible MAC diversity gains for both interference-prone 2.4 GHz and
interference-free 5 GHz channels when using 802.11n. This is different to the
observations made with single antenna systems based on 802.11b/g, as well as in
initial studies with 802.11n."
"The growing diffusion of wireless-enabled portable devices and the recent
advances in Mobile Ad-hoc NETworks (MANETs) open new scenarios where users can
benefit from anywhere and at any time for impromptu collaboration. However,
energy constrained nodes, low channel bandwidth, node mobility, high channel
error rates, channel variability and packet loss are some of the limitations of
MANETs. MANETs presents also security challenges. These networks are prone to
malicious users attack, because any device within the frequency range can get
access to the MANET. There is a need for security mechanisms aware of these
challenges. Thus, this work aims to provide a secure MANET by changing the
frequency of data transmission. This security approach was tested, and the
results shows an interesting decreased of throughput from malicious node when
the number of frequency used is increased, that way the MANET will not waste
it's resources treating malicious packets. The other contribution of this work
is a mobility aware routing approach, which aims to provide a more reliable
routing by handling effectively the nodes mobility."
"Operational network data, management data such as customer care call logs and
equipment system logs, is a very important source of information for network
operators to detect problems in their networks. Unfortunately, there is lack of
efficient tools to automatically track and detect anomalous events on
operational data, causing ISP operators to rely on manual inspection of this
data. While anomaly detection has been widely studied in the context of network
data, operational data presents several new challenges, including the
volatility and sparseness of data, and the need to perform fast detection
(complicating application of schemes that require offline processing or
large/stable data sets to converge).
  To address these challenges, we propose Tiresias, an automated approach to
locating anomalous events on hierarchical operational data. Tiresias leverages
the hierarchical structure of operational data to identify high-impact
aggregates (e.g., locations in the network, failure modes) likely to be
associated with anomalous events. To accommodate different kinds of operational
network data, Tiresias consists of an online detection algorithm with low time
and space complexity, while preserving high detection accuracy. We present
results from two case studies using operational data collected at a large
commercial IP network operated by a Tier-1 ISP: customer care call logs and
set-top box crash logs. By comparing with a reference set verified by the ISP's
operational group, we validate that Tiresias can achieve >94% accuracy in
locating anomalies. Tiresias also discovered several previously unknown
anomalies in the ISP's customer care cases, demonstrating its effectiveness."
"The IEEE 802.15.4 is a wireless standard introduced for low power, low cost
wireless communication with moderate data rates. In the next few years, it is
expected that Low Rate Wireless Personal Area Networks (LR-WPAN) will be used
in a wide variety of embedded applications, including home automation,
industrial sensing and control, environmental monitoring and sensing. In these
applications, numerous embedded devices running on batteries are distributed in
an area communicating via wireless radios. This work presents a method which
can be used for comparing current consumption of wireless data transfer
embedded systems. This paper implements a small subset of the IEEE 802.15.4
protocol to achieve a point to point communication. The implemented protocol
uses 802.15.4 MAC compliant data and acknowledgment packets. Current
consumption is measured while doing one data packet transmission. Measurements
are compared with existing work. IEEE 802.15.4 protocol implementation is done
using Verilog language. Code implementation is done in such a manner so that it
can be ported to any platform with minimal changes. It can also be modified to
suit any special experimental setup requirements."
"In the cloud computing application area of accomplish, we find the fact that
cloud computing covers a lot of areas are its main asset. At a top level, it is
an approach to IT where many users, some even from different companies get
access to shared IT resources such as servers, routers and various file
extensions, instead of each having their own dedicated servers. This offers
many advantages like lower costs and higher efficiency. Unfortunately there
have been some high profile incidents where some of the largest cloud providers
have had outages and even lost data, and this underscores that it is important
to have backup, security and disaster recovery capabilities. In education
field, it gives better choice and flexibility to IT departments than others.
The platform and applications you use can be on-premises, off-premises, or a
combination of both, depending on your academic organization's needs. With
cloud computing in education, you get powerful software and massive computing
resources where and when you need them. Use cloud services to best combine:
*On-demand computing and storage. *A familiar development experience with
on-demand scalability. *Online services for anywhere, anytime access to
powerful web-based tools."
"One of the major task of wireless sensor network is to sense accurate data
from the physical environment. Hence in this paper, we develop an estimated
data accuracy model for randomly deployed sensor nodes which can sense more
accurate data from the physical environment. We compare our results with other
information accuracy models and shows that our estimated data accuracy model
performs better than the other models. Moreover we simulate our estimated data
accuracy model under such situation when some of the sensor nodes become
malicious due to extreme physical environment. Finally using our estimated data
accuracy model we construct a probabilistic approach for selecting an optimal
set of sensor nodes from the randomly deployed maximal set of sensor nodes in
the network."
"This paper deals with multi-user detection techniques in asynchronous
multibeam satellite communications. The proposed solutions are based on
successive interference cancellation architecture (SIC) and channel decoding
algorithms. The aim of these detection methods is to reduce the effect of
cochannel interference due to co-frequency access, and consequently, improves
the capacity of the mulitbeam communications systems, by improving frequency
reuse. Channel estimation allows the determination of interference
coefficients, which helps their effects compensation. The developed multiuser
detections techniques are iterative. Therefore, detection quality is improved
from a stage to another. Moreover, a signals combining method, which is
integrated into these detection solutions, enhances their capability. The
proposed solutions are evaluated through computer simulations, where an
asynchronous multibeam satellite link is considered over an AWGN channel. The
obtained simulation results showed the robustness of these multi-user detection
techniques."
"Accurate estimation of licensed channel Primary User's (PU) temporal
statistics is important for Dynamic Spectrum Access (DSA) systems. With
accurate estimation of the mean duty cycle, u, and the mean off- and on-times
of PUs, DSA systems can more efficiently assign PU resources to its
subscribers, thus, increasing channel utilization. This paper presents a
mathematical analysis of the accuracy of estimating u, as well as the PU mean
off- and on-times, where the estimation accuracy is expressed as the mean
squared estimation error. The analysis applies for the traffic model assuming
exponentially distributed PU off- and on-times, which is a common model in
traffic literature. The estimation accuracy is quantified as a function of the
number of samples and observation window length, hence, this work provides
guidelines on traffic parameters estimation for both energy-constrained and
delay-constrained applications. For estimating u, we consider uniform,
non-uniform, and weighted sample stream averaging, as well as maximum
likelihood estimation. The estimation accuracy of the mean PU off- and on-times
is studied when maximum likelihood estimation is employed. Furthermore, we
develop algorithms for the blind estimation of the traffic parameters based on
the derived theoretical estimation accuracy expressions. We show that the
estimation error for all traffic parameters is lower bounded for a fixed
observation window length due to the correlation between the traffic samples.
Moreover, we prove that for estimating u, maximum likelihood estimation can
yield the same estimation error as weighted sample averaging using only half
the observation window length."